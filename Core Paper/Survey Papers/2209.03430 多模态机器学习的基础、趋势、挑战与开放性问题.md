---
论文名称: Foundations & Trends in Multimodal Machine Learning Principles, Challenges, and Open Questions
tags:
  - "#多模态机器学习"
  - "#表示学习"
  - "#数据异构性"
  - "#特征交互"
  - "#语言与视觉"
  - "#多媒体"
摘要: 多模态机器学习是一个充满活力的多学科研究领域，旨在通过整合多种交流模式，包括语言、声学、视觉、触觉和生理信息，设计具有理解、推理和学习等智能能力的计算机代理。随着最近对视频理解、具身自主代理、文本到图像生成以及医疗保健和机器人等应用领域中的多传感器融合的兴趣，多模态机器学习由于数据源异质性和模式之间的相互关联，给机器学习社区带来了独特的计算和理论挑战。然而，多模态研究取得的广泛进展使得难以识别该领域的共同主题和开放性问题。通过综合历史和近期视角下的广泛应用领域和理论框架，本文旨在提供多模态机器学习的计算和理论基础概览。 我们首先定义了三个关键原则：模态异质性、连接和交互，这些原则推动了后续的创新，并提出了六个核心技术挑战的分类法：表示、对齐、推理、生成、迁移和量化，涵盖了历史和最近的趋势。将通过这个分类法展示最近的技术成就，使研究人员能够理解新方法之间的相似性和差异性。最后，我们根据分类法激励了几个未来研究中的开放性问题。
发布时间: 2022
链接: https://arxiv.org/abs/2209.03430
---

## 论文核心思想

- **三大原则：** 模态异质性、模态关联性、模态交互性是多模态机器学习的基石。
- **六大挑战：** 表示、对齐、推理、生成、迁移和量化是多模态机器学习面临的核心技术难题。
- **未来展望：** 论文指出了多模态机器学习未来研究的多个方向和开放性问题，例如理论框架构建、更高级的模态交互建模、组合性推理、高模态迁移学习、生成伦理以及模态效用量化等。

## 关键信息提取

- **模态异质性：** 不同模态数据在质量、结构和表示上存在差异。论文从元素表示、分布、结构、信息量、噪声和相关性等维度分析了模态异质性。
- **模态关联性：** 模态之间并非孤立存在，而是通过共享互补信息相互连接。论文从统计和语义角度探讨了模态连接。
- **模态交互性：**  模态在任务推理中相互作用，产生新的信息。
- **六大挑战细分：**
    - **表示：** 如何学习反映模态异质性和关联性的表示。
    - **对齐：** 如何识别模态元素之间的联系和交互。
    - **推理：** 如何利用多模态证据进行知识组合和推理。
    - **生成：** 如何生成反映跨模态交互和连贯性的多模态数据。
    - **迁移：** 如何在模态之间传递知识，解决资源受限问题。
    - **量化：** 如何实证和理论地理解多模态学习过程，包括异质性维度、模态连接类型以及学习优化挑战。
- **未来研究方向：** 论文在表示、对齐、推理、迁移、生成和量化六个挑战方向上分别提出了具体的开放性问题，例如：
    - 如何构建多模态机器学习的理论框架？
    - 如何建模更复杂的模态交互关系（因果、逻辑、时间）？
    - 如何解决多模态组合性推理的挑战？
    - 如何进行高模态学习和非并行数据处理？
    - 如何应对多模态生成模型的伦理风险？
    - 如何量化模态的效用并进行权衡选择？
    - 如何提高多模态模型的可解释性和鲁棒性？

**总结：**

该论文为多模态机器学习研究人员提供了一个全面的框架，帮助理解该领域的核心概念、挑战和未来趋势。论文提出的三大原则和六大挑战分类法有助于研究人员系统地分析和解决多模态机器学习问题。同时，论文提出的开放性问题也为未来的研究指明了方向。

## 多模态机器学习：原理、挑战与开放性问题


######   摘要。

多模态机器学习是一个充满活力的多学科研究领域，旨在通过整合多种交流模式，包括语言、声学、视觉、触觉和生理信息，设计具有理解、推理和学习等智能能力的计算机代理。随着最近对视频理解、具身自主代理、文本到图像生成以及医疗保健和机器人等应用领域中的多传感器融合的兴趣，多模态机器学习由于数据源异质性和模式之间的相互关联，给机器学习社区带来了独特的计算和理论挑战。然而，多模态研究取得的广泛进展使得难以识别该领域的共同主题和开放性问题。通过综合历史和近期视角下的广泛应用领域和理论框架，本文旨在提供多模态机器学习的计算和理论基础概览。 我们首先定义了三个关键原则：模态异质性、连接和交互，这些原则推动了后续的创新，并提出了六个核心技术挑战的分类法：表示、对齐、推理、生成、迁移和量化，涵盖了历史和最近的趋势。将通过这个分类法展示最近的技术成就，使研究人员能够理解新方法之间的相似性和差异性。最后，我们根据分类法激励了几个未来研究中的开放性问题。

多模态机器学习，表示学习，数据异构性，特征交互，语言与视觉，多媒体

copyright: rightsretainedjournalyear: 2022doi: XXXXXXX.XXXXXXXjournal: CSURjournalvolume: 1journalnumber: 1article: 1publicationmonth: 10ccs: Computing methodologies Machine learningccs: Computing methodologies Artificial intelligenceccs: Computing methodologies Computer visionccs: Computing methodologies Natural language processing

##   1\. 引言

人工智能始终致力于开发具有智能能力的计算机代理，如通过多模态经验和数据理解、推理和学习，类似于人类使用多种感官模态感知和与世界互动的方式。随着具身自主代理（Brodeur 等人，2017；Savva 等人，2019）、自动驾驶汽车（Xiao 等人，2020）、图像和视频理解（Alayrac 等人，2022；Sun 等人，2019）、图像和视频生成（Ramesh 等人，2021；Singer 等人，2022）以及在机器人（Lee 等人，2019；Marge 等人，2022）和医疗保健（Johnson 等人，2016；Liang 等人，2021a）等应用领域中的多传感器融合等近期进展，我们现在比以往任何时候都更接近能够整合和从多种感官模态中学习的智能代理。 多模态机器学习这一充满活力的跨学科研究领域，由于数据的异质性和模态之间的相互关联，带来了独特的挑战，并在多媒体（Naphade 等人，2006 年）、情感计算（Poria 等人，2017 年）、机器人技术（Kirchner 等人，2019 年；Lee 等人，2019 年）、人机交互（Obrenovic 和 Starcevic，2004 年；Sharma 等人，2002 年）和医疗保健（Cai 等人，2019 年；Muhammad 等人，2021 年）等领域具有广泛的应用。

然而，多模态研究进展的速度使得难以识别历史和近期工作中所蕴含的共同主题，以及该领域的关键开放性问题。通过综合广泛的多模态研究，本文旨在提供多模态机器学习的方法论、计算和理论基础的概述，这补充了近期在视觉和语言（Uppal 等，2022 年）、语言和强化学习（Luketina 等，2019 年）、多媒体分析（Atrey 等，2010 年）以及人机交互（Jaimes 和 Sebe，2007 年）方面的应用导向调查。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x1.png)

图 1. 多模态学习中的核心研究挑战：（1）表示研究如何表示和总结多模态数据，以反映各个模态元素之间的异质性和相互联系。（2）对齐旨在识别所有元素之间的联系和交互。（3）推理旨在通过多个推理步骤从多模态证据中组合知识以完成任务。（4）生成涉及学习生成过程以产生反映跨模态交互、结构和连贯性的原始模态。（5）迁移旨在在模态及其表示之间转移知识。（6）量化涉及实证和理论研究，以更好地理解多模态学习过程。

为了更好地理解多模态机器学习的基础，我们首先在§2 中定义了三个关键原则，这些原则推动了后续的技术挑战和创新：（1）模态是异质的，因为存在的信息通常具有不同的品质、结构和表示，（2）模态是相互关联的，因为它们通常相关并具有共性，（3）当用于任务推理时，模态相互作用产生新的信息。在这些定义的基础上，我们提出了多模态学习六个核心挑战的新分类：表示、对齐、推理、生成、迁移和量化（见图 1）。这些构成了核心的多模态技术挑战，在传统的单模态机器学习中研究不足，需要解决以推动该领域向前发展：

1.  (1)
    
    表示（§3）：我们能否学习到反映模态元素异质性和相互连接的表示？我们将涵盖以下方法：（1）表示融合：整合来自两个或更多模态的信息以捕捉跨模态交互，（2）表示协调：交换跨模态信息，目的是保持相同数量的表示但提高多模态情境化，（3）表示分裂：创建一个更大的不交表示集，反映关于内部结构的知识，如数据聚类或因子分解。
    
2.  (2)
    
    对齐（§4）：我们如何识别模态元素之间的联系和交互？对齐具有挑战性，因为它可能依赖于长距离依赖关系，涉及模糊的分割（例如，单词或话语），并且可能是一对一、多对多，或者根本不存在。我们涵盖了以下三个方面：（1）离散对齐：识别跨模态离散元素之间的联系，（2）连续对齐：对模糊分割的连续模态信号进行建模，（3）上下文表示：通过捕捉元素之间的跨模态交互来学习更好的表示。
    
3.  (3)
    
    推理（§5）被定义为通过多个推理步骤组合知识，通常用于特定任务，以利用问题结构。推理包括（1）建模组合发生的结构，（2）组合过程中的中间概念，（3）理解更抽象概念的推理范式，以及（4）在结构、概念和推理研究中利用大规模外部知识。
    
4.  (4)
    
    生成（§6）涉及学习一个生成过程以产生原始模态。我们将其子挑战分为（1）摘要：总结多模态数据以减少信息内容，同时突出输入中最显著的部分，（2）翻译：从一种模态翻译到另一种模态，保持信息内容的同时与跨模态连接保持一致，（3）创作：同时生成多个模态以增加信息内容，同时在模态内部和跨模态之间保持连贯性。
    
5.  (5)
    
    转移（§7）旨在在模态之间传递知识，通常是为了帮助目标模态，该模态可能存在噪声或资源有限。转移包括以下示例：（1）跨模态转移：将模型适应涉及主要模态的任务，（2）协同学习：通过在两种模态之间共享表示空间，将信息从次要模态传递到主要模态，（3）模型归纳：保持单个单模态模型独立，但在这类模型之间传递信息。
    
6.  (6)
    
    量化（§8）：第六个也是最后的挑战涉及实证和理论研究，以更好地理解（1）多模态数据集中异质性的维度以及它们如何随后影响建模和学习，（2）多模态数据集中存在的模态连接和交互类型以及被训练模型捕获的情况，（3）涉及异质数据的学习和优化挑战。
    

最后，我们以多模态学习的一个长期视角结束本文，通过激发由本分类法确定的开放性研究问题。本文作者还在 CVPR 2022 和 NAACL 2022 的教程中，以及 CMU 的 11-777 多模态机器学习和 11-877 多模态机器学习高级课程中，通过视觉媒介进行了介绍。鼓励读者查看这些公开的视频记录、额外的阅读材料和激发多模态学习开放性研究问题的讨论问题。

## 2\. 多模态研究的基础原理

模态是指感知或表达自然现象的方式。例如，模态包括通过麦克风记录的语音和音频、通过摄像头捕获的图像和视频，以及通过触觉传感器捕获的力量和振动。模态可以放置在从原始到抽象的连续谱上：原始模态是那些更接近传感器的，例如麦克风中的语音录音或摄像头捕获的图像。抽象模态是那些远离传感器的，例如从语音录音中提取的语言、从图像中检测到的物体，甚至是情感强度和物体类别等抽象概念。

多模态指的是涉及多种模态的情况。从研究角度来看，多模态包括对异构和互联（连接+交互）模态的计算研究。首先，模态是异构的，因为不同模态中存在的信息通常会表现出不同的品质、结构和表示。其次，这些模态不是独立的实体，而是由于互补信息而共享连接。第三，当模态用于任务时，它们以不同的方式相互作用。我们将在以下小节中扩展这些多模态研究的基础原则。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x2.png)

图 2.不同模态中存在的信息通常会显示出不同的质量、结构和表示。异质性的维度可以通过个体元素及其分布、元素结构以及模态信息、噪声和任务相关性来衡量。

### 2.1. 原则 1：模态是异构的

异质性原则反映了这样一个观察结果：不同模态中存在的信 息通常会表现出不同的品质、结构和表示。异质性应被视为一个连续体：来自同一相机且在相机磨损和磨损程度相同的情况下捕捉到相同视图的两张图像更接近同质，两种不同的语言在捕捉相同意义的同时，根据语言家族的不同而略有异质，语言和视觉甚至更加异质，等等。在本节中，我们列举了异质性的几个维度（见图 2），这些维度是互补的，可能存在重叠；每个多模态问题可能涉及多个维度的异质性。

1.  (1)
    
    元素表示：每种模态通常由一组元素组成——这是无法（或者更确切地说，用户选择不）进一步分解的最基本的数据单元（Barthes，1977；Liang，2022）。例如，文本通过一组字符记录，视频通过一组帧记录，图通过一组节点和边记录。每种模态中存在哪些基本元素，我们如何表示它们？形式上，这个维度衡量了模态元素样本空间或表示空间的异质性。
    
2.  (2)
    
    分布指的是模态中元素的频率和可能性。元素通常遵循独特的分布，例如，语言语料库中的单词遵循 Zipf 定律就是一个经典例子。分布异质性则指元素频率和可能性的差异，如记录信号中的不同频率和元素的密度。
    
3.  (3)
    
    结构：自然数据在个体元素组成整个模态的方式上表现出结构（Bronstein 等人，2021 年）。例如，图像在单个对象元素上表现出空间结构，语言按层次由单个单词组成，信号在时间上表现出时间结构。结构异质性指的是这种潜在结构之间的差异。
    
4.  (4)
    
    信息衡量每个模态中存在的总信息内容。随后，信息异质性衡量不同模态之间信息内容的差异，这可以通过信息论指标（香农，1948 年）进行正式测量。
    
5.  (5)
    
    噪声：噪声可以在自然发生的数据的多个层面上引入，也可以在数据记录过程中引入。自然数据噪声包括遮挡、人类生成数据的不完美（例如，不完美的键盘输入或不清的语音），或由于传感器故障导致的数据模糊（Liang 等人，2021a）。噪声异质性衡量了不同模态之间噪声分布的差异，以及信噪比的不同。
    
6.  (6)
    
    相关性：最后，每种模态对特定任务和上下文具有不同的相关性——某些模态可能比其他模态对某些任务更有用（Gat 等，2021）。任务相关性描述了模态如何用于推理，而上下文相关性描述了模态如何与其他模态进行语境化。
    

在研究单模态和多模态数据时，考虑这些异质性的维度是有用的。在单模态情况下，通常设计专门的编码器来捕捉每个模态的独特特征（Bronstein 等人，2021 年）。在多模态情况下，在学习表示和捕捉对齐时，建模异质性是有用的，并且是量化多模态模型的关键子挑战（Liang 等人，2022 年）。

### 2.2.原则 2：模态是相互关联的

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x3.png)

图 3. 模态连接描述了模态之间的关系和共同点，例如语言和图像中相同概念之间的对应关系或空间和时间维度上的依赖关系。连接可以通过统计和语义两个角度进行研究。

尽管模态异质，但它们通常由于共享的互补信息而相互连接。共享信息的存在通常与仅存在于单个模态中的独特信息形成对比（威廉姆斯和比尔，2010 年）。模态连接描述了信息可以在模态之间共享的程度和维度。在推理多模态数据中的连接时，考虑自下而上（统计）和自上而下（语义）的方法是有帮助的（见图 3）。从统计数据驱动的角度来看，连接是通过多模态数据中的分布模式来识别的，而语义方法则是基于我们对模态如何共享和包含独特信息的领域知识来定义连接的。

1.  (1)
    
    统计关联存在于一个变量的值与另一个变量的值相关联时。例如，两个元素可能同时出现，导致它们同时出现的频率更高。从统计学的角度来看，这可能导致相关性——元素线性相关程度的度量，或其他的非线性关联。从数据驱动的角度来看，发现哪些元素相互关联对于在多模态表示和对齐期间建模跨模态的联合分布很重要（Tian 等，2020b）。
    
2.  (2)
    
    统计依赖比关联更深，需要了解两个元素之间确切类型的统计依赖。例如，是否存在一个元素对另一个元素的因果依赖，或者是否存在一个潜在的混杂因素导致两个元素同时存在？其他形式的依赖可能是空间或时间的：一个元素出现在另一个元素之上，或者在其之后。通常，虽然可以从数据中纯粹估计统计关联，但理解统计依赖的性质需要一些关于元素及其潜在关系的知识（Nickel 等人，2015；Turney 和 Littman，2005）。
    
3.  (3)
    
    语义对应可以看作是确定一个模态中的哪些元素与另一个模态中的元素具有相同语义意义的问题（Otto 等人，2020）。在许多与语言基础（Chandu 等人，2021）、翻译和检索（Plummer 等人，2015）以及跨模态对齐（Tan 和 Bansal，2019）相关的问题中，识别对应关系是基本的。
    
4.  (4)
    
    语义关系：最后，语义关系概括了语义对应关系：不是模态元素共享完全相同的意义，而是语义关系包括一个属性，描述两个模态元素之间关系的确切性质，例如语义、逻辑、因果或功能关系。识别这些语义相关的联系对于高级推理很重要（Marsh 和 Domas White，2003；Barthes，1977）。
    

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x4.png)

图 4. 模态交互的几个维度：（1）交互信息研究是否涉及共同冗余信息或独特非冗余信息；（2）交互力学研究交互发生的方式；（3）交互响应研究在存在多个模态的情况下，推断任务如何变化。

### 2.3.原则 3：模态交互

模态交互研究模态元素如何相互作用，在任务推理中整合在一起时产生新的信息。我们注意到模态连接和交互之间的重要区别：连接存在于多模态数据本身中，而交互只有在模态整合并共同处理时才会出现，以带来新的响应。在图 4 中，我们提供了一个高级的交互维度示例。

1.  (1)
    
    交互信息研究在交互中涉及到的连接信息类型。当一个交互涉及两种模态共有的共享信息时，该交互是冗余的，而非冗余的交互则是指不单纯依赖共享信息，而是依赖于共享、独特或甚至可能是协同信息的不同比例（Williams 和 Beer，2010）。
    
2.  (2)
    
    交互机制是在整合模态元素进行任务推理时涉及的功能算子。例如，交互可以表示为统计加性、非加性和非线性形式（Jayakumar 等，2020 年），以及从语义角度，两个元素通过逻辑、因果或时间操作进行交互（Unsworth 和 Cléirigh，2014 年）。
    
3.  (3)
    
    交互响应研究在存在多种模态的情况下，推断出的响应如何变化。例如，通过细分冗余交互，我们可以认为两种模态创建了一个等效响应，如果多模态响应与任一模态的响应相同，或者增强，如果多模态响应显示出更高的置信度。另一方面，如调制或涌现等非冗余交互发生在存在不同的多模态与单模态响应时（Partan 和 Marler，1999）。
    

### 2.4 核心技术挑战

表 1. 本表总结了我们在多模态机器学习领域中的核心挑战分类，包括其子挑战、相应方法的类别以及代表性示例。我们相信这个分类法可以帮助我们记录该领域的快速进展并更好地识别开放的研究问题。

<table id="S2.T1.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><tbody data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><tr id="S2.T1.3.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.1.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.1.1.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">挑战</span></span></span></span></td><td id="S2.T1.3.1.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.1.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">子挑战</span></span></span></span></td><td id="S2.T1.3.1.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.1.3.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">方法与关键示例</span></span></span></span></td><td id="S2.T1.3.1.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.2.1.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">表示（§3）</span></span></span></span></td><td id="S2.T1.3.2.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.2.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">融合（§3.1）</span></span></span></span></td><td id="S2.T1.3.2.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">摘要（Jayakumar 等人，2020；Zadeh 等人，2017）&amp; 原始（Barnum 等人，2020；Rajagopalan 等人，2016）融合</span></span></span></td><td id="S2.T1.3.2.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.3.1"><span id="S2.T1.3.3.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.3.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.3.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.3.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.4.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.4.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.4.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">协调（§3.2）</span></span></span></span></td><td id="S2.T1.3.4.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">强（Frome 等人，2013；Radford 等人，2021）与部分（Vendrov 等人，2015；张等人，2016）协调</span></span></span></td><td id="S2.T1.3.4.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.5" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.5.1"><span id="S2.T1.3.5.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.5.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.5.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.5.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.6" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.6.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.6.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.6.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">裂变（§3.3）</span></span></span></span></td><td id="S2.T1.3.6.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">模态级（Hessel 和 Lee，2020；Tsai 等，2019b）与细粒度（Abavisani 和 Patel，2018；Chen 等，2021b）分裂</span></span></span></td><td id="S2.T1.3.6.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.7" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.7.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.7.1.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">对齐（§4）</span></span></span></span></td><td id="S2.T1.3.7.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.7.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">离散连接（§4.1）</span></span></span></span></td><td id="S2.T1.3.7.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">本地（Cirik 等，2018b；Hsu 等，2018）与全局（Li 等，2022b）对齐</span></span></span></td><td id="S2.T1.3.7.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.8" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.8.1"><span id="S2.T1.3.8.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.8.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.8.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.8.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.9" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.9.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.9.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.9.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">连续对齐（§4.2）</span></span></span></span></td><td id="S2.T1.3.9.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">扭曲（Hu 等，2019b；Haresh 等，2021）&amp;分割（Sun 等，2019）</span></span></span></td><td id="S2.T1.3.9.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.10" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.10.1"><span id="S2.T1.3.10.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.10.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.10.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.10.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.11" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.11.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.11.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.11.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">上下文化（§4.3）</span></span></span></span></td><td id="S2.T1.3.11.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">联合（Li 等，2019a）、跨模态（Hendricks 等，2021；Lu 等，2019）和图形（Yang 等，2021）</span></span></span></td><td id="S2.T1.3.11.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.12" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.12.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.12.1.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">推理（§5）</span></span></span></span></td><td id="S2.T1.3.12.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.12.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">结构建模（§5.1）</span></span></span></span></td><td id="S2.T1.3.12.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">分层（Andreas et al., 2016）、时序（Xiong et al., 2016）、交互式（Luketina et al., 2019）与发现（Pérez-Rúa et al., 2019）</span></span></span></td><td id="S2.T1.3.12.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.13" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.13.1"><span id="S2.T1.3.13.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.13.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.13.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.13.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.14" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.14.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.14.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.14.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">中间概念（§5.2）</span></span></span></span></td><td id="S2.T1.3.14.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">注意（Xu 等，2015a），离散符号（Amizadeh 等，2020；Vedantam 等，2019）与语言（Hudson 和 Manning，2019a；Zeng 等，2022）</span></span></span></td><td id="S2.T1.3.14.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.15" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.15.1"><span id="S2.T1.3.15.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.15.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.15.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.15.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.16" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.16.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.16.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.16.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">推理范式（§5.3）</span></span></span></span></td><td id="S2.T1.3.16.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">逻辑（Gokhale 等，2020；Suzuki 等，2019）&amp; 因果（Agarwal 等，2020；Niu 等，2021；Yi 等，2019）</span></span></span></td><td id="S2.T1.3.16.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.17" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.17.1"><span id="S2.T1.3.17.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.17.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.17.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.17.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.18" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.18.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.18.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.18.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">外部知识（§5.4）</span></span></span></span></td><td id="S2.T1.3.18.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">知识图谱（Gui 等人，2021；Zhu 等人，2015b）与常识（Park 等人，2020；Zellers 等人，2019）</span></span></span></td><td id="S2.T1.3.18.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.19" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.19.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.19.1.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">生成（§6）</span></span></span></span></td><td id="S2.T1.3.19.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.19.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">摘要（§6.1）</span></span></span></span></td><td id="S2.T1.3.19.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">提取式（陈和诸葛，2018b；UzZaman 等人，2011）&amp; 抽取式（李等人，2017；Palaskar 等人，2019）</span></span></span></td><td id="S2.T1.3.19.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.20" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.20.1"><span id="S2.T1.3.20.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.20.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.20.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.20.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.21" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.21.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.21.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.21.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">翻译（§6.2）：翻译（第 6.2 节）</span></span></span></span></td><td id="S2.T1.3.21.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">基于示例的（Karpathy 等，2014；Lebret 等，2015）与生成式（Ahuja 等，2020；Jamaludin 等，2019；Ramesh 等，2021）</span></span></span></td><td id="S2.T1.3.21.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.22" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.22.1"><span id="S2.T1.3.22.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.22.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.22.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.22.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.23" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.23.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.23.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.23.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">创建（§6.3）</span></span></span></span></td><td id="S2.T1.3.23.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">条件解码（Denton and Fergus，2018；Oord et al，2018；Zhu et al，2021）</span></span></span></td><td id="S2.T1.3.23.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.24" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.24.1"><span id="S2.T1.3.24.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.24.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.24.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.24.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.25" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.25.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.25.1.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">迁移（§7）</span></span></span></span></td><td id="S2.T1.3.25.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.25.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">跨模态迁移（§7.1）</span></span></span></span></td><td id="S2.T1.3.25.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">调整（Rahman 等，2020；Tsimpoukelli 等，2021）、多任务（Singh 等，2021；Liang 等，2022）与迁移（Lu 等，2021）</span></span></span></td><td id="S2.T1.3.25.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.26" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.26.1"><span id="S2.T1.3.26.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.26.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.26.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.26.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.27" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.27.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.27.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.27.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">协同学习（§7.2）</span></span></span></span></td><td id="S2.T1.3.27.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">表示（贾等，2021；扎德赫等，2020）与生成（范等，2019；谭和班萨尔，2020）</span></span></span></td><td id="S2.T1.3.27.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.28" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.28.1"><span id="S2.T1.3.28.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.28.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.28.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.28.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.29" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.29.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.29.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.29.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">模型归纳（§7.3）</span></span></span></span></td><td id="S2.T1.3.29.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">协同训练（Blum and Mitchell，1998；Dunnmon 等，2020）&amp; 协同正则化（Sridharan and Kakade，2008；Yang 等，2019）</span></span></span></td><td id="S2.T1.3.29.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.30" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.30.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.30.1.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">量化（§8）</span></span></span></span></td><td id="S2.T1.3.30.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.30.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">异构性（§8.1）</span></span></span></span></td><td id="S2.T1.3.30.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">重要性（Gat et al., 2021；Park et al., 2018），偏差（Hendricks et al., 2018；Peña et al., 2020）与噪声（Ma et al., 2021）</span></span></span></td><td id="S2.T1.3.30.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.31" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.31.1"><span id="S2.T1.3.31.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.31.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.31.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.31.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.32" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.32.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.32.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.32.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">互连（§8.2）</span></span></span></span></td><td id="S2.T1.3.32.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">连接（Aflalo 等人，2022；Cao 等人，2020；Thrush 等人，2022）与交互（Hessel 和 Lee，2020；Liang 等人，2023；Wang 等人，2021）</span></span></span></td><td id="S2.T1.3.32.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.33" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.33.1"><span id="S2.T1.3.33.1.1">5</span><span>&nbsp;</span></td><td id="S2.T1.3.33.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.33.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.33.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr><tr id="S2.T1.3.34" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><td id="S2.T1.3.34.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td><td id="S2.T1.3.34.2" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"><span id="S2.T1.3.34.2.1" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</span><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">学习（§8.3）</span></span></span></span></td><td id="S2.T1.3.34.3" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f" data-immersive-translate-paragraph="1"><span data-immersive-translate-translation-element-mark="1" lang="zh-CN"><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">泛化（Liang 等，2022；Reed 等，2022）、优化（Wang 等，2020b；Wu 等，2022）与权衡（Liang 等，2021a）</span></span></span></td><td id="S2.T1.3.34.4" data-immersive-translate-walked="39fd0eea-6179-496b-bd75-e2bbd128f01f"></td></tr></tbody></table>

基于这三个核心原则以及对我们近期工作的详细审查，我们提出了一种新的分类法来描述多模态研究中的核心技术挑战：表示、对齐、推理、生成、迁移和量化。在表 1 中，我们总结了这六个核心挑战、其子挑战、相应方法的类别以及每个类别中的近期示例。在接下来的章节中，我们详细描述了我们的新分类法，并重新审视了异质性、连接和交互的原则，以了解它们如何提出研究问题和激发在这六个挑战中的研究。

## 挑战 1：表示

第一个基本挑战是学习反映不同模态之间个体元素之间跨模态交互的表示。这一挑战可以被视为学习元素之间的“局部”表示，或使用整体特征的表示。本节涵盖以下内容：（1）表示融合：整合来自 2 个或更多模态的信息，有效减少单独表示的数量，（2）表示协调：通过交换跨模态信息，以保持相同数量的表示但提高多模态情境化，（3）表示分裂：创建一个新的解耦表示集，通常比输入集更大，反映关于内部结构的知识，如数据聚类或因子分解（图 5）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x5.png)

图 5.挑战 1 旨在通过以下方式学习反映单个模态元素之间跨模态交互的表示：(1)融合：整合信息以减少单独表示的数量，(2)协调：交换跨模态信息，目标是保持相同数量的表示但提高多模态情境化，(3)分裂：创建一个更大的解耦表示集，反映关于内部结构的知识。

### 3.1.子挑战 1a：表示融合

表示融合旨在学习一个联合表示，该表示模型化不同模态中单个元素之间的跨模态交互，从而有效地减少单独表示的数量。我们将这些方法分为与抽象模态的融合和与原始模态的融合（图 6）。在抽象模态的融合中，首先应用合适的单模态编码器来捕捉每个元素（或整个模态）的整体表示，之后使用几个表示融合的构建块来学习联合表示。因此，融合发生在抽象表示层面。另一方面，与原始模态的融合涉及在非常早期的阶段进行表示融合，预处理最少，可能甚至涉及原始模态本身。

融合抽象模态：我们开始处理抽象表示的表示融合，涉及加法和乘法交互。这些算子可以被视为可微的构建块，它们结合来自两个数据流的信息，可以灵活地插入到几乎任何单模态机器学习管道中。对于单模态数据或特征 $𝐱1subscript1\\mathbf{x}\_${1} 和 𝐱2subscript2\\mathbf{x}\_{2} ，加性融合可以看作是学习一个新的联合表示 𝐳mm\=w0+w1𝐱1+w2𝐱2+ϵsubscriptsubscript0subscript1subscript1subscript2subscript2\\mathbf{z}\_{\\textrm{mm}}=w\_{0}+w\_{1}\\mathbf{x}\_{1}+w\_{2}\\mathbf{x}\_{2}+\\epsilon ，其中 w1subscript1w\_{1} 和 w2subscript2w\_{2} 是用于加性融合 𝐱1subscript1\\mathbf{x}\_{1} 和 𝐱2subscript2\\mathbf{x}\_{2} 学习的权重， w0subscript0w\_{0} 是偏置项， ϵ\\epsilon 是误差项。如果直接将联合表示 𝐳mmsubscript\\mathbf{z}\_{\\textrm{mm}} 作为预测 y^\\hat{y} ，那么加性融合类似于晚期或集成融合 y^\=f1(𝐱1)+f2(𝐱2)subscript1subscript1subscript2subscript2\\hat{y}=f\_{1}(\\mathbf{x}\_{1})+f\_{2}(\\mathbf{x}\_{2}) ，与单模态预测器 f1subscript1f\_{1} 和 f2subscript2f\_{2} （Friedman 和 Popescu，2008）。否则，加性表示 𝐳mmsubscript\\mathbf{z}\_{\\textrm{mm}} 也可以进行后续的单模态或多模态处理（Baltrušaitis 等，2018）。乘法交互将加法交互扩展到包括交叉项 w3(𝐱1×𝐱2)subscript3subscript1subscript2w\_{3}(\\mathbf{x}\_{1}\\times\\mathbf{x}\_{2}) 。 这些模型在统计学中得到了广泛的应用，其中它可以被解释为 𝐱1subscript1\\mathbf{x}\_{1} 对 𝐱2subscript2\\mathbf{x}\_{2} 和 yy 之间线性关系的调节效应（Baron 和 Kenny，1986）。总的来说，纯粹加性交互作用 𝐳mm\=w0+w1𝐱1+w2𝐱2subscriptsubscript0subscript1subscript1subscript2subscript2\\mathbf{z}\_{\\textrm{mm}}=w\_{0}+w\_{1}\\mathbf{x}\_{1}+w\_{2}\\mathbf{x}\_{2} 可以被视为输入模态 𝐱1subscript1\\mathbf{x}\_{1} 和 𝐱2subscript2\\mathbf{x}\_{2} 之间的一阶多项式，结合加性和乘性 𝐳mm\=w0+w1𝐱1+w2𝐱2+w3(𝐱1×𝐱2)subscriptsubscript0subscript1subscript1subscript2subscript2subscript3subscript1subscript2\\mathbf{z}\_{\\textrm{mm}}=w\_{0}+w\_{1}\\mathbf{x}\_{1}+w\_{2}\\mathbf{x}\_{2}+w\_{3}(\\mathbf{x}\_{1}\\times\\mathbf{x}\_{2}) 则捕捉到二阶多项式。

为了进一步超越一阶和二阶交互，张量被特别设计来显式捕捉跨模态的高阶交互（Zadeh 等人，2017 年）。对于单模态数据 𝐱1,𝐱2subscript1subscript2\\mathbf{x}\_{1},\\mathbf{x}\_{2} ，张量被定义为 𝐳mm\=𝐱1⊗𝐱2subscripttensor-productsubscript1subscript2\\mathbf{z}\_{\\textrm{mm}}=\\mathbf{x}\_{1}\\otimes\\mathbf{x}\_{2} ，其中 ⊗tensor-product\\otimes 表示外积（Ben-Younes 等人，2017 年；Fukui 等人，2016 年）。高阶张量积表示元素之间的高阶多项式交互（Hou 等人，2019 年）。然而，计算张量积是昂贵的，因为它们的维度随着模态数量的指数增长，因此已经提出了基于低秩分解的几种高效近似方法（Hou 等人，2019 年；Liu 等人，2018 年）。最后，乘性交互（MI）将加法和乘法算子推广到包括可学习的参数，这些参数可以捕捉二阶交互（Jayakumar 等人，2020 年）。在其最一般的形式中，MI 定义了一个双线性积 𝐳mm\=𝐱1𝕎𝐱2+𝐱1⊤𝐔+𝐕𝐱2+𝐛subscriptsubscript1subscript2superscriptsubscript1topsubscript2\\mathbf{z}\_{\\textrm{mm}}=\\mathbf{x}\_{1}\\mathbb{W}\\mathbf{x}\_{2}+\\mathbf{x}\_{1}^{\\top}\\mathbf{U}+\\mathbf{V}\\mathbf{x}\_{2}+\\mathbf{b} ，其中 𝕎,𝐔,𝐙\\mathbb{W},\\mathbf{U},\\mathbf{Z} 和 𝐛\\mathbf{b} 是可训练的参数。

多模态门控单元/注意力单元学习动态变化的表示，以适应每个输入（Chaplot 等人，2018；Wang 等人，2020b）。其通用形式可以写为 𝐳mm\=𝐱1⊙h(𝐱2)subscriptdirect-productsubscript1subscript2\\mathbf{z}\_{\\textrm{mm}}=\\mathbf{x}\_{1}\\odot h(\\mathbf{x}\_{2}) ，其中 hh 表示具有 sigmoid 激活的函数， ⊙direct-product\\odot 表示逐元素乘积。 h(𝐱2)subscript2h(\\mathbf{x}\_{2}) 通常被称为“注意力权重”，这些权重是从 𝐱2subscript2\\mathbf{x}\_{2} 学习以关注 𝐱1subscript1\\mathbf{x}\_{1} 的。最近的研究探讨了学习注意力权重的更多表达形式，例如使用查询-键-值机制（Tsai 等人，2019a）、全连接神经网络层（Arevalo 等人，2017；Chaplot 等人，2018）或甚至用于更尖锐注意力的硬门控单元（Chen 等人，2017）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x6.png)

图 6.我们将表示融合方法分为两类：（1）与抽象模态的融合，其中单模态编码器首先捕获每个元素的总体表示，然后在相对同质化的表示中进行融合；（2）与原始模态的融合，这涉及到在非常早期的阶段进行表示融合，可能直接涉及异构的原始模态。

融合原始模态涉及在非常早期的阶段进行表示融合，可能甚至涉及原始模态本身。这些方法通常类似于早期融合（Baltrušaitis 等，2018），在应用预测模型之前对输入数据进行拼接（即， 𝐳mm\=\[𝐱1,𝐱2\]subscriptsubscript1subscript2\\mathbf{z}\_{\\textrm{mm}}=\\left\[\\mathbf{x}\_{1},\\mathbf{x}\_{2}\\right\] ）。在原始模态级别进行融合更具挑战性，因为原始模态可能表现出更多异质维度。尽管如此，Barnum 等（2020）证明了在早期融合中的鲁棒性优势，而 Gadzicki 等（2020）也发现复杂的早期融合可以优于抽象融合。为了在复杂的早期融合期间处理更大的异质性，许多方法依赖于适用于两种模态的通用编码器，例如卷积层（Barnum 等，2020；Gadzicki 等，2020）和 Transformer（Liang 等，2022；Likhosherstov 等，2022）。然而，这些复杂的非加性融合模型实际上是否真的学习了模态元素之间的非加性交互？Hessel 和 Lee（2020）认为并不一定。我们在量化挑战（§8）中涵盖了这些基本分析问题以及其他问题。

### 3.2.子挑战 1b：表示协调

表示协调旨在学习通过其相互连接协调的多模态上下文表示（图 7）。与表示融合不同，协调保持相同的表示数量但改进了多模态上下文。我们首先讨论强协调，它强制模态元素之间具有强等价性，然后转向部分协调，它捕捉更一般的连接，如相关性、顺序、层次或超越相似性的关系。

强协调旨在在协调空间中将语义对应的模态紧密地放在一起，从而在模态元素之间强制执行强等价性。例如，这些模型会鼓励将单词“狗”和狗的图像表示得靠近（即，语义上积极的成对），而将单词“狗”和汽车图像之间的距离拉远（即，语义上消极的成对）（Frome 等人，2013）。协调距离通常是余弦距离（Mekhaldi，2007；Wehrmann 等人，2018）或最大间隔损失（Hu 等人，2019a）。最近的研究通过扩大图像和文本对的对比学习来探索大规模表示协调（Radford 等人，2021），并发现对比学习可以证明地捕捉到两个视图之间的冗余信息（Tian 等人，2020a；Tosh 等人，2021）（但不是非冗余信息）。除了对比学习之外，几种方法通过将一个模态的对应数据映射到另一个模态来学习协调空间（Dyer，2014）。例如，Socher 等人（2013）将图像嵌入映射到单词嵌入空间，用于零样本图像分类。 类似的想法被用于学习文本、视频和音频之间的协调表示（Pham 等，2019），以及预训练语言模型和图像特征之间的协调表示（Tan 和 Bansal，2020）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x7.png)

图 7.存在一系列表示协调函数：强协调旨在在所有维度上强制执行强等价性，而在部分协调中，只有某些维度可能被协调以捕捉更普遍的联系，例如相关性、顺序、层次或关系。

部分协调：不是通过严格的强协调来捕捉等价性，而是部分协调捕捉更一般的模态连接，如相关性、顺序、层次或关系。为了实现这些目标，部分协调模型在语义相似性之外对表示空间施加不同类型的约束，也许只对表示的某些维度施加。

典型相关分析（CCA）计算一个线性投影，以最大化两个随机变量之间的相关性，同时强制新表示中的每个维度相互正交（Thompson，2000）。CCA 模型已被广泛用于跨模态检索（Rasiwasia 等人，2010）、音频-视觉信号分析（Sargin 等人，2007）和情感识别（Nemati 等人，2019）。为了提高 CCA 的表达能力，已经提出了几种非线性扩展，包括核 CCA（Lai 和 Fyfe，2000）、深度 CCA（Andrew 等人，2013）和 CCA 自编码器（Wang 等人，2015）。

有序和分层空间：另一个表示协调的例子来自图像和语言的顺序嵌入（Vendrov 等人，2015），该嵌入旨在捕捉语言和图像嵌入上的部分顺序，以在协调空间中强制执行层次结构。Young 等人（2014）也提出了一种类似模型，其中使用指称图来诱导这种部分顺序层次结构。

关系协调：为了学习一个捕捉元素之间语义关系的协调空间，张等人（2016）使用文本和图像的结构化表示来创建多模态概念分类法。Delaherche 和 Chetouani（2010）学习捕获层次关系的协调表示，而 Alviar 等人（2020）使用部分相关度度量应用语音和音乐的多尺度协调。最后，Xu 等人（2015b）使用柯西损失来增强对异常值的鲁棒性。

### 3.3.子挑战 1c：表示分解

最终，表示分解旨在创建一组新的解耦表示（通常比输入表示集更大），这些表示反映了关于内部多模态结构的知识，例如数据聚类、独立变化因素或模态特定信息。与联合和协调表示相比，表示分解可以实现细致的解释和细粒度的可控性。根据解耦因素的粒度，方法可以分为模态级和细粒度分解（图 8）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x8.png)

图 8. 表示分解创建了一个更大的解耦表示集，反映了关于内部结构的知识。（1）模态级分解主要在每个模态中将模态特定信息分解，同时在两个模态中冗余的多模态信息，而（2）细粒度分解试图进一步将多模态数据分解成单个子空间。

模态级分解旨在将信息分解为每个模态特有的信息和在两个模态中都冗余的多模态信息（Hsu 和 Glass，2018；Tsai 等人，2019b）。解耦表示学习旨在学习相互独立的潜在变量，每个变量解释数据的一个特定变化（Bengio 等人，2013；Higgins 等人，2016），并且通过在模态特定和多模态潜在变量上施加独立性约束，对模态级分解很有用（Hsu 和 Glass，2018；Tsai 等人，2019b）。Tsai 等人（2019b）和 Hsu 与 Glass（2018）研究了分解的多模态表示，并证明了模态特定和多模态因素对生成和预测的重要性。Shi 等人（2019）使用专家混合层研究多模态变分自编码器中的模态级分解，而 Wu 和 Goodman（2018）则使用专家乘积层。

后处理表示解耦适用于难以重新训练解耦模型的情况，尤其是对于大型预训练的多模态模型。经验多模态加性函数投影（EMAP）（Hessel 和 Lee，2020）是一种用于多模态任务中后处理解耦单模态（加性）贡献效应的方法，适用于任意多模态模型和任务。EMAP 也与使用 Shapley 值进行特征解耦和解释（Merrick 和 Taly，2020）密切相关，这也可以用于一般模型中的后处理表示解耦。

精细粒度分裂：不仅将多模态数据分解为单个模态表示，精细粒度分裂还试图进一步将多模态数据分解为模态覆盖的各个子空间（Vidal，2011）。基于语义相似性分组数据的聚类方法（Madhulatha，2012）已与多模态网络集成，用于端到端表示分裂和预测。例如，Hu 等人（2019a）将表示中的 kk -means 聚类与无监督视听学习相结合。Chen 等人（2021b）将 kk -means 聚类与视频上的自监督对比学习相结合。子空间聚类（Abavisani 和 Patel，2018）、近似图拉普拉斯算子（Khan 和 Maji，2019）、共轭混合模型（Khalidov 等人，2011）和字典学习（Kim 等人，2016）也已与多模态模型集成。受类似表示分裂目标的启发，矩阵分解技术也在多模态预测（Aktukmak 等人，2019）和图像检索（Caicedo 和 González，2012）中看到了几个应用。

##   挑战 2：对齐

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x9.png)

图 9. 对齐旨在识别模态元素之间的跨模态连接和交互。最近的工作包括（1）离散对齐以识别离散元素之间的连接，（2）连续信号与模糊分割的连续对齐，以及（3）上下文表示学习以捕捉连接元素之间的这些跨模态交互。

第二个挑战是识别多个模态元素之间的跨模态连接和交互。例如，在分析人类受试者的语音和手势时，我们如何将特定的手势与说话的词语或话语对齐？模态之间的对齐具有挑战性，因为它可能依赖于长距离依赖关系，涉及模糊的分割（例如，词语或话语），并且可能是一对一、多对多，或者根本不存在。本节涵盖了涉及以下方面的多模态对齐的最新工作：（1）离散对齐：识别跨模态离散元素之间的连接，（2）连续对齐：对具有模糊分割的连续模态信号进行建模，（3）上下文表示：通过捕获元素之间的跨模态交互来学习更好的多模态表示（图 9）。

### 4.1.子挑战 2a：离散对齐

第一个子挑战旨在识别多个模态的离散元素之间的联系。我们描述了最近在以下方面的工作：（1）局部对齐以发现给定匹配对模态元素之间的联系；（2）全局对齐，其中必须全局执行对齐以学习联系和匹配（图 10）。

本地对齐在多模态任务中特别适用，这些任务中有明确的分割成离散元素，如文本中的单词或图像或视频中的对象边界框（例如，如视觉核心词消歧（Kottur 等人，2018 年）、视觉指代表达式识别（Cirik 等人，2018a，2020 年）和跨模态检索（Frome 等人，2013 年；Plummer 等人，2015 年）等任务）。当我们有以连接模态对的形式存在的监督数据时，对比学习是一种流行的方法，其目标是匹配不同模态中表达相同概念的表示（Baltrušaitis 等人，2018 年）。已经提出了从不同数量的成对（Cao 等人，2017 年；Huang 等人，2017 年）和无配对（Grave 等人，2019 年）数据中学习对齐空间的目标函数。许多强制执行强（Frome 等人，2013 年；Liang 等人，2021b）或部分（Andrew 等人，2013 年；Vendrov 等人，2015 年；Zhang 等人，2016 年）表示协调（§3.2）的想法也适用于本地对齐。 一些例子包括将书籍与其对应的电影/剧本进行对齐（Zhu 等人，2015a），将指称表达式与视觉对象进行匹配（Mao 等人，2016），以及寻找图像区域与其描述之间的相似性（Hu 等人，2016）。局部对齐方法也使得学习基于语言但同时也基于视觉（Huang 等人，2017）、声音（Cirik 等人，2018b；Socher 等人，2013）和多媒体（Zhu 等人，2015a）等额外模态的共享语义概念成为可能，这些概念对于下游任务很有用。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x10.png)

图 10. 离散对齐识别离散元素之间的连接，包括（1）局部对齐以发现匹配对之间的连接，以及（2）全局对齐，其中必须全局执行对齐以学习模态元素之间的连接和匹配。

全局对齐：当没有可用的事实模态配对时，必须在两种模态的所有元素之间进行全局对齐。基于最优传输（OT）（Villani，2009）（属于更广泛的匹配算法集）的方法是潜在解决方案，因为它们将配对作为散度最小化问题提出，从而共同优化协调函数和模态元素之间的最优耦合。这些方法适用于对齐多模态表示空间（Pramanick 等，2022；Li 等，2022b）。为了缓解计算问题，一些最近的研究将它们与神经网络（Chen 等，2020b）相结合，用熵正则化近似最优传输（Wei 等，2018），并为高效学习制定凸松弛（Grave 等，2019）。

### 4.2.子挑战 2b：连续对齐

截至目前，我们已做出的一项重要假设是模态元素已经分割和离散化。虽然某些模态具有明显的分割（例如，句子中的单词/短语或图像中的对象区域），但在许多情况下，分割并不容易提供，例如在连续信号（例如，金融或医疗时间序列）、时空数据（例如，卫星或天气图像）或没有明显语义边界的（例如，MRI 图像）数据中。在这些情况下，最近提出了基于变形和分割的方法：

连续扭曲旨在通过将它们表示为连续表示空间并在这两个表示空间之间建立桥梁，来对两组模态元素进行对齐。对抗训练是将一个表示空间扭曲到另一个空间的一种流行方法。最初用于领域自适应（Ben-David 等人，2006 年），对抗训练学习在领域分类器无法识别特征来自哪个领域（Ajakan 等人，2014 年）的域之间学习领域不变表示。这些想法已被扩展到对齐多模态空间（Hsu 等人，2018 年；Hu 等人，2019b；Munro 和 Damen，2020 年）。Hsu 等人（2018 年）使用对抗训练对齐图像和医疗报告，Hu 等人（2019b）设计了一个用于跨模态检索的对抗网络，Munro 和 Damen（2020 年）为多模态动作识别设计了自监督对齐和对抗对齐目标。动态时间扭曲（DTW）（Kruskal，1983 年）是一种相关方法，用于分割和对齐多视图时间序列数据。 DTW 测量两个序列之间的相似度，并通过时间扭曲（插入帧）找到它们之间的最佳匹配，使得它们在分段时间边界上对齐。对于多模态任务，有必要设计模态之间的相似性度量（Anguera 等人，2014；Tapaswi 等人，2015）。DTW 通过 CCA 扩展，将模态映射到一个协调空间，允许不同模态流之间同时进行对齐（通过 DTW）和协调（通过 CCA）（Trigeorgis 等人，2017）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x11.png)

图 11. 连续对齐解决了难以对齐连续信号的问题，其中元素分割不易获得。我们涵盖了相关工作，包括（1）表示空间的连续变形和（2）将连续信号以适当的粒度分割成离散元素的方法。

模态分割涉及将高维数据划分为具有语义意义的边界元素。一个常见问题涉及时间分割，其目标是发现序列数据中的时间边界。几种时间分割方法包括强制对齐，这是一种将离散语音单元与转录中的单个单词对齐的流行方法（Yuan 等人，2008 年）。Malmaud 等人（2015 年）使用因子隐马尔可夫模型探索多模态对齐，以将 ASR 转录与真实情况对齐。聚类方法也已被用于根据语义相似性对连续数据进行分组（Madhulatha，2012 年）。基于聚类的离散化最近已成为将基于语言的预训练（具有清晰的词/字节对分割边界和离散元素）泛化到基于视频或音频的预训练（没有清晰的分割边界和连续元素）的重要预处理步骤。通过将原始视频或音频特征聚类成一个离散集，类似于 VideoBERT（Sun 等人，2019 年）的方法在原始视频和音频数据上执行掩码预训练。 同样，DALL.E（Ramesh 等人，2021 年）、VQ-VAE（Van Den Oord 等人，2017 年）和 CMCM（Liu 等人，2022 年）等方法也利用通过矢量量化获得的离散化中间层，并在模态对齐方面显示出益处。

### 4.3.子挑战 2c：上下文表示

最后，情境化表示学习旨在建模所有模态之间的连接和交互，以学习更好的表示。情境化表示已被用作中间（通常是潜在）步骤，使许多下游任务（包括语音识别、机器翻译、媒体描述和视觉问答）的性能得到提升。我们将情境化表示的工作分为三类：（1）联合无向对齐，（2）跨模态有向对齐，（3）与图网络对齐（图 12）。

联合无向对齐旨在捕捉跨模态对之间的无向连接，这些连接在任一方向上都是对称的。这在文献中通常被称为单模态、双模态、三模态交互等（Macaluso 和 Driver，2005）。联合无向对齐通常通过使用对齐层参数化模型并针对多模态任务进行端到端训练来捕捉。这些对齐层可以包括注意力权重（Chaplot 等人，2018）、张量积（Liu 等人，2018；Zadeh 等人，2017）和乘性交互（Jayakumar 等人，2020）。最近，通过自动对齐和捕捉不同时间步的互补特征，Transformer 模型（Vaswani 等人，2017）已成为强大的序列数据编码器。在基于初始文本 Transformer 模型的基础上，已经提出了多模态 Transformer，它们通过在序列维度上连接模态元素的全自注意力进行联合对齐（即早期融合）（Li 等人，2019a；Sun 等人，2019）。 因此，所有模态元素都与其他所有模态元素联合连接（即，使用点积相似性核来建模所有连接）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x12.png)

图 12. 上下文表示学习旨在建模模态连接以学习更好的表示。最近的研究方向包括（1）联合无向对齐，捕捉无向对称连接，（2）跨模态有向对齐，以有向方式建模不对称连接，（3）图对齐，将序列模式推广到任意图结构。

跨模态定向对齐以定向方式将源模态的元素与目标模态相关联，可以建模不对称连接。例如，时间注意力模型将对齐作为潜在步骤来改进许多基于序列的任务（Xiong 等人，2016；Zeng 等人，2017）。这些注意力机制通常从输出到输入进行定向，以便结果权重反映输入的软对齐分布。多模态 Transformer 使用查询-键-值注意力机制进行定向对齐，从一种模态的序列关注到另一种模态，然后再以双向方式重复。这导致产生两套不对称的上下文表示，以解释模态之间可能的不对称连接（Lu 等人，2019；Tan 和 Bansal，2019；Tsai 等人，2019a）。这些方法对于序列数据很有用，可以通过自动对齐和捕获不同时间步长的互补特征。 自监督多模态预训练也已成为训练这些架构的有效方法，旨在从更大规模的无标签多模态数据中学习通用表示，然后再通过监督微调转移到特定的下游任务（Li 等人，2019a）。这些预训练目标通常包括单模态掩码预测、跨模态掩码预测和多模态对齐预测（Hendricks 等人，2021）。

图形对齐将无向或有向对齐中观察到的顺序模式推广到元素之间的任意图结构。这有几个优点，因为它不需要所有元素都连接在一起，并允许用户为不同的连接选择不同的边函数。这个子类别的解决方案通常使用图神经网络（Veličković等人，2018）来递归地学习局部连接邻域中与元素上下文相关的元素表示（Scarselli 等人，2008；Veličković等人，2018）。这些方法已通过 MTAG（Yang 等人，2021）应用于多模态顺序数据，该模型捕捉了人类视频中的连接，以及 F2F-CL（Wilf 等人，2022），该模型还沿说话者转换分解节点。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x13.png)

图 13. 推理旨在通过多个推理步骤结合知识，利用问题结构。推理包括（1）结构建模：定义或学习推理发生的关系，（2）推理中使用的中间概念，（3）从证据中推断越来越抽象的概念，以及（4）在研究结构、概念和推理时利用外部知识。

##   挑战 3：推理

推理被定义为通过多个推理步骤结合知识，利用多模态对齐和问题结构。我们将多模态推理的工作分为结构建模、中间概念、推理范式和外部知识（图 13）这四个子挑战。①结构建模涉及定义或学习推理发生的关系，②中间概念研究推理过程中单个多模态概念参数化，③推理范式学习如何从单个多模态证据中推断出越来越抽象的概念，④外部知识旨在利用大规模数据库来研究结构、概念和推理。

### 5.1.子挑战 3a：结构建模

结构建模旨在捕捉组成发生时的层次关系，通常通过参数化原子、关系和推理过程的数据结构来实现。常用的数据结构包括树（Hong 等人，2019 年）、图（Yu 等人，2019 年）或神经模块（Andreas 等人，2016 年）。我们涵盖了最近在建模潜在层次、时序和交互结构方面的工作，以及当潜在结构未知时的结构发现（图 14）。

层次结构定义了一种组织系统，其中抽象概念被定义为比它们更不抽象的概念的函数。层次结构存在于许多涉及语言句法、视觉句法或高阶推理的任务中。这些方法通常在应用（异构变体的）图神经网络来捕捉结构表示之前，基于预定义的节点和边类别构建一个图（Shi 等人，2016 年），例如使用语言句法结构来引导视觉模块发现图像中的特定信息（Andreas 等人，2016 年；Cirik 等人，2018a）。基于图的推理方法已被应用于视觉常识推理（Lin 等人，2019 年）、视觉问答（Saqur 和 Narasimhan，2020 年）、机器翻译（Yin 等人，2020 年）、推荐系统（Tao 等人，2020 年）、网络图像搜索（Wang 等人，2012 年）和社交媒体分析（Schinas 等人，2015 年）。

时间结构将组合性的概念扩展到跨越时间的元素，这在模态包含时间信息时是必要的，例如在视频、音频或时间序列数据中。显式记忆机制已成为积累跨时间多模态信息的一种流行选择，以便通过存储和检索从记忆中捕获长距离跨模态交互。Rajagopalan 等人（2016）探讨了包括多模态融合、协调和因子分解在内的各种记忆表示。从键值记忆（Xiong 等人，2016）和基于注意力的记忆（Zadeh 等人，2018a）的见解也已被成功应用于包括问答、视频字幕、情感识别和情感分析在内的应用。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x14.png)

图 14. 结构建模旨在定义组成发生的关系，可以是（1）层次结构（即更抽象的概念被定义为较不抽象的概念的函数），（2）时间顺序（即在时间上组织），（3）交互式（即状态根据每一步的决定而变化），以及（4）当潜在结构未知时发现，而是直接从数据和优化中推断出来。

交互结构将推理的挑战扩展到交互式场景，其中推理代理的状态取决于每一步所做的局部决策。通常通过顺序决策框架进行形式化，挑战在于尽管只能通过短期行动与环境交互，但仍然要最大化长期累积奖励（Sutton 和 Barto，2018）。为了应对交互式推理的挑战，多模态强化学习（RL）这一日益增长的研究领域从语言理解、视觉世界的具身化、深度强化学习和机器人学的交叉点中产生。我们建议读者参考 Luketina 等人（2019）的广泛调查论文和 Bisk 等人（2020）的立场论文，以全面了解这一领域。 Luketina 等人（2019 年）将文献分为多模态条件强化学习（其中多模态交互由问题本身所必需，例如指令跟随（Chaplot 等人，2018 年；Wang 等人，2019 年））和语言辅助强化学习（其中多模态数据可选用于促进学习，例如阅读说明书（Narasimhan 等人，2018 年））。

结构发现：在没有给定任务的领域知识的情况下，可能难以定义多模态组合的结构。作为一种替代方法，最近的研究也探索了使用可微策略以完全数据驱动的方式自动搜索结构。为此，首先需要定义一组候选的推理原子和关系，然后使用“元”方法，如架构搜索，来自动搜索给定任务的理想组合序列（Pérez-Rúa 等，2019；Xu 等，2021）。这些方法可以从神经架构搜索文献中常用的优化技巧中受益。记忆、注意和组合（MAC）以端到端的方式从数据中搜索一系列基于注意力的推理步骤（Hudson 和 Manning，2018）。最后，Hu 等（2017）通过使用策略梯度而不是语言解析来优化离散神经网络模块的组合结构，扩展了 Andreas 等（2016）通过语言解析获得的前定义推理结构。

### 5.2.子挑战 3b：中级概念

第二子挑战研究如何在推理过程中对单个多模态概念进行参数化。虽然中间概念通常在标准神经网络架构中是密集向量表示，但也已有大量工作致力于可解释的注意力图、离散符号以及作为推理中介的语言。

注意力图是中间概念的流行选择，因为它们在一定程度上是可被人理解的，同时保持可微性。例如，Andreas 等人（2016）设计了“关注”、“组合”、“计数”和“测量”等个体模块，这些模块分别通过输入图像上的注意力操作进行参数化，用于视觉问答。Xu 等人（2015a）探索了软硬注意力机制在图像字幕生成中的推理。相关研究还通过双重注意力架构（Nam 等人，2017）或堆叠潜在注意力架构（Fan 和 Zhou，2018）使用注意力图进行多模态推理。这些通常应用于涉及复杂推理步骤的问题，如 CLEVR（Johnson 等人，2017）或 VQA（Zhang 等人，2020）。

离散符号：在注意力图之上的进一步离散化涉及使用离散符号来表示中间概念。最近神经符号学习领域的工作旨在将这些离散符号作为多模态推理任务（如视觉问答（Andreas 等人，2016 年；Mao 等人，2018 年；Vedantam 等人，2019 年）或指代表达识别（Cirik 等人，2018a））中的中间步骤进行整合。这种方法的一个核心挑战在于保持离散符号的可微性，这一问题已通过基于逻辑的可微推理（Amizadeh 等人，2020 年；Serafini 和 Garcez，2016 年）得到解决。

语言作为一种媒介：最后，也许最符合人类理解的中介概念形式是通过离散的词语或短语作为媒介的自然语言。最近，曾等（2022）探讨了使用语言作为中介媒介以零样本方式协调多个独立的预训练模型。几种方法也使用了从外部知识图谱中获得的语言短语来促进可解释推理（Gui 等，2021；Zhu 等，2015b）。Hudson 和 Manning（2019a）设计了一种神经状态机来模拟关于图像的问题的执行，同时使用离散的词语作为中介概念。

### 5.3.子挑战 3c：推理范式

第三项多模态推理子挑战定义了从单个多模态证据中推断越来越抽象概念的方式。虽然局部表示融合（如加法、乘法、张量基础、注意力基础和顺序融合，参见§3.1 以获取全面回顾）在这里也通常适用，但推理的目标是通过关于多模态问题的领域知识在推理过程中更具可解释性。为此，我们以通过逻辑和因果运算符显式建模推理过程为例，介绍了这一方向上的最新趋势。

逻辑推理：基于逻辑的可微分推理已被广泛用于在神经网络中表示知识（Amizadeh 等人，2020 年；Serafini 和 Garcez，2016 年）。许多这些方法使用可微分模糊逻辑（van Krieken 等人，2022 年），它为逻辑谓词、函数和常量提供了一种概率解释，以确保可微性。这些逻辑运算符已应用于视觉问答（Gokhale 等人，2020 年）和视觉推理（Amizadeh 等人，2020 年）。逻辑推理的最大优点之一在于其执行可解释和组合多步推理的能力（Hudson 和 Manning，2019 年 b）。逻辑框架在视觉-文本蕴涵（Suzuki 等人，2019 年）和几何数值推理（Chen 等人，2021 年 c）等领域也很有用，在这些领域中，逻辑归纳偏差对于强大性能至关重要。

因果推理将推理的关联层次扩展到干预和反事实层次（Pearl，2009），这需要广泛的世界知识来想象反事实效应。例如，Yi 等人（2019）提出了 CLEVRER 基准，重点关注视频推理的四个特定元素：描述性（例如，“什么颜色”）、解释性（“什么负责”）、预测性（“接下来会发生什么”）和反事实性（“如果”）。在 CLEVRER 之外，最近的研究还提出了因果 VQA（Agarwal 等人，2020）和反事实 VQA（Niu 等人，2021），以衡量 VQA 模型在受控干预下对问题的鲁棒性，作为减轻 VQA 模型中语言偏差的一步。将因果推理能力整合到神经网络模型中的方法也已被证明可以提高鲁棒性并减少偏差（Wang 等人，2020a）。

### 5.4.子挑战 3d：外部知识

最终子挑战研究在定义组合和结构研究中的知识推导。知识通常从特定任务数据集的领域知识中推导而来。作为使用领域知识预先定义组合结构的一种替代方法，最近的研究也探索了使用数据驱动方法自动推理，例如在直接任务领域之外广泛可访问但更弱监督的数据。

多模态知识图谱扩展了语言和符号知识图谱的经典工作（例如，Freebase（Bollacker 等人，2008 年），DBpedia（Auer 等人，2007 年），YAGO（Suchanek 等人，2007 年），WordNet（Miller，1995 年）），将包含多模态概念作为节点和多模态关系作为边的语义网络（Zhu 等人，2022 年）。多模态知识图谱之所以重要，是因为它们能够使结构化信息在视觉和物理世界中得以扎根。例如，Liu 等人（2019 年）构建了包含实体数值特征和图像的多模态知识图谱。视觉基因组是另一个例子，它包含图像和文本中对象、属性和关系的密集注释（Krishna 等人，2017 年）。这些多模态知识库已被证明对视觉问答（Wu 等人，2016 年；Zhu 等人，2015b）、知识库补全（Pezeshkpour 等人，2018 年）和图像字幕（Melas-Kyriazi 等人，2018 年；Yao 等人，2018 年）有益。Gui 等人（2021 年）将知识集成到视觉和语言转换器中，以自动对两种知识源进行推理。 我们建议读者参考 Zhu 等人（2022 年）的全面综述以获取更多参考文献。

多模态常识推理需要更深入的现实世界知识，可能涉及概念之间的逻辑、因果和时间关系。例如，因果推理的元素是回答 VCR（Zellers 等人，2019 年）和 VisualCOMET（Park 等人，2020 年）中图像相关问题的必要条件，而其他研究也引入了包含视频和文本输入的数据集来测试时间推理（例如，MovieQA（Tapaswi 等人，2016 年），MovieFIB（Maharaj 等人，2017 年），TVQA（Lei 等人，2018 年））。多模态常识的基准通常需要利用知识库（Song 等人，2021 年）或在大规模数据集上的预训练范式（Lu 等人，2019 年；Zellers 等人，2021 年）的外部知识。

## 挑战 4：生成

第四个挑战涉及学习一个生成过程，通过摘要、翻译和创作来产生反映跨模态交互、结构和连贯性的原始模态（图 15）。这三个类别根据输入到输出模态的信息变化进行区分，遵循文本生成的分类（Deng 等人，2021 年）。我们将涵盖最近的研究进展以及生成内容的评估。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x15.png)

图 15. 我们如何学习一个生成过程来产生反映跨模态交互、结构和连贯性的原始模态？生成涉及（1）总结多模态数据以突出最显著的部分，（2）在保持模态连接一致性的情况下从一种模态翻译到另一种模态，以及（3）同时创建多个模态并保持连贯性。

### 6.1.子挑战 4a：摘要

摘要旨在压缩数据以创建一个摘要，该摘要代表原始内容中最重要或相关的信息。最近的研究探讨了各种输入模态来指导文本摘要，例如图像（陈和诸葛，2018a）、视频（李等，2020）和音频（Evangelopoulos 等，2013；Jangra 等，2020；李等，2017）。多模态摘要的最新趋势包括提取和抽象方法。提取方法旨在从输入中过滤单词、短语和其他单模态元素以创建摘要（陈和诸葛，2018b；Jangra 等，2020；李等，2017）。除了文本作为输出之外，视频摘要的任务是通过封装最有信息量的部分（Sah 等，2017）来生成视频的紧凑版本（视觉摘要）。李等（2017）收集了一个新闻视频和文章数据集，其中包含与手动标注的摘要配对，作为多模态摘要的基准。最后，UzZaman 等（2011）旨在通过提取多模态摘要来简化复杂句子以提高可访问性。 另一方面，抽象方法定义了一个生成模型，以多粒度级别生成摘要（陈和朱格，2018a；李等，2019b）。尽管大多数方法仅关注从多模态数据生成文本摘要（帕拉斯卡尔等，2019），但也有一些方向探索生成摘要图像以补充生成的文本摘要（陈和朱格，2018a；李等，2020）。

### 6.2.子挑战 4b：翻译

翻译旨在将一种模态映射到另一种模态，同时尊重语义联系和信息内容（Vinyals 等人，2016 年）。例如，生成图像的描述性字幕可以帮助提高盲人对视觉内容的可访问性（Gurari 等人，2018 年）。多模态翻译带来了新的困难，包括生成高维结构化数据以及它们的评估。最近的方法可以分为基于示例的方法，这些方法限于从训练实例中检索以在模态之间进行翻译，但保证了忠实度（Farhadi 等人，2010 年），以及生成模型，这些模型可以将任意实例翻译成超出数据的范围，但面临着质量、多样性和评估的挑战（Koh 等人，2021 年；Ramesh 等人，2021 年；Tsimpoukelli 等人，2021 年）。 尽管存在这些挑战，但大规模翻译模型最近取得的进展已使生成内容的质量令人印象深刻，包括文本到图像（Rombach 等人，2022 年；Ramesh 等人，2021 年）、文本到视频（Singer 等人，2022 年）、音频到图像（Jamaludin 等人，2019 年）、文本到语音（Ren 等人，2019 年）、语音到手势（Ahuja 等人，2020 年）、说话者到听众（Ng 等人，2022 年）、语言到姿态（Ahuja 和 Morency，2019 年）以及语音和音乐生成（Oord 等人，2018 年）。

### 6.3.子挑战 4c：创建

创造旨在从少量初始示例或潜在条件变量生成新颖的高维数据（这些数据可能包括文本、图像、音频、视频和其他模态）。这种条件解码过程极具挑战性，因为它需要（1）条件性：保留从初始种子到一系列长距离并行模态的语义上有意义的映射，（2）同步性：在模态间语义上保持一致，（3）随机性：捕捉给定特定状态下的许多可能未来生成，（4）在可能的长距离上自回归。许多模态已被考虑作为创造的靶标。语言生成已被探索很长时间（Radford 等人，2019 年），最近的研究探讨了使用神经网络进行高分辨率语音和声音生成（Oord 等人，2018 年）。由于大规模生成建模的进步，逼真的图像生成最近也成为可能（Karras 等人，2020 年）。此外，还有许多尝试生成抽象场景（Tan 等人，2019 年）、计算机图形（Mildenhall 等人，2020 年）和说话人头（Zhu 等人，2021 年）。 尽管在视频生成方面取得了一些进展（Singer 等人，2022 年），但完全同步生成逼真的视频、文本和音频仍然是一个挑战。

最后，多模态生成面临的最大挑战之一是评估生成内容的困难，尤其是在存在严重伦理问题时，如虚假新闻（Bender 等人，2021 年）、仇恨言论（Gehman 等人，2020 年；Abid 等人，2021 年）、深度伪造（Hancock 和 Bailenson，2021 年）和同步视频（Suwajanakorn 等人，2017 年）可以轻易生成。虽然评估生成内容的理想方式是通过用户研究，但这既耗时又昂贵，可能会在评估过程中引入主观偏见（Geva 等人，2019 年）。已经提出了几种自动代理指标（Anderson 等人，2016 年；Chen 等人，2020a），但没有一种是跨多个生成任务普遍稳健的。

## 挑战 5：迁移

迁移旨在在不同模态及其表示之间转移知识。从次要模态（例如，预测标签或表示）学习到的知识如何帮助在主要模态上训练的模型？当主要模态资源有限时——缺乏标注数据、噪声输入或不稳定标签——这一挑战尤为相关。我们称之为迁移，因为从次要模态转移信息引发了在主要模态中以前未见过的新的行为。我们确定了三种迁移方法：（1）跨模态迁移，（2）多模态协同学习，（3）模型归纳（图 16）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x16.png)

图 16. 转移研究模态之间的知识迁移，通常用于帮助噪声或有限的原始模态，通过（1）从在次要模态中用大量数据训练的模型中进行跨模态迁移，（2）通过共享表示来跨模态协同学习以共享信息，以及（3）模型归纳，保持单个单模态模型独立但诱导不同模型中的行为。

### 7.1.子挑战 5a：跨模态迁移

在大多数情况下，收集次级模态的标记或未标记数据可能更容易，然后训练强大的监督或预训练模型。这些模型随后可以针对涉及主要模态的下游任务进行条件化或微调。换句话说，这一研究将单模态迁移和微调扩展到跨模态设置。

调整：受先前涉及前缀调整（Li 和 Liang，2021）和提示调整（Lester 等人，2021）的 NLP 工作启发，最近的研究也研究了预训练语言模型的调整，以便对视觉和其他模态进行条件化。例如，Tsimpoukelli 等人（2021）快速将预训练的冻结语言模型在图像上进行条件化以进行图像描述。相关研究还针对图像描述（Chen 等人，2021a）、多模态融合（Hasan 等人，2021）和摘要（Yu 等人，2021）对前缀调整进行了调整。虽然前缀调整简单且高效，但它只向用户提供有限的控制，以了解信息是如何传递的。表示调整通过与其他模态进行上下文化来修改语言模型的内部表示，从而深入一层。例如，Ziegler 等人（2019）在语言模型层和外部模态之间包含额外的自注意力层。Rahman 等人（2020）设计了一个移位门，以适应包含音频和视觉信息的语言模型层。

多任务学习旨在通过使用多个大规模任务来提高性能，与单个任务学习相比。一些模型，如 Perceiver（Jaegle 等人，2021 年）、MultiModel（Kaiser 等人，2017 年）、ViT-BERT（Li 等人，2021 年）和 PolyViT（Likhosherstov 等人，2022 年）探讨了使用相同的单模态编码器架构对不同输入进行跨单模态任务的可能性（即语言、图像、视频或仅音频）。由于 Transformer 架构适用于序列化输入，如文本（标记序列）（Devlin 等人，2019 年）、图像（补丁序列）（Dosovitskiy 等人，2021 年）、视频（图像序列）（Sun 等人，2019 年）和其他时间序列数据（时间步序列）（Lim 等人，2021 年），因此它已成为一种流行的选择。此外，还有几个尝试构建一个在一系列多模态任务上表现良好的单一模型，包括但不限于 HighMMT（Liang 等人，2022 年）、VATT（Akbari 等人，2021 年）、FLAVA（Singh 等人，2021 年）和 Gato（Reed 等人，2022 年）。

迁移学习：虽然更多研究集中在同一模态内的迁移以及外部信息（Socher 等人，2013；Xing 等人，2019；Zadeh 等人，2020），Liang 等人（2021b）研究了使用少量配对但未标记的数据向新模态的迁移。Lu 等人（2021）发现，在语言迁移预训练的 Transformer 也可以迁移到其他序列模态。Liang 等人（2022）构建了一个能够迁移到完全新模态和任务的单一多模态模型。最近，也有一系列工作在研究预训练语言模型在规划（Huang 等人，2022）和交互式决策（Li 等人，2022a）中的迁移。

### 7.2.子挑战 5b：多模态协同学习

多模态协同学习旨在通过在两种模态之间共享中间表示空间，将通过次级模态学习到的信息转移到涉及主要模态的目标任务中。这些方法本质上导致所有模态都只有一个联合模型。

通过表示学习进行协同学习旨在利用两种模态作为输入来学习联合或协调的表示空间。通常，这涉及到在训练过程中添加次要模态，设计一个合适的表示空间，并研究多模态模型在测试期间如何转移到主要模态。例如，DeViSE 通过学习图像和文本之间的协调相似性空间来改进图像分类（Frome 等人，2013）。Marino 等人（2017）使用基于图的知识图通过图联合表示空间进行图像分类。Jia 等人（2021）通过图像和噪声标题之间的对比表示学习来改进图像分类器。最后，Zadeh 等人（2020）表明，即使没有显式的协同学习目标，隐式协同学习也是可能的。

通过生成进行协同学习，而不是从主模态学习翻译模型到次级模态，从而得到丰富的主模态表示，可以预测标签并“虚构”包含共享信息的次级模态。这一类别的经典例子包括将上下文化的文本嵌入映射到图像中的语言建模（Tan 和 Bansal，2020 年），将图像嵌入投影到词嵌入中的图像分类（Socher 等人，2013 年），以及将语言翻译成视频和音频的语言情感分析（Pham 等人，2019 年）。

### 7.3.子挑战 5c：模型归纳

与协同学习不同，模型归纳方法在主要和次要模态之间保持单个单模态模型分离，但旨在在两个模型中诱导行为。模型归纳以协同训练为例，其中两个学习算法分别在每个数据视图中单独训练，然后在每个算法的预测中使用，以伪标记新的未标记示例来扩大另一个视图的训练集（Blum 和 Mitchell，1998）。因此，信息是通过模型预测而不是共享表示空间在多个视图中传递的。

多模态协同训练通过联合学习多个模态的分类器来扩展协同训练（Hinami 等人，2018 年）。Guillaumin 等人（2010 年）通过在图像和文本上使用分类器来对未标记图像进行伪标记，然后在标记和未标记图像上训练最终分类器来研究半监督学习。Cheng 等人（2016 年）使用保留多样性的协同训练算法进行半监督多模态学习。最后，Dunnmon 等人（2020 年）将数据编程的思想应用于跨模态弱监督问题，其中从次要模态（例如，文本）派生的弱标签用于在主要模态（例如，图像）上训练模型。

共正则化：另一组模型采用正则化器惩罚来自两种模态且相互不一致的函数。这类模型被称为共正则化，是一种有用的技术，通过优先选择包含在两个视图上预测相似的模型的理论类来控制模型复杂度（Sindhwani 等，2005）。Sridharan 和 Kakade（2008）使用信息论框架为这些方法提供了保证。最近，类似的共正则化方法也已被应用于多模态特征选择（Hsieh 等，2019）、半监督多模态学习（Yang 等，2019）和视频摘要（Morore 等，2015）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x17.png)

图 17. 量化：我们可以设计哪些实证和理论研究来更好地理解（1）异质性的维度，（2）相互联系的存在和类型，以及（3）学习和优化挑战？

## 挑战 6：量化

量化旨在对多模态模型进行更深入的实证和理论研究，以获得见解并提高其在实际应用中的鲁棒性、可解释性和可靠性。我们将量化分解为 3 个子挑战：（1）量化异质性的维度以及它们随后如何影响建模和学习，（2）量化多模态数据集和训练模型中连接和交互的存在和类型，（3）描述从异质数据中学习时涉及的学习和优化挑战（图 17）。

### 8.1.子挑战 6a：异质性的维度

本子挑战旨在理解在多模态研究中常见异质性的维度，以及它们随后如何影响建模和学习（图 18）。

模态信息：理解整个模态及其组成信息对于确定每个模态的哪个部分对后续建模有贡献至关重要。近期的工作可以分为以下几类：（1）可解释的方法，明确地建模每个模态的使用方式（Park 等人，2018；Tsai 等人，2020a；Zadeh 等人，2018b）或（2）对黑盒模型的后期解释（Chandrasekaran 等人，2018；Goyal 等人，2016）。在前者中，概念瓶颈模型（Koh 等人，2020）和将稀疏线性层（Wong 等人，2021）或决策树（Wan 等人，2020）拟合在深度特征表示之上等方法已作为有希望的选项出现。在后者中，基于梯度的可视化（Simonyan 等人，2013；Goyal 等人，2016；Selvaraju 等人，2017）和特征归因（例如，模态贡献（Gat 等人，2021），LIME（Ribeiro 等人，2016），以及 Shapley 值（Merrick 和 Taly，2020））已被用于突出模型使用的每个模态的区域。

模态偏差是在数据收集（Birhane 等人，2021 年；Bolukbasi 等人，2016 年）、建模（Geirhos 等人，2020 年）或人工标注（Devillers 等人，2005 年）过程中无意引入的输入与输出之间的相关性。模态偏差可能导致在现实世界中性能意外地差（Sakaguchi 等人，2020 年），甚至更危险的是，可能对代表性不足的群体造成伤害（Peña 等人，2020 年；Hendricks 等人，2018 年）。例如，Goyal 等人（2017 年）在 VQA 任务的语言模态中发现了单模态偏差，导致由于忽略视觉信息而出现错误（Agrawal 等人，2016 年）。后续工作已经开发出精心策划的诊断基准来减轻数据收集偏差，如 VQA 2.0（Goyal 等人，2017 年）、GQA（Hudson 和 Manning，2019 年 b）、以及 NLVR2（Suhr 和 Artzi，2019 年）。最近的研究还发现了多模态系统中累积的社会偏差（Ross 等人，2020 年；Srinivasan 和 Bisk，2021 年；Cho 等人，2022 年），这些偏差源于语言和视觉模态中的性别偏见（Buolamwini 和 Gebru，2018 年；Sheng 等人，2019 年），在部署时可能造成危险（Peña 等人，2020 年）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x18.png)

图 18.异质性量化子挑战旨在理解在多模态研究中常见异质性的维度，例如（1）不同数量和用途的模态信息，（2）模态偏差的存在，以及（3）量化并减轻模态噪声。

模态噪声拓扑和鲁棒性：模态噪声拓扑的研究旨在评估和改进多模态模型在现实世界数据不完美情况下的表现。每个模态都有独特的噪声拓扑，这决定了它通常遇到的噪声和不完美分布。例如，图像容易受到模糊和偏移的影响，键盘位置后的文本容易出错，而多模态时间序列数据容易受到同步时间步长间的相关不完美影响。Liang 等人（2021a）收集了一套针对每个模态的独特噪声分布。除了自然噪声拓扑外，相关研究还探讨了多模态系统中的对抗攻击（Ding 等人，2021）和分布偏移（Foltyn 和 Deuschel，2021）。此外，通过使用概率模型（Ma 等人，2021）、自编码器（Tran 等人，2017）、翻译模型（Pham 等人，2019）或低秩近似（Liang 等人，2019）对噪声或缺失模态进行模态插补，也取得了一些进展。 然而，它们存在可能错误累积的风险，并且需要事先知道哪些模态是不完善的。

### 8.2.子挑战 6b：模态互联

模态连接和交互是多模态模型的一个基本组成部分，这激发了对可视化和理解数据集和训练模型中模态间连接本质的重要研究。我们将近期的工作分为对（1）连接的量化：模态如何相关和共享共性，以及（2）交互的量化：模态元素在推理过程中的交互（图 19）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x19.png)

图 19. 量化模态互连研究（1）连接：我们能否发现哪些模态元素彼此相关以及为什么，以及（2）交互：我们能否理解模态元素在推理过程中的交互方式？

连接：最近的研究通过联合表示空间的可视化工具（Itkina 等人，2020 年）或注意力图（Aflalo 等人，2022 年）探讨了模态连接的量化。基于扰动的分析通过扰动输入并观察输出变化来理解内部连接（Liang 等人，2023 年；Niu 等人，2021 年）。最后，专门定制的诊断数据集也有助于理解语义连接：Winoground（Thrush 等人，2022 年）探测视觉和语言模型对视觉-语言组合性的处理，而 PaintSkills（Cho 等人，2022 年）衡量视觉推理所需的连接。

交互：一种常见的交互分类涉及冗余、独特性和协同（Williams 和 Beer，2010）。冗余描述了特征之间共享的任务相关信息，独特性研究仅存在于一个特征中的任务相关信息，协同则调查当两个特征都存在时新信息的出现。从统计角度来看，冗余的度量包括互信息（Blum 和 Mitchell，1998；Balcan 等，2004）和对比学习估计器（Tosh 等，2021；Tsai 等，2020b）。其他方法已经单独研究了这些度量，例如通过使用任一特征预测 logits 之间的距离来衡量冗余（Mazzetto 等，2021），对输入特征进行统计分布测试（Auffarth 等，2010），或通过人工标注（Ruiz 等，2006）。从语义观点来看，因果视觉问答（Causal VQA）中的最近工作（Agarwal 等，2020）和反事实视觉问答（Counterfactual VQA）中的最近工作（Niu 等，2021）试图通过测量它们在受控语义编辑问题或图像下的鲁棒性来理解训练模型捕获的交互。 最后，近期的研究已经形式化了非加性交互的定义，以量化其在训练模型中的存在（Sorokina 等，2008；Tsang 等，2018）。与之并行的研究，如 EMAP（Hessel 和 Lee，2020）、DIME（Lyu 等，2022）、M2Lens（Wang 等，2021）和 MultiViz（Liang 等，2023），旨在量化现实世界多模态数据集和模型中的交互。

### 8.3.子挑战 6c：多模态学习过程

最后，需要描述从异构数据中学习时涉及的学习和优化挑战。本节涵盖了以下最近的工作：（1）跨模态和任务的泛化，（2）更优化的平衡和高效训练，（3）在现实部署中平衡性能、鲁棒性和复杂性之间的权衡（图 20）。

泛化：随着感知技术的进步，许多现实平台如手机、智能设备、自动驾驶汽车、医疗技术和机器人现在集成了比典型文本、视频和音频模态多得多的传感器（黄等，2019）。最近的研究探讨了配对模态输入的泛化（梁等，2021b；Radford 等，2021）以及在未配对场景中的泛化，其中每个任务仅定义在所有模态的小子集上（梁等，2022；卢等，2021；里德等，2022）。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2209.03430/assets/x20.png)

图 20.研究多模态学习过程涉及理解（1）跨模态和任务的泛化，（2）平衡和高效的训练优化，（3）在多模态模型现实部署中性能、鲁棒性和复杂性之间的权衡。

优化挑战：相关研究也探讨了多模态学习的优化挑战，由于容量增加，多模态网络往往容易过拟合，并且不同模态的过拟合和泛化速度不同，因此使用单一优化策略联合训练它们是不理想的（王等，2020b）。后续工作建议进行实证和理论研究，以探讨为什么联合训练多模态网络可能困难，并提出了通过加权方法改进优化过程的方法（吴等，2022）。

模态权衡：在实际部署中，通常需要在性能、鲁棒性和复杂性之间取得平衡。因此，人们往往需要在数据收集和建模中额外模态的效用与额外复杂性之间取得平衡（Liang 等人，2021a），以及增加对额外模态中的噪声和不完美的敏感性（Pham 等人，2019）。我们如何正式量化每个输入模态的效用和风险，同时平衡这些权衡以确保可靠的现实世界应用？已有几个尝试将多模态表示的语义形式化，以及这些好处如何转移到下游任务中（Liang，2022；Tsai 等人，2020b；Thomason 等人，2016），同时信息论论证也提供了有用的见解（Blum 和 Mitchell，1998；Sridharan 和 Kakade，2008）。

##   9\. 结论

本文定义了多模态机器学习研究中的三个核心原则：模态异质性、连接和交互，随后提出了六个核心技术挑战的分类法：表示、对齐、推理、生成、迁移和量化，涵盖了历史和最近的发展方向。尽管多模态机器学习在最近取得的进展提供了巨大的机遇，但在理论、计算和应用方面仍存在许多未解决的问题：

###   9.1.未来方向

表示：理论框架和实证框架。我们如何形式化定义异质性、连接性和交互性的三个核心原则？哪些数学或实证框架能够使我们对异质性和相互连接的维度进行分类，并随后量化它们在多模态数据集和模型中的存在？回答这些基本问题将有助于更好地理解当前多模态表示的能力和局限性。超越加性和乘性跨模态交互。尽管最近的研究在建模递增阶乘的乘性交互方面取得了成功，但我们如何捕捉因果关系、逻辑关系和时间上的连接和交互？建模这些交互需要什么样的数据和领域知识？大脑和多模态感知。从大脑和人类认知中可以获得许多关于多模态处理的核心见解，包括大脑的神经网络架构（Blum 和 Blum，2022 年）、内在的多模态属性（Kosslyn 等人，2010 年）、心理意象（Nanay，2018 年）以及神经信号的本质（Palazzo 等人，2020 年）。 人类大脑如何表示不同的模态，多感官整合是如何进行的，以及这些见解如何指导多模态学习？从另一个方向来看，处理高分辨率脑信号（如 fMRI 和 MEG/EEG）存在哪些挑战和机遇，多模态学习如何有助于未来对神经科学收集的数据进行分析？

对齐：记忆和长期交互。许多当前的多模态基准测试只有短暂的时序维度，这限制了能够准确处理长序列和学习长期交互的模型的需求。捕捉长期交互具有挑战性，因为当它们在时间或空间上相隔很远时，很难在语义上关联信息，并引发复杂性问题。我们如何设计模型（可能带有记忆机制）以确保捕捉到这些长期跨模态交互？

推理：多模态组合性。我们如何理解训练模型的推理过程，特别是它们如何结合模态元素的信息？由于许多元素组合在训练期间通常不存在，且组合的数量随着元素数量的增加而呈指数增长（Thrush 等人，2022 年），这种组合泛化挑战是困难的。我们如何最好地测试组合性，以及哪些推理方法可以促进组合泛化？

迁移：高模态学习旨在从大量异构数据源中学习表示，这是许多现实世界多模态系统（如自动驾驶汽车和物联网）的常见特征（Huang 等，2019）。更多的模态引入更多的异质性维度，在单模态和多模态处理中带来复杂性挑战，并需要处理非并行数据（即并非所有模态同时存在）。

生成：创造和现实世界的伦理问题。完整同步创建逼真的视频、文本和音频仍然是一个挑战。此外，模态生成最近的成功也带来了关于其使用的伦理担忧。例如，大规模预训练语言模型可能生成贬低特定社会群体的文本（Sheng 等人，2019 年），有害言论（Gehman 等人，2020 年），以及敏感的预训练数据（Carlini 等人，2021 年）。未来的工作应研究当数据集为多模态时，这些风险如何可能放大或减少，以及是否存在特定于多模态生成的伦理问题。

量化：模态效用、权衡与选择。我们如何形式化为什么模态对任务有用，以及模态可能有害的潜在原因？我们能提出正式的指导方针来比较这些权衡并随后选择模态吗？可解释性和可理解性。在模型可以安全地被现实世界的利益相关者，如医生、教育工作者或政策制定者使用之前，我们需要理解数据集和训练模型中多模态现象的分类，并应努力解释。我们如何评估这些现象是否被准确解释？对于相对未被充分研究的模态（超出语言和视觉），其中模态本身不易可视化，这些挑战更加严重。最后，我们如何调整这些解释，可能以人机交互的方式，以告知现实世界的决策？在理解和量化模态和社会偏见以及对抗不完美、噪声和分布外模态的鲁棒性方面也存在核心挑战。

总之，我们相信我们的分类法将有助于对未来的研究论文进行编目，并更好地理解多模态机器学习领域尚未解决的剩余问题。

##   致谢

本材料部分基于美国国家科学基金会（奖项编号#1722822 和#1750439）、美国国立卫生研究院（奖项编号#R01MH125740、#R01MH096951 和#U01MH116925）、宝马北美公司和 Meta 的支持工作。PPL 部分由 Facebook 博士奖学金和卡内基梅隆大学机器学习与健康中心奖学金支持。本材料中表达的意见、发现、结论或建议均为作者的观点，并不一定反映美国国家科学基金会、美国国立卫生研究院、宝马北美公司、Facebook 或卡内基梅隆大学机器学习与健康中心的观点，不应推断出官方认可。我们非常感谢 Alex Wilf、Arav Agarwal、Catherine Cheng、Chaitanya Ahuja、Daniel Fried、Dong Won Lee、Jack Hessel、Leena Mathur、Lenore Blum、Manuel Blum、Martin Ma、Peter Wu、Richard Chen、Ruslan Salakhutdinov、Santiago Benoit、Su Min Park、Torsten Wortwein、Victoria Lin、Volkan Cirik、Yao-Hung Hubert Tsai、Yejin Choi、Yiwei Lyu、Yonatan Bisk 和 Youssouf Kebe 就本文初稿的有益讨论和反馈。
