---
论文名称: Qwen2.5-VL Technical Report
tags:
  - "#技术报告"
  - "#Qwen"
摘要: "Qwen2.5-VL 是 Qwen 视觉语言系列的最新旗舰模型，它在基础能力和创新功能方面都 демонстрирует 了显著的进步。Qwen2.5-VL 在通过增强的视觉识别、精确的对象定位、强大的文档解析和长视频理解来理解世界和与世界互动方面实现了重大飞跃。\r\rQwen2.5-VL 的一个突出特点是其使用边界框或点精确定位对象的能力。它可以从发票、表格和表格中提供强大的结构化数据提取，以及对图表、diagram 和布局的详细分析。为了处理复杂的输入，Qwen2.5-VL 引入了动态分辨率处理和绝对时间编码，使其能够处理不同大小的图像和长时间（长达数小时）的视频，并进行秒级的事件定位。这使得模型能够原生感知空间尺度和时间动态，而无需依赖传统的归一化技术。\r\r通过从头开始训练原生动态分辨率 Vision Transformer (ViT) 并结合 Window Attention，我们在保持原生分辨率的同时显著降低了计算开销。因此，Qwen2.5-VL 不仅在静态图像和文档理解方面表现出色，而且还作为一种交互式视觉 agent，能够在计算机和移动设备等真实场景中进行推理、工具使用和任务执行。该模型在跨领域实现了强大的泛化能力，而无需进行特定于任务的微调。\r\rQwen2.5-VL 提供三种尺寸，满足从边缘 AI 到高性能计算的各种用例。旗舰模型 Qwen2.5-VL-72B 与 GPT-4o 和 Claude 3.5 Sonnet 等最先进的模型相媲美，尤其是在文档和 diagram 理解方面表现出色。较小的 Qwen2.5-VL-7B 和 Qwen2.5-VL-3B 模型优于同类竞争对手，即使在资源受限的环境中也提供了强大的功能。此外，Qwen2.5-VL 保持了强大的语言性能，保留了 Qwen2.5 LLM 的核心语言能力。"
发布时间: 2024
链接: https://arxiv.org/abs/2412.15115
---
## Abstract

Qwen2.5-VL 是 Qwen 视觉语言系列的最新旗舰模型，它在基础能力和创新功能方面都 демонстрирует 了显著的进步。Qwen2.5-VL 在通过增强的视觉识别、精确的对象定位、强大的文档解析和长视频理解来理解世界和与世界互动方面实现了重大飞跃。

Qwen2.5-VL 的一个突出特点是其使用边界框或点精确定位对象的能力。它可以从发票、表格和表格中提供强大的结构化数据提取，以及对图表、diagram 和布局的详细分析。为了处理复杂的输入，Qwen2.5-VL 引入了动态分辨率处理和绝对时间编码，使其能够处理不同大小的图像和长时间（长达数小时）的视频，并进行秒级的事件定位。这使得模型能够原生感知空间尺度和时间动态，而无需依赖传统的归一化技术。

通过从头开始训练原生动态分辨率 Vision Transformer (ViT) 并结合 Window Attention，我们在保持原生分辨率的同时显著降低了计算开销。因此，Qwen2.5-VL 不仅在静态图像和文档理解方面表现出色，而且还作为一种交互式视觉 agent，能够在计算机和移动设备等真实场景中进行推理、工具使用和任务执行。该模型在跨领域实现了强大的泛化能力，而无需进行特定于任务的微调。

Qwen2.5-VL 提供三种尺寸，满足从边缘 AI 到高性能计算的各种用例。旗舰模型 Qwen2.5-VL-72B 与 GPT-4o 和 Claude 3.5 Sonnet 等最先进的模型相媲美，尤其是在文档和 diagram 理解方面表现出色。较小的 Qwen2.5-VL-7B 和 Qwen2.5-VL-3B 模型优于同类竞争对手，即使在资源受限的环境中也提供了强大的功能。此外，Qwen2.5-VL 保持了强大的语言性能，保留了 Qwen2.5 LLM 的核心语言能力。

## 1 Introduction

大型视觉语言模型 (LVLM) 代表了人工智能领域的一个关键突破，它标志着多模态理解和交互方式的变革性方法。通过将视觉感知与自然语言处理无缝集成，这些先进的模型从根本上改变了机器解释和分析跨不同领域复杂信息的方式。尽管多模态大型语言模型取得了显著进步，但这些模型目前的能力可以比作夹心饼干的中间层——在各种任务中都表现出色，但未能达到卓越的性能。精细的视觉任务构成了这种类比的基础层。在 Qwen2.5-VL 的迭代中，我们致力于探索精细的感知能力，旨在为 LVLM 建立强大的基础，并为实际应用创建 agentic 放大器。该框架的顶层是多模态推理，它通过利用最新的 Qwen2.5 LLM 和采用多模态 QA 数据构建得到增强。

大量研究促进了多模态大型模型的发展，其特点是架构设计、视觉输入处理和数据管理。LVLM 进步的主要驱动力之一是架构的不断创新。研究 (Alayrac et al., 2022; Li et al., 2022a; 2023b; Liu et al., 2023b;a; Wang et al., 2024i; Zhang et al., 2024b; Wang et al., 2023) 逐步塑造了当前的范式，该范式通常由视觉编码器、跨模态投影仪和 LLM 组成。精细感知模型已成为另一个关键领域。模型 (Xiao et al., 2023; Liu et al., 2023c; Ren et al., 2024; Zhang et al., 2024a;d; Peng et al., 2023; Deitke et al., 2024) 突破了详细视觉理解方面的可能性界限。Omni (Li et al., $2024\mathrm{g};$ 2025b; Ye et al., 2024) 和 MoE (Riquelme et al., 2021; Lee et al., 2024; Li et al., 2024h;c; Wu et al., 2024b) 的架构也为 LVLM 的未来发展提供了灵感。视觉编码器 (Chen et al., 2023; Liu et al., 2024b; Liang et al., 2025) 和分辨率缩放 (Li et al., 2023c; Ye et al., 2023; Li et al., 2023a) 的增强在提高实际视觉理解的质量方面发挥了关键作用。使用更多样化的场景和更高质量的管理数据是训练高级 LVLM 的重要步骤。 (Guo et al., 2024; Chen et al., 2024d; Liu et al., 2024a; Chen et al., 2024a; Tong et al., 2024; Li et al., 2024a) 中提出的工作对这项工作做出了非常有价值的贡献。

然而，尽管视觉语言模型取得了显著进展，但目前仍面临发展瓶颈，包括计算复杂性、有限的上下文理解、较差的细粒度视觉感知以及不同序列长度下不一致的性能。

在本报告中，我们介绍了最新的工作 Qwen2.5-VL，它延续了 Qwen 系列的开源理念，在各种基准测试中达到甚至超过了顶级的闭源模型。从技术上讲，我们的贡献有四个方面：(1) 我们在视觉编码器中实现了窗口注意力，以优化推理效率；(2) 我们引入了动态 FPS 采样，将动态分辨率扩展到时间维度，并实现了跨不同采样率的全面视频理解；(3) 我们通过与绝对时间对齐来升级时间域中的 MRoPE，从而促进更复杂的时间序列学习；(4) 我们在管理用于预训练和监督微调的高质量数据方面做出了重大努力，进一步将预训练语料库从 1.2 万亿个 token 扩展到 4.1 万亿个 token。

Qwen2.5-VL 的闪光点特征如下：

*   **强大的文档解析能力**：Qwen2.5-VL 将文本识别升级为全文档解析，擅长处理多场景、多语言和各种内置（手写、表格、图表、化学公式和乐谱）文档。
*   **跨格式的精确对象 grounding**：Qwen2.5-VL 提高了检测、指向和计数对象的准确性，适应绝对坐标和 JSON 格式以进行高级空间推理。
*   **超长视频理解和细粒度视频 grounding**：我们的模型将原生动态分辨率扩展到时间维度，增强了理解长达数小时的视频的能力，同时以秒为单位提取事件片段。
*   **增强的计算机和移动设备 Agent 功能**：利用先进的 grounding、推理和决策能力，增强了模型在智能手机和计算机上的卓越 agent 功能。

**图 1**：Qwen2.5-VL 框架展示了视觉编码器和语言模型解码器的集成，以处理多模态输入，包括图像和视频。视觉编码器旨在处理原生分辨率的输入，并支持动态 FPS 采样。不同尺寸的图像和具有不同 FPS 速率的视频帧被动态映射到不同长度的 token 序列。值得注意的是，MRoPE 沿时间维度将时间 ID 与绝对时间对齐，使模型能够更好地理解时间动态，例如事件的步调和精确定位时刻。处理后的视觉数据随后被馈送到 Qwen2.5 LM 解码器。我们重新设计了视觉 Transformer (ViT) 架构，整合了先进的组件，例如具有 SwiGLU 激活的 FFN、用于归一化的 RMSNorm 和基于窗口的注意力机制，以增强性能和效率。

## 2 Approach

Qwen2.5-VL 系列模型的架构更新和数据训练细节概述。

### 2.1 模型架构

Qwen2.5-VL 的整体模型架构包含三个主要组件：

*   **大型语言模型 (LLM)**:  Qwen2.5-VL 系列采用大型语言模型作为基础组件，并使用 Qwen2.5 LLM 的预训练权重进行初始化。为了更好地满足多模态理解的需求，模型将 1D RoPE (Rotary Position Embedding) 修改为多模态旋转位置嵌入 (MRoPE)，并与绝对时间对齐。

*   **视觉编码器**:  视觉编码器采用重新设计的 Vision Transformer (ViT) 架构。在结构上，ViT 结合了 2D-RoPE 和窗口注意力机制，以支持原生输入分辨率，并加速整个视觉编码器的计算。在训练和推理过程中，输入图像的高度和宽度都会被调整为 28 的倍数，然后再输入 ViT。ViT 通过将图像分割成步长为 14 的 patches 来处理图像，生成一组图像特征。

*   **基于 MLP 的视觉-语言融合器**:  为了解决图像特征长序列带来的效率挑战，模型采用了一种简单而有效的方法来压缩特征序列，然后再将其馈送到 LLM。具体来说，模型首先将空间上相邻的四组 patch 特征分组，然后将这些分组的特征连接起来，并通过一个两层 MLP 进行投影，使其维度与 LLM 中使用的文本嵌入对齐。

**表 1: Qwen2.5-VL 的配置**

| 配置                      | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
| :------------------------ | :------------ | :------------ | :------------- |
| **Vision Transformer (ViT)** |               |               |                |
| Hidden Size               | 1280          | 1280          | 1280           |
| # Layers                  | 32            | 32            | 32             |
| #Num Heads                | 16            | 16            | 16             |
| Intermediate Size         | 3456          | 3456          | 3456           |
| Patch Size                | 14            | 14            | 14             |
| Window Size               | 112           | 112           | 112            |
| Full Attention Block Indexes | {7, 15, 23, 31} | {7, 15, 23, 31} | {7, 15, 23, 31}  |
| **Vision-Language Merger**  |               |               |                |
| In Channel                | 1280          | 1280          | 1280           |
| Out Channel               | 2048          | 3584          | 8192           |
| **Large Language Model (LLM)** |               |               |                |
| Hidden Size               | 2048          | 3,584         | 8192           |
| # Layers                  | 36            | 28            | 80             |
| #KV Heads                 | 2             | 4             | 8              |
| Head Size                 | 128           | 128           | 128            |
| Intermediate Size         | 4864          | 18944         | 29568          |
| Embedding Tying           |               | ×             | ×              |
| Vocabulary Size           | 151646        | 151646        | 151646         |
| # Trained Tokens          | 4.1T          | 4.1T          | 4.1T           |

### 2.1.1 快速高效的视觉编码器

视觉编码器在多模态大型语言模型 (MLLM) 中起着关键作用。为了解决原生分辨率输入在训练和推理过程中带来的计算负载不均衡问题，Qwen2.5-VL 重新设计了 Vision Transformer (ViT) 架构。

*   **窗口注意力机制**:  为了缓解处理不同尺寸图像时出现的二次计算复杂度问题，模型在大多数层中引入了窗口注意力机制，确保计算成本与 patch 的数量呈线性关系，而不是二次关系。只有四个层采用完全自注意力机制，而其余层则使用窗口注意力机制，最大窗口大小为 112x112 (对应 8x8 个 patch)。小于 112x112 的区域在处理时无需 padding，保留其原始分辨率。

*   **2D 旋转位置嵌入 (RoPE)**:  在位置编码方面，模型采用 2D 旋转位置嵌入 (RoPE) 来有效捕捉 2D 空间中的空间关系。

*   **3D patch 分区**:  为了更好地处理视频输入，模型将方法扩展到 3D patch 分区。具体来说，模型使用 14x14 图像 patch 作为基本单元，这与用于静态图像的传统 ViT 保持一致。对于视频数据，两个连续帧被组合在一起，显著减少了馈送到语言模型的 token 数量。

*   **网络结构优化**:  为了简化整体网络结构，模型使 ViT 架构更接近大型语言模型 (LLM) 的设计原则。具体来说，模型采用 RMSNorm (Zhang & Sennrich, 2019) 进行归一化，并采用 SwiGLU (Dauphin et al., 2017) 作为激活函数。

*   **从头开始训练的 ViT**:  在训练方面，Qwen2.5-VL 从头开始训练重新设计的 ViT。训练过程包括 CLIP 预训练、视觉-语言对齐和端到端微调等多个阶段。为了确保模型在不同输入分辨率下的鲁棒性，模型在训练期间采用原生分辨率的动态采样。图像根据其原始宽高比进行随机采样，使模型能够有效地泛化到不同分辨率的输入。

### 2.1.2 原生动态分辨率和帧率

Qwen2.5-VL 在空间和时间维度上都进行了改进，以有效地处理各种多模态输入。

*   **空间维度**:  Qwen2.5-VL 动态地将不同尺寸的图像转换为具有相应长度的 token 序列。与传统的坐标归一化方法不同，模型直接使用输入图像的实际尺寸来表示边界框、点和其他空间特征。这使得模型能够固有地学习尺度信息，提高其处理不同分辨率图像的能力。

*   **时间维度**:  对于视频输入，Qwen2.5-VL 采用了动态帧率 (FPS) 训练和绝对时间编码。通过适应可变的帧率，模型可以更好地捕捉视频内容的时间动态。与其他结合文本时间戳或利用额外 head 来实现时间 grounding 的方法不同，Qwen2.5-VL 引入了一种新颖而有效的策略，将 MRoPE ID 直接与时间戳对齐。

### 2.1.3 多模态旋转位置嵌入 (MRoPE) 与绝对时间对齐

位置嵌入对于建模视觉和语言模态的序列数据至关重要。Qwen2.5-VL 在 Qwen2-VL 中引入的多模态旋转位置嵌入 (MRoPE) 的基础上，扩展了其功能，以更好地处理视频中的时间信息。

*   **MRoPE 的分解**:  Qwen2-VL 中的 MRoPE 将位置嵌入分解为三个不同的组成部分：时间、高度和宽度，以有效地建模多模态输入。对于文本输入，所有三个组成部分都使用相同的位置 ID，这使得 MRoPE 在功能上等同于传统的 1D RoPE (Su et al., 2024)。对于图像，时间 ID 在视觉 token 中保持不变，而唯一的 ID 则根据每个 token 在图像中的空间位置分配给高度和宽度组成部分。当处理被视为帧序列的视频时，时间 ID 会为每个帧递增，而高度和宽度组成部分则遵循与静态图像相同的分配模式。

*   **与绝对时间对齐**:  为了解决 Qwen2-VL 中 MRoPE 的时间位置 ID 与输入帧数相关联，而没有考虑内容变化速度或视频中事件绝对时间的问题，Qwen2.5-VL 引入了一个关键改进：将 MRoPE 的时间组成部分与绝对时间对齐。通过利用时间 ID 之间的间隔，模型能够学习跨不同 FPS 采样率视频的一致时间对齐。

### 2.2 预训练

预训练数据集的构建以及整体训练流程和配置的概述。

#### 2.2.1 预训练数据

与 Qwen2-VL 相比，Qwen2.5-VL 显著扩展了预训练数据的规模，从 1.2 万亿 token 增加到约 4 万亿 token。预训练数据集是通过多种方法构建的，包括清理原始网络数据、合成数据等。数据集涵盖了各种多模态数据，例如图像 caption、交错的图像-文本数据、光学字符识别 (OCR) 数据、视觉知识 (例如，名人、地标、植物和动物识别)、多模态学术问题、定位数据、文档解析数据、视频描述、视频定位和基于 agent 的交互数据。在整个训练过程中，模型在不同阶段仔细调整这些数据类型的组成和比例，以优化学习效果。

*   **交错的图像-文本数据**:  交错的图像-文本数据对于多模态学习至关重要，它具有三个关键优势：(1) 支持使用同步的视觉和文本提示进行上下文学习 (Alayrac et al., 2022)，(2) 在缺少图像时保持强大的纯文本能力 (Lin et al., 2024)，以及 (3) 包含广泛的通用信息。然而，许多可用的交错数据缺乏有意义的文本-图像关联，并且通常包含噪声，限制了其在复杂推理和创造性生成方面的用途。为了解决这些挑战，Qwen2.5-VL 开发了一个用于评分和清理数据的 pipeline，确保只使用高质量、相关的交错数据。

    *   **图像-文本评分标准**:  (1) 纯文本质量，(2) 图像-文本相关性，(3) 图像-文本互补性，以及 (4) 信息密度平衡。

*   **具有绝对位置坐标的 Grounding 数据**:  Qwen2.5-VL 采用原生分辨率训练，旨在更准确地感知世界。为了提高 grounding 能力的泛化性，模型开发了一个综合数据集，其中包含带有指代表达式的边界框和点，利用了公开可用的数据集和专有数据。

*   **文档全方位解析数据**:  为了训练 Qwen2.5-VL，模型合成了大量的文档数据，并创新性地将文档中的各种元素 (例如表格、图表、公式、图像、乐谱、化学式等) 统一格式化为 HTML，将布局框信息和插图描述集成到 HTML 标签结构中。

*   **OCR 数据**:  收集和整理来自不同来源的数据，以提高 OCR 性能，包括合成数据、开源数据和内部收集的数据。为了支持更广泛的语言范围并增强多语言能力，模型整合了一个大规模多语言 OCR 数据集，支持法语、德语、意大利语、西班牙语、葡萄牙语、阿拉伯语、俄语、日语、韩语和越南语等多种语言。

*   **视频数据**:  为了确保在理解具有不同帧率 (FPS) 的视频数据时具有更强的鲁棒性，模型在训练期间动态采样 FPS，以在训练数据集中实现更均匀的 FPS 分布。此外，对于时长超过半小时的视频，模型专门构建了一组长视频 caption，通过有针对性的合成 pipeline 合成多帧 caption。

*   **Agent 数据**:  为了增强 Qwen2.5-VL 的感知和决策能力，以构建 agent 能力，模型在移动设备、Web 和桌面平台上收集屏幕截图，并使用合成数据引擎生成屏幕截图 caption 和 UI 元素 grounding 注释。

**表 2: 不同阶段的训练数据量和组成**

| 阶段                 | Visual Pre-Training | Multimodal Pre-Training | Long-Context Pre-Training |
| :------------------- | :------------------ | :---------------------- | :------------------------ |
| **Data**             | Image Caption       | +                       | +                         |
|                      |                     | Pure text               | Long Video                |
|                      |                     | InterleavedData         |                           |
|                      |                     | Knowledge VQA,Video     | Long Agent                |
|                      |                     | Grounding,Agent         | Long Document             |
| **Tokens**           | 1.5T                | 2T                      | 0.6T                      |
| **Sequence length**  | 8192                | 8192                      | 32768                     |
| **Training**         | ViT                 | ViT&LLM                 | ViT&LLM                   |

#### 2.2.2 训练配方

Qwen2.5-VL 从头开始训练 Vision Transformer (ViT)，使用 DataComp (Gadre et al., 2023) 和一些内部数据集作为视觉编码器的初始化，同时利用预训练的 Qwen2.5 大型语言模型 (LLM) (Yang et al., 2024a) 作为 LLM 组件的初始化。预训练过程分为三个不同的阶段，每个阶段采用不同的数据配置和训练策略，以逐步增强模型的能力。

*   **第一阶段**:  仅训练 Vision Transformer (ViT)，以提高其与语言模型的对齐，为多模态理解奠定坚实的基础。

*   **第二阶段**:  解冻所有模型参数，并在各种多模态图像数据上训练模型，以增强其处理复杂视觉信息的能力。

*   **第三阶段**:  为了进一步增强模型在更长序列上的推理能力，模型整合了视频和基于 agent 的数据，并增加了序列长度。

*   **动态数据 packing**:  为了解决不同图像尺寸和文本长度带来的挑战，优化训练效率，模型采用了一种动态 packing 数据样本的策略，根据其对应的 LLM 输入序列长度来 packing 数据样本，确保计算负载均衡。

### 2.3 后期训练

Qwen2.5-VL 的后期训练对齐框架采用双阶段优化范式，包括监督式微调 (SFT) 和直接偏好优化 (DPO) (Rafailov et al., 2023)。

*   **监督式微调 (SFT)**:  旨在通过有针对性的指令优化，弥合预训练表示和下游任务需求之间的差距。在 SFT 阶段，模型采用 ChatML 格式 (Openai, 2024) 来构建指令跟随数据，并使用高质量的微调数据集 (约 200 万条条目) 来增强模型的指令跟随能力。

    *   **指令数据**:  SFT 阶段采用了精心策划的数据集，旨在增强模型在不同模态下的指令跟随能力。该数据集包含约 200 万条条目，纯文本数据 (50%) 和多模态数据 (50%) 均匀分布，其中包括图像-文本和视频-文本组合。数据集主要由中文和英文数据组成，并辅以多语言条目，以支持更广泛的语言多样性。数据集结构旨在反映不同程度的对话复杂性，包括单轮和多轮交互。

    *   **数据过滤 pipeline**:  为了解决开源和合成数据集通常表现出的显著可变性问题，模型实施了一个两阶段数据过滤 pipeline，旨在系统地提高监督式微调 (SFT) 数据集的质量。

        *   **阶段 1：特定领域分类**:  在初始阶段，模型采用 Qwen2-VL-Instag (一种专门的分类模型，源自 Qwen2-VL-72B) 对问答 (QA) 对进行分层分类。

        *   **阶段 2：特定领域定制的过滤**:  第二阶段涉及特定领域定制的过滤，它集成了基于规则和基于模型的方法，以全面提高数据质量。

    *   **用于增强推理的拒绝采样**:  为了补充结构化数据过滤 pipeline，模型采用拒绝采样作为一种策略来优化数据集并增强视觉-语言模型 (VLM) 的推理能力。

*   **直接偏好优化 (DPO)**:  DPO 阶段专门关注图像-文本和纯文本数据，利用偏好数据使模型与人类偏好保持一致，每个样本只处理一次，以确保高效优化。

*   **训练配方**:  Qwen2.5-VL 的后期训练过程包括两个阶段：监督式微调 (SFT) 和直接偏好优化 (DPO)，这两个阶段都冻结了 Vision Transformer (ViT) 参数。

## 3 Experiments

本节首先介绍整体模型，并将其与当前最先进 (SoTA) 的模型进行比较。然后，评估模型在各种子能力方面的性能。

### 3.1 与 SOTA 模型的比较

**表 3: Qwen2.5-VL 和 State-of-the-art 的性能**

| Datasets                     | Previous Open-sourceSoTA | Claude-3.5 Sonnet-0620 | GPT-4o 0513 | InternVL2.5 78B | Qwen2-VL 72B | Qwen2.5-VL 72B | Qwen2.5-VL 7B | Qwen2.5-VL 3B |
| :--------------------------- | :----------------------- | :----------------------- | :---------- | :---------------- | :------------- | :--------------- | :-------------- | :-------------- |
| **College-levelProblems**    |                          |                          |             |                    |                |                  |                 |                 |
| MMMUval (Yue et al., 2023)    | 70.1 Chen et al. (2024d)  | 68.3                     | 69.1        | 70.1               | 64.5           | 70.2             | 58.6            | 53.1            |
| MMMU-Prooverall (Yue et al., 2024) | 48.6 Chen et al.(2024d)  | 51.5                     | 51.9        | 48.6               | 46.2           | 51.1             | 38.3            | 31.56           |
| **Math**                       |                          |                          |             |                    |                |                  |                 |                 |
| MathVistamini (Lu et al., 2024) | 72.3 Chen et al. (2024d)  | 67.7                     | 63.8        | 72.3               | 70.5           | 74.8             | 68.2            | 62.3            |
| MATH-Visionfull (Wang et al., 2024d) | 32.2 Chen et al. (2024d)  |                          | 30.4        | 32.2               | 25.9           | 38.1             | 25.1            | 21.2            |
| MathVersemini (Zhang et al., 2024c) | 51.7Chen et al. (2024d)  |                          | 50.2        | 51.7               | 一              | 57.6             | 49.2            | 47.6            |
| **GeneralVisualQuestionAnswering** |                          |                          |             |                    |                |                  |                 |                 |
| MegaBench(Chen et al.,2024b)  | 47.4 MiniMax et al. (2025) | 52.1                     | 54.2        | 45.6               | 46.8           | 51.3             | 36.8            | 28.9            |
| MMBench-ENtest (Liu et al., 2023d) | 88.3 Chen et al.(2024d)  | 82.6                     | 83.4        | 88.3               | 86.9           | 88.6             | 83.5            | 79.1            |
| MMBench-CNtest (Liu et al.,2023d) | 88.5 Chen et al. (2024d)  | 83.5                     | 82.1        | 88.5               | 86.7           | 87.9             | 83.4            | 78.1            |
| MMBench-V1.1-ENtest (Liu et al., 2023d) | 87.4 Chen et al.(2024d)  | 80.9                     | 83.1        | 87.4               | 86.1           | 88.4             | 82.6            | 77.4            |
| MMStar(Chen et al.,2024c)    | 69.5 Chen et al. (2024d)  | 65.1                     | 64.7        | 69.5               | 68.3           | 70.8             | 63.9            | 55.9            |
| MMEsum (Fu et al., 2023)      | 2494 Chen et al.(2024d)  | 1920                     | 2328        | 2494               | 2483           | 2448             | 2347            | 2157            |
| MuirBench (Wang et al.,2024a)  | 63.5 Chen et al. (2024d)  |                          | 68.0        | 63.5               |                | 70.7             | 59.6            | 47.7            |
| BLINKval (Fu et al., 2024c)   | 63.8 Chen et al.(2024d)  |                          | 68.0        | 63.8               |                | 64.4             | 56.4            | 47.6            |
| CRPErelation (Wang et al., 2024h) | 78.8 Chen et al. (2024d)  |                          | 76.6        | 78.8               |                | 79.2             | 76.4            | 73.6            |
| HallBenchavg (Guan et al., 2023) | 58.1 Wang et al. (2024f)  | 55.5                     | 55.0        | 57.4               | 58.1           | 55.2             | 52.9            | 46.3            |
| MTVQA (Tang et al., 2024)    | 31.9 Chen et al. (2024d)  | 25.7                     | 27.8        | 31.9               | 30.9           | 31.7             | 29.2            | 24.8            |
| RealWorldQAavg (X.AI, 2024)  | 78.7 Chen et al. (2024d)  | 60.1                     | 75.4        | 78.7               | 77.8           | 75.7             | 68.5            | 65.4            |
| MME-RealWorlden (Zhang et al.,2024f) | 62.9 Chen et al. (2024d)  | 51.6                     | 45.2        | 62.9               |                | 63.2             | 57.4            | 53.1            |
| MMVetturbo (Yu et al., 2024)  | 74.0 Wang et al. (2024f)  | 70.1                     | 69.1        | 72.3               | 74.0           | 76.2             | 67.1            | 61.8            |
| MM-MT-Bench (Agrawal et al.,2024) | 7.4 Agrawal et al. (2024) | 7.5                      | 7.72        |                    | 6.59           | 7.6              | 6.3             | 5.7             |

实验部分评估了 Qwen2.5-VL 在各种数据集上的性能，并将其与最先进的模型（如 Claude-3.5-Sonnet-0620 (Anthropic, 2024a)、GPT-4o-0513 (OpenAI, 2024)、InternVL2.5 (Chen et al., 2024d) 以及不同尺寸的 Qwen2-VL (Wang et al., 2024e)）进行了比较。

*   **大学水平问题**：Qwen2.5-VL-72B 在 MMMU (Yue et al., 2023) 上取得了 70.2 的分数。在 MMMUPro (Yue et al., 2024) 上，Qwen2.5-VL-72B 的得分为 51.1，超过了之前最先进的开源模型，并达到了与 GPT-4o 相媲美的性能。
*   **数学相关任务**：Qwen2.5-VL-72B 展示了强大的能力。在 MathVista (Lu et al., 2024) 上，它取得了 74.8 的分数，超过了之前 72.3 的开源最佳分数。对于 MATH-Vision (Wang et al., 2024d)，Qwen2.5-VL-72B 的得分为 38.1，而 MathVerse (Zhang et al., 2024c) 的得分为 57.6，这两者都显示出与其他领先模型相比具有竞争力的结果。
*   **通用视觉问答**：Qwen2.5-VL-72B 在多个基准测试中表现出色。在 MMbenchEN (Liu et al., 2023d) 上，它取得了 88.6 的分数，略微超过了之前 88.3 的最佳分数。该模型在 MuirBench (Wang et al., 2024a) 和 BLINK (Fu et al., $2024c)$ 中也表现良好，得分分别为 70.7 和 64.4。在 MTVQA (Tang et al., 2024) 的多语言能力评估中，Qwen2.5-VL-72B 取得了 31.7 的分数，展示了其强大的多语言文本识别能力。在 MMVet (Yu et al., 2024) 和 MM-MT-Bench (Agrawal et al., 2024) 等主观评估中，Qwen2.5-VL-72B 的得分分别为 76.2 和 7.6，表明其具有出色的自然对话体验和用户满意度。
*   **小规模版本**：即使是小规模版本的 Qwen2.5-VL，特别是 Qwen2.5-VL-7B 和 Qwen2.5-VL-3B，也表现出极具竞争力的性能。例如，在 MMStar 数据集上，Qwen2.5-VL-7B 取得了 $63.9\%$ 的分数，而 Qwen2.5-VL-3B 的得分为 $55.9\%$。

### 3.2 纯文本任务的性能

**表 4: ${70}\mathbf{B}+$ Instruct 模型和 Qwen2.5-VL 在纯文本任务上的性能**

| Datasets         | Llama-3.1-70B | Llama-3.1-405B | Qwen2-72B | Qwen2.5-72B | Qwen2.5-VL-72B |
| :--------------- | :------------ | :------------- | :---------- | :------------ | :--------------- |
| **GeneralTasks** |               |                |             |               |                  |
| MMLU-Pro         | 66.4          | 73.3           | 64.4        | 71.1          | 71.2             |
| MMLU-redux       | 83.0          | 86.2           | 81.6        | 86.8          | 85.9             |
| LiveBench-0831   | 46.6          | 53.2           | 41.5        | 52.3          | 57.0             |
| **Mathematics&ScienceTasks** |               |                |             |               |                  |
| GPQA             | 46.7          | 51.1           | 42.4        | 49.0          | 49.0             |
| MATH             | 68.0          | 73.8           | 69.0        | 83.1          | 83.0             |
| GSM8K            | 95.1          | 96.8           | 93.2        | 95.8          | 95.3             |
| **Coding Tasks**   |               |                |             |               |                  |
| HumanEval        | 80.5          | 89.0           | 86.0        | 86.6          | 87.8             |
| MultiPL-E        | 68.2          | 73.5           | 69.2        | 75.1          | 79.5             |
| **Alignment Tasks** |               |                |             |               |                  |
| IFEval           | 83.6          | 86.0           | 77.6        | 84.1          | 86.3             |

为了 критически 评估指令调整模型在纯文本任务上的性能，我们选择了几个具有代表性的基准测试来评估模型在各种领域的能力，包括通用任务 (Wang et al., 2024j; Gema et al., 2024; White et al., 2024)、数学和科学任务 (Rein et al., 2023; Hendrycks et al., 2021; Cobbe et al., 2021)、编码任务 (Chen et al., 2021; Cassano et al., 2023) 和对齐任务 (Zhou et al., 2023)。结果表明，Qwen2.5-VL 不仅在多模态任务上取得了最先进 (SoTA) 的性能，而且在纯文本任务上也表现出领先的性能，展示了其在不同评估标准下的多功能性和鲁棒性。

### 3.3 定量结果

#### 3.3.1 通用视觉问答

为了全面评估模型在通用视觉问答 (VQA) 和对话方面的能力，我们在各种数据集上进行了广泛的实验。Qwen2.5-VL 在各种 VQA 任务、主观评估、多语言场景和多图像问题中都表现出了最先进的性能。即使是小规模版本的 Qwen2.5-VL，也表现出极具竞争力的性能。

#### 3.3.2 文档理解和 OCR

**表 5: Qwen2.5-VL 和其他模型在 OCR、图表和文档理解基准测试中的性能**

| Datasets                 | Sonnet | Claude-3.5Gemini1.5 Pro | GPT 40 | InternVL2.5 78B | Qwen2.5-VL 72B | Qwen2.5-VL 7B | Qwen2.5-VL 3B |
| :----------------------- | :----- | :----------------------- | :------- | :---------------- | :--------------- | :-------------- | :-------------- |
| **OCR-relatedParsingTasks** |        |                          |          |                    |                  |                 |                 |
| CC-OCR OmniDocBenchedit en/zh√ 0.330/0.381 0.230/0.281 0 | 62.5   | 73.0                     | 66.9     | 0.265/0.435        | 64.7 0.275/0.324  | 79.8 0.226/0.324  | 77.8 0.308/0.398  |
| **OCR-relatedUnderstandingTasks** |        |                          |          |                    |                  |                 |                 |
| AI2Dw.M.                 | 81.2   | 88.4                     | 84.6     | 89.1               | 88.7             | 83.9            | 81.6            |
| TextVQAval               | 76.5   | 78.8                     | 77.4     | 83.4               | 83.5             | 84.9            | 79.3            |
| DocVQAtest               | 95.2   | 93.1                     | 91.1     | 95.1               | 96.4             | 95.7            | 93.9            |
| InfoVQAtest              | 74.3   | 81.0                     | 80.7     | 84.1               | 87.3             | 82.6            | 77.1            |
| ChartQAtest Avg.         | 90.8   | 87.2                     | 86.7     | 88.3               | 89.5             | 87.3            | 84.0            |
| CharXivRQ/DQ             | 60.2/84.3 | 43.3/72.0                | 47.1/84.5 | 42.4/82.3          | 49.7/87.4        | 42.5/73.9       | 31.3/58.6       |
| SEED-Bench-2-Plus OCRBench | 71.7 788 | 70.8                     | 72.0     | 71.3               | 73.0             | 70.4 864        | 67.6 797        |
| VCREn-Hard-EM            | 41.7   | 754 28.1                 | 736 73.2 | 854                | 885 79.8         | 80.5            | 37.5            |
| **OCR-relatedComprehensiveTasks** |        |                          |          |                    |                  |                 |                 |
| OCRBench_v2en/zh         | 45.2/39.6 | 51.9/43.1                | 46.5/32.2 | 49.8/52.1          | 61.5/63.7        | 56.3/57.2       | 54.3/52.1       |

在多场景、多语言和各种内置（手写、表格、图表、化学公式和数学表达式）文档的元素解析 OCR 相关解析基准测试 (如 CC-OCR 和 OmniDocBench) 中，Qwen2.5-VL-72B 模型由于管理的数据和 LLM 模型的出色能力，创造了新的最先进水平。在场景文本、图表、diagram 和文档的 OCR 相关理解基准测试中，Qwen2.5-VL 模型以良好的理解能力取得了令人印象深刻的性能。值得注意的是，在综合 OCR 相关理解基准测试 (如 OCRBench、InfoVQA (侧重于信息图) 和 SEED-Bench-2-Plus (涵盖包括图表、地图和网络在内的富文本场景)) 中，Qwen2.5-VL-72B 取得了显著成果，大大优于强大的竞争对手，如 InternVL2.5-78B。此外，对于 OCR 相关综合基准测试 (如 OCRBench_v2，包括各种 OCR 相关解析和理解任务)，Qwen2.5-VL 模型也取得了最佳性能，在英语和中文 track 上分别超过最佳模型 Gemini 1.5-Pro $9.6\%$ 和 $20.6\%$。

#### 3.3.3 空间理解

**表 6: Qwen2.5-VL 和其他模型在 grounding 方面的性能**

| Datasets       | Gemini1.5 Pro | Grounding DINO | Molmo72B | InternVL2.5 78B | Qwen2.5-VL 72B | Qwen2.5-VL 7B | Qwen2.5-VL 3B |
| :------------- | :------------ | :------------- | :--------- | :---------------- | :--------------- | :-------------- | :-------------- |
| RefcocOval     | 73.2          | 90.6           |            | 93.7               | 92.7             | 90.0            | 89.1            |
| RefcocOtestA   | 72.9          | 93.2           |            | 95.6               | 94.6             | 92.5            | 91.7            |
| RefcocOtestB   | 74.6          | 88.2           |            | 92.5               | 89.7             | 85.4            | 84.0            |
| Refcoco+val    | 62.5          | 88.2           |            | 90.4               | 88.9             | 84.2            | 82.4            |
| Refcoco+testA  | 63.9          | 89.0           |            | 94.7               | 92.2             | 89.1            | 88.0            |
| Refcoco+testB  | 65.0          | 75.9           |            | 86.9               | 83.7             | 76.9            | 74.1            |
| Refcocogval    | 75.2          | 86.1           |            | 92.7               | 89.9             | 87.2            |                 |
| Refcocogtest   | 76.2          | 87.0           |            | 92.2               | 90.3             | 87.2            | 85.2 85.7       |
| ODinW          | 36.7          | 55.0           |            | 31.7               | 43.1             | 37.3            | 37.5            |
| PointGrounding |               |                | 69.2       |                    | 67.5             | 67.3            | 58.3            |

**表 7: Qwen2.5-VL 和其他模型在计数方面的性能**

| Datasets   | Gemini1.5-ProGPT-4oClaude-3.5SonnetMolmo-72b | InternVL2.5-78B | Qwen2.5-VL-72B |
| :--------- | :--------------------------------------------- | :---------------- | :--------------- |
| CountBench | 85.5                                           | 87.9              | 89.7              | 91.2              | 72.1              | 93.6             |

Qwen2.5-VL 在从 box-grounding、point-grounding 到 counting 的不同基准测试中都取得了领先的性能。通过为 Qwen2.5-VL 配备 box 和 point-grounding 能力，它能够理解、定位和推理图像某些部分的非常细节。对于开放词汇对象检测，Qwen2.5-VL 在 ODinW-13 上取得了 $43.1~\mathrm{mAP}$ 的良好性能，超过了大多数 LVLM，并迅速缩小了通用模型和专家模型之间的差距。此外，Qwen2.5-VL 解锁了基于点的 grounding 能力，使其能够精确定位过去难以用边界框表示的特定对象的非常细节。Qwen2.5-VL 的计数能力也取得了巨大进步，在使用 “detect then count”-style 提示的 Qwen2.5-VL-72B 上，CountBench 的领先准确率达到了 93.6。

#### 3.3.4 视频理解和 Grounding

**表 8: Qwen2.5-VL 和其他模型在视频基准测试中的性能**

| Datasets             | Gemini1.5-Pro | GPT-40 | Qwen2.5-VL-72B | Qwen2.5-VL-7B | Qwen2.5-VL-3B |
| :------------------- | :------------ | :------- | :--------------- | :-------------- | :-------------- |
| **Video Understanding Tasks** |               |          |                  |                 |                 |
| Video-MMEw/o sub.    | 75.0          | 71.9     | 73.3             | 65.1            | 61.5            |
| Video-MMEw sub.      | 81.3          | 77.2     | 79.1             | 71.6            | 67.6            |
| Video-MMMU           | 53.9          | 61.2     | 60.2             | 47.4            |                 |
| MMVUval              | 65.4          | 67.4     | 62.9             | 50.1            |                 |
| MVBench              | 60.5          | 64.6     | 70.4             | 69.6            | 67.0            |
| MMBench-Video        | 1.30          | 1.63     | 2.02             | 1.79            | 1.63            |
| LongVideoBenchval    | 64.0          | 66.7     | 60.7             | 56.0            | 54.2            |
| LVBench              | 33.1          | 30.8     | 47.3             | 45.3            | 43.3            |
| EgoSchematest        | 71.2          | 72.2     | 76.2             | 65.0            | 64.8            |
| PerceptionTesttest   | -             |          | 73.2             | 70.5            | 66.9            |
| MLVUM-Avg            |               | 64.6     | 74.6             | 70.2            | 68.2            |
| TempCompassAvg       | 67.1          | 73.8     | 74.8             | 71.7            | 64.4            |
| **Video GroundingTasks** |               |          |                  |                 |                 |
| Charades-STAmIoU     |               | 35.7     | 50.9             | 43.6            | 38.8            |

在 LVBench 和 MLVU (通过问答任务评估长视频理解能力) 上，Qwen2.5-VL-72B 取得了显著成果，大大优于 GPT-4o 等强大的竞争对手。通过利用所提出的同步 MRoPE，Qwen2.5-VL 增强了其在时间敏感型视频理解方面的能力，具有改进的时间戳参考、时间 grounding、密集 caption 和其他功能。在 Charades-STA 数据集 (评估使用精确时间戳准确定位事件或活动的能力) 上，Qwen2.5-VL-72B 取得了令人印象深刻的 mIoU 分数 50.9，从而超越了 GPT-4o 的性能。

#### 3.3.5 Agent

**表 9: Qwen2.5-VL 和其他模型在 GUI Agent 基准测试中的性能**

| Benchmarks               | GPT-40   | Gemini2.0 | Claude   | Aguvis-72B | Qwen2-VL-72B | Qwen2.5-VL-72B |
| :----------------------- | :------- | :-------- | :------- | :----------- | :------------- | :--------------- |
| ScreenSpot               | 18.1     | 84.0      | 83.0     | 89.2         |                | 87.1             |
| ScreenSpotPro            |          | =         | 17.1     | 23.6         | 1.6            | 43.6             |
| Android Control HighEM   | 20.8     | 28.5      | 12.5     | 66.4         | 59.1           | 67.36            |
| Android Control LowEM    | 19.4     | 60.2      | 19.4     | 84.4         | 59.2           | 93.7             |
| AndroidWorldsR           | 34.5% (SoM) | 26% (SoM) | 27.9%    | 26.1%        | 6% (SoM)       | 35%              |
| MobileMiniWob++sR        | 61%      | 42% (SoM) | 61% (SoM) | 66%          | 50%(SoM)      | 68%              |
| OsWorld                  | 5.03     | 4.70      | 14.90    | 10.26        | 2.42           | 8.83             |

Qwen2.5-VL-72B 的性能证明了 GUI grounding 基准测试的卓越进步。它在 ScreenSpot 上实现了 $87.1\%$ 的准确率，与 Gemini 2.0 $(84.0\%)$ 和 Claude $(83.0\%)$ 展开了激烈竞争，同时在 ScreenSpot Pro 上以 $43.6\%$ 的准确率树立了新标准，远远超过了 Aguvis-72B $(\dot{2}3.6\%)$ 及其基础模型 Qwen2-VL-72B $(1.6\%)$。利用这些卓越的 grounding 能力，Qwen2.5-VL-72B 在所有离线评估基准测试中都以巨大差距优于基线模型。在在线评估中，一些基线模型由于 grounding 能力有限而难以完成任务。因此，我们将 Set-of-Mark (SoM) 应用于这些模型的输入。结果表明，Qwen2.5-VL-72B 在 AndroidWorld 和 MobileMiniWob $^{++}$ 上优于基线模型，并在 OSWorld 在线评估中取得了可比的性能，而无需辅助标记。

## 4 Conclusion

Qwen2.5-VL 是一个最先进的视觉语言模型系列，在多模态理解和交互方面取得了显著进步。凭借在视觉识别、对象定位、文档解析和长视频理解方面的增强功能，Qwen2.5-VL 在静态和动态任务中都表现出色。其原生动态分辨率处理和绝对时间编码实现了对各种输入的强大处理能力，而 Window Attention 则在不牺牲分辨率保真度的情况下降低了计算开销。Qwen2.5-VL 适用于从边缘 AI 到高性能计算的广泛应用。旗舰模型 Qwen2.5-VL-72B 在文档和图表理解方面与 GPT-4o 和 Claude 3.5 Sonnet 等领先模型相媲美或超越，同时在纯文本任务上保持了强大的性能。较小的 Qwen2.5-VL-7B 和 Qwen2.5-VL-3B 变体优于同等规模的竞争对手，提供了效率和多功能性。Qwen2.5-VL 为视觉语言模型树立了新的基准，展示了卓越的跨领域泛化和任务执行能力。它的创新为更智能和交互式的系统铺平了道路，弥合了感知和实际应用之间的差距。

## 5 Authors

**核心贡献者**：Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin

**贡献者1**：An Yang, Binyuan Hui, Bowen Yu, Chen Cheng, Dayiheng Liu, Fan Hong, Fei Huang, Jiawei Liu, Jin Xu, Jianhong Tu, Jianyuan Zeng, Jie Zhang, Jinkai Wang, Jianwei Zhang, Jingren Zhou, Kexin Yang, Mei Li, Ming Yan, Na Ni, Rui Men, Songtao Jiang, Xiaodong Deng, Xiaoming Huang, Ximing Zhou, Xingzhang Ren, Yang Fan, Yichang Zhang, Yikai Zhu, Yuqiong Liu, Zhifang Guo