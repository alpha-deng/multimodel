# 2024-11 月度论文分类汇总

共有624篇相关领域论文, 另有66篇其他

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

该领域共有 2 篇论文

### Commissioning An All-Sky Infrared Camera Array for Detection Of Airborne Objects 
[[arxiv](https://arxiv.org/abs/2411.07956)] [[cool](https://papers.cool/arxiv/2411.07956)] [[pdf](https://arxiv.org/pdf/2411.07956)]
> **Authors**: Laura Dominé,Ankit Biswas,Richard Cloete,Alex Delacroix,Andriy Fedorenko,Lucas Jacaruso,Ezra Kelderman,Eric Keto,Sarah Little,Abraham Loeb,Eric Masson,Mike Prior,Forrest Schultz,Matthew Szenher,Wes Watters,Abby White
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: ef:Sensors 2025, 25(3), 783
- **标题**: 调试全天空红外摄像头阵列以检测空气中的物体
- **领域**: 天体物理学仪器和方法,计算机视觉和模式识别,图像和视频处理
- **摘要**: 迄今为止，几乎没有关于身份不明的空中现象（UAP）的公开科学数据，其性质和运动学据称位于已知现象的性能外。为了解决这一缺陷，伽利略项目正在设计，建造和调试多模式地面天文台，以不断监视天空，并对包括天然和人工制造的所有空中现象进行严格的长期空中普查。关键仪器之一是使用八个未冷却的长波玻璃玻色子640摄像机的全天空红外摄像头阵列。它们的校准包括一种新型的外部校准方法，该方法使用来自自动依赖性监视广播（ADS-B）数据的飞机位置。我们使用从ADS-B数据，合成3-D轨迹和手工标记的现实世界数据集的现实世界数据集中建立了五个月的现场操作的系统性能的第一个基线。我们报告了在各种天气条件，范围和飞机尺寸的情况下，报告了接受率（例如，记录的可见飞机）和检测效率（例如，已成功检测到的记录的飞机）。我们重建$ \ sim $ 500,000从此调试期开始的空中物体轨迹。玩具异常值的搜索重点是二维重建轨迹的巨大尖曲，约有16％的轨迹作为异常值。经过手动审查后，144个轨迹仍然模棱两可：它们可能是平凡的对象，但在没有距离和运动学估计或其他传感器方式的情况下，在开发阶段无法阐明。我们观察到的模棱两可的异常值与系统不确定性相结合的计数产生的五个月间隔的上限为18,271个异常值，其置信度为95％。评估意义的基于可能性的方法适用于我们所有未来的异常搜索。

### AstroM$^3$: A self-supervised multimodal model for astronomy 
[[arxiv](https://arxiv.org/abs/2411.08842)] [[cool](https://papers.cool/arxiv/2411.08842)] [[pdf](https://arxiv.org/pdf/2411.08842)]
> **Authors**: Mariia Rizhko,Joshua S. Bloom
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: Astrom $^3 $：一种天文学的自我监督的多模式模型
- **领域**: 天体物理学仪器和方法,人工智能
- **摘要**: 尽管机器学习的模型现在通常用于促进天文询问，但模型输入往往仅限于主要数据源（即图像或时间序列），在更高级的方法中，一些元数据。然而，随着宽场，多重观测资源的使用，各个感兴趣的来源通常具有广泛的观察模式。在这里，我们构建了一种天文多模式数据集，并提出了Astrom $^3 $，这是一种自制的预训练方法，使模型能够同时从多种方式中学习。具体而言，我们将剪辑（对比性语言图像预处理）模型扩展到三座设置，从而可以整合时间序列的光度计数据，光谱和天体物理元数据。在微调监督的环境中，我们的结果表明，剪辑预训练可改善时间序列光度法的分类性能，其中精度从84.6％增加到91.5％。此外，当标记数据的可用性受到限制时，剪辑将分类精度提高了12.6％，显示了利用较大的无标记数据语料库的有效性。除了微调分类外，我们还可以在其他下游任务中使用训练有素的模型，这些模型在构建自我监督模型期间未明确考虑。特别是，我们显示了使用学习的嵌入进行错误分类识别，相似性搜索和异常检测的功效。一个令人惊讶的亮点是使用流形学习和降低算法的Mira亚型的“重新发现”和两个旋转变量子类。据我们所知，这是天文学中$ n> 2 $模式模型的第一次结构。通过这种方法，自然会预期到$ n> 3 $模式的扩展。

## 材料科学(cond-mat.mtrl-sci:Materials Science)

该领域共有 1 篇论文

### Revealing the Evolution of Order in Materials Microstructures Using Multi-Modal Computer Vision 
[[arxiv](https://arxiv.org/abs/2411.09896)] [[cool](https://papers.cool/arxiv/2411.09896)] [[pdf](https://arxiv.org/pdf/2411.09896)]
> **Authors**: Arman Ter-Petrosyan,Michael Holden,Jenna A. Bilbrey,Sarah Akers,Christina Doty,Kayla H. Yano,Le Wang,Rajendra Paudel,Eric Lang,Khalid Hattar,Ryan B. Comes,Yingge Du,Bethany E. Matthews,Steven R. Spurgeon
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: 30 pages, 5 figures, 2 tables
- **标题**: 使用多模式计算机视觉揭示材料微观结构中订单的演变
- **领域**: 材料科学,机器学习
- **摘要**: 用于微电子，能源存储和极端环境的高性能材料的开发取决于我们描述和直接定义微观结构秩序的能力。我们目前的理解通常源自对成像和光谱数据的费力手动分析，这些分析难以扩展，挑战繁殖，并且缺乏揭示机械模型所需的潜在关联的能力。在这里，我们演示了一种多模式的机器学习（ML）方法，以描述复杂氧化物LA $ _ {1-x} $ sr $ _x $ feo $ feo $ _3 $的电子显微镜分析中的顺序。我们基于完全和半监督的分类构建了混合管道，使我们能够评估每种数据模式的特征和每种模态的值增加了整体。我们观察到单型和多模式模型的性能有着明显的差异，我们从中汲取了使用计算机视觉描述晶体顺序的一般课程。

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 31 篇论文

### AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents 
[[arxiv](https://arxiv.org/abs/2410.24024)] [[cool](https://papers.cool/arxiv/2410.24024)] [[pdf](https://arxiv.org/pdf/2410.24024)]
> **Authors**: Yifan Xu,Xiao Liu,Xueqiao Sun,Siyi Cheng,Hao Yu,Hanyu Lai,Shudan Zhang,Dan Zhang,Jie Tang,Yuxiao Dong
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: AndroidLab：Android Autonomous代理的培训和系统基准测试
- **领域**: 人工智能
- **摘要**: 自主代理对于与现实世界的互动变得越来越重要。尤其是Android药物最近是一种经常提到的相互作用方法。但是，现有用于培训和评估Android代理的研究缺乏对开源和封闭源模型的系统研究。在这项工作中，我们建议AndroidLab作为系统的Android代理框架。它包括具有不同方式，动作空间和可再现基准的操作环境。它支持大型语言模型（LLMS）和多模型模型（LMM）在同一动作领域中。 AndroidLab基准测试包括预定义的Android虚拟设备以及在这些设备上构建的九个应用程序中的138个任务。通过使用AndroidLab环境，我们开发了一个Android指令数据集并培训六个开源LLMS和LMM，从而将LLMS的平均成功率从4.59％提高到21.50％，LMMS的平均成功率从1.93％提高到1.93％至13.28％。 AndroidLab是开源的，可在https://github.com/thudm/android-lab上公开获得。

### Reasoning Limitations of Multimodal Large Language Models. A case study of Bongard Problems 
[[arxiv](https://arxiv.org/abs/2411.01173)] [[cool](https://papers.cool/arxiv/2411.01173)] [[pdf](https://arxiv.org/pdf/2411.01173)]
> **Authors**: Mikołaj Małkiński,Szymon Pawlonka,Jacek Mańdziuk
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 多模式大语言模型的推理局限性。邦加德问题的案例研究
- **领域**: 人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 抽象的视觉推理（AVR）涵盖了一系列任务，其解决方案需要通过类比制作过程来发现图片集的基础概念的能力，类似于人类智商测试。 1968年提出的邦加德问题（BPS）构成了该领域的一个基本挑战，主要是因为它们需要将视觉推理和言语描述结合起来。这项工作提出了一个问题，是否固有地旨在结合视觉和语言来解决BPS的多模式大型语言模型（MLLM）。为此，我们提出了一系列适合MLLM的策略来解决BP并检查四个流行的专有MLLM：GPT-4O，GPT-4 Turbo，Gemini 1.5 Pro和Claude 3.5 SONNET和四个开放型号：Internvl2-8B，Llava-1.6 Mistral-7b，Phi-7b，phi-33.5-Vision＆Pixt＆Pixt＆Pixt＆PIXB.在三个BP数据集上比较了上述MLLM：一组原始的BP实例依赖于合成，基于几何的图像和两个基于现实世界图像的最新数据集，即Bongard-Hoi和Bongard-openworld。实验揭示了MLLM在求解BPS中的显着局限性。特别是，尽管它们的视觉简单性，但模型仍在努力解决经典的合成BP。尽管它们的性能可以改善Bongard-Hoi和Bongard-Openworld在现实世界中的概念，但这些模型仍然难以利用新信息来改善其预测，并有效地利用对话框上下文窗口。为了捕获合成和现实世界中AVR域之间性能差异的原因，我们提出了Bongard-RWR，这是一个由现实世界图像组成的新的BP数据集，将概念从手工制作的合成BPS转化为现实世界中的概念。 MLLMS对Bongard-RWR的结果表明，它们在经典BP上的性能不佳不是由于域的特异性，而是反映了其一般的AVR限制。

### TurtleBench: A Visual Programming Benchmark in Turtle Geometry 
[[arxiv](https://arxiv.org/abs/2411.00264)] [[cool](https://papers.cool/arxiv/2411.00264)] [[pdf](https://arxiv.org/pdf/2411.00264)]
> **Authors**: Sina Rismanchian,Yasaman Razeghi,Sameer Singh,Shayan Doroudi
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 海龟顿顿：乌龟几何形状中的视觉编程基准
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 人类有能力在年轻时代的图像和场景中推理几何模式。但是，开发具有类似推理的大型多模型模型（LMM）仍然是一个挑战，强调了对评估这些能力的强大评估方法的需求。我们介绍了Turtlebench，这是一种旨在评估LMM解释几何模式的能力的基准测试 - 给定视觉示例，文本说明或两者 - 并生成精确的代码输出。受乌龟几何形状的启发，这是一种用于教给孩子基础编码和几何概念的概念，Turtlebench具有具有基本算法逻辑的图案形状的任务。我们的评估表明，领先的LMM在这些任务上很大程度上挣扎，GPT-4O在最简单的任务上仅达到19％的准确性，而很少射击仅促使其性能略有提高（$ <2 \％$）。 Turtlebench强调了人类与人工智能表现在直观和视觉几何理解中的差距，这为该领域的未来研究奠定了基础。 Turtlebench是评估LMM中视觉理解和代码产生能力整合的少数基准之一，为将来的研究奠定了基础。此处提供了本文的代码和数据集：https：//github.com/sinaris76/turtlebench

### Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem 
[[arxiv](https://arxiv.org/abs/2411.00238)] [[cool](https://papers.cool/arxiv/2411.00238)] [[pdf](https://arxiv.org/pdf/2411.00238)]
> **Authors**: Declan Campbell,Sunayana Rane,Tyler Giallanza,Nicolò De Sabbata,Kia Ghods,Amogh Joshi,Alexander Ku,Steven M. Frankland,Thomas L. Griffiths,Jonathan D. Cohen,Taylor W. Webb
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 通过绑定问题的镜头了解视觉语言模型的极限
- **领域**: 人工智能,计算机视觉和模式识别,机器学习,神经元和认知
- **摘要**: 最近的工作记录了最先进的视觉语言模型（VLM）的震撼异质性，包括多模式的语言模型和文本对图像模型。这些模型能够描述并生成各种复杂，自然主义的图像，但它们在基本的多对象推理任务（例如计数，本地化和简单的视觉类比）上表现出令人惊讶的失败，即人类在几乎完美的准确性上表现出色。为了更好地理解成功和失败的令人困惑的模式，我们转向认知科学和神经科学中约束性问题的理论上说明，这是当必须使用共享的代表资源集以表示不同实体（例如，代表图像中的多个对象）时，就会出现一个基本问题，以避免使用序列处理。我们发现，最先进的VLM的许多令人困惑的失败可以解释为由于结合问题而引起的，并且这些故障模式与人类大脑中快速，进食的处理所表现出的局限性非常相似。

### Responsibility-aware Strategic Reasoning in Probabilistic Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2411.00146)] [[cool](https://papers.cool/arxiv/2411.00146)] [[pdf](https://arxiv.org/pdf/2411.00146)]
> **Authors**: Chunyan Mu,Muhammad Najib,Nir Oren
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 概率多代理系统中的责任感战略推理
- **领域**: 人工智能
- **摘要**: 责任在可信赖的自主系统的开发和部署中起着关键作用。在本文中，我们专注于具有责任感代理的概率多代理系统中的战略推理问题。我们介绍了逻辑patl+r，这是概率交替时间时间逻辑的一种变体。 PATL+R的新颖性在于其因果责任的形式，为有责任感的多代理战略推理提供了一个框架。我们提出了一种合成联合策略的方法，该策略满足PATL+R中指定的结果，同时优化预期的因果责任和奖励的份额。这提供了代理商之间责任和奖励收益平衡分配的概念。为此，我们利用NASH均衡作为我们战略推理问题的解决方案概念，并通过减少参数模型检查并发随机多游戏游戏来证明如何计算责任感的NASH均衡策略。

### Foundations and Recent Trends in Multimodal Mobile Agents: A Survey 
[[arxiv](https://arxiv.org/abs/2411.02006)] [[cool](https://papers.cool/arxiv/2411.02006)] [[pdf](https://arxiv.org/pdf/2411.02006)]
> **Authors**: Biao Wu,Yanda Li,Meng Fang,Zirui Song,Zhiwei Zhang,Yunchao Wei,Ling Chen
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: 8 pages, 1 figure
- **标题**: 多模式移动代理的基础和最新趋势：一项调查
- **领域**: 人工智能
- **摘要**: 移动代理对于在复杂且动态的移动环境中自动化任务至关重要。随着基础模型的发展，对可以实时和过程多模式数据的代理的需求已经增长。这项调查提供了对移动代理技术的全面综述，重点是最新进步，以增强实时适应性和多模式相互作用。最新的评估基准已经得到了更好的开发，可以捕获移动任务的静态和交互式环境，从而更准确地评估了代理的性能。然后，我们将这些进步分类为两种主要方法：基于及时的方法，这些方法利用大型语言模型（LLMS）进行基于指令的任务执行，以及基于培训的方法，这些方法对特定于移动的应用程序进行了多模式调整多模型。此外，我们探讨了增强代理性能的互补技术。通过讨论关键挑战并概述未来的研究方向，该调查为推进移动代理技术提供了宝贵的见解。可以从https://github.com/aialt/awesome-mobile-agents获得综合资源列​​表

### HumanVLM: Foundation for Human-Scene Vision-Language Model 
[[arxiv](https://arxiv.org/abs/2411.03034)] [[cool](https://papers.cool/arxiv/2411.03034)] [[pdf](https://arxiv.org/pdf/2411.03034)]
> **Authors**: Dawei Dai,Xu Long,Li Yutang,Zhang Yuanhui,Shuyin Xia
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: 34 pages,11 figures
- **标题**: HumanVLM：人类视觉视觉模型的基础
- **领域**: 人工智能,多媒体
- **摘要**: 在各种社交应用中，人类娱乐视力语言任务越来越普遍，但是最近的进步主要依赖于专门针对单个任务量身定制的模型。新兴研究表明，大型视觉模型（VLM）可以提高各种下游视觉理解任务的性能。但是，通用域模型通常在专业领域的表现不佳。这项研究介绍了一个特定领域的大型视觉模型，即人类视觉语言模型（HumanVLM），旨在为人类场景视觉任务提供基础。具体而言，（1）我们创建了一个大规模的人类场景多模式图像文本数据集（Humancaption-10m），从互联网中采购，以促进特定于域的特定对齐方式； （2）为以人为中心的图像开发字幕方法，捕获人的面孔，身体和背景，并构建高质量的人类景观图像 - 文本数据集（Humancaptionhq，约311k对），其中包含尽可能多的详细信息。 （3）使用Humancaption-10m和HumancaptionHQ，我们训练人类VLM。在实验中，我们随后评估了跨多种下游任务的人类VLM，在该任务中，它显示了可比规模的多模式模型之间的卓越整体性能，尤其是在与人类相关的任务中尤为出色，并且在包括QWEN2VL和CANTGPT-4O在内的类似模型中表现出色。 HumanVLM与引入的数据一起将刺激人类环境领域的研究。

### Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications 
[[arxiv](https://arxiv.org/abs/2411.03782)] [[cool](https://papers.cool/arxiv/2411.03782)] [[pdf](https://arxiv.org/pdf/2411.03782)]
> **Authors**: Daan Schouten,Giulia Nicoletti,Bas Dille,Catherine Chia,Pierpaolo Vendittelli,Megan Schuurmans,Geert Litjens,Nadieh Khalili
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: 28 pages
- **标题**: 在医学中浏览多模式AI的景观：有关技术挑战和临床应用的范围审查
- **领域**: 人工智能,计算机与社会,机器学习
- **摘要**: 医疗保健的最新技术进步导致患者数据数量和多样性的前所未有。尽管人工智能（AI）模型在分析单个数据模式方面表现出了令人鼓舞的结果，但越来越多的认识是，整合多个互补数据源的模型，即所谓的多模式AI，可以增强临床决策。该范围审查研究了整个医学领域的深度学习多模式AI应用的景观，分析了2018年至2024年之间发表的432篇论文。我们提供了各种医疗纪律中多模式AI开发的广泛概述，研究了各种体系结构方法，融合策略和常见的应用领域。我们的分析表明，多模式AI模型的表现始终优于其单峰对应物，而AUC中平均提高了6.2个百分点。但是，一些挑战持续存在，包括跨部门协调，异质数据特征和不完整的数据集。我们严格评估开发多模式AI系统的技术和实践挑战，并讨论其临床实施的潜在策略，包括对临床决策的市售多模式AI模型的简要概述。此外，我们确定了推动多模式AI开发的关键因素，并提出建议以加速该领域的成熟。这篇综述使研究人员和临床医生对医学多模式AI的当前状态，挑战和未来方向有透彻的了解。

### GUI Agents with Foundation Models: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2411.04890)] [[cool](https://papers.cool/arxiv/2411.04890)] [[pdf](https://arxiv.org/pdf/2411.04890)]
> **Authors**: Shuai Wang,Weiwen Liu,Jingxuan Chen,Yuqi Zhou,Weinan Gan,Xingshan Zeng,Yuhan Che,Shuai Yu,Xinlong Hao,Kun Shao,Bin Wang,Chuhan Wu,Yasheng Wang,Ruiming Tang,Jianye Hao
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: No comments
- **标题**: 具有基础模型的GUI代理：一项全面调查
- **领域**: 人工智能,人机交互
- **摘要**: 基础模型，尤其是大型语言模型（LLM）和多模式大语言模型（MLLM）的最新进展，促进了能够执行复杂任务的智能代理的发展。通过利用（M）LLM处理和解释图形用户界面（GUI）的能力，这些代理可以自主执行用户指令，模拟类似人类的交互，例如单击和键入。这项调查合并了基于LLM的GUI代理的最新研究，突出了数据资源，框架和应用程序中的关键创新。我们首先审查代表性数据集和基准，然后概述一个广义的，统一的框架，该框架封装了先前研究的基本组成部分，并得到了详细的分类法的支持。此外，我们探索相关的商业应用。从现有工作中汲取见解，我们确定关键挑战并提出未来的研究方向。我们希望这项调查能够激发（M）基于LLM的GUI代理领域的进一步进步。

### MEANT: Multimodal Encoder for Antecedent Information 
[[arxiv](https://arxiv.org/abs/2411.06616)] [[cool](https://papers.cool/arxiv/2411.06616)] [[pdf](https://arxiv.org/pdf/2411.06616)]
> **Authors**: Benjamin Iyoya Irving,Annika Marie Schoene
> **First submission**: 2024-11-10
> **First announcement**: 2024-11-11
> **comment**: ef:In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), Miami, FL, pages 8579-8600
- **标题**: 意思是：多模式编码器以获取前文信息
- **领域**: 人工智能
- **摘要**: 股票市场提供了丰富的信息，这些信息可以跨模式分开，使其成为多模式评估的理想候选人。多模式数据在机器学习的开发中起着越来越重要的作用，并已证明对性能产生积极影响。但是信息可以做的不仅仅是跨模式的存在 - 它可以跨时间存在。我们应该如何参与由多种信息类型组成的时间数据？这项工作介绍了（i）含义模型，一种用于先行信息的多模式编码器，以及（ii）一个名为Tempstock的新数据集，该数据集由价格，推文和图形数据组成，其中包含来自S＆P 500 Index中所有公司的一百万个推文。我们发现这意味着将现有基准的性能提高了15％以上，并且文本信息对性能的影响远远超过了我们消融研究中有关时间依赖的任务的视觉信息。

### A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks 
[[arxiv](https://arxiv.org/abs/2411.06284)] [[cool](https://papers.cool/arxiv/2411.06284)] [[pdf](https://arxiv.org/pdf/2411.06284)]
> **Authors**: Chia Xin Liang,Pu Tian,Caitlyn Heqi Yin,Yao Yua,Wei An-Hou,Li Ming,Tianyang Wang,Ziqian Bi,Ming Liu
> **First submission**: 2024-11-09
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 视觉任务中多模式大语模型的全面调查和指南
- **领域**: 人工智能
- **摘要**: 该多模式大语言模型（MLLMS）的调查和应用指南探讨了MLLM的快速发展领域，研究其架构，应用程序，以及对AI和生成模型的影响。从基础概念开始，我们深入研究MLLM如何整合各种数据类型，包括文本，图像，视频和音频，以启用复杂的AI系统，以进行跨模式理解和生成。它涵盖了基本主题，例如培训方法，建筑组件和各个领域的实际应用，从视觉讲故事到增强的可访问性。通过详细的案例研究和技术分析，本文研究了突出的MLLM实现，同时解决了可伸缩性，鲁棒性和跨模式学习方面的关键挑战。最后，以道德考虑，负责任的AI发展和未来的方向进行了讨论，这种权威资源既提供了理论框架和实际见解。它对MLLM的开发和部署的机遇和挑战提供了平衡的观点，对于研究人员，从业人员和对自然语言处理和计算机愿景相交感兴趣的学生来说是非常有价值的。

### Multimodal Contrastive Learning of Urban Space Representations from POI Data 
[[arxiv](https://arxiv.org/abs/2411.06229)] [[cool](https://papers.cool/arxiv/2411.06229)] [[pdf](https://arxiv.org/pdf/2411.06229)]
> **Authors**: Xinglei Wang,Tao Cheng,Stephen Law,Zichao Zeng,Lu Yin,Junyuan Liu
> **First submission**: 2024-11-09
> **First announcement**: 2024-11-11
> **comment**: 19 pages, 5 figures, 7 tables
- **标题**: 从POI数据中对城市空间表示的多模式对比度学习
- **领域**: 人工智能
- **摘要**: 从利点数据（POI）数据学习城市空间表示的现有方法面临着几个局限性，包括地理描述，空间信息建模不足，POI语义属性的不足以及计算效率低下的问题。为了解决这些问题，我们提出了一种新颖的表示模型，该模型将Calliper（对比性语言 - 地点预训练）直接将连续的城市空间嵌入矢量表示中，以捕获城市环境的空间和语义分布。该模型利用多模式的对比学习目标，将位置嵌入与文本POI描述相结合，从而绕开了需要进行复杂的训练语料库结构和负面采样的需求。我们通过将其应用于英国伦敦的城市空间表示形式来验证Calliper的有效性，与先进方法相比，它显示出土地使用分类和社会经济地图任务的预测性能提高了5-15％。学习表示形式的可视化进一步说明了我们模型在以高准确性和精细分辨率捕获城市语义中的空间变化方面的优势。此外，Calliper可以减少训练时间，展示其效率和可扩展性。这项工作为可扩展的，语义丰富的城市空间表示学习提供了有希望的途径，可以支持地理空间基础模型的发展。实现代码可在https://github.com/xlwang233/calliper上获得。

### A Multimodal Adaptive Graph-based Intelligent Classification Model for Fake News 
[[arxiv](https://arxiv.org/abs/2411.06097)] [[cool](https://papers.cool/arxiv/2411.06097)] [[pdf](https://arxiv.org/pdf/2411.06097)]
> **Authors**: Jun-hao,Xu
> **First submission**: 2024-11-09
> **First announcement**: 2024-11-11
> **comment**: 8 pages
- **标题**: 假新闻的多模式自适应基于图形的智能分类模型
- **领域**: 人工智能
- **摘要**: 已经提出了许多研究来检测基于机器和/或深度学习的多模式的假新闻。但是，缺乏使用几何深度学习的基于图的结构的研究。为了应对这一挑战，我们介绍了用于假新闻检测的多模式自适应智能分类（恰当地称为魔术）。具体而言，将来自变压器的编码器表示用于文本矢量化，而Resnet50则用于图像。在通过SoftMax函数对多模式输入进行分类之前，使用自适应图注意网络构建了全面的信息交互图。 Magic在两个假新闻数据集中进行了训练和测试，即Fakeddit（英语）和多模式假新闻检测（中文），该模型的精度分别达到98.8 \％\％和86.3 \％。消融实验还揭示了在两个数据集中产生卓越性能的魔术。研究结果表明，基于图的深度学习自适应模型可有效地检测多模式假新闻，超过最新方法。

### LLM-PySC2: Starcraft II learning environment for Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.05348)] [[cool](https://papers.cool/arxiv/2411.05348)] [[pdf](https://arxiv.org/pdf/2411.05348)]
> **Authors**: Zongyuan Li,Yanan Ni,Runnan Qi,Lumin Jiang,Chang Lu,Xiaojie Xu,Xiangbei Liu,Pengfei Li,Yunzheng Guo,Zhe Ma,Xian Guo,Kuihua Huang,Xuebo Zhang
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: LLM-PYSC2：大语模型的Starcraft II学习环境
- **领域**: 人工智能
- **摘要**: 本文介绍了一个新的环境LLM-PYSC2（大语模型Starcraft II学习环境），该平台源自DeepMind的Starcraft II学习环境，该环境可开发基于大型语言模型（LLMS）的决策方法。这种环境是第一个提供完整的星际争霸II动作空间，多模式观察接口和结构化的游戏知识数据库，它们与各种LLM无缝连接​​，以促进基于LLMS的决策的研究。为了进一步支持多代理研究，我们开发了一个LLM协作框架，该框架支持多代理并发查询和多代理通信。在我们的实验中，LLM-PYSC2环境适应与Starcraft Multi-Agent Challenge（SMAC）任务组兼容，并提供了八个专注于宏观决策能力的新方案。我们在实验中评估了9个主流LLM，结果表明，LLMS必须进行决策，但提高推理能力是必要的，但是提高推理能力并不能直接导致更好的决策结果。我们的发现进一步表明了使大型模型能够通过参数培训或无火车学习技术在部署环境中自主学习的重要性。最终，我们希望LLM-PYSC2环境可以促进有关LLM的学习方法的研究，从而帮助基于LLM的方法更好地适应了任务方案。

### Multi-modal Iterative and Deep Fusion Frameworks for Enhanced Passive DOA Sensing via a Green Massive H2AD MIMO Receiver 
[[arxiv](https://arxiv.org/abs/2411.06927)] [[cool](https://papers.cool/arxiv/2411.06927)] [[pdf](https://arxiv.org/pdf/2411.06927)]
> **Authors**: Jiatong Bai,Minghao Chen,Wankai Tang,Yifan Li,Cunhua Pan,Yongpeng Wu,Feng Shu
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: No comments
- **标题**: 多模式迭代和深层融合框架，可通过绿色的H2AD MIMO接收器增强被动DOA感测
- **领域**: 人工智能
- **摘要**: 大多数现有的DOA估计方法都采用理想的源入射角，其噪声最小。此外，直接使用预估计的角度计算加权系数可能会导致性能损失。因此，提出了绿色的多模式（MM）融合DOA框架，以实现H $^2 $ AD数组的更实用，低成本和高时间效率的DOA估计。首先，提出了两种更有效的聚类方法，全局最大cos \ _ simarlity聚类（GMAXC）和全局最小距离聚类（GMIND），以从候选解决方案集中推断出更精确的真实解决方案。基于此，通过使用估计值将基于迭代的加权融合（IWF）方法引入了迭代更新的加权融合系数和真实解决方案类的聚类中心。特别是，由完全数字（FD）子阵列计算的粗糙DOA是初始集群中心。以上过程得出两种称为MM-IWF-GMAXC和MM-IWF-GMIND的方法。为了进一步提供更高的精确性DOA估计，提出了融合网络（FusionNet）来汇总推断的两部分的真实角度，从而产生两种称为MM-FusionNet-Gmaxcs和MM-FusionNet-Gmind的有效方法。模拟结果表明，提出的四种方法可以达到理想的DOA性能和CRLB。同时，与MM-IWF-GMAXC和MM-IWF-GMIND相比，提出的MM-FusionNet-GMAXC和MM-FusionNet-Gmind表现出卓越的DOA性能，尤其是在极低的SNR范围内。

### Generative midtended cognition and Artificial Intelligence. Thinging with thinging things 
[[arxiv](https://arxiv.org/abs/2411.06812)] [[cool](https://papers.cool/arxiv/2411.06812)] [[pdf](https://arxiv.org/pdf/2411.06812)]
> **Authors**: Xabier E. Barandiaran,Marta Pérez-Verdugo
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: 16 pages, 2 figures. Submitted to "Synthese" Journal, accepted
- **标题**: 生成的中端认知和人工智能。与事物的事情
- **领域**: 人工智能,计算机与社会,机器学习
- **摘要**: 本文介绍了``生成的中态认知''的概念，并探讨了生成AI与人类认知的整合。 “生成”一词反映了AI迭代产生结构化输出的能力，而“中间”捕获了该过程的潜在混合（人类）性质。它坐落在预期创造的传统概念之间，从内部被理解，并扩展过程，这些过程将外部生物学过程带入创作过程中。我们检查了当前的生成技术（基于大型语言模型（例如Chatgpt）典型的多模式变压器体系结构），以解释它们如何将人类认知代理转变为超出扩展认知的标准理论可以捕获的标准理论。我们建议，人类和生成技术之间耦合的典型认知活性类型比社会认知更接近（但不是等效），而不是与经典的扩展​​认知范式相比。然而，它值得特定的治疗方法。我们提供了生成中期认知的明确定义，在这种认知中，我们将AI系统视为代理商有意创作过程的构成。此外，我们区分了生成杂种创造力的两个维度：1。宽度：捕获生成过程的上下文的灵敏度（从单个字母到整个历史数据和周围数据），2。深度：捕获该过程中涉及的迭代环节的粒度。生成的中端认知位于认知的对话形式之间的中间深度，在这种对话形式中，交换了完整的话语或创造性单元，以及微观认知（例如神经）的子人物过程。最后，本文讨论了采用广泛生成AI的潜在风险和好处，包括真实性，生成力量不对称的挑战以及创造性的增强或萎缩。

### Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data 
[[arxiv](https://arxiv.org/abs/2411.06735)] [[cool](https://papers.cool/arxiv/2411.06735)] [[pdf](https://arxiv.org/pdf/2411.06735)]
> **Authors**: Kai Kim,Howard Tsai,Rajat Sen,Abhimanyu Das,Zihao Zhou,Abhishek Tanpure,Mathew Luo,Rose Yu
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: 21 pages, 4 tables, 2 figures
- **标题**: 多模式预报员：共同预测时间序列和文本数据
- **领域**: 人工智能
- **摘要**: 当前的预测方法在很大程度上是单峰的，并且忽略了由于缺乏经过精心策划的多模式基准数据集而经常伴随时间序列的丰富文本数据。在这项工作中，我们开发了TimeText语料库（TTC），这是一个经过精心策划的，经过时期的文本和时间数据集，用于多模式预测。我们的数据集由数字序列组成，文本与时间戳保持一致，并包括来自两个不同领域的数据：气候科学和医疗保健。我们的数据是对可用多模式数据集的罕见选择的重要贡献。我们还提出了混合多模式预测器（Hybrid-MMF），这是一种多模式LLM，共同预测使用共享嵌入的文本和时间序列数据。但是，与我们的期望相反，我们的混合MMF模型在实验中并不能胜过现有的基线。这种负面结果突出了多模式预测固有的挑战。我们的代码和数据可在https://github.com/rose-stl-lab/multimodal_预测上获得。

### Leveraging Multimodal Models for Enhanced Neuroimaging Diagnostics in Alzheimer's Disease 
[[arxiv](https://arxiv.org/abs/2411.07871)] [[cool](https://papers.cool/arxiv/2411.07871)] [[pdf](https://arxiv.org/pdf/2411.07871)]
> **Authors**: Francesco Chiumento,Mingming Liu
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: The paper has been accepted by the conference: "2024 International Conference on Big Data (IEEE Big Data 2024)"
- **标题**: 利用多模型模型来增强阿尔茨海默氏病的神经影像学诊断。
- **领域**: 人工智能,图像和视频处理
- **摘要**: 大语言模型（LLM）和视觉语言模型（VLM）的快速进步在医学诊断方面表现出很大的潜力，尤其是在放射学方面，X射线（例如X射线）与人类生成的诊断报告配对。然而，由于缺乏可用于模型微调的模型，由于缺乏全面的诊断报告，因此神经影像领域中存在一个重大的研究差距，特别是对于阿尔茨海默氏病等疾病。本文通过使用GPT-4O-MINI在OASIS-4数据集的结构化数据上生成合成诊断报告来解决这一差距，该数据包括663名患者。然后，使用合成报告作为培训和验证的基础真理，然后我们直接从数据集中的图像中直接生成了神经学报告，从而利用了预先训练的生物胶囊和T5模型。我们提出的方法的BLEU-4得分为0.1827，胭脂-L得分为0.3719，流星得分为0.4163，揭示了其在产生临床相关和准确的诊断报告中的潜力。

### Is Cognition consistent with Perception? Assessing and Mitigating Multimodal Knowledge Conflicts in Document Understanding 
[[arxiv](https://arxiv.org/abs/2411.07722)] [[cool](https://papers.cool/arxiv/2411.07722)] [[pdf](https://arxiv.org/pdf/2411.07722)]
> **Authors**: Zirui Shao,Chuwei Luo,Zhaoqing Zhu,Hangdi Xing,Zhi Yu,Qi Zheng,Jiajun Bu
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: Preprint
- **标题**: 认知与感知一致吗？评估和缓解文档理解中的多模式知识冲突
- **领域**: 人工智能
- **摘要**: 多模式的大语言模型（MLLM）在文档理解方面表现出了令人印象深刻的能力，近年来，迅速发展的研究领域，工业需求巨大。作为一项多模式的任务，文档理解要求模型具有感知和认知能力。但是，当前的MLLM经常在感知和认知之间面临冲突。以文档VQA任务（认知）为例，MLLM可能会生成不匹配其OCR（感知）标识的相应视觉内容的答案。这场冲突表明，MLLM可能很难在其“看到”的信息与“理解”的信息之间建立固有的联系。这种冲突挑战了直觉观念，即认知与感知一致，阻碍了MLLM的性能和解释性。在本文中，我们将认知和感知之间的冲突定义为认知和感知（C＆P）知识冲突，一种多模式知识冲突的一种形式，并系统地评估它们，重点是文档理解。我们的分析表明，即使是领先的MLLM GPT-4O，也只能达到68.6％的C＆P一致性。为了减轻C＆P知识冲突，我们提出了一种称为多模式知识一致性微调的新颖方法。该方法首先确保特定于任务的一致性，然后连接认知和感知知识。我们的方法大大降低了所有测试的MLLM的C＆P知识冲突，并在大多数情况下都在认知和感知任务中提高了它们的表现。

### Artificial Intelligence in Pediatric Echocardiography: Exploring Challenges, Opportunities, and Clinical Applications with Explainable AI and Federated Learning 
[[arxiv](https://arxiv.org/abs/2411.10255)] [[cool](https://papers.cool/arxiv/2411.10255)] [[pdf](https://arxiv.org/pdf/2411.10255)]
> **Authors**: Mohammed Yaseen Jabarulla,Theodor Uden,Thomas Jack,Philipp Beerbaum,Steffen Oeltze-Jafra
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: This article is planned for submission to Frontiers Journal
- **标题**: 小儿超声心动图中的人工智能：通过可解释的AI和联合学习探索挑战，机会和临床应用
- **领域**: 人工智能
- **摘要**: 小儿心脏病表现出各种各样的先天性疾病。更复杂的先天性畸形需要差异化和多模式的决策过程，通常包括超声心动图作为中心成像方法。人工智能（AI）通过促进小儿超声心动图数据的自动解释为临床医生提供了巨大的希望。但是，将AI技术适应小儿超声心动图分析的挑战，例如有限的公共数据可用性，数据隐私和AI模型透明度。最近，研究人员专注于破坏性技术，例如联合学习（FL）和可解释的AI（XAI），以改善自动诊断和决策支持工作流程。这项研究对小儿超声心动图中AI的局限性和机会进行了全面概述，强调了XAI和FL的协同工作流程和作用，确定了研究差距，并探索了潜在的未来发展。此外，三个相关的临床用例证明了XAI和FL的功能，重点是（i）观察识别，（ii）疾病分类，（iii）对心脏结构的分割以及（iv）心脏功能的定量评估。

### Artificial Scientific Discovery 
[[arxiv](https://arxiv.org/abs/2411.11672)] [[cool](https://papers.cool/arxiv/2411.11672)] [[pdf](https://arxiv.org/pdf/2411.11672)]
> **Authors**: Antonio Norelli
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: PhD thesis, 123 pages
- **标题**: 人工科学发现
- **领域**: 人工智能,机器学习
- **摘要**: 该论文植根于过去十年中深度学习的爆炸爆炸，跨越了从alphago到chatgpt，以经验研究实现人工科学家愿景所需的基本概念：一种具有自主性生成原始研究并有助于人类知识扩展的机器。调查始于{\ sc olivaw}，这是一种类似于零的代理，从头开始发现奥赛罗知识，但无法传达它。这种实现导致了解释性学习（EL）框架的发展，这是科学家试图向同龄人解释新现象时面临的问题的形式化。有效的EL处方使我们能够破解Zendo，这是一个模拟科学努力的棋盘游戏。这一成功带来了一个基本的见解：人工科学家必须对用于解释其发现的语言进行自身的解释。然后，这种观点使我们将现代多式联模型视为口译员，并设计了一种新的方式来构建可解释和具有成本效益的剪辑型模型：通过使用少量多模式数据耦合两个单峰模型，没有进一步的培训。最后，我们讨论了哪些chatgpt及其兄弟姐妹成为人工科学家，并介绍了Odeen，这是解释解释解释的基准，而解释LLMS不再是随机的机会，而而是由人类充分解决。

### Neurosymbolic Graph Enrichment for Grounded World Models 
[[arxiv](https://arxiv.org/abs/2411.12671)] [[cool](https://papers.cool/arxiv/2411.12671)] [[pdf](https://arxiv.org/pdf/2411.12671)]
> **Authors**: Stefano De Giorgis,Aldo Gangemi,Alessandro Russo
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 扎根世界模型的神经符号图富集
- **领域**: 人工智能,计算语言学,新兴技术
- **摘要**: 能够理解和推理复杂现实世界情景的人工智能系统的发展是一个重大挑战。在这项工作中，我们提出了一种新颖的方法，以增强和利用LLM的反应能力来解决复杂问题并深刻解释上下文现实世界的含义。我们介绍了一种方法和一种工具，用于创建一种多模式，知识增强的形式表示含义，将大语言模型的优势与结构化语义表示结合在一起。我们的方法从图像输入开始，利用最先进的大语言模型来生成自然语言描述。然后将此描述转换为抽象含义表示（AMR）图，该图被形式化并具有逻辑设计模式，并从语言和事实知识基础中得出的分层语义。然后将所得的图反馈到LLM中，以通过复杂的启发式学习激活的隐式知识扩展，包括语义含义，道德价值，体现认知和隐喻表示。通过弥合非结构化语言模型和正式语义结构之间的差距，我们的方法为解决自然语言理解和推理中的复杂问题开辟了新的途径。

### AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations 
[[arxiv](https://arxiv.org/abs/2411.13451)] [[cool](https://papers.cool/arxiv/2411.13451)] [[pdf](https://arxiv.org/pdf/2411.13451)]
> **Authors**: Gaurav Verma,Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Tucker Balch,Manuela Veloso
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: 18 pages, 3 figures, an abridged version to appear in NeurIPS 2024 AFM Workshop
- **标题**: 自适应：从人类示范中调整多模式网络代理商很少学习
- **领域**: 人工智能,计算语言学,机器学习
- **摘要**: 由多模式大语言模型（MLLM）提供支持的最新的多模式Web代理可以通过处理用户指令并与图形用户界面（GUIS）进行交互来自主执行许多Web任务。当前构建Web代理的策略依赖于（i）基础MLLM的普遍性及其通过提示进行的可管制性，以及（ii）在与Web相关的任务上对MLLM的大规模微调。但是，Web代理仍然很难自动在看不见的网站和域上自动化任务，从而将其适用性限制在企业特定和专有平台上。除了大规模的预训练和微调的概括外，我们还建议使用人类示范的建筑物来进行几次适应性。我们介绍了适应性的框架，该框架可以使专有和开放量的多模式网络代理使用很少的人类演示（最多2）适应新的网站和域。我们在两个流行的基准测试（Mind2Web＆VisualWebarena）上进行的实验表明，在非改装的州立型号的模型中，使用中下文演示（对于专有模型）或元适应演示（用于元学习的开放式型号）将任务成功率提高了3.36％至7.21％，相对增长了21.03％。此外，我们的其他分析（a）显示了多模式演示对仅文本的有效性，（b）揭示了元学习过程中不同数据选择策略对代理概括的影响，并且（c）证明了少数射击示例数量对网络代理商成功率的影响。总体而言，我们的结果解锁了一个互补的轴，用于在大规模的预训练和微调之外开发广泛适用的多模式Web代理，从而强调了很少的适应性。

### LLM-based Multi-Agent Systems: Techniques and Business Perspectives 
[[arxiv](https://arxiv.org/abs/2411.14033)] [[cool](https://papers.cool/arxiv/2411.14033)] [[pdf](https://arxiv.org/pdf/2411.14033)]
> **Authors**: Yingxuan Yang,Qiuying Peng,Jun Wang,Ying Wen,Weinan Zhang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 基于LLM的多代理系统：技术和业务观点
- **领域**: 人工智能
- **摘要**: 在（多模式）大语言模型的时代，大多数运行过程都可以使用LLM代理重新校正和再现。 LLM代理可以感知，控制和从环境中获得反馈，以便以自主的方式完成给定的任务。除了环境互动属性外，LLM代理还可以调用各种外部工具以简化任务完成过程。这些工具可以被视为具有LLMS参数中不存在的私有或实时知识的预定义操作过程。作为开发的自然趋势，呼叫的工具正在成为自主代理，因此，完整的智能系统原来是基于LLM的多代理系统（LAMAS）。与以前的单个LLM代理系统相比，LAMA具有i）动态任务分解和有机专业化的优点，ii）更高的系统更改的灵活性，iii）为每个参与实体保留的专有数据，iv）iv）每个实体的可行性。本文讨论了喇嘛的技术和商业景观。为了支持喇嘛的生态系统，我们考虑了技术要求，数据隐私和业务激励措施的此类喇嘛协议的初步版本。因此，喇嘛将是在不久的将来实现人造集体智能的实用解决方案。

### Creating Scalable AGI: the Open General Intelligence Framework 
[[arxiv](https://arxiv.org/abs/2411.15832)] [[cool](https://papers.cool/arxiv/2411.15832)] [[pdf](https://arxiv.org/pdf/2411.15832)]
> **Authors**: Daniel A. Dollinger,Michael Singleton
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: 8 pages, IEEE SYSCON 2025 Submission
- **标题**: None
- **领域**: 人工智能
- **摘要**: 人工智能（AI）的最新进步，尤其是大型语言模型（LLMS），导致了狭窄任务的重大进展，例如图像分类，语言翻译，编码和写作。但是，这些模型由于其孤立的体系结构而面临的可靠性和可伸缩性限制，这些模型旨在一次仅处理一种数据模式（数据类型）。这种单态方法阻碍了他们整合现实世界中挑战和解决问题的任务所需的复杂数据点的能力，例如医学诊断，质量保证，设备故障排除和财务决策。应对这些现实世界中的挑战需要更有能力的人工智能（AGI）系统。我们的主要贡献是开放通用情报（OGI）框架的开发，这是一种新型的系统体系结构，可作为AGI的宏观设计参考。 OGI框架采用模块化方法来设计智能系统的设计，这是基于在多个可以作为单个系统无缝操作的专用模块中进行认知的前提。 OGI使用动态处理系统和织物互连来集成这些模块，从而实现实时适应性，多模式集成和可扩展处理。 OGI框架由三个关键组成部分组成：（1）指导操作设计和处理的总体宏设计指导，（2）一个动态处理系统，该系统控制路由，主要目标，指令和权重，以及（3）框架区域，一组专门的模块，可以凝聚形成统一的认知系统。通过将人类认知的已知原理纳入AI系统，OGI框架旨在克服当今智能系统中观察到的挑战，为更多的整体和背景意识到的解决问题的能力铺平了道路。

### Decoding Urban Industrial Complexity: Enhancing Knowledge-Driven Insights via IndustryScopeGPT 
[[arxiv](https://arxiv.org/abs/2411.15758)] [[cool](https://papers.cool/arxiv/2411.15758)] [[pdf](https://arxiv.org/pdf/2411.15758)]
> **Authors**: Siqi Wang,Chao Liang,Yunfan Gao,Yang Liu,Jing Li,Haofen Wang
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: 9 pages, 6 figures, the 32nd ACM International Conference on Multimedia
- **标题**: 解码城市工业复杂性：通过IndustrysCopegpt增强知识驱动的见解
- **领域**: 人工智能,计算机与社会,社交和信息网络
- **摘要**: 工业园对于城市经济增长至关重要。然而，他们的发展经常遇到挑战，这是由于工业需求与城市服务之间的不平衡，强调了战略规划和运营的需求。本文介绍了Industryscopekg，这是一种开创性的大规模多模式，多级工业公园知识图，该图整合了各种城市数据，包括街头景观，公司，社会经济和地理空间信息，捕获工业园区的复杂关系和语义。除此之外，我们介绍了IndustryScopegpt框架，该框架利用大型语言模型（LLMS）和蒙特卡洛树搜索来增强工业公园计划和运营（IPPO）的工具提升的推理和决策。我们的工作大大改善了现场建议和功能计划，证明了将LLM与结构化数据集相结合以推进工业园区管理的潜力。这种方法为智能IPPO研究树立了新的基准，并为推动城市工业发展的强大基础奠定了基础。数据集和相关代码可在https://github.com/tongji-kgllm/industryscope上找到。

### mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA 
[[arxiv](https://arxiv.org/abs/2411.15041)] [[cool](https://papers.cool/arxiv/2411.15041)] [[pdf](https://arxiv.org/pdf/2411.15041)]
> **Authors**: Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Zhongang Qi,Chunfeng Yuan,Bing Li,Junfu Pu,Yuxuan Zhao,Zehua Xie,Jin Ma,Ying Shan,Weiming Hu
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: MR $^2 $ AG：基于知识的VQA的多模式检索 - 反射式增长一代
- **领域**: 人工智能,计算语言学
- **摘要**: 高级多模式大型语言模型（MLLM）由于其有限且冷冻的知识范围而与最近基于知识的VQA任务（例如Infoseek和百科全书-VQA）斗争，通常会导致歧义和不准确的响应。因此，自然引入了多模式检索型生成（MRAG），以为MLLM提供全面和最新的知识，从而有效地扩大了知识范围。但是，当前的MRAG方法具有固有的缺点，包括：1）即使不需要外部知识，也要进行检索。 2）缺乏支持查询的证据的识别。 3）由于其他信息过滤模块或规则，增加了模型复杂性。要解决这些缺点，我们提出了一个新颖的广义框架，称为\ textbf {m} ult-imodal \ textbf {r} etrieval- \ textbf {r}易于实现的反射操作，防止高模型复杂性。在MR $^2 $ ag中，检索反射旨在区分不同的用户查询并避免冗余呼叫，并引入了相关性反射，以指导MLLM找到获取内容的有益证据并相应地生成答案。此外，MR $^2 $ ag可以集成到任何训练有素的MLLM中，并在拟议的MR $^2 $ ag指令数据集（MR $ $^2 $ ag-it）上进行有效的微调。 MR $^2 $ ag在Infoseek和Infoseek和百科全书VQA上的最先进的MLLM（例如GPT-4V/O）和基于抹布的MLLM的表现明显胜过，同时保持了广泛依赖视觉依赖性任务的基本MLLM的特殊功能。

### Human Motion Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2411.16805)] [[cool](https://papers.cool/arxiv/2411.16805)] [[pdf](https://arxiv.org/pdf/2411.16805)]
> **Authors**: Lei Li,Sen Jia,Wang Jianhao,Zhongyu Jiang,Feng Zhou,Ju Dai,Tianfang Zhang,Wu Zongkai,Jenq-Neng Hwang
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: Accepted by CVPR 2025
- **标题**: 人类运动教学调整
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 本文介绍了Llamo（大语言和人类运动助手），这是人类运动指导调整的多模式框架。与将非语言输入（例如视频或运动序列）转换为语言令牌的常规指令调查方法相反，Llamo保留了其本机形式的运动以进行教学调整。该方法保留了特定于运动的细节，这些细节通常会减少令牌化，从而提高了模型解释复杂人类行为的能力。通过将视频和运动数据与文本输入一起处理，Llamo可以进行灵活的以人为本的分析。跨高复杂性领域（包括人类行为和专业活动）的实验评估表明，Llamo有效地捕获了特定于领域的知识，增强了运动密集型情景的理解和预测。我们希望Llamo为将来的多模式AI系统提供基础，从体育分析到行为预测，具有广泛的应用。我们的代码和模型可在项目网站上找到：https：//github.com/ilglj/llamo。

### Cross-modal Information Flow in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.18620)] [[cool](https://papers.cool/arxiv/2411.18620)] [[pdf](https://arxiv.org/pdf/2411.18620)]
> **Authors**: Zhi Zhang,Srishti Yadav,Fengze Han,Ekaterina Shutova
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: 多模式大语言模型中的跨模式信息流
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 自动回归多模式大型语言模型（MLLM）的最新进展已证明了视觉任务的有希望的进展。尽管存在各种研究，这些研究研究了大语言模型中语言信息的处理，但目前对MLLM的内部工作机制以及语言和视觉信息在这些模型中的相互作用几乎知之甚少。在这项研究中，我们旨在通过检查MLLM中不同方式（语言和视觉）之间的信息流来填补这一空白，重点关注视觉问题的回答。具体而言，给定图像问题对作为输入，我们研究了模型中的位置以及如何合并视觉和语言信息以生成最终预测。通过LLAVA系列的一系列模型进行实验，我们发现两种模式的整合过程中有两个不同的阶段。在下层中，该模型首先将整个图像的更通用的视觉特征传输到（语言）问题令牌的表示形式中。在中间层中，它再次将有关与问题相关的特定对象的视觉信息传输到问题的各个令牌位置。最后，在较高的层中，最终预测的输入序列的最后一个位置将所得的多模式表示。总体而言，我们的发现为MLLM中图像和语言处理的空间和功能方面提供了新的全面观点，从而促进了对多模式信息本地化和编辑的未来研究。

### Large Language Model-Brained GUI Agents: A Survey 
[[arxiv](https://arxiv.org/abs/2411.18279)] [[cool](https://papers.cool/arxiv/2411.18279)] [[pdf](https://arxiv.org/pdf/2411.18279)]
> **Authors**: Chaoyun Zhang,Shilin He,Jiaxu Qian,Bowen Li,Liqun Li,Si Qin,Yu Kang,Minghua Ma,Guyue Liu,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Qi Zhang
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a searchable webpage is available at https://aka.ms/gui-agent for easier access and exploration
- **标题**: 大型语言模型的GUI代理：调查
- **领域**: 人工智能,计算语言学,人机交互
- **摘要**: Guis长期以来一直是人类计算机互动的核心，它提供了一种直观且视觉驱动的方式来访问和与数字系统进行交互。 LLM的出现，尤其是多模型模型，已经迎来了GUI自动化的新时代。他们在自然语言理解，代码生成和视觉处理方面表现出了出色的功能。这为新一代LLM脑的GUI代理铺平了道路，能够解释复杂的GUI元素并根据自然语言指示自主执行动作。这些代理代表范式偏移，使用户能够通过简单的对话命令执行复杂的多步任务。它们的应用程序跨越Web导航，移动应用交互和桌面自动化，提供了一种变革性的用户体验，彻底改变了个人与软件互动的方式。这个新兴领域正在迅速发展，在研究和行业中取得了重大进展。为了提供对这一趋势的结构性理解，本文介绍了对LLM脑的GUI代理的全面调查，探讨了其历史发展，核心组成部分和先进的技术。我们解决了研究问题，例如现有的GUI代理框架，用于培训专业GUI代理的数据的收集和利用，针对GUI任务量身定制的大型行动模型的开发以及评估指标和基准评估其有效性所需的评估指标和基准。此外，我们检查了这些代理支持的新兴应用。通过详细的分析，这项调查确定了关键的研究差距，并概述了该领域未来进步的路线图。通过巩固基础知识和最先进的发展，这项工作旨在指导研究人员和从业人员克服挑战并释放LLM脑的GUI代理人的全部潜力。

### DuMapper: Towards Automatic Verification of Large-Scale POIs with Street Views at Baidu Maps 
[[arxiv](https://arxiv.org/abs/2411.18073)] [[cool](https://papers.cool/arxiv/2411.18073)] [[pdf](https://arxiv.org/pdf/2411.18073)]
> **Authors**: Miao Fan,Jizhou Huang,Haifeng Wang
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: Dumapper：在Baidu Maps上自动验证大规模pois带有街景
- **领域**: 人工智能,信息检索
- **摘要**: 随着移动设备的普及，Web映射服务已成为我们日常生活中必不可少的工具。为了提供用户满意的服务（例如位置搜索），感兴趣的点（POI）数据库是基本基础架构，因为它可以归类与数十亿个地理位置的多模式信息，这些信息与人们的生活密切相关，例如商店或银行。因此，验证大型POI数据库的正确性至关重要。为了实现这一目标，许多工业公司采用了自愿的地理信息（VGI）平台，使成千上万的众筹者和专家映射者能够无缝验证Pois；但是，为此，他们每年必须花费数百万美元。为了节省巨大的劳动力成本，我们设计了Dumapper，这是一种自动系统，用于使用Baidu Maps的多模式街景数据进行大规模POI验证。 Dumapper拍摄了招牌图像和现实世界中的坐标，作为生成低维矢量的输入，ANN算法可以利用它来利用它，以通过数据库中的数十亿个存档POIS进行更准确的搜索，以在毫秒内进行验证。它可以将POI验证的吞吐量显着增加$ 50 $倍。自\ dmplonline以来，Dumapper已经被部署在生产中，该林线大大提高了Baidu Maps的POI验证的生产率和效率。截至2021年12月31日，它已在3。5年内颁布了超过4.05亿美元的POI验证迭代，这大约为800美元的高性能专家映射者。

## 计算复杂度(cs.CC:Computational Complexity)

该领域共有 1 篇论文

### Social Distancing Induced Coronavirus Optimization Algorithm (COVO): Application to Multimodal Function Optimization and Noise Removal 
[[arxiv](https://arxiv.org/abs/2411.17282)] [[cool](https://papers.cool/arxiv/2411.17282)] [[pdf](https://arxiv.org/pdf/2411.17282)]
> **Authors**: Om Ramakisan Varma,Mala Kalra
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 社会远处引起的冠状病毒优化算法（COVO）：应用多模式功能优化和降噪
- **领域**: 计算复杂度,人工智能
- **摘要**: 元启发式优化技术对处理复杂优化问题的意识更加认识。在过去的几年中，已经开发了许多受自然现象启发的优化技术。最近，新的Covid-19的传播意味着对公共卫生系统的负担造成了几次死亡。疫苗接种，口罩和社会距离是最大程度地减少致命Covid-19病毒传播所采取的主要步骤。考虑到与冠状病毒流行作斗争的社会距离，在这项工作中提出了一种新型的生物启发的元启发式优化模型，并被称为社会疏远诱导的冠状病毒优化算法（COVO）。冠状病毒传播的速度确实可以通过保持社会距离来减慢。 13个基准函数用于评估COVO性能的离散，连续和复杂问题，并将COVO模型性能与其他众所周知的优化算法进行比较。 COVO优化的主要动机是通过解决更快的收敛速度来解决各种应用程序的全局解决方案。最后，经过验证的结果描述了所提出的COVO优化具有合理且可接受的性能。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 56 篇论文

### 'No' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue 
[[arxiv](https://arxiv.org/abs/2410.23883)] [[cool](https://papers.cool/arxiv/2410.23883)] [[pdf](https://arxiv.org/pdf/2410.23883)]
> **Authors**: Rena Gao,Xuetong Wu,Siwen Luo,Caren Han,Feng Liu
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: 16 pages, 5 figures
- **标题**: “没有”事项：多模式长的对话中的分布式检测
- **领域**: 计算语言学,人工智能,机器学习,多媒体
- **摘要**: 多模式上下文中的分布（OOD）检测对于确定与不同模式的联合输入的偏差至关重要，尤其是在开放域对话系统或现实生活中对话交互之类的应用中。本文旨在通过有效检测OOD对话和图像来改善涉及多轮长对话的用户体验。我们介绍了一个名为“对话”图像对齐和增强框架（DIAEF）的新颖评分框架，将视觉语言模型与新颖的分数集成在一起，这些分数在两个关键方案中检测到OOD（1）对话对和图像输入对之间的不匹配以及（2）输入对与以前看不见的标签。我们的实验结果以各种基准得出，表明整合图像和多轮对话OOD检测比以前看不见的标签比独立使用任何一种模式更有效。在存在不匹配的对的情况下，我们提出的分数有效地确定了这些不匹配，并在长时间对话中表现出强大的鲁棒性。这种方法增强了域感知，自适应对话剂，并为未来的研究建立了基准。

### Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.23861)] [[cool](https://papers.cool/arxiv/2410.23861)] [[pdf](https://arxiv.org/pdf/2410.23861)]
> **Authors**: Hao Yang,Lizhen Qu,Ehsan Shareghi,Gholamreza Haffari
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: 音频是阿喀琉斯的鞋跟：红色的音频大型多模型
- **领域**: 计算语言学,多媒体,声音,音频和语音处理
- **摘要**: 大型多模型模型（LMM）已经证明了通过将大语言模型（LLMS）和模态编码器结合到将多模式信息（视觉和听觉）与文本相结合的能力。但是，此类模型引起了新的安全挑战，即在文本上与安全模型是否也表现出一致的多模式输入的保障措施。尽管最近对视觉LMM的安全性调整研究，但音频LMM的安全性仍然不足。在这项工作中，我们在三个环境下全面地对红色团队进行了五个高级音频LMM的安全性：（i）有害问题和文本格式，（ii）有害问题，伴随着分散非语音音频的文本格式，以及（iii）语音特定的越狱。我们在这些环境下的结果表明，开源音频LMM在有害音频问题上平均攻击成功率为69.14％，并且在因非语音音频噪声而分心时表现出安全漏洞。我们对Gemini-1.5-Pro的言语特定越狱在有害查询基准方面取得了70.67％的攻击成功率。我们提供有关可能导致这些报告的安全 - 分离的洞察力。警告：本文包含令人反感的例子。

### UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.01703)] [[cool](https://papers.cool/arxiv/2411.01703)] [[pdf](https://arxiv.org/pdf/2411.01703)]
> **Authors**: Sejoon Oh,Yiqiao Jin,Megha Sharma,Donghyun Kim,Eric Ma,Gaurav Verma,Srijan Kumar
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-04
> **comment**: 14 pages
- **标题**: Uniguard：迈向通用安全护栏，以实现对多模式模型的越狱攻击
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 多模式的大语言模型（MLLM）彻底改变了视力语言的理解，但仍然容易受到多模式越狱攻击的影响，在这些袭击中，对抗性输入是精心制作的，以引起有害或不适当的反应。我们提出了一种新型的多式联压护栏Uniguard，共同考虑了单峰和跨模式有害信号。 Uniguard训练多模式护栏，以最大程度地减少在有毒语料库中产生有害反应的可能性。在推断最低的计算成本时，可以将护栏无缝应用于任何输入提示。广泛的实验表明，跨多种模式，攻击策略和多种最先进的MLLM，包括Llava，Gemini Pro，GPT-4O，GPT-4O，Minigpt-4和TendentBlip的多种概述。值得注意的是，这种强大的防御机制保持了模型的整体视觉理解能力。

### Can Multimodal Large Language Model Think Analogically? 
[[arxiv](https://arxiv.org/abs/2411.01307)] [[cool](https://papers.cool/arxiv/2411.01307)] [[pdf](https://arxiv.org/pdf/2411.01307)]
> **Authors**: Diandian Guo,Cong Cao,Fangfang Yuan,Dakui Wang,Wei Ma,Yanbing Liu,Jianhui Fu
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 多模式大语模型可以类似地思考吗？
- **领域**: 计算语言学
- **摘要**: 类比推理，尤其是在多模式背景下，是人类感知和创造力的基础。多模式大语言模型（MLLM）最近由于其新兴能力引发了广泛的讨论。在本文中，我们深入研究了MLLM的多模式类似推理能力。具体来说，我们探索了两个方面：\ textit {mllm作为解释器}和\ textit {mllm作为predictor}。在\ textit {mllm作为解释器}中，我们主要关注MLLM是否可以深入理解多模式的类似推理问题。我们提出了一个统一的提示模板和一种利用MLLM的理解能力增强现有模型的方法。在\ textit {mllm作为预测变量}中，我们旨在确定MLLM是否可以直接解决多模式类似推理问题。实验表明，我们的方法在流行数据集上的现有方法优于现有方法，为MLLM的类比推理能力提供了初步证据。

### Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An Evaluation Using TORGO 
[[arxiv](https://arxiv.org/abs/2411.00980)] [[cool](https://papers.cool/arxiv/2411.00980)] [[pdf](https://arxiv.org/pdf/2411.00980)]
> **Authors**: Macarious Hui,Jinda Zhang,Aanchan Mohan
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 在电子卫生设置中增强违规扬声器的AAC软件：使用TORGO的评估
- **领域**: 计算语言学,人机交互,声音,音频和语音处理
- **摘要**: 患有脑瘫（CP）和肌萎缩性侧索硬化症（ALS）的个体经常面临挑战，导致构音障碍并导致非典型语音模式。在医疗保健环境中，沟通分解降低了护理质量。在建立增强和替代通信（AAC）工具以实现流体通信的同时，我们发现最先进的（SOTA）自动语音识别（ASR）技术（如Whisper和Wav2Vec2.0）在很大程度上是由于缺乏训练数据而边缘化非典型扬声器。我们的工作旨在利用SOTA ASR，然后是域特异性错误校正。在Torgo数据集上，经常评估英语违规ASR性能。及时越过的是该数据集的一个众所周知的问题，在该数据集中，培训和测试扬声器之间的短语重叠。我们的工作提出了一种算法，以打破此及时的重叠。减少及时越来越快的速度后，SOTA ASR模型的结果为轻度和严重构音障碍的扬声器产生极高的单词错误率。此外，为了改善ASR，我们的工作着眼于基于N-Gram语言模型和大型语言模型（LLM）的多模式生成误差校正算法（例如Whispering-lalla）在第二次通过ASR中的影响。我们的工作强调了为了改善非典型扬声器的ASR，还需要做更多的工作，以实现面对面和电子卫生环境的公平医疗保健访问。

### Survey of Cultural Awareness in Language Models: Text and Beyond 
[[arxiv](https://arxiv.org/abs/2411.00860)] [[cool](https://papers.cool/arxiv/2411.00860)] [[pdf](https://arxiv.org/pdf/2411.00860)]
> **Authors**: Siddhesh Pawar,Junyeong Park,Jiho Jin,Arnav Arora,Junho Myung,Srishti Yadav,Faiz Ghifari Haznitrama,Inhwa Song,Alice Oh,Isabelle Augenstein
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 语言模型中文化意识的调查：文本及以后
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 大型语言模型（LLM）在各种应用程序（例如聊天机器人和虚拟助手）中的大规模部署要求LLMS对用户具有文化敏感，以确保包容性。文化在心理学和人类学领域进行了广泛的研究，最近的研究激增了LLM在LLM中更具文化包容性的研究，这超出了多语言性，并以心理学和人类学的发现为基础。在本文中，我们调查了将文化意识纳入基于文本和多模式LLM的工作。我们首先定义LLM中的文化意识，将人类学和心理学的文化定义作为出发点。然后，我们研究用于创建跨文化数据集的方法论，在下游任务中的文化包容策略以及用于基准LLMS文化意识的方法。此外，我们讨论了文化一致性的伦理含义，人类计算机相互作用在推动LLM的文化包容中的作用以及文化一致性在推动社会科学研究中的作用。我们终于根据有关文献差距的发现为未来研究提供指示。

### Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction 
[[arxiv](https://arxiv.org/abs/2411.00646)] [[cool](https://papers.cool/arxiv/2411.00646)] [[pdf](https://arxiv.org/pdf/2411.00646)]
> **Authors**: Houjing Wei,Hakaze Cho,Yuting Shi,Naoya Inoue
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: 6 pages, 5 figures
- **标题**: 视觉的阶段图大语言模型推断：跨图像和指令相互作用的视角
- **领域**: 计算语言学
- **摘要**: 视觉大语言模型（VLLM）通常将输入作为图像令牌嵌入和文本令牌嵌入并进行因果建模的串联。但是，它们的内部行为仍然没有被忽视，从而提出了两种类型的令牌之间的相互作用问题。为了研究模型推断期间的多模式相互作用，在本文中，我们测量了来自不同模态的代币隐藏状态向量之间的上下文化。我们的实验发现了VLLM的四相推理动力学针对基于变压器的LMS的深度，包括（i）对齐：在很早的层中，情境之间出现了情境化，表明特征空间对齐。 （ii）模式内编码：在早期层中，抑制模式间相互作用时，模式内情境化得到增强，这表明在模态内进行了局部编码。 （iii）模式间编码：在后来的层中，跨模态的上下文化得到了增强，这表明跨模态的融合更深。 （iv）输出准备：在很晚的层中，上下文化在全球范围内降低，而隐藏的状态则与und脚的空间保持一致。

### A Demonstration of Adaptive Collaboration of Large Language Models for Medical Decision-Making 
[[arxiv](https://arxiv.org/abs/2411.00248)] [[cool](https://papers.cool/arxiv/2411.00248)] [[pdf](https://arxiv.org/pdf/2411.00248)]
> **Authors**: Yubin Kim,Chanwoo Park,Hyewon Jeong,Cristina Grau-Vilchez,Yik Siu Chan,Xuhai Xu,Daniel McDuff,Hyeonhoon Lee,Cynthia Breazeal,Hae Won Park
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-04
> **comment**: Under Review for ML4H 2024
- **标题**: 大型语言模型的自适应协作以进行医学决策
- **领域**: 计算语言学
- **摘要**: 医疗决策（MDM）是一个多方面的过程，要求临床医生通常会协作评估复杂的多模式患者数据患者。大型语言模型（LLMS）有望通过综合庞大的医学知识和多模式健康数据来简化这一过程。但是，单一代理通常不适合细微的医学环境，需要适应性的协作解决问题。我们的Mdagents通过基于任务复杂性将协作结构动态分配给LLM，模仿现实世界中的临床协作和决策来解决这一需求。该框架提高了诊断的准确性，并支持复杂的现实医疗场景中的自适应反应，使其成为各种医疗机构中临床医生的宝贵工具，同时，与静态多代理决策方法相比，计算成本的效率更高。

### Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models 
[[arxiv](https://arxiv.org/abs/2411.00029)] [[cool](https://papers.cool/arxiv/2411.00029)] [[pdf](https://arxiv.org/pdf/2411.00029)]
> **Authors**: Donghoon Kim,Gusang Lee,Kyuhong Shim,Byonghyo Shim
> **First submission**: 2024-10-29
> **First announcement**: 2024-11-04
> **comment**: Findings of EMNLP 2024
- **标题**: 保留预训练的表示空间：关于大型多模式模型的前缀调整的有效性
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 最近，我们观察到，大型多模式模型（LMM）正在彻底改变机器与世界互动的方式，从而解开各种多模式应用程序的新可能性。为了适应LMMS的下游任务，仅训练其他前缀令牌或模块的参数有效的微调（PEFT）已获得了越来越流行。然而，对PEFT在LMM中的工作方式几乎没有分析。在本文中，我们深入研究了每种调整策略的优势和劣势，将重点从通常与这些方法相关的效率转移到了。我们首先发现模型参数调整方法（例如洛拉和适配器）扭曲了在预训练期间学到的特征表示空间，并限制了预训练的知识的全面利用。我们还证明，尽管在下游任务上的性能较低，但前缀调整在保留表示空间方面表现出色。这些发现提出了一种简单的两步PEFT策略，称为前缀调节的PEFT（PT-PEFT），该策略连续执行前缀调整，然后Peft（即Adapter，Lora）结合了两者的益处。实验结果表明，与香草PEFT方法相比，PT-PEFT不仅可以提高图像字幕和视觉问题回答的性能，而且还有助于保留四个预训练模型的表示空间。

### Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning 
[[arxiv](https://arxiv.org/abs/2411.02793)] [[cool](https://papers.cool/arxiv/2411.02793)] [[pdf](https://arxiv.org/pdf/2411.02793)]
> **Authors**: Mingcheng Li,Dingkang Yang,Yang Liu,Shunli Wang,Jiawei Chen,Shuaibing Wang,Jinjie Wei,Yue Jiang,Qingyao Xu,Xiaolu Hou,Mingyang Sun,Ziyun Qian,Dongliang Kou,Lihua Zhang
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: Accepted by NeurIPS 2024
- **标题**: 通过层次表示学习，朝着强大的多模式情感分析
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 多模式情感分析（MSA）是一个重要的研究领域，旨在通过多种方式理解和认识人类情感。与仅利用单一模态相比，多模式融合提供的互补信息可促进更好的情感分析。然而，在实际应用中，许多不可避免的因素可能导致不确定的方式缺失，从而阻碍了多模式建模的有效性并降低了模型的性能。为此，我们在不确定的缺失模式下为MSA任务提出了一个分层表示学习框架（HRLF）。具体而言，我们提出了一个细粒度的表示分解模块，该模块通过通过跨模式的翻译和情感语义重建来将模态分配到与情感相关和模态特定表示中，通过将模态分配到与情感相关的和模态特定的表示中。此外，引入了分层互信息最大化机制，以逐步最大化多尺度表示之间的相互信息，以在表示形式中对齐和重建高级语义。最终，我们提出了一种分层对抗学习机制，该机制进一步使与情感相关表示的潜在分布与产生强大的联合多模式表示形式相结合。在三个数据集上进行的全面实验表明，HRLF在不确定的模式缺失案例下显着提高了MSA性能。

### Multimodal Commonsense Knowledge Distillation for Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2411.02722)] [[cool](https://papers.cool/arxiv/2411.02722)] [[pdf](https://arxiv.org/pdf/2411.02722)]
> **Authors**: Shuo Yang,Siwen Luo,Soyeon Caren Han
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: AAAI 2025 (Accepted, Oral)
- **标题**: 视觉问题回答的多模式常识知识蒸馏
- **领域**: 计算语言学,人工智能
- **摘要**: 现有的多模式大型语言模型（MLLM）和视觉语言预处理模型（VLPM）在一般的视觉问题回答（VQA）中表现出色。但是，由于在产生高质量提示和微调的高计算成本方面的挑战，这些模型与需要外常识性知识的VQA问题斗争。在这项工作中，我们提出了一个新型的基于图的多模式常识性知识蒸馏框架，该框架通过教师学生的环境，通过图形卷积网络（GCN）来构建常识性知识，视觉对象和问题的统一关系图。该提议的框架对于任何类型的教师和学生模型都可以灵活，而无需进行其他微调，并且在ScienceQA数据集上取得了竞争性的表现。

### Context-Informed Machine Translation of Manga using Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.02589)] [[cool](https://papers.cool/arxiv/2411.02589)] [[pdf](https://arxiv.org/pdf/2411.02589)]
> **Authors**: Philip Lippmann,Konrad Skublicki,Joshua Tanner,Shonosuke Ishiwatari,Jie Yang
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: COLING 2025
- **标题**: 使用多模式的大型语言模型的上下文知识的机器翻译
- **领域**: 计算语言学
- **摘要**: 由于翻译所需的大量时间和精力，大多数漫画永远不会离开国内日本市场。自动漫画翻译是一种有希望的潜在解决方案。但是，这是一个萌芽和欠发达的领域，由于需要有效地将视觉元素纳入翻译过程以解决歧义，因此比标准翻译中的复杂性更大。在这项工作中，我们研究了多模式大语言模型（LLM）可以提供有效的漫画翻译，从而帮助漫画作者和出版商吸引更多的观众。具体而言，我们提出了一种利用多模式LLM的视觉组成部分来提高翻译质量并评估翻译单元大小，上下文长度的影响，并提出了对漫画翻译的有效方法的影响。此外，我们介绍了一个新的评估数据集（第一个平行的日本漫画翻译数据集），作为将来研究的基准的一部分。最后，我们为开源软件套件提供了贡献，使其他人能够基于漫画翻译基准LLM。我们的发现表明，我们提出的方法为日语 - 英语翻译获得了最先进的结果，并为日式波兰设定了新的标准。

### MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2411.02571)] [[cool](https://papers.cool/arxiv/2411.02571)] [[pdf](https://arxiv.org/pdf/2411.02571)]
> **Authors**: Sheng-Chieh Lin,Chankyu Lee,Mohammad Shoeybi,Jimmy Lin,Bryan Catanzaro,Wei Ping
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: Accepted at ICLR 2025. We release the model weights at: https://huggingface.co/nvidia/MM-Embed
- **标题**: MM Embed：多模式LLMS的通用多模式检索
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,信息检索,机器学习
- **摘要**: 最新的检索模型通常解决了一个直接的搜索方案，其中固定了检索任务（例如，找到一个段落来回答特定问题），并且仅对查询和检索结果仅支持单个模式。本文介绍了通过多模式大语言模型（MLLM）推进信息检索的技术，从而实现了更广泛的搜索场景，称为通用多模式检索，其中有多种方式和多种检索任务。为此，我们首先在10个数据集上进行了16个检索任务，将MLLM微调作为双重编码器猎犬进行微调。我们的经验结果表明，经过微调的MLLM猎犬能够理解由文本和图像组成的具有挑战性的查询，但是与跨模式检索任务中较小的夹夹相比，由于MLLM所表现出的模态偏差，它的表现不佳。为了解决这个问题，我们提出了模态意识的硬采矿，以减轻MLLM猎犬表现出的模态偏差。其次，我们建议对通用多模式猎犬进行持续微调，以增强其文本检索能力，同时保留多模式检索能力。结果，我们的模型（MM插入）在多模式检索基准M-Beir上实现了最先进的性能，该基准M-Beir跨越了多个域和任务，同时还超过了最先进的文本检索模型NV-Embed-V1，在MTEB回收基准测试基准上。我们还探索促使现成的MLLMS作为零射击者，以优化从多模式猎犬中的候选人的排名。我们发现，当用户查询（例如，文本图像组成的查询）更加复杂且充满挑战时，通过及时及时，MLLM可以进一步改善多模式检索。这些发现也为未来的普遍多式联运检索铺平了道路。

### Generative Emotion Cause Explanation in Multimodal Conversations 
[[arxiv](https://arxiv.org/abs/2411.02430)] [[cool](https://papers.cool/arxiv/2411.02430)] [[pdf](https://arxiv.org/pdf/2411.02430)]
> **Authors**: Lin Wang,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 产生情感引起多模式对话中的解释
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 多模式对话是人类交流的一种关键形式，具有丰富的情感内容，使探索其中情绪的原因是重要性重要性的研究。但是，现有关于情绪原因的研究通常使用子句选择方法来定位话语的原因，而无需对情感原因提供详细的解释。在本文中，我们提出了一项新任务，\ textbf {m} ult -imodal \ textbf {c} onversation \ textbf {e}运动\ textbf {c} ause \ textbf {e} xplanation（mcece），旨在产生一个详细的解释，以产生详细的说明，以征求对情感的说明，以乘以情感的说法。 Building upon the MELD dataset, we develop a new dataset (ECEM) that integrates video clips with detailed explanations of character emotions, facilitating an in-depth examination of the causal factors behind emotional expressions in multimodal conversations.A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos.通过利用面部情绪的传染效应，名望网络有效地捕捉了参与对话的人的情感原因。我们在新建的数据集上的实验结果表明，名望网络的表现明显优于几个出色的大型语言模型基线。代码和数据集可在\ url {https://github.com/32222345200/ecemdataset.git}中获得

### Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent 
[[arxiv](https://arxiv.org/abs/2411.02937)] [[cool](https://papers.cool/arxiv/2411.02937)] [[pdf](https://arxiv.org/pdf/2411.02937)]
> **Authors**: Yangning Li,Yinghui Li,Xinyu Wang,Yong Jiang,Zhen Zhang,Xinran Zheng,Hui Wang,Hai-Tao Zheng,Pengjun Xie,Philip S. Yu,Fei Huang,Jingren Zhou
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: No comments
- **标题**: 使用动态VQA数据集和自适应计划代理进行基准测试多模式检索增强生成
- **领域**: 计算语言学
- **摘要**: 多模式检索增强发电（MRAG）在减轻多模式大语言模型（MLLM）固有的“幻觉”问题中起着重要作用。尽管很有希望，但现有的启发式MRAG通常是预定义的固定检索过程，这导致了两个问题：（1）非自适应检索查询。 （2）重载的检索查询。但是，这些缺陷不能通过当前寻求知识的视觉问题答录（VQA）数据集充分反映，因为可以通过标准的两步检索轻松获得最需的知识。为了弥合数据集差距，我们首先构建了Dyn-VQA数据集，该数据集由三种类型的“动态”问题组成，这些问题需要查询，工具和时间中的复杂知识检索策略变量：（1）具有快速变化的答案的问题。 （2）需要多模式知识的问题。 （3）多跳问题。关于Dyn-VQA的实验表明，现有的启发式MRAGS由于其严格的检索过程而难以为动态问题提供足够的相关知识。因此，我们进一步提出了多模式检索的第一个自适应计划代理。潜在的思想是模仿有关解决方案中的人类行为，该解决方案将复杂的多模式问题动态分解为以检索作用的子问题链。广泛的实验证明了我们的综合搜索的有效性，也为推进MRAG提供了方向。代码和数据集将在https://github.com/alibaba-nlp/omnisearch上开放。

### Unfair Alignment: Examining Safety Alignment Across Vision Encoder Layers in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.04291)] [[cool](https://papers.cool/arxiv/2411.04291)] [[pdf](https://arxiv.org/pdf/2411.04291)]
> **Authors**: Saketh Bachu,Erfan Shayegani,Trishna Chakraborty,Rohit Lal,Arindam Dutta,Chengyu Song,Yue Dong,Nael Abu-Ghazaleh,Amit K. Roy-Chowdhury
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: Preprint, Under Review
- **标题**: 不公平的对齐方式：在视觉模型中检查跨视觉编码器层的安全对准
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 在多模式任务中，视觉语言模型（VLM）已显着改善，但是它们更复杂的架构使他们的安全一致性比大型语言模型（LLMS）的一致性更具挑战性。在本文中，我们揭示了VLM视觉编码器层的安全性不公平分布，与更强大的最终层相比，早期和中间层易于恶意输入。这种“跨层”脆弱性源于该模型无法从培训期间使用的默认建筑设置概括其安全培训，以看不见或分布外情景，从而使某些层暴露出来。我们通过预测各种中间层的激活来进行全面的分析，并证明这些层在暴露于恶意输入时更有可能产生有害输出。我们对LLAVA-1.5和LLAMA 3.2进行的实验表明，跨层的攻击成功率和毒性得分的差异表明，目前的安全对准策略集中在单个默认层上不足。

### M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models 
[[arxiv](https://arxiv.org/abs/2411.04075)] [[cool](https://papers.cool/arxiv/2411.04075)] [[pdf](https://arxiv.org/pdf/2411.04075)]
> **Authors**: Chuhan Li,Ziyao Shangguan,Yilun Zhao,Deyuan Li,Yixin Liu,Arman Cohan
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: No comments
- **标题**: M3SCIQA：用于评估基础模型的多模式多文档科学质量质量质量标准
- **领域**: 计算语言学,人工智能
- **摘要**: 用于评估基础模型的现有基准主要关注单文件，仅文本任务。但是，他们通常无法完全捕获研究工作流的复杂性，这通常涉及解释非文本数据并通过多个文档收集信息。为了解决这一差距，我们介绍了M3Sciqa，这是一种多模式的多档案科学问题，回答旨在对基础模型进行更全面评估的基准。 M3SCIQA由1,452个专家注册的问题组成，涵盖了70个自然语言处理簇簇，其中每个群集代表一个主要的纸张以及其所有引用的文档，反映了通过需要多模式和多模式的数据来理解单个论文的工作流程。使用M3SCIQA，我们对18种基础模型进行了全面评估。我们的结果表明，与多模式信息检索和跨多个科学文档推理的人类专家相比，目前的基础模型的表现仍然显着不足。此外，我们探讨了这些发现对将基础模型应用于多模式科学文献分析的未来进步的含义。

### Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech Detection with Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.03888)] [[cool](https://papers.cool/arxiv/2411.03888)] [[pdf](https://arxiv.org/pdf/2411.03888)]
> **Authors**: Minh Duc Bui,Katharina von der Wense,Anne Lauscher
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: Accepted to NAACL 2025 Main (Camera-Ready Version)
- **标题**: 多3HATE：具有视觉模型的多模式，多语言和多元文化仇恨语音检测
- **领域**: 计算语言学
- **摘要**: 警告：本文包含可能令人反感或令人沮丧的仇恨言论节奏的内容，这是由于内容的多模式和多语言性质以及各种文化认知所带来的独特挑战。当前的视觉模型（VLMS）如何导航这些细微差别？为了调查这一点，我们创建了第一个多模式和多语言平行的仇恨语音数据集，该数据集由称为Multi3hate的多元文化注释者注释。它包含5种语言的300个平行模因样本：英语，德语，西班牙语，印地语和普通话。我们证明，文化背景显着影响我们数据集中的多模式仇恨言论注释。国家之间的平均成对协议仅为74％，明显低于随机选择的注释群。我们的定性分析表明，在美国和印度 - 坎恩之间，最低的成对标签一致性最低的67％归因于文化因素。然后，我们在零拍的环境中使用5个大VLM进行实验，发现这些模型与来自美国的注释更紧密地与其他文化的注释保持一致，即使当模因和提示以其他文化的主导性语言呈现时，这些模型也与其他文化的注释相一致。代码和数据集可在https://github.com/minhducbui/multi3hate上找到。

### From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.05036)] [[cool](https://papers.cool/arxiv/2411.05036)] [[pdf](https://arxiv.org/pdf/2411.05036)]
> **Authors**: Charles Zhang,Benji Peng,Xintian Sun,Qian Niu,Junyu Liu,Keyu Chen,Ming Li,Pohsun Feng,Ziqian Bi,Ming Liu,Yichao Zhang,Cheng Fei,Caitlyn Heqi Yin,Lawrence KQ Yan,Tianyang Wang
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-08
> **comment**: 21 pages
- **标题**: 从单词矢量到多模式嵌入：大语言模型的技术，应用和未来方向
- **领域**: 计算语言学
- **摘要**: 单词嵌入和语言模型通过促进连续矢量空间中语言元素的表示，改变了自然语言处理（NLP）。这篇综述访问了基础概念，例如分布假设和上下文相似性，从而追踪了从稀疏表示的演变，例如单旋转编码到包括Word2Vec，Glove和FastText的密集嵌入。我们检查了静态和上下文化的嵌入，强调了Elmo，Bert和GPT等模型中的进步及其对跨语性和个性化应用程序的改编。讨论扩展到句子和文档嵌入，涵盖聚合方法和生成主题模型，以及在多模式领域中的嵌入，包括视觉，机器人和认知科学。分析了高级主题，例如模型压缩，可解释性，数值编码和缓解偏见，从而解决了技术挑战和道德含义。此外，我们确定了未来的研究方向，强调了对非文本方式的可扩展培训技术的需求，增强的解释性和强大的基础。通过综合当前的方法论和新兴趋势，该调查为研究人员和从业人员提供了深入的资源，以突破基于嵌入的语言模型的界限。

### Multimodal Quantum Natural Language Processing: A Novel Framework for using Quantum Methods to Analyse Real Data 
[[arxiv](https://arxiv.org/abs/2411.05023)] [[cool](https://papers.cool/arxiv/2411.05023)] [[pdf](https://arxiv.org/pdf/2411.05023)]
> **Authors**: Hala Hawashin
> **First submission**: 2024-10-29
> **First announcement**: 2024-11-08
> **comment**: This thesis, awarded a distinction by the Department of Computer Science at University College London, was successfully defended by the author in September 2024 in partial fulfillment of the requirements for an MSc in Emerging Digital Technologies
- **标题**: 多模式量子自然语言处理：使用量子方法分析真实数据的新型框架
- **领域**: 计算语言学,机器学习,量子物理学
- **摘要**: 尽管在各个领域的量子计算方面取得了重大进展，但将量子方法应用于语言组成的研究（例如建模语言结构和相互作用）仍然有限。该差距扩展到将量子语言数据与来自图像，视频和音频等源的现实世界数据集成在一起。本文探讨了量子计算方法如何通过多模式数据集成来增强语言的组成建模。具体而言，它通过应用Lambeq工具包进行多种组合模型的比较分析并评估其对图像文本分类任务的影响，从而进步多模式量子自然语言处理（MQNLP）。结果表明，基于语法的模型，尤其是Discocat和Treereader，在有效地捕获语法结构方面表现出色，而词具和顺序模型由于句法意识有限而挣扎。这些发现突显了随着量子技术的发展，量子方法增强语言建模并推动突破的潜力。

### Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models 
[[arxiv](https://arxiv.org/abs/2411.04996)] [[cool](https://papers.cool/arxiv/2411.04996)] [[pdf](https://arxiv.org/pdf/2411.04996)]
> **Authors**: Weixin Liang,Lili Yu,Liang Luo,Srinivasan Iyer,Ning Dong,Chunting Zhou,Gargi Ghosh,Mike Lewis,Wen-tau Yih,Luke Zettlemoyer,Xi Victoria Lin
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: No comments
- **标题**: 转化器的混合物：多模式基础模型的稀疏且可扩展的体系结构
- **领域**: 计算语言学
- **摘要**: 大型语言模型（LLM）的开发已扩展到能够在统一框架内处理文本，图像和语音的多模式系统。培训这些模型需要与仅文本LLM相比，需要更大的数据集和计算资源。为了应对规模挑战，我们引入了转化器（MOT），这是一种稀疏的多模式变压器体系结构，可大大降低预仔的计算成本。 MOT通过模态将模型的非安装参数（包括馈电网络，注意矩阵和层次归一化）分解，从而使模式特定的处理能够通过全局自我注意，而在完整的输入序列上进行了全局自我注意。我们在多个设置和模型量表上评估MOT。在变色龙7B设置（自回归文本和图像生成）中，MOT仅使用55.8％的拖鞋匹配密集的基线性能。当扩展到包括语音的语音时，MOT的语音性能与仅37.2 \％拖鞋的密集基线相当。在输血设置中，文本和图像经过不同的目标训练，一个7B MOT模型与密集基线的图像模态性能与三分之一的FLOPS相匹配，而760m MOT模型的表现优于关键图像生成指标的1.4B密度基线。系统概况进一步强调了MOT的实际好处，在47.2％的壁式时间和文本质量的47.2％中达到了密集的基线图像质量（75.6 \％的壁式时间）（以AWS P4DE.24XLARGE实例测量，使用NVIDIA A100 gpus进行测量）。

### M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework 
[[arxiv](https://arxiv.org/abs/2411.06176)] [[cool](https://papers.cool/arxiv/2411.06176)] [[pdf](https://arxiv.org/pdf/2411.06176)]
> **Authors**: Yew Ken Chia,Liying Cheng,Hou Pong Chan,Chaoqun Liu,Maojia Song,Sharifah Mahani Aljunied,Soujanya Poria,Lidong Bing
> **First submission**: 2024-11-09
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: M-LongDoc：多式联运文档理解和检索感知框架的基准
- **领域**: 计算语言学
- **摘要**: 了解和回答文档问题的能力在许多业务和实际应用中都有用。但是，文档通常包含冗长而多样化的多模式内容，例如文本，数字和表格，这些内容非常耗时，对于人类来说，彻底阅读。因此，迫切需要开发有效和自动化的方法来帮助人类执行此任务。在这项工作中，我们介绍了M-LongDoc，851个样品的基准和一个自动框架，以评估大型多模型的性能。我们进一步提出了一种检索意识调整方法，以进行有效有效的多模式文档阅读。与现有作品相比，我们的基准包括更多的详细和冗长的文档，其中包含数百页，同时还需要开放式解决方案，而不仅仅是提取答案。据我们所知，我们的培训框架是第一个直接解决多模式长文档的检索设置的框架。为了启用开源模型，我们以完全自动的方式构建了一个培训语料库，以在此类文档上进行问题的任务。实验表明，与基线开源模型相比，我们的调整方法对于模型响应的正确性，相对改善为4.6％。我们的数据，代码和模型可从https://multimodal-documents.github.io获得。

### Towards Low-Resource Harmful Meme Detection with LMM Agents 
[[arxiv](https://arxiv.org/abs/2411.05383)] [[cool](https://papers.cool/arxiv/2411.05383)] [[pdf](https://arxiv.org/pdf/2411.05383)]
> **Authors**: Jianzhao Huang,Hongzhan Lin,Ziyan Liu,Ziyang Luo,Guang Chen,Jing Ma
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: EMNLP 2024
- **标题**: 朝着LMM代理的低资源有害模因检测
- **领域**: 计算语言学
- **摘要**: 社交媒体时代的互联网模因的扩散需要有效识别有害的模因。由于模因的动态性质，现有的数据驱动模型可能在低资源场景中挣扎，只有几个标记的示例可用。在本文中，我们提出了一个以机构为驱动的框架，用于低资源有害模因检测，并使用几乎没有带注释的样本进行外向和内向分析。受到大型多模型（LMM）在多模式推理的强大容量的启发，我们首先检索带有注释的相对模因，以利用标签信息作为LMM代理的辅助信号。然后，我们引起LMM代理中的知识依据行为，以获得对模因危害性的良好的见解。通过结合这些策略，我们的方法可以使辩证性推理在复杂和隐含的危害指示模式上。在三个模因数据集上进行的广泛实验表明，我们所提出的方法在低资源有害模因检测任务上的最先进方法比最先进的方法表现出色。

### Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks 
[[arxiv](https://arxiv.org/abs/2411.05361)] [[cool](https://papers.cool/arxiv/2411.05361)] [[pdf](https://arxiv.org/pdf/2411.05361)]
> **Authors**: Chien-yu Huang,Wei-Chih Chen,Shu-wen Yang,Andy T. Liu,Chen-An Li,Yu-Xiang Lin,Wei-Cheng Tseng,Anuj Diwan,Yi-Jen Shih,Jiatong Shi,William Chen,Xuanjun Chen,Chi-Yuan Hsiao,Puyuan Peng,Shih-Heng Wang,Chun-Yi Kuan,Ke-Han Lu,Kai-Wei Chang,Chih-Kai Yang,Fabian Ritter-Gutierrez,Ming To Chuang,Kuan-Po Huang,Siddhant Arora,You-Kuan Lin,Eunjung Yeo, et al. (53 additional authors not shown)
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 动态 - 苏普布阶段2：协作扩展基准，用于通过180个任务来衡量口语模型的功能
- **领域**: 计算语言学,音频和语音处理
- **摘要**: 多模式的基础模型，例如双子座和Chatgpt，通过无缝整合各种形式的数据来彻底改变了人机相互作用。开发一种通用的口语模型，该模型理解广泛的自然语言指令对于弥合沟通差距和促进更直观的互动至关重要。但是，缺乏全面的评估基准构成了重大挑战。我们提出了动态 - 苏普布期2阶段，这是对基于教学的通用语音模型进行全面评估的开放且不断发展的基准。在第一代的基础上，第二个版本结合了125个新任务，由全球研究界合作，将基准扩展到总共180个任务，使其成为语音和音频评估的最大基准。虽然第一代动态套件仅限于分类任务，但动态苏普布阶段2通过引入各种新颖和多样化的任务，包括回归和序列，跨越语音，音乐和环境音频，可以扩大其评估能力。评估结果表明，这些模型均未普遍执行。 Salmonn-13B在英语ASR方面表现出色，而Wavllm在情感识别方面表现出很高的精度，但是当前的模型仍然需要进一步的创新来处理更广泛的任务。我们将很快开放所有任务数据和评估管道。

### Prompt-enhanced Network for Hateful Meme Classification 
[[arxiv](https://arxiv.org/abs/2411.07527)] [[cool](https://papers.cool/arxiv/2411.07527)] [[pdf](https://arxiv.org/pdf/2411.07527)]
> **Authors**: Junxi Liu,Yanyan Feng,Jiehai Chen,Yun Xue,Fenghuan Li
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: Published in Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence Main Track. Pages 6397-6405
- **标题**: 及时增强的仇恨模因分类网络
- **领域**: 计算语言学
- **摘要**: 社交媒体的动态扩展导致媒体平台上的仇恨模因淹没了，这突显了对有效识别和删除的日益增长的需求。我们承认常规多模式可恨模因分类的限制，这些模因分类在很大程度上取决于外部知识，并带来了包括无关紧要或冗余内容的风险，我们开发了笔 - 基于及时学习方法的迅速增强的网络框架。具体而言，在通过提示方法构造序列并使用语言模型对序列进行编码后，我们在编码序列上对多视图感知进行了全局提取。通过捕获有关推理实例和演示的全球信息，笔通过完全利用序列信息来促进类别选择。这种方法可显着提高模型分类精度。此外，为了加强模型在功能空间中的推理能力，我们将及时感知的对比度学习引入了框架中，以提高样本特征分布的质量。通过在两个公共数据集上进行广泛的消融实验，我们评估了笔框架的有效性，并同时将其与最先进的模型基准进行了比较。我们的研究发现凸显了笔超过手动及时方法，展示了可恶的模因分类任务中的出色概括和分类准确性。我们的代码可在https://github.com/juszzi/pen上找到。

### Multimodal Clinical Reasoning through Knowledge-augmented Rationale Generation 
[[arxiv](https://arxiv.org/abs/2411.07611)] [[cool](https://papers.cool/arxiv/2411.07611)] [[pdf](https://arxiv.org/pdf/2411.07611)]
> **Authors**: Shuai Niu,Jing Ma,Liang Bai,Zhihua Wang,Yida Xu,Yunya Song,Xian Yang
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: 11 pages. 4 figures
- **标题**: 通过知识提高理由产生的多模式临床推理
- **领域**: 计算语言学,人工智能
- **摘要**: 临床原理在准确的疾病诊断中起关键作用；但是，许多模型主要使用歧视方法，并忽略了产生支持原理的重要性。理由蒸馏是一个将知识从大语言模型（LLM）转移到较小语言模型（SLM）的过程，从而增强了后者分解复杂任务的能力。尽管有好处，但仅基本蒸馏就无法解决需要专业知识（例如疾病诊断）的任务中的领域知识限制。有效地嵌入SLM中的领域知识是一个重大挑战。虽然当前的LLM主要旨在处理文本数据，但包含时间序列数据（尤其是电子健康记录（EHR））的多模式LLM仍在不断发展。为了应对这些局限性，我们引入了Clinragen，这是一种针对疾病诊断中多模式原理产生的SLM。 Cillragen使用逐步的理由蒸馏策略，将独特的知识增强注意力机制与时间序列EHR数据合并为EHR数据，以产生基于文本和时间序列的临床原理。我们的评估表明，Clinragen明显提高了SLM解释多模式EHR数据并产生准确的临床理由的能力，支持更可靠的疾病诊断，推动LLM在医疗保健中的应用，并范围缩小LLMS和SLMS之间的性能分歧。

### Piecing It All Together: Verifying Multi-Hop Multimodal Claims 
[[arxiv](https://arxiv.org/abs/2411.09547)] [[cool](https://papers.cool/arxiv/2411.09547)] [[pdf](https://arxiv.org/pdf/2411.09547)]
> **Authors**: Haoran Wang,Aman Rangapur,Xiongxiao Xu,Yueqing Liang,Haroon Gharwi,Carl Yang,Kai Shu
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: COLING 2025
- **标题**: 将它们拼凑在一起：验证多跳多模式主张
- **领域**: 计算语言学,人工智能
- **摘要**: 现有的索赔验证数据集通常不需要系统来执行复杂的推理或有效解释多模式证据。为了解决这个问题，我们介绍了一项新任务：多跳多模式索赔验证。这项任务挑战模型以对来自不同来源的多个证据（包括文本，图像和表格）进行推论，并确定合并的多模式证据是否支持或反驳给定的主张。为了研究这项任务，我们构建了MMCV，这是一个大规模的数据集，其中包括15K多跳索赔与多模式证据配对，并使用大语言模型生成和精制，并提供了人类反馈的其他输入。我们表明，即使对于最新的最先进的多模式模型，MMCV也在挑战，尤其是随着推理啤酒花的数量增加。此外，我们在MMCV的子集上建立了人类绩效基准。我们希望该数据集及其评估任务将鼓励对多模式多跳索赔验证的未来研究。

### Cross-Modal Consistency in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.09273)] [[cool](https://papers.cool/arxiv/2411.09273)] [[pdf](https://arxiv.org/pdf/2411.09273)]
> **Authors**: Xiang Zhang,Senyu Li,Ning Shi,Bradley Hauer,Zijun Wu,Grzegorz Kondrak,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: 多模式大语言模型中的跨模式一致性
- **领域**: 计算语言学,人工智能
- **摘要**: 多模式方法论的最新发展标志着一个令人兴奋的时代的开始，用于处理各种数据类型，包括文本，音频和视觉内容。像GPT-4V这样的模型将计算机视觉与先进的语言处理合并，在处理复杂的任务方面表现出非凡的熟练程度，这些任务需要同时了解文本和视觉信息。先前的研究工作已精心评估了这些视力大语言模型（VLLM）在各个领域的功效，包括对象检测，图像字幕和其他相关领域。但是，现有的分析通常受到限制，主要集中在对每种模式表现的孤立评估中，同时忽略了探索其复杂的跨模式相互作用。具体而言，在面对不同模式的相同任务实例时，这些模型是否达到相同准确性的问题仍未得到解答。在这项研究中，我们采取了主动性通过引入一种称为跨模式一致性的新型概念来深入研究这些感兴趣方式之间的相互作用和比较。此外，我们提出了一个基于此概念的定量评估框架。我们的实验发现是从我们开发的平行视觉语言数据集的策划集合中得出的，尽管它描绘成统一的多模型模型，但GPT-4V内的视觉和语言模式之间存在明显的不一致性。我们的研究能够洞悉此类模型的适当利用，并提示潜在的途径，以增强其设计。

### MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal Large Language Models on Human Emotion Analysis 
[[arxiv](https://arxiv.org/abs/2411.11235)] [[cool](https://papers.cool/arxiv/2411.11235)] [[pdf](https://arxiv.org/pdf/2411.11235)]
> **Authors**: Yingjie Zhou,Zicheng Zhang,Jiezhang Cao,Jun Jia,Yanwei Jiang,Farong Wen,Xiaohong Liu,Xiongkuo Min,Guangtao Zhai
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 备忘录：关于人类情感分析的文本对图像和多模式模型的多个基准测试
- **领域**: 计算语言学,人工智能
- **摘要**: 人工智能（AI）在各个领域都表现出显着的能力，以及人工互动（HCI），体现智能以及虚拟数字人类的设计和动画等领域，从业人员和用户都越来越关注AI AI的能力理解和表达情感。因此，AI是否可以准确解释人类情绪的问题仍然是一个关键的挑战。迄今为止，已经参与了人类情感分析的两个主要类AI模型：生成模型和多模式大语言模型（MLLMS）。为了评估这两类模型的情感能力，本研究介绍了备忘录，这是一个由7,145张肖像组成的全面基准，每种肖像都描绘了六种不同情绪之一，由12个文本到图像图像（T2i）模型产生。与以前的作品不同，备忘录提供了一个在情感分析的背景下评估T2I模型和MLLM的框架。此外，采用了一种渐进式评估方法，从粗粒度到细粒度指标，以对MLLM的情感分析能力进行更详细，更全面的评估。实验结果表明，现有的T2I模型比负面情绪更有效地产生积极的情绪。同时，尽管MLLM在区分和识别人类情绪方面表现出一定程度的有效性，但它们却没有人类水平的准确性，尤其是在细粒度的情绪分析中。将公开提供备忘录，以支持该领域的进一步研究。

### Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava in Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2411.10950)] [[cool](https://papers.cool/arxiv/2411.10950)] [[pdf](https://arxiv.org/pdf/2411.10950)]
> **Authors**: Zeping Yu,Sophia Ananiadou
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: preprint
- **标题**: 了解多模式LLM：LLAVA在视觉问题中的机械解释性
- **领域**: 计算语言学
- **摘要**: 了解大语言模型（LLM）背后的机制对于设计改进的模型和策略至关重要。尽管最近的研究对文本LLM的机制产生了宝贵的见解，但多模式大语言模型（MLLM）的机制仍未得到充实。在本文中，我们采用机械性解释性方法来分析第一个MLLM Llava中的视觉问题回答（VQA）机制。我们比较了颜色回答任务中VQA和文本质量质量质量检查（TQA）之间的机制，并发现：a）VQA表现出类似于TQA中观察到的内在学习机制的机制； b）视觉特征在将视觉嵌入到嵌入空间中时具有明显的解释性； c）LLAVA在视觉教学调整期间增强了相应的文本LLM Vicuna的现有功能。基于这些发现，我们开发了一种可解释性工具，可以帮助用户和研究人员确定最终预测的重要视觉位置，以帮助理解视觉幻觉。与现有的可解释性方法相比，我们的方法证明了更快，更有效的结果。代码：\ url {https://github.com/zepingyu0512/llava-mechanism}

### Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2411.10928)] [[cool](https://papers.cool/arxiv/2411.10928)] [[pdf](https://arxiv.org/pdf/2411.10928)]
> **Authors**: Wenke Huang,Jian Liang,Zekun Shi,Didi Zhu,Guancheng Wan,He Li,Bo Du,Dacheng Tao,Mang Ye
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 从下游学习，并在多模式大语言模型微调中做自己
- **领域**: 计算语言学,人工智能
- **摘要**: 多模式的大语言模型（MLLM）证明了在不同的分布和任务中具有强大的概括能力，这在很大程度上是由于广泛的预培训数据集所致。微调MLLM已成为提高特定下游任务的性能的普遍做法。但是，在微调过程中，MLLM经常面临忘记在预训练期间获得的知识的风险，这可能导致概括能力下降。为了平衡概括和专业化之间的权衡，我们提出了基于冷冻的预训练的重量幅度和累积的微调梯度值的衡量参数的重要性。我们进一步应用了一种重要的权重分配策略，选择性地更新了下游任务的相对重要参数。我们使用各种MLLM体系结构对图像字幕和视觉提问任务进行经验评估。全面的实验分析表明了提出的解决方案的有效性，突出了至关重要的模块在增强下游专业化性能方面的效率，同时减轻MLLM微调中的概括降解。

### MLAN: Language-Based Instruction Tuning Improves Zero-Shot Generalization of Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.10557)] [[cool](https://papers.cool/arxiv/2411.10557)] [[pdf](https://arxiv.org/pdf/2411.10557)]
> **Authors**: Jianhong Tu,Zhuohao Ni,Nicholas Crispino,Zihao Yu,Michael Bendersky,Beliz Gunel,Ruoxi Jia,Xin Liu,Lingjuan Lyu,Dawn Song,Chenguang Wang
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: MLAN：基于语言的指令调整改善了多模式模型的零拍概括
- **领域**: 计算语言学
- **摘要**: 我们提出了一种新颖的教学调整配方，以改善多模式大语模型的零击任务概括。与在很大程度上依赖视觉说明的现有指令调谐机制相反，我们的方法着重于基于语言的教学调整，为多峰指令调整提供了独特，更有效的训练有效的路径。我们评估了在语言和视力方式上的9个看不见的数据集上提出的方法的性能。我们的结果表明，我们仅使用语言的指令调整能够显着提高基于Llama 2和Vicuna在那些看不见的数据集中的两种预审计的多模型。有趣的是，以下语言指令还有助于解锁模型，无需明确的培训即可遵循视觉说明。与主要基于视觉指示的最先进的多模式指导调谐方法相比，我们基于语言的方法不仅可以达到卓越的性能，而且可以显着提高训练效率。例如，仅语言指令调整在评估的数据集（在语言数据集上具有更好的性能）中产生有竞争力的平均性能，并有了重大的培训效率提高（平均4倍），这要归功于对视觉数据的需求的显着降低。通过少量的视觉说明，这种新兴的语言指令以后能力将其传递到看不见的视觉数据集，以更高的训练效率优于最先进的技术。

### Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization 
[[arxiv](https://arxiv.org/abs/2411.10442)] [[cool](https://papers.cool/arxiv/2411.10442)] [[pdf](https://arxiv.org/pdf/2411.10442)]
> **Authors**: Weiyun Wang,Zhe Chen,Wenhai Wang,Yue Cao,Yangzhou Liu,Zhangwei Gao,Jinguo Zhu,Xizhou Zhu,Lewei Lu,Yu Qiao,Jifeng Dai
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 通过混合偏好优化增强多模式大语言模型的推理能力
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 现有的开源多模式模型（MLLM）通常遵循涉及预训练和监督微调的培训过程。但是，这些模型遭受了分配变化的影响，这限制了它们的多模式推理，尤其是在经营链（COT）的性能中。为了解决这个问题，我们引入了一个偏好优化（PO）过程，以增强MLLM的多模式推理能力。具体来说，（1）在数据端，我们设计了一个自动偏好数据构建管道，以创建MMPR，这是一种高质量的大规模多模式推理偏好数据集。 （2）在模型侧，我们探索与MLLM的PO集成，开发一种简单而有效的方法，称为混合偏好优化（MPO），从而提高了多模式COT性能。我们的方法表明，在多个基准测试中的性能提高了，尤其是在多模式推理任务中。值得注意的是，我们的模型InternVL2-8B-MPO在MathVista上的精度达到了67.0，表现优于Intervl2-8b的8.77分，并实现与10x较大的Intervl2-76b相当的性能。我们希望这项研究能够激发MLLM的进一步进步。代码，数据和模型应公开发布。

### Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2411.10436)] [[cool](https://papers.cool/arxiv/2411.10436)] [[pdf](https://arxiv.org/pdf/2411.10436)]
> **Authors**: Yuhan Fu,Ruobing Xie,Xingwu Sun,Zhanhui Kang,Xirong Li
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 通过以幻觉为目标的直接偏好优化在多模式大语言模型中缓解幻觉
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,多媒体
- **摘要**: 众所周知，多模式的大语言模型（MLLM）是幻觉，这限制了其实际应用。最近的工作试图应用直接偏好优化（DPO）来增强MLLM的性能，但在缓解幻觉方面显示出不一致的改善。为了更有效地解决此问题，我们引入了以幻觉为目标的直接偏好优化（HDPO），以减少MLLM中的幻觉。与以前的方法不同，我们的方法从各种形式和原因中探讨了幻觉。具体而言，我们开发了针对MLLM幻觉原因的三种偏好对数据：（1）视觉功能不足，（2）长上下文生成，以及（3）多模式冲突。实验结果表明，我们的方法在多个幻觉评估数据集中达到了卓越的性能，超过了大多数最新方法（SOTA）方法，并突出了我们方法的潜力。消融研究和深入分析进一步证实了我们方法的有效性，并提出了通过扩展进行进一步改进的潜力。

### Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.11496)] [[cool](https://papers.cool/arxiv/2411.11496)] [[pdf](https://arxiv.org/pdf/2411.11496)]
> **Authors**: Chenhang Cui,Gelei Deng,An Zhang,Jingnan Zheng,Yicong Li,Lianli Gao,Tianwei Zhang,Tat-Seng Chua
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: 安全 +安全=不安全？探索如何利用安全图像到越狱大型视觉模型
- **领域**: 计算语言学
- **摘要**: 大型视觉模型（LVLM）的最新进展已展示了多种模式的强大推理能力，在各种现实世界应用中取得了重大突破。尽管取得了巨大的成功，但LVLMS的安全护栏可能无法涵盖视觉方式引入的不可预见的域。现有的研究主要集中于引发LVLM，以通过精心制作的基于图像的越狱旨在绕过一致性防御能力产生有害反应。在这项研究中，我们揭示了可以利用安全的图像，以与其他安全的图像和提示结合使用，以达到相同的越狱后果。这源于LVLM的两个基本特性：通用推理能力和安全雪球效应。在这些见解的基础上，我们建议安全雪球代理（SSA），这是一个新颖的基于特工的框架，利用代理商的自主和工具使用能力来越狱LVLM。 SSA通过两个主要阶段进行操作：（1）初始响应生成，工具基于潜在有害意图产生或检索越狱图像，以及（2）有害滚雪球，随后精致的提示会逐渐造成有害的产量。我们的实验表明，我们的实验几乎可以使用任何图像来诱导LVLM来产生不安全的内容，从而实现了最新的LVLMS成功越来越越来越多的越狱率。与利用对齐缺陷的先前作品不同，我们利用了LVLM的固有特性，这对在生成多模式系统中的安全性方面面临着深远的挑战。我们的代码可在\ url {https://github.com/gzcch/safety_snowball_agent}中避免使用。

### Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts 
[[arxiv](https://arxiv.org/abs/2411.11479)] [[cool](https://papers.cool/arxiv/2411.11479)] [[pdf](https://arxiv.org/pdf/2411.11479)]
> **Authors**: Jingxuan Li,Yuning Yang,Shengqi Yang,Linfan Zhang,Ying Nian Wu
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: 价值光谱：通过社交媒体环境中的价值分解来量化视觉模型的偏好
- **领域**: 计算语言学
- **摘要**: 视觉模型（VLM）的最新进展扩大了多模式应用的范围。但是，评估通常仍然限于功能任务，忽略了诸如人格特征和人类价值观等抽象方面。为了解决这一差距，我们介绍了价值光谱，这是一个新颖的视觉问题回答（VQA）基准测试，旨在根据Schwartz的价值维度评估VLM，该价值维度捕获了指导人们的偏好和行动的核心价值。我们设计了VLM代理管道，以模拟视频浏览并构建了一个矢量数据库，该数据库由Tiktok，YouTube Shorts和Instagram Reels组成50,000多个短视频。这些视频涵盖了多个月，并涵盖了各种主题，包括家庭，健康，爱好，社会，技术等。基于价值光谱的基准测试重点介绍了VLM如何处理价值的内容的显着变化。除了确定VLM的内在偏好外，我们还探索了VLM代理在明确提示时采用特定角色的能力，从而揭示了对模型在角色扮演方案中的适应性的见解。这些发现凸显了价值光谱作为跟踪基于价值任务和能力的VLM对齐方式的全面评估的潜力，以模拟各种角色。

### CUE-M: Contextual Understanding and Enhanced Search with Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2411.12287)] [[cool](https://papers.cool/arxiv/2411.12287)] [[pdf](https://arxiv.org/pdf/2411.12287)]
> **Authors**: Dongyoung Go,Taesun Whang,Chanhee Lee,Hwa-Yeon Kim,Sunghoon Park,Seunghwan Ji,Jinho Kim,Dongchan Kim,Young-Bum Kim
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: Preprint. Under review
- **标题**: CUE-M：通过多模式大语言模型的上下文理解和增强搜索
- **领域**: 计算语言学
- **摘要**: 将检索功能生成（RAG）与多模式大语言模型（MLLM）的整合彻底改变了信息检索并扩大了AI的实际应用。但是，当前的系统在准确解释用户意图，采用各种检索策略以及有效地过滤意外或不适当的响应方面努力努力，从而限制了他们的有效性。本文通过MLLM（CUE-M）介绍了上下文理解和增强搜索，该搜索是一种新颖的多模式搜索框架，通过包括图像上下文丰富的多个阶段管道来解决这些挑战，该挑战包括图像上下文丰富，意图细化，上下文查询产生，外部API集成和基于相关性的过滤。 CUE-M结合了一个可靠的过滤管道，结合了基于图像的基于图像，基于文本的分类器，并动态适应了由组织策略定义的实例和类别特定的关注点。对多模式问答数据集和公共安全基准的评估表明，CUE-M在准确性，知识整合和安全性方面都优于基准，从而提高了多模式检索系统的功能。

### Benchmarking Multimodal Models for Ukrainian Language Understanding Across Academic and Cultural Domains 
[[arxiv](https://arxiv.org/abs/2411.14647)] [[cool](https://papers.cool/arxiv/2411.14647)] [[pdf](https://arxiv.org/pdf/2411.14647)]
> **Authors**: Yurii Paniv,Artur Kiulian,Dmytro Chaplynskyi,Mykola Khandoga,Anton Polishko,Tetiana Bas,Guillermo Gabrielli
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 基准测试跨学术和文化领域的乌克兰语言理解的多模式模型
- **领域**: 计算语言学
- **摘要**: 虽然对以英语为中心的多模式的评估是具有许多基准的活跃研究领域，但对于低资源语言和中低资源语言，缺乏基准或评估套件。我们介绍了Zno-Vision，这是一种综合的多模式以乌克兰为中心的基准，该基准衍生自标准化大学入学考试（ZnO）。该基准由4,300多个专家提出的问题组成，涵盖了12个学科，包括数学，物理，化学和人文学科。我们评估了开源模型和API提供商的性能，发现只有少数模型在基线上方进行。除了新的基准外，我们还对乌克兰语言的多模式文本生成进行了首次评估研究：我们测量了Multiy30k-UK数据集的字幕生成质量，将VQA基准转换为乌克兰人，并相对于原始英语版本测量了性能降级。最后，我们从关于民族美食知识的文化角度测试了一些模型。我们认为，我们的工作将推动乌克兰语言的多模式生成能力，我们的方法对于其他低资源语言可能很有用。

### Understanding World or Predicting Future? A Comprehensive Survey of World Models 
[[arxiv](https://arxiv.org/abs/2411.14499)] [[cool](https://papers.cool/arxiv/2411.14499)] [[pdf](https://arxiv.org/pdf/2411.14499)]
> **Authors**: Jingtao Ding,Yunke Zhang,Yu Shang,Yuheng Zhang,Zefang Zong,Jie Feng,Yuan Yuan,Hongyuan Su,Nian Li,Nicholas Sukiennik,Fengli Xu,Yong Li
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 了解世界还是预测未来？对世界模型的全面调查
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 由于多模式大语言模型（例如GPT-4和视频生成模型）（例如Sora）的进步，世界模型的概念引起了极大的关注，这些模型是追求人工通用情报的核心。这项调查对有关世界模型的文献进行了全面的评论。通常，世界模型被视为理解当前世界状态或预测其未来动态的工具。这篇评论提出了对世界模型的系统分类，强调了两个主要功能：（1）构建内部表示以了解世界机制，以及（2）预测未来的状态以模拟和指导决策。最初，我们检查了这两个类别中的当前进展。然后，我们探索世界模型在关键领域的应用，包括自主驾驶，机器人和社交模拟物，重点是每个领域如何利用这些方面。最后，我们概述了关键挑战，并提供了对潜在的未来研究方向的见解。

### Mediating Modes of Thought: LLM's for design scripting 
[[arxiv](https://arxiv.org/abs/2411.14485)] [[cool](https://papers.cool/arxiv/2411.14485)] [[pdf](https://arxiv.org/pdf/2411.14485)]
> **Authors**: Moritz Rietschel,Fang Guo,Kyle Steinfeld
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-22
> **comment**: Published at ACADIA 2024
- **标题**: 中介思想模式：LLM的设计脚本
- **领域**: 计算语言学,人工智能,人机交互
- **摘要**: 建筑师采用视觉脚本和参数设计工具来探索更广阔的设计空间（Coates，2010），完善他们对设计的几何逻辑的思考（Woodbury，2010），并克服了传统的软件限制（Burry，2011）。尽管努力使设计脚本更加易于访问，但仍然存在设计师的自由思维方式和算法刚性之间的脱节（Burry，2011）。大型语言模型（LLM）的最新发展表明，这可能很快改变，因为LLMS编码了对人类环境的一般理解并具有产生几何逻辑的能力。该项目猜测，如果LLM可以有效地调解用户意图和算法，它们将成为使设计中的脚本更广泛和有趣的强大工具。我们探索此类系统是否可以解释自然语言提示以组装与计算设计脚本相关的几何操作。在系统中，LLM代理的多层配置了特定上下文，以推断用户意图并构建顺序逻辑。鉴于用户的高级文本提示，创建了几何描述，将其提炼成一系列逻辑操作，并映射到特定于软件的命令中。完成的脚本是在用户的Visual编程接口中构造的。该系统成功地生成完整的视觉脚本达到一定的复杂性，但失败了这种复杂性阈值。它显示了LLM如何使设计脚本与人类的创造力和思想更加一致。未来的研究应探索对话互动，扩展到多模式输入和输出，并评估这些工具的性能。

### StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2411.14476)] [[cool](https://papers.cool/arxiv/2411.14476)] [[pdf](https://arxiv.org/pdf/2411.14476)]
> **Authors**: Zongrong Li,Junhao Xu,Siqin Wang,Yifan Wu,Haiyang Li
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: StreetViewllm：使用经过思考链多模式模型提取地理信息
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 地理空间预测对于灾难管理，城市规划和公共卫生等各个领域至关重要。传统的机器学习方法在处理非结构化或多模式数据（例如街道视图图像）时通常会面临局限性。为了应对这些挑战，我们提出了StreetViewllm，这是一个新颖的框架，将大型语言模型与经过思考的推理和多模式数据源集成在一起。通过将街道视图图像与地理坐标和文本数据相结合，StreetViewllm提高了地理空间预测的精度和粒度。我们的方法使用检索增强的生成技术增强了地理信息提取，从而对城市环境进行了详细的分析。该模型已应用于包括香港，东京，新加坡，洛杉矶，纽约，伦敦和巴黎在内的七个全球城市，在预测城市指标，包括人口密度，医疗保健，归一化差异植被指数，建筑物高度和不受欢迎的表面方面表现出了卓越的表现。结果表明，StreetViewllm始终胜过基线模型，提供了提高的预测准确性和对建筑环境的更深入的见解。这项研究为将大语言模型纳入城市分析，城市规划，基础设施管理和环境监测的新机会开辟了新的机会。

### TransCompressor: LLM-Powered Multimodal Data Compression for Smart Transportation 
[[arxiv](https://arxiv.org/abs/2411.16020)] [[cool](https://papers.cool/arxiv/2411.16020)] [[pdf](https://arxiv.org/pdf/2411.16020)]
> **Authors**: Huanqi Yang,Rucheng Wu,Weitao Xu
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: 6 pages
- **标题**: 截面器：智能传输的LLM驱动多模式数据压缩
- **领域**: 计算语言学
- **摘要**: 大型语言模型（LLM）纳入智能运输系统为提高数据管理和运营效率铺平了道路。这项研究介绍了Transcressor，这是一个新型框架，该框架利用LLMS进行多模式运输传感器数据的有效压缩和解压缩。经压缩器对各种运输模式等各种运输模式（包括晴雨表，速度和高度测量）进行了多种传感器数据类型进行了彻底的评估。全面的评估说明了按压率以不同的压缩比重建运输传感器数据的有效性。结果表明，通过精心制作的提示，LLM可以利用其庞大的知识库来为数据压缩过程做出贡献，从而在智能运输环境中增强数据存储，分析和检索。

### Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark 
[[arxiv](https://arxiv.org/abs/2411.15488)] [[cool](https://papers.cool/arxiv/2411.15488)] [[pdf](https://arxiv.org/pdf/2411.15488)]
> **Authors**: Rong-Cheng Tu,Zi-Ao Ma,Tian Lan,Yuehao Zhao,Heyan Huang,Xian-Ling Mao
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 文本到图像生成的自动评估：任务执行的框架，蒸馏培训和元评估基准测试
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 在扩散模型的显着进步的推动下，文本到图像的生成取得了长足的进步，从而对生成的图像的自动质量评估产生了紧迫的需求。当前的最新自动评估方法在很大程度上依赖于多模式的大语言模型（MLLM），尤其是强大的商业模型，例如GPT-4O。尽管这些模型非常有效，但它们的大量成本限制了大规模评估中的可伸缩性。采用开源MLLM是另一种选择。但是，由于与商业MLLM相比，由于处理多模式数据的显着限制，它们的性能缺乏。为了解决这些问题，我们首先提出了一个基于GPT-4O的任务分解评估框架，以自动构建一个新的培训数据集，其中复杂的评估任务被解耦为更简单的子任务，从而有效地降低了学习的复杂性。基于此数据集，我们设计了创新的培训策略，以有效地将GPT-4O的评估功能提炼到7B开源MLLM，Minicpm-V-2.6。此外，为了可靠，全面评估先前的工作和我们提出的模型，我们手动注释了一个元评估基准，其中包括经过思考的解释以及生成图像的质量分数。实验结果表明，我们蒸馏的开源MLLM明显优于当前最新的GPT-4O基线基线Viescore，Spearman和Kendall与人类判断的相关性的改善超过4.6 \％。

### Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts 
[[arxiv](https://arxiv.org/abs/2411.15432)] [[cool](https://papers.cool/arxiv/2411.15432)] [[pdf](https://arxiv.org/pdf/2411.15432)]
> **Authors**: Qizhou Chen,Chengyu Wang,Dakan Wang,Taolin Zhang,Wangyue Li,Xiaofeng He
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 终身知识编辑视觉语言模型，具有低级专家的混合物
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 模型编辑旨在纠正不准确的知识，更新过时的信息，并将新数据纳入大型语言模型（LLMS），而无需重新培训。此任务在终身场景中构成了挑战，必须将编辑不断用于现实世界应用。尽管一些编辑在纯LLMS中表现出强大的稳健性，但结合了额外视觉方式的Vision LLMS（VLLM）并不直接适应现有的LLM编辑器。在本文中，我们建议Liveedit，这是一种终生视觉语言模型编辑，以弥合终身LLM编辑和VLLM之间的差距。首先，我们培训编辑专家生成器，为每个编辑实例独立生产低级专家，目的是纠正VLLM的相关响应。开发了一种硬过滤机制来利用视觉语义知识，从而在编辑后的模型的推理阶段粗略地消除了视觉上无关的专家。最后，为了整合视觉上相关的专家，我们引入了基于文本语义相关性的软路由机制，以实现多杂制融合。为了进行评估，我们为终身VLLM编辑建立了基准。广泛的实验表明，LiveDit在终身VLLM编辑方案中具有显着优势。进一步的实验验证了LiveEdit中每个模块设计的合理性和有效性。

### Exploring Large Language Models for Multimodal Sentiment Analysis: Challenges, Benchmarks, and Future Directions 
[[arxiv](https://arxiv.org/abs/2411.15408)] [[cool](https://papers.cool/arxiv/2411.15408)] [[pdf](https://arxiv.org/pdf/2411.15408)]
> **Authors**: Shezheng Song
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 探索大型语言模型以进行多模式情感分析：挑战，基准和未来方向
- **领域**: 计算语言学,人工智能
- **摘要**: 基于多模式的情感分析（MABSA）旨在从多模式信息（包括文本和图像）中提取方面术语及其相应的情感极性。尽管传统的监督学习方法在此任务中显示出有效性，但大语言模型（LLM）对MABSA的适应性仍然不确定。 LLAMA2，LLAVA和CHATGPT等LLM的最新进展表现出了一般任务的强大功能，但是它们在像Mabsa这样的复杂且细粒度的场景中的表现却没有得到充实。在这项研究中，我们对LLMS对MABSA的适用性进行了全面研究。为此，我们构建了一个基准，以评估LLM在MABSA任务上的性能，并将其与最先进的监督学习方法进行比较。我们的实验表明，尽管LLM在多模式理解中表现出潜力，但它们在获得MABSA的令人满意的结果方面面临着巨大的挑战，尤其是在准确性和推理时间方面。基于这些发现，我们讨论了当前LLM和大纲方向的局限性，以增强其在多模式情感分析中的能力。

### Information Extraction from Heterogeneous Documents without Ground Truth Labels using Synthetic Label Generation and Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2411.14957)] [[cool](https://papers.cool/arxiv/2411.14957)] [[pdf](https://arxiv.org/pdf/2411.14957)]
> **Authors**: Aniket Bhattacharyya,Anurag Tripathi
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: Accepted to WACV 2025
- **标题**: 使用合成标签的生成和知识蒸馏，从无基础真相标签的异质文档中提取信息
- **领域**: 计算语言学
- **摘要**: 员工提交的发票和收据是具有文本，视觉和布局信息的视觉丰富文档（VRD）。为了防止欺诈和虐待的风险，对于组织有效从提交的收据中提取所需信息至关重要。这有助于评估关键因素，例如费用索赔的适当性，遵守支出和交易策略，收据的有效性以及在各个级别的下游异常检测。这些文档是异质的，具有多种格式和语言，上载具有不同的图像质量，并且通常不包含地面真相标签以进行模型的有效培训。在本文中，我们提出了基于任务意识的基于教学的标签（TAIL），这是一种无需标签的VRD库库中合成标签生成的方法，并使用基于响应的知识蒸馏进行了多模式的视觉文档理解模型（VRDU），而无需使用教师模型的权重或训练数据集，以在适当的形式中生成适当形式的注释。使用可用地面真实标签的基准测试外部数据集，我们证明了通过经验研究与Claude 3十四行诗相提并论的条件。 We then show that the resulting model performs at par or better on the internal expense documents of a large multinational organization than state-of-the-art LMM (large multimodal model) Claude 3 Sonnet while being 85% less costly and ~5X faster, and outperforms layout-aware baselines by more than 10% in Average Normalized Levenshtein Similarity (ANLS) scores due to its ability to reason and extract information from rare formats.最后，我们说明了我们在预防多付额外的方法的用法。

### De-biased Multimodal Electrocardiogram Analysis 
[[arxiv](https://arxiv.org/abs/2411.14795)] [[cool](https://papers.cool/arxiv/2411.14795)] [[pdf](https://arxiv.org/pdf/2411.14795)]
> **Authors**: Haitao Li,Ziyu Li,Yiheng Mao,Ziyi Liu,Zhoujian Sun,Zhengxing Huang
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 偏偏的多模式心电图分析
- **领域**: 计算语言学
- **摘要**: 多模式大语模型（MLLM）越来越多地应用于医学领域，尤其是在医学成像中。但是，在临床环境中开发MLLM的ECG信号是至关重要的，这是医学成像之外的重大挑战。先前的研究试图通过以无训练方式使用外部分类器将ECG转换为多个文本标签来解决这一问题。但是，这种方法大大压缩了ECG中的信息，并使LLMS的推理能力不足。在这项工作中，我们通过投影层直接将ECG的嵌入到LLM中，保留有关ECG的更多信息，并更好地利用LLM的推理能力。我们的方法还可以有效地处理临床实践中的共同情况，在这种情况下，有必要比较在不同时间进行的两个ECG。最近的研究发现，MLLM可能仅依靠文本输入来提供答案，而忽略了其他模式的输入。我们在ECG MLLM的背景下从因果的角度分析了这一现象，发现混杂因素的严重程度（疾病的严重性）引入了问题和答案之间的杂乱无章的相关性，导致该模型依靠这种虚假的相关性并忽略了ECG输入。这样的模型不理解心电图的输入，并且在对抗测试中表现不佳，在训练和测试集中使用了同一问题的不同表达式。我们设计了一种偏低的预训练方法，以根据后门调整理论消除混杂因素的效果。在对抗测试下，我们的模型在ECG-QA任务上表现良好，并证明了零拍功能。一个有趣的随机ECG测试进一步验证了我们的模型可以有效地理解并利用输入ECG信号。

### Enhancing In-Hospital Mortality Prediction Using Multi-Representational Learning with LLM-Generated Expert Summaries 
[[arxiv](https://arxiv.org/abs/2411.16818)] [[cool](https://papers.cool/arxiv/2411.16818)] [[pdf](https://arxiv.org/pdf/2411.16818)]
> **Authors**: Harshavardhan Battula,Jiacheng Liu,Jaideep Srivastava
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 使用LLM生成的专家摘要使用多代表性学习来增强住院死亡率预测
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: ICU患者的院内死亡率（IHM）预测对于及时干预和有效的资源分配至关重要。尽管结构化的生理数据提供了定量的见解，但临床注释提供了非结构化的，上下文丰富的叙述。这项研究将这些模式与大语言模型（LLM）生成的专家摘要相结合，以提高IHM预测准确性。使用MIMIC-III数据库，我们分析了ICU入院前48小时内的时间序列生理数据和临床注释。每位患者的临床记录是按时间顺序串联的，并使用MED42-V2 70B转化为专家摘要。开发了一个多代表性的学习框架，以整合这些数据源，利用LLM来增强文本数据，同时减轻对LLM预测的直接依赖，这可以引入不确定性量化和解释性方面的挑战。与仅限基线相比，提出的模型的AUPRC为0.6156（+36.41％），AUROC为0.8955（+7.64％）。专家摘要仅优于临床注释或时间序列数据，证明了LLM生成的知识的价值。在人群群体之间，绩效的增长是一致的，其代表性不足的人群有了显着改善，强调了该框架的公平应用潜力。通过将LLM生成的摘要与结构化和非结构化数据整合在一起，该框架捕获了互补的患者信息，从而显着提高了预测性能。这种方法展示了LLMS增强重症监护预测模型的潜力，强调了针对特定领域的验证和高级整合策略的需求，以进行更广泛的临床采用。

### Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines 
[[arxiv](https://arxiv.org/abs/2411.16365)] [[cool](https://papers.cool/arxiv/2411.16365)] [[pdf](https://arxiv.org/pdf/2411.16365)]
> **Authors**: Zi-Ao Ma,Tian Lan,Rong-Cheng Tu,Yong Hu,Yu-Shi Zhu,Tong Zhang,Heyan Huang,Xian-Ling Mao
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 多模式检索增强多模式生成：数据集，评估指标和强大基线
- **领域**: 计算语言学
- **摘要**: 我们介绍了多模式检索增强多模式生成（M $^2 $ rag）的系统研究，这是一项新型任务，使基础模型能够处理多模式Web内容并生成多模式响应，从而表现出更好的信息密度和可读性。尽管有潜在的影响，但M $^2 $ rag仍在研究中，缺乏全面的分析和高质量的数据资源。为了解决这一差距，我们通过严格的数据策划管道建立了全面的基准，并基于基础模型采用文本模式指标和多模式指标进行评估。我们进一步为基础模型提出了几种策略，以有效地处理M $^2 $ rag，并通过使用设计的指标过滤高质量样本来构建训练设置。我们的广泛实验证明了我们提出的指标的可靠性，我们设计的策略中模型性能的景观，并表明我们的微调7b-8b模型的表现优于最先进的GPT-4O模型。此外，我们在各种领域进行细粒度分析，并验证我们在数据策展管道中设计的有效性。所有资源，包括代码，数据集和模型权重，都将公开发布。

### Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach 
[[arxiv](https://arxiv.org/abs/2411.17760)] [[cool](https://papers.cool/arxiv/2411.17760)] [[pdf](https://arxiv.org/pdf/2411.17760)]
> **Authors**: Shijian Deng,Wentian Zhao,Yu-Jhe Li,Kun Wan,Daniel Miranda,Ajinkya Kale,Yapeng Tian
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 在多模式大语言模型中有效的自我完善：一种模型级的无法官方法
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 多模式大语言模型（MLLM）中的自我改善对于增强其可靠性和鲁棒性至关重要。但是，当前的方法通常很大程度上依赖于MLLM本身作为法官，从而导致高计算成本和潜在的陷阱，例如奖励黑客和模型崩溃。本文介绍了一个新颖的，无法官的自我完善框架。我们的方法采用了一种受控的反馈机制，同时消除了验证循环中对MLLM的需求。我们使用可控的幻觉机制生成偏好学习对，并通过利用轻巧的对比语言图像编码器来评估和反向对，从而优化数据质量。跨公共基准测试和我们新引入的IC数据集进行的评估旨在挑战幻觉控制，这表明我们的模型表现优于传统技术。我们获得了卓越的精度，并以明显降低的计算需求来召回。该方法为MLLM中可扩展的自我完善提供了有效的途径，平衡了绩效增长与资源要求降低的途径。

### Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting 
[[arxiv](https://arxiv.org/abs/2411.17674)] [[cool](https://papers.cool/arxiv/2411.17674)] [[pdf](https://arxiv.org/pdf/2411.17674)]
> **Authors**: Liyun Zhang,Dian Ding,Yu Lu,Yi-Chao Chen,Guangtao Xue
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 通过提示LLM并具有感受性的注意力加权来推动多模式情绪识别的极限
- **领域**: 计算语言学
- **摘要**: 了解对话中的情绪通常需要外部知识才能准确理解内容。随着LLM越来越强大，我们不想确定预先训练的语言模型的有限能力。但是，LLMS要么只能处理文本模式，要么太昂贵了，无法处理多媒体信息。我们旨在利用LLM的功能和多媒体模式的补充特征。在本文中，我们提出了一个框架灯笼，可以通过促使大型语言模型具有吸引人的注意力加权来提高某个香草模型的性能。该框架训练了多任务香草模型，以产生情感类别和维度得分的概率。这些预测被送入LLMS，是作为参考，以调整每个情绪类别的预测概率，其外部知识和上下文理解。我们将对话切成不同的接收场，每个样本都包含在t的接受场中。最后，LLM的预测与感受的注意力驱动的加权模块合并。在实验中，使用GPT-4或Llama-3.1-405B部署了香草模型Corect和SDT。 Iemocap中具有4向和6向设置的实验表明，灯笼可以显着提高当前香草模型的性能高达1.23％和1.80％。

### How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations 
[[arxiv](https://arxiv.org/abs/2411.17666)] [[cool](https://papers.cool/arxiv/2411.17666)] [[pdf](https://arxiv.org/pdf/2411.17666)]
> **Authors**: Hyunji Lee,Danni Liu,Supriti Sinhamahapatra,Jan Niehues
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: NAACL 2025
- **标题**: 多模式基础模型如何编码文本和语音？跨语性和跨模式表示的分析
- **领域**: 计算语言学
- **摘要**: 多模式基础模型旨在创建一个统一的表示空间，该空间从语言语法或模态差异之类的表面特征中抽象出来。为了调查这一点，我们研究了三个最新模型的内部表示，分析了文本和语音方式中语言上语言等效句子的模型激活。我们的发现表明：1）跨模式表示在模型层上收敛，除非在文本和语音处理的初始层中。 2）长度适应对于减少文本和语音之间的跨模式差距至关重要，尽管当前方法的有效性主要限于高资源语言。 3）语音表现出比文本更大的跨语性差异。 4）对于未经明确训练模态敏捷表示的模型，模态差距比语言差距更为突出。

### What Differentiates Educational Literature? A Multimodal Fusion Approach of Transformers and Computational Linguistics 
[[arxiv](https://arxiv.org/abs/2411.17593)] [[cool](https://papers.cool/arxiv/2411.17593)] [[pdf](https://arxiv.org/pdf/2411.17593)]
> **Authors**: Jordan J. Bird
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 教育文献有什么区别？变压器和计算语言学的多模式融合方法
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 将新文献整合到英语课程中仍然是一个挑战，因为教育者通常缺乏可扩展的工具来快速评估可读性并适应各种课堂需求。这项研究建议通过一种多模式方法来解决这一差距，该方法将基于变压器的文本分类与语言特征分析结合在一起，以使文本与英国的关键阶段相结合。对分段的文本数据进行了微调，八个最先进的变压器进行了微调，而伯特的最高单峰F1得分为0.75。同时，搜索了500个深神经网络拓扑以获取语言特征的分类，其F1得分为0.392。这些模式的融合显示出显着的改进，每种多模式方法的表现都超过了所有单峰模型。特别是，与神经网络融合的电动变压器的F1得分为0.996。除推理时间外，单峰和多模式方法在所有验证指标（准确性，精度，回忆，F1得分）上具有统计学上的显着差异。提出的方法最终被封装在面向利益相关者的Web应用程序中，从而使非技术利益相关者可以访问有关文本复杂性，阅读难度，课程一致性和学习年龄范围的建议的实时见解。该应用程序通过将基于AI的建议集成到英语文学的课程计划中，从而赋予数据驱动的决策并减少手动工作量。

### Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 
[[arxiv](https://arxiv.org/abs/2411.17558)] [[cool](https://papers.cool/arxiv/2411.17558)] [[pdf](https://arxiv.org/pdf/2411.17558)]
> **Authors**: Jiayi Kuang,Jingyou Xie,Haohao Luo,Ronghao Li,Zhe Xu,Xianfeng Cheng,Yinghui Li,Xika Lin,Ying Shen
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 自然语言理解和与MLLM在视觉问题回答中的推论：一项调查
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 视觉问题回答（VQA）是一项挑战任务，结合了自然语言处理和计算机视觉技术，并逐渐成为多模式大语模型（MLLM）中的基准测试任务。我们调查的目的是概述VQA的开发以及及时性高的最新模型的详细描述。这项调查提供了对图像和文本的自然语言理解的最新综合，以及基于核心VQA任务的图像问题信息的知识推理模块。此外，我们详细介绍了在VQA中将模态信息与视觉预科模型和多模式大语言模型的最新进展。我们还通过详细说明内部知识的提取和引入外部知识的提取，详尽地回顾了VQA中知识推理的进度。最后，我们介绍VQA的数据集和不同的评估指标，并讨论未来工作的可能指示。

### A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs 
[[arxiv](https://arxiv.org/abs/2411.17265)] [[cool](https://papers.cool/arxiv/2411.17265)] [[pdf](https://arxiv.org/pdf/2411.17265)]
> **Authors**: Lehan He,Zeren Chen,Zhelun Shi,Tianyu Yu,Jing Shao,Lu Sheng
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 主题级别的自我纠正方法来减轻MLLMS的幻觉
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 将多模式大语言模型（MLLM）的行为与人类偏好保持一致，对于开发可靠和值得信赖的AI系统至关重要。尽管最近的尝试使用了人类专家或强大的辅助AI系统来提供更准确的偏好反馈，例如确定MLLM的可取响应或直接重写无幻觉的响应，但广泛的资源开销损害了反馈收集的可扩展性。在这项工作中，我们介绍了主题级的偏好覆盖（TPO），这是一种自我纠正方法，指导模型本身在主题级别减轻自己的幻觉。通过一种脱浓的策略，该策略用模型本身产生的最佳或最差替代方案取代了响应中的每个主题，TPO创造了更对比的成对偏好反馈，从而在没有人类或专有模型干预的情况下增强了反馈质量。值得注意的是，实验结果表明，提议的TPO在可信度方面达到了最新的表现，将物体幻觉显着降低了92％，总体幻觉却降低了38％。代码，模型和数据集现已可用。

### AMPS: ASR with Multimodal Paraphrase Supervision 
[[arxiv](https://arxiv.org/abs/2411.18368)] [[cool](https://papers.cool/arxiv/2411.18368)] [[pdf](https://arxiv.org/pdf/2411.18368)]
> **Authors**: Amruta Parulekar,Abhishek Gupta,Sameep Chattopadhyay,Preethi Jyothi
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: 放大器：带有多模式释义监督的ASR
- **领域**: 计算语言学,人工智能,机器学习,音频和语音处理
- **摘要**: 自发或对话的多语言语音给最先进的自动语音识别（ASR）系统带来了许多挑战。在这项工作中，我们提出了一种新技术放大器，它可以增强多语言的多模式ASR系统，并基于释义的监督，以改进多种语言，包括印地语，Marathi，Marathi，Malayalam，Kannada和Nyanja。我们在训练多模式ASR模型的同时，使用参考转录的释义作为额外的监督，并选择性地调用了ASR性能较差的话语。使用具有最先进的多模型SeamlessM4T的AMP，我们获得了高达5％的单词错误率（WER）的显着相对降低。我们使用客观和人类评估指标对系统进行了详细的分析。

## 密码学和安全(cs.CR:Cryptography and Security)

该领域共有 6 篇论文

### Seeing is Deceiving: Exploitation of Visual Pathways in Multi-Modal Language Models 
[[arxiv](https://arxiv.org/abs/2411.05056)] [[cool](https://papers.cool/arxiv/2411.05056)] [[pdf](https://arxiv.org/pdf/2411.05056)]
> **Authors**: Pete Janowczyk,Linda Laurier,Ave Giulietta,Arlo Octavia,Meade Cleti
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: No comments
- **标题**: 看到的是欺骗：在多模式模型中对视觉途径的剥削
- **领域**: 密码学和安全,人工智能
- **摘要**: 多模式语言模型（MLLMS）通过结合视觉和文本数据，使图像字幕，视觉询问答案和多模式内容创建诸如创建诸如图像字幕，视觉询问响应之类的应用程序，从而改变了人工智能。这种理解和使用复杂信息的能力使MLLM在医疗保健，自主系统和数字内容等领域有用。但是，整合多种类型的数据还会产生安全风险。攻击者可以操纵视觉或文本输入，或两者兼而有之，以使模型产生意外甚至有害的响应。本文回顾了如何通过各种攻击策略来利用MLLM中的视觉输入。我们将这些攻击分为类别：简单的视觉调整和跨模式操作，以及诸如Vlattack，Hades和协作多模式对抗性攻击（Co-攻击）等高级策略。这些攻击甚至可以误导最强大的模型，同时看起来与原始视觉效果几乎相同，从而难以检测。我们还讨论了更广泛的安全风险，包括对重要应用中隐私和安全的威胁。为了应对这些风险，我们回顾当前的防御方法，例如SmoothVLM框架，像素随机化和Mirror Check，以查看它们的优势和局限性。我们还讨论了使MLLM更安全的新方法，包括自适应防御，更好的评估工具以及保护视觉和文本数据的安全方法。通过汇总最新的发展并确定改进的关键领域，该评论旨在支持创建更安全，可靠的多模式AI系统以实现现实世界的使用。

### BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation 
[[arxiv](https://arxiv.org/abs/2411.11006)] [[cool](https://papers.cool/arxiv/2411.11006)] [[pdf](https://arxiv.org/pdf/2411.11006)]
> **Authors**: Haiyang Yu,Tian Xie,Jiaping Gui,Pengyang Wang,Ping Yi,Yue Wu
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: Backdoombti：用于后门防御评估的后门学习多模式基准工具套件
- **领域**: 密码学和安全,人工智能
- **摘要**: 在过去的几年中，后门攻击的出现对深度学习系统提出了重大挑战，使攻击者可以将后门插入神经网络。当带有触发器的数据由后门模型处理时，它可能导致攻击者针对的错误预测，而正常数据会产生常规结果。后门攻击的范围正在超越计算机视觉，并侵占自然语言处理和语音识别等领域。但是，现有的后门防御方法通常是针对特定数据模式量身定制的，从而限制了其在多模式上下文中的应用。尽管多模式学习被证明在面部识别，情感分析，行动识别和视觉问题上的高度适用，但这些模型的安全仍然是一个至关重要的问题。具体而言，没有针对多模式应用程序或相关任务的现有后门基准。为了促进多模式后门的研究，我们介绍了Backdoombti，这是第一个后门学习工具包和基准测试，旨在跨11个常用数据集的三种代表性方式进行多模式评估。 BackdooMBTI提供了系统的后门学习管道，包括数据处理，数据中毒，后门培训和评估。生成的毒数据集和后门模型可以详细评估后门防御。鉴于模式的多样性，BackdooMBTI有助于跨不同数据类型的系统评估。此外，BackdooMbti提供了一种标准化方法来处理后门学习中的实际因素，例如与数据质量和错误标签有关的问题。我们预计，BackdooMBTI将在多模式背景下加快后门防御方法的未来研究。代码可在https://github.com/sjtuhaiyangyu/backdoormbti上找到。

### LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2411.10918)] [[cool](https://papers.cool/arxiv/2411.10918)] [[pdf](https://arxiv.org/pdf/2411.10918)]
> **Authors**: Danial Abshari,Chenglong Fu,Meera Sridhar
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: LLM辅助物理不变提取用于网络物理系统异常检测
- **领域**: 密码学和安全,人工智能
- **摘要**: 现代工业基础设施严重依赖于网络物理系统（CPS），但这些基础设施容易受到具有潜在灾难性影响的网络攻击。为了降低这些风险，已经开发了基于物理不变的异常检测方法。但是，这些方法通常需要特定于领域的专业知识来手动定义不变性，从而使它们昂贵且难以扩展。为了解决这一局限性，我们提出了一种新的方法，可以从CPS测试床上提取物理不变性以进行异常检测。我们的见解是，CPS设计文档通常包含对物理过程的语义上丰富的描述，这些描述可以在系统组件之间介绍相关的动态。利用最近生成AI模型的内置物理和工程知识，我们旨在自动化传统的手动过程，提高可扩展性并降低成本。这项工作着重于设计和优化检索仪式（RAG）工作流程，并使用定制的用于CPS文档的定制提示系统，从而准确地提取语义信息并从复杂的多模式内容中推断物理不变性。然后，我们不是直接将推断的不变性用于异常检测，而是引入了一种基于创新的统计学习方法，将这些不变性集成到培训数据集中。该方法解决了诸如幻觉和概念漂移之类的局限性，从而提高了模型的可靠性。我们评估了现实世界中公共CPS安全数据集的方法，该数据集包含86个数据点和58个攻击案例。结果表明，我们的方法的高精度为0.923，准确地检测到异常，同时最小化错误警报。

### Blockchain Meets LLMs: A Living Survey on Bidirectional Integration 
[[arxiv](https://arxiv.org/abs/2411.16809)] [[cool](https://papers.cool/arxiv/2411.16809)] [[pdf](https://arxiv.org/pdf/2411.16809)]
> **Authors**: Jianghao Gong,Peiqi Yan,Yue Zhang,Hongli An,Logan Liu
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 区块链遇到LLM：双向整合的生活调查
- **领域**: 密码学和安全,人工智能
- **摘要**: 在大型语言模型的领域中，通过持续的技术进步和创新所推动的多模式大语模型和解释性研究已取得了很大的进步。尽管如此，安全和隐私问题仍在该领域的巨大挑战中构成巨大挑战。区块链技术的出现以其分散性的性质，防篡改属性，分布式存储功能和可追溯性为特征，为解决这些问题提供了新的方法。这两种技术都独立地具有巨大的发展潜力。然而，他们的结合揭示了大量的跨学科机会和增长前景。当前的研究趋势越来越集中于区块链与大语言模型的整合，目的是通过这种融合来弥补其各自的局限性，并促进进一步的技术进化。在这项研究中，我们评估了这两种技术的优势和发展限制，并探讨了它们组合的可能性和发展潜力。本文主要研究了两个方向的技术融合：首先，大型语言模型在区块链中的应用，我们在其中确定了六个主要的开发方向，并探索了解决区块链技术及其应用程序方案的缺点的解决方案；其次，将区块链技术应用于大型语言模型，利用区块链的特征来纠正大型语言模型的缺陷并探索其在多个领域的应用潜力。

### RealSeal: Revolutionizing Media Authentication with Real-Time Realism Scoring 
[[arxiv](https://arxiv.org/abs/2411.17684)] [[cool](https://papers.cool/arxiv/2411.17684)] [[pdf](https://arxiv.org/pdf/2411.17684)]
> **Authors**: Bhaktipriya Radharapu,Harish Krishna
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: Best Paper Award, Blue Sky Track at 26th ACM International Conference onMultimodalInteraction, Nov 2024, San Jose, Costa Rica
- **标题**: 真实：通过实时现实主义评分彻底改变媒体身份验证
- **领域**: 密码学和安全,人工智能
- **摘要**: 深层发展和操纵媒体的威胁日益严重，需要对媒体身份验证进行根本性的重新思考。水印合成数据的现有方法很容易被删除或更改，并且当前的DeepFake检测算法无法达到完美的准确性。依靠元数据来验证内容来源的出处技术未能解决上演或假媒体的基本问题。本文通过提倡在其来源的真实含量的水印而不是水印合成数据来引入媒体身份验证的开创性范式变化。我们的创新方法采用多感官输入和机器学习来实时和不同情况下评估内容的现实主义。我们建议将强大的现实主义分数嵌入图像元数据中，从根本上改变了图像的信任和循环方式。通过结合植根于固件和硬件安全性的人类关于现实的既定原则，以及当代机器学习系统的复杂推理能力，我们开发了一种整体方法，可以从多个角度分析信息。这种雄心勃勃的蓝天方法代表了该领域的重大飞跃，推动了媒体真实性和信任的界限。通过拥抱技术和跨学科研究的最先进的进步，我们旨在建立一个新的标准来验证数字媒体的真实性。

### Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment 
[[arxiv](https://arxiv.org/abs/2411.18688)] [[cool](https://papers.cool/arxiv/2411.18688)] [[pdf](https://arxiv.org/pdf/2411.18688)]
> **Authors**: Soumya Suvra Ghosal,Souradip Chakraborty,Vaibhav Singh,Tianrui Guan,Mengdi Wang,Ahmad Beirami,Furong Huang,Alvaro Velasquez,Dinesh Manocha,Amrit Singh Bedi
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: 免疫：通过推理时间对齐，提高了多模式LLMS越狱的安全性
- **领域**: 密码学和安全,人工智能,机器学习
- **摘要**: 随着多模式大语言模型（MLLM）的广泛部署，用于视觉策划任务，提高安全性变得至关重要。最近的研究表明，尽管训练时间安全统一，这些模型仍然容易受到越狱攻击的影响。在这项工作中，我们首先强调了一个重要的安全差距，以描述仅通过安全培训实现的一致性可能不足以抵抗越狱攻击。为了解决这种漏洞，我们提出了免疫，这是一个推理时间防御框架，该框架通过受控解码来防御越狱攻击来利用安全的奖励模型。此外，我们提供了免疫的数学表征，为越狱提供可证明的保证。使用最近的MLLM对各种越狱基准测试的广泛评估表明，免疫有效增强了模型安全性，同时保留了模型的原始功能。例如，与基本的MLLM和最先进的国防战略相比，针对基于文本的越狱攻击对LLAVA-1.6，免疫攻击成功率分别降低了57.82％和16.78％。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 329 篇论文

### Nearest Neighbor Normalization Improves Multimodal Retrieval 
[[arxiv](https://arxiv.org/abs/2410.24114)] [[cool](https://papers.cool/arxiv/2410.24114)] [[pdf](https://arxiv.org/pdf/2410.24114)]
> **Authors**: Neil Chowdhury,Franklin Wang,Sumedh Shenoy,Douwe Kiela,Sarah Schwettmann,Tristan Thrush
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: ef:EMNLP 2024
- **标题**: 最近的邻居归一化改善了多模式检索
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 多模式模型利用大规模预训练来实现强大但仍然不完美的性能，例如图像字幕，视觉问题回答和跨模式检索。在本文中，我们提出了一种简单有效的方法，用于纠正受过训练的对比的图像文本检索模型中的错误，没有其他训练，称为最近的邻居归一化（NNN）。对于我们测试的所有对比模型（剪辑，blip，albef，siglip，beit）以及我们使用的两个数据集（MS-Coco和FlickR30K），我们显示了文本检索和图像检索中检索指标的改进。 NNN需要一个参考数据库，但不需要在此数据库上进行任何培训，甚至可以在填充后提高模型的检索准确性。

### Handwriting Recognition in Historical Documents with Multimodal LLM 
[[arxiv](https://arxiv.org/abs/2410.24034)] [[cool](https://papers.cool/arxiv/2410.24034)] [[pdf](https://arxiv.org/pdf/2410.24034)]
> **Authors**: Lucian Li
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: 具有多模式LLM的历史文档中的手写识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 仅作为手稿而存在的大量历史和文化文献。同时，相对于数字化打印的过程，跨脚本和不同的手写样式进行OCR已被证明是一个非常困难的问题。尽管最近基于变压器的模型取得了相对较强的性能，但它们在很大程度上依赖于手动转录的训练数据，并且很难在作家中概括。 GPT-4V和GEMINI等多模式LLM在执行OCR和计算机视觉任务方面具有很少的射击提示。在本文中，我评估了Gemini对基于Transformer的当前方法产生的手写文档转录的准确性。关键字：光学特征识别，多模式模型，文化保护，质量数字化，手写识别

### A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems using Disparity Maps 
[[arxiv](https://arxiv.org/abs/2410.24031)] [[cool](https://papers.cool/arxiv/2410.24031)] [[pdf](https://arxiv.org/pdf/2410.24031)]
> **Authors**: Ariel Larey,Eyal Rond,Omer Achrack
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: 使用差异图的非校准系统中面部抗旋转的多模式方法
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 面部识别技术越来越多地用于各种应用中，但它们容易面临欺骗攻击。这些欺骗攻击通常涉及独特的3D结构，例如印刷纸或移动设备屏幕。尽管立体深度相机可以有效地检测到此类攻击，但它们的高成本限制了它们的广泛采用。相反，没有外部校准的两个传感器系统提供了一种具有成本效益的替代方案，但无法使用立体声技术计算深度。在这项工作中，我们提出了一种通过非校准的系统利用面部属性来得出差异信息和估算相对深度的方法来克服这一挑战的方法。我们介绍了一个多模式的反启动模型，即创建的差异模型，该模型将创建的差异图与两个原始传感器模态一起结合在一起。我们使用从Intel Realsense ID解决方案F455收集的综合数据集来反驳各种欺骗攻击方面的有效性。我们的方法优于文献中的现有方法，以1.71％的误差率（EER）为1.71％，假阴性率（FNR）为2.77％，为1％。这些误差分别低于最佳比较方法的误差的2.45％和7.94％。此外，我们还引入了一个模型集合，该集合还解决了3D欺骗攻击，以2.04％的EER和FNR的FNR达到1％，FNR为3.83％。总体而言，我们的工作为缺乏深度信息的非校准系统中反欺骗的挑战性任务提供了最先进的解决方案。

### Re-assembling the past: The RePAIR dataset and benchmark for real world 2D and 3D puzzle solving 
[[arxiv](https://arxiv.org/abs/2410.24010)] [[cool](https://papers.cool/arxiv/2410.24010)] [[pdf](https://arxiv.org/pdf/2410.24010)]
> **Authors**: Theodore Tsesmelis,Luca Palmieri,Marina Khoroshiltseva,Adeela Islam,Gur Elkin,Ofir Itzhak Shahar,Gianluca Scarpellini,Stefano Fiorini,Yaniv Ohayon,Nadav Alali,Sinem Aslan,Pietro Morerio,Sebastiano Vascon,Elena Gravina,Maria Cristina Napolitano,Giuseppe Scarpati,Gabriel Zuchtriegel,Alexandra Spühler,Michel E. Fuchs,Stuart James,Ohad Ben-Shahar,Marcello Pelillo,Alessio Del Bue
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: NeurIPS 2024, Track Datasets and Benchmarks, 10 pages
- **标题**: 重新组装过去：现实世界2D和3D拼图解决的维修数据集和基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文提出了维修数据集，该数据集代表了一个具有挑战性的基准，用于测试现代计算和数据驱动方法，以解决拼图解决和重新组装任务。我们的数据集具有独特的属性，对于2D和3D拼图解决的当前基准很少见。这些碎片和裂缝是现实的，是由于在庞贝考古公园发生的第二次世界大战爆炸案中壁画的崩溃引起的。这些碎片也被侵蚀，并且缺少不规则形状和不同尺寸的碎片，从而进一步挑战了重新组装算法。该数据集是多模式的，可提供具有特征性绘画元素的高分辨率图像，详细的3D扫描片段和由考古学家注释的元数据。通过几年的不断实地调查，包括每个片段的挖掘和清洁，然后由考古学家解决了大约一部分的手动拼图。 16000个可用的1000件。在将3D中的所有片段数字化后，准备了一个基准，以挑战当前的重新组装和拼图解决方法，这些方法通常可以解决更简单的合成场景。经过测试的基线表明，显然存在一个差距，以填补解决这个计算复杂问题的问题。

### ImOV3D: Learning Open-Vocabulary Point Clouds 3D Object Detection from Only 2D Images 
[[arxiv](https://arxiv.org/abs/2410.24001)] [[cool](https://papers.cool/arxiv/2410.24001)] [[pdf](https://arxiv.org/pdf/2410.24001)]
> **Authors**: Timing Yang,Yuanliang Ju,Li Yi
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: Accepted by NeurIPS 2024. Code link https://github.com/yangtiming/ImOV3D
- **标题**: IMOV3D：学习开放式摄取点云仅从2D图像中检测3D对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 开放式Vocabulary 3D对象检测（OV-3DET）旨在概括训练阶段标记的基本类别数量有限。最大的瓶颈是带注释的3D数据的稀缺性，而2D图像数据集则丰富且注释丰富。因此，直观的是利用2D图像中的大量注释来减轻OV-3DET中固有的数据稀缺性。在本文中，我们通过探索仅使用2D图像学习OV-3DET的潜力来将任务设置推向其限制。这种设置的主要挑战是训练图像和测试点云之间的方式差距，这阻止了2D知识有效地集成到OV-3DET中。为了应对这一挑战，我们提出了一个新颖的框架IMOV3D，以利用包含图像和点云（PC）的伪多模式表示，以缩小模态差距。 IMOV3D的关键在于灵活的模态转换，其中可以使用单眼深度估计将2D图像提升为3D，也可以通过渲染从3D场景得出。这允许将训练图像和测试点云统一为通用的图像-PC表示形式，包括大量的2D语义信息，并结合了3D空间数据的深度和结构特征。我们仔细进行这种转换，以最大程度地减少训练和测试案例之间的域间隙。在两个基准数据集（SunRGBD和Scannet）上进行的广泛实验表明，即使没有地面真相3D培训数据，IMOV3D也明显胜过现有方法。通过包含最少的实际3D数据进行微调，该性能也显着超过了先前的最新。代码和预培训模型在https://github.com/yangtiming/imov3d上发布。

### JEMA: A Joint Embedding Framework for Scalable Co-Learning with Multimodal Alignment 
[[arxiv](https://arxiv.org/abs/2410.23988)] [[cool](https://papers.cool/arxiv/2410.23988)] [[pdf](https://arxiv.org/pdf/2410.23988)]
> **Authors**: Joao Sousa,Roya Darabi,Armando Sousa,Frank Brueckner,Luís Paulo Reis,Ana Reis
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: 26 pages, 14 figures
- **标题**: JEMA：与多模式比对的可扩展学习的联合嵌入框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 这项工作引入了JEMA（与多模式对齐的联合嵌入），这是一种针对激光金属沉积（LMD）量身定制的新型共学习框架，这是金属添加剂制造中的关键过程。随着行业5.0在工业应用中获得关注，有效的过程监控变得越来越重要。但是，有限的数据和AI的不透明性质对其在工业环境中的应用提出了挑战。 JEMA通过利用多模式数据（包括多视图图像和元数据（例如过程参数）来学习可转移的语义表示形式，解决了这一挑战。通过应用监督的对比损失函数，JEMA仅使用主要模式就可以进行鲁棒的学习和后续的过程监视，从而简化了硬件要求和计算开销。我们研究了JEMA在LMD过程监测中的有效性，专门针对其对下游任务（例如熔体池几何预测）的概括，而无需进行广泛的微调。我们的经验评估证明了JEMA的高扩展性和性能，尤其是与视觉变压器模型相结合时。与监督的对比度学习相比，我们报告的多模式设置的性能提高了8％，单峰设置的提高了1％。此外，学到的嵌入表示形式可以预测元数据，增强解释性并使您可以评估增加的元数据的贡献。我们的框架奠定了将多传感器数据与元数据集成的基础，从而实现了LMD域内及以后的多种下游任务。

### Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model 
[[arxiv](https://arxiv.org/abs/2410.23905)] [[cool](https://papers.cool/arxiv/2410.23905)] [[pdf](https://arxiv.org/pdf/2410.23905)]
> **Authors**: Hao Zhang,Lei Cao,Jiayi Ma
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: Accepted by the 38th Conference on Neural Information Processing Systems (NeurIPS 2024)
- **标题**: 文本列表：基于文本调节模型的交互式多模式图像融合框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的多模式图像融合方法无法解决源图像中呈现的复合降解，从而导致融合图像受到噪声，颜色偏见，暴露不当，\ textit {etc}的困扰。此外，这些方法经常忽略前景对象的特异性，从而削弱了融合图像中感兴趣的对象的显着性。为了应对这些挑战，本研究提出了一个基于文本调节的扩散模型的新型交互式多模式图像融合框架，称为文本局部。首先，该框架将特征级信息集成到扩散过程中，从而使自适应降解去除和多模式信息融合。这是在扩散过程中深入和显式嵌入信息融合的首次尝试，有效地解决了图像融合中的复合降解。其次，通过将文本和零射击位置模型的组合嵌入到扩散融合过程中，可以开发出文本控制的融合重新调节策略。这使用户注定的文本控制能够提高融合性能并突出融合图像中的前景对象。关于不同公共数据集的广泛实验表明，我们的文本局部在各种情况下都具有复杂的降级。此外，语义分割实验验证了我们文本控制的融合重新调节策略实现的语义性能的显着增强。该代码可在https://github.com/leiii-cao/text-difuse上公开获取。

### AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in Satellite Imagery 
[[arxiv](https://arxiv.org/abs/2410.23891)] [[cool](https://papers.cool/arxiv/2410.23891)] [[pdf](https://arxiv.org/pdf/2410.23891)]
> **Authors**: Hangyu Zhou,Chia-Hsiang Kao,Cheng Perng Phoo,Utkarsh Mall,Bharath Hariharan,Kavita Bala
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: Accepted at NeurIPS 2024 Datasets and Benchmarks Track. Code and data available at https://allclear.cs.cornell.edu/
- **标题**: Allclear：卫星图像中云删除的全面数据集和基准
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 卫星图像中的云对下游应用构成了重大挑战。当前的云清除研究中的一个主要挑战是缺乏全面的基准和足够大而多样化的培训数据集。为了解决这个问题，我们介绍了最大的公共数据集 -  $ \ textit {allclear} $，用于清除云，其中包含23,742个全球分布的利益区域（ROIS），具有不同的土地利用模式，包含400万张图像。每个投资回报率包括2022年以来的完整时间捕获，（1）来自Sentinel-2和Landsat 8/9，（2）的多光谱光学图像，（2）来自Sentinel-1的合成孔径（SAR）图像，以及（3）辅助遥感产品，例如云蒙版和陆盖映射。我们通过基准性能来验证数据集的有效性，证明了扩展定律 -  PSNR从$ 28.47 $上升到33.87美元，$ 30 \ times $ 30 \ times $ $数据，并就时间长度和各个方式的重要性进行消融研究。该数据集旨在提供对地球表面的全面覆盖范围，并促进更好的云拆除结果。

### Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding 
[[arxiv](https://arxiv.org/abs/2410.23822)] [[cool](https://papers.cool/arxiv/2410.23822)] [[pdf](https://arxiv.org/pdf/2410.23822)]
> **Authors**: Jinlong He,Pengfei Li,Gang Liu,Shenjun Zhong
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: 用于医学视觉接地的参数有效的微调医学多模式模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式的大语言模型（MLLM）继承了LLMS的卓越理解能力，并将这些功能扩展到多模式场景。这些模型在多模式任务的一般领域中获得了出色的结果。但是，在医疗领域中，大量的培训成本以及对医疗MLLM的发展构成挑战的大量培训成本。此外，由于答案的自由文本形式，诸如视觉接地之类的任务需要以规定的形式产生输出，这对于MLLM来说很难。到目前为止，在医疗视觉接地区域还没有医疗MLLMS。对于医学视觉接地任务，涉及根据简短文本描述识别医疗图像中的位置，我们提出了用于Medcial Visual接地（PFMVG）的参数有效的微调医学多模型模型。为了验证该模型的性能，我们在公共基准数据集上对其进行评估，以实现竞争成果，并且表现明显优于GPT-4V。我们的代码将在同行评审后开放。

### MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed Image Retrieval 
[[arxiv](https://arxiv.org/abs/2410.23736)] [[cool](https://papers.cool/arxiv/2410.23736)] [[pdf](https://arxiv.org/pdf/2410.23736)]
> **Authors**: Haiwen Li,Fei Su,Zhicheng Zhao
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: Motadual：模态任务双重对准，以增强零击组成图像检索
- **领域**: 计算机视觉和模式识别,信息检索
- **摘要**: 组成的图像检索（CIR）是一项具有挑战性的视觉语言任务，利用双模式（图像+文本）查询来检索目标图像。尽管有监督的CIR表现令人印象深刻，但对昂贵，手动标记的三重态的依赖限制了其可扩展性和零击功能。为了解决此问题，介绍了零击组成的图像检索（ZS-CIR）以及基于投影的方法。但是，这种方法遇到了两个主要问题，即，预训练（图像$ \ leftrightarrow $ text）和推理（图像+文本$ \ rightarrow $ image）和模态差异之间的任务差异。由于在推断过程中需要从参考图像中提取特征，因此后者与基于仅文本投影训练的方法有关。在本文中，我们提出了一个两阶段的框架来解决这两个差异。首先，为了确保效率和可扩展性，在大规模标题数据集上预先训练了文本反演网络。随后，我们将模式任务双重对准（Motadual）作为第二阶段，在该阶段中，大型语言模型（LLMS）生成用于微调的三重态数据，此外，在多模式上下文中引入了及时的学习，以有效地减轻模式和任务差异。实验结果表明，我们的Motadual在四个广泛使用的ZS-CIR基准测试中实现了最先进的性能，同时保持了较低的训练时间和计算成本。该代码将很快发布。

### Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach 
[[arxiv](https://arxiv.org/abs/2410.23676)] [[cool](https://papers.cool/arxiv/2410.23676)] [[pdf](https://arxiv.org/pdf/2410.23676)]
> **Authors**: Mathilde Caron,Alireza Fathi,Cordelia Schmid,Ahmet Iscen
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: NeurIPS 2024
- **标题**: 网络尺度视觉实体识别：LLM驱动的数据方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 网络尺寸的视觉实体识别，即在Wikipedia等广阔知识库中将图像与其相应实体相关联的任务，由于缺乏干净，大规模的培训数据，面临着重大挑战。在本文中，我们提出了一种新的方法来策划这样的数据集，利用多模式大语模型（LLM）进行标签验证，元数据生成和理由解释。我们没有依靠多模式LLM直接注释数据（我们发现这是次优的数据），而是通过访问其他上下文相关的信息（例如Wikipedia）来提示它来推理潜在的候选实体标签，从而导致更准确的注释。我们进一步使用多模式LLM来通过生成问题 - 答案对以及扎根的良好的文本描述（称为“理性”）来丰富数据集，该描述解释了图像及其分配的实体之间的连接。实验表明，经过此自动策划数据训练的模型在网络尺度的视觉实体识别任务上实现了最先进的性能（例如，在烤箱实体任务中提高了6.9％），强调了该领域中高质量培训数据的重要性。

### Posture-Informed Muscular Force Learning for Robust Hand Pressure Estimation 
[[arxiv](https://arxiv.org/abs/2410.23629)] [[cool](https://papers.cool/arxiv/2410.23629)] [[pdf](https://arxiv.org/pdf/2410.23629)]
> **Authors**: Kyungjin Seo,Junghoon Seo,Hanseok Jeong,Sangpil Kim,Sang Ho Yoon
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: Accepted to NeurIPS 2024. Project Page Link: https://pimforce.hcitech.org/
- **标题**: 姿势信息的肌肉力量学习，用于强大的手动压力估计
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **摘要**: 我们提出了Pimforce，这是一个新型框架，通过利用3D手姿势信息来增强前臂表面肌电图（SEMG）信号来增强手动压力估计。我们的方法与从SEMG的动态肌肉活动一起利用了3D手姿势的详细空间信息，以在不同的手动相互作用下实现准确且强大的全手压力测量。我们还开发了一个多模式数据收集系统，该系统结合了压力手套，SEMG臂章和无标记的手指跟踪模块。我们从21名参与者创建了一个全面的数据集，使用我们的收集系统捕获了手工姿势，SEMG信号的同步数据，以及在各种手部姿势和手动相互作用方案上施加的手动压力。我们的框架可以在复杂和自然的相互作用方案中进行精确的手动压力估计。我们的方法通过将3D手姿势信息与SEMG信号集成在一起，从而大大减轻了传统的基于SEMG或基于视力的方法的局限性。视频演示，数据和代码可在线提供。

### On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection 
[[arxiv](https://arxiv.org/abs/2410.23623)] [[cool](https://papers.cool/arxiv/2410.23623)] [[pdf](https://arxiv.org/pdf/2410.23623)]
> **Authors**: Xiufeng Song,Xiao Guo,Jiache Zhang,Qirui Li,Lei Bai,Xiaoming Liu,Guangtao Zhai,Xiaohong Liu
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: 10 pages, 9 figures
- **标题**: 在学习多模式伪造表示扩散的视频检测中
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散模型的大量合成视频对信息安全性和真实性构成威胁，从而导致对生成内容检测的需求不断增长。但是，现有的视频级检测算法主要集中于检测面部伪造，并且通常无法识别具有各种语义范围的扩散生成的内容。为了推进视频取证领域，我们提出了一种创新算法，称为多模式检测（MM-DET），用于检测扩散生成的视频。 MM-DET通过从LMM的多模式空间产生多模式的伪造表示（MMFR）来利用大型多模型（LMM）的深刻感知和全面能力，从而增强了其检测到看不见的伪造含量的能力。此外，MM-DET还利用了一个内在的框架注意力（IAFA）机制来扩大时空结构域中的特征增强。动态融合策略有助于完善融合的伪造表示。此外，我们在广泛的伪造视频中构建了一个称为扩散视频取证（DVF）的综合扩散视频数据集。 MM-DET在DVF中实现了最先进的性能，证明了我们的算法的有效性。源代码和DVF均可在https://github.com/sparklexfantasy/mm-det上找到。

### SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey 
[[arxiv](https://arxiv.org/abs/2411.00172)] [[cool](https://papers.cool/arxiv/2411.00172)] [[pdf](https://arxiv.org/pdf/2411.00172)]
> **Authors**: Kien X. Nguyen,Fengchun Qiao,Arthur Trembanis,Xi Peng
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: Seafloorai：海底地质调查局的大规模视觉语言数据集
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 海洋科学中机器学习模型进步的主要障碍，尤其是在声纳图像分析中，是AI-Ready数据集的稀缺性。尽管已经努力公开提供AI-Ready Sonar图像数据集，但它们在环境环境和规模方面受到了限制。为了弥合这一差距，我们介绍了Seafloorai，这是第一个广泛的AI-Ready数据集，用于与海洋科学家合作策划的5个地质层的海底映射。我们通过合并语言组件来进一步将数据集扩展到海底，以促进声纳图像的视觉和具有语言能力的机器学习模型的发展。该数据集由62个地理分布的数据调查组成，跨越17,300平方公里，具有696K声纳图像，827K注释的分段掩码，696K详细的语言描述和大约7m的Question-Question-Asswer Pairs。通过使我们的数据处理源代码公开可用，我们旨在吸引海洋科学界丰富数据库并激发机器学习社区开发更健壮的模型。这种协作方法将增强两个字段中数据集的功能和应用。

### ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2411.01756)] [[cool](https://papers.cool/arxiv/2411.01756)] [[pdf](https://arxiv.org/pdf/2411.01756)]
> **Authors**: Yiming Sun,Fan Yu,Shaoxiang Chen,Yu Zhang,Junwei Huang,Chenhui Li,Yang Li,Changbo Wang
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: ChatTracker：通过与多模式大语言模型聊天来增强视觉跟踪性能
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉对象跟踪旨在根据初始边界框中的视频序列找到目标对象。最近，Vision语言〜（VL）跟踪器提议利用其他自然语言描述来增强各种应用程序的多功能性。但是，就跟踪性能而言，VL跟踪器仍然不如最新的视觉跟踪器（SOTA）视觉跟踪器。我们发现这种自卑主要是由于他们对手动文本注释的严重依赖，其中包括经常提供模棱两可的语言描述。在本文中，我们建议ChatTracker在多模式大语模型（MLLM）中利用世界知识的财富来生成高质量的语言描述并增强跟踪性能。为此，我们提出了一个基于反射的新及时优化模块，以通过跟踪反馈对目标的模棱两可和不准确的描述进行迭代。为了进一步利用MLLM产生的语义信息，提出了一个简单而有效的VL跟踪框架，并且可以轻松地集成为插件模块，以提高VL和Visual Trackers的性能。实验结果表明，我们提出的聊天式聊天者的性能与现有方法相当。

### One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2411.01584)] [[cool](https://papers.cool/arxiv/2411.01584)] [[pdf](https://arxiv.org/pdf/2411.01584)]
> **Authors**: Zhenyu Wang,Yali Li,Hengshuang Zhao,Shengjin Wang
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-04
> **comment**: NeurIPS 2024
- **标题**: 一个全部：基于点云的3D对象检测的多域关节培训
- **领域**: 计算机视觉和模式识别
- **摘要**: 计算机视觉的当前趋势是利用一个通用模型来解决所有各种任务。不可避免地，实现这种通用模型需要将多域数据纳入联合培训，以在多种问题方案中学习。但是，在基于点云的3D对象检测中，这种多域关节训练是高度挑战性的，因为不同数据集的点云之间的较大域间隙导致严重的域干扰问题。在本文中，我们提出了\ textbf {oneDet3D}，这是一个通用的所有模型，它在\ emph {same}框架内，涉及各个领域的3D检测，包括多样化的室内和室外场景，并且只有\ emph {一个emph {一个}}集合。我们在以路由机制为指导下，以解决数据干扰问题的指导下，在散点和上下文中提出域感知分区，并进一步合并了语言引导分类的文本模式，以统一多数据库标签空间并减轻类别干扰问题。完全稀疏的结构和无锚的头部进一步适应具有显着尺度差异的点云。广泛的实验表明，OnEdet3D仅利用一个训练有素的模型来解决几乎所有3D对象检测任务的强大通用能力。

### Finding NeMo: Negative-mined Mosaic Augmentation for Referring Image Segmentation 
[[arxiv](https://arxiv.org/abs/2411.01494)] [[cool](https://papers.cool/arxiv/2411.01494)] [[pdf](https://arxiv.org/pdf/2411.01494)]
> **Authors**: Seongsu Ha,Chaeyun Kim,Donghwa Kim,Junho Lee,Sangho Lee,Joonseok Lee
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-04
> **comment**: Accepted at ECCV 2024. Project page: https://dddonghwa.github.io/NeMo/
- **标题**: 查找Nemo：用于引用图像分割的负摩擦式增强
- **领域**: 计算机视觉和模式识别
- **摘要**: 参考图像分割是一项综合任务，是将文本查询引用的对象从图像引用。在本质上，此任务的难度级别受相似对象的存在和参考表达的复杂性的影响。最近的RIS模型仍然显示出简单和硬场景之间的显着性能差距。我们提出了数据中存在瓶颈，并提出了一种简单但功能强大的数据增强方法，即摩西式增强（NEMO）。该方法将训练图像扩大到马赛克中，并通过预处理的多模式比对模型（例如夹子）精心策划的其他三个负面图像，以使样本更具挑战性。我们发现，正确调整难度水平，既不是模棱两可也不太琐碎至关重要。增强的训练数据鼓励RIS模型认识到相似的视觉实体之间的细微差异和关系，并具体理解整个表达式以更好地定位正确的目标。我们的方法在通过广泛的实验验证的各种数据集和模型上显示出一致的改进。

### EEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark 
[[arxiv](https://arxiv.org/abs/2411.01492)] [[cool](https://papers.cool/arxiv/2411.01492)] [[pdf](https://arxiv.org/pdf/2411.01492)]
> **Authors**: Ming Li,Jike Zhong,Tianle Chen,Yuxiang Lai,Konstantinos Psounis
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-04
> **comment**: Accepted to CVPR 2025
- **标题**: EEE板凳：全面的多模式电气和电子工程基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 关于大型语言模型（LLM）和大型多模型模型（LMM）的最新研究表明，包括科学和数学在内的各个领域中有希望的技能。但是，尚未系统地研究它们在更具挑战性和现实世界中与工程等相关场景（例如工程）中的能力。为了弥合这一差距，我们建议使用电气和电子工程（EEE）作为测试床，旨在评估LMMS解决实用工程任务的多模式基准。我们的基准由2860个精心策划的问题组成，这些问题涵盖了10个基本子域，例如模拟电路，控制系统等。与其他域中的基准测试相比，工程问题本质上是1）视觉上更复杂且通用性和用途较大，并且2）在解决方案中的确定性较小。解决这些问题的成功解决方案通常需要比寻常的视觉和文本信息更严格整合，因为模型需要在进行专业指导的同时了解复杂的图像，例如抽象电路和系统图，使其成为LMM评估的出色候选人。除了EEE基础板外，我们还对17种广泛使用的开放和封闭式LLMS和LMM进行了广泛的定量评估和细粒度分析。我们的结果表明，EEE当前基础模型的缺陷，平均性能从19.48％到46.78％。最后，我们揭示并探索了LMM中的重要缺点，我们称之为懒惰：在推理技术图像问题时忽略视觉上下文的同时，通过依靠文本来进行快捷方式。总而言之，我们认为EEE板凳不仅揭示了LMM的一些值得注意的局限性，而且还提供了一种宝贵的资源来推进其在实践工程任务中的应用，从而推动其处理复杂的现实世界情景的未来改善。

### A Visual Question Answering Method for SAR Ship: Breaking the Requirement for Multimodal Dataset Construction and Model Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2411.01445)] [[cool](https://papers.cool/arxiv/2411.01445)] [[pdf](https://arxiv.org/pdf/2411.01445)]
> **Authors**: Fei Wang,Chengcheng Chen,Hongyu Chen,Yugang Chang,Weiming Zeng
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: SAR船的视觉问题回答方法：打破多模式数据集构建和模型微调的要求
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的视觉问题回答（VQA）任务通常需要构建多模式数据集和微调视觉语言模型，这需要大量的时间和资源。这极大地阻碍了VQA在下游任务中的应用，例如基于合成孔径雷达（SAR）图像的船舶信息分析。为了应对这一挑战，这封信提出了一种新颖的VQA方法，该方法将对象检测网络与视觉语言模型集成在一起，专门设计用于分析SAR图像中的船只。该集成旨在增强VQA系统的功能，重点关注船舶位置，密度和尺寸分析以及风险行为检测等方面。最初，我们在两个代表性SAR检测数据集SSDD和HRSID上使用Yolo网络进行了基线实验，以评估每个模型的检测准确性。基于这些结果，我们选择了最佳模型Yolov8n，作为该任务最合适的检测网络。随后，我们利用视觉模型QWEN2-VL，我们设计并实施了专门针对SAR场景的VQA任务。此任务采用检测网络的船舶位置和大小信息输出，以生成用于SAR图像的多转对话和场景描述。实验结果表明，这种方法不仅可以无需其他数据集或微调即可启用基本的SAR场景提问。

### MambaReg: Mamba-Based Disentangled Convolutional Sparse Coding for Unsupervised Deformable Multi-Modal Image Registration 
[[arxiv](https://arxiv.org/abs/2411.01399)] [[cool](https://papers.cool/arxiv/2411.01399)] [[pdf](https://arxiv.org/pdf/2411.01399)]
> **Authors**: Kaiang Wen,Bin Xie,Bin Duan,Yan Yan
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: Mambareg：基于MAMBA的散布卷积稀疏编码，用于无监督的可变形多模式图像注册
- **领域**: 计算机视觉和模式识别
- **摘要**: 具有固有特征差异的多模式图像的精确比对在可变形的图像注册中构成了关键的挑战。传统的基于学习的方法通常将注册网络视为没有解释性的黑匣子。一个核心洞察力是，跨模式的解开对齐功能和不调整功能带来了好处。同时，对于图像注册任务的突出方法（例如卷积神经网络），通过其本地接受场来捕获长期依赖性，这是具有挑战性的。由于缺乏有效学习的远程依赖性和对应关系，因此当给定的图像对差异很大时，方法通常会失败。在本文中，我们提出了Mambareg，这是一种基于Mamba的新型建筑，该建筑集成了Mamba在捕获长序列以应对这些挑战方面的强大能力。借助我们提出的几个子模块，Mambareg可以有效地解开与模式无关的特征，这些功能负责从模态依赖性的，不与对接的特征进行注册。通过选择性地参与相关特征，我们的网络熟练地捕获了多模式图像之间的相关性，从而实现了集中的变形场预测和精确的图像对齐。基于MAMBA的体系结构无缝将卷积层的局部特征提取能力与Mamba的远程依赖建模能力集成在一起。公共非刚性RGB-IR图像数据集进行的实验证明了我们方法的优越性，从注册精度和变形场平滑度方面优于现有方法。

### X-Drive: Cross-modality consistent multi-sensor data synthesis for driving scenarios 
[[arxiv](https://arxiv.org/abs/2411.01123)] [[cool](https://papers.cool/arxiv/2411.01123)] [[pdf](https://arxiv.org/pdf/2411.01123)]
> **Authors**: Yichen Xie,Chenfeng Xu,Chensheng Peng,Shuqi Zhao,Nhat Ho,Alexander T. Pham,Mingyu Ding,Masayoshi Tomizuka,Wei Zhan
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: X-drive：用于驾驶场景的跨模式一致的多传感器数据综合
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近的进步利用了在驾驶场景中合成LiDAR点云或相机图像数据的扩散模型。尽管它们在建模单模式数据边缘分布方面取得了成功，但在不同方式之间相互依赖的探索尚未探索，以描述复杂的驾驶场景。为了填补这一空白，我们提出了一个新颖的框架X驱动器，以通过双分支潜在的扩散模型体系结构对点云和多视图图像的联合分布进行建模。考虑到两种方式的不同几何空间，X驱动条件是从其他方式上综合了相应的局部区域上每种方式的综合，从而确保了更好的比对和现实主义。为了进一步处理在DeNoising期间的空间歧义，我们基于相邻线设计了跨模式条件模块，以适应性地学习跨模式的局部对应关系。此外，X-Drive允许通过多级输入条件（包括文本，边界框，图像和点云）进行控制生成。广泛的结果证明了点云和多视图图像的X驱动器的高保真合成结果，并遵守输入条件，同时确保可靠的交叉模式一致性。我们的代码将在https://github.com/yichen928/x-drive上公开提供。

### SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document Understanding 
[[arxiv](https://arxiv.org/abs/2411.01106)] [[cool](https://papers.cool/arxiv/2411.01106)] [[pdf](https://arxiv.org/pdf/2411.01106)]
> **Authors**: Jian Chen,Ruiyi Zhang,Yufan Zhou,Tong Yu,Franck Dernoncourt,Jiuxiang Gu,Ryan A. Rossi,Changyou Chen,Tong Sun
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: Accepted to ICLR 2025
- **标题**: SV-rag：MLLM的洛拉语言改编，以供长期文档理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大型语言模型（MLLM）最近在文本丰富的图像理解中显示出巨大的进步，但他们仍然在复杂的，多页的视觉上富裕文档中挣扎。使用文档解析器进行检索的传统方法遭受了性能和效率限制，而将所有页面直接呈现给MLLMS会导致效率低下，尤其是冗长的效率。在这项工作中，我们提出了一个名为** s ** elf的新颖框架 -  ** v ** iSual ** r ** r ** etrieval- ** a ** a ** uginged ** g ** gy **势能（sv-rag），它可以拓宽任何MLLM的视野以支持长期记录的理解。我们证明，** mllms本身可以是一个有效的多模式检索器**，以获取相关页面，然后根据这些页面回答用户问题。 SV-rag具有两个特定的MLLM适配器，一个用于证据页面检索，另一个用于回答。经验结果表明，公共基准的最先进表现，证明了SV-rag的有效性。

### Identifying Implicit Social Biases in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.00997)] [[cool](https://papers.cool/arxiv/2411.00997)] [[pdf](https://arxiv.org/pdf/2411.00997)]
> **Authors**: Kimia Hamidieh,Haoran Zhang,Walter Gerych,Thomas Hartvigsen,Marzyeh Ghassemi
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 在视觉模型中确定隐性社会偏见
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,计算机与社会
- **摘要**: 视觉语言模型，例如剪辑（对比性语言图像），在各种多模式检索任务中变得越来越流行。但是，先前的工作表明，大型语言和深远的模型可以学习训练集中包含的历史偏见，从而导致刻板印象和潜在的下游伤害持续。在这项工作中，我们对剪辑中存在的社会偏见进行了系统的分析，重点是图像和文本方式之间的相互作用。我们首先提出了称为SO-B-IT的社会偏见的分类法，其中包含在十种偏见中分类的374个单词。如果与特定的人口组相关联，每种类型都会导致社会伤害。使用此分类法，我们使用每个单词作为提示的一部分检查了从面部图像数据集中从面部图像数据集中检索到的图像。我们发现，剪辑经常显示有害单词和特定人口群体之间的不良关联，例如当被要求检索“恐怖分子”的图像时，主要是在中东男人的照片中取回。最后，我们通过表明在用于训练剪辑模型的大型图像text数据集中也存在相同的有害刻板印象来对这种偏见的来源进行分析。我们的发现突出了评估和解决视觉模型中偏见的重要性，并表明需要大型预训练数据集的透明度和公平感知的策划。

### Enhancing Osteoporosis Detection: An Explainable Multi-Modal Learning Framework with Feature Fusion and Variable Clustering 
[[arxiv](https://arxiv.org/abs/2411.00916)] [[cool](https://papers.cool/arxiv/2411.00916)] [[pdf](https://arxiv.org/pdf/2411.00916)]
> **Authors**: Mehdi Hosseini Chagahi,Saeed Mohammadi Dashtaki,Niloufar Delfan,Nadia Mohammadi,Alireza Samari,Behzad Moshiri,Md. Jalil Piran,Oliver Faust
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 骨质疏松症是增加骨折风险的常见疾病，尤其是在老年人中。早期诊断对于预防骨折，降低治疗成本和保留活动性至关重要。但是，医疗保健提供者面临诸如有限标记的数据和处理医疗图像困难之类的挑战。这项研究提出了一个新型的多模式学习框架，该框架整合了临床和成像数据，以提高诊断准确性并建模可解释性。该模型利用了三个预训练的网络-VGG19，InceptionV3和Resnet50，从X射线图像中提取深度特征。这些功能使用PCA进行转换，以降低维度并专注于最相关的组件。基于聚类的选择过程确定了最具代表性的成分，然后将其与预处理的临床数据结合在一起，并通过完全连接的网络（FCN）处理以进行最终分类。特征重要的图突出了关键变量，表明病史，BMI和身高是主要的贡献者，强调了特定于患者的数据的重要性。虽然成像特征很有价值，但其重要性较低，这表明临床数据对于准确的预测至关重要。该框架可促进精确且可解释的预测，提高透明度并在AI驱动的临床整合诊断中建立信任。

### V-LoRA: An Efficient and Flexible System Boosts Vision Applications with LoRA LMM 
[[arxiv](https://arxiv.org/abs/2411.00915)] [[cool](https://papers.cool/arxiv/2411.00915)] [[pdf](https://arxiv.org/pdf/2411.00915)]
> **Authors**: Liang Mi,Weijun Wang,Wenming Tu,Qingfeng He,Rui Kong,Xinyu Fang,Yazhu Dong,Yikang Zhang,Yunchun Li,Meng Li,Haipeng Dai,Guihai Chen,Yunxin Liu
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: EuroSys'2025
- **标题**: V-lora：一种有效而灵活的系统，可以通过Lora LMM提高视觉应用
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型多模型模型（LMM）在各种复杂的视力任务中显示出了很大的进步，并从大型语言模型（LMMS）继承的固体语言和推理能力。低级适应性（LORA）提供了一种有前途的方法，将外部知识整合到LMM中，从而弥补了它们对特定于领域任务的局限性。但是，现有的洛拉模型在计算上的昂贵过分昂贵，并且会导致极高的延迟。在本文中，我们提出了一种端到端的解决方案，该解决方案赋予了各种视觉任务并丰富了Lora LMM的视觉应用。 Our system, VaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware LoRA adapter generation approach that generates LoRA adapters rich in domain-specific knowledge to meet application-specific accuracy requirements, 2) an adaptive-tiling LoRA adapters batching operator that efficiently computes concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter orchestration mechanism这管理了申请请求和洛拉适配器，以达到平均响应延迟的最低。我们对三个LMM的五项流行视觉任务进行了原型。实验结果表明，与原始LMM相比，Valora提高了准确性的24-62％，与最先进的Lora型号服务系统相比，Valora的延迟占延迟的20-89％。

### A Simple and Effective Temporal Grounding Pipeline for Basketball Broadcast Footage 
[[arxiv](https://arxiv.org/abs/2411.00862)] [[cool](https://papers.cool/arxiv/2411.00862)] [[pdf](https://arxiv.org/pdf/2411.00862)]
> **Authors**: Levi Harris
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 篮球广播镜头的简单有效的时间基础管道
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们提出了可靠的时间基础管道，用于篮球广播镜头的视频对分析。鉴于一系列输入，我们的方法可以快速准确地从篮球广播场景中提取时间来调查和四分之一值。我们的工作旨在加快开发大型多模式视频数据集，以在运动动作识别域中训练渴望数据的视频模型。我们的方法将预先标记的逐场注释与视频帧的密集事件注释相结合，从而可以快速检索标记的视频片段。与以前的方法不同，我们可以通过微调开箱即用的对象检测器直接查找语义文本区域来定位游戏时钟。我们的端到端方法改善了我们工作的一般性。此外，插值和并行化技术准备在大型计算集群中部署的管道。所有代码均可公开使用。

### Dreaming Out Loud: A Self-Synthesis Approach For Training Vision-Language Models With Developmentally Plausible Data 
[[arxiv](https://arxiv.org/abs/2411.00828)] [[cool](https://papers.cool/arxiv/2411.00828)] [[pdf](https://arxiv.org/pdf/2411.00828)]
> **Authors**: Badr AlKhamissi,Yingtian Tang,Abdülkadir Gökce,Johannes Mehrer,Martin Schrimpf
> **First submission**: 2024-10-29
> **First announcement**: 2024-11-04
> **comment**: Accepted to BabyLM Challenge at CoNLL 2024
- **标题**: 大声做梦：一种具有发展合理数据的培训视觉语言模型的自我合成方法
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 虽然当今的大型语言模型在产生类似人类的文本方面具有令人印象深刻的能力，但它们在培训过程中需要大量数据。在这里，我们从人类的认知发展中汲取灵感，以在有限的数据条件下训练模型。具体而言，我们提出了一种自我合成方法，该方法通过四个阶段进行迭代：第1阶段设置了基本语言能力，在小型语料库上从头开始训练模型。然后，语言与第2阶段的视觉环境相关联，将模型与视觉编码器集成在一起，以从标记的图像中生成描述性字幕。在“自我合成”第3阶段中，该模型为未标记的图像生成字幕，然后它用来进一步训练其语言组成部分，并混合了合成和以前的现实世界文本。此阶段旨在扩大模型的语言曲目，类似于人类自我宣布的新体验。最后，第4阶段通过训练模型，例如视觉问答和推理等特定任务来发展高级认知技能。我们的方法提供了使用开发量的数据培训多模型模型的概念证明。

### IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves 
[[arxiv](https://arxiv.org/abs/2411.00827)] [[cool](https://papers.cool/arxiv/2411.00827)] [[pdf](https://arxiv.org/pdf/2411.00827)]
> **Authors**: Ruofan Wang,Juncheng Li,Yixu Wang,Bo Wang,Xiaosen Wang,Yan Teng,Yingchun Wang,Xingjun Ma,Yu-Gang Jiang
> **First submission**: 2024-10-29
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 构思：越狱和基准测试大型视觉模型自己使用
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着大型视觉模型（VLM）的突出性，确保其安全部署变得至关重要。最近的研究探索了VLM的鲁棒性，以防止越狱攻击技术利用模型漏洞引起有害产出。但是，各种多模式数据的有限可用性限制了当前方法，以严重依赖于从有害文本数据集中衍生出的对抗性或手动制作的图像，这些图像通常缺乏不同环境中的效率和多样性。在本文中，我们提出了一种新颖的越狱方法，该方法自主会产生恶意的图像文本对，用于黑盒越狱攻击。 Iteator的基础是，VLM自己可以用作强大的红色团队模型来产生多模式越狱提示。具体而言，Ideator利用VLM创建有针对性的越狱文本，并将其与最先进的扩散模型产生的越狱图像配对。广泛的实验表明，Iseator的高效率和可传递性，在越狱的Minigpt-4中达到了94％的攻击成功率（ASR），平均只有5.34个查询，高的ASRS分别转移到Lllava，Distractblip和Chameleon时，高度为82％，88％和75％。在Iteseator的强大可传递性和自动化过程的基础上，我们介绍了VLBREAKBENCH，这是一个安全基准，包括3,654个多模式越狱样品。我们在11个最近发布的VLM上的基准结果揭示了安全对准方面的显着差距。例如，我们的挑战设定的ASR在GPT-4O上获得了46.31％的ASR，在Claude-3.5-Sonnet上获得了19.65％的ASR，强调了迫切需要更强大的防御能力。

### Uncertainty Quantification via Hölder Divergence for Multi-View Representation Learning 
[[arxiv](https://arxiv.org/abs/2411.00826)] [[cool](https://papers.cool/arxiv/2411.00826)] [[pdf](https://arxiv.org/pdf/2411.00826)]
> **Authors**: an Zhang,Ming Li,Chun Li,Zhaoxia Liu,Ye Zhang,Fei Richard Yu
> **First submission**: 2024-10-29
> **First announcement**: 2024-11-04
> **comment**: NA
- **标题**: 通过Hölder差异进行多视图表示学习的不确定性量化
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 基于循证的深度学习代表了不确定性估计的新兴范式，并提供可靠的预测，并具有可靠的额外计算开销。现有方法通常采用kullback-leibler差异来估计网络预测的不确定性，忽略了各种方式之间的域间隙。为了解决这个问题，本文介绍了一种基于HölderDivergence（HD）的新颖算法，以通过解决不完整或嘈杂数据固有的不确定性挑战来提高多视图学习的可靠性。通常，我们的方法通过并行网络分支提取多种模式的表示，然后采用HD来估计预测不确定性。通过Dempster-Shafer理论，不同方式的不确定性整合，从而产生了考虑所有可用表示形式的全面结果。从数学上讲，HD被证明可以更好地衡量模型的实际数据分布和预测分布之间的``距离''，并改善了多级识别任务的性能。具体而言，我们的方法超过了所有评估基准的现有最新方法。我们进一步在不同的骨干上进行了广泛的实验，以验证我们的出色鲁棒性。证明我们的方法成功地推动了相应的性能边界。最后，我们对更具挑战性的场景进行实验，\ textit {i.e。}，使用不完整或嘈杂的数据学习，表明我们的方法对这种损坏的数据具有很高的耐受性。

### EEG-based Multimodal Representation Learning for Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2411.00822)] [[cool](https://papers.cool/arxiv/2411.00822)] [[pdf](https://arxiv.org/pdf/2411.00822)]
> **Authors**: Kang Yin,Hye-Bin Shin,Dan Li,Seong-Whan Lee
> **First submission**: 2024-10-28
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 基于脑电图的多模式表示情感识别
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **摘要**: 多模式学习一直是研究的流行领域，但是整合脑电图（EEG）数据由于其固有的可变性和有限的可用性而带来了独特的挑战。在本文中，我们介绍了一个新颖的多模式框架，该框架不仅适合传统的模式，例如视频，图像和音频，而且还包含了脑电图数据。我们的框架旨在灵活处理不同的输入尺寸，同时动态调整注意力以说明跨模式的特征重要性。我们在最近引入的情绪识别数据集上评估了我们的方法，该数据集结合了三种模式的数据，使其成为多模式学习的理想测试。实验结果为数据集提供了基准，并证明了所提出的框架的有效性。这项工作突出了将脑电图整合到多模式系统中的潜力，为在情感识别及其他方面的更强大而全面的应用铺平了道路。

### GameGen-X: Interactive Open-world Game Video Generation 
[[arxiv](https://arxiv.org/abs/2411.00769)] [[cool](https://papers.cool/arxiv/2411.00769)] [[pdf](https://arxiv.org/pdf/2411.00769)]
> **Authors**: Haoxuan Che,Xuanhua He,Quande Liu,Cheng Jin,Hao Chen
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: Homepage: https://gamegen-x.github.io/ Github: https://github.com/GameGen-X/GameGen-X
- **标题**: GameGen-X：交互式开放世界游戏视频生成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们介绍GameGen-X，这是专为生成和交互式控制开放世界游戏视频而设计的第一个扩散变压器模型。该模型通过模拟广泛的游戏引擎功能，例如创新角色，动态环境，复杂的动作和多样化的事件来促进高质量的开放域生成。此外，它提供了交互性可控性，根据当前剪辑预测和更改未来内容，从而允许进行游戏模拟。为了实现这一愿景，我们首先从头开始收集并构建了一个开放世界的视频游戏数据集。它是开放世界游戏视频生成和控制的第一个也是最大的数据集，该数据集由150多个游戏中的一百万个不同的游戏剪辑进行了播放，并带有GPT-4O的信息标题。 GameGen-X经历了两个阶段的训练过程，包括基础模型预训练和指导调整。首先，该模型是通过文本到视频的生成和视频延续进行预训练的，从而赋予了长期，高质量的开放式游戏视频生成的能力。此外，为了实现交互性可控性，我们设计了指令网以合并与游戏相关的多模式控制信号专家。这允许模型根据用户输入调整潜在表示，在视频生成中首次统一角色交互和场景内容控制。在指令调整过程中，仅在冻结预训练的基础模型的同时更新指令网，从而可以集成交互式可控性，而不会丧失多样性和生成的视频内容的质量。

### TaxaBind: A Unified Embedding Space for Ecological Applications 
[[arxiv](https://arxiv.org/abs/2411.00683)] [[cool](https://papers.cool/arxiv/2411.00683)] [[pdf](https://arxiv.org/pdf/2411.00683)]
> **Authors**: Srikumar Sastry,Subash Khanal,Aayush Dhakal,Adeel Ahmad,Nathan Jacobs
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: Accepted to WACV 2025
- **标题**: Taxabind：生态应用的统一嵌入空间
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 我们介绍了税收，这是一个统一的嵌入空间，用于表征任何感兴趣的物种。 Taxabind是跨六个方式的多模式嵌入空间：物种的地面图像，地理位置，卫星图像，文本，音频和环境特征，可用于解决生态问题。为了学习这个关节嵌入空间，我们利用物种的地面图像作为结合方式。我们提出了多模式补丁，这是一种有效地将知识从各种方式提炼成结合方式的技术。我们构建了两个大型数据集用于预处理：带有物种图像和卫星图像的ISATNAT，以及带有物种图像和音频的Isoundnat。此外，我们介绍了Dasxabench-8K，这是一个多种模式数据集，具有六个配对模式，用于评估有关生态任务的深度学习模型。进行税押实验证明了其在一系列任务上的强大零射击和紧急功能，包括物种分类，跨模型检索和音频分类。数据集和模型可在https://github.com/mvrl/taxabind上找到。

### GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2411.00340)] [[cool](https://papers.cool/arxiv/2411.00340)] [[pdf](https://arxiv.org/pdf/2411.00340)]
> **Authors**: Xiaotian Li,Baojie Fan,Jiandong Tian,Huijie Fan
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: gafusion：自适应融合激光雷达和相机，并带有多个指导3D对象检测的指导
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，基于Bird's-eye-View（BEV）观点的3D多模式对象检测方法的显着进步。但是，他们中的大多数忽略了LiDAR和相机之间的互补互动和指导。在这项工作中，我们提出了一种新型的多模式3D反对检测方法，称为gafusion，并以激光雷达引导的全局相互作用和适应性融合。具体而言，我们引入了稀疏深度指南（SDG）和激光雷达占用指南（LOG），以生成具有足够深度信息的3D功能。在下文中，开发了激光雷达引导的自适应融合变压器（LGAFT），以从全局的角度适应性地增强不同模态BEV特征的相互作用。同时，具有稀疏高度压缩和多尺度双路线变压器（MSDPT）的额外倒数采样旨在扩大不同模态特征的接受场。最后，引入了一个时间融合模块，以从以前的框架中进行聚合特征。 Gafusion以73.6 $ \％$地图和74.9 $ \％$ nds在Nuscenes测试集中实现最新的3D对象检测结果。

### Unified Generative and Discriminative Training for Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.00304)] [[cool](https://papers.cool/arxiv/2411.00304)] [[pdf](https://arxiv.org/pdf/2411.00304)]
> **Authors**: Wei Chow,Juncheng Li,Qifan Yu,Kaihang Pan,Hao Fei,Zhiqi Ge,Shuai Yang,Siliang Tang,Hanwang Zhang,Qianru Sun
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 多模式大语言模型的统一生成和判别培训
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 最近，视觉模型（VLM）已在两个主要范式下进行了训练。生成培训使多模式大语言模型（MLLM）能够应对各种复杂的任务，但诸如幻觉和弱物体歧视之类的问题持续存在。诸如剪辑之类的模型以零拍的图像文本分类和检索为例，但在需要精细粒度语义差异的复杂场景中挣扎，以零拍的图像分类和检索为例，以示例性的训练为例。本文通过提出一种整合两个范式优势的统一方法来解决这些挑战。将交织的图像文本序列视为输入样品的一般格式，我们引入了结构引起的训练策略，该策略在输入样本与MLLM的隐藏状态之间实现了语义关系。这种方法增强了MLLM捕获全球语义并区分细粒语义的能力。通过利用动态序列对齐方式在动态时间扭曲框架内并集成了新颖的内核来进行细粒度的语义差异化，我们的方法有效地平衡了生成性和歧视性任务。广泛的实验证明了我们方法的有效性，实现最先进的实验会导致多个生成任务，尤其是那些需要认知和歧视能力的任务。此外，我们的方法超过了交错且细粒的检索任务中的歧视基准。通过采用检索提升的生成策略，我们的方法进一步增强了一个模型中某些生成任务的性能，为视觉模型中的未来研究提供了有希望的方向。

### ResiDual Transformer Alignment with Spectral Decomposition 
[[arxiv](https://arxiv.org/abs/2411.00246)] [[cool](https://papers.cool/arxiv/2411.00246)] [[pdf](https://arxiv.org/pdf/2411.00246)]
> **Authors**: Lorenzo Basile,Valentino Maiorca,Luca Bortolussi,Emanuele Rodolà,Francesco Locatello
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 残留的变压器对准与光谱分解
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 当通过其残留流的镜头进行检查时，在变压器网络中出现了令人困惑的属性：剩余贡献（例如，注意力头）有时专门研究特定任务或输入属性。在本文中，我们在视觉变压器中分析了这种现象，重点是残留物的光谱几何形状，并探索了其对视觉模型中模态对齐的影响。首先，我们将其链接到视觉头表示的本质上低维结构，将其缩小到其主要组件中，并表明它们在各种输入数据分布中编码专业角色。然后，我们分析了多模式模型中头部专业化的效果，重点介绍了文本和专业头部之间的改进对齐方式如何影响零拍的性能。这种专业化 - 性能链接始终构成各种预训练数据，网络大小和目标，这表明了一种有力的新机制，可以通过有针对性的对准来增强零射击分类。最终，我们通过引入残留物（一种用于残差流的光谱对齐的技术）将这些见解转化为可行的术语。就像黄金的平移一样，它可以使单位主组件（即属性）的噪音洗净以扩大与任务相关的噪声。值得注意的是，这种对模态对准的双重视角在不同的数据分布上产生了微调级别的性能，同时对极其可解释的参数有效转换进行建模，因为我们在50多个（预训练的网络，数据集）对上广泛显示。

### DDFAV: Remote Sensing Large Vision Language Models Dataset and Evaluation Benchmark 
[[arxiv](https://arxiv.org/abs/2411.02733)] [[cool](https://papers.cool/arxiv/2411.02733)] [[pdf](https://arxiv.org/pdf/2411.02733)]
> **Authors**: Haodong Li,Haicheng Qu,Xiaofeng Zhang
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: DDFAV：遥感大型视觉语言模型数据集和评估基准测试
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着大型视觉语言模型（LVLM）的快速发展，这些模型在各种多模式任务中表现出了出色的结果。由于LVLM容易出现幻觉，目前很少有专门设计用于遥感的数据集和评估方法，因此将其应用于遥感任务时的性能通常很差。为了解决这些问题，本文介绍了使用数据增强和数据混合策略创建的高质量遥感LVLMS数据集DDFAV。接下来，基于从建议的数据集中选择的一些高质量遥感图像制作培训指令集。最后，我们基于提出的数据集开发了遥感LVLMS幻觉评估方法Rspope，并评估不同LVLM的零发功能。我们提出的数据集，指令集和评估方法文件可在https://github.com/haodongli2024/rspope上找到。

### Multi-Transmotion: Pre-trained Model for Human Motion Prediction 
[[arxiv](https://arxiv.org/abs/2411.02673)] [[cool](https://papers.cool/arxiv/2411.02673)] [[pdf](https://arxiv.org/pdf/2411.02673)]
> **Authors**: Yang Gao,Po-Chien Luan,Alexandre Alahi
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: CoRL 2024
- **标题**: 多转移：人类运动预测的预训练模型
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 智能系统预测人类行为的能力至关重要，尤其是在自动驾驶汽车导航和社会机器人技术等领域。但是，人类运动的复杂性阻止了用于人类运动预测的标准化数据集的开发，从而阻碍了预训练的模型的建立。在本文中，我们通过整合多个数据集（包括轨迹和3D姿势关键点）来解决这些局限性，以提出一个预先训练的人类运动预测模型。我们合并了七个不同的数据集跨不同的方式，并标准化其格式。为了促进多模式的预训练，我们引入了多种趋势，这是一种基于创新的变压器模型，旨在跨模式预训练。此外，我们提出了一种新颖的掩盖策略来捕获丰富的代表。我们的方法证明了各个数据集的竞争性能在几个下游任务上，包括NBA和JTA数据集中的轨迹预测，以及Amass和3DPW数据集中的姿势预测。该代码公开可用：https：//github.com/vita-epfl/multi-transmotion

### Semantic-Aligned Adversarial Evolution Triangle for High-Transferability Vision-Language Attack 
[[arxiv](https://arxiv.org/abs/2411.02669)] [[cool](https://papers.cool/arxiv/2411.02669)] [[pdf](https://arxiv.org/pdf/2411.02669)]
> **Authors**: Xiaojun Jia,Sensen Gao,Qing Guo,Ke Ma,Yihao Huang,Simeng Qin,Yang Liu,Ivor Tsang Fellow,Xiaochun Cao
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 具有高转移性视觉攻击的语义对抗进化三角形三角形
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言预训练（VLP）模型在解释图像和文本方面表现出色，但仍然容易受到多模式对抗示例（AES）的影响。推进可转移的AE的生成，即在看不见的模型中取得成功，这是开发更健壮和实用的VLP模型的关键。先前的方法增强了图像文本对，以增强对抗性示例生成过程中的多样性，旨在通过扩大图像​​文本特征的对比度空间来提高可传递性。但是，这些方法仅着眼于当前AE的多样性，从而获得有限的可传递性增长。为了解决这个问题，我们建议通过在优化期间利用沿对抗轨迹的交叉区域来增加AE的多样性。具体而言，我们提出了由由干净，历史和当前对抗性示例组成的对抗进化三角形的取样，以增强对抗性的多样性。我们提供了理论分析，以证明所提出的对抗进化三角形的有效性。此外，我们发现冗余的非活性维度可以主导相似性计算，扭曲特征匹配并使AES模型依赖于可降低的可传递性。因此，我们建议在语义图像文本特征对比空间中生成AE，该空间可以将原始特征空间投射到语义语料库子空间中。所提出的语义对准子空间可以减少图像特征冗余，从而提高对抗性转移性。跨不同数据集和模型进行的广泛实验表明，所提出的方法可以有效地改善对抗性转移性，并表现优于最先进的对抗攻击方法。该代码在https://github.com/jiaxiaojunqaq/sa-aet上发布。

### Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach 
[[arxiv](https://arxiv.org/abs/2411.02624)] [[cool](https://papers.cool/arxiv/2411.02624)] [[pdf](https://arxiv.org/pdf/2411.02624)]
> **Authors**: Minghao Ning,Yaodong Cui,Yufeng Yang,Shucheng Huang,Zhenan Liu,Ahmad Reza Alghooneh,Ehsan Hashemi,Amir Khajepour
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 通过连接的传感器节点增强室内移动性：一种实时，延迟感知的合作感方法
- **领域**: 计算机视觉和模式识别,人工智能,机器人技术
- **摘要**: 本文介绍了一种新型的实时，延迟感知的合作感知系统，专为在动态室内环境中运行的智能移动平台而设计。该系统包含一个多模式传感器节点的网络和一个集体为移动平台提供感知服务的中心节点。拟议的层次聚类考虑了扫描模式和基于接地功能的LIDAR相机融合可改善拥挤环境的节点感知。该系统还具有延迟感知的全局感知，以同步和跨节点汇总数据。为了验证我们的方法，我们介绍了室内行人跟踪数据集，该数据集是根据由两个室内传感器节点捕获的数据编译的。与基线相比，我们的实验表明，针对延迟的检测准确性和鲁棒性有了显着提高。该数据集可在存储库中提供：https：//github.com/ningmhao/mvslab-indoorcooperativeperceptionpection

### INQUIRE: A Natural World Text-to-Image Retrieval Benchmark 
[[arxiv](https://arxiv.org/abs/2411.02537)] [[cool](https://papers.cool/arxiv/2411.02537)] [[pdf](https://arxiv.org/pdf/2411.02537)]
> **Authors**: Edward Vendrow,Omiros Pantazis,Alexander Shepard,Gabriel Brostow,Kate E. Jones,Oisin Mac Aodha,Sara Beery,Grant Van Horn
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: Published in NeurIPS 2024, Datasets and Benchmarks Track
- **标题**: 查询：自然世界的文本对图像检索基准测试
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,信息检索
- **摘要**: 我们介绍了询问，这是一种文本到图像检索基准测试，旨在挑战专家级查询的多模式视觉模型。询问包括Inaturalist 2024（Inat24），这是一个新的500万自然世界图像的数据集，以及250个专家级的检索查询。这些查询与INAT24中全面标记的所有相关图像配对，包括33,000个匹配。查询涵盖物种识别，上下文，行为和外观等类别，强调需要细微的图像理解和领域专业知识的任务。我们的基准测试评估了两个核心检索任务：（1）查询 - 富勒克（Fullrank），完整的数据集排名任务，以及（2）询问rerank，这是精炼前100名检索的重新访问任务。对最近的一系列多模式的详细评估表明，查询构成了一个重大挑战，最佳模型未能在50％以上获得50％的地图。此外，我们表明，使用更强大的多峰模型重新骑行可以提高检索性能，但仍有很大的改进范围。通过专注于科学动机的生态挑战，询问旨在弥合AI能力和现实世界科学探究需求之间的差距，鼓励开发可以帮助加速生态和生物多样性研究的检索系统。我们的数据集和代码可从https://inquire-benchmark.github.io获得。

### Cross-D Conv: Cross-Dimensional Transferable Knowledge Base via Fourier Shifting Operation 
[[arxiv](https://arxiv.org/abs/2411.02441)] [[cool](https://papers.cool/arxiv/2411.02441)] [[pdf](https://arxiv.org/pdf/2411.02441)]
> **Authors**: Mehmet Can Yavuz,Yang Yang
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-05
> **comment**: Accepted by IEEE ISBI 2025 4-page paper
- **标题**: Cross-D Conv：通过傅立叶移动操作通过近距离转移知识库
- **领域**: 计算机视觉和模式识别
- **摘要**: 在生物医学成像分析中，2D和3D数据之间的二分法提出了重大挑战。虽然3D卷提供了卓越的现实世界适用性，但它们对于每种模式的可用性都较少，并且不容易大规模训练，而2D样品却丰富但不全面。本文介绍了Cross-D Conver操作，这是一种新颖的方法，它通过学习傅立叶域中的相位转移来弥合维度差距。我们的方法使2D和3D卷积操作之间的无缝重量转移有效地促进了跨二维学习。所提出的架构利用了大量的2D训练数据来增强3D模型性能，从而为3D医学模型预处理提供了多模式数据稀缺挑战的实用解决方案。对Radimagenet（2D）和多模式体积集的实验验证表明，我们的方法在功能质量评估中实现了可比或卓越的性能。增强的卷积操作为在医学成像中开发有效的分类和分割模型提供了新的机会。这项工作代表了跨维和多模式医学图像分析的进步，为在3D模型中使用2D先验提供了强大的框架，同时维持2D培训的计算效率。该代码可在https://github.com/convergedmachine/cross-d-conv上找到。

### Learning General-Purpose Biomedical Volume Representations using Randomized Synthesis 
[[arxiv](https://arxiv.org/abs/2411.02372)] [[cool](https://papers.cool/arxiv/2411.02372)] [[pdf](https://arxiv.org/pdf/2411.02372)]
> **Authors**: Neel Dey,Benjamin Billot,Hallee E. Wong,Clinton J. Wang,Mengwei Ren,P. Ellen Grant,Adrian V. Dalca,Polina Golland
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: ICLR 2025: International Conference on Learning Representations. Code and model weights available at https://github.com/neel-dey/anatomix. Keywords: synthetic data, representation learning, medical image analysis, image registration, image segmentation
- **标题**: 使用随机合成学习通用生物医学体积表示
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 当前的体积生物医学基础模型努力概括，因为公共3D数据集很小，并且不涵盖医疗程序，条件，解剖区域和成像协议的广泛多样性。我们通过创建一种表示的学习方法来解决这一问题，该方法可以预测训练时间本身的强大领域变化。我们首先提出了一个数据引擎，该数据引擎综合了高度可变的训练样本，该样本将使对新的生物医学环境进行概括。然后，为任何体素级任务训练单个3D网络，我们开发了一种对比度学习方法，该方法预告了网络以与数据引擎模拟的滋扰成像变化稳定，这是数据引擎的关键电感偏置。该网络的功能可以用作用于下游任务的输入图像的强大表示，其权重为新数据集上的填充提供了强大的数据集 /固定语言初始化。结果，我们在多模式注册和少量分段中设置了新的标准，这是任何3D生物医学视觉模型的第一个，所有这些都没有在任何现有的真实图像数据集中（预 - 进行）培训。

### One VLM to Keep it Learning: Generation and Balancing for Data-free Continual Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2411.02210)] [[cool](https://papers.cool/arxiv/2411.02210)] [[pdf](https://arxiv.org/pdf/2411.02210)]
> **Authors**: Deepayan Das,Davide Talon,Massimiliano Mancini,Yiming Wang,Elisa Ricci
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 一个可以保持学习的VLM：生成和平衡无数据的持续视觉问题回答
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLM）通过利用Web规模的多模式数据集在视觉问题回答（VQA）任务中显示出巨大的希望。但是，由于适应新任务时的灾难性遗忘，这些模型通常在不断学习中挣扎。作为减轻灾难性遗忘的有效补救措施，排练策略在学习新任务时使用了过去任务的数据。但是，这种策略迫切需要存储过去的数据，由于硬件约束或隐私问题，这可能是不可行的。在这项工作中，我们提出了第一个利用VLM语言生成能力而不是依靠外部模型的无数据的方法，以生成伪式删除数据来解决持续的VQA。我们的建议（称为GAB）通过在新任务数据上提出以前的任务问题来生成伪级别数据。然而，尽管有效，但由于有限和特定于任务的培训数据，生成的问题的分布偏向最常提出的问题。为了减轻此问题，我们引入了一个伪划分平衡模块，该模块使用问题元统计数据或无监督的聚类方法将生成的数据与地面真相数据分布保持一致。我们在两个最近的基准分析\ IE VQACL-VQAV2和丁香功能基准的基准上评估了我们的方法。 GAB的表现优于所有无数据的基准，在跨不断发展的任务中保持VQA性能方面有了很大的改进，同时又可以使用可以访问过去数据的方法。

### Multi-modal biometric authentication: Leveraging shared layer architectures for enhanced security 
[[arxiv](https://arxiv.org/abs/2411.02112)] [[cool](https://papers.cool/arxiv/2411.02112)] [[pdf](https://arxiv.org/pdf/2411.02112)]
> **Authors**: Vatchala S,Yogesh C,Yeshwanth Govindarajan,Krithik Raja M,Vishal Pramav Amirtha Ganesan,Aashish Vinod A,Dharun Ramesh
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: :F.2.2, I.2.7
- **标题**: 多模式生物识别身份验证：利用共享层架构以增强安全性
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在这项研究中，我们介绍了一种新型的多模式生物特征验证系统，该系统集成了面部，声音和签名数据以增强安全措施。利用卷积神经网络（CNN）和经常性神经网络（RNN）的组合，我们的模型体系结构独特地融合了双共享层以及特定于模态的增强功能，以进行全面的特征提取。该系统通过关节损失功能进行严格的训练，以优化各种生物识别输入的准确性。通过主成分分析（PCA）和通过梯度提升机（GBM）进行分类的特征级融合，进一步完善了身份验证过程。我们的方法表明了身份验证准确性和鲁棒性的显着提高，为先进的安全身份验证解决方案铺平了道路。

### SPECTRUM: Semantic Processing and Emotion-informed video-Captioning Through Retrieval and Understanding Modalities 
[[arxiv](https://arxiv.org/abs/2411.01975)] [[cool](https://papers.cool/arxiv/2411.01975)] [[pdf](https://arxiv.org/pdf/2411.01975)]
> **Authors**: Ehsan Faghihi,Mohammedreza Zarenejad,Ali-Asghar Beheshti Shirazi
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 频谱：通过检索和理解方式进行语义处理和情感信息的视频捕捉
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 通过分析微妙的细节来捕获视频的含义和关键概念是视频字幕中的一项基本而又具有挑战性的任务。在视频中识别主导的情感语调可显着增强其上下文的感知。尽管非常强调视频字幕，但现有模型通常需要充分解决情感主题，从而导致次优字幕结果。为了解决这些局限性，本文提出了一种新颖的语义处理和情感信息，通过检索和理解方式（Spectrum）框架来增强情感和语义上可信的字幕的能力。通过使用视觉文本属性调查（VTAI）来利用我们的开创性结构，可以辨别多模式语义和情感主题，并通过整体概念的主题（HCOT）来确定描述性字幕的方向，表达情感上的情感和现场成熟的参考文献。他们利用视频到文本检索功能和视频内容的多方面性质来估计候选字幕的情感概率。然后，视频的主要主题是通过适当加权嵌入式属性向量并应用粗糙和细粒度的情感概念来确定的，从而定义了视频的上下文对齐。此外，使用两个损失函数，优化了频谱以整合情绪信息并最大程度地减少预测错误。关于EMVIDCAP，MSVD和MSRVTT视频字幕数据集的广泛实验表明，我们的模型显着超过了最新方法。定量和定性评估突出了该模型准确捕获和传达视频情感和多模式属性的能力。

### KptLLM: Unveiling the Power of Large Language Model for Keypoint Comprehension 
[[arxiv](https://arxiv.org/abs/2411.01846)] [[cool](https://papers.cool/arxiv/2411.01846)] [[pdf](https://arxiv.org/pdf/2411.01846)]
> **Authors**: Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Ruimao Zhang
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: NeurIPS 2024
- **标题**: KPTLLM：揭示大型语言模型的力量以供关键点理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的最新进展极大地提高了它们在图像理解方面的能力。但是，这些模型通常在掌握像素级的语义细节（例如对象的关键点）方面遇到困难。为了弥合这一差距，我们介绍了语义关键点理解的新颖挑战，该挑战旨在理解各个任务场景的关键点，包括关键点语义识别，基于视觉及时及时及时及时的及时及时及时的及时及时及时的关键点检测。此外，我们介绍了KPTLLM，这是一个统一的多模式模型，它利用识别策略有效地应对这些挑战。 kptllm强调了Kepoints语义的初始识别，然后通过经过思考的过程来精确地确定其位置。 KPTLLM凭借几个精心设计的模块，可以很好地处理各种模态输入，从而促进了语义内容和关键点位置的解释。我们的广泛实验表明，KPTLLM在各种关键点检测基准中的优势及其在解释关键点时的独特语义功能。

### 3D Audio-Visual Segmentation 
[[arxiv](https://arxiv.org/abs/2411.02236)] [[cool](https://papers.cool/arxiv/2411.02236)] [[pdf](https://arxiv.org/pdf/2411.02236)]
> **Authors**: Artem Sokolov,Swapnil Bhosale,Xiatian Zhu
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: Accepted at the NeurIPS 2024 Workshop on Audio Imagination
- **标题**: 3D视听分段
- **领域**: 计算机视觉和模式识别,多媒体,声音,音频和语音处理
- **摘要**: 在体现AI中识别场景中的声音对象是一个长期的目标，在机器人技术和AR/VR/MR中具有不同的应用。为此，最近已经提出了视听分割（AVS），以音频信号为条件，以识别带有同步摄像头和麦克风传感器的输入图像中目标响料对象的掩模。但是，由于缺少从2D图像到3D场景的映射，该范式仍然不足以实现现实操作。为了解决这一基本限制，我们引入了一个新的研究问题，即3D音频视频分割，将现有的AV扩展到3D输出空间。由于摄像头外部散射，遮挡，遮挡和各种声学的声音类别的变化，此问题构成了更多挑战。为了促进这项研究，我们创建了第一个基于仿真的基准标准3DAVS-S34-O7，在单个固定和多种设备设置下，在34个场景和7个对象类别中，在单一构想和多实体设置下提供了接地的空间音频。通过重新塑造栖息地模拟器来生成对物体位置和相应的3D蒙版的全面注释，这是可能的。随后，我们提出了一种新方法，即Echosegnet，其特征是从鉴定的2D音频视频基础基础模型中整合了从空间音频吸引掩码的掩盖和改进中与3D视觉场景表示相结合的现成知识。广泛的实验表明，Echosegnet可以有效地在我们新基准的3D空间中分段探测对象，这代表了体现AI领域的显着进步。项目页面：https：//surrey-uplab.github.io/research/3d-audio-visual-segmentation/

### Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All Lighting Conditions 
[[arxiv](https://arxiv.org/abs/2411.03638)] [[cool](https://papers.cool/arxiv/2411.03638)] [[pdf](https://arxiv.org/pdf/2411.03638)]
> **Authors**: Zihan Qin,Jialei Xu,Wenbo Zhao,Junjun Jiang,Xianming Liu
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: No comments
- **标题**: 在所有照明条件下具有多光谱图像的自适应立体深度估计
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在不利条件下的深度估计仍然是一个重大挑战。最近，整合了可见光和热图像的多光谱深度估计在解决此问题方面已显示出希望。但是，现有的算法在精确的像素级特征匹配方面遇到了困难，从而限制了它们完全利用不同光谱的几何约束的能力。为了解决这个问题，我们提出了一个新的框架，其中包含立体深度估计，以执行准确的几何约束。特别是，我们将可见光和热图像视为立体对，并利用跨模式特征匹配（CFM）模块来构建像素级匹配的成本量。为了减轻较差的照明对立体声匹配的影响，我们引入了降解掩蔽，该降解掩盖了降解区域中稳健的单眼热深度估计。我们的方法在多光谱立体声（MS2）数据集上实现了最新的（SOTA）性能，并进行了定性评估，证明了在不同的照明条件下的高质量深度图。

### StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding 
[[arxiv](https://arxiv.org/abs/2411.03628)] [[cool](https://papers.cool/arxiv/2411.03628)] [[pdf](https://arxiv.org/pdf/2411.03628)]
> **Authors**: Junming Lin,Zheng Fang,Chi Chen,Zihao Wan,Fuwen Luo,Peng Li,Yang Liu,Maosong Sun
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: No comments
- **标题**: 溪流板：评估MLLM的差距以实现流视频理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式大语言模型（MLLM）的快速发展已将其功能从图像理解到视频理解扩展。但是，这些MLLM的大多数主要集中在离线视频理解上，需要在进行所有查询之前进行大量处理。与人类观看，倾听，思考和对流媒体输入的能力相比，这有一个很大的差距，突出了当前MLLM的局限性。在本文中，我们介绍了Streamingbench，这是第一个旨在评估MLLM的流视频理解功能的全面基准。 StreamingBench评估了流视频理解的三个核心方面：（1）实时视觉理解，（2）Omni-Source理解，以及（3）上下文理解。该基准由18个任务组成，其中包含900个视频和4,500个人类策划的QA对。每个视频都有在不同时间点提出的五个问题，以模拟连续的流媒体场景。我们使用13个开源和专有的MLLM在流台台上进行实验，发现即使是Gemini 1.5 Pro和GPT-4O（GPT-4O）等最先进的专有MLLM，其性能明显低于人级流媒体视频理解能力。我们希望我们的工作能够促进MLLM的进一步进步，使他们能够在更现实的情况下与人类级别的视频理解和互动接触。

### Personalized Video Summarization by Multimodal Video Understanding 
[[arxiv](https://arxiv.org/abs/2411.03531)] [[cool](https://papers.cool/arxiv/2411.03531)] [[pdf](https://arxiv.org/pdf/2411.03531)]
> **Authors**: Brian Chen,Xiangyuan Zhao,Yingnan Zhu
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: In Proceedings of CIKM 2024 Applied Research Track
- **标题**: 通过多模式视频理解的个性化视频摘要
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视频摘要技术已被证明可以改善访问和理解视频内容的总体用户体验。如果已知用户的喜好，则视频摘要可以从输入视频中识别出重要的信息或相关内容，从而帮助他们获取必要的信息或确定其观看原始视频的兴趣。将视频摘要适应各种类型的视频和用户偏好需要大量的培训数据和昂贵的人类标签。为了促进此类研究，我们为视频摘要提出了一个新的基准测试，以捕获各种用户的喜好。此外，我们提出了一条名为“视频摘要”的管道，该管道使用语言（VSL），用于用户偏爱的视频摘要，该视频摘要基于预先训练的视觉语言模型（VLMS），以避免在大型培训数据集中训练视频摘要系统。该管道将​​视频和封闭字幕作为输入进行，并通过将视频帧转换为文本，在场景级别执行语义分析。随后，用户的类型偏好被用作选择相关文本场景的基础。实验结果表明，我们提出的管道的表现优于当前无监督的视频摘要模型。我们表明，与基于监督查询的视频摘要模型相比，我们的方法在不同数据集中更适合自适应。最后，运行时分析表明，在扩展用户偏好和视频的数量时，我们的管道更适合实际使用。

### Unlocking the Archives: Using Large Language Models to Transcribe Handwritten Historical Documents 
[[arxiv](https://arxiv.org/abs/2411.03340)] [[cool](https://papers.cool/arxiv/2411.03340)] [[pdf](https://arxiv.org/pdf/2411.03340)]
> **Authors**: Mark Humphries,Lianne C. Leddy,Quinn Downton,Meredith Legace,John McConnell,Isabella Murray,Elizabeth Spence
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-06
> **comment**: 29 Pages, 11 Tables, 2 Figures
- **标题**: 解锁档案：使用大型语言模型转录手写的历史文档
- **领域**: 计算机视觉和模式识别,计算语言学,数字图书馆,机器学习
- **摘要**: 这项研究表明，大型语言模型（LLMS）可以比专业手写文本识别（HTR）软件更高的精度转录历史手写文档，同时更快，更具成本效益。我们介绍了一个名为Transcription Pearl的开源软件工具，该工具利用这些功能使用OpenAI，Anthropic和Google的市售多模式LLMS自动转录和纠正手写文档的批次。在对18/19世纪英语语言手写文档的多样化语料库的测试中，LLMS的角色错误率（CER）为5.7至7％，单词错误率（WER）为8.9％至15.9％，分别提高了14％和32％，超过了特殊的Transkribus（例如Transkribus）的专业最先进的HTR HTR软件。最重要的是，当使用LLMS纠正这些转录以及传统HTR软件生成的文本时，它们达到了近乎人类的准确性，即CERS低至1.8％，WERS且WERS为3.5％。 LLM的完成这些任务的速度还快了50倍，大约是专有HTR计划成本的1/50。这些结果表明，当将LLMS纳入转录珍珠等软件工具中时，它们为历史手写文档的大规模转录提供了一种易于访问，快速且高度准确的方法，从而显着简化了数字化过程。

### MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning 
[[arxiv](https://arxiv.org/abs/2411.03314)] [[cool](https://papers.cool/arxiv/2411.03314)] [[pdf](https://arxiv.org/pdf/2411.03314)]
> **Authors**: Ziliang Gan,Yu Lu,Dong Zhang,Haohan Li,Che Liu,Jian Liu,Ji Liu,Haipang Wu,Chaoyou Fu,Zenglin Xu,Rongjunchen Zhang,Yong Dai
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: Project Page: https://hithink-research.github.io/MME-Finance/
- **标题**: MME金融：用于专家级别理解和推理的多模式金融基准
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 近年来，通用域的多模式基准指导了一般任务上多模型的快速发展。但是，金融领域具有其特殊性。它具有独特的图形图像（例如，烛台图表，技术指标图表），并具有丰富的专业财务知识（例如，期货，营业额）。因此，一般领域的基准通常无法衡量金融领域中多模型的性能，因此无法有效地指导大型财务模型的快速发展。为了促进大型财务多模型的开发，我们提出了MME Finance，这是一种双语开放式和实用用法的视觉问题回答（VQA）基准。我们的基准的特征是财务和专业知识，其中包括构建图表，这些图表反映了用户的实际使用需求（例如，计算机屏幕截图和移动摄影），根据金融领域查询的偏好以及在金融行业有10年以上经验的专家的问题来提出问题。此外，我们已经开发了一个定制设计的财务评估系统，其中首先在多模式评估过程中引入视觉信息。对19个主流MLLM进行了广泛的实验评估，以测试其感知，推理和认知能力。结果表明，在一般基准上表现良好的模型在MME金融上不能很好地表现；例如，表现最佳的开源和封闭源模型分别获得65.69（QWEN2VL-72B）和63.18（GPT-4O）。他们的性能在与金融最相关的类别中特别较差，例如烛台图表和技术指标图表。此外，我们提出了一个中文版本，这有助于比较中文背景下的MLLM的性能。

### Precise Drive with VLM: First Prize Solution for PRCV 2024 Drive LM challenge 
[[arxiv](https://arxiv.org/abs/2411.02999)] [[cool](https://papers.cool/arxiv/2411.02999)] [[pdf](https://arxiv.org/pdf/2411.02999)]
> **Authors**: Bin Huang,Siyu Wang,Yuanpeng Chen,Yidan Wu,Hui Song,Zifan Ding,Jing Leng,Chengpeng Liang,Peng Xue,Junliang Zhang,Tiankun Zhao
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: No comments
- **标题**: 使用VLM的精确驱动：PRCV 2024 Drive LM Challenge的一等奖解决方案
- **领域**: 计算机视觉和模式识别
- **摘要**: 该技术报告概述了我们针对PRCV挑战的方法，重点是驾驶场景中的认知和决策。我们采用了InternVL-2.0，这是一种开创性的开源多模式模型，并通过完善模型输入和培训方法来增强它。对于输入数据，我们从战略上串联并格式化了多视图图像。值得一提的是，我们在没有转换的情况下利用了原始图像的坐标。在模型培训方面，我们最初对公开可用的自动驾驶方案数据集进行了预培训，以增强其对挑战任务的一致性功能，然后在Drivelm-Nuscenes数据集中进行微调。在微调阶段，我们创新地修改了损耗函数，以增强模型在预测坐标值时的精度。这些方法确保我们的模型在驱动方案中具有先进的认知和决策能力。因此，我们的模型的得分为0.6064，在比赛的最终成绩上获得了第一名。

### Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2411.02969)] [[cool](https://papers.cool/arxiv/2411.02969)] [[pdf](https://arxiv.org/pdf/2411.02969)]
> **Authors**: Xavier Timoneda,Markus Herb,Fabian Duerr,Daniel Goehring,Fisher Yu
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024
- **标题**: 雷达语义分段的多模式NERF自upervision
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: LiDAR语义分割是自主驾驶感知的一项基本任务，包括将每个LiDAR点与语义标签相关联。完全监督的模型已广泛解决了这项任务，但是它们需要每次扫描的标签，这要么限制其域，要么需要不切实际的昂贵注释。相机图像通常与LiDAR PointCloud一起记录，可以通过广泛可用的2D基础模型来处理，这些模型是通用和数据集 - 静态的。但是，从2D数据中提取知识以改善激光雷达的感知会增加域的适应性挑战。例如，经典的透视投影受到了在两个传感器各自捕获时间之间位置移动产生的视差效应。我们提出了一个半监督的学习设置，以利用无标记的激光云点以及来自相机图像的蒸馏知识。为了在未标记的扫描上自我避免我们的模型，我们添加了一个辅助NERF头，并通过未标记的Voxel功能从相机视点铸造射线。 NERF头预测每个采样射线位置的密度和语义逻辑，用于渲染像素语义。同时，我们使用相机图像查询各个部分的基础模型（SAM）基础模型以生成一组未标记的通用掩码。我们将口罩与来自LiDAR的渲染像素语义融合在一起，以产生伪标记，以监督像素预测。在推断期间，我们放下NERF头，仅使用LiDAR运行模型。我们在三个公共激光雷达语义分割基准中展示了方法的有效性：Nuscenes，Semantickitti和Scribblekitti。

### Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization 
[[arxiv](https://arxiv.org/abs/2411.02920)] [[cool](https://papers.cool/arxiv/2411.02920)] [[pdf](https://arxiv.org/pdf/2411.02920)]
> **Authors**: Pengkun Jiao,Na Zhao,Jingjing Chen,Yu-Gang Jiang
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: TMM 2024
- **标题**: 开源单源域概括的域扩展和边界增长
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 开放键单源域的概括旨在使用单源域来学习一个可靠的模型，该模型可以推广到具有域移位和标签偏移的未知目标域。源域的稀缺性和目标域的未知数据分布对域不变特征学习和未知类识别构成了巨大的挑战。在本文中，我们提出了一种基于领域扩展和边界增长的新型学习方法，以扩大稀缺的来源样本并扩大已知类别的边界，以间接扩大已知类别和未知类别之间的边界。具体而言，我们通过对源数据上的背景抑制和样式增强来综合新样本来实现域扩展。然后，我们强迫模型从合成样本中提取一致的知识，以便模型可以学习域不变信息。此外，我们通过使用边缘图作为训练多二进制分类器时的样品的附加方式来实现各个类别的边界增长。通过这种方式，它扩大了嵌入式和离群值之间的边界，因此在开放设定的概括过程中改善了未知的类识别。广泛的实验表明，我们的方法可以在几个跨域图像分类数据集上实现重大改进并达到最先进的性能。

### Membership Inference Attacks against Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.02902)] [[cool](https://papers.cool/arxiv/2411.02902)] [[pdf](https://arxiv.org/pdf/2411.02902)]
> **Authors**: Zhan Li,Yongtao Wu,Yihang Chen,Francesco Tonin,Elias Abad Rocamora,Volkan Cevher
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: NeurIPS 2024
- **标题**: 对大型视觉模型的会员推断攻击
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,密码学和安全,机器学习
- **摘要**: 大型视觉模型（VLLM）具有在各种应用程序方面处理多模式任务的有希望的功能。但是，鉴于他们的培训数据集中可能包含敏感信息，例如私人照片和病历，例如私人照片和病历等敏感信息，它们的出现也引起了重大的数据安全问题。在VLLMS中检测不当使用的数据仍然是一个关键且尚未解决的问题，这主要是由于缺乏标准化数据集和合适的方法。在这项研究中，我们介绍了针对各种VLLM量身定制的首次会员推理攻击（MIA）基准，以促进培​​训数据检测。然后，我们提出了一种专门为令牌级图像检测而设计的新型MIA管道。最后，我们提出了一个名为Maxrényi-k％的新指标，该指标基于模型输出的置信度，并适用于文本和图像数据。我们认为，我们的工作可以加深对VLLM背景下MIA的理解和方法。我们的代码和数据集可在https://github.com/lions-epfl/vl-mia上找到。

### H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.04077)] [[cool](https://papers.cool/arxiv/2411.04077)] [[pdf](https://arxiv.org/pdf/2411.04077)]
> **Authors**: Nhi Pham,Michael Schott
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: Poster at https://sites.google.com/berkeley.edu/bb-stat/home
- **标题**: H-Pope：基于分层的投票探测评估大型视觉模型中的幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过利用文本和图像，大型视觉语言模型（LVLM）在各种多模式任务中显示出显着的进展。然而，这些模型通常会遭受幻觉的困扰，例如，它们在视觉输入和文本输出之间表现出不一致。为了解决这个问题，我们提出了H-Pope，这是一种粗到五的基准，该基准系统地评估了对象存在和属性中的幻觉。我们的评估表明，模型很容易幻觉，甚至是对细粒属性的幻觉。我们进一步研究这些模型是否依赖于视觉输入来制定输出文本。

### ReEdit: Multimodal Exemplar-Based Image Editing with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2411.03982)] [[cool](https://papers.cool/arxiv/2411.03982)] [[pdf](https://arxiv.org/pdf/2411.03982)]
> **Authors**: Ashutosh Srivastava,Tarun Ram Menta,Abhinav Java,Avadhoot Jadhav,Silky Singh,Surgan Jandial,Balaji Krishnamurthy
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: First three authors contributed equally to this work
- **标题**: Reddit：具有扩散模型的基于多模式的基于示例的图像编辑
- **领域**: 计算机视觉和模式识别
- **摘要**: 现代文本对图像（T2I）扩散模型通过实现高质量的影像图像来彻底改变了图像编辑。尽管使用T2I模型进行编辑的事实上的方法是通过文本说明，但由于自然语言和图像之间的复杂多一到多的映射，这种方法是非平凡的。在这项工作中，我们解决了基于示例的图像编辑 - 将编辑从示例对传输到内容映像的任务。我们提出了Reedit，这是一种模块化，高效的端到端框架，可以捕获文本和图像模式中的编辑，同时确保编辑的图像的忠诚度。我们通过与最先进的基线和关键设计选择的敏感性分析来验证Reedit的有效性。我们的结果表明，Reedit在定性和定量上始终优于当代方法。此外，Reedit具有很高的实际适用性，因为它不需要任何特定于任务的优化，并且比下一个最佳基线要快四倍。

### Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination 
[[arxiv](https://arxiv.org/abs/2411.03823)] [[cool](https://papers.cool/arxiv/2411.03823)] [[pdf](https://arxiv.org/pdf/2411.03823)]
> **Authors**: Dingjie Song,Sicheng Lai,Shunian Chen,Lichao Sun,Benyou Wang
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect
- **标题**: 文本和图像都泄漏了！多模式LLM数据污染的系统分析
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体
- **摘要**: 多模式大语言模型（MLLM）的快速发展已在各种多模式基准上表现出卓越的性能。但是，培训期间的数据污染问题在绩效评估和比较中引起了挑战。尽管存在许多用于检测大语模型（LLMS）污染的方法，但由于其各种方式和多个训练阶段，它们对MLLM的有效性较小。在这项研究中，我们介绍了为MLLM设计的多模式数据污染检测框架MM-DECT。我们的实验结果表明，MM-DETECT在识别不同程度的污染中非常有效且敏感，并且由于多模式基准训练集的泄漏，可以突出显示出重大的性能改善。此外，我们探索污染是否源自MLLM使用的基本LLM或多模式训练阶段，从而为引入污染的阶段提供了新的见解。

### VQA$^2$: Visual Question Answering for Video Quality Assessment 
[[arxiv](https://arxiv.org/abs/2411.03795)] [[cool](https://papers.cool/arxiv/2411.03795)] [[pdf](https://arxiv.org/pdf/2411.03795)]
> **Authors**: Ziheng Jia,Zicheng Zhang,Jiaying Qian,Haoning Wu,Wei Sun,Chunyi Li,Xiaohong Liu,Weisi Lin,Guangtao Zhai,Xiongkuo Min
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: 23 pages 12 figures
- **标题**: VQA $^2 $：视觉质量评估的视觉问题回答
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型多模式模型（LMM）的出现和扩散已引入了计算机视觉的新范式，将各种任务转换为统一的视觉问题答案框架。视频质量评估（VQA）是一个低级视觉感知的经典领域，最初集中在定量视频质量评分上。但是，在LMMS的进步驱动下，它正在朝着更全面的视觉质量理解任务发展。图像域中的最新研究表明，视觉问题回答（VQA）可以显着增强低级视觉质量评估。但是，在视频域中尚未探索相关工作，留下了很大的改进空间。为了解决此差距，我们介绍了VQA2指令数据集 - 第一个视觉问题回答指令数据集，该数据集侧重于视频质量评估。该数据集由3个子集组成，涵盖了各种视频类型，其中包含157,755个指令提问 - 答案对。然后，利用这个基础，我们提出了VQA2系列模型。 VQA2系列模型交织了视觉和运动令牌，以增强视频中时空质量细节的感知。我们对视频质量评分和理解任务进行了广泛的实验，结果表明，VQA2系列模型在这两个任务中都具有出色的性能。值得注意的是，我们的最终模型，即VQA2辅助因素，在视觉质量理解任务中超过了著名的GPT-4O，同时在质量得分任务中保持了强大的竞争力。我们的工作为将低级视频质量评估和与LMM的理解整合在一起提供了基础和可行的方法。

### Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2411.03702)] [[cool](https://papers.cool/arxiv/2411.03702)] [[pdf](https://arxiv.org/pdf/2411.03702)]
> **Authors**: Depanshu Sani,Saket Anand
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: An extended abstract accepted at Young Researchers' Symposium, ICVGIP '24. This extended abstract contains the following: 1. Short summary of our work, SAGA-KF, accepted at ICPR'24. 2. A proposal that was awarded the Qualcomm Innovation Fellowship'24
- **标题**: 基于图的多模式传感器融合用于自动驾驶
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 在移动机器人技术和自动驾驶中对强大场景理解的需求不断增长，这突显了整合多种感应方式的重要性。通过将来自相机和激光镜等不同传感器的数据结合在一起，融合技术可以克服单个传感器的局限性，从而使对环境有更完整，更准确的感知。我们引入了一种新型的多模式传感器融合方法，重点是开发基于图的状态表示，该状态表示支持自主驾驶中的关键决策过程。我们提出了一种传感器无形图形感知的卡尔曼滤波器[3]，这是第一个在线状态估计技术，旨在融合从嘈杂的多传感器数据中得出的多模式图。估计的基于图形的状态表示为高级应用程序（例如多对象跟踪（MOT））的基础，为增强自主系统的情境意识和安全提供了全面的框架。我们通过对合成和实际驾驶数据集（Nuscenes）进行的广泛实验来验证我们提出的框架的有效性。我们的结果显示了使用SAGA-KF的跟踪对象的MOTA改进以及估计位置误差（MOTP）和身份开关（IDS）的改进。此外，我们强调了这样一个框架的能力，可以从各种感应方式中利用可以利用异质信息（例如语义对象和几何结构）的方法，从而使场景理解并增强自主系统的安全性和有效性的更全面方法。

### ZOPP: A Framework of Zero-shot Offboard Panoptic Perception for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2411.05311)] [[cool](https://papers.cool/arxiv/2411.05311)] [[pdf](https://arxiv.org/pdf/2411.05311)]
> **Authors**: Tao Ma,Hongbin Zhou,Qiusheng Huang,Xuemeng Yang,Jianfei Guo,Bo Zhang,Min Dou,Yu Qiao,Botian Shi,Hongsheng Li
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Accepted by NeurIPS 2024
- **标题**: ZOPP：一个自动驾驶的零射击板式全面感知的框架
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: Offboard感知旨在自动生成用于自动驾驶（AD）场景的高质量3D标签。现有的卸货方法着重于使用封闭式分类法检测的3D对象检测，并且在快速发展的感知任务上无法匹配人级识别能力。由于非常依赖人类标签以及数据不平衡和稀疏性的流行，这是一个在广告场景中自动标记各种元素的统一框架，尚未完全探索满足感知任务的独特需求的各种元素。在本文中，我们提出了一种新型的多模式零射击板式式倾斜感知（ZOPP）框架，用于自主驾驶场景。 ZOPP集成了视觉基础模型的强大零射击识别能力和从点云得出的3D表示。据我们所知，ZOPP代表了在自主驾驶场景的多模式全景感知和自动标签领域的开创性努力。我们对Waymo开放数据集进行了全面的经验研究和评估，以验证拟议的ZOPP各种感知任务。为了进一步探索我们提出的ZOPP的可用性和可扩展性，我们还在下游应用中进行了实验。结果进一步证明了我们的ZOPP对现实情况的巨大潜力。

### Hierarchical Visual Feature Aggregation for OCR-Free Document Understanding 
[[arxiv](https://arxiv.org/abs/2411.05254)] [[cool](https://papers.cool/arxiv/2411.05254)] [[pdf](https://arxiv.org/pdf/2411.05254)]
> **Authors**: Jaeyoo Park,Jin Young Choi,Jeonghyung Park,Bohyung Han
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: NeurIPS 2024
- **标题**: 无OCR文档理解的分层视觉特征聚合
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一种基于预读的多模式模型（MLLM）的新型无OCR文档理解框架。我们的方法采用多尺度的视觉特征来有效处理文档图像中的各种字体大小。为了解决考虑MLLM的多尺度视觉输入的成本，我们提出了分层视觉特征聚合（HVFA）模块，旨在减少对LLMS的输入令牌的数量。利用具有跨集合的功能金字塔，我们的方法有效地管理了信息丢失和效率之间的权衡，而不会受到不同文档图像大小的影响。此外，我们介绍了一项新颖的教学调整任务，该任务通过学习预测输入文本的相对位置来促进该模型的文本阅读能力，最终最大程度地减少了由LLMS有限的能力造成的截断文本的风险。全面的实验验证了我们方法的有效性，证明了各种文档理解任务的卓越性能。

### Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2411.05005)] [[cool](https://papers.cool/arxiv/2411.05005)] [[pdf](https://arxiv.org/pdf/2411.05005)]
> **Authors**: Shuhong Zheng,Zhipeng Bao,Ruoyu Zhao,Martial Hebert,Yu-Xiong Wang
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: 26 pages, 14 figures
- **标题**: DIFF-2-IN-1：扩散模型的桥接产生和致密感知
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 除了高保真图像合成之外，扩散模型最近在密集的视觉感知任务中表现出了令人鼓舞的结果。但是，大多数现有的工作都将扩散模型视为感知任务的独立组件，仅用于现成的数据增强或仅作为功能提取器。与这些孤立的，因此是亚最佳努力相反，我们引入了统一的，多功能的，基于扩散的框架Diff-2-In-1，可以通过对扩散造成扩散的过程的独特利用来同时处理多模式数据生成和密集的视觉感知。在此框架内，我们通过使用Denoising网络来创建反映原始训练集的分布的多模式数据，进一步通过多模式生成增强了歧视性视觉感知。重要的是，Diff-2-In-1通过利用一种新颖的自我改进学习机制来优化创建的多元化和忠实数据的利用。全面的实验评估验证了我们的框架的有效性，展示了各种判别性骨架和高质量的多模式数据生成的一致性提高，其特征是现实主义和有用性。

### HourVideo: 1-Hour Video-Language Understanding 
[[arxiv](https://arxiv.org/abs/2411.04998)] [[cool](https://papers.cool/arxiv/2411.04998)] [[pdf](https://arxiv.org/pdf/2411.04998)]
> **Authors**: Keshigeyan Chandrasegaran,Agrim Gupta,Lea M. Hadzic,Taran Kota,Jimming He,Cristóbal Eyzaguirre,Zane Durante,Manling Li,Jiajun Wu,Li Fei-Fei
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: NeurIPS 2024 Datasets and Benchmarks Track; 28 pages
- **标题**: HourVideo：1小时的视频语言理解
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 我们介绍Hourvideo，这是一个基准数据集，可用于长达一个小时的视频语言理解。我们的数据集由一个新的任务套件组成，该套件包括摘要，感知（回忆，跟踪），视觉推理（空间，时间，预测，因果关系，因果关系）和导航（房间，对象检索）任务。 Hourvideo包括来自EGO4D数据集的500个手动策划的以自我为中心的视频，持续时间为20至120分钟，并具有12,976个高质量的五向多项选择问题。基准测试结果表明，包括GPT-4和LLAVA-NEXT在内的多模型模型在随机机会上实现了边缘改进。与之形成鲜明对比的是，人类专家的表现明显优于最先进的长篇文本多模型，Gemini Pro 1.5（85.0％vs. 37.3％），突出了多模式功能的巨大差距。我们的基准，评估工具包，提示和文档可在https://hourvideo.stanford.edu上找到。

### LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation 
[[arxiv](https://arxiv.org/abs/2411.04997)] [[cool](https://papers.cool/arxiv/2411.04997)] [[pdf](https://arxiv.org/pdf/2411.04997)]
> **Authors**: Weiquan Huang,Aoqi Wu,Yifan Yang,Xufang Luo,Yuqing Yang,Liang Hu,Qi Dai,Xiyang Dai,Dongdong Chen,Chong Luo,Lili Qiu
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: No comments
- **标题**: LLM2CLIP：强大的语言模型解锁更丰富的视觉表示
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 剪辑是一种基础多模型，使用大规模图像文本对的对比度学习将图像和文本特征对齐为共享空间。它的优势在于利用自然语言作为丰富的监督信号。随着大语言模型（LLM）的快速发展，我们探索了它们进一步增强Clip多模式表示学习的潜力。这项工作介绍了一种微调方法，该方法将LLM与预验证的剪辑视觉编码器集成在一起，利用LLMS的高级文本理解和开放世界知识来提高剪贴画的能力，可以处理较长且复杂的字幕。为了应对LLMS自回归性质的挑战，我们提出了一个标题对比对比学习框架，以增强其产出的判别能力。我们的方法在各种下游任务上取得了可观的性能，证明了将LLM与夹子相结合以增强多模式学习的有效性。

### VAIR: Visuo-Acoustic Implicit Representations for Low-Cost, Multi-Modal Transparent Surface Reconstruction in Indoor Scenes 
[[arxiv](https://arxiv.org/abs/2411.04963)] [[cool](https://papers.cool/arxiv/2411.04963)] [[pdf](https://arxiv.org/pdf/2411.04963)]
> **Authors**: Advaith V. Sethuraman,Onur Bagoren,Harikrishnan Seetharaman,Dalton Richardson,Joseph Taylor,Katherine A. Skinner
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: https://umfieldrobotics.github.io/VAIR_site/
- **标题**: VAIR：低成本，多模式透明的表面重建的视觉声学隐式表示
- **领域**: 计算机视觉和模式识别
- **摘要**: 在室内操作的移动机器人必须准备驾驶包含透明表面的具有挑战性的场景。本文提出了一种通过隐式神经表示融合声学和视觉传感方式的新方法，以使室内场景中透明表面的密集重建。我们提出了一个新型模型，该模型利用生成的潜在优化学习由透明表面组成的室内场景的隐式表示。我们证明我们可以查询隐式表示，以在图像空间或3D几何重建（点云或网格）中具有透明的表面预测。我们在定性和定量上对使用具有RGB-D摄像机和超声传感器的定制低成本传感平台收集的新数据集进行了定性和定量评估。我们的方法对透明表面重建的最先进表现出显着改善。

### CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM 
[[arxiv](https://arxiv.org/abs/2411.04954)] [[cool](https://papers.cool/arxiv/2411.04954)] [[pdf](https://arxiv.org/pdf/2411.04954)]
> **Authors**: Jingwei Xu,Chenyu Wang,Zibo Zhao,Wen Liu,Yi Ma,Shenghua Gao
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Project page: https://cad-mllm.github.io/
- **标题**: CAD-MLLM：用MLLM统一多模式条件的CAD生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文旨在设计一个统一的计算机辅助设计（CAD）生成系统，该系统可以以文本描述，图像，点云甚至其组合的形式轻松地基于用户输入的CAD模型生成CAD模型。为了实现这一目标，我们介绍了CAD-MLLM，这是第一个能够生成以多模式输入为条件的参数CAD模型的系统。具体而言，在CAD-MLLM框架内，我们利用CAD模型的命令序列，然后采用先进的大语言模型（LLMS）来对齐这些多样的多模式数据和CAD模型的矢量化表示。为了促进模型培训，我们设计了一个综合的数据构建和注释管道，该管道将每个CAD模型与相应的多模式数据配合使用。我们最终的数据集（名为Omni-CAD）是第一个包含每个CAD模型的文本描述，多视图图像，点和命令序列的多模式CAD数据集。它包含大约450K实例及其CAD构造序列。为了彻底评估我们生成的CAD模型的质量，我们超越了当前的评估指标，这些指标通过引入评估拓扑质量和表面外壳范围的其他指标来关注重建质量。广泛的实验结果表明，CAD-MLLM显着胜过现有的条件生成方法，并且对噪音和缺失点仍然非常健壮。项目页面和更多可视化可以在以下网址找到：https：//cad-mllm.github.io/

### M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding 
[[arxiv](https://arxiv.org/abs/2411.04952)] [[cool](https://papers.cool/arxiv/2411.04952)] [[pdf](https://arxiv.org/pdf/2411.04952)]
> **Authors**: Jaemin Cho,Debanjan Mahata,Ozan Irsoy,Yujie He,Mohit Bansal
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Project webpage: https://m3docrag.github.io
- **标题**: M3Docrag：多模式检索是您需要的多页多文档理解所需的
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 文档视觉问题回答（DOCVQA）管道，回答文档中问题的管道具有广泛的应用。现有方法着重于使用多模式语言模型（MLMS）处理单页文档，或依赖于使用基于文本的检索增强生成（RAG），该生成（RAG）使用文本提取工具，例如光学字符识别（OCR）。但是，在现实世界中应用这些方法存在困难：（a）问题通常需要在不同页面或文档上进行信息，其中MLMS无法处理许多长期文档； （b）文档通常在视觉元素（例如数字）中具有重要信息，但是文本提取工具忽略了它们。我们介绍了M3Docrag，这是一种新型的多模式RAG框架，该框架灵活地适应各种文档上下文（封闭域和开放域），问题啤酒花（单跳和多跳）以及证据方式（文本，图表，图等）。 M3Docrag使用多模式猎犬和MLM找到相关的文档并回答问题，以便在保留视觉信息的同时，可以有效地处理单个或许多文档。由于以前的DOCVQA数据集在特定文档的上下文中提出问题，因此我们还提出了M3DOCVQA，这是一种新的基准，用于评估40,000多页的3,000多个PDF文档的开放域DOCVQA。在三个基准测试中（M3DOCVQA/MMLONGBENCH-DOC/MP-DOCVQA），经验结果表明，M3Docrag与Colpali和Qwen2-Vl 7b相比，M3Docrag在MP-DOCVQA中的最先进的底座都比许多强大的基准表现出色。我们提供有关不同索引，MLM和检索模型的全面分析。最后，我们定性地表明，M3Docrag可以成功处理各种情况，例如在多个页面上存在相关信息以及仅在图像中存在答案证据时。

### SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2411.04933)] [[cool](https://papers.cool/arxiv/2411.04933)] [[pdf](https://arxiv.org/pdf/2411.04933)]
> **Authors**: Tianyu Yang,Yiyang Nan,Lisen Dai,Zhenwen Liang,Yapeng Tian,Xiangliang Zhang
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: EMNLP 2024
- **标题**: SASR-NET：源感知语义表示网络，用于增强视听问题的回答
- **领域**: 计算机视觉和模式识别
- **摘要**: 视听问题回答（AVQA）是一项艰巨的任务，涉及根据视频中的听觉和视觉信息回答问题。一个重大的挑战是解释复杂的多模式场景，其中包括视觉对象和声音来源，并将它们连接到给定的问题。在本文中，我们介绍了为AVQA设计的新型模型。 SASR-NET利用源可学习的代币来有效地捕获和对齐视听元素与相应的问题。它使用空间和时间注意机制简化了音频和视觉信息的融合，以识别多模式场景中的答案。关于音乐-AVQA和AVQA-YANG数据集的广泛实验表明，SASR-NET优于最先进的AVQA方法。

### VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos 
[[arxiv](https://arxiv.org/abs/2411.04923)] [[cool](https://papers.cool/arxiv/2411.04923)] [[pdf](https://arxiv.org/pdf/2411.04923)]
> **Authors**: Shehan Munasinghe,Hanan Gani,Wenqi Zhu,Jiale Cao,Eric Xing,Fahad Shahbaz Khan,Salman Khan
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Technical Report of VideoGLaMM
- **标题**: VieveoGlamm：视频中用于像素级视觉接地的大型多模式
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于视频中的复杂空间和时间动态，视频和文本之间的细粒度对齐是具有挑战性的。现有的基于视频的大型多模式模型（LMM）处理基本对话，但在视频中以精确的像素级接地而挣扎。为了解决这个问题，我们介绍了VivioGlamm，这是一种基于用户提供的文本输入的视频中精细的像素级接地的LMM。我们的设计无缝连接三个关键组成部分：大型语言模型，一种强调空间和时间细节的双重视觉编码器，以及一个时空的解码器，以进行准确的掩码生成。通过可调的V-L和L-V适配器来促进该连接，该适配器可以实现近视语言（VL）对齐。该体系结构经过培训，可以将视频内容的空间和时间元素与文本说明同步。为了启用细粒接地，我们使用半自动注释管道策划了一个多式联运数据集，其中包含详细的视觉接地对话，从而产生了一组38k Video-QA三胞胎以及83K对象和671K蒙版。我们评估了VideoGlamm，以三个具有挑战性的任务：扎根的对话，视觉接地和参考视频细分。实验结果表明，我们的模型在所有三个任务中始终胜过现有方法。

### Dynamic Brightness Adaptation for Robust Multi-modal Image Fusion 
[[arxiv](https://arxiv.org/abs/2411.04697)] [[cool](https://papers.cool/arxiv/2411.04697)] [[pdf](https://arxiv.org/pdf/2411.04697)]
> **Authors**: Yiming Sun,Bing Cao,Pengfei Zhu,Qinghua Hu
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Accepted by IJCAI 2024
- **标题**: 强大的多模式图像融合的动态亮度适应
- **领域**: 计算机视觉和模式识别
- **摘要**: 红外图像融合旨在整合视觉增强图像的模态强度。在现实世界中的可见成像容易受到动态环境亮度波动的影响，从而导致质地降解。现有的融合方法缺乏这种亮度扰动的鲁棒性，严重损害了融合图像的视觉保真度。为了应对这一挑战，我们提出了亮度自适应多模式动力融合框架（BA融合），尽管动态亮度波动，但它仍可以实现强大的图像融合。具体而言，我们引入了一个亮度自适应门（BAG）模块，该模块旨在动态选择与亮度相关的通道中的功能以进行标准化，同时保留源图像中与亮度无关的结构信息。此外，我们提出了亮度一致性损失函数，以优化袋子模块。整个框架都是通过交替的培训策略来调整的。广泛的实验验证了我们的方法在保留多模式图像信息和视觉保真度方面超过了最新方法，同时在不同的亮度水平上表现出显着的鲁棒性。我们的代码可用：https：//github.com/sunym2020/ba-fusion。

### Explainable Search and Discovery of Visual Cultural Heritage Collections with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.04663)] [[cool](https://papers.cool/arxiv/2411.04663)] [[pdf](https://arxiv.org/pdf/2411.04663)]
> **Authors**: Taylor Arnold,Lauren Tilton
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: 16 pages, CHR 2024: Computational Humanities Research Conference, December 4 - 6, 2024, Aarhus University, Denmark
- **标题**: 可解释的搜索和发现具有多模式模型的视觉文化遗产收藏
- **领域**: 计算机视觉和模式识别
- **摘要**: 许多文化机构通常在允许的重复使用许可下提供大量数字化的视觉收藏。创建用于探索和搜索这些收藏的界面很困难，尤其是在没有颗粒状元数据的情况下。在本文中，我们介绍了一种使用最先进的多模式大型语言模型（LLMS）来启用视觉收集的开放式，可解释的搜索和发现接口的方法。我们展示了我们的方法如何创建新颖的聚类和推荐系统，这些系统避免了直接基于视觉嵌入的方法的常见方法。特别有趣的是能够提供每个建议的具体文本说明，而无需预先选择感兴趣的功能。这些功能共同创建了一个更开放和灵活的数字界面，同时也更适合于解决隐私和道德问题。通过使用纪录片集的案例研究，我们提供了几个指标，以显示我们方法的功效和可能性。

### ICH-SCNet: Intracerebral Hemorrhage Segmentation and Prognosis Classification Network Using CLIP-guided SAM mechanism 
[[arxiv](https://arxiv.org/abs/2411.04656)] [[cool](https://papers.cool/arxiv/2411.04656)] [[pdf](https://arxiv.org/pdf/2411.04656)]
> **Authors**: Xinlei Yu,Ahmed Elazab,Ruiquan Ge,Hui Jin,Xinchen Jiang,Gangyong Jia,Qing Wu,Qinglei Shi,Changmiao Wang
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: 6 pages, 2 figures, 3 tables, published to BIBM 2024
- **标题**: ICH-SCNET：使用夹子引导的SAM机制，脑内出血分割和预后分类网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 脑内出血（ICH）是中风的最致命亚型，其特征是残疾人的发生率高。 ICH区域和预测预测的准确分割对于制定和完善近亲患者的治疗计划至关重要。但是，现有方法独立地解决了这两个任务，主要集中于成像数据，从而忽略了任务与模式之间的固有相关性。本文介绍了用于ICH分割和预后分类的多任务网络ICH-SCNET。具体而言，我们集成了SAM-CLIP跨模式相互作用机制，该机制将医学文本和分割辅助信息与神经成像数据结合在一起，以增强跨模式特征识别。此外，我们开发了一个有效的功能融合模块和多任务损耗函数，以进一步提高性能。在ICH数据集上进行的广泛实验表明，我们的方法超过了其他最先进的方法。在所有细分任务指标中，它在分类任务的整体性能和胜过竞争模型方面都表现出色。

### l0-Regularized Sparse Coding-based Interpretable Network for Multi-Modal Image Fusion 
[[arxiv](https://arxiv.org/abs/2411.04519)] [[cool](https://papers.cool/arxiv/2411.04519)] [[pdf](https://arxiv.org/pdf/2411.04519)]
> **Authors**: Gargi Panda,Soumitra Kundu,Saumik Bhattacharya,Aurobinda Routray
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: No comments
- **标题**: L0限制的基于稀疏编码的可解释网络，用于多模式图像融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式图像融合（MMIF）通过结合从不同模态传感器图像获得的唯一和常见特征，改进可视化，对象检测以及更多任务，从而增强了融合图像的信息内容。在这项工作中，我们基于L0规范化的多模式卷积稀疏编码（MCSC）模型，为MMIF任务引入了一个可解释的网络。具体而言，为了解决L0调查的CSC问题，我们开发了一种基于算法的基于L0的稀疏编码（LZSC）块。给定不同的模源图像，FNET首先使用LZSC块将其独特和共同特征与它们分开，然后将这些功能组合在一起以生成最终的融合图像。此外，我们为反融合过程提出了一个L0调查的MCSC模型。基于此模型，我们引入了一个名为IFNET的可解释的反融合网络，该网络在FNET培训期间使用。广泛的实验表明，FNET在五个不同的MMIF任务中实现了高质量的融合结果。此外，我们表明FNET在可见的热图像对中增强了下游对象检测。我们还可以看到FNET的中间结果，该结果证明了我们网络的良好解释性。

### FreeCap: Hybrid Calibration-Free Motion Capture in Open Environments 
[[arxiv](https://arxiv.org/abs/2411.04469)] [[cool](https://papers.cool/arxiv/2411.04469)] [[pdf](https://arxiv.org/pdf/2411.04469)]
> **Authors**: Aoru Xue,Yiming Ren,Zining Song,Mao Ye,Xinge Zhu,Yuexin Ma
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: No comments
- **标题**: FreeCap：在开放环境中无混合校准的运动捕获
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一种新型的混合校准方法FreeCap，以准确捕获开放环境中的全球多人运动。我们的系统将单个LIDAR与可扩展的移动相机结合在一起，从而可以在统一的世界坐标中进行灵活而精确的运动估计。特别是，我们介绍了一个局部到全球姿势感知的跨传感器人匹配模块，该模块即使没有校准，也可以预测每个传感器之间的对齐方式。此外，我们的粗到最新传感器张开的姿势优化器进一步优化了3D人体的关键点和对齐，它也能够合并其他相机以提高准确性。关于Human-M3和Freemotion数据集的广泛实验表明，我们的方法显着胜过最先进的单模式方法，为在各种应用程序上提供了可扩展和高效的多人运动捕获解决方案。

### LFSamba: Marry SAM with Mamba for Light Field Salient Object Detection 
[[arxiv](https://arxiv.org/abs/2411.06652)] [[cool](https://papers.cool/arxiv/2411.06652)] [[pdf](https://arxiv.org/pdf/2411.06652)]
> **Authors**: Zhengyi Liu,Longzhen Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang
> **First submission**: 2024-11-10
> **First announcement**: 2024-11-11
> **comment**: Accepted by SPL
- **标题**: LFSAMBA：与山姆嫁给Mamba，以获取光场显着对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 光场摄像头可以使用捕获的多对焦图像重建3D场景，这些图像包含丰富的空间几何信息，增强立体摄影，虚拟现实和机器人视觉的应用。在这项工作中，引入了一个称为LFSAMBA的多聚焦光场图像的最先进的对象检测模型，以强调四个主要见解：（a）有效的特征提取，其中SAM用于提取模态感知的歧视性特征； （b）切片间关系建模，利用Mamba捕获多个焦点切片的远距离依赖性，从而提取隐式深度提示； （c）使用曼巴（Mamba）整合全聚焦和多聚焦图像，从而实现相互增强； （d）弱监督的学习能力，从现有的像素级掩码数据集中开发涂鸦注释数据集，建立第一个用于光场较高对象检测的涂鸦式掩码数据集。

### KMM: Key Frame Mask Mamba for Extended Motion Generation 
[[arxiv](https://arxiv.org/abs/2411.06481)] [[cool](https://papers.cool/arxiv/2411.06481)] [[pdf](https://arxiv.org/pdf/2411.06481)]
> **Authors**: Zeyu Zhang,Hang Gao,Akide Liu,Qi Chen,Feng Chen,Yiran Wang,Danning Li,Hao Tang
> **First submission**: 2024-11-10
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: KMM：钥匙框面膜妈妈，用于扩展运动生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类运动生成是生成计算机视觉研究的剪裁领域，在视频创建，游戏开发和机器人操作中具有有希望的应用。最近的Mamba体系结构在有效地建模长而复杂的序列方面显示出令人鼓舞的结果，但仍有两个重大挑战：首先，直接将MAMBA应用于扩展运动的产生是无效的，因为隐式存储器的有限能力导致内存衰减。其次，与变压器相比，曼巴（Mamba）在多模式融合中挣扎，并且缺乏与文本查询的一致性，通常会混淆方向（左右）或省略更长的文本查询部分。为了应对这些挑战，我们的论文提出了三个关键贡献：首先，我们介绍了KMM，这是一种具有关键框架掩盖建模的新颖架构，旨在增强Mamba对运动段中关键动作的关注。此方法解决了内存衰减问题，并代表了一种开创性的方法，用于自定义SSM中的战略框架级掩蔽。此外，我们设计了一种对比的学习范式，用于解决曼巴（Mamba）中的多模式融合问题并改善运动文本对齐。最后，与以前的最新方法相比，我们对Babel进行了广泛的实验，Babel，Babel实现了最先进的性能，降低了57％以上的FID和70％的参数。请参阅项目网站：https：//steve-zeyu-zhang.github.io/kmm

### NeuReg: Domain-invariant 3D Image Registration on Human and Mouse Brains 
[[arxiv](https://arxiv.org/abs/2411.06315)] [[cool](https://papers.cool/arxiv/2411.06315)] [[pdf](https://arxiv.org/pdf/2411.06315)]
> **Authors**: Taha Razzaq,Asim Iqbal
> **First submission**: 2024-11-09
> **First announcement**: 2024-11-11
> **comment**: 15 pages, 5 figures, 5 tables
- **标题**: Neureg：人类和小鼠大脑的域不变3D图像注册
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,神经和进化计算,定量方法
- **摘要**: 医疗脑成像在很大程度上依赖图像注册来准确地策划各种医疗保健应用的大脑特征的结构边界。近年来，深度学习模型在图像注册中表现出色。尽管如此，他们经常努力处理3D大脑体积的多样性，这些脑量的结构和对比变化及其成像领域的挑战。在这项工作中，我们提出了Neureg，这是一种神经启发的3D图像登记架构，具有域不变性的特征。 Neureg生成了成像特征的域 - 不可能的表示，并结合了基于窗口的Swin Transformer块作为编码器。这使我们的模型能够捕获大脑成像方式和物种之间的变化。我们在包括人和老鼠3D脑量的多域公开数据集中展示了一个新的基准。广泛的实验表明，我们的模型（NEUREG）的表现优于现有的基线深度学习图像注册模型，并在跨域数据集上提供了高性能的提升，在跨域数据集上，在“纯源”域上对模型进行了培训，并在完全“看不见的”目标域进行了测试。我们的工作建立了一个由基于神经启发的变压器架构支撑的域 - 敏捷3D脑图像注册的新最先进的。

### Personalize to generalize: Towards a universal medical multi-modality generalization through personalization 
[[arxiv](https://arxiv.org/abs/2411.06106)] [[cool](https://papers.cool/arxiv/2411.06106)] [[pdf](https://arxiv.org/pdf/2411.06106)]
> **Authors**: Zhaorui Tan,Xi Yang,Tan Pan,Tianyi Liu,Chen Jiang,Xin Guo,Qiufeng Wang,Anh Nguyen,Yuan Qi,Kaizhu Huang,Yuan Cheng
> **First submission**: 2024-11-09
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 个性化以推广：通过个性化迈向通用的多模式概括
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 由不同的基本原则驱动的医学成像方式之间的差异对多模式医疗任务的概括构成了重大挑战。除模态差距外，个体变化（例如器官大小和代谢率的差异）进一步阻碍了模型有效地跨越模式和不同种群的能力。尽管个性化的重要性，但现有的多模式概括方法通常忽略了个体差异，仅着眼于共同的解剖特征。这种限制可能导致各种医疗任务的概括减弱。在本文中，我们揭示了个性化对于多模式概括至关重要。具体而言，我们提出了一种方法，通过近似于通过利用个体级别的约束和可学习的生物学先验来实现各种模式的基本个性化不变表示$ {x} _H $来实现个性化的概括。我们验证学习个性化的$ {x} _h $的可行性和好处，表明此表示在各种多模式医疗任务中都是可以推广的，并且可以转移。广泛的实验结果始终表明，另外纳入的个性化可显着改善各种情况的性能和概括，从而确认其有效性。

### An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2411.06048)] [[cool](https://papers.cool/arxiv/2411.06048)] [[pdf](https://arxiv.org/pdf/2411.06048)]
> **Authors**: Fatemeh Shiri,Xiao-Yu Guo,Mona Golestan Far,Xin Yu,Gholamreza Haffari,Yuan-Fang Li
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 大型多模型的空间推理能力的经验分析
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型多模型模型（LMM）在一系列视觉和语言任务中取得了强大的性能。但是，它们的空间推理能力不足。在本文中，我们构建了一个新颖的VQA数据集，即空间-MM，以全面研究LMMS的空间理解和推理能力。我们对对象关系和多跳推理的分析揭示了一些重要的发现。首先，边界框和场景图，甚至是合成图形，都可以显着增强LMMS的空间推理。其次，LMM在人类的角度提出的问题比相机对图像的角度挣扎。第三，思想链（COT）提示并不能改善涉及空间关系的复杂多跳问题的模型性能。此外，与MLLM的非空间推理相比，空间推理步骤的准确性要少得多。最后，我们对GQA空间的扰动分析表明，与复杂的空间推理相比，LMM在基本对象检测下要强得多。我们认为，我们的基准数据集和深入分析可以引发对LMMS空间推理的进一步研究。空间-MM基准可在以下网址提供：https：//github.com/fatemehshiri/spatial-mm

### Autoregressive Models in Vision: A Survey 
[[arxiv](https://arxiv.org/abs/2411.05902)] [[cool](https://papers.cool/arxiv/2411.05902)] [[pdf](https://arxiv.org/pdf/2411.05902)]
> **Authors**: Jing Xiong,Gongye Liu,Lun Huang,Chengyue Wu,Taiqiang Wu,Yao Mu,Yuan Yao,Hui Shen,Zhongwei Wan,Jinfa Huang,Chaofan Tao,Shen Yan,Huaxiu Yao,Lingpeng Kong,Hongxia Yang,Mi Zhang,Guillermo Sapiro,Jiebo Luo,Ping Luo,Ngai Wong
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 视觉中的自回归模型：一项调查
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 自自然语言处理领域（NLP）领域取得了巨大成功。最近，自回归模型已成为计算机视觉中的重要重点领域，它们在产生高质量的视觉内容方面表现出色。 NLP中的自动回归模型通常在子字代币上运行。但是，计算机视觉中的表示策略可能会在不同级别上有所不同，\ textit {i.e。}，像素级，令牌级别或比例级别，反映了与语言的顺序结构相比，视觉数据的多样性和分层性质。这项调查全面研究了有关视觉应用的自回旋模型的文献。为了提高来自不同研究背景的研究人员的可读性，我们从视觉中的初步序列表示和建模开始。接下来，我们将视觉自回归模型的基本框架分为三个一般子类别，包括基于代表策略的基于像素，基于令牌的基于代币和基于比例的模型。然后，我们探索自回归模型与其他生成模型之间的互连。此外，我们提出了计算机视觉中自回归模型的多面分类，包括图像生成，视频生成，3D代和多模式生成。我们还详细介绍了它们在不同领域的应用，包括体现的AI和3D Medical AI等新兴领域，其中约有250个相关参考。最后，我们强调了目前对视觉自回归模型的挑战，并提出了有关潜在研究方向的建议。我们还设置了一个GitHub存储库来组织本调查中包含的论文：\ url {https://github.com/chaofantao/autoregrelistion-models-models-in-vision-survey}。

### Enhancing Cardiovascular Disease Prediction through Multi-Modal Self-Supervised Learning 
[[arxiv](https://arxiv.org/abs/2411.05900)] [[cool](https://papers.cool/arxiv/2411.05900)] [[pdf](https://arxiv.org/pdf/2411.05900)]
> **Authors**: Francesco Girlanda,Olga Demler,Bjoern Menze,Neda Davoudi
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: Accepted to British Machine Vision Conference (BMVC) 2024
- **标题**: 通过多模式自我监督学习来增强心血管疾病预测
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 对于早期诊断和干预，对心血管疾病的准确预测至关重要，需要坚固而精确的预测模型。最近，仅通过单独模式数据集就无法发现新的见解，人们对多模式学习的兴趣越来越大。通过结合心脏磁共振图像，心电图信号和可用的医疗信息，我们的方法可以通过利用跨模态的共同信息来捕获个人心血管健康的整体状态。我们的模型从多种模式中整合信息并受益于自我监督的学习技术，为通过有限的注释数据集提供了一个全面的框架，以增强心血管疾病预测。我们采用蒙版自动编码器预先培训心电图ECG编码器，使其能够从原始心电图数据中提取相关特征，并通过图像编码器从心脏磁共振图像中提取相关特征。随后，我们利用多模式的对比学习目标将知识从昂贵且复杂的模态，心脏磁共振图像转移到廉价而简单的模态，例如心电图和医学信息。最后，我们在特定的预测任务（例如心肌梗塞）上微调了预训练的编码器。我们提出的方法通过利用不同的可用方式增强了图像信息，并以平衡精度优于监督方法7.6％。

### Integrating Object Detection Modality into Visual Language Model for Enhanced Autonomous Driving Agent 
[[arxiv](https://arxiv.org/abs/2411.05898)] [[cool](https://papers.cool/arxiv/2411.05898)] [[pdf](https://arxiv.org/pdf/2411.05898)]
> **Authors**: Linfeng He,Yiming Sun,Sihao Wu,Jiaxu Liu,Xiaowei Huang
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: accepted by SafeGenAI workshop of NeurIPS 2024
- **标题**: 将对象检测方式集成到视觉语言模型中，以增强自主驱动剂
- **领域**: 计算机视觉和模式识别,人工智能,机器人技术
- **摘要**: 在本文中，我们提出了一个新颖的框架，以通过将视觉语言模型（VLM）与专门用于对象检测的其他视觉感知模块进行集成，以增强自主驾驶系统中的视觉理解。我们通过将基于YOLOS的检测网络与剪辑感知网络一起结合，扩展了Llama-Adapter架构，从而解决了对象检测和本地化的限制。我们的方法介绍了相机ID分离器，以改善多视图处理，对于全面的环境意识至关重要。在Drivelm视觉问题上回答挑战的实验表明，基线模型的显着改善，在ChatGPT分数，BLEU分数和苹果酒指标中的性能提高，表明模型对地面真相的答案很紧密。我们的方法代表了朝着更有能力和可解释的自主驾驶系统迈出的有希望的步骤。还讨论了通过检测方式启用可能的安全性增强。

### Smile upon the Face but Sadness in the Eyes: Emotion Recognition based on Facial Expressions and Eye Behaviors 
[[arxiv](https://arxiv.org/abs/2411.05879)] [[cool](https://papers.cool/arxiv/2411.05879)] [[pdf](https://arxiv.org/pdf/2411.05879)]
> **Authors**: Yuanyuan Liu,Lin Wei,Kejun Liu,Yibing Zhan,Zijing Chen,Zhe Chen,Shiguang Shan
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-11
> **comment**: The paper is part of ongoing work and we request to withdraw it from arXiv to revise it further. And The paper was submitted without agreement from all co-authors
- **标题**: 在脸上微笑，但眼睛的悲伤：基于面部表情和眼睛行为的情感识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 情绪识别（ER）是从给定数据中识别人类情绪的过程。目前，该领域在很大程度上依赖面部表情识别（FER），因为面部表情包含丰富的情感提示。但是，重要的是要注意，面部表情可能并不总是准确地反映出真正的情绪，基于FER的结果可能会产生误导性的ER。为了理解和弥合FER和ER之间的这一差距，我们将眼睛的行为引入了创建新的眼神辅助多模式情感识别（EMER）数据集的重要情感线索。与现有的多模式数据集不同，EMER数据集采用刺激物质引起的自发情感生成方法来整合非侵入性眼部行为数据，例如眼动和眼睛固定图，旨在获得自然而准确的人类情感。值得注意的是，我们第一次为EMER中的ER和FER提供注释，从而使全面的分析更好地说明了这两个任务之间的差距。此外，我们专门设计了一种新的EMERT体系结构，通过有效识别和弥合两者之间的情感差距，以同时提高ER和ER的性能。特别是，我们的EMERT采用模态 - 逆转功能 - 逆转功能解耦合和多任务变压器，从而增强眼行为的模型，从而提供有效的互补表情。在实验中，我们介绍了七个多模式基准协议，以对EMER数据集进行各种全面评估。结果表明，EMERT的表现优于其他最先进的多模式方法，从而揭示了建模眼睛行为对鲁棒ER的重要性。综上所述，我们对眼睛行为在ER中的重要性进行了全面分析，从而推进了解决FER和ER之间的差距，以提高ER性能。

### From Pixels to Prose: Advancing Multi-Modal Language Models for Remote Sensing 
[[arxiv](https://arxiv.org/abs/2411.05826)] [[cool](https://papers.cool/arxiv/2411.05826)] [[pdf](https://arxiv.org/pdf/2411.05826)]
> **Authors**: Xintian Sun,Benji Peng,Charles Zhang,Fei Jin,Qian Niu,Junyu Liu,Keyu Chen,Ming Li,Pohsun Feng,Ziqian Bi,Ming Liu,Yichao Zhang
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-11
> **comment**: 10 pages, 1 figure
- **标题**: 从像素到散文：推进遥感的多模式语言模型
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 遥感已经从简单的图像获取到能够集成和处理视觉和文本数据的复杂系统。这篇综述研究了多模式语言模型（MLLM）在遥感中的开发和应用，重点是使用自然语言解释和描述卫星图像的能力。我们涵盖了MLLM的技术基础，包括双重编码器体系结构，变压器模型，自学和对比度学习以及跨模式集成。遥感数据的独特挑战（各种空间分辨率，光谱丰富度和时间变化）已经分析了它们对MLLM性能的影响。讨论了关键应用程序，例如场景说明，对象检测，变更检测，文本对图像检索，图像到文本生成和视觉问题答案，以证明它们在环境监控，城市规划和灾难响应中的相关性。我们回顾了支持这些模型的培训和评估的重要数据集和资源。强调了与计算需求，可扩展性，数据质量和域适应性相关的挑战。我们通过提出未来的研究方向和技术进步来总结，以进一步增强遥感中的MLLM实用程序。

### Tell What You Hear From What You See -- Video to Audio Generation Through Text 
[[arxiv](https://arxiv.org/abs/2411.05679)] [[cool](https://papers.cool/arxiv/2411.05679)] [[pdf](https://arxiv.org/pdf/2411.05679)]
> **Authors**: Xiulong Liu,Kun Su,Eli Shlizerman
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: NeurIPS 2024
- **标题**: 从您看到的内容中告诉您听到的信息 - 视频通过文字到达音频发电
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,声音,音频和语音处理
- **摘要**: 视觉和音频场景的内容是多方面的，因此可以将视频与各种音频和反之亦然配对。因此，在视频到审计生成任务中，必须介绍控制生成音频的转向方法。虽然视频到审计是一项完善的生成任务，但现有方法缺乏这种可控性。在这项工作中，我们提出了VATT，这是一种多模式生成框架，该框架以视频为输入，并为音频提供音频和可选的文本描述。这样的框架具有两个优点：i）可以通过文本来完善和控制视频到审计的过程，以补充视觉信息的上下文，ii）ii）该模型可以建议通过生成音频字幕来为视频生成哪些音频。 VATT由两个关键模块组成：Vatt Converter，一个用于指令的LLM，其中包括一个投影层，将视频功能映射到LLM矢量空间；和Vatt Audio，一种使用迭代并行解码从视觉帧和可选文本提示中生成音频令牌的变压器。音频令牌通过验证的神经编解码器转换为波形。实验表明，当将VATT与客观指标中的现有视频到原告生成方法进行比较时，当不提供音频字幕时，它就会达到竞争性能。当提供音频字幕作为提示时，Vatt会取得更加精致的性能（最低的KLD得分为1.41）。此外，主观研究表明，与现有方法生成的音频相比，Vatt Audio被选为首选的音频。 Vatt通过文本启用可控的视频到ADIO生成，并通过音频字幕提示文本提示视频，解锁新颖的应用程序，例如文本引导的视频到ADIO生成和视频对审计字幕。

### Predicting Stroke through Retinal Graphs and Multimodal Self-supervised Learning 
[[arxiv](https://arxiv.org/abs/2411.05597)] [[cool](https://papers.cool/arxiv/2411.05597)] [[pdf](https://arxiv.org/pdf/2411.05597)]
> **Authors**: Yuqing Huang,Bastian Wittmann,Olga Demler,Bjoern Menze,Neda Davoudi
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: Accepted as oral paper at ML-CDS workshop, MICCAI 2024
- **标题**: 通过视网膜图和多模式自学学习预测中风
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 中风的早期鉴定对于干预需要可靠的模型至关重要。我们提出了有效的视网膜图像表示以及临床信息，以捕获心血管健康的全面概述，利用大型的多模式数据集用于新的医疗见解。我们的方法是使用源自视网膜图像的容器图来积分图形和表格数据的第一个对比框架之一，以有效表示。这种方法与多模式的对比学习结合，通过整合来自多个来源的数据并使用对比度学习进行转移学习，从而显着提高了中风预测的准确性。所采用的自制学习技术允许该模型从未标记的数据中有效学习，从而降低了对大注释数据集的依赖性。我们的框架显示，从监督到自我监督的方法，AUROC的提高了3.78％。此外，图级表示方法在图像编码器上达到了出色的性能，同时显着降低了预训练和微调的运行时间。这些发现表明，视网膜图像是改善心血管疾病预测的一种经济高效的方法，并为未来对视网膜和脑血管连接的研究和使用基于图的视网膜血管表示铺平了道路。

### AuthFormer: Adaptive Multimodal biometric authentication transformer for middle-aged and elderly people 
[[arxiv](https://arxiv.org/abs/2411.05395)] [[cool](https://papers.cool/arxiv/2411.05395)] [[pdf](https://arxiv.org/pdf/2411.05395)]
> **Authors**: Yang rui,Meng ling-tao,Zhang qiu-yu
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 授权者：中年和老年人的自适应多模式生物识别验证变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式生物识别身份验证方法解决了安全性，鲁棒性和用户适应性中单峰生物识别技术的局限性。但是，大多数现有方法取决于固定组合和生物识别方式的数量，这限制了现实世界应用中的灵活性和适应性。为了克服这些挑战，我们提出了一种自适应多模式生物识别验证模型，Authformer，为老年用户量身定制。 Authformer在Lutbio多模式生物识别数据库上进行了训练，其中包含来自老年人的生物识别数据。通过纳入跨注意机制和门控剩余网络（GRN），该模型可改善对老年使用者生理变化的适应性。实验表明，Authformer的精度为99.73％。此外，与传统的基于变压器的模型相比，其编码器仅需要两层才能发挥最佳性能，从而降低了复杂性。

### Contrastive Language Prompting to Ease False Positives in Medical Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2411.07546)] [[cool](https://papers.cool/arxiv/2411.07546)] [[pdf](https://arxiv.org/pdf/2411.07546)]
> **Authors**: YeongHyeon Park,Myung Jin Kim,Hyeong Seok Kim
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: 4 pages, 3 figures, 2 tables
- **标题**: 对比语言促使在医学异常检测中简化误报
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 预先训练的视觉语言模型，对比的语言图像预训练（剪辑）成功地完成了文本提示的各种下游任务，例如查找图像或图像中的本地化区域。尽管Clip具有强大的多模式数据功能，但在医疗应用等专业环境中，它仍然有限。为此，出现了许多夹子变体-I.E。，生物胶囊和MedClip-Samv2 have，但与正常区域有关的假阳性持续存在。因此，我们旨在提出一个简单而重要的目标，以减少医学异常检测中的假阳性。我们引入了一种对比语言提示（拍手）方法，该方法利用正面和负面文本提示。这种直接的方法通过视觉注意给定图像中的正提示来确定潜在的病变区域。为了减少误报，我们使用负提示来减轻对正常区域的关注。使用BMAD数据集进行的广泛实验，包括六个生物医学基准，表明CLAP方法可以增强异常检测性能。我们的未来计划包括开发一种自动化的良好提示方法，以进行更实际的用法。

### SparrowVQE: Visual Question Explanation for Course Content Understanding 
[[arxiv](https://arxiv.org/abs/2411.07516)] [[cool](https://papers.cool/arxiv/2411.07516)] [[pdf](https://arxiv.org/pdf/2411.07516)]
> **Authors**: Jialu Li,Manish Kumar Thota,Ruslan Gokhman,Radek Holik,Youshan Zhang
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: No comments
- **标题**: SparrowVQE：课程内容理解的视觉问题说明
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉问题回答（VQA）的研究试图创建AI系统来回答图像中的自然语言问题，但是VQA方法通常会产生过度简单和简短的答案。本文旨在通过引入视觉问题解释（VQE）来推进该领域，从而增强了VQA提供详细说明的能力，而不是简短的回答，并满足了与视觉内容更复杂的相互作用的需求。我们首先从14周流的视频机学习课程中创建了一个MLVQE数据集，其中包括885个幻灯片图像，110,407个成绩单单词和9,416个设计的Question-Asswer（QA）对。接下来，我们提出了一种新型的SparroWVQE，这是一个小的30亿个参数多模式。我们通过多模式预训练（幻灯片图像和转录本具有对齐方式），指令调整（通过转录本和质量质量记录调整预训练的模型）以及域微调（微调幻灯片图像和QA对）。最终，我们的SparroWVQE可以使用Siglip模型使用带有MLP适配器的PHI-2语言模型来理解和连接视觉信息。实验结果表明，我们的SparroWVQE在我们开发的MLVQE数据集中取得了更好的性能，并且在其他五个基准VQA数据集中胜过最先进的方法。源代码可在\ url {https://github.com/youshanzhang/sparrowvqe}中获得。

### MSEG-VCUQ: Multimodal SEGmentation with Enhanced Vision Foundation Models, Convolutional Neural Networks, and Uncertainty Quantification for High-Speed Video Phase Detection Data 
[[arxiv](https://arxiv.org/abs/2411.07463)] [[cool](https://papers.cool/arxiv/2411.07463)] [[pdf](https://arxiv.org/pdf/2411.07463)]
> **Authors**: Chika Maduabuchi,Ericmoore Jossou,Matteo Bucci
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: Under Review in ICML 25
- **标题**: MSEG-VCUQ：具有增强视觉基础模型，卷积神经网络的多模式分割以及高速视频阶段检测数据的不确定性量化
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: 高速视频（HSV）相位检测（PD）分割对于监测工业过程中的蒸气，液体和微层相至关重要。尽管基于CNN的模型像U-NET这样的模型在简化的基于阴影的两相流（TPF）分析中表现出了成功，但它们在复杂的HSV PD任务中的应用仍未开发，并且视觉基础模型（VFMS）尚未解决基于阴影的基于阴影或PD TPF视频的复杂性。现有的不确定性定量（UQ）方法缺乏关键指标（例如接触线密度和干燥面积分数）的像素级可靠性，并且缺乏针对PD分割的大规模的，多模式的实验数据集进一步阻碍了进度。为了解决这些差距，我们提出了MSEG-VCUQ。该混合框架将U-NET CNN与基于变压器的任何模型（SAM）集成在一起，以实现增强的分割精度和交叉模式概括。我们的方法结合了系统的UQ，用于可靠的错误评估，并引入了第一个开源多模式HSV PD数据集。经验结果表明，MSEG-VCUQ优于基线CNN和VFM，可用于实现真实世界沸腾动力学的可扩展和可靠的PD分割。

### BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions 
[[arxiv](https://arxiv.org/abs/2411.07461)] [[cool](https://papers.cool/arxiv/2411.07461)] [[pdf](https://arxiv.org/pdf/2411.07461)]
> **Authors**: Anas Awadalla,Le Xue,Manli Shu,An Yan,Jun Wang,Senthil Purushwalkam,Sheng Shen,Hannah Lee,Oscar Lo,Jae Sung Park,Etash Guha,Silvio Savarese,Ludwig Schmidt,Yejin Choi,Caiming Xiong,Ran Xu
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: No comments
- **标题**: Blip3-kale：知识增强大规模密集标题
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们介绍了Blip3-kale，这是一个2.18亿个图像文本对的数据集，该数据集弥合了描述性合成字幕和事实Web规模的Alt-Text之间的差距。羽衣甘蓝增强带有Web尺度alt-Text的合成密集图像标题，以生成实际接地的图像标题。我们的两阶段方法利用大型视觉模型和语言模型来创建知识提名的字幕，然后将其用于训练专门的VLM来扩大数据集。我们在羽衣甘蓝上训练视觉模型，并展示了视力语言任务的改进。我们的实验表明，羽衣甘蓝的实用性用于训练更有能力，知识渊博的多模型模型。我们在https://huggingface.co/datasets/salesforce/blip3-kale上发布羽衣甘蓝数据集

### OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision 
[[arxiv](https://arxiv.org/abs/2411.07199)] [[cool](https://papers.cool/arxiv/2411.07199)] [[pdf](https://arxiv.org/pdf/2411.07199)]
> **Authors**: Cong Wei,Zheyang Xiong,Weiming Ren,Xinrun Du,Ge Zhang,Wenhu Chen
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: 21 pages
- **标题**: Omniedit：通过专业监督构建图像编辑通才模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 指导指导的图像编辑方法通过自动合成或手动注释的图像编辑对训练扩散模型表现出了巨大的潜力。但是，这些方法远非实用，现实生活中的应用。我们确定了导致这一差距的三个主要挑战。首先，由于综合过程有偏见，现有模型的编辑技能有限。其次，这些方法是使用具有大量噪声和人工制品的数据集训练的。这是由于应用了简单的过滤方法（例如剪贴得分）。第三，所有这些数据集都仅限于单个低分辨率和固定的长宽比，从而限制了处理现实世界用例的多功能性。在本文中，我们介绍了\ Omniedit，它是一个无所不能的编辑器，可以无缝处理七个不同的图像编辑任务。我们的贡献分为四倍：（1）\ Omniedit通过利用七个不同专家模型的监督来确保任务覆盖范围。 （2）我们根据大型多模型（例如GPT-4O）而不是剪辑得分提供的分数来利用重要性采样来提高数据质量。 （3）我们提出了一种称为Editnet的新编辑体系结构，以极大地提高编辑成功率，（4）我们提供具有不同纵横比的图像，以确保我们的模型可以处理野外的任何图像。我们已经策划了一个包含不同纵横比的图像的测试集，并伴随着各种说明，以涵盖不同的任务。自动评估和人类评估都表明，\ omniedit可以显着优于所有现有模型。我们的代码，数据集和模型将在\ url {https://tiger-ai-lab.github.io/omniedit/}可用

### ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition 
[[arxiv](https://arxiv.org/abs/2411.07118)] [[cool](https://papers.cool/arxiv/2411.07118)] [[pdf](https://arxiv.org/pdf/2411.07118)]
> **Authors**: Mallika Garg,Debashis Ghosh,Pyari Mohan Pradhan
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: ef:WACV 2025
- **标题**: ConvmixFormer-用于基于变压器的动态手势识别的资源有效卷积混合器
- **领域**: 计算机视觉和模式识别,人机交互,机器学习
- **摘要**: 变压器模型在许多领域（例如自然语言处理（NLP）和计算机视觉）中取得了巨大的成功。随着对基于变压器的架构的兴趣日益兴趣，它们现在被用于手势识别。因此，我们还探索并设计了一种新颖的ConvmixFormer架构，以进行动态手势。变压器使用顺序数据使用二次缩放，因此这些模型在计算上是复杂且重型的。我们已经考虑了变压器的这一缺点，并设计了一种资源有效的模型，该模型用基于简单的基于卷积的令牌混合器代替了变压器中的自我注意力。用于基于卷积的混合器的计算成本和参数相对小于二次自我注意力。卷积混合仪可帮助模型捕获自我发场由于其顺序处理性质而难以捕获的局部空间特征。此外，采用有效的栅极机制，而不是变压器中的常规进发纸网络，以帮助模型控制所提出模型的不同阶段内的特征流。该设计使用较少的可学习参数，该参数几乎是Vanilla Transformer的一半，可帮助快速有效地训练。对NVIDIA动态手势和BRIARIO数据集进行了评估，我们的模型已在单个和多模态输入上获得了最新的结果。我们还显示了与其他方法相比，提出的ConvmixFormer模型的参数效率。源代码可在https://github.com/mallikagarg/convmixformer上找到。

### StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification 
[[arxiv](https://arxiv.org/abs/2411.07076)] [[cool](https://papers.cool/arxiv/2411.07076)] [[pdf](https://arxiv.org/pdf/2411.07076)]
> **Authors**: Yichen He,Yuan Lin,Jianchao Wu,Hanchong Zhang,Yuchen Zhang,Ruicheng Le
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: No comments
- **标题**: 讲故事的人：通过全球视听性角色标识改进长时间的视频描述
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 现有的大型视觉模型（LVLM）在很大程度上仅限于处理短，几秒钟的视频，并努力为扩展视频跨度或更长时间生成连贯的描述。长视频描述引入了新的挑战，例如一致的角色标识和绘制视觉和音频信息的情节级描述。为了解决这些问题，我们找出视听角色标识，将角色名称与每个对话匹配，这是关键因素。我们提出了讲故事的人，这是一种用于生成长视频的密集描述的系统，同时结合了低级视觉概念和高级情节信息。讲故事的人使用多模式的大型语言模型，该模型集成了视觉，音频和文本模式，以在长时间的视频剪辑上执行视听角色识别。然后将结果馈入LVLM，以增强视频描述的一致性。我们在电影描述任务上验证了我们的方法，并介绍了Moviestory101，这是一个具有密集描述的三分钟电影剪辑的数据集。为了评估长时间的视频描述，我们创建了StoryQa，这是Moviestory101测试集的大量多项选择问题。我们通过将其输入GPT-4来评估描述，以将精度作为自动评估指标回答这些问题。实验表明，讲故事的人在StoryQa上的表现优于所有开放和封闭式基线，比最强的基线（Gemini-1.5-Pro）的精度高9.5％，并且在人类并排评估中证明了 +15.56％的优势。此外，在讲故事的人中结合了视听角色识别，可提高所有视频描述模型的性能，Gemini-1.5-Pro和GPT-4O分别显示了5.5％和13.0％的相对改善，在StoryQA上的准确性。

### Multi-Modal interpretable automatic video captioning 
[[arxiv](https://arxiv.org/abs/2411.06872)] [[cool](https://papers.cool/arxiv/2411.06872)] [[pdf](https://arxiv.org/pdf/2411.06872)]
> **Authors**: Antoine Hanna-Asaad,Decky Aspandi,Titus Zaharia
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: No comments
- **标题**: 多模式可解释的自动视频字幕
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视频字幕旨在使用自然语言格式描述视频内容，涉及理解和解释在视图上同时发生的场景，动作和事件。当前的方法主要集中在视觉提示上，通常忽略了来自其他重要的音频信息（包括相互依存）的丰富信息。在这项工作中，我们介绍了一种新型的视频字幕方法，该方法训练有多模式对比损失，强调多模式集成和解释性。我们的方法旨在捕获这些方式之间的依赖性，从而产生更准确的字幕。此外，我们强调了解释性的重要性，采用多种注意力机制，为模型的决策过程提供了解释。我们的实验结果表明，我们所提出的方法对MSR-VTT和VATEX的常用基准数据集的最新模型有利。

### CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.06869)] [[cool](https://papers.cool/arxiv/2411.06869)] [[pdf](https://arxiv.org/pdf/2411.06869)]
> **Authors**: Junho Kim,Hyungjin Chung,Byung-Hoon Kim
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: No comments
- **标题**: CAPELLM：通过多模式模型的无支持类别 - 不足的姿势估计
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 类别不足的姿势估计（CAPE）传统上依赖带有带注释的关键的支持图像，该过程通常很麻烦，并且可能无法完全捕获各种对象类别的必要对应关系。最近的努力已经开始探索基于文本的查询的使用，其中消除了对支持关键的需求。但是，对关键点的文本描述的最佳用途仍然是一个未经置换的区域。在这项工作中，我们介绍了Capellm，这是一种新颖的方法，它利用基于文本的多模式模型（MLLM）为CAPE。我们的方法仅采用查询图像和详细的文本说明作为估算类别 - 不合时件关键点的输入。我们进行了广泛的实验，以系统地探索基于LLM的CAPE的设计空间，并调查了选择关键点，神经网络体系结构和培训策略的最佳描述。由于预先训练的MLLM的高级推理能力，Capellm表现出了出色的概括和稳健的性能。我们的方法在具有挑战性的1-Shot环境中为MP-100基准设定了新的最新设备，这标志着类别 - 不合稳定姿势估算领域的显着进步。

### Multi-Stage Knowledge Integration of Vision-Language Models for Continual Learning 
[[arxiv](https://arxiv.org/abs/2411.06764)] [[cool](https://papers.cool/arxiv/2411.06764)] [[pdf](https://arxiv.org/pdf/2411.06764)]
> **Authors**: Hongsheng Zhang,Zhong Ji,Jingren Liu,Yanwei Pang,Jungong Han
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: No comments
- **标题**: 视力语言模型的多阶段知识整合用于持续学习
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在大规模图像文本数据集上进行了预训练的视觉语言模型（VLMS），对看不见的数据启用零射击预测，但在特定看不见的任务上可能表现不佳。持续学习（CL）可以帮助VLM有效地适应新的数据分布而无需联合培训，但面临着灾难性遗忘和概括遗忘的挑战。尽管通过基于蒸馏的方法取得了重大进展，但它们表现出两个严重的局限性。一种是普遍采用的单教师范式无法赋予全面的知识，另一种是现有的方法不足地利用了原始培训数据集中的多模式信息，而是它们依靠其他数据进行蒸馏，这增加了计算和存储的空间。为了减轻这两种局限性，通过利用知识整合理论（KIT），我们提出了一个多阶段知识集成网络（Mulki），以在蒸馏方法中模仿人类学习过程。 Mulki通过四个阶段实现这一目标，包括提出想法，添加新想法，区分思想和建立联系。在这四个阶段中，我们首先利用原型来对齐方式，引起跨模式知识，然后通过与原型构建细粒度内和模式间关系来增加新知识。之后，来自两个教师模型的知识被自适应地区分和重新加权。最后，我们之间的模型与任务间和任务之间的联系，整合了先前的知识和新知识。我们的方法表明，在维持零射击功能方面的显着改善，同时支持跨不同下游任务的持续学习，从而展示其在将VLMS调整到不断发展的数据分布中的潜力。

### MIRe: Enhancing Multimodal Queries Representation via Fusion-Free Modality Interaction for Multimodal Retrieval 
[[arxiv](https://arxiv.org/abs/2411.08334)] [[cool](https://papers.cool/arxiv/2411.08334)] [[pdf](https://arxiv.org/pdf/2411.08334)]
> **Authors**: Yeong-Joon Ju,Ho-Joong Kim,Seong-Whan Lee
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: preprint
- **标题**: MIRE：通过无融合模态相互作用增强多模式的查询表示
- **领域**: 计算机视觉和模式识别,人工智能,信息检索,多媒体
- **摘要**: 最近的多模式检索方法通过利用预训练策略进行视觉文本对齐，具有具有多模式能力的基于文本的检索器。他们经常将两种方式直接融合在一起，以便在对齐过程中了解多模式查询。但是，现有方法通常由于文本主导问题而忽略关键的视觉信息，这过于取决于文本驱动的信号。在本文中，我们介绍了Mire，这是一个检索框架，可实现模式相互作用，而无需在对齐过程中融合文本功能。我们的方法允许文本查询参与视觉嵌入，而不会将文本驱动的信号馈回视觉表示。此外，我们通过将简洁的问题 - 答案对构造为多模式查询检索的预训练数据集，以扩展段落。我们的实验表明，我们的预训练策略显着增强了对多模式查询的理解，从而在零摄影设置下的四个多模式检索基准中实现了强劲的性能。我们的代码公开可用：https：//github.com/yeongjoonju/mire。

### DPU: Dynamic Prototype Updating for Multimodal Out-of-Distribution Detection 
[[arxiv](https://arxiv.org/abs/2411.08227)] [[cool](https://papers.cool/arxiv/2411.08227)] [[pdf](https://arxiv.org/pdf/2411.08227)]
> **Authors**: Shawn Li,Huixian Gong,Hao Dong,Tiankai Yang,Zhengzhong Tu,Yue Zhao
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: No comments
- **标题**: DPU：多式模式脱离检测的动态原型更新
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 通过识别偏离训练分布的样本，分别分布（OOD）检测对于确保机器学习模型的鲁棒性至关重要。尽管传统的OOD检测主要集中在单模式输入上，例如图像，但多模式模型的最新进展证明了利用多种方式（例如，视频，光流，音频，音频）来增强检测性能的潜力。但是，假设同一类的样本具有完全凝聚力和一致性，现有方法通常会忽略分布（ID）数据中的类内变异性。此假设可能导致性能降解，尤其是当预测差异在所有样本中均匀放大时。为了解决此问题，我们提出了动态原型更新（DPU），这是一个用于多模式OOD检测的新颖的插入式框架，该框架涉及阶级内部变化。我们的方法通过测量每个批次中相似样本的方差，从而动态更新每个类别的类中心表示，从而实现自适应调整。这种方法使我们能够根据更新的班级中心扩大预测差异，从而改善模型跨不同模式的稳健性和概括。对两个任务，五个数据集和九个基本OOD算法进行的广泛实验表明，DPU显着改善了OOD检测性能，为多模式OOD检测设定了新的最新最新技术，在Far-OOD检测中提高了80％。为了促进可访问性和可重复性，我们的代码在Github上公开可用。

### Latent Space Disentanglement in Diffusion Transformers Enables Precise Zero-shot Semantic Editing 
[[arxiv](https://arxiv.org/abs/2411.08196)] [[cool](https://papers.cool/arxiv/2411.08196)] [[pdf](https://arxiv.org/pdf/2411.08196)]
> **Authors**: Zitao Shuai,Chenwei Wu,Zhengxu Tang,Bowen Song,Liyue Shen
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: arXiv admin note: substantial text overlap with arXiv:2408.13335
- **标题**: 扩散变压器中的潜在空间分离可以实现精确的零光语义编辑
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散变压器（DIT）最近在文本引导的图像生成方面取得了巨大的成功。在图像编辑中，将项目文本和图像输入放在联合潜在空间中，从中解码并合成新图像。但是，在很大程度上尚未探索多模式信息如何统一形成此关节空间以及它们如何指导合成图像的语义。在本文中，我们研究了DIT模型的潜在空间，并发现了两个关键属性：首先，DIT的潜在空间本质上是语义上的，其中可以通过特定的编辑方向来控制不同的语义属性。其次，一致的语义编辑需要利用整个联合潜在空间，因为既不编码图像也不包含足够的语义信息。我们表明，可以直接从文本提示中获得这些编辑说明，从而实现精确的语义控制，而无需其他培训或掩盖注释。基于这些见解，我们提出了一个简单而有效的编码识别操作（EIM）框架，用于零射击细粒图像编辑。具体来说，我们首先编码给定的源图像和描述图像的文本提示，以获取联合潜在嵌入。然后，使用我们提出的Hessian评分蒸馏采样（HSD）方法，我们确定在保留其他图像特征的同时控制特定目标属性的编辑方向。这些方向以文本提示为指导，并用于操纵潜在嵌入。此外，我们提出了一个新的指标，以量化扩散模型潜在空间的分离程度。我们新的策划基准数据集的广泛实验结果和分析表明，DIT的分离特性和EIM框架的有效性。

### GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation 
[[arxiv](https://arxiv.org/abs/2411.08033)] [[cool](https://papers.cool/arxiv/2411.08033)] [[pdf](https://arxiv.org/pdf/2411.08033)]
> **Authors**: Yushi Lan,Shangchen Zhou,Zhaoyang Lyu,Fangzhou Hong,Shuai Yang,Bo Dai,Xingang Pan,Chen Change Loy
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: project page: https://nirvanalan.github.io/projects/GA/
- **标题**: 高斯（Gaussianything）：3D代的交互点云潜扩散
- **领域**: 计算机视觉和模式识别,人工智能,图形
- **摘要**: 尽管3D内容生成已大大提高，但现有方法仍然面临着输入格式，潜在空间设计和输出表示形式的挑战。本文介绍了一个新颖的3D生成框架，该框架解决了这些挑战，并提供了可扩展的高质量3D生成，并具有交互性点云结构的潜在潜在空间。我们的框架采用了带有多视图的RGB-D（EPTH）-N（ORMAL）渲染的多视图的各种自动编码器（VAE）作为输入，使用了独特的潜在空间设计，可保留3D形状信息，并结合了层叠的潜在扩散模型，以改善形状型号的分离分离式分离。所提出的方法Ghussianything支持多模式的条件3D生成，允许点云，标题和单/多视图像输入。值得注意的是，新提出的潜在空间自然实现了几何形状 - 文本解开，从而允许3D感知编辑。实验结果证明了我们在多个数据集上的方法的有效性，在文本和图像条件的3D生成中都优于现有方法。

### JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation 
[[arxiv](https://arxiv.org/abs/2411.07975)] [[cool](https://papers.cool/arxiv/2411.07975)] [[pdf](https://arxiv.org/pdf/2411.07975)]
> **Authors**: Yiyang Ma,Xingchao Liu,Xiaokang Chen,Wen Liu,Chengyue Wu,Zhiyu Wu,Zizheng Pan,Zhenda Xie,Haowei Zhang,Xingkai yu,Liang Zhao,Yisong Wang,Jiaying Liu,Chong Ruan
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: No comments
- **标题**: Janusflow：统一自动化和整流流程，以进行统一的多模式理解和生成
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 我们提出Janusflow，这是一个强大的框架，可以在单个模型中统一图像理解和生成。 Janusflow推出了一种简约的体系结构，该体系结构将自回归语言模型与整流流程（一种生成建模的最新方法）集成在一起。我们的关键发现表明，可以在大型语言模型框架中直接训练整流的流程，从而消除了对复杂的建筑修改的需求。为了进一步提高统一模型的绩效，我们采用了两种关键策略：（i）将理解和生成编码器解耦，以及（ii）在统一培训期间对齐其表示形式。广泛的实验表明，Janusflow具有与其各自域中的专业模型相当或出色的性能，同时显着超过了跨标准基准的现有统一方法。这项工作代表了迈向更高效，更通用的视觉语言模型的一步。

### SimBase: A Simple Baseline for Temporal Video Grounding 
[[arxiv](https://arxiv.org/abs/2411.07945)] [[cool](https://papers.cool/arxiv/2411.07945)] [[pdf](https://arxiv.org/pdf/2411.07945)]
> **Authors**: Peijun Bao,Alex C. Kot
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: Technical report
- **标题**: Simbase：时间视频接地的简单基线
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了Simbase，这是时间视频接地的简单而有效的基线。尽管时间基础的最新进展导致了令人印象深刻的性能，但它们还将网络体系结构驱动到更大的复杂性，并采用一系列方法来捕获（1）捕获时间关系，并且（2）实现有效的多模式融合。相比之下，本文探讨了一个问题：简化方法的有效性如何？为了调查，我们设计了Simbase，这是一个利用轻质，一维时间卷积层而不是复杂的时间结构的网络。对于跨模式相互作用，Simbase仅采用元素的乘积而不是复杂的多模式融合。值得注意的是，Simbase在两个大规模数据集上实现了最先进的结果。作为一个简单而强大的基线，我们希望Simbase能够在时间视频接地中引发新的想法并简化未来评估。

### Towards Vision Mixture of Experts for Wildlife Monitoring on the Edge 
[[arxiv](https://arxiv.org/abs/2411.07834)] [[cool](https://papers.cool/arxiv/2411.07834)] [[pdf](https://arxiv.org/pdf/2411.07834)]
> **Authors**: Emmanuel Azuh Mensah,Anderson Lee,Haoran Zhang,Yitong Shan,Kurtis Heimerl
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: No comments
- **标题**: 朝着边缘的野生动植物监测专家的视觉混合物
- **领域**: 计算机视觉和模式识别
- **摘要**: 工业，消费者和遥感用例中的物联网传感器的爆炸率伴随着对计算基础架构的前所未有的需求，以传输和分析数据的数据。同时，世界正在逐渐将其专注力转向更可持续的计算。由于这些原因，最近一直在努力减少相关计算基础架构的足迹，尤其是通过深度学习算法，用于先进的见解生成。 “ Tinyml”社区正在积极提出方法来节省通信带宽和过度云存储成本，同时减少算法推断潜伏期并促进数据隐私。这样的方法理想情况下应处理多种类型的数据，包括时间序列，音频，卫星图像和视频，因为已经证明多个数据流可以提高学习算法的判别能力，尤其是用于生成细粒度的结果。顺便说一句，最近在数据驱动的子网络的条件计算上进行了一些工作，这些计算在使用单个模型在非常不同类型的输入（例如图像和文本）之间共享参数方面表现出了真正的进步，从而减少了多型多模式网络的计算要求。受此类工作的启发，我们首次为移动视觉变压器（仅视觉案例）探索了相似的每个贴片条件计算，最终将用于单高型多模式边缘模型。我们评估了康奈尔汁吸盘木材60的模型，这是一种细粒的鸟类歧视数据集。与MobileVitv2-1.0相比，我们的初始实验使用的参数少4倍，而作为SSW60数据集的一部分，inaturalist '21鸟类测试数据的准确度下降了1美元。

### Constraint-Aware Feature Learning for Parametric Point Cloud 
[[arxiv](https://arxiv.org/abs/2411.07747)] [[cool](https://papers.cool/arxiv/2411.07747)] [[pdf](https://arxiv.org/pdf/2411.07747)]
> **Authors**: Xi Cheng,Ruiqi Lei,Di Huang,Zhichao Liao,Fengyuan Piao,Yan Chen,Pingfa Feng,Long Zeng
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: No comments
- **标题**: 参数点云的约束感知功能学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 参数点云是从CAD形状采样的，并且在工业制造中变得越来越普遍。大多数现有的CAD特定深度学习方法仅关注几何特征，而忽略了在CAD形状中固有且重要的约束。这限制了他们辨别出具有相似外观但不同约束的CAD形状的能力。为了应对这一挑战，我们首先通过简单的验证实验来分析约束重要性。然后，我们介绍了具有三个矢量组件的深度学习友好的约束表示形式，并设计一个约束感知的特征学习网络（CSTNET），其中包括两个阶段。第1阶段根据形状本地信息从B-REP数据或点云中提取约束功能。在模型预训练后，它可以更好地概括能力，从而无法看到数据集。第2阶段采用注意层来自适应调整三个约束组件的权重。它促进了约束的有效利用。此外，我们构建了第一个多模式的参数 - 简单数据集，即param20k，其中包括75个类的约20K形状实例。在此数据集上，我们进行了分类和旋转鲁棒性实验，CSTNET在实例准确性中分别获得了3.52 \％和26.17 \％的绝对改进，分别对最先进的方法。据我们所知，CSTNET是针对CAD域中参数点云分析量身定制的第一个约束深度学习方法。

### Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG 
[[arxiv](https://arxiv.org/abs/2411.07688)] [[cool](https://papers.cool/arxiv/2411.07688)] [[pdf](https://arxiv.org/pdf/2411.07688)]
> **Authors**: Zilun Zhang,Haozhan Shen,Tiancheng Zhao,Yuhao Wang,Bin Chen,Yuxiang Cai,Yongheng Shang,Jianwei Yin
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: full paper
- **标题**: 通过Imagerag增强超高分辨率遥感成像分析
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 超高分辨率（UHR）遥感图像（RSI）（例如100,000 $ \ times $ 100,000像素或更多）对当前遥感多模式大语言模型（RSMLLMS）构成了重大挑战。如果选择将UHR图像大小调整到标准输入图像大小，则将忽略UHR图像所包含的广泛空间和上下文信息。否则，这些图像的原始大小通常超过标准rsmllms的令牌限制，因此很难处理整个图像并捕获远程依赖关系以基于丰富的视觉上下文来回答查询。在本文中，我们介绍了RS Imagerag，这是一个无训练的框架，以解决分析UHR遥感图像的复杂性。通过将UHR遥感图像分析任务转换为图像的长上下文选择任务，我们根据检索效果生成（RAG）技术设计创新的图像上下文检索机制，称为Imagerag。 Imagerag的核心创新在于它有能力选择性地检索和专注于UHR图像的最相关部分，作为与给定查询有关的视觉上下文。在此框架中提出了快速路径和慢速路径，以有效，有效地处理此任务。 Imagerag允许RSMLLM管理UHR RSI的广泛上下文和空间信息，从而确保分析既准确又有效。

### Understanding Audiovisual Deepfake Detection: Techniques, Challenges, Human Factors and Perceptual Insights 
[[arxiv](https://arxiv.org/abs/2411.07650)] [[cool](https://papers.cool/arxiv/2411.07650)] [[pdf](https://arxiv.org/pdf/2411.07650)]
> **Authors**: Ammarah Hashmi,Sahibzada Adil Shahzad,Chia-Wen Lin,Yu Tsao,Hsin-Min Wang
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: No comments
- **标题**: 理解视听深泡得出：技术，挑战，人为因素和感知见解
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体,声音,图像和视频处理
- **摘要**: 深度学习已成功地应用于不同的领域，其对深层检测的影响也不例外。 Deepfakes是假的但现实的合成内容，可以欺骗地用于政治模仿，网络钓鱼，诽谤或传播错误信息。尽管对单峰深层检测进行了广泛的研究，但通过对音频和视觉流的联合分析来识别复杂的深击仍然相对尚未探索。为了填补这一空白，这项调查首先提供了视听的深层生成技术，应用程序及其后果的概述，然后对结合音频和视觉模式的最先进方法进行了全面审查，以提高检测准确性，概述并进行彻底分析其强度和局限性。此外，我们讨论了现有的开源数据集，以深入了解，这可以为研究社区做出贡献，并向想要分析基于深度学习的视频取证方法的初学者提供必要的信息。通过弥合单峰和多模式方法之间的差距，本文旨在提高深击检测策略的有效性，并指导网络安全性和媒体完整性的未来研究。

### Multimodal Object Detection using Depth and Image Data for Manufacturing Parts 
[[arxiv](https://arxiv.org/abs/2411.09062)] [[cool](https://papers.cool/arxiv/2411.09062)] [[pdf](https://arxiv.org/pdf/2411.09062)]
> **Authors**: Nazanin Mahjourian,Vinh Nguyen
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: 使用深度和制造零件的图像数据的多模式对象检测
- **领域**: 计算机视觉和模式识别,人工智能,机器人技术
- **摘要**: 制造需要可靠的对象检测方法，以精确采摘和处理各种类型的制造零件和组件。传统对象检测方法仅利用来自相机的2D图像或来自LiDARS或类似3D传感器的3D数据。但是，这些传感器中的每一个都有弱点和局限性。相机没有深度感知，3D传感器通常不会带有颜色信息。这些弱点会破坏工业制造系统的可靠性和鲁棒性。为了应对这些挑战，这项工作提出了一个多传感器系统，该系统结合了红色绿色（RGB）摄像头和3D点云传感器。对两个传感器进行校准，以精确地对齐从两个硬件设备捕获的多模式数据。开发了一种新型的多模式检测方法来处理RGB和深度数据。该对象检测器基于最初设计用于仅处理相机图像的更快的R-CNN基线。结果表明，多模型模型在已建立的对象检测指标上明显胜过仅深度和仅RGB的基准。更具体地说，与仅RGB的基线相比，多模型模型将MAP提高了13％，并将平均精度提高了11.8％。与仅深度基线相比，它的地图提高了78％，并将平均精度提高了57％。因此，此方法有助于为智能制造应用提供更可靠和健壮的对象检测。

### A Transformer-Based Visual Piano Transcription Algorithm 
[[arxiv](https://arxiv.org/abs/2411.09037)] [[cool](https://papers.cool/arxiv/2411.09037)] [[pdf](https://arxiv.org/pdf/2411.09037)]
> **Authors**: Uros Zivanovic,Carlos Eduardo Cancino-Chacón
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: 9 pages, 2 figures
- **标题**: 基于变压器的视觉钢琴转录算法
- **领域**: 计算机视觉和模式识别
- **摘要**: 音乐表演的自动音乐转录（AMT）是音乐信息检索领域的一个长期存在的问题（MIR）。 Visual Piano转录（VPT）是AMT的多模式子问题，它专注于仅从视觉信息中提取钢琴性能的符号表示（例如，从钢琴键盘的自上而下的视频中）。受到变压器对基于音频的AMT的成功及其在其他计算机视觉任务中的成功的启发，在本文中，我们为VPT提供了基于变压器的体系结构。提出的VPT系统将钢琴边界框检测模型与发作和俯仰检测模型相结合，使我们的系统在更自然的条件下表现良好，例如钢琴周围的不完美图像作物和稍微倾斜的图像。

### CoMiX: Cross-Modal Fusion with Deformable Convolutions for HSI-X Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2411.09023)] [[cool](https://papers.cool/arxiv/2411.09023)] [[pdf](https://arxiv.org/pdf/2411.09023)]
> **Authors**: Xuming Zhang,Xingfa Gu,Qingjiu Tian,Lorenzo Bruzzone
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: comix：跨模式融合，具有可变形的卷积用于HSI-X语义分段
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过从补充数据类型中利用互补信息（称为X模式）来改善高光谱图像（HSI）语义分割是很有希望的，但由于成像传感器，图像内容和分辨率的差异而挑战。当前的技术难以增强模式特异性和模态共享的信息，并捕获不同方式之间的动态互动和融合。作为回应，这项研究提出了Comix，Comix是一种不对称的编码器架构，具有可变形的卷积（DCN），用于HSI-X语义分割。 Comix旨在从HSI和X数据提取，校准和融合信息。它的管道包括一个编码器，带有两个平行和相互作用的骨干和一个轻量级的全人使用者感知器（All-MLP）解码器。编码器由四个阶段组成，每个阶段都包含X模型的2D DCN块，以适应HSIS的几何变化和3D DCN块，以适应骨料的空间光谱特征。此外，每个阶段都包括一个交叉模式功能增强和交换（CMFEX）模块和特征融合模块（FFM）。 CMFEX旨在利用从不同模式的空间光谱相关性来重新校准和增强模态特定和模态共享的特征，同时自适应地交换它们之间的互补信息。来自CMFEX的输出被馈入FFM进行融合，并传递到下一阶段以进行进一步的信息学习。最后，每个FFM的输出由ALL-MLP解码器集成以进行最终预测。广泛的实验表明，我们的comix实现了卓越的性能，并可以很好地推广到各种多模式识别任务。 Comix代码将发布。

### Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions 
[[arxiv](https://arxiv.org/abs/2411.09018)] [[cool](https://papers.cool/arxiv/2411.09018)] [[pdf](https://arxiv.org/pdf/2411.09018)]
> **Authors**: Moran Yanuka,Assaf Ben Kish,Yonatan Bitton,Idan Szpektor,Raja Giryes
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: Accepted to NAACL 2025
- **标题**: 桥接视觉差距：带有知识适应标题的微调多模型模型
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 最近的研究越来越集中于带有长而详细的图像标题的训练视觉模型（VLM）。但是，小规模的VLM通常很难平衡这些标题的丰富性和在微调过程中幻觉的风险。在本文中，我们探讨了VLMS对此类字幕的适应程度。为了量化字幕质量，我们提出了分解的NLI（DNLI），这是一个评估框架，将生成的字幕分解为单个命题，并孤立地评估每个命题。这种细粒度的分析揭示了捕获描述性细节和防止幻觉之间的关键平衡。我们的发现表明，简单地降低标题复杂性或采用标准数据策划技术并不能有效地解决此问题。为了应对这一挑战，我们引入了适应性的知识（知识）微调，这是一种以数据为中心的方法，可以自动通过模型的现有知识和视觉理解来自动调整培训数据。知识量最大程度地减少了幻觉，同时保留了很高的描述性。我们在几个小规模的VLM（最多7B参数）和密集的字幕数据集中验证了这种方法，这表明知识有效地平衡了幻觉的降低和描述性。我们的结果表明，知识群在自动指标和人类评估中都优于各种基准。我们将发布我们的代码和模型。

### Predicting household socioeconomic position in Mozambique using satellite and household imagery 
[[arxiv](https://arxiv.org/abs/2411.08934)] [[cool](https://papers.cool/arxiv/2411.08934)] [[pdf](https://arxiv.org/pdf/2411.08934)]
> **Authors**: Carles Milà,Teodimiro Matsena,Edgar Jamisse,Jovito Nunes,Quique Bassat,Paula Petrone,Elisa Sicuri,Charfudin Sacoor,Cathryn Tonne
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: :J.4; I.2.1; I.4.9
- **标题**: 使用卫星和家用图像预测莫桑比克的家庭社会经济地位
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 许多研究都预测了使用卫星数据的诸如村庄的总空间单位（SEP）的社会经济地位（SEP），但是尚未探索家庭层面的SEP预测和其他图像来源。我们在莫桑比克南部的一个半农村地区组装了一个由975个家庭组成的数据集，其中包括自我报告的资产，支出和收入SEP数据，以及包括卫星图像和11个家庭元素的基于地面的照片调查，包括卫星图像和基于地面的照片调查。我们对卷积神经网络进行了微调，以从图像中提取特征向量，然后在回归分析中使用它们，用于使用不同的图像类型进行对家用SEP进行建模。当使用具有所有图像类型的随机森林模型对基于资产的SEP进行建模时，发现最佳预测性能，而基于支出和基于收入的SEP的性能较低。使用Shap，我们观察到具有最大正面和负面影响的图像之间的明显差异，并确定了预测中最相关的家庭元素。最后，我们仅使用已确定的相关家庭元素拟合了一个额外的减少模型，与使用所有图像的模型相比，该元素的性能仅略低。我们的结果表明，基于地面的家用照片如何允许从区域级别缩小到单个家庭预测，同时使用可解释的机器学习来最大程度地减少数据收集工作。开发的工作流程可能会被潜在地整合到常规的家庭调查中，其中收集的家用图像可用于其他目的，例如精制资产表征和环境暴露评估。

### Multimodal Instruction Tuning with Hybrid State Space Models 
[[arxiv](https://arxiv.org/abs/2411.08840)] [[cool](https://papers.cool/arxiv/2411.08840)] [[pdf](https://arxiv.org/pdf/2411.08840)]
> **Authors**: Jianing Zhou,Han Li,Shuai Zhang,Ning Xie,Ruijie Wang,Xiaohan Nie,Sheng Liu,Lingyun Wang
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: 使用混合状态空间模型进行多模式指令调整
- **领域**: 计算机视觉和模式识别
- **摘要**: 处理冗长的上下文对于增强在处理高分辨率图像或高帧速率视频等应用中的多模式大语言模型（MLLM）的识别和理解能力至关重要。由于输入令牌的数量增加，图像分辨率和帧速率的上升大大增加了计算需求。相对于自我发项机制的序列长度，二次复杂性进一步加剧了这一挑战。大多数先前的工作要么可以使用较长上下文的预训练模型，要忽略效率问题，或者试图通过降采样（例如，识别关键图像补丁或帧）来减少上下文长度以减少上下文长度，从而导致信息丢失。为了绕过这个问题，在保持MLLM的显着效果的同时，我们提出了一种新颖的方法，使用混合变压器模型，以有效处理多模式应用中的长篇小说。我们的多模式模型可以有效地处理超过100K令牌的长上下文输入，从而超过各种基准测试的现有模型。值得注意的是，与当前模型相比，我们的模型提高了高分辨率图像和高帧速率视频的推理效率约4倍，随着图像分辨率或视频框架的上升，效率提高的提高。此外，我们的模型是第一个在低分辨率图像或低帧速率视频上接受培训的模型，同时能够推断高分辨率图像和高帧速率视频，从而为在不同情况下的推断提供了灵活性。

### Retrieval Augmented Recipe Generation 
[[arxiv](https://arxiv.org/abs/2411.08715)] [[cool](https://papers.cool/arxiv/2411.08715)] [[pdf](https://arxiv.org/pdf/2411.08715)]
> **Authors**: Guoshan Liu,Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo,Yu-Gang Jiang
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: ACCEPT on IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2025
- **标题**: 检索增强食谱生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 鉴于从食品图像中生成食谱的潜在应用，近年来，该领域引起了研究人员的极大关注。现有的食谱生成作品主要采用两阶段的训练方法，首先生成成分，然后从图像和成分中获得指令。大型多模式模型（LMM）在各种视觉和语言任务中取得了显着的成功，从图像中直接从图像中生成成分和说明。然而，LMM在食谱生成过程中仍然面临着幻觉的共同问题，导致了次优的性能。为了解决这个问题，我们提出了一种用于食谱生成的大型大型多模式。我们首先介绍随机多元化的检索增强（SDRA），以从现有数据存储中检索与图像的语义相关的配方作为补充，将它们集成到提示中，以在输入图像中添加多样而丰富的上下文。此外，提出了自信合奏投票机制，以确定最自信的预测食谱作为最终产出。它计算出生成的食谱候选者之间的一致性，这些食谱候选者使用不同的检索配方作为生成背景。广泛的实验验证了我们提出的方法的有效性，该方法证明了在配方1M数据集上的配方生成任务中的最新性能（SOTA）性能。

### TRACE: Transformer-based Risk Assessment for Clinical Evaluation 
[[arxiv](https://arxiv.org/abs/2411.08701)] [[cool](https://papers.cool/arxiv/2411.08701)] [[pdf](https://arxiv.org/pdf/2411.08701)]
> **Authors**: Dionysis Christopoulos,Sotiris Spanos,Valsamis Ntouskos,Konstantinos Karantzalos
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: 痕迹：基于变压器的临床评估风险评估
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 我们提出了痕量（基于变压器的临床评估风险评估），这是一种基于临床数据的临床风险评估的新方法，利用了自我发挥的机制来增强功能相互作用和结果解释。我们的方法能够处理不同的数据模式，包括连续，分类和多项选择（复选框）属性。所提出的体系结构具有通过整合每个数据模式的专门嵌入来获得的临床数据的共享表示，从而可以使用变压器编码器层检测高风险个体。为了评估所提出方法的有效性，引入了基于非负多层感知器（MLP）的强基线。所提出的方法的表现优于在临床风险评估领域广泛使用的各种基线，同时有效地处理缺失值。在解释性方面，我们基于变压器的方法通过注意力重量提供了易于解释的结果，从而进一步增强了临床医生的决策过程。

### A Survey on Vision Autoregressive Model 
[[arxiv](https://arxiv.org/abs/2411.08666)] [[cool](https://papers.cool/arxiv/2411.08666)] [[pdf](https://arxiv.org/pdf/2411.08666)]
> **Authors**: Kai Jiang,Jiaxing Huang
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: This work will be integrated into another project
- **标题**: 有关视力自回旋模型的调查
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 自回归模型在自然语言处理（NLP）方面表现出色，具有令人印象深刻的可伸缩性，适应性和可推广性。受其在NLP领域取得杰出成功的启发，最近对计算机视觉进行了深入的研究，该模型通过将视觉数据表示为视觉令牌，并为广泛的视觉任务做出自动回归建模，从而执行了下一步的预测，从视觉生成和视觉上的理解到最新的多媒体生成，并与单个自动化的模型结合了单个自动效果模型。本文对视力自回归模型进行了系统的审查，包括开发现有方法的分类法，并强调其主要贡献，优势和局限性，涵盖了各种视觉任务，例如图像生成，视频生成，图像编辑，运动生成，运动图像分析，3D代生成，3D代生成，机器人的机器人操作，并在统一的多态生成和分析中，我们在范围内进行了详尽的研究，我们将详细介绍，我们在范围内进行详尽的研究，包括自动的多模式，综合性，综合了，我们的详细信息，包括自动的范围，综合了，我们的讨论，包括自动的范围，构成了范围，并分析了，我们的范围，包括自动的范围，我们在范围内进行了综合，我们的讨论，以及众所周知的范围，以及众所周知的模型，综合了范围。各种评估数据集的方法。最后，我们概述了未来研究的主要挑战和有希望的方向，提供了路线图，以指导视觉自回旋模型的进一步进步。

### Towards More Accurate Fake Detection on Images Generated from Advanced Generative and Neural Rendering Models 
[[arxiv](https://arxiv.org/abs/2411.08642)] [[cool](https://papers.cool/arxiv/2411.08642)] [[pdf](https://arxiv.org/pdf/2411.08642)]
> **Authors**: Chengdong Dong,Vijayakumar Bhagavatula,Zhenyu Zhou,Ajay Kumar
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: 13 pages, 8 Figures
- **标题**: 在先进的生成和神经渲染模型产生的图像上更加准确地检测
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 神经网络驱动的视觉数据生成，尤其是神经渲染技术（例如神经辐射场和3D高斯裂开）的显着进步，为gan和扩散模型提供了强大的替代方法。这些方法可以产生高保真图像和栩栩如生的化身，从而突出了对强大检测方法的需求。作为响应，提出了一种无监督的训练技术，该技术使该模型能够从傅立叶光谱幅度中提取全面的特征，从而克服了由于其中心对称特性而重建光谱的挑战。通过利用光谱域并将其与空间域信息组合，我们创建了一个强大的多模式检测器，该检测器在识别最新图像合成技术产生的挑战性合成图像时展示了出色的概括能力。为了解决缺乏基于3D神经渲染的假图像数据库，我们开发了一个全面的数据库，其中包括由不同的神经渲染技术生成的图像，为评估和前进的检测方法提供了强大的基础。

### LG-Gaze: Learning Geometry-aware Continuous Prompts for Language-Guided Gaze Estimation 
[[arxiv](https://arxiv.org/abs/2411.08606)] [[cool](https://papers.cool/arxiv/2411.08606)] [[pdf](https://arxiv.org/pdf/2411.08606)]
> **Authors**: Pengwei Yin,Jingjing Wang,Guanzhong Zeng,Di Xie,Jiang Zhu
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: Accepted to ECCV 2024
- **标题**: LG凝视：学习几何学意识连续提示语言引导的目光估计
- **领域**: 计算机视觉和模式识别
- **摘要**: 凝视估计模型概括的能力通常受到与凝视无关的各种因素的限制，尤其是当训练数据集受到限制时。当前的策略旨在通过不同的领域泛化技术来应对这一挑战，但是由于仅依靠价值标签进行回归时，他们的成功率有限。预训练的视觉模型的最新进展激发了我们利用可用的大量语义信息。我们在本文中提出了一种新颖的方法，将目光估算任务重新构架为视觉对准问题。我们提出的框架，名为语言引导的凝视估计（LG DAIN），从远见模型的先前知识中学习了连续且对几何敏感的特征，从而获得了凝视估计的益处。具体而言，LG凝视通过我们提出的多模式对比回归损失将视线特征与连续的语言特征相结合，该损失为不同的负样本定制自适应权重。此外，为了更好地适应凝视估计任务的标签，我们提出了一种几何学意识到的插值方法，以获取更精确的凝视嵌入。通过广泛的实验，我们验证了四个不同的跨域评估任务中框架的功效。

### NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation 
[[arxiv](https://arxiv.org/abs/2411.08579)] [[cool](https://papers.cool/arxiv/2411.08579)] [[pdf](https://arxiv.org/pdf/2411.08579)]
> **Authors**: Youzhi Liu,Fanglong Yao,Yuanchang Yue,Guangluan Xu,Xian Sun,Kun Fu
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: NAVAGENT：多尺度的Urban Street View Fusion for UAV具体视觉和语言导航
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 视觉和语言导航（VLN）作为一个广泛讨论的体现智能的研究方向，旨在使体现的代理通过自然语言命令在复杂的视觉环境中导航。大多数现有的VLN方法都集中在室内机器人方案上。但是，当在户外城市场景中应用于无人机VLN时，面临两个重大挑战。首先，城市场景包含许多物体，这使得与这些地标的复杂文本描述中的图像中的细粒度地标匹配，这使其具有挑战性。其次，总体环境信息包含多种模态维度，并且表示的多样性显着增加了编码过程的复杂性。为了应对这些挑战，我们提出了NAVAGENT，这是由大型视觉模型驱动的第一个Urban UAV体现的导航模型。 Navatent通过综合多尺度环境信息（包括拓扑图（全球），全景（中）和细粒度的地标（本地）来执行导航任务。具体来说，我们利用GLIP来建立一个视觉识别器，以识别和语言化细粒度地标。随后，我们开发了动态增长的场景拓扑图，以整合环境信息并采用图形卷积网络来编码全球环境数据。此外，为了训练“地标视觉识别器”，我们开发了Navateent-Landmark2k，这是第一个用于真正的城市街景场景的精细颗粒地标数据集。在在达阵和MAP2SEQ数据集上进行的实验中，Navagent的表现优于强大的基线模型。该代码和数据集将被发布给社区，以促进户外VLN的探索和开发。

### Can MLLMs Guide Weakly-Supervised Temporal Action Localization Tasks? 
[[arxiv](https://arxiv.org/abs/2411.08466)] [[cool](https://papers.cool/arxiv/2411.08466)] [[pdf](https://arxiv.org/pdf/2411.08466)]
> **Authors**: Quan Zhang,Yuxin Qi
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: MLLM可以指导弱监督的时间动作本地化任务吗？
- **领域**: 计算机视觉和模式识别
- **摘要**: 在多模式大语言模型（MLLM）中的最新突破已在深度学习社区中获得了重大认可，在深度学习社区中，视频基础模型（VFM）和大语言模型（LLMS）的融合已证明在构建强大的视频理解系统方面有效地构建了与预期的视觉任务相关的约束。这些复杂的MLLM在理解视频方面表现出了极大的熟练程度，迅速在各种基准测试中达到了前所未有的绩效水平。但是，他们的操作需要大量的内存和计算资源，强调了传统模型在视频理解任务中的持续重要性。在本文中，我们介绍了一种新颖的学习范式，称为mllm4wtal。该范式利用了MLLM提供时间动作关键语义和完整的语义先验的潜力，用于常规弱监督的时间动作定位（WTAL）方法。 MLLM4WTAL通过利用MLLM指导来促进WTAL的增强。它通过集成两个不同的模块来实现这一目标：键语义匹配（KSM）和完整的语义重建（CSR）。这些模块串联起作用，可以有效解决WTAL方法中常见的不完整和过度完整结果等普遍问题。进行了严格的实验，以验证我们提出的方法在增强各种异质WTAL模型的性能方面的功效。

### V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with Denoising Diffusion 
[[arxiv](https://arxiv.org/abs/2411.08402)] [[cool](https://papers.cool/arxiv/2411.08402)] [[pdf](https://arxiv.org/pdf/2411.08402)]
> **Authors**: Xun Huang,Jinlong Wang,Qiming Xia,Siheng Chen,Bisheng Yang,Xin Li,Cheng Wang,Chenglu Wen
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: Accepted by CVPR2025
- **标题**: V2X-R：用于3D对象检测的合作激光雷达-4D雷达融合，并通过降解扩散
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的车辆到所有设施（V2X）系统使用LIDAR和相机数据显着增强了3D对象检测。但是，这些方法在不利天气条件下遭受性能降解。 WeatherRobust 4D雷达提供多普勒和其他几何信息，从而提高了应对这一挑战的可能性。为此，我们提出了V2X-R，这是第一个模拟的V2X数据集，其中包含LIDAR，相机和4D雷达。 V2X-R包含12,079个场景，带有37,727帧的LIDAR和4D雷达点云，150,908张图像和170,859个注释的3D车辆边界框。随后，我们提出了一种新型的合作激光雷达-4D雷达融合管道，用于3D对象检测，并通过各种融合策略实施它。为了实现天气风格的检测，我们还提出了在融合管道中的多模式去胶扩散（MDD）模块。 MDD利用天气稳定的4D雷达功能作为条件，以促使扩散模型DENOIS嘈杂的激光雷达功能。实验表明，我们的Lidar-4D雷达融合管道在V2X-R数据集中表现出卓越的性能。除此之外，我们的MDD模块进一步提高了基本融合模型的性能，在有雾/雪的条件下，最高可达5.73％/6.70％，几乎不会破坏正常性能。数据集和代码将在以下网址公开可用：https：//github.com/ylwhxht/v2x-r。

### High-resolution optical and acoustic remote sensing datasets of the Puck Lagoon, Southern Baltic 
[[arxiv](https://arxiv.org/abs/2411.08712)] [[cool](https://papers.cool/arxiv/2411.08712)] [[pdf](https://arxiv.org/pdf/2411.08712)]
> **Authors**: Łukasz Janowski,Dimitrios Skarlatos,Panagiotis Agrafiotis,Paweł Tysiąc,Andrzej Pydyn,Mateusz Popek,Anna M. Kotarba-Morley,Gottfried Mandlburger,Łukasz Gajewski,Mateusz Kołakowski,Alexandra Papadaki,Juliusz Gajewski
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: 波罗的海南部的冰球泻湖的高分辨率光学和声学遥感数据集
- **领域**: 计算机视觉和模式识别,信号处理
- **摘要**: 波罗的海南部北部海岸的帕克泻湖非常浅的海洋盆地，拥有宝贵的底栖栖息地和文化遗产。其中包括受保护的Zostera码头Meadows，波罗的海主要的中世纪港口之一，船墓地以及可能尚未发现的其他淹没特征。在进行此项目之前，该领域尚无全面的高分辨率遥感数据。本文介绍了第一台数字高程模型（DEMS），这些模型（DEMS）来自机载的测深雷达，多冰组回声器，机载摄影测量法和卫星图像的组合。这些数据集还包括多光束回声器的反向散射和激光强度，从而确定了海底的特征和特性。这些数据集结合在一起，是评估和了解海底形态，底栖栖息地，文化遗产和淹没景观的重要资源。鉴于Puck Lagoon的水文，生态，地质和考古环境的重要性，我们项目获得的高分辨率测深，可以为这一感兴趣领域的可持续管理和知情决策提供基础。

### JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging 
[[arxiv](https://arxiv.org/abs/2411.09933)] [[cool](https://papers.cool/arxiv/2411.09933)] [[pdf](https://arxiv.org/pdf/2411.09933)]
> **Authors**: Kaito Baba,Ryota Yagi,Junichiro Takahashi,Risa Kishikawa,Satoshi Kodera
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: Accepted by NeurIPS'24 Workshop on AIM-FM: Advancements In Medical Foundation Models: Explainability, Robustness, Security, and Beyond
- **标题**: Jradievo：日本放射学报告生成模型通过模型合并的进化优化增强
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,神经和进化计算
- **摘要**: 随着大语言模型（LLM）的快速发展，基础模型（FMS）已取得了重大进步。鉴于医生分析大量患者数据所需的大量时间和精力，医疗保健是这些FMS最关键的应用领域之一。最近的努力集中在通过教学策略等技术将多模式FMS适应医疗领域，从而导致医疗基金会模型（MFMS）的发展。但是，这些方法通常需要大量的培训数据，以有效地适应医疗领域。此外，大多数现有的模型都在英语数据集上进行了培训，从而限制了他们在说话的非英语区域的实用性，在这些地区，医疗保健专业人员和患者并不总是能流利的英语。翻译的需求引入了额外的费用和效率低下。为了应对这些挑战，我们提出了一个\ textbf {j} apanese \ textbf {radi} ology报告生成模型，该模型通过\ textbf {evo}模型合并（JRADIEVO）的效率优化。这是通过模型合并的进化优化将非医学视觉基础模型扩展到医疗领域的首次尝试。我们成功地创建了一个模型，该模型仅使用公开数据中的50个翻译样本从X射线图像中生成准确的日语报告。该模型以高效利用有限数据的高效开发，超过了在更大的数据集上培训的最新研究的领先模型。此外，只有80亿个参数，该相对紧凑的基础模型可以在医院内部部署，这使其成为由于严格的隐私和安全要求而无法使用API​​和其他外部服务的环境解决方案。

### Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level 
[[arxiv](https://arxiv.org/abs/2411.09921)] [[cool](https://papers.cool/arxiv/2411.09921)] [[pdf](https://arxiv.org/pdf/2411.09921)]
> **Authors**: Andong Deng,Tongjia Chen,Shoubin Yu,Taojiannan Yang,Lincoln Spencer,Yapeng Tian,Ajmal Saeed Mian,Mohit Bansal,Chen Chen
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: 运动接地的视频推理：在像素级别的理解和感知运动
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在本文中，我们介绍了运动座的视频推理，这是一项新的运动理解任务，需要根据输入问题生成视觉答案（视频分割掩码），因此需要隐含的时空推理和接地。这项任务通过通过问题启用隐性推理，将重点放在明确的动作/运动接地上的现有时空接地工作扩展到了更通用的格式。为了促进新任务的开发，我们收集了一个名为GroundMore的大规模数据集，该数据集包括1,715个视频剪辑，249k对象蒙版，这些镜头是故意设计的，具有4种问题类型（因果关系，顺序，反事实和描述性），用于基于深度和全面的运动推理能力。 GroundMore独特需要模型来产生视觉答案，比纯文本提供了更具体和视觉解释的响应。它评估了时空接地和推理的模型，以促进与运动相关的视频推理，时间感知和像素级的理解中的复杂挑战。此外，我们介绍了一种名为运动界视频推理助理（MORA）的新颖基线模型。莫拉（Mora）从多模式LLM，从接地模型（SAM）中的像素级感知能力以及来自轻量级定位头的时间感知能力的多模式推理能力。莫拉（Mora）在地面上取得了可观的表现，相对较高的现有视觉接地基线模型的表现平均相对21.5％。我们希望这项小说和挑战性的任务将通过视频推理细分为未来的发展和一般运动理解铺平道路

### A Self-Supervised Model for Multi-modal Stroke Risk Prediction 
[[arxiv](https://arxiv.org/abs/2411.09822)] [[cool](https://papers.cool/arxiv/2411.09822)] [[pdf](https://arxiv.org/pdf/2411.09822)]
> **Authors**: Camille Delgrange,Olga Demler,Samia Mora,Bjoern Menze,Ezequiel de la Rosa,Neda Davoudi
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: Accepted as oral paper at AIM-FM workshop, Neurips 2024
- **标题**: 多模式中风风险预测的自我监督模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 预测中风风险是一个复杂的挑战，可以通过整合各种临床可用的数据模式来增强。这项研究介绍了一个自制的多模式框架，该框架结合了3D脑成像，临床数据和图像衍生的特征，以改善发作前的中风风险预测。通过利用大型未经注释的临床数据集，该框架可以捕获图像和表格数据方式的互补和协同信息。我们的方法基于对比的学习框架，该框架将对比的语言图像与图像 - 纽带匹配模块进行了对比，以更好地对齐共享潜在空间中的多模式数据表示。该模型在英国生物库中进行了培训，其中包括结构性大脑MRI和临床数据。我们使用表格，图像和图像 - 纽带组合在多种冻结和可训练的模型设置下使用表格，图像和图像 - 纽带组合对其最新的单峰和多模式方法进行基准测试。所提出的模型在ROC-AUC中的自我监督表格（图像）方法的表现优于2.6％（2.6％），平衡精度优于3.3％（5.6％）。此外，与最佳的多模式监督模型相比，它的平衡精度增加了7.6％。通过可解释的工具，我们的方法可以更好地整合表格和图像数据，从而提供了更丰富，更合适的嵌入。梯度加权类激活映射热图进一步揭示了文献中通常与脑衰老，中风风险和临床结局相关的激活大脑区域。这种强大的自我监管的多模式框架超过了中风风险预测的最先进方法，并为将来的研究提供了强大的基础，以整合多种数据模式以推动临床预测建模。

### Towards Neural Foundation Models for Vision: Aligning EEG, MEG, and fMRI Representations for Decoding, Encoding, and Modality Conversion 
[[arxiv](https://arxiv.org/abs/2411.09723)] [[cool](https://papers.cool/arxiv/2411.09723)] [[pdf](https://arxiv.org/pdf/2411.09723)]
> **Authors**: Matteo Ferrante,Tommaso Boccato,Grigorii Rashkov,Nicola Toschi
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: 迈向视力的神经基础模型：对脑电图，梅格和fMRI表示，用于解码，编码和模态转换
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 本文提出了一种新的方法，用于创建一种基础模型，以通过利用对比度学习来对齐大脑活动的神经数据和视觉刺激。我们使用了脑电图（EEG），磁脑电图（MEG）和功能性磁共振成像（fMRI）数据。通过三个关键实验来证明我们的框架的功能：从神经数据中解码视觉信息，将图像编码为神经表示以及在神经模态之间转换。结果突出了该模型在不同脑成像技术上准确捕获语义信息的能力，这说明了其在解码，编码和模态转换任务中的潜力。

### MagicQuill: An Intelligent Interactive Image Editing System 
[[arxiv](https://arxiv.org/abs/2411.09703)] [[cool](https://papers.cool/arxiv/2411.09703)] [[pdf](https://arxiv.org/pdf/2411.09703)]
> **Authors**: Zichen Liu,Yue Yu,Hao Ouyang,Qiuyu Wang,Ka Leong Cheng,Wen Wang,Zhiheng Liu,Qifeng Chen,Yujun Shen
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: Code and demo available at https://magic-quill.github.io
- **标题**: MagicQuill：智能互动图像编辑系统
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像编辑涉及各种复杂的任务，需要有效，精确的操纵技术。在本文中，我们提出了MagicQuill，这是一个集成的图像编辑系统，可以快速实现创意。我们的系统具有简化但功能强大的接口，允许用最小输入的最小输入表达编辑操作（例如，插入元素，擦除对象，更改颜色）。这些相互作用由多模式大语言模型（MLLM）监视，以实时预测编辑意图，从而绕开了对明确提示的需求。最后，我们通过精心学习的两个分支插件模块增强了功能强大的扩散先验，以精确控制处理编辑请求。实验结果证明了MagicQuill在实现高质量图像编辑方面的有效性。请访问https://magic-quill.github.io尝试我们的系统。

### Advancing Fine-Grained Visual Understanding with Multi-Scale Alignment in Multi-Modal Models 
[[arxiv](https://arxiv.org/abs/2411.09691)] [[cool](https://papers.cool/arxiv/2411.09691)] [[pdf](https://arxiv.org/pdf/2411.09691)]
> **Authors**: Wei Wang,Zhaowei Li,Qi Xu,Linfeng Li,YiQing Cai,Botian Jiang,Hang Song,Xingcan Hu,Pengyu Wang,Li Xiao
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: 通过多模型中的多尺度对齐来推进细粒的视觉理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大型语言模型（MLLM）在跨多个任务的细粒度视觉理解方面取得了巨大的成功。但是，由于对精细元素知识的一致性不足，他们经常遇到重大挑战，这限制了他们准确捕获当地细节并获得全面的全球感知的能力。尽管最近的进步集中在将对象表达式与接地信息保持一致，但它们通常缺乏对象图像的明确集成，这些对象图像包含富裕的信息，而不是单纯的文本或坐标。为了弥合这一差距，我们介绍了一种新颖的细粒视觉知识对准方法，该方法有效地对齐并整合了对象的多尺度知识，包括文本，坐标和图像。这种创新方法的基础是我们的多尺度细粒增强数据综合管道，该管道提供了超过300K的基本培训数据，以增强对齐方式并提高整体性能。此外，我们提出了TinyGroundingGpt，这是一系列针对高级比对的紧凑模型。在大约3b参数的比例尺中，TinyGroundingGpt在接地任务中取得了出色的成果，同时在复杂的视觉场景中提供了与较大的MLLM相当的性能。

### Image Regeneration: Evaluating Text-to-Image Model via Generating Identical Image with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.09449)] [[cool](https://papers.cool/arxiv/2411.09449)] [[pdf](https://arxiv.org/pdf/2411.09449)]
> **Authors**: Chutian Meng,Fan Ma,Jiaxu Miao,Chi Zhang,Yi Yang,Yueting Zhuang
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: 图像再生：通过使用多模式模型生成相同的图像来评估文本对图像模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散模型振兴了图像产生领域，在学术研究和艺术表达中扮演着至关重要的角色。随着新扩散模型的出现，评估文本对图像模型的性能变得越来越重要。当前的指标集中于将输入文本与生成的图像直接匹配，但由于跨模式信息不对称，这会导致不可靠或不完整的评估结果。在此激励的情况下，我们在本研究中介绍了图像再生任务，以通过根据参考图像生成图像来评估文本对图像模型。我们使用GPT4V来弥合T2i模型的参考图像和文本输入之间的差距，从而允许T2i模型理解图像内容。由于生成的图像和参考图像之间的比较很简单，因此简化了此评估过程。引入了两个涵盖内容多样性和样式多样性评估数据集的再生数据集，以评估当前可用的领先扩散模型。此外，我们提出了ImagerePainter框架，以通过MLLM指导迭代生成和修订来提高内容理解，从而提高产生的图像的质量。我们的全面实验展示了该框架在评估模型的生成能力方面的有效性。通过利用MLLM，我们已经证明了强大的T2M可以产生更类似于参考图像的图像。

### Spider: Any-to-Many Multimodal LLM 
[[arxiv](https://arxiv.org/abs/2411.09439)] [[cool](https://papers.cool/arxiv/2411.09439)] [[pdf](https://arxiv.org/pdf/2411.09439)]
> **Authors**: Jinxiang Lai,Jie Zhang,Jun Liu,Jian Li,Xiaocheng Lu,Song Guo
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: 蜘蛛：任何一流的多模式LLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式LLM（MLLM）已成为大型语言模型（LLM）的扩展，从而使各种模式的集成。但是，任何一对一的MLLM都仅限于在单个响应中生成成对模式的“文本 + x”，例如文本 + {image或Audio或Video}。为了解决此限制，我们介绍了蜘蛛，这是一种新颖的有效到达的模式生成（AMMG）框架，它可以生成模态的“文本 + XS”的任意组合，例如文本 + {图像，音频和视频}。为了实现有效的AMMG，我们的蜘蛛集成了三个核心组件：基本X-TO-TO-X（即任何一对一）模态处理的基本模型，这是一种用于控制多模式解码器的新型有效解码器 - 控制器，以生成XS（许多模态）内容，并为生产XS信号提示。为了训练蜘蛛，我们构建了一种新颖的文本形式多模式（TMM）数据集，该数据集有助于学习AMMG所需的X-TO-TO-XS（即任何一对一的）功能。最终，训练有素的蜘蛛生成了伪X-TO-XS数据集，这是有史以来第一个X-TO-XS多模式数据集，从而增强了未来研究中AMMG任务的潜力。总体而言，这项工作不仅推动了多模式交互的边界，而且还为前进的领域提供了丰富的数据支持。

### Script-centric behavior understanding for assisted autism spectrum disorder diagnosis 
[[arxiv](https://arxiv.org/abs/2411.09413)] [[cool](https://papers.cool/arxiv/2411.09413)] [[pdf](https://arxiv.org/pdf/2411.09413)]
> **Authors**: Wenxing Liu,Yueran Pan,Ming Li
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: 5 pages, 4 figures, submitted to ICASSP 2025
- **标题**: 以脚本为中心的自闭症频谱障碍诊断的行为理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 观察和分析儿童的社交行为对于早期诊断自闭症谱系障碍（ASD）至关重要。这项工作着重于使用计算机视觉技术和大型语言模型（LLMS）自动检测ASD。现有方法通常依赖于监督学习。但是，ASD诊断数据集的稀缺性以及诊断结果中缺乏可解释性的稀缺性大大限制了其临床应用。为了应对这些挑战，我们基于以脚本为中心的行为理解引入了一种新颖的无监督方法。我们的管道将视频内容转换为描述字符行为的脚本，利用大型语言模型以零击或几次拍摄方式检测ASD的概括性。具体来说，我们为多模式行为数据文本化提出了一个脚本转录模块，并且域提示模块桥接LLMS。我们的方法在平均24个月的儿童诊断ASD方面达到了92.00 \％的准确性，绝对超过监督学习方法的表现，绝对超过3.58 \％。广泛的实验证实了我们方法的有效性，并提出了通过LLMS推进ASD研究的潜力。

### LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation 
[[arxiv](https://arxiv.org/abs/2411.09301)] [[cool](https://papers.cool/arxiv/2411.09301)] [[pdf](https://arxiv.org/pdf/2411.09301)]
> **Authors**: Zhenshi Li,Dilxat Muhtar,Feng Gu,Xueliang Zhang,Pengfeng Xiao,Guangjun He,Xiaoxiang Zhu
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: LHRS-Bot-Nova：改进的多式模式模型用于遥感视觉解释
- **领域**: 计算机视觉和模式识别
- **摘要**: 自动，快速地了解地球的表面是我们对生活环境和知情决策的掌握的基础。这强调了对统一系统具有全面能力来分析地球表面以满足各种人类需求的需求。多模式大语模型（MLLM）的出现在提高智能地球观察的效率和便利性方面具有巨大的潜力。这些模型可以进行类似人类的对话，用作统一的平台，以理解图像，遵循各种说明并提供有见地的反馈。在这项研究中，我们介绍了LHRS-Bot-Nova，这是一个专门了解遥感图像（RS）图像的MLLM，旨在专业地执行与人类指示相符的广泛RS理解任务。 LHRS-Bot-Nova具有增强的视觉编码器和新颖的桥梁层，可有效地视觉压缩和更好的语言视觉对齐。为了进一步增强面向RS的视力语言对齐，我们提出了一个大规模的RS图像符号数据集，该数据集是通过特征引导的图像重新启动生成的。此外，我们介绍了一个专门设计的指令数据集，以提高空间识别能力。广泛的实验表明，LHRS-Bot-Nova在各种RS图像理解任务中的表现出色。我们还使用复杂的多项选择问题评估基准评估了复杂的RS感知和指导中的不同MLLM表现，为将来的模型选择和改进提供了可靠的指南。数据，代码和模型将在https://github.com/nju-lhrs/lhrs-bot上找到。

### LES-Talker: Fine-Grained Emotion Editing for Talking Head Generation in Linear Emotion Space 
[[arxiv](https://arxiv.org/abs/2411.09268)] [[cool](https://papers.cool/arxiv/2411.09268)] [[pdf](https://arxiv.org/pdf/2411.09268)]
> **Authors**: Guanwen Feng,Zhihao Qian,Yunan Li,Siyu Jin,Qiguang Miao,Chi-Man Pun
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: Les-Talker：在线性情感空间中谈论头部发电的精细颗粒情绪编辑
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管现有的一声谈话校长模型在粗粒度的情感编辑中取得了进步，但仍然缺乏具有高解释性的精细颗粒情感编辑模型。我们认为，要将一种被视为细粒度的方法，它需要提供明确的定义和足够详细的区分。我们提出了Les-Talker，这是一种具有高解释性的新颖的单次说话的校长生成模型，以实现跨情绪类型，情感水平和面部单位的精细颗粒情绪编辑。我们提出了基于面部动作单元的线性情感空间（LES）定义，以将情感转换描述为向量转换。我们设计了跨维注意网（CDAN），以深入挖掘LES表示与3D模型表示之间的相关性。通过在不同特征和结构维度上挖掘多个关系，我们使LES表示能够指导3D模型的可控变形。为了使多模式数据偏离LES并提高视觉质量，我们利用了专业的网络设计和培训策略。实验表明，我们的方法提供了高视觉质量以及多级和可解释的细粒情绪编辑，优于主流方法。

### How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception 
[[arxiv](https://arxiv.org/abs/2411.09266)] [[cool](https://papers.cool/arxiv/2411.09266)] [[pdf](https://arxiv.org/pdf/2411.09266)]
> **Authors**: Sahibzada Adil Shahzad,Ammarah Hashmi,Yan-Tsung Peng,Yu Tsao,Hsin-Min Wang
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: 在视听深击检测中，Chatgpt的表现如何：AI模型和人类感知的比较研究
- **领域**: 计算机视觉和模式识别,人工智能,人机交互,机器学习,多媒体
- **摘要**: 涉及视听操作的多模式深击是一种日益严重的威胁，因为它们很难用肉眼检测到它们，或者使用单峰深度学习的伪造方法。视听法医模型虽然比单峰模型更有能力，但需要大型的培训数据集，并且在培训和推理中计算昂贵。此外，这些模型缺乏可解释性，并且通常不能很好地推广到看不见的操作。在这项研究中，我们研究了大语言模型（LLM）（即chatgpt）的检测功能，以识别并说明视觉和听觉的视觉和听觉伪影和操纵中的检测功能。从基准多模式DeepFake数据集的视频上进行了广泛的实验，以评估ChatGPT的检测性能，并将其与最先进的多模式模型和人类的检测功能进行比较。实验结果证明了域知识和及时工程对使用LLM的视频检测任务的重要性。与基于端到端学习的方法不同，Chatgpt可以说明空间和时空伪像以及可能存在或跨跨模态内的不一致之处。此外，我们讨论了CHATGPT对多媒体法医任务的局限性。

### Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey 
[[arxiv](https://arxiv.org/abs/2411.09259)] [[cool](https://papers.cool/arxiv/2411.09259)] [[pdf](https://arxiv.org/pdf/2411.09259)]
> **Authors**: Xuannan Liu,Xing Cui,Peipei Li,Zekun Li,Huaibo Huang,Shuhan Xia,Miaoxuan Zhang,Yueying Zou,Ran He
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: ongoing work
- **标题**: 越狱攻击和针对多模式生成模型的防御：一项调查
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式基础模型的快速发展导致了跨模式理解和产生的显着进步，包括文本，图像，音频和视频。但是，这些模型仍然容易受到越狱攻击的影响，越狱攻击可以绕过内置的安全机制并引起潜在有害内容的生产。因此，了解越狱攻击和现有的防御机制的方法对于确保在现实世界中，尤其是在对安全敏感的应用程序中安全部署多模式生成模型至关重要。为了全面了解该主题，这项调查回顾了越狱和防御模型。首先，鉴于多模式越狱的广义生命周期，我们系统地探索了四个级别的攻击和相应的防御策略：输入，编码器，发电机和输出。基于此分析，我们提出了针对多模式生成模型的攻击方法，防御机制和评估框架的详细分类法。此外，我们涵盖了广泛的输入输出配置，包括任何对文本，任何对视频以及生成系统中的任何对任何对任何一无所有的模式。最后，我们重点介绍了当前的研究挑战，并提出了未来研究的潜在方向。可以在https://github.com/liuxuannan/awesome-multimodal-jailbreak上找到与此工作相对应的开源存储库。

### Towards Open-Vocabulary Audio-Visual Event Localization 
[[arxiv](https://arxiv.org/abs/2411.11278)] [[cool](https://papers.cool/arxiv/2411.11278)] [[pdf](https://arxiv.org/pdf/2411.11278)]
> **Authors**: Jinxing Zhou,Dan Guo,Ruohao Guo,Yuxin Mao,Jingjing Hu,Yiran Zhong,Xiaojun Chang,Meng Wang
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-18
> **comment**: accepted by CVPR 2025; Project page: https://github.com/jasongief/OV-AVEL
- **标题**: 朝着开放式视听视听事件本地化
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 视听事件本地化（AVEL）任务旨在在时间上找到和分类既可以听到又可见的视频事件。该领域的大多数研究都采用了封闭设置，这限制了这些模型在培训过程中处理不存在事件类别的测试数据的能力。最近，一些研究探索了开放式设置的AVEL，从而使人们能够识别未见事件为``未知''，但没有提供特定类别的语义。在本文中，我们通过引入开放式视听事件本地化（OV-avel）问题来推进该领域，该问题需要定位视听事件并预测推理时可见数据和看不见的数据的明确类别。为了解决这一新任务，我们提出了OV-avebench数据集，其中包括67个现实生活中的视听场景中的24,800个视频（可见：Unseen = 46:21），每个视频都带有手动片段级别的注释。我们还为此任务建立了三个评估指标。此外，我们研究了两种基线方法，一种无训练，一种使用进一步的微调范式。具体而言，我们利用统一的多模式从验证的映像模型来提取音频，视觉和文本（事件类）功能。然后，通过比较音频文本和视觉文本的一致性，无训练的基线来确定预测。微调基线结合了轻巧的时间层，以使用OV-avebench训练数据进行模型进行微调来编码音频和视觉方式内的时间关系。我们在拟议的OV-avebench数据集上评估了这些基准，并讨论了在这个新领域的未来工作的潜在方向。

### Efficient Transfer Learning for Video-language Foundation Models 
[[arxiv](https://arxiv.org/abs/2411.11223)] [[cool](https://papers.cool/arxiv/2411.11223)] [[pdf](https://arxiv.org/pdf/2411.11223)]
> **Authors**: Haoxing Chen,Zizheng Huang,Yan Hong,Yanshuo Wang,Zhongcai Lyu,Zhuoer Xu,Jun Lan,Zhangxuan Gu
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 视频基础模型的有效转移学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 预训练的视觉语言模型为跨各种下游任务的有效转移学习提供了强大的基础。在视频动作识别领域，主流方法通常会引入其他参数模块以捕获时间信息。尽管这些附加参数带来的模型能力增加有助于更好地适应视频特定的电感偏见，但现有方法需要学习大量参数，并且容易忘记原始的可推广知识。在本文中，我们提出了一个简单而有效的多模式时空适配器（MSTA），以改善文本和视觉分支中的表示之间的一致性，从而在一般知识和特定于任务的知识之间达到平衡。此外，为了减轻过度拟合并增强普遍性，我们引入了时空描述引导的一致性约束。该约束涉及喂食模板输入（即````$ \ {\ textbf {cls} \} $''''的视频到可训练的语言分支中，而llm生成的时空描述是将两个分支之间的预期分支一致性输入到预先训练的语言分支中。这种机制防止了过度适合下游任务，并提高了时空语义空间内可训练的分支的区分性。我们在四个任务中评估了方法的有效性：零射击转移，很少的学习，基础对概括和完全监督的学习。与许多最先进的方法相比，我们的MSTA在所有评估中都取得了出色的性能，而原始模型中仅使用2-7％的可训练参数。代码将在https://github.com/chenhaoxing/etl4video上可用。

### TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.11066)] [[cool](https://papers.cool/arxiv/2411.11066)] [[pdf](https://arxiv.org/pdf/2411.11066)]
> **Authors**: Tingyu Qu,Mingxiao Li,Tinne Tuytelaars,Marie-Francine Moens
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-18
> **comment**: work in progress
- **标题**: TS-LALAVA：通过缩略图和抽样构建视觉令牌，以训练免费视频大语模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（LLM）的最新进展在理解多模式内容方面取得了巨大的成功。为了了解视频理解任务，由于缺乏高质量，精选的视频文本配对数据，因此很难构建基于培训的视频LLM。相反，配对的图像文本数据更容易获得，并且图像和视频之间存在实质性的相似性。因此，扩展图像LLM进行视频理解任务提供了一种吸引人的选择。制定有效的策略来从多个帧中压缩视觉令牌是利用强大的预训练图像LLM的一种有希望的方法。在这项工作中，我们探讨了构建无培训视频LLM的现有压缩策略的局限性。这些发现导致了我们的方法TS-LALAVA，该方法通过缩略图和抽样策略构建了视觉令牌。给定视频，我们从所有输入帧中选择了几个等距帧来构造缩略图图像作为详细的视觉提示，并以所有输入帧的采样视觉令牌进行补充。我们的方法在各种基准测试中建立了新的无培训视频LLM的最新性能。值得注意的是，我们的34B型号在MVBench基准上优于GPT-4V，并且在具有挑战性的MLVU基准上实现了与基于72B培训的视频LLM相当的性能。代码可在https://github.com/tingyu215/ts-llava上找到。

### Framework for developing and evaluating ethical collaboration between expert and machine 
[[arxiv](https://arxiv.org/abs/2411.10983)] [[cool](https://papers.cool/arxiv/2411.10983)] [[pdf](https://arxiv.org/pdf/2411.10983)]
> **Authors**: Ayan Banerjee,Payal Kamboj,Sandeep Gupta
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-18
> **comment**: Accepted in ECAI Workshop AIEB
- **标题**: 用于开发和评估专家与机器之间道德合作的框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 精密医学是一种有前途的疾病诊断和个性化干预计划的有前途的方法，例如冠状动脉疾病（CAD），耐药性癫痫（DRE）和1型型糖尿病（T1D）等慢性疾病。通过利用人工智能（AI），精密医学通过对病理生理学的明确建模差异来定制单个患者的诊断和治疗解决方案。但是，在医疗应用中采用AI面临着重大挑战，包括各个中心，人口统计学和合并症的可推广性差，临床上有限的解释性以及对道德决策的缺乏信任。本文提出了一个框架，以开发和道德评估专家指导的多模式AI，以解决精密医学中的AI整合中的这些挑战。我们通过有关T1D胰岛素管理的案例研究来说明此框架。为了确保道德考虑和临床医生的参与，我们采用了一种共同设计的方法，在该方法中，AI担任辅助作用，最终诊断或治疗计划来自临床医生与AI之间的协作。

### VidComposition: Can MLLMs Analyze Compositions in Compiled Videos? 
[[arxiv](https://arxiv.org/abs/2411.10979)] [[cool](https://papers.cool/arxiv/2411.10979)] [[pdf](https://arxiv.org/pdf/2411.10979)]
> **Authors**: Yunlong Tang,Junjia Guo,Hang Hua,Susan Liang,Mingqian Feng,Xinyang Li,Rui Mao,Chao Huang,Jing Bi,Zeliang Zhang,Pooyan Fazli,Chenliang Xu
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: vidComposition：MLLM可以在编译视频中分析组成吗？
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式大语言模型（MLLM）的进步已在多模式理解方面取得了重大进展，从而扩大了他们分析视频内容的能力。但是，MLLM的现有评估基准主要集中在抽象的视频理解上，缺乏对其理解视频构图能力的详细评估，对视觉元素如何在高度编译的视频环境中结合和互动的细微解释。我们介绍了VidComposition，这是一种新的基准测试，专门旨在使用精心策划的编译视频和电影级注释来评估MLLM的视频组成能力。 VidComposition包括982个视频，其中包含1706个多项选择性问题，涵盖了各种组成方面，例如摄像机运动，角度，射击大小，叙事结构，性格动作和情感等。我们对33个开源和专有MLLM的全面评估揭示了人类和模型能力之间的显着性能差距。这重点介绍了当前MLLM在理解复杂，编译的视频作品中的局限性，并提供了有关进一步改进领域的见解。排行榜和评估代码可在https://yunlong10.github.io/vidcomposition/上获得。

### V2X-Radar: A Multi-modal Dataset with 4D Radar for Cooperative Perception 
[[arxiv](https://arxiv.org/abs/2411.10962)] [[cool](https://papers.cool/arxiv/2411.10962)] [[pdf](https://arxiv.org/pdf/2411.10962)]
> **Authors**: Lei Yang,Xinyu Zhang,Jun Li,Chen Wang,Zhiying Song,Tong Zhao,Ziying Song,Li Wang,Mo Zhou,Yang Shen,Kai Wu,Chen Lv
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: 11 pages, 5 figures
- **标题**: V2X-radar：具有4D雷达的多模式数据集用于合作感知
- **领域**: 计算机视觉和模式识别
- **摘要**: 现代的自动驾驶汽车感知系统通常会在阻塞和有限的感知范围内挣扎。先前的研究表明，合作感的有效性在扩展感知范围和克服阻塞中，从而提高了自主驾驶的安全性。近年来，出现了一系列合作感知数据集。但是，这些数据集仅专注于相机和激光雷达，俯瞰4D雷达，这是一种在不利天气条件下用于自动驾驶的传感器，用于自动驾驶。在本文中，为了弥合缺少4D雷达数据集的差距，我们提出了V2X-Radar，这是第一个具有4D雷达的大型真实世界多模式数据集。我们的V2X-radar数据集使用连接的车辆平台和配备4D雷达，激光镜头和多视图摄像头的智能路边单元收集。收集的数据包括晴天和多雨的天气状况，跨白天，黄昏和夜间以及典型的挑战性场景。该数据集包含20K激光镜框架，40k摄像头图像和20K 4D雷达数据，其中五个类别的注释边界框具有350k注释的边界框。为了促进各种研究领域，我们建立了用于合作感知的V2X-Radar-C，V2X-Radar-I用于路边感知，而V2X-Radar-V用于单车感知。我们进一步提供了上述三个子数据集的近期感知算法的全面基准。数据集和基准代码库将通过\ url {http://openmpd.com/column/v2x-radar}提供。

### Memory-Augmented Multimodal LLMs for Surgical VQA via Self-Contained Inquiry 
[[arxiv](https://arxiv.org/abs/2411.10937)] [[cool](https://papers.cool/arxiv/2411.10937)] [[pdf](https://arxiv.org/pdf/2411.10937)]
> **Authors**: Wenjun Hou,Yi Cheng,Kaishuai Xu,Yan Hu,Wenjie Li,Jiang Liu
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 通过独立查询的手术VQA的记忆启动的多模式LLM
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 全面理解手术视觉问题回答（手术VQA）中的手术场景需要对多个对象进行推理。先前的方法使用跨模式融合策略来解决此任务，以增强推理能力。但是，这些方法通常在有限的场景理解和质疑理解力方面遇到困难，有些方法依靠外部资源（例如，提取的对象特征），这可以引入错误并在各种外科手术环境中概括不良。为了应对这些挑战，我们提出了扫描，这是一个简单而有效的记忆启动框架，利用多模式LLMS通过自我访问的查询来改善手术环境理解。扫描可自主运行，生成两种类型的内存以进行上下文增强：直接内存（DM），它为最终答案提供了多个候选（或提示），并间接内存（IM），该记忆（IM）由独立的问题先知对组成，以捕获更广泛的场景上下文。 DM直接有助于回答这个问题，同时IM增强了对直接查询以外的手术场景的理解。对这些对象感知的记忆进行推理，使模型能够准确解释图像并回答问题。对三个可公开的外科VQA数据集进行了广泛的实验表明，扫描可实现最先进的性能，在各种手术方案中提供了提高的准确性和鲁棒性。

### Deep BI-RADS Network for Improved Cancer Detection from Mammograms 
[[arxiv](https://arxiv.org/abs/2411.10894)] [[cool](https://papers.cool/arxiv/2411.10894)] [[pdf](https://arxiv.org/pdf/2411.10894)]
> **Authors**: Gil Ben-Artzi,Feras Daragma,Shahar Mahpod
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 深度Bi-Rads网络，可改善乳房X线照片的癌症检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管用于乳腺癌检测的最新模型利用了增强诊断准确性的多视图乳房X线照片，但它们通常仅专注于视觉乳房X线摄影数据。但是，放射科医生记录了有价值的病变描述，其中包含可以增强基于乳房X线摄影的乳腺癌筛查的其他信息。一个关键的问题是，深度学习模型是否可以从这些专家衍生的功能中受益。为了解决这个问题，我们介绍了一种新型的多模式方法，该方法将文本Bi-Rads病变描述符与视觉乳房X线照片含量相结合。我们的方法采用迭代的注意层有效地融合了这些不同的方式，从而显着改善了仅图像模型的分类性能。 CBIS-DDSM数据集的实验显示了所有指标的实质改进，证明了手工制作的功能对端到端的贡献。

### ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2411.10867)] [[cool](https://papers.cool/arxiv/2411.10867)] [[pdf](https://arxiv.org/pdf/2411.10867)]
> **Authors**: Vipula Rawte,Sarthak Jain,Aarush Sinha,Garv Kaushik,Aman Bansal,Prathiksha Rumale Vishwanath,Samyak Rajesh Jain,Aishwarya Naresh Reganti,Vinija Jain,Aman Chadha,Amit P. Sheth,Amitava Das
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: Vibe：用于评估大型多模型幻觉的文本至评估基准
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型多模型模型（LMM）的最新发展扩大了它们的能力，以包括视频理解。具体而言，文本对视频（T2V）模型在质量，理解和持续时间方面取得了重大进展，在从简单的文本提示中创建视频方面表现出色。然而，他们仍然经常产生幻觉的内容，这些内容明显地表明了该视频是AI生成的。我们介绍了Vibe：来自T2V型号的幻觉视频的大规模文本与视频基准。我们确定幻觉的五种主要类型：消失的主题，数字变异性，颞肌畸形，遗漏误差和身体上的不一致。使用10种开源T2V型号，我们开发了第一个大型幻觉视频数据集，其中包括3,782个由人类注释的视频，分为这五个类别。 Vibe提供了一种独特的资源来评估T2V模型的可靠性，并为改善视频生成的幻觉检测和缓解措施提供了基础。我们将分类作为基线建立，并呈现各种集合分类器配置，而TimeFormer + CNN组合可产生最佳性能，达到0.345的精度和0.342 F1得分。该基准旨在推动强大的T2V模型的开发，该模型可以更准确地与输入提示保持一致。

### Automatic Discovery and Assessment of Interpretable Systematic Errors in Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2411.10845)] [[cool](https://papers.cool/arxiv/2411.10845)] [[pdf](https://arxiv.org/pdf/2411.10845)]
> **Authors**: Jaisidh Singh,Sonam Singh,Amit Arvind Kale,Harsh K Gandhi
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: 7 pages main paper (without references), total 13 pages & 9 figures
- **标题**: 自动发现和评估语义细分中可解释的系统错误
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文提出了一种在分割模型中发现系统错误的新方法。例如，分割模型中的系统错误可能是该模型的大量错误分类，作为目标类行人的停车收费表。随着这些模型在自主驾驶等关键应用中的快速部署，检测和解释这些系统错误至关重要。但是，关键的挑战是自动在未标记的数据上发现此类故障，并形成可解释的语义子组进行干预。为此，我们利用多模式的基础模型来检索错误并使用概念上的联系以及错误的性质来研究这些错误的系统性。我们证明，在SOTA分割模型（Upernet Convnext和Upernet Swin）中，在伯克利深度驱动器上进行了训练，并在定性和定量上对方法进行了基准，从而通过发现这些模型的连贯的系统错误来显示其有效性。我们的工作为迄今为止在语义细分中尚未忽视的模型分析和干预途径开辟了途径。

### Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2411.10803)] [[cool](https://papers.cool/arxiv/2411.10803)] [[pdf](https://arxiv.org/pdf/2411.10803)]
> **Authors**: Ting Liu,Liangtao Shi,Richang Hong,Yue Hu,Quanjun Yin,Linfeng Zhang
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: 8 pages, 4figures
- **标题**: 多阶段视觉令牌下降：迈向有效的多模式大语模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型中的视觉令牌通常表现出明显的空间和时间冗余，并占用大多数输入令牌，这会损害其推理效率。为了解决这个问题，引入了一些最新的作品，以删除在推断期间不重要的令牌，在这种推论中，每个令牌的重要性仅由视觉编码阶段或预填充阶段的信息决定。在本文中，我们提出了多阶段令牌掉落（Mustdrop），以测量从整个生命周期中的每个令牌的重要性，包括视觉编码阶段，预填充阶段和解码阶段。具体而言，在视觉编码阶段，必须将必须在空间相邻的代币中合并，并建立一个关键令牌设置，以保留最视力临界令牌，以防止它们在后期阶段被丢弃。在预填充阶段，必须通过双重注意过滤策略，通过文本语义的指导进一步压缩视觉令牌。在解码阶段，提出了输出感知的缓存策略，以进一步降低KV缓存的大小。通过在多阶段过程中利用量身定制的策略，必须更精确地认识到重要和多余的代币，从而在性能和效率之间取得最佳平衡。例如，在LLAVA上，必须降低约88.5 \％的触发器，而压缩率为92.2 \％，同时保持了可比的精度。我们的代码可在\ url {https://github.com/liuting20/mustdrop}中找到。

### EVT: Efficient View Transformation for Multi-Modal 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2411.10715)] [[cool](https://papers.cool/arxiv/2411.10715)] [[pdf](https://arxiv.org/pdf/2411.10715)]
> **Authors**: Yongjin Lee,Hyeon-Mun Jeong,Yurim Jeon,Sanghyun Kim
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: EVT：多模式3D对象检测的有效视图转换
- **领域**: 计算机视觉和模式识别
- **摘要**: 鸟眼视图中的多模式传感器融合（BEV）表示已成为3D对象检测的主要方法。但是，现有方法通常依赖于深度估计器或变压器编码器来查看转换，从而产生了大量的计算开销。此外，2D和3D空间之间缺乏精确的几何对应关系会导致空间和射线方向错位，从而限制了BEV表示的有效性。为了应对这些挑战，我们通过有效的视图转换（EVT）提出了一种新颖的3D对象检测器，该检测器利用结构良好的BEV表示以提高准确性和效率。 EVT专注于两个主要领域。首先，它采用自适应采样和自适应投影（ASAP），使用LIDAR指南生成3D采样点和自适应核。然后使用生成的点和内核来促进图像特征转换为BEV空间并完善BEV特征。其次，EVT包括一个改进的基于变压器的检测框架，该框架包含一个小组查询初始化方法和增强的查询更新框架。它旨在有效利用变压器解码器中获得的多模式BEV功能。通过利用对象查询的几何特性，该框架显着提高了检测性能，尤其是在多层变压器解码器结构中。 EVT以实时推理速度的Nuscenes测试集实现了最先进的性能。

### SPDFusion: An Infrared and Visible Image Fusion Network Based on a Non-Euclidean Representation of Riemannian Manifolds 
[[arxiv](https://arxiv.org/abs/2411.10679)] [[cool](https://papers.cool/arxiv/2411.10679)] [[pdf](https://arxiv.org/pdf/2411.10679)]
> **Authors**: Huan Kang,Hui Li,Tianyang Xu,Rui Wang,Xiao-Jun Wu,Josef Kittler
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: 14 pages, 12 figures
- **标题**: SPDFusion：基于Riemannian歧管的非欧几里得表示的红外且可见的图像融合网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 欧几里得表示学习方法在图像融合任务中取得了值得称赞的结果，这可以归因于它们在线性空间处理方面的明确优势。但是，从现实场景中收集的数据通常具有非欧几里得结构，在该结构中，欧几里得公制在表示真实的数据关系时可能会受到限制，从而降低了融合性能。为了解决这个问题，为多模式图像融合提出了一种新颖的SPD（对称正定义）流形学习框架，该框架名为SPDFusion，该框架将图像融合方法从欧几里得空间扩展到SPD歧管。具体而言，我们根据Riemannian几何形状编码图像来利用其内在的统计相关性，从而与人类的视觉感知保持一致。实际上，SPD矩阵的基础是我们的网络学习，采用了用于利用特定于模式的依赖性和增强互补信息的跨模式融合策略。随后，注意模块设计用于处理学习的权重矩阵，从而通过SPD矩阵乘法促进了空间全局相关语义的加权。基于此，我们设计了一个基于跨模式流动学习的端到端融合网络。公共数据集上的广泛实验表明，与当前的最新方法相比，我们的框架表现出色。

### Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts 
[[arxiv](https://arxiv.org/abs/2411.10669)] [[cool](https://papers.cool/arxiv/2411.10669)] [[pdf](https://arxiv.org/pdf/2411.10669)]
> **Authors**: Jinqiang Long,Yanqi Dai,Guoxing Yang,Hongpeng Lin,Nanyi Fei,Yizhao Gao,Zhiwu Lu
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: AWAKER2.5-VL：稳定地缩放MLLM，具有专家的参数有效混合物
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着多模式大语言模型（MLLM）的研究变得流行，通常需要一个前进的MLLM模型来处理现实世界应用的各种文本和视觉任务（例如VQA，VQA，检测，OCR和ChartQA）。 However, due to the significant differences in representation and distribution among data from various tasks, simply mixing data of all tasks together leads to the well-known``multi-task conflict" issue, resulting in performance degradation across various tasks. To address this issue, we propose Awaker2.5-VL, a Mixture of Experts~(MoE) architecture suitable for MLLM, which acquires the multi-task capabilities through multiple sparsely activated experts. To speed在Awaker2.5-VL的培训和推理中，我们模型中的每个专家都被设计为低级适应（LORA）结构。

### BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices 
[[arxiv](https://arxiv.org/abs/2411.10640)] [[cool](https://papers.cool/arxiv/2411.10640)] [[pdf](https://arxiv.org/pdf/2411.10640)]
> **Authors**: Xudong Lu,Yinghao Chen,Cheng Chen,Hui Tan,Boheng Chen,Yina Xie,Rui Hu,Guanxin Tan,Renshou Wu,Yan Hu,Yi Zeng,Lei Wu,Liuyang Bian,Zhaoxiong Wang,Long Liu,Yanzhou Yang,Han Xiao,Aojun Zhou,Yafei Wen,Xiaoxin Chen,Shuai Ren,Hongsheng Li
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: 21 pages
- **标题**: Bluelm-V-3B：移动设备上多模式模型的算法和系统共同设计
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式大语言模型（MLLM）的出现和日益普及具有重要的潜力，可以增强日常生活的各个方面，从改善沟通到促进学习和解决问题。移动电话作为必不可少的每日伴侣，代表了MLLM的最有效，最容易访问的部署平台，从而使无缝集成到日常任务中。但是，由于内存大小和计算能力的限制，在手机上部署MLLM提出了挑战，因此很难在没有广泛优化的情况下实现平稳而实时的处理。在本文中，我们提出了Bluelm-V-3B，这是一种专门针对MLLM在移动平台上有效部署的算法和系统共同设计方法。具体来说，我们重新设计了主流MLLM采用的动态分辨率方案，并为硬件感知的部署实现系统优化，以优化手机上的模型推断。 Bluelm-V-3B拥有以下关键亮点：（1）小尺寸：Bluelm-V-3B具有具有2.7b参数的语言模型和具有400m参数的视觉编码器。 （2）快速速度：Bluelm-V-3B在带有4位LLM重量量化的Mediatek Dimente 9300处理器上达到24.4令牌/s的生成速度。 （3）出色的性能：Bluelm-V-3B在具有$ \ leq $ 4B参数的模型中，在OpenCompass基准上获得了最高的平均得分，并超过了一系列具有更大参数尺寸的模型（例如Minicpm-V-2.6，InternVL2-8B）。

### MTA: Multimodal Task Alignment for BEV Perception and Captioning 
[[arxiv](https://arxiv.org/abs/2411.10639)] [[cool](https://papers.cool/arxiv/2411.10639)] [[pdf](https://arxiv.org/pdf/2411.10639)]
> **Authors**: Yunsheng Ma,Burhaneddin Yaman,Xin Ye,Jingru Luo,Feng Tao,Abhirup Mallik,Ziran Wang,Liu Ren
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: 10 pages
- **标题**: MTA：BEV感知和字幕的多模式任务对齐
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 基于Bird的眼景（BEV）3D感知在自主驾驶应用中起着至关重要的作用。大型语言模型的兴起激发了人们对基于BEV的字幕的兴趣，以了解周围环境中的对象行为。但是，现有方法将感知和字幕视为单独的任务，重点是仅执行一项任务，并忽略了多模式对齐的潜在优势。为了弥合模式之间的差距，我们介绍了MTA，这是一种新型的多模式对齐框架，可以提高BEV感知和字幕。 MTA由两个关键组成部分组成：（1）BEV语言对齐（BLA），这是一种将BEV场景表示与基础真相表示形式保持一致的上下文学习机制，以及（2）检测接收对准（DCA），一种交叉模态提示机制，可使检测和图形成分输出保持一致。 MTA在培训过程中无缝集成到最先进的基线中，在运行时不增加额外的计算复杂性。在Nuscenes和Tod3CAP数据集上进行了广泛的实验表明，MTA在这两个任务中的表现都显着胜过最先进的基线，在具有挑战性的稀有感知情景方面提高了10.7％，并且在字幕上提高了9.2％。这些结果强调了统一对齐在核对基于BEV的感知和字幕方面的有效性。

### The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual Localisation, Reconstruction and Radiance Field Methods 
[[arxiv](https://arxiv.org/abs/2411.10546)] [[cool](https://papers.cool/arxiv/2411.10546)] [[pdf](https://arxiv.org/pdf/2411.10546)]
> **Authors**: Yifu Tao,Miguel Ángel Muñoz-Bañón,Lintong Zhang,Jiahao Wang,Lanke Frank Tarimo Fu,Maurice Fallon
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: Website: https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/
- **标题**: 牛津尖顶数据集：基准测试大规模激光雷达 - 视觉定位，重建和辐射现场方法
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 本文使用定制的多传感器感知单元以及陆地底激光扫描仪（TLS）介绍了在牛津大学及其周围捕获的大型多模式数据集。感知单元包括三个同步的全局快门颜色摄像机，一个汽车3D激光扫描仪和一个惯性传感器 - 均精确校准。我们还建立了涉及本地化，重建和新型视图合成的任务的基准，该任务可以评估同时本地化和映射（SLAM）方法，结构 - 触发器 - 胶合（SFM）（SFM）和多视图立体（MVS）方法（MVS）方法以及诸如Neural radiance cpprancian cpprance cpprance（ndd ga）。为了评估3D重建，TLS 3D模型用作地面真相。通过将移动激光射线扫描注册到TLS 3D模型中来计算本地化地面真相。不仅从输入轨迹取样的姿势来评估辐射场方法，还可以从与训练姿势相距遥远的轨迹的视点进行评估。我们的评估证明了最先进的辐射场方法的关键局限性：我们表明，它们倾向于过度适合训练姿势/图像，并且不能很好地推广到序列姿势。与使用相同的视觉输入的MVS系统相比，它们在3D重建中的表现也不佳。我们的数据集和基准旨在促进辐射现场方法和SLAM系统的更好整合。可以通过https://dynamic.robots.ox.ac.ac.uk/datasets/oxford-spires/访问原始数据和处理的数据以及用于解析和评估的软件。

### Any2Any: Incomplete Multimodal Retrieval with Conformal Prediction 
[[arxiv](https://arxiv.org/abs/2411.10513)] [[cool](https://papers.cool/arxiv/2411.10513)] [[pdf](https://arxiv.org/pdf/2411.10513)]
> **Authors**: Po-han Li,Yunhao Yang,Mohammad Omama,Sandeep Chinchali,Ufuk Topcu
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: Any2any：不完整的多模式检索与保形预测
- **领域**: 计算机视觉和模式识别,信息检索,多媒体
- **摘要**: 自主剂通过整合视觉，音频和激光镜等多模式输入来感知和解释其周围环境。这些感知方式支持检索任务，例如机器人技术中的位置识别。但是，当由于传感器故障或无法访问性而缺少数据时，当前的多模式检索系统会遇到困难，例如无RGB信息，例如无声视频或LIDAR扫描。我们提出任何2个新的检索框架，该框架解决了查询和参考实例都不完整的情况的情况。与以前的方法限于两种模式的插补不同，任何2个没有训练生成模型的方法都可以处理任何数量的模式。它计算出与跨模式编码器的成对相似性，并采用了具有共形预测的两阶段校准过程以对齐相似性。任何2y可以在多模式数据集（例如文本范围和文本时间系列）上进行有效检索。它在KITTI数据集中获得了35％的召回@5，该数据集与具有完整模式的基线模型相当。

### Everything is a Video: Unifying Modalities through Next-Frame Prediction 
[[arxiv](https://arxiv.org/abs/2411.10503)] [[cool](https://papers.cool/arxiv/2411.10503)] [[pdf](https://arxiv.org/pdf/2411.10503)]
> **Authors**: G. Thomas Hudson,Dean Slack,Thomas Winterbottom,Jamie Sterling,Chenghao Xiao,Junjie Shentu,Noura Al Moubayed
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: 10 pages, 10 figures
- **标题**: 一切都是视频：通过下一框架预测统一模式
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 多模式学习涉及整合来自文本，图像，音频和视频等各种模式的信息，对于许多复杂的任务，例如视觉质量回答，跨模式检索和字幕生成都是关键的。传统方法依赖于特定于模式的编码器和晚期融合技术，在适应新任务或方式时，这可能会阻碍可扩展性和灵活性。为了解决这些局限性，我们引入了一个新颖的框架，该框架将任务重新印度的概念扩展到了自然语言处理（NLP）范围为多模式学习。我们建议将各种多模式的任务重新将统一的下一框架预测问题重新调整，从而使单个模型可以处理不同的模式而无需模态特定组件。该方法将所有输入和输出视为视频中的顺序帧，从而使模态的无缝集成以及跨任务的有效知识转移。我们的方法对一系列任务进行了评估，包括文本到文本，图像到文本，视频对视频，视频对文本以及音频到文本，这表明了该模型可以跨越最小适应的模态跨模态的能力。我们表明，任务重新重新制定可以大大简化各种任务的多模式模型设计，从而为更广义的多模式基础模型奠定了基础。

### Hateful Meme Detection through Context-Sensitive Prompting and Fine-Grained Labeling 
[[arxiv](https://arxiv.org/abs/2411.10480)] [[cool](https://papers.cool/arxiv/2411.10480)] [[pdf](https://arxiv.org/pdf/2411.10480)]
> **Authors**: Rongxin Ouyang,Kokil Jaidka,Subhayan Mukerjee,Guangyu Cui
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-18
> **comment**: AAAI-25 Student Abstract, Oral Presentation
- **标题**: 可恶的模因通过上下文敏感的提示和细粒度的标签检测
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,多媒体
- **摘要**: 社交媒体上多模式内容的普遍性使自动审核策略复杂化。这需要增强多模式分类，并对图像和模因中低估的含义有更深入的了解。尽管以前的努力旨在通过微调改善模型性能，但很少有人探索了端到端优化管道，该管道说明了模式，提示，标签和微调。在这项研究中，我们为复杂任务中的模型优化提出了一个端到端的概念框架。实验支持了这个传统但新颖的框架的功效，从而达到了最高的准确性和AUROC。消融实验表明，孤立的优化本身并非无效。

### LLaVA-CoT: Let Vision Language Models Reason Step-by-Step 
[[arxiv](https://arxiv.org/abs/2411.10440)] [[cool](https://papers.cool/arxiv/2411.10440)] [[pdf](https://arxiv.org/pdf/2411.10440)]
> **Authors**: Guowei Xu,Peng Jin,Hao Li,Yibing Song,Lichao Sun,Li Yuan
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: llava-cot：让视觉语言模型逐步
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型语言模型已经证明了推理能力方面的显着进步，尤其是通过推理时间缩放，如Openai的O1之类的模型所示。但是，当前的视觉模型（VLM）通常很难执行系统的和结构化的推理，尤其是在处理复杂的视觉提问任务时。在这项工作中，我们介绍了Llava-Cot，这是一种旨在进行自主多阶段推理的新型VLM。与经过思考的提示不同，LLAVA-COT独立参与了摘要，视觉解释，逻辑推理和结论生成的顺序阶段。这种结构化的方法使Llava-Cot能够在推理密集型任务的精确度上取得明显的改进。为了实现这一目标，我们编译了LLAVA-COT-100K数据集，集成了来自各种视觉问题的样本，并提供结构化的推理注释。此外，我们提出了一种推理时间阶段级梁搜索方法，该方法可以有效地缩放。值得注意的是，只有100k培训样本和一种简单但有效的推理时间缩放方法，LLAVA-COT不仅在广泛的多模式推理基准上优于其基本模型7.4％，而且还超过了较大甚至封闭的模型的性能，例如Gemini-1.5-Pro，gemini-1.5-pro，gppt-pro，gpt-4o-mini-ins ins and llllama-ins and lllama-instruction and lllama-instruction。

### Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations 
[[arxiv](https://arxiv.org/abs/2411.10414)] [[cool](https://papers.cool/arxiv/2411.10414)] [[pdf](https://arxiv.org/pdf/2411.10414)]
> **Authors**: Jianfeng Chi,Ujjwal Karn,Hongyuan Zhan,Eric Smith,Javier Rando,Yiming Zhang,Kate Plawiak,Zacharie Delpierre Coudert,Kartikeya Upasani,Mahesh Pasupuleti
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: Llama Guard 3视觉：保护人类图像理解对话
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 我们介绍了Llama Guard 3 Vision，这是一种基于多模式LLM的人类对话的保障，涉及图像理解：它可用于保护多模式LLM输入（及时分类）和输出（响应分类）的内容。与以前仅使用文本的Llama Guard版本（Inan等，2023; Llama Team，2024b，a）不同，它是专门设计用于支持图像推理用例的专门设计的，并经过优化以检测有害的多模式（文本和图像）提示和对这些提示的文本响应。 Llama Guard 3对Llama 3.2视觉进行了微调，并使用MLCommons分类法在内部基准测试方面表现出强烈的表现。我们还测试了其针对对抗性攻击的鲁棒性。我们认为，Llama Guard 3 Vision是一个很好的起点，可以构建具有多模式功能的人类对话的更有能力，强大的内容审核工具。

### Modification Takes Courage: Seamless Image Stitching via Reference-Driven Inpainting 
[[arxiv](https://arxiv.org/abs/2411.10309)] [[cool](https://papers.cool/arxiv/2411.10309)] [[pdf](https://arxiv.org/pdf/2411.10309)]
> **Authors**: Ziqi Xie,Xiao Lai,Weidong Zhao,Siqi Jiang,Xianhui Liu,Wenlong Hou
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: 18 pages, 10 figures
- **标题**: 修改需要勇气：通过参考驱动的inpainting缝制无缝图像
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的图像缝合方法通常在具有挑战性的场景中产生明显的接缝，例如不均匀的色调和大型视差。为了解决这个问题，我们提出了由参考驱动的插入式缝合器（RDISTITCHER），该缝制器（RDISTITCHER）重新定义了图像融合和矩形为基于参考的镶嵌模型，并结合了比以前的方法更大的修饰融合区域和更强的修改强度。此外，我们引入了一种自我监督的模型训练方法，该方法可以通过微调文本对图像（T2I）扩散模型来实现Rdistitcher，而无需标记数据。认识到评估缝合图像质量的困难，我们提出了基于多模式的大语模型（MLLM）的指标，为评估缝合图像质量提供了新的视角。与最新方法（SOTA）方法相比，广泛的实验表明，我们的方法显着增强了缝合图像中的内容相干性和无缝过渡。尤其是在零拍实验中，我们的方法具有强大的概括能力。代码：https：//github.com/yayoyo66/rdistitcher

### Visual-Linguistic Agent: Towards Collaborative Contextual Object Reasoning 
[[arxiv](https://arxiv.org/abs/2411.10252)] [[cool](https://papers.cool/arxiv/2411.10252)] [[pdf](https://arxiv.org/pdf/2411.10252)]
> **Authors**: Jingru Yang,Huan Yu,Yang Jingxin,Chentianye Xu,Yin Biao,Yu Sun,Shengfeng He
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 视觉语言代理：迈向协作上下文对象推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在图像中的描述性任务上表现出色，但通常会在精确的对象定位中挣扎，这是可靠的视觉解释的关键元素。相比之下，传统的对象检测模型提供了较高的本地化精度，但由于对象间关系的建模有限，因此经常生成缺乏上下文连贯性的检测。为了解决这一基本限制，我们介绍了\ textbf {Visual-linguistic Agent（VLA），这是一个协作框架，将MLLM的关系推理优势与传统对象探测器的确切本地化功能相结合。在VLA范式中，MLLM充当中央语言代理，与专门的视觉代理合作进行对象检测和分类。语言代理通过对对象之间的空间和上下文关系进行推理来评估和完善检测，而分类视觉代理提供纠正反馈以提高分类精度。这种协作方法使VLA能够显着增强空间推理和对象定位，从而解决多模式理解中的关键挑战。对可可数据集的广泛评估表明，多个检测模型的性能改进，强调了VLA在准确且上下文相干对象检测中设定新基准的潜力。

### Multi-Task Adversarial Variational Autoencoder for Estimating Biological Brain Age with Multimodal Neuroimaging 
[[arxiv](https://arxiv.org/abs/2411.10100)] [[cool](https://papers.cool/arxiv/2411.10100)] [[pdf](https://arxiv.org/pdf/2411.10100)]
> **Authors**: Muhammad Usman,Azka Rehman,Abdullah Shahid,Abd Ur Rehman,Sung-Min Gho,Aleum Lee,Tariq M. Khan,Imran Razzak
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 多任务对抗变异自动编码器，用于估计具有多模式神经影像学的生物脑年龄
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 尽管从结构性MRI数据中估算大脑年龄的深度学习进展，但由于其复杂的结构和功能连通性测量的嘈杂性质，将功能性MRI数据纳入挑战。为了解决这个问题，我们介绍了多任务对抗变量自动编码器，这是一个自定义的深度学习框架，旨在通过多模式MRI数据集成来改善大脑年龄预测。该模型将潜在变量分离为通用和独特的代码，从而隔离了共享和模态特异性特征。通过将多任务学习与性别分类作为另一个任务，该模型捕获了性别特定的衰老模式。该模型在OpenBHB数据集中进行了评估，该数据集的平均绝对误差为2。77年，表现优于传统方法。这种成功将M-Avae定位为在大脑年龄估计中基于元偏的医疗保健应用的强大工具。

### Influence of Depth Camera Noise Models on Respiration Estimation 
[[arxiv](https://arxiv.org/abs/2411.10081)] [[cool](https://papers.cool/arxiv/2411.10081)] [[pdf](https://arxiv.org/pdf/2411.10081)]
> **Authors**: Maurice Rohr,Sebastian Dill
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: Poster Prague 2023 Conference, 4 pages
- **标题**: 深度相机噪声模型对呼吸估计的影响
- **领域**: 计算机视觉和模式识别
- **摘要**: 深度摄像机是捕获呼吸率等生命体征的有趣方式。有很多方法可以在受控设置中提取生命体征，但是为了更灵活地将其应用于多相机设置，需要一个模拟环境来生成足够的数据以培训和测试新算法。我们展示了3D渲染模拟管道的第一个结果，该管道的重点是不同的噪声模型，以便使用合成和实际呼吸信号作为基准生成逼真的，深度相机的呼吸信号。在这种情况下，大多数噪声可以准确地建模为高斯，但我们可以证明，一旦可用的图像分辨率太低，不同的噪声模型表面之间的差异。

### Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion 
[[arxiv](https://arxiv.org/abs/2411.10036)] [[cool](https://papers.cool/arxiv/2411.10036)] [[pdf](https://arxiv.org/pdf/2411.10036)]
> **Authors**: Dan He,Guofen Wang,Weisheng Li,Yucheng Shu,Wenbo Li,Lijian Yang,Yuping Huang,Feiyan Li
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 重新思考归一化策略和多模式图像融合的卷积内核
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式图像融合（MMIF）旨在集成来自不同模式的信息，以获得全面的图像，可帮助下游任务。但是，现有方法倾向于优先考虑自然图像融合，并专注于信息互补和网络培训策略。他们忽略了自然图像和医学图像融合与潜在组成部分的影响之间的基本区别。本文剖析了两个任务在融合目标，统计属性和数据分布方面的显着差异。基于这一点，我们重新考虑了临床策略和卷积内核对端到端MMIF的适用性。本文提出了实例归一化和组归一化的混合物，以保持样品独立性并增强内在特征相关性。该策略促进了丰富特征图的潜在特征图，从而提高了融合性能。为此，我们进一步引入了大型内核卷积，有效地扩大了接受场并增强了图像细节的保存。此外，提出的多径自适应融合模块可以通过各种尺度和接受场的特征重新校准解码器输入，从而确保了关键信息的传输。广泛的实验表明，我们的方法在多个融合任务中表现出最先进的性能，并显着改善了下游应用。该代码可在https://github.com/hedan-11/lkc-funet上找到。

### VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying Misinformation of Short Videos 
[[arxiv](https://arxiv.org/abs/2411.10032)] [[cool](https://papers.cool/arxiv/2411.10032)] [[pdf](https://arxiv.org/pdf/2411.10032)]
> **Authors**: Weihao Zhong,Yinhao Xiao,Minghui Xu,Xiuzhen Cheng
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: arXiv admin note: text overlap with arXiv:2211.10973 by other authors
- **标题**: VMID：用于检测和识别简短视频错误信息的多模式融合LLM框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 简短的视频平台已成为新闻传播的重要渠道，为用户提供了一种高度引人入胜且直接的方法，可以访问时事和共享信息。但是，这些平台也已成为迅速传播错误信息的重要管道，因为虚假新闻和谣言可以利用视觉吸引力和短视频的广泛范围，以在观众之间广泛流传。现有的伪造新闻检测方法主要依赖于单模式信息，例如文本或图像，或仅应用基本的融合技术，从而限制了它们处理复杂的，多层信息的能力。为了解决这些局限性，本文介绍了一种基于多模式信息的新颖的假新闻检测方法，旨在通过对视频内容的多层次分析来识别错误信息。这种方法有效地利用不同的模态表示产生统一的文本描述，然后将其馈入大型语言模型以进行全面评估。拟议的框架成功地将多模式功能集成到视频中，从而显着提高了假新闻检测的准确性和可靠性。实验结果表明，所提出的方法在准确性，鲁棒性和多模式信息的利用方面优于现有模型，达到90.93％的准确性，这显着高于最佳基线模型（SV-FEND），即81.05％。此外，案例研究还提供了该方法在准确区分假新闻，揭穿内容和实际事件的有效性的其他证据，从而强调了其在现实世界应用中的可靠性和鲁棒性。

### Explanation for Trajectory Planning using Multi-modal Large Language Model for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2411.09971)] [[cool](https://papers.cool/arxiv/2411.09971)] [[pdf](https://arxiv.org/pdf/2411.09971)]
> **Authors**: Shota Yamazaki,Chenyu Zhang,Takuya Nanri,Akio Shigekane,Siyuan Wang,Jo Nishiyama,Tao Chu,Kohei Yokosawa
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: Accepted and presented at ECCV 2024 2nd Workshop on Vision-Centric Autonomous Driving (VCAD) on September 30, 2024. 13 pages, 5 figures
- **标题**: 使用多模式大语言模型用于自动驾驶的轨迹计划的说明
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 端到端样式自动驾驶模型最近已经开发出来。这些模型缺乏从感知到控制自我车辆的决策过程的解释性，导致乘客的焦虑。为了减轻它，建立一个模型，该模型可以输出标题描述描述自我车辆的未来行为及其原因。但是，现有的方法会产生推理文本，这反映了自我车辆的未来计划，因为它们训练模型以使用瞬时控制信号作为输入来输出字幕。在这项研究中，我们提出了一个推理模型，该模型将EGO车辆的未来计划轨迹作为输入，以通过新收集的数据集解决此限制。

### Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs 
[[arxiv](https://arxiv.org/abs/2411.09968)] [[cool](https://papers.cool/arxiv/2411.09968)] [[pdf](https://arxiv.org/pdf/2411.09968)]
> **Authors**: Xiaofeng Zhang,Yihao Quan,Chaochen Gu,Chen Shen,Xiaosong Yuan,Shaotian Yan,Hao Cheng,Kaijie Wu,Jieping Ye
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: 第二层清楚地看到：增强注意力的头部以减轻LVLMS的幻觉
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式大语言模型（MLLM）中的幻觉问题仍然是一个普遍的问题。尽管图像令牌占MLLM的大部分输入序列，但仍有有限的研究来探索图像令牌与幻觉之间的关系。在本文中，我们分析了图像令牌的注意力评分分布在模型的每一层和头部，揭示了一种有趣且共同的现象：大多数幻觉与图像令牌的自我发场矩阵中的注意力下沉密切相关，在浅层层的自我发场矩阵中，浅层层表现出严重的注意下层和深层注意力的关注点显示出稀疏的注意力沉没。我们进一步分析了不同层的注意力头，发现图像中具有高密度注意力的头部在减轻幻觉中起着积极的作用。在本文中，我们提出了一种无训练的方法，名为\ textColor {red} {\ textbf {e}} nHancing \ textColor {red} {\ textbf {a}} tteention \ textcolor \ textcolor \ textcolor {red}沉入浅层。 EAH识别了显示浅层视觉下沉并提取其注意力矩阵的注意力头。然后，该注意力图会广播到图层中的其他头部，从而加强图层以更加关注图像本身。通过广泛的实验，EAH在不同的MLLM和指标上显示出明显的幻觉降低性能，证明其有效性和一般性。

### Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era 
[[arxiv](https://arxiv.org/abs/2411.09955)] [[cool](https://papers.cool/arxiv/2411.09955)] [[pdf](https://arxiv.org/pdf/2411.09955)]
> **Authors**: Thanh Tam Nguyen,Zhao Ren,Trinh Pham,Thanh Trung Huynh,Phi Le Nguyen,Hongzhi Yin,Quoc Viet Hung Nguyen
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: Fixed a serious error in author information
- **标题**: 指导指导的图像和多媒体的编辑控件：LLM时代的调查
- **领域**: 计算机视觉和模式识别,人工智能,人机交互,机器学习,多媒体
- **摘要**: 大型语言模型（LLM）和多模式学习的快速发展已改变了数字内容的创建和操纵。传统的视觉编辑工具需要大量的专业知识，从而限制可访问性。基于教学的编辑的最新进展已使自然语言作为用户意图和复杂编辑操作之间的桥梁实现了与视觉内容的直观互动。这项调查概述了这些技术，重点介绍了LLM和多模型模型如何使用户能够在没有深层技术知识的情况下实现精确的视觉修改。通过综合100多个出版物，我们探讨了从生成对抗网络到扩散模型的方法，检查了多模式集成以进行细颗粒内容控制。我们讨论了跨时尚，3D场景操纵和视频综合等领域的实际应用，突出了可访问性的增加并与人类直觉保持一致。我们的调查比较了现有文献，强调了LLM授权的编辑，并确定了刺激进一步研究的关键挑战。我们旨在使从娱乐到教育的各个行业之间的强大视觉编辑民主化。鼓励有兴趣的读者通过https://github.com/tamlhp/awsome-instruction-editing访问我们的存储库。

### The Sound of Water: Inferring Physical Properties from Pouring Liquids 
[[arxiv](https://arxiv.org/abs/2411.11222)] [[cool](https://papers.cool/arxiv/2411.11222)] [[pdf](https://arxiv.org/pdf/2411.11222)]
> **Authors**: Piyush Bagad,Makarand Tapaswi,Cees G. M. Snoek,Andrew Zisserman
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-18
> **comment**: Project page at https://bpiyush.github.io/pouring-water-website. Short version accepted to ICASSP 2025
- **标题**: 水的声音：从倒液体中推断出物理特性
- **领域**: 计算机视觉和模式识别,多媒体,声音,音频和语音处理
- **摘要**: 我们研究了视听观察与平凡但有趣的日常活动的潜在物理学之间的联系：倒入液体。鉴于液体的声音倒入容器中，我们的目标是自动推断物理特性，例如液位，容器的形状和大小，浇注速率和填充时间。为此，我们：（i）从理论上表明可以从基本频率（音调）确定这些属性； （ii）通过模拟数据和视觉数据的监督训练俯仰检测模型，并以物理启发的目标进行训练； （iii）引入了一个新的大型真实浇注视频数据集，以进行系统研究； （iv）表明，训练有素的模型确实可以推断出这些物理特性的真实数据；最后，（v）我们证明了对各种容器形状，其他数据集和野外YouTube视频的强烈概括。我们的工作对声学，物理学和学习的交集中对一个狭窄而丰富的问题​​有了敏锐的了解。它打开了应用程序，以增强机器人浇注中的多感觉感知。

### Rethinking Text-Promptable Surgical Instrument Segmentation with Robust Framework 
[[arxiv](https://arxiv.org/abs/2411.12199)] [[cool](https://papers.cool/arxiv/2411.12199)] [[pdf](https://arxiv.org/pdf/2411.12199)]
> **Authors**: Tae-Min Choi,Juyoun Park
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: 11 pages, 6 figures, 7 tables, submitted to IEEE Journal of Biomedical and Health Informatics
- **标题**: 重新思考具有强大框架的可启发文本的外科手术仪器分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 手术仪器分割（SIS）在计算机辅助手术中至关重要，深度学习方法提高了复杂环境的准确性。最近，已经引入了可提供文本的分割方法，从而基于文本描述生成面具。但是，他们假设存在文本描述的对象，并且即使缺乏对象，也始终生成关联的掩码。现有方法仅通过仅将提示用于现有现有的对象，该提示依赖于无法访问的信息。为了解决这个问题，我们重新考虑可启发文本的SIS，并在健壮的条件下重新定义它为可靠的文本支持SIS（R-SIS）。与以前的方法不同，R-SIS是一个过程，可以分析文本提示所有手术仪器类别的提示，而不依赖外部知识，可以识别现场中存在的仪器，并相应地片段。在此基础上，我们提出了强大的手术仪器分割（ROSIS），这是一个优化的框架，结合了视觉和语言特征，可在R-SIS设置中迅速分割。 ROSIS采用具有多模式融合块（MMFB）和选择性门块（SGB）的编码器架构来平衡视觉和语言特征的整合。此外，一种迭代的改进策略通过两步过程增强了细分面罩：带有基于名称的提示的初始通行证，然后进行改进，并带有位置提示。多个数据集和设置之间的实验表明，在健壮条件下，Rosis的表现优于现有的基于视觉的分割方法。通过重新思考文本支持SIS，我们的工作建立了一种公平有效的手术仪器分割方法。

### CCIS-Diff: A Generative Model with Stable Diffusion Prior for Controlled Colonoscopy Image Synthesis 
[[arxiv](https://arxiv.org/abs/2411.12198)] [[cool](https://papers.cool/arxiv/2411.12198)] [[pdf](https://arxiv.org/pdf/2411.12198)]
> **Authors**: Yifan Xie,Jingge Wang,Tao Feng,Fei Ma,Yang Li
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: 5 pages, 4 figures
- **标题**: CCIS-DIFF：一种具有稳定扩散的生成模型。
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 结肠镜检查对于鉴定腺瘤息肉和预防结直肠癌至关重要。但是，由于现有结肠镜检查数据集的大小和可访问性有限，开发用于息肉检测的强大模型具有挑战性。尽管以前的努力试图综合结肠镜检查图像，但当前的方法却遭受了不稳定性和数据多样性的不足。此外，这些方法缺乏对生成过程的精确控制，从而产生了无法符合临床质量标准的图像。为了应对这些挑战，我们提出了CCIS-DIFF，这是一种基于扩散体系结构的高质量结肠镜检查图像合成的受控生成模型。我们的方法可以精确控制与临床描述一致的息肉的空间属性（息肉位置和形状）和临床特征。具体而言，我们引入了一种模糊的面膜加权策略，以无缝将合成的息肉与结肠粘膜无缝融合，以及一种引导产生的图像以反映临床特征的文本感知注意机制。值得注意的是，为了实现这一目标，我们构建了一个新的多模式结肠镜检查数据集，该数据集集成了图像，掩盖注释和相应的临床文本描述。实验结果表明，我们的方法产生了高质量的，多样化的结肠镜检查图像，并对空间约束和临床一致性进行了很好的控制，为下游分割和诊断任务提供了宝贵的支持。

### A Survey of Medical Vision-and-Language Applications and Their Techniques 
[[arxiv](https://arxiv.org/abs/2411.12195)] [[cool](https://papers.cool/arxiv/2411.12195)] [[pdf](https://arxiv.org/pdf/2411.12195)]
> **Authors**: Qi Chen,Ruoshan Zhao,Sinuo Wang,Vu Minh Hieu Phan,Anton van den Hengel,Johan Verjans,Zhibin Liao,Minh-Son To,Yong Xia,Jian Chen,Yutong Xie,Qi Wu
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: 对医学视觉和语言应用的调查及其技术
- **领域**: 计算机视觉和模式识别
- **摘要**: 医疗视觉和语言模型（MVLM）由于能力提供了自然语言界面来解释复杂的医学数据，因此引起了重大兴趣。它们的应用程序用途广泛，有可能提高个别患者的诊断准确性和决策，同时还可以通过对大型数据集进行更有效的分析来增强公共卫生监测，疾病监测和决策。 MVLM将自然语言处理与医学图像相结合，以使对医学图像的更全面和上下文的理解以及其相应的文本信息。与经过多种非专业数据集培训的一般视觉和语言模型不同，MVLM是针对医疗领域的专门构建的，可以自动从医学图像和文本报告中提取和解释关键信息，以支持临床决策。 MVLM的流行临床应用包括自动化医学报告生成，医学视觉问题答案，医学多模式分割，诊断和预后以及医学图像文本检索。在这里，我们提供了MVLMS及其应用的各种医疗任务的全面概述。我们对各种视觉和语言模型体系结构进行了详细的分析，重点介绍了他们独特的跨模式集成/剥削医学视觉和文本特征的策略。我们还检查了用于这些任务的数据集，并根据标准化评估指标比较了不同模型的性能。此外，我们强调了潜在的挑战，并总结了未来的研究趋势和方向。论文和代码的完整集合可在以下网址提供：https：//github.com/ytongxie/medical-vision-vision-and-language-tasks-and-methodologies-a-survey。

### AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2411.11930)] [[cool](https://papers.cool/arxiv/2411.11930)] [[pdf](https://arxiv.org/pdf/2411.11930)]
> **Authors**: Kun Xiang,Zhili Liu,Zihao Jiang,Yunshuang Nie,Runhui Huang,Haoxiang Fan,Hanhui Li,Weiran Huang,Yihan Zeng,Jianhua Han,Lanqing Hong,Hang Xu,Xiaodan Liang
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: AtomThink：多模式数学推理的缓慢思考框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在本文中，我们通过将“缓慢思考”的能力纳入多模式的大语言模型（MLLMS）来解决多模式数学推理的具有挑战性的任务。与依赖直接或快速思考的现有方法相反，我们的关键思想是构建长长的思想链（COT）（COT），该链条（COT）在Quonge notword中构建了一个新型的指导性ME，以使其在逐步进行中，以供您构成复杂的启动，以便在逐步进行启动，以便努力地启动型号。由三个关键模块组成：（i）自动产生高质量的COT注释，以解决高质量的视觉数学数据； Atommath，一个大尺度的多模式数据集的长COTS，以及用于数学任务的原子能评估度量。广泛的实验结果表明，所提出的原子思维可显着提高基线MLLM的性能，从而在MathVista上获得约50 \％的相对准确性，而Mathverse的相对准确度获得了120 \％。为了支持多模式慢思维模型的进步，我们将在https://github.com/quinn7777/atomthink上公开提供代码和数据集。

### VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation 
[[arxiv](https://arxiv.org/abs/2411.11919)] [[cool](https://papers.cool/arxiv/2411.11919)] [[pdf](https://arxiv.org/pdf/2411.11919)]
> **Authors**: Ruiyang Zhang,Hu Zhang,Zhedong Zheng
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: VL  - 不确定性：通过不确定性估计检测大视觉模型中的幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 鉴于与单模式LLM相比，大型视力语言模型（LVLM）处理的更高信息负载，检测LVLM幻觉需要更多的人力和时间支出，因此引起了更广泛的安全问题。在本文中，我们介绍了VL-Un-un-un-unclinaty，这是第一个基于不确定性的框架，用于检测LVLMS中的幻觉。与大多数需要地面或伪注释的现有方法不同，VL不确定性将不确定性用作固有度量。我们通过分析跨语义上等效但受扰动的提示（包括视觉和文本数据）的预测差异来衡量不确定性。当LVLM高度自信时，它们会对语义上等效的查询提供一致的响应。但是，如果不确定，目标LVLM的响应会变得更加随机。考虑使用不同措辞的语义上相似的答案，我们根据其语义含量群集lvlm响应，然后计算群集分布熵作为检测幻觉的不确定性度量。我们对涵盖自由形式和多选择任务的10个基准的10个LVLM进行了广泛的实验，表明VL不确定度在幻觉检测中明显优于强大的基线方法。

### F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics 
[[arxiv](https://arxiv.org/abs/2411.11912)] [[cool](https://papers.cool/arxiv/2411.11912)] [[pdf](https://arxiv.org/pdf/2411.11912)]
> **Authors**: Pramit Saha,Felix Wagner,Divyanshu Mishra,Can Peng,Anshul Thakur,David Clifton,Konstantinos Kamnitsas,J. Alison Noble
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: f $^3 $ ocus-通过多目标meta-heuristics使用最佳客户层更新策略的视觉基础模型的联合填充
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 有效地培训大型视力模型（VLM）对联合学习（FL）中资源受限的客户设备的有效培训需要使用参数有效的微调（PEFT）策略。为此，我们演示了两个因素\ textit {viz。}的影响，特定于客户端的层重要性评分，它选择了最重要的VLM层来进行微调和偏之间的层间多样性评分，从而鼓励客户选择各个客户层以用于最佳VLM层选择的客户选择。我们首先在理论上激励并利用层神经切线内核的主要特征值大小，并显示出其有效性作为客户特定层的重要性评分。接下来，我们提出了一个新颖的层更新策略，称为f $^3 $ ocus，该策略通过在服务器上采用无数据的，多目标，元元素优化来共同优化层的重要性和多样性因素。我们探索了5种不同的元元素算法，并比较了它们选择模型层和适配器层向PEFT-FL的有效性。此外，我们发布了一个新的MEDVQA-FL数据集，涉及总体707,962个VQA三胞胎和9个特定于模式的客户端，并利用它来训练和评估我们的方法。总体而言，我们在6个视觉语言FL任务设置上进行了10,000多个客户级实验，涉及58个医疗图像数据集和4种不同尺寸的VLM架构，以证明该方法的有效性。

### SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2411.11909)] [[cool](https://papers.cool/arxiv/2411.11909)] [[pdf](https://arxiv.org/pdf/2411.11909)]
> **Authors**: Hongrui Jia,Chaoya Jiang,Haiyang Xu,Wei Ye,Mengfan Dong,Ming Yan,Ji Zhang,Fei Huang,Shikun Zhang
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: SYMDPO：通过符号演示直接偏好优化的大型多模型模型的促进内在学习的内在学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着语言模型继续扩展，大型语言模型（LLMS）在文化学习（ICL）中表现出了新兴的功能，从而使它们能够通过将一些文字示范（ICDS）作为上下文来解决语言任务。受这些进步的启发，研究人员扩展了这些技术，以开发具有ICL功能的大型多模型（LMM）。但是，现有的LMM面临着一个关键问题：它们通常无法有效利用多模式演示中的视觉上下文，而只是遵循文本模式。这表明LMM在多模式演示和模型输出之间无法实现有效的对齐。为了解决这个问题，我们提出了符号演示直接优先优化（SYMDPO）。具体而言，Symdpo旨在通过使用随机符号在实例中替换文本答案来打破构建多模式演示的传统范式。这迫使模型仔细理解演示图像，并在图像与符号之间建立关系，以正确回答问题。我们验证了该方法对多个基准测试的有效性，表明使用Symdpo，LMM可以更有效地了解示例中的多模式上下文，并利用此知识更好地回答问题。代码可在https://github.com/apiaog/symdpo上找到。

### The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning 
[[arxiv](https://arxiv.org/abs/2411.11758)] [[cool](https://papers.cool/arxiv/2411.11758)] [[pdf](https://arxiv.org/pdf/2411.11758)]
> **Authors**: Longju Bai,Angana Borah,Oana Ignat,Rada Mihalcea
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: 许多人的力量：文化图像字幕的多代理多模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 大型多模型（LMM）在各种多模式任务中表现出令人印象深刻的性能。但是，由于大多数数据和模型的主要以西方为中心的性质，它们在跨文化环境中的有效性仍然受到限制。相反，多代理模型在解决复杂任务方面表现出了重要的能力。我们的研究评估了LMM在多代理相互作用环境中的集体绩效，用于文化图像字幕的新任务。我们的贡献如下：（1）我们介绍了Mosaic，这是一个多代理框架，可增强使用具有不同文化角色的LMMS来增强跨文化图像字幕； （2）我们在三个数据集中提供了来自中国，印度和罗马尼亚的图像的文化丰富图像标题的数据集：Geode，GD-VCR，CVQA； （3）我们提出了一个培养适应的度量标准，用于评估图像标题中的文化信息； （4）我们表明，多代理的交互作用的表现优于不同指标的单一代理模型，并为未来的研究提供了宝贵的见解。可以通过https://github.com/michigannlp/mosaic访问我们的数据集和模型。

### The ADUULM-360 Dataset -- A Multi-Modal Dataset for Depth Estimation in Adverse Weather 
[[arxiv](https://arxiv.org/abs/2411.11455)] [[cool](https://papers.cool/arxiv/2411.11455)] [[pdf](https://arxiv.org/pdf/2411.11455)]
> **Authors**: Markus Schön,Jona Ruof,Thomas Wodtko,Michael Buchholz,Klaus Dietmayer
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: 2024 IEEE International Conference on Intelligent Transportation Systems (ITSC)
- **标题**: Aduulm-360数据集 - 多模式数据集，用于不利天气中的深度估算
- **领域**: 计算机视觉和模式识别
- **摘要**: 深度估计是全面理解的必不可少的任务，因为它允许照相机捕获的丰富语义信息投影到3D空间中。尽管该领域最近引起了很多关注，但深度估计的数据集缺乏场景多样性或传感器方式。这项工作介绍了Aduulm-360数据集，这是一种新型的多模式数据集，用于深度估计。 Aduulm-360数据集涵盖了所有已建立的自主驾驶传感器模式，相机，激光雷达和雷达。它涵盖了面向正面的立体声设置，六个环绕摄像头覆盖了整个360度，两个高分辨率的长距离激光传感器和五个远程雷达传感器。它也是第一个深度估计数据集，其中包含在良好和不利的天气条件下的各种场景。我们在不同的培训任务下使用最先进的自我监督深度估计方法进行了广泛的实验，例如单眼培训，立体声培训和全面的环绕培训。在讨论这些结果时，我们证明了最新方法的共同局限性，尤其是在不利天气条件下，希望这会激发该领域的未来研究。我们的数据集，开发套件和受过训练的基线可在https://github.com/uulm-mrm/aduulm_360_dataset上找到。

### Relevance-guided Audio Visual Fusion for Video Saliency Prediction 
[[arxiv](https://arxiv.org/abs/2411.11454)] [[cool](https://papers.cool/arxiv/2411.11454)] [[pdf](https://arxiv.org/pdf/2411.11454)]
> **Authors**: Li Yu,Xuanzhe Sun,Pan Gao,Moncef Gabbouj
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: 相关性指导的视觉融合用于视频显着性预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 音频数据通常与视频帧同步，在指导观众的视觉关注方面起着至关重要的作用。将音频信息纳入视频显着性预测任务可以增强人类视觉行为的预测。但是，现有的视听显着性预测方法通常直接融合音频和视觉特征，这些方法忽略了两种模式之间不一致的可能性，例如当音频充当背景音乐时。为了解决这个问题，我们提出了一个新颖的相关性引导的视听显着性预测网络，称为AVRSP。具体而言，相关性引导的视听特征融合模块（RAVF）根据音频和视觉元素之间的语义相关性，动态调整音频特征的保留，从而与视觉特征完善了集成过程。此外，多尺度功能协同（MS）模块集成了来自不同编码阶段的视觉特征，从而增强了网络在各种尺度上表示对象的能力。多尺度调节器门（MRG）可以将关键的融合信息传递到视觉特征，从而优化多尺度视觉特征的利用。对六个视听眼动数据集进行的广泛实验表明，我们的AVRSP网络在视听显着性预测中实现了竞争性能。

### GLDesigner: Leveraging Multi-Modal LLMs as Designer for Enhanced Aesthetic Text Glyph Layouts 
[[arxiv](https://arxiv.org/abs/2411.11435)] [[cool](https://papers.cool/arxiv/2411.11435)] [[pdf](https://arxiv.org/pdf/2411.11435)]
> **Authors**: Junwen He,Yifan Wang,Lijun Wang,Huchuan Lu,Jun-Yan He,Chenyang Li,Hanyuan Chen,Jin-Peng Lan,Bin Luo,Yifeng Geng
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: GLDESIGNER：利用多模式LLM作为设计师，以增强美学文字的布局
- **领域**: 计算机视觉和模式识别
- **摘要**: 文本徽标设计在很大程度上依赖于专业设计师的创造力和专业知识，其中安排元素布局是最重要的过程之一。但是，很少有人注意这项特定任务，该任务需要考虑精确的纹理细节和用户约束，但仅在更广泛的任务上，例如文档/海报布局生成。在本文中，我们提出了一个基于VLM的框架，该框架通过将多模式输入与用户约束集成在一起，从而生成内容吸引的文本徽标布局，从而在现实世界应用程序中支持更灵活，更稳定的布局设计。我们介绍了两种模型技术，以减少同时处理多个字形图像的计算，而不会面临性能降解。为了支持OUT模型的指令调整，我们构建了两个广泛的文本徽标数据集，它们比现有的公共数据集大5倍。除了几何注释（例如文本掩码和角色识别）外，我们还以自然语言格式的全面布局描述，以便在处理复杂的布局和自定义用户约束时具有更有效的培训。实验研究证明了我们提出的模型和数据集的有效性，与各种基准中的先前方法进行比较以评估几何美学和人类偏好。代码和数据集将公开可用。

### TL-CLIP: A Power-specific Multimodal Pre-trained Visual Foundation Model for Transmission Line Defect Recognition 
[[arxiv](https://arxiv.org/abs/2411.11370)] [[cool](https://papers.cool/arxiv/2411.11370)] [[pdf](https://arxiv.org/pdf/2411.11370)]
> **Authors**: Ke Zhang,Zhaoye Zheng,Yurong Guo,Jiacun Wang,Jiyuan Yang,Yangjie Xiao
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: TL-CLIP：用于传输线缺陷识别的功率特异性多模式预训练的视觉基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 传统上，传输线缺陷识别模型传统上使用了一般的预训练权重作为其训练的初始基础。由于训练数据集缺乏领域知识，这些模型通常具有较弱的概括能力。为了解决这个问题，我们提出了一个两阶段传输线的对比语言图像预训练（TL-CLIP）框架，该框架为传输线缺陷识别奠定了更有效的基础。训练前过程采用了一种新颖的功率特异性多模式算法，该算法辅助了两项功率特定的预训练任务，以更好地对检查数据中包含的功率相关语义知识进行更好的建模。为了微调预训练的模型，我们制定了一种转移学习策略，即使用预训练目标（FTP）进行微调，以减轻由有限检查数据引起的过度拟合问题。实验结果表明，所提出的方法显着提高了分类和检测任务中传输线缺陷识别的性能，这表明在传输线检查现场中，与传统预训练模型相比明显优势。

### MAIRA-Seg: Enhancing Radiology Report Generation with Segmentation-Aware Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.11362)] [[cool](https://papers.cool/arxiv/2411.11362)] [[pdf](https://arxiv.org/pdf/2411.11362)]
> **Authors**: Harshita Sharma,Valentina Salvatelli,Shaury Srivastav,Kenza Bouzid,Shruthi Bannur,Daniel C. Castro,Maximilian Ilse,Sam Bond-Taylor,Mercy Prasanna Ranjit,Fabian Falck,Fernando Pérez-García,Anton Schwaighofer,Hannah Richardson,Maria Teodora Wetscherek,Stephanie L. Hyland,Javier Alvarez-Valle
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: Accepted as Proceedings Paper at ML4H 2024
- **标题**: maira-seg：通过分割感知的多模式模型增强放射学报告的生成
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 对将AI应用于放射学报告的生成越来越兴趣，特别是对于胸部X射线（CXR）。本文研究了通过分割面罩合并像素级信息是否可以改善用于放射学报告生成的多模式大语言模型（MLLM）的细粒图像解释。我们介绍了Maira-Seg，这是一种细分感知的MLLM框架，旨在利用语义分割掩码与CXR一起生成放射学报告。我们训练专家分割模型，以获取CXR中放射学特异性结构的面具假贴。随后，我们建立了Maira的架构，Maira是报告生成的CXR专用模型，我们集成了可训练的分割代币提取器，该提取器利用这些面具的假单位，并采用了面膜意识，以提示生成放射学草记学报告。我们在公开可用的模拟CXR数据集上的实验表明，Maira-Seg的表现优于非分段基准。我们还调查了与Maira促使的标记集合，发现Maira-Seg始终表现出可比或卓越的性能。结果证实，使用分割掩模会增强MLLM的细微推理，从而有助于更好的临床结果。

### CCExpert: Advancing MLLM Capability in Remote Sensing Change Captioning with Difference-Aware Integration and a Foundational Dataset 
[[arxiv](https://arxiv.org/abs/2411.11360)] [[cool](https://papers.cool/arxiv/2411.11360)] [[pdf](https://arxiv.org/pdf/2411.11360)]
> **Authors**: Zhiming Wang,Mingze Wang,Sheng Xu,Yanjing Li,Baochang Zhang
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: CCEXPERT：通过差异感知集成和基础数据集在遥感更改字幕中提高MLLM功能
- **领域**: 计算机视觉和模式识别
- **摘要**: 遥感图像更改字幕（RSICC）旨在生成自然语言描述，以详细介绍了多个暂时的遥感图像之间的表面变化，详细介绍了更改对象的类别，位置和动态（例如，添加或失踪）。许多当前的方法试图利用多模式大型语言模型（MLLM）的长期理解和推理能力来完成此任务。但是，如果没有全面的数据支持，这些方法通常会改变MLLM的基本特征传输途径，从而破坏模型中的内在知识并限制其在RSICC中的潜力。在本文中，我们根据新的高级多模式大型框架提出了一种新型模型CCEXPERT。首先，我们设计了一个差异感知的集成模块，以捕获双阶段图像之间的多尺度差异，并将它们整合到原始图像上下文中，从而增强了差异特征的信噪比。其次，我们构建了一个称为CC-Foundation的高质量，多样化的数据集，其中包含200,000张图像对和120万个字幕，以提供大量数据支持，以继续在该域中进行预处理。最后，我们采用了三阶段的渐进式训练过程，以确保差异感知的集成模块与预验证的MLLM的深度整合。 CCEXPERT在Levir-CC基准上实现了$ S^*_ M = 81.80 $的显着性能，大大超过了先前的最新方法。数据集的代码和部分将很快在https://github.com/meize0729/ccexpert上开源。

### Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization 
[[arxiv](https://arxiv.org/abs/2411.13036)] [[cool](https://papers.cool/arxiv/2411.13036)] [[pdf](https://arxiv.org/pdf/2411.13036)]
> **Authors**: Sanghyeob Song,Jaihyun Lew,Hyemi Jang,Sungroh Yoon
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: This paper is accepted to the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)
- **标题**: 通过交替优化对多模式图像对的无监督同型估计
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 估计两个图像之间的同谱图对于中或高级视觉任务（例如图像缝合和融合）至关重要。但是，由于难以收集基地真相数据，使用监督的学习方法通​​常具有挑战性或昂贵。作为回应，已经出现了无监督的学习方法。但是，大多数早期方法都假定给定的图像对来自同一相机或具有较小的照明差异。因此，尽管这些方法在这种条件下有效地执行，但当输入图像对来自不同域（称为多模式图像对）时，它们通常会失败。为了解决这些局限性，我们提出了Alto，这是一个无监督的学习框架，用于在多模式图像对中估算同构象。我们的方法采用了两相交替的优化框架，类似于期望最大化（EM），其中一个阶段降低了几何差距，而另一个阶段则解决了模态差距。为了处理这些差距，我们将Barlow Twins损失用于模态差距，并提出扩展版本Barlow Twins，用于几何差距。结果，我们证明我们的方法可以在没有任何基础数据的情况下在多模式数据集上进行培训。它不仅要优于其他无监督的方法，而且与同型估计器的各种体系结构兼容。源代码可以在：〜\ url {https://github.com/songsang7/alto}中找到

### ORID: Organ-Regional Information Driven Framework for Radiology Report Generation 
[[arxiv](https://arxiv.org/abs/2411.13025)] [[cool](https://papers.cool/arxiv/2411.13025)] [[pdf](https://arxiv.org/pdf/2411.13025)]
> **Authors**: Tiancheng Gu,Kaicheng Yang,Xiang An,Ziyong Feng,Dongnan Liu,Weidong Cai
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: 13 pages, 11 figures, WACV2025
- **标题**: ORID：放射学报告生成的器官区域信息驱动框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 放射学报告生成（RRG）的目的是根据放射学图像自动对疾病进行连贯的文本分析，从而减轻放射科医生的工作量。 RRG的当前基于AI的方法主要集中于对编码器模型体系结构的修改。为了推进这些方法，本文介绍了一个器官区域驱动的（ORID）框架，该框架可以有效地整合多模式信息并减少无关器官噪声的影响。具体而言，基于LLAVA-MED，我们首先构建了与RRG相关的指令数据集，以提高器官区域诊断描述能力并获得LLAVA-MED-RRG。之后，我们提出了一个基于器官的跨模式融合模块，以有效地结合了器官区域诊断描述和放射学图像的信息。为了进一步降低无关器官对放射学报告生成的影响，我们引入了器官重要性系数分析模块，该模块利用图形神经网络（GNN）来检查每个器官区域的跨模式信息的互连。在各种评估指标上与最先进方法的广泛实验AN1D比较证明了我们提出的方法的出色性能。

### VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge 
[[arxiv](https://arxiv.org/abs/2411.12915)] [[cool](https://papers.cool/arxiv/2411.12915)] [[pdf](https://arxiv.org/pdf/2411.12915)]
> **Authors**: Vishwesh Nath,Wenqi Li,Dong Yang,Andriy Myronenko,Mingxin Zheng,Yao Lu,Zhijian Liu,Hongxu Yin,Yucheng Tang,Pengfei Guo,Can Zhao,Ziyue Xu,Yufan He,Greg Heinrich,Yee Man Law,Benjamin Simon,Stephanie Harmon,Stephen Aylward,Marc Edgar,Michael Zephyr,Song Han,Pavlo Molchanov,Baris Turkbey,Holger Roth,Daguang Xu
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: Vila-M3：增强具有医学专家知识的视觉语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 通才视觉语言模型（VLM）在计算机视觉方面取得了长足的进步，但在专家知识的专业领域中，它们在专业知识至关重要的情况下却缺乏。在传统的计算机视觉任务中，创造性或近似答案可能是可以接受的，但是在医疗保健中，精确性是最重要的。由于它们依赖着记忆的互联网知识，而不是医疗保健中所需的细微专业知识，因此Gemini和GPT-4O（例如Gemini和GPT-4O）的电流大型多模型不足。 VLM通常在三个阶段进行训练：视觉预训练，视力语言预训练和指导微调（IFT）。通常使用通用和医疗保健数据的混合物应用IFT。相比之下，我们建议对于医疗VLM，这是必要的专业IFT的第四阶段，重点关注医疗数据，并包括来自域专家模型的信息。为医学使用开发的领域专家模型至关重要，因为它们是针对某些临床任务的专门培训的，例如通过细分和分类来检测肿瘤并对异常进行分类，这些特征学习了医疗数据的细粒特征 -  $  -  $特征通常太复杂了，以至于VLM无法有效捕获有效捕获。本文介绍了一个新的框架Vila-M3，用于医学VLM，该框架通过专家模型利用域知识。通过我们的实验，我们显示出改进的最新性能（SOTA）性能，比以前的SOTA模型Med-Gemini的平均提高约为9％，比对特定任务进行培训的模型的平均提高约为6％。我们的方法强调了领域专业知识在为医疗应用创建精确，可靠的VLM方面的重要性。

### Mitigating Perception Bias: A Training-Free Approach to Enhance LMM for Image Quality Assessment 
[[arxiv](https://arxiv.org/abs/2411.12791)] [[cool](https://papers.cool/arxiv/2411.12791)] [[pdf](https://arxiv.org/pdf/2411.12791)]
> **Authors**: Siyi Pan,Baoliang Chen,Danni Huang,Hanwei Zhu,Lingyu Zhu,Xiangjie Sui,Shiqi Wang
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 减轻感知偏见：一种无训练的方法来增强LMM的图像质量评估
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 尽管大型多模型模型（LMM）在高级视觉任务中的表现令人印象深刻，但它们的图像质量评估能力（IQA）仍然有限。一个主要原因是LMM主要接受高级任务（例如图像字幕）的培训，强调统一的图像语义语义在各种质量下提取。当这些LMM被迫获得质量评级时，这种语义意识但质量不敏感的感知不可避免地会导致对图像语义的严重依赖。在本文中，我们提出了一个无训练的伪造框架，而不是重新训练或调整LMM昂贵的框架，其中通过减轻图像语义引起的偏见来纠正图像质量预测。具体而言，我们首先探索几种语义上的扭曲，这些扭曲可能会显着降低图像质量，同时保持可识别的语义。通过将这些特定的扭曲应用于查询或测试图像，我们确保将降解的图像识别为质量差，而其语义仍然保留。在质量推断期间，查询图像及其相应的退化版本都被馈送到LMM，并提示表明，应在降级的条件下，应推断出查询图像质量的质量差的条件很差，这在先前的条件上有效地与LMM的质量感知相吻合，因为所有质量都不足以降级，因为其质量不足，因此质量不足，质量不足，并且质量不足，而质量则是质量的，并且质量不足。使用条件概率模型汇总条件（降级版本）。在各种IQA数据集上进行的广泛实验表明，我们的偏见框架可以始终提高LMM性能，并且代码将公开可用。

### Visual-Oriented Fine-Grained Knowledge Editing for MultiModal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.12790)] [[cool](https://papers.cool/arxiv/2411.12790)] [[pdf](https://arxiv.org/pdf/2411.12790)]
> **Authors**: Zhen Zeng,Leijiang Gu,Xun Yang,Zhangling Duan,Zenglin Shi,Meng Wang
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 以视觉为导向的高元素知识编辑用于多模式大语言模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 知识编辑旨在有效，成本效益地纠正不准确性并更新过时的信息。最近，人们对将知识编辑从大语言模型（LLM）扩展到多模式大语模型（MLLM）的兴趣越来越大，这些模型（MLLMS）都集成了文本和视觉信息，从而引入了其他编辑复杂性。现有的多模式知识编辑主要集中于面向文本的粗粒场景，无法解决多模式上下文所带来的独特挑战。在本文中，我们提出了一个面向视觉的，细粒度的多模式知识编辑任务，该任务针对具有多个相互作用实体的图像中的精确编辑。我们介绍了细粒度的视觉知识编辑（FGVEDIT）基准，以评估此任务。此外，我们提出了一个基于多模式范围分类器的知识编辑器（MSCKE）框架。 MSCKE利用多模式范围分类器，该分类器同时集成了视觉和文本信息，以准确识别和更新与图像中特定实体相关的知识。这种方法可确保精确编辑，同时保留无关紧要的信息，从而克服传统的仅文本编辑方法的局限性。 FGVEDIT基准测试的广泛实验表明，MSCKE胜过现有方法，展示了其在解决多模式知识编辑的复杂挑战方面的有效性。

### Efficient Physics Simulation for 3D Scenes via MLLM-Guided Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2411.12789)] [[cool](https://papers.cool/arxiv/2411.12789)] [[pdf](https://arxiv.org/pdf/2411.12789)]
> **Authors**: Haoyu Zhao,Hao Wang,Xingyue Zhao,Hao Fei,Hongqiu Wang,Chengjiang Long,Hua Zou
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 通过MLLM指导的高斯脱落为3D场景的有效物理模拟
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D代模型的最新进展为模拟动态3D对象运动和自定义行为开辟了新的可能性，但是创建此内容仍然具有挑战性。当前的方法通常需要手动分配精确的物理属性以进行仿真或依靠视频生成模型来预测它们，这在计算上是密集的。在本文中，我们重新考虑了基于物理的模拟中多模式大语言模型（MLLM）的使用，并介绍任何基于物理的方法，一种基于物理的方法，它赋予静态3D对象具有交互式动力学。我们从详细的场景重建和对象级别的3D开放式视频分割开始，然后在绘制中进行多视图图像。受到人类视觉推理的启发，我们提出了基于MLLM的物理特性感知（MLLM-P3）以零拍的方式预测对象的平均物理特性。基于平均值和对象的几何形状，材料属性分布预测模型（MPDP）模型估算了完整的分布，将问题重新定义为概率分布估计以降低计算成本。最后，我们在开放世界中模拟对象，并通过物理几何自适应采样（PGAS）策略采样粒子，有效地捕获复杂的变形并显着降低计算成本。广泛的实验和用户研究证明了我们的SIM，任何东西都比在单个GPU的2分钟内实现了更现实的运动。

### Visual Cue Enhancement and Dual Low-Rank Adaptation for Efficient Visual Instruction Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2411.12787)] [[cool](https://papers.cool/arxiv/2411.12787)] [[pdf](https://arxiv.org/pdf/2411.12787)]
> **Authors**: Pengkun Jiao,Bin Zhu,Jingjing Chen,Chong-Wah Ngo,Yu-Gang Jiang
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 视觉提示增强和双重低级适应，以进行有效的视觉教学微调
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 参数有效的微调多模式大语言模型（MLLM）提出了重大挑战，包括依赖限制细粒细节理解的高级视觉特征，以及由任务复杂性引起的数据冲突。为了解决这些问题，我们提出了一种有效的微调框架，采用两种新颖的方法：视觉提示增强（VCE）和双重低级适应（Dual-Lora）。 VCE通过集成多级视觉提示来增强视觉投影仪，从而提高了模型捕获细粒视觉特征的能力。 Dual-Lora引入了双重低级结构，以进行指导调整，将学习分解为技能和任务空间，以实现各种任务的精确控制和有效的适应。我们的方法简化了实现，增强了视觉理解并提高了适应性。对下游任务和一般基准测试的实验证明了我们提出的方法的有效性。

### Joint Vision-Language Social Bias Removal for CLIP 
[[arxiv](https://arxiv.org/abs/2411.12785)] [[cool](https://papers.cool/arxiv/2411.12785)] [[pdf](https://arxiv.org/pdf/2411.12785)]
> **Authors**: Haoyu Zhang,Yangyang Guo,Mohan Kankanhalli
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 剪辑的联合视觉社会偏见消除
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言（V-L）预训练的模型（例如剪辑）在各种下游任务中显示出突出的功能。尽管有这样的承诺，V-L模型仍受其固有的社会偏见的限制。一个典型的证明是，V-L模型通常会对特定人群产生偏见的预测，从而严重破坏了其现实世界的适用性。现有方法通过从模型嵌入中删除有偏见的属性信息来努力减轻V-L模型中的社会偏见问题。但是，在我们重新审视这些方法之后，我们发现它们的偏见去除经常伴随着极大的V-L对准能力。然后，我们揭示了这种性能降解源于图像和文本嵌入中的不平衡词汇。为了解决这个问题，我们提出了一个新颖的V-L依据框架，以使图像和文本偏见保持一致，然后将它们从两种方式中删除。通过这样做，我们的方法可以实现多模式偏置缓解措施，同时维持依据的嵌入中的V-L比对。此外，我们提倡一种新的评估协议，该协议可以1）整体量化模型歧义和V-L对准能力，以及2）评估社会偏见删除模型的概括。我们认为，这项工作将为解决剪辑中社会偏见问题的未来研究提供新的见解和指导。

### Med-2E3: A 2D-Enhanced 3D Medical Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2411.12783)] [[cool](https://papers.cool/arxiv/2411.12783)] [[pdf](https://arxiv.org/pdf/2411.12783)]
> **Authors**: Yiming Shi,Xun Zhu,Ying Hu,Chenyi Guo,Miao Li,Ji Wu
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: Med-2e3：2D增强的3D医学多模式模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D医学图像的分析对于现代医疗保健至关重要，但是由于各种临床方案的可推广性有限，传统的特定于任务的模型越来越不足。多模式大语模型（MLLM）为这些挑战提供了有希望的解决方案。但是，现有的MLLM在完全利用3D医学图像中嵌入的丰富的，分层的信息方面存在局限性。受临床实践的启发，放射科医生专注于3D空间结构和2D平面含量，我们提出了Med-2e3，这是一种用于整合3D和2D编码器的3D医学图像分析的新型MLLM。为了更有效地汇总2D功能，我们设计了文本引导的切片间（TG-IS）评分模块，该模块根据切片内容和任务说明来评分每个2D切片的注意力。据我们所知，MED-2E3是第一个集成3D和2D功能以进行3D医疗图像分析的MLLM。在大型开源3D医疗多模式基准上进行的实验表明，MED-2E3表现出特定于任务的注意力分布，并且显着优于当前的最新模型，报告生成14％，并且在医疗视觉问题上增长了5％的增长（VQA）（VQA），突出了该模型的潜力，可以解决复杂的多模型临床任务。该代码将在接受后发布。

### IoT-Based 3D Pose Estimation and Motion Optimization for Athletes: Application of C3D and OpenPose 
[[arxiv](https://arxiv.org/abs/2411.12676)] [[cool](https://papers.cool/arxiv/2411.12676)] [[pdf](https://arxiv.org/pdf/2411.12676)]
> **Authors**: Fei Ren,Chao Ren,Tianyi Lyu
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: 17 pages
- **标题**: 基于物联网的3D姿势估计和运动员的运动优化：C3D和OpenPose的应用
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 这项研究提出了IOT增强姿势优化网络（IE-PONET），用于高精度的3D姿势估计和田径运动员的运动优化。 IE-PONET集成了C3D用于时空特征提取，用于实时关键点检测的开放式特征，以及用于超参数调整的贝叶斯优化。 NTURGB+D和FINEGYM数据集的实验结果表现出卓越的性能，AP \（^p50 \）得分为90.5和91.0，MAP得分分别为74.3和74.0。消融研究证实了每个模块在增强模型准确性中的重要作用。 IE-PONET为运动性能分析和优化提供了强大的工具，为训练和预防伤害提供了精确的技术见解。未来的工作将集中于进一步的模型优化，多模式数据集成以及开发实时反馈机制以增强实际应用。

### A Multimodal Approach Combining Structural and Cross-domain Textual Guidance for Weakly Supervised OCT Segmentation 
[[arxiv](https://arxiv.org/abs/2411.12615)] [[cool](https://papers.cool/arxiv/2411.12615)] [[pdf](https://arxiv.org/pdf/2411.12615)]
> **Authors**: Jiaqi Yang,Nitish Mehta,Xiaoling Hu,Chao Chen,Chia-Ling Tsai
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: 21 pages, 9 figures, 8 tables
- **标题**: 一种多模式方法，结合了弱监督的OCT分割的结构和跨域文本指南
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 光学相干断层扫描（OCT）图像的准确分割对于诊断和监测视网膜疾病至关重要。但是，像素级注释的劳动密集型性质限制了大型数据集监督学习的可扩展性。弱监督的语义细分（WSSS）通过利用图像级标签提供了有希望的替代方法。在这项研究中，我们提出了一种新颖的WSSS方法，该方法将结构指导与文本驱动的策略相结合，以产生高质量的伪标签，从而显着提高了细分性能。在视觉信息方面，我们的方法采用了两个处理模块，它们可以从OCT图像中交换原始图像特征和结构特征，从而指导模型以确定可能发生病变的地方。在文本信息方面，我们利用跨域源的大规模预认证模型来实施具有标签的文本指导和合成描述性集成，并与两个文本处理模块结合了局部语义特征和一致的合成描述。通过将这些视觉和文本组件融合在多模式框架中，我们的方法增强了病变定位的精度。三个OCT数据集的实验结果表明，我们的方法实现了最先进的性能，突出了其提高诊断准确性和医学成像效率的潜力。

### Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination 
[[arxiv](https://arxiv.org/abs/2411.12591)] [[cool](https://papers.cool/arxiv/2411.12591)] [[pdf](https://arxiv.org/pdf/2411.12591)]
> **Authors**: Haojie Zheng,Tianyang Xu,Hanchi Sun,Shu Pu,Ruoxi Chen,Lichao Sun
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 查看之前的思考：通过缓解视觉幻觉改善多模式LLM推理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式的大语言模型（MLLM）已提高了视觉和语言方式的整合，将自己确立为视觉语言任务的主要范式。当前的思维链（COT）推理等当前方法增强了大语言模型（LLMS）的认知能力，但是它们对MLLM的适应受到了跨模式理解中幻觉的增加而阻碍。在本文中，我们发现，在当前多模式COT方法中寻找范式时的思维 - 在其中与视觉输入一起生成了推理链，以减轻由误导图像引起的幻觉。为了解决这些局限性，我们提出了视觉推断链（VIC）框架，这是一种新颖的方法，在引入视觉输入之前，仅使用文本上下文构建推理链，有效地降低了跨模式的偏见并增强了多模式推理精度。全面的评估表明，VIC显着提高了各种与视觉相关的任务的零射击性能，从而减轻幻觉，同时完善MLLM的推理能力。我们的代码存储库可以在https://github.com/terry-xu-666/visual_inference_chain上找到。

### Debias your Large Multi-Modal Model at Test-Time with Non-Contrastive Visual Attribute Steering 
[[arxiv](https://arxiv.org/abs/2411.12590)] [[cool](https://papers.cool/arxiv/2411.12590)] [[pdf](https://arxiv.org/pdf/2411.12590)]
> **Authors**: Neale Ratzlaff,Matthew Lyle Olson,Musashi Hinck,Estelle Aflalo,Shao-Yen Tseng,Vasudev Lal,Phillip Howard
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-20
> **comment**: 10 pages, 3 Figures, 3 Tables. arXiv admin note: text overlap with arXiv:2410.13976
- **标题**: Debias在测试时间内使用非对抗性视觉属性转向进行DEBIAS您的大型多模式模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 大型多模式模型（LMM）表现出令人印象深刻的功能，作为通用聊天机器人，可以参与有关提供的输入（例如图像）的对话。但是，他们的反应受到培训数据集中存在的社会偏见的影响，导致模型的响应方式不良差异，描绘了描绘不同人口统计学的人的图像。在这项工作中，我们为LMM提出了一个新颖的伪造框架，该框架在文本生成过程中直接消除了偏见的表示，以减少与受保护属性相关的输出，甚至在内部表示它们。我们提出的方法是无训练的；给定单个图像和目标属性列表，我们可以在图像本身上仅使用梯度下降的一个步骤消融相应的表示。我们的实验表明，我们不仅可以最大程度地减少LMM的倾向生成与受保护属性有关的文本的倾向，而且我们可以改善情感，甚至可以简单地使用综合数据来告知消融，同时在实际数据（例如可可或方面）保留语言建模能力。此外，我们发现来自DEBIAS的LMM的产生世代表现出与基线偏置模型相似的精度，表明可以在不牺牲模型性能的情况下实现辩护效应。

### Infrared-Assisted Single-Stage Framework for Joint Restoration and Fusion of Visible and Infrared Images under Hazy Conditions 
[[arxiv](https://arxiv.org/abs/2411.12586)] [[cool](https://papers.cool/arxiv/2411.12586)] [[pdf](https://arxiv.org/pdf/2411.12586)]
> **Authors**: Huafeng Li,Jiaqi Fang,Yafei Zhang,Yu Liu
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 红外辅助单阶段框架，用于在朦胧条件下可见图像的联合修复和融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 红外线和可见（IR-VIS）图像融合因其广泛的应用值而引起了极大的关注。但是，现有方法通常忽略红外图像在朦胧条件下恢复可见图像特征中的互补作用。为了解决这个问题，我们提出了一个联合学习框架，该框架利用红外图像恢复和融合了朦胧的IR-VIS图像。为了减轻IR-VIS图像之间特征多样性的不利影响，我们引入了一种迅速的生成机制，该机制调节了特定于模态特征不相容性。这将创建来自非共享图像信息的提示选择矩阵，然后再由提示池生成的提示嵌入。这些嵌入有助于生成候选功能以进行飞去。我们进一步设计了一种红外辅助功能恢复机制，该机制可以根据雾糊状的密度选择候选功能，从而在单级框架内同时恢复和融合。为了提高融合质量，我们构建了一个多级及时嵌入融合模块，该模块利用及时生成模块的补充。我们的方法有效地融合了IR-VIS图像，同时消除了雾霾，从而产生了清晰，无雾的融合结果。与DeHaze然后融合的两阶段方法相反，我们的方法可以在单阶段框架中进行协作培训，从而使模型相对轻巧且适合实用。实验结果证明了其有效性，并证明了与现有方法相比的优势。该论文的源代码可在\ href {https://github.com/fangjiaqi0909/iassf} {\ textcolor {blue}

### Leveraging MLLM Embeddings and Attribute Smoothing for Compositional Zero-Shot Learning 
[[arxiv](https://arxiv.org/abs/2411.12584)] [[cool](https://papers.cool/arxiv/2411.12584)] [[pdf](https://arxiv.org/pdf/2411.12584)]
> **Authors**: Xudong Yan,Songhe Feng,Yang Zhang,Jian Yang,Yueguan Lin,Haojun Fei
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 利用MLLM嵌入和属性平滑以进行组成零弹药学习
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 组成零摄入学习（CZSL）旨在识别从所见构图中学到的属性和对象的新颖组成。以前的作品通过在图像对之间提取共享和独家零件，共享相同的属性（对象），并与预验证的单词嵌入以改善看不见的属性-Object识别，从而解开属性和对象。尽管现有努力取得了重大成就，但它们受到三个局限性的阻碍：（1）由于背景的影响以及在同一部位中对象与对象的属性的错综复杂，因此脱离的功效受到了损害。 （2）现有的单词嵌入未能捕获复杂的多模式语义信息。 （3）现有模型在看到的组合物中表现出的过度自信阻碍了它们对新成分的概括。认识到这些，我们提出了一个名为多模式大语言模型（MLLM）嵌入的新型框架，以及CZSL的属性平滑指导性解散（TRIDEN）。首先，我们利用特征自适应聚合模块来减轻背景的影响，并利用可学习的掩码来捕获多方面的特征以进行分解。然后，MLLM的最后一个隐藏状态被用作其出色表示能力的单词嵌入。此外，我们提出了通过大语模型（LLM）生成的辅助属性（可见构图）的属性平滑，通过鼓励模型在一个给定的组合中学习更多属性来解决过度自信的问题。广泛的实验表明，Trident在三个基准测试中实现了最先进的性能。

### Contourlet Refinement Gate Framework for Thermal Spectrum Distribution Regularized Infrared Image Super-Resolution 
[[arxiv](https://arxiv.org/abs/2411.12530)] [[cool](https://papers.cool/arxiv/2411.12530)] [[pdf](https://arxiv.org/pdf/2411.12530)]
> **Authors**: Yang Zou,Zhixin Chen,Zhipeng Zhang,Xingyuan Li,Long Ma,Jinyuan Liu,Peng Wang,Yanning Zhang
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: 13 figures, 6 tables
- **标题**: 热频谱分布正则红外图像超分辨率的高光谱分布的Contourlet改进门框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像超分辨率（SR）是一个经典但仍有活跃的低级视觉问题，旨在从其低分辨率（LR）对应物中重建高分辨率（HR）图像，作为图像增强的关键技术。解决SR任务的当前方法，例如基于变压器和基于扩散的方法，要么致力于提取RGB图像特征，要么假设相似的降解模式，从而忽略了红外和可见图像之间固有的模态差异。当直接应用于红外图像SR任务时，这些方法不可避免地会扭曲红外光谱分布，从而损害下游任务中的机器感知。在这项工作中，我们强调红外光谱分布保真度，并提出了一个Contourlet Refinempt Gate框架，以恢复红外模态特异性特征，同时保留光谱分布的保真度。我们的方法从多尺度和多方向红外光谱分解中捕获了高通子带，以通过门架构恢复红外衰落的信息。拟议的光谱保真度损失使重建过程中的光谱频率分布正常，这确保了保留高频和低频组件，并保持红外特异性特征的保真度。我们提出了一个两阶段的及时学习优化，以指导LR降解中学习红外HR特征的模型。广泛的实验表明，我们的方法在视觉和感知任务中都优于现有的图像SR模型，同​​时特别增强了下游任务中的机器感知。我们的代码可在https://github.com/hey-it-s-me/corple上找到。

### Generative Timelines for Instructed Visual Assembly 
[[arxiv](https://arxiv.org/abs/2411.12293)] [[cool](https://papers.cool/arxiv/2411.12293)] [[pdf](https://arxiv.org/pdf/2411.12293)]
> **Authors**: Alejandro Pardo,Jui-Hsien Wang,Bernard Ghanem,Josef Sivic,Bryan Russell,Fabian Caba Heilbron
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 指导视觉组装的生成时间表
- **领域**: 计算机视觉和模式识别,人机交互,多媒体
- **摘要**: 这项工作的目的是通过自然语言说明来操纵视觉时间表（例如视频），从而使非专家甚至可能残障用户可以访问复杂的时间表编辑任务。我们称此任务指示为视觉组件。此任务具有挑战性，因为它需要（i）确定输入时间表中的相关视觉内容，并在给定输入（视频）集合中检索相关的视觉内容，（ii）了解输入自然语言指令，以及（iii）执行输入视觉时间表的所需编辑以产生输出时间表。为了应对这些挑战，我们提出了时间表组装程序，这是一种生成模型，该模型训练有素，可以执行指导的视觉组装任务。这项工作的贡献是三倍。首先，我们开发了一个大型的多模式模型，该模型旨在处理视觉内容，紧凑地表示时间表并准确解释时间表编辑指令。其次，我们引入了一种新的方法，用于自动生成用于视觉组装任务的数据集，从而无需人工标记的数据就可以有效地训练我们的模型。第三，我们通过为图像和视频组装创建两个新颖的数据集来验证我们的方法，这表明时间轴组件的表现大大优于建立的基线模型，包括最近的GPT-4O，以准确地在各种现实世界中灵感的场景中准确执行复杂的组装指令。

### Neuro-3D: Towards 3D Visual Decoding from EEG Signals 
[[arxiv](https://arxiv.org/abs/2411.12248)] [[cool](https://papers.cool/arxiv/2411.12248)] [[pdf](https://arxiv.org/pdf/2411.12248)]
> **Authors**: Zhanqiang Guo,Jiamin Wu,Yonghao Song,Jiahui Bu,Weijian Mai,Qihao Zheng,Wanli Ouyang,Chunfeng Song
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: Neuro-3D：朝着EEG信号进行3D视觉解码
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类对视觉世界的看法是由3D信息的立体处理来塑造的。了解大脑在现实世界中如何感知和过程3D视觉刺激是神经科学的长期努力。为了实现这一目标，我们介绍了一项新的神经科学任务：从EEG信号中解码3D视觉感知，这是一种神经影像学技术，可以实时监测具有复杂视觉提示的神经动力学。为了提供必需的基准测试，我们首先提出EEG-3D，这是一个开创性的数据集，其中包含多模式分析数据和来自12个受试者的大量EEG记录，这些主题查看了72个类别的3D对象，呈现了视频和图像中的3D对象。此外，我们提出了基于EEG信号的3D视觉解码框架Neuro-3D。该框架自适应地整合了从静态和动态刺激中得出的EEG特征，以学习互补和健壮的神经表示，随后将其用于通过所提出的基于基于扩散的彩色点云解码器来恢复3D对象的形状和颜色。据我们所知，我们是第一个探索基于脑电图的3D视觉解码的人。实验表明，Neuro-3D不仅可以重建具有高保真性的彩色3D对象，而且还学习有效的神经表示，可以实现洞察力的大脑区域分析。数据集和关联的代码将公开可用。

### Decompose and Leverage Preferences from Expert Models for Improving Trustworthiness of MLLMs 
[[arxiv](https://arxiv.org/abs/2411.13697)] [[cool](https://papers.cool/arxiv/2411.13697)] [[pdf](https://arxiv.org/pdf/2411.13697)]
> **Authors**: Rui Cao,Yuming Jiang,Michael Schlichtkrull,Andreas Vlachos
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: No comments
- **标题**: 分解和利用专家模型的偏好，以提高MLLM的可信度
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语模型（MLLM）可以通过与人类的偏好保持一致来增强信任度。由于人类偏好标签很费力，最近的工作采用评估模型来评估MLLM的响应，并使用基于模型的评估来自动化偏好数据集构建。但是，这种方法面临MLLM的冗长和构图反应面临的挑战，通常需要单个评估模型可能无法完全拥有的各种推理技能。此外，大多数现有方法都依靠封闭源模型作为评估器。为了解决局限性，我们建议Decompgen，这是一个使用开源专家模型集合的可分解框架。 Decompgen将每个响应分解为原子验证任务，将每个任务分配给适当的专家模型以生成细粒度的评估。 Decompgen反馈用于自动构建我们的偏好数据集DGPREF。通过偏好学习与DGPREF保持一致的MLLM显示出可信赖性的改善，证明了Decompgen的有效性。

### FabuLight-ASD: Unveiling Speech Activity via Body Language 
[[arxiv](https://arxiv.org/abs/2411.13674)] [[cool](https://papers.cool/arxiv/2411.13674)] [[pdf](https://arxiv.org/pdf/2411.13674)]
> **Authors**: Hugo Carneiro,Stefan Wermter
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: 23 pages, 8 figures, 3 tables, accepted for publication in Neural Computing and Applications
- **标题**: ASD：通过肢体语言揭示语音活动
- **领域**: 计算机视觉和模式识别,机器学习,神经和进化计算,声音,音频和语音处理
- **摘要**: 多模式环境中的主动扬声器检测（ASD）对于从视频会议到人类机器人相互作用的各种应用至关重要。本文介绍了Fabulight-ASD，这是一种先进的ASD模型，该模型集成了面部，音频和身体姿势信息，以提高检测准确性和鲁棒性。我们的模型通过合并通过骨架图表示的人类姿势数据来建立在现有的轻型ASD框架上，从而最大程度地减少了计算开销。使用Wilder Active扬声器检测（WASD）数据集，以可靠的面部和身体边界框注释而闻名，我们在现实世界中演示了Abulight-ASD的有效性。 Fabulight-ASD的总体平均平均精度（MAP）为94.3％，超过了Light-ASD，在各种具有挑战性的情况下，总体地图为93.7％。身体姿势信息的结合显示出特别有利的影响，在具有语音障碍，面部遮挡和人类语音背景噪声的情况下观察到的地图有了显着改善。此外，效率分析仅表明参数计数（27.3％）和多功能操作（高达2.4％）仅适度增加，从而强调了该模型的效率和可行性。这些发现证明了通过整合身体姿势数据来增强ASD性能的功效。 Fabulight-ASD的代码和模型权重可从https://github.com/knowledgetechnologyuhh/fabulight-asd获得。

### Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning 
[[arxiv](https://arxiv.org/abs/2411.13623)] [[cool](https://papers.cool/arxiv/2411.13623)] [[pdf](https://arxiv.org/pdf/2411.13623)]
> **Authors**: Tim Lenz,Peter Neidlinger,Marta Ligero,Georg Wölflein,Marko van Treeck,Jakob Nikolas Kather
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: No comments
- **标题**: 无监督的基础模型幻灯片级表示学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 病理学全扫描图像（WSIS）的表示学主要依赖于多个实例学习（MIL）的弱监督。这种方法导致幻灯片表示高度针对特定的临床任务。自我监督学习（SSL）已成功地应用于培训组织病理学基础模型（FMS），以进行补丁嵌入生成。但是，产生患者或幻灯片水平的嵌入仍然具有挑战性。幻灯片表示学习的现有方法通过对齐幻灯片的不同增强或利用多模式数据，将SSL的原理从补丁级别的学习到整个幻灯片扩展。通过集成来自多个FMS的图块嵌入，我们在特征空间中提出了一种新的单一模态SSL方法，该方法生成有用的幻灯片表示。我们的对比训练术，称为眼镜蛇，采用了多个FMS和基于Mamba-2的体系结构。 COBRA超过了在四个不同的公共临床肿瘤分析联盟（CPTAC）队列中，最先进的幻灯片编码器的性能平均至少为 +4.5％AUC，尽管仅在癌症基因组地图集（​​TCGA）的3048 WSI上进行了预估。此外，在推理时间与以前看不见的特征提取器时，眼镜蛇很容易兼容。可在https://github.com/katherlab/cobra上找到代码。

### VioPose: Violin Performance 4D Pose Estimation by Hierarchical Audiovisual Inference 
[[arxiv](https://arxiv.org/abs/2411.13607)] [[cool](https://papers.cool/arxiv/2411.13607)] [[pdf](https://arxiv.org/pdf/2411.13607)]
> **Authors**: Seong Jong Yoo,Snehesh Shrestha,Irina Muresanu,Cornelia Fermüller
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-21
> **comment**: Accepted by WACV 2025 in Round 1. First two authors contributed equally
- **标题**: Viopose：小提琴性能4D姿势估算通过等级视听推断
- **领域**: 计算机视觉和模式识别
- **摘要**: 音乐家精心控制自己的身体以产生音乐。有时，他们的动作太微妙了，无法被人眼捕获。为了分析它们如何产生音乐，我们需要估计精确的4D人类姿势（随着时间的推移3D姿势）。然而，由于遮挡，部分视图和人类对象相互作用，目前的最新视觉姿势姿势估计算法难以产生准确的单眼4D姿势。它们受到摄像机的视角，像素密度和采样率的限制，并且无法估计快速而微妙的运动，例如颤音的音乐效果。我们利用产生的音乐与人类动作之间的直接因果关系来解决这些挑战。我们提出了Viopose：一种新型的多模式网络，可以分层估计动态。高级功能级联到低级功能，并集成到贝叶斯更新中。我们的体系结构被证明可以产生准确的姿势序列，促进精确运动分析，并且表现优于SOTA。作为这项工作的一部分，我们收集了最大，最多样化的小提琴播放数据集，包括视频，声音和3D运动捕获姿势。代码和数据集可以在我们的项目页面\ url {https://sj-yoo.info/viopose/}中找到。

### Public Health Advocacy Dataset: A Dataset of Tobacco Usage Videos from Social Media 
[[arxiv](https://arxiv.org/abs/2411.13572)] [[cool](https://papers.cool/arxiv/2411.13572)] [[pdf](https://arxiv.org/pdf/2411.13572)]
> **Authors**: Naga VS Raviteja Chappa,Charlotte McCormick,Susana Rodriguez Gongora,Page Daniel Dobbs,Khoa Luu
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-21
> **comment**: Under review at International Journal of Computer Vision (IJCV); 29 figures, 5 figures;
- **标题**: 公共卫生倡导数据集：来自社交媒体的烟草使用视频数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 公共卫生倡导数据集（PHAD）是与来自Tiktok和YouTube这样的社交媒体平台采购的烟草产品相关的5,730个视频的综合集合。该数据集包含430万帧，并包括详细的元数据，例如用户参与度量标准，视频说明和搜索关键字。这是第一个具有这些功能的数据集，它为分析与烟草相关的内容及其影响提供了宝贵的资源。我们的研究采用了两阶段的分类方法，结合了视觉语言（VL）编码器，在准确地对各种类型的烟草产品和使用情况方案进行准确分类时表明了卓越的性能。该分析揭示了重要的用户参与趋势，尤其是随着烟雾和电子烟的含量，突出了针对有针对性的公共卫生干预措施的领域。 PHAD解决了公共卫生研究中多模式数据的需求，提供了可以为监管政策和公共卫生策略提供信息的见解。该数据集是理解和减轻烟草使用影响的关键一步，以确保公共卫生的努力更具包容性和有效性。

### Can Reasons Help Improve Pedestrian Intent Estimation? A Cross-Modal Approach 
[[arxiv](https://arxiv.org/abs/2411.13302)] [[cool](https://papers.cool/arxiv/2411.13302)] [[pdf](https://arxiv.org/pdf/2411.13302)]
> **Authors**: Vaishnavi Khindkar,Vineeth Balasubramanian,Chetan Arora,Anbumani Subramanian,C. V. Jawahar
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: No comments
- **标题**: 原因可以帮助改善行人意图估计吗？跨模式的方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着自主导航系统的重要性，越来越需要保护弱势道路使用者（VRU）（例如行人）的安全。预测行人意图是一项充满挑战的任务，即先前的工作可以通过视觉和运动特征融合到二进制交叉/无交叉意图。但是，到目前为止，没有努力以人为理解的原因进行对冲预测。我们通过引入一个新的问题设置来解决这个问题，以探索行人意图背后的直觉推理。特别是，我们表明，预测“为什么”在理解“什么”方面非常有用。为此，我们提出了一个新颖的，富含理性的PIE ++数据集，该数据集由多标签的文本解释/行人意图的原因组成。我们还介绍了一个名为Mindread的新型多任务学习框架，该框架利用跨模式表示学习框架来预测行人意图以及意图背后的原因。我们的全面实验显示，使用MindRead在PIE ++数据集上的意图预测任务中，准确性和F1得分的准确性和F1得分的显着提高了5.6％和7％。在常用JAAD数据集上，我们还提高了4.4％的准确性。使用定量/定性指标和用户研究的广泛评估显示了我们方法的有效性。

### VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation 
[[arxiv](https://arxiv.org/abs/2411.13281)] [[cool](https://papers.cool/arxiv/2411.13281)] [[pdf](https://arxiv.org/pdf/2411.13281)]
> **Authors**: Ziyang Luo,Haoning Wu,Dongxu Li,Jing Ma,Mohan Kankanhalli,Junnan Li
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: Project Page: https://videoautoarena.github.io/
- **标题**: VideoAutoArena：一个自动竞技场，用于通过用户模拟评估视频分析中的大型多模式
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体
- **摘要**: 具有高级视频分析功能的大型多模型模型（LMM）最近引起了极大的关注。但是，大多数评估都依赖于传统方法，例如基准中的多项选择问题，例如Videmomme和LongVideObench，这些方法很容易缺乏捕获现实世界用户的复杂需求所需的深度。为了解决这一限制，并且由于人类注释的成本高昂和视频任务的速度缓慢，我们介绍了VideoAutoArena，这是一个受LMSYS Chatbot Arena框架启发的竞技场风格的基准，旨在自动评估LMMS的视频分析能力。 VideoAutoArena利用用户模拟生成开放式的自适应问题，这些问题严格评估视频理解中的模型性能。该基准具有一个可自动的，可扩展的评估框架，并结合了修改的ELO评级系统，可在多个LMM上进行公平和连续的比较。为了验证我们的自动评审系统，我们使用经过精心策划的人类注释子集构建了“黄金标准”，这表明我们的舞台在维持可伸缩性的同时与人类的判断力密切一致。此外，我们引入了一种故障驱动的进化策略，逐渐增加了问题的复杂性，以推动模型来处理更具挑战性的视频分析方案。实验结果表明，VideoAutoarena有效地区分了最先进的LMM，从而提供了对模型优势和改进领域的见解。为了进一步简化我们的评估，我们将VideoAutobench作为辅​​助基准进行了介绍，在VideoAutoarena战斗中，人类注释者标签赢家。我们使用GPT-4O作为法官，将回答与这些人类验证的答案进行比较。 VideoAutoArena和VideoAutobench一起提供了一个具有成本效率且可扩展的框架，用于评估以用户为中心的视频分析。

### TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.13136)] [[cool](https://papers.cool/arxiv/2411.13136)] [[pdf](https://arxiv.org/pdf/2411.13136)]
> **Authors**: Xin Wang,Kai Chen,Jiaming Zhang,Jingjing Chen,Xingjun Ma
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: No comments
- **标题**: TAPT：测试时间对抗及时调整视觉模型的鲁棒推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型预训练的视觉模型（VLM）（例如剪辑）在各种下游任务中都表现出极好的零光推广性。但是，最近的研究表明，剪辑的推理性能可以通过小小的对抗性扰动（尤其是其视觉方式）极大地降低，从而带来了重大的安全威胁。为了减轻这种脆弱性，在本文中，我们提出了一种新颖的防御方法，称为“测试时间对抗及时调谐（TAPT）”，以增强剪辑对视觉对抗性攻击的推理鲁棒性。 TAPT是一种测试时间防御方法，它学习防御性双峰（文本和视觉）提示以鲁棒化剪辑的推理过程。具体而言，这是一种无监督的方法，可以通过最大程度地减少多视图熵并对准对抗性 - 清洁分布来优化每个测试样本的防御提示。我们评估了TAPT对11个基准数据集的有效性，包括ImageNet和其他10个零摄像机数据集，表明它可以使原始剪辑的零拍对对抗性鲁棒性至少在对AutoAttack（AA）中至少增强48.9％，同时在很大程度上保持了清洁示例的性能。此外，TAPT优于各种骨架上现有的对抗及时调整方法，实现了至少36.6％的平均鲁棒性提高。

### DriveMLLM: A Benchmark for Spatial Understanding with Multimodal Large Language Models in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2411.13112)] [[cool](https://papers.cool/arxiv/2411.13112)] [[pdf](https://arxiv.org/pdf/2411.13112)]
> **Authors**: Xianda Guo,Ruijun Zhang,Yiqun Duan,Yuhang He,Chenming Zhang,Shuai Liu,Long Chen
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: Code will be available at \url{https://github.com/XiandaGuo/Drive-MLLM}
- **标题**: drivemllm：在自动驾驶中使用多模式大型语言模型的空间理解的基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 自主驾驶需要对3D环境有全面的了解，以促进高级任务，例如运动预测，计划和映射。在本文中，我们介绍了Drivemllm，这是一种专门设计的基准，旨在评估自动驾驶中多模式大语言模型（MLLM）的空间理解能力。 Drivemllm包括880个面向前置的摄像头图像，并介绍了绝对和相对空间推理任务，并伴随着语言上多样化的自然语言问题。为了衡量MLLM的性能，我们提出了针对空间理解的新型评估指标。我们在DriveMllM上评估了一些最新的MLLM，我们的结果揭示了当前模型在理解驾驶环境中复杂空间关系的局限性。我们认为，这些发现强调了对更先进的基于MLLM的空间推理方法的需求，并强调了Drivemllm推动自主驾驶进一步研究的潜力。代码将在\ url {https://github.com/xiandaguo/drive-mllm}上找到。

### Hints of Prompt: Enhancing Visual Representation for Multimodal LLMs in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2411.13076)] [[cool](https://papers.cool/arxiv/2411.13076)] [[pdf](https://arxiv.org/pdf/2411.13076)]
> **Authors**: Hao Zhou,Zhanning Gao,Maosheng Ye,Zhili Chen,Qifeng Chen,Tongyi Cao,Honggang Qi
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: No comments
- **标题**: 提示的提示：在自动驾驶中增强多模式LLM的视觉表示
- **领域**: 计算机视觉和模式识别
- **摘要**: 鉴于自动驾驶环境的动态性质和严格的安全要求，一般的MLLM与剪辑相结合通常很难准确地代表特定的驾驶场景，尤其是在复杂的互动和长尾案例中。 To address this, we propose the Hints of Prompt (HoP) framework, which introduces three key enhancements: Affinity hint to emphasize instance-level structure by strengthening token-wise connections, Semantic hint to incorporate high-level information relevant to driving-specific cases, such as complex interactions among vehicles and traffic signs, and Question hint to align visual features with the query context, focusing on question-relevant regions.这些提示是通过提示融合模块融合的，丰富了视觉表示，并增强了自动驾驶VQA任务的多模式推理。广泛的实验证实了啤酒花框架的有效性，表明它在所有关键指标上的表现都大大优于先前的最新方法。

### Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale Benchmark 
[[arxiv](https://arxiv.org/abs/2411.13056)] [[cool](https://papers.cool/arxiv/2411.13056)] [[pdf](https://arxiv.org/pdf/2411.13056)]
> **Authors**: Bing Cao,Quanhao Lu,Jiekang Feng,Qilong Wang,Qinghua Hu,Pengfei Zhu
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: ICLR25
- **标题**: 有效的蒙版自动编码器用于视频对象计数和大规模基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 前后背景的动态不平衡是视频对象计数的主要挑战，这通常是由目标对象的稀疏引起的。这在现有作品中仍在研究，并且通常会导致严重的不足/过度预测错误。为了在视频对象计数中解决此问题，我们在本文中提出了一个密度插入的有效掩盖自动编码器计数（E-MAC）框架。为了赋予模型在密度回归上的表示能力，我们开发了一个新的$ \ sathtt {d} $ endity-$ \ m arthtt {e} $ mbedded $ \ mathtt {m} $询问m $ $ \ m $ \ mathtt {o} $ deling（$ \ mathtt {demo} $）图像和密度图的自代表学习。尽管$ \ mathtt {demo} $有助于有效的跨模式回归指导，但它也带来了冗余的背景信息，因此很难专注于前景区域。为了应对这一难题，我们提出了从密度图得出的有效的空间自适应掩模以提高效率。同时，我们采用基于光流的时间协作融合策略来有效地捕获跨帧之间的动态变化，从而使特征与得出多帧密度残差进行对齐。通过利用相邻帧的信息来提高当前帧的计数精度。此外，考虑到大多数现有数据集限于以人为本的情况，我们首先在自然场景中提出了一个大型视频鸟类计数数据集，即Dronebird，以进行迁移鸟类保护。在三个人群数据集和我们的\ textit {dronebird}上进行了广泛的实验，可以验证我们对同行的优势。代码和数据集可用。

### MEGL: Multimodal Explanation-Guided Learning 
[[arxiv](https://arxiv.org/abs/2411.13053)] [[cool](https://papers.cool/arxiv/2411.13053)] [[pdf](https://arxiv.org/pdf/2411.13053)]
> **Authors**: Yifei Zhang,Tianxu Jiang,Bo Pan,Jingyu Wang,Guangji Bai,Liang Zhao
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: No comments
- **标题**: MEGL：多模式解释引导学习
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 解释人工智能（AI）模型的决策过程对于解决其“黑匣子”性质至关重要，尤其是在图像分类之类的任务中。传统可解释的AI（XAI）方法通常依赖于单峰解释，无论是视觉还是文本，每个解释都有固有的局限性。视觉解释突出了关键区域，但通常缺乏理由，而文本解释则提供了上下文，而无需空间基础。此外，两种解释类型都可能不一致或不完整，从而限制了它们的可靠性。为了应对这些挑战，我们提出了一种新颖的多模式解释引导学习（MEGL）框架，该框架利用视觉和文本解释来增强模型可解释性并提高分类性能。我们以显着驱动的文本基础（SDTG）方法将视觉解释的空间信息集成到文本理由中，从而提供了空间基础和上下文丰富的解释。此外，我们在视觉解释中介绍文本监督，以使视觉解释与文本原理相结合，即使在缺少地面真相视觉注释的情况下。视觉解释分布一致性损失通过将生成的视觉解释与数据集级别模式对齐，从而进一步增强了视觉连贯性，从而使模型能够从不完整的多模式监督中有效学习。我们在两个新数据集（即对象 - 我和Action-ME）上验证MEGL，以用于图像分类，并使用多模式说明。实验结果表明，MEGL在视觉和文本领域的预测准确性和解释质量方面都优于先前的方法。我们的代码将在接受文件后提供。

### Evaluating and Advancing Multimodal Large Language Models in Ability Lens 
[[arxiv](https://arxiv.org/abs/2411.14725)] [[cool](https://papers.cool/arxiv/2411.14725)] [[pdf](https://arxiv.org/pdf/2411.14725)]
> **Authors**: Feng Chen,Chenhui Gou,Jing Liu,Yang Yang,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 评估和推进能力镜头中的多模式大语模型
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 随着多模式的大语言模型（MLLM）迅速发展，严格的评估已成为必不可少的，为其发展提供了进一步的指导。在这项工作中，我们专注于对\ textbf {Vision感知}能力的统一和强大的评估，即MLLM的基本技能。我们发现，现有的感知基准，每个基准都集中在不同的问题类型，域和评估指标上，引入了显着的评估差异，在依靠任何单个基准测试时，对感知能力的全面评估变得复杂。为了解决这个问题，我们介绍了\ textbf {cambillens}，这是一个统一的基准测试，旨在评估六个关键感知能力的MLLM，重点介绍准确性和稳定性，每种能力都包含各种问题类型，域，域和指标。在能力的帮助下，我们：（1）确定当前模型的优势和劣势，突出稳定性模式，并揭示开源和封闭源模型之间的显着性能差距； （2）引入在线评估模式，该模式在MLLM培训期间发现了有趣的能力冲突和早期收敛现象； （3）设计一种简单的特定能力模型合并方法，结合了早期训练阶段的最佳能力检查点，从而有效地减轻了由于能力冲突而导致的绩效下降。基准和在线排行榜将很快发布。

### Any-to-3D Generation via Hybrid Diffusion Supervision 
[[arxiv](https://arxiv.org/abs/2411.14715)] [[cool](https://papers.cool/arxiv/2411.14715)] [[pdf](https://arxiv.org/pdf/2411.14715)]
> **Authors**: Yijun Fan,Yiwei Ma,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 通过混合扩散监督任何一到3D的生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散模型提供的强大先验促进了3D对象产生的最新进展。但是，现有模型是针对特定任务量身定制的，一次仅适应一种方式，并且需要重新训练以更改方式。给定图像到3D模型和文本提示，一种天真的方法是将文本提示转换为图像，然后将图像到3D模型用于生成。这种方法既耗时又是劳动力密集的，从而导致模式转换过程中不可避免的信息丢失。为了解决这个问题，我们介绍了Xbind，这是使用跨模式预先对准技术的任何一对3D生成的统一框架。 XBIND与预训练的扩散模型集成了多模式一致的编码器，以从任何模态（包括文本，图像和音频）中生成3D对象。随后，我们提出了一种新颖的损失函数，称为模态相似性（MS）损失，该损失与模态提示和渲染图像的嵌入相一致，从而促进了具有多种方式的3D对象的对齐。此外，混合扩散监督与三相优化过程相结合，改善了生成的3D对象的质量。广泛的实验在任何一到3D的情况下都展示了Xbind的广泛一代能力。据我们所知，这是从任何模态提示生成3D对象的第一种方法。项目页面：https：//zeroooooooow1440.github.io/。

### Cross-Modal Pre-Aligned Method with Global and Local Information for Remote-Sensing Image and Text Retrieval 
[[arxiv](https://arxiv.org/abs/2411.14704)] [[cool](https://papers.cool/arxiv/2411.14704)] [[pdf](https://arxiv.org/pdf/2411.14704)]
> **Authors**: Zengbao Sun,Ming Zhao,Gaorui Liu,André Kaup
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: ef:IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 1-18, 2024, Art no. 4709118
- **标题**: 跨模式预先对准方法，以及全球和本地信息，用于遥感图像和文本检索
- **领域**: 计算机视觉和模式识别,信息检索
- **摘要**: 遥感跨模式文本图像检索（RSCTIR）因其在信息挖掘方面的效用而引起了人们的关注。然而，由于遥感图像中的变化并确保模态融合前的适当特征预一致性，挑战仍在有效地整合全球和本地信息，这会影响检索准确性和效率。为了解决这些问题，我们提出了CMPAGL，这是一种利用全球和本地信息的跨模式预一致方法。我们的GSWIN变压器块结合了本地窗口自我注意力和全球本地窗口交叉注意，以捕获多尺度功能。预先安装机制简化了模态融合训练，从而提高了检索性能。此外，我们引入了用于重新我们的相似性矩阵重新加权（SMR）算法，并使用类内距离项来增强三重态损耗函数，以优化特征学习。在包括RSICD和RSITMD在内的四个数据集上的实验验证了CMPAGL的有效性，在R@1和2.28％的均值召回（MR）中，在最先进的方法中提高了4.65％。

### GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI 
[[arxiv](https://arxiv.org/abs/2411.14522)] [[cool](https://papers.cool/arxiv/2411.14522)] [[pdf](https://arxiv.org/pdf/2411.14522)]
> **Authors**: Tianbin Li,Yanzhou Su,Wei Li,Bin Fu,Zhe Chen,Ziyan Huang,Guoan Wang,Chenglong Ma,Ying Chen,Ming Hu,Yanjun Li,Pengcheng Chen,Xiaowei Hu,Zhongying Deng,Yuanfeng Ji,Jin Ye,Yu Qiao,Junjun He
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: Gmail-vl＆gmail-vl-5.50万：一种大型视觉模型和全面的多模式数据集，用于一般医疗AI
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管通用人工智能（例如GPT-4）的大幅进步，但由于缺乏专门的医学知识，它们在医疗领域（General Medical AI，GMAI）中的有效性仍然受到限制。为了应对这一挑战，我们提出了GMAI-VL-5.5M，这是一个通过将数百个专业的医疗数据集转换为精心构造的图像文本对而创建的全面的多模式数据集。该数据集具有全面的任务覆盖范围，不同的方式和高质量的图像文本数据。在此多模式数据集的基础上，我们提出了GMAI-VL，这是一种具有逐步三阶段培训策略的一般医学视觉语言模型。这种方法通过整合视觉和文本信息可以显着增强模型的能力，从而提高其处理多模式数据的能力并支持准确的诊断和临床决策。实验评估表明，GMAI-VL在各种多模式医学任务（例如视觉问答答案和医学图像诊断）中取得了最新的结果。我们的贡献包括开发GMAI-VL-550万数据集，GMAI-VL模型的引入以及在多个医疗领域中建立新的基准。代码和数据集将在https://github.com/uni-medical/gmai-vl上发布。

### LLaVA-MR: Large Language-and-Vision Assistant for Video Moment Retrieval 
[[arxiv](https://arxiv.org/abs/2411.14505)] [[cool](https://papers.cool/arxiv/2411.14505)] [[pdf](https://arxiv.org/pdf/2411.14505)]
> **Authors**: Weiheng Lu,Jian Li,An Yu,Ming-Ching Chang,Shengpeng Ji,Min Xia
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: Llava-MR：视频时刻检索的大型语言和视觉助手
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）广泛用于视觉感知，理解和推理。但是，由于LLMS有限的上下文大小和粗框架提取，长期的视频处理和精确的力矩检索仍然具有挑战性。我们提出了瞬间检索的大型语言和视觉助手（LLAVA-MR），该助理可以使用MLLM在视频中进行准确的时刻检索和上下文基础。 LLAVA-MR结合了空间框架和时间编码（DFTE），用于时空特征提取，信息性框架选择（IFS），用于捕获简短的视觉和运动模式以及动态令牌压缩（DTC）来管理LLM上下文上下文限制。对Charades-STA和QVHighlights等基准测试的评估表明，LLAVA-MR的表现优于11种最先进的方法，在r1@0.5中获得了1.82％的提高，在r1@0.5中，在qvhighlightlights dataset上获得了map@0.5的1.29％。我们的实施将在接受后开源。

### Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.14432)] [[cool](https://papers.cool/arxiv/2411.14432)] [[pdf](https://arxiv.org/pdf/2411.14432)]
> **Authors**: Yuhao Dong,Zuyan Liu,Hai-Long Sun,Jingkang Yang,Winston Hu,Yongming Rao,Ziwei Liu
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: Insight-V：使用多模式大语言模型探索长链视觉推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型语言模型（LLMS）通过推理更多的推理，展现了增强的功能和可靠性，从促使促使openai o1等产品级别的解决方案演变而来。尽管为改善LLM推理的各种努力，但在视觉任务中，高质量的长链推理数据和优化的培训管道仍然不足。在本文中，我们提出了Insight-V的早期努力1）为复杂的多模式任务产生长期且可靠的推理数据，以及2）有效的培训管道，以增强多模式大语言模型（MLLMS）的推理能力。具体而言，为了创建长期且结构化的推理数据，我们设计了一条具有渐进策略的两步管道，以生成足够长而多样的推理路径和多种范围评估方法来确保数据质量。我们观察到，使用如此长而复杂的推理数据直接监督MLLM不会产生理想的推理能力。为了解决这个问题，我们设计了一个多代理系统，该系统由专门用于执行长链推理的推理代理和经过培训的摘要代理来判断和总结推理结果。我们进一步合并了一种迭代DPO算法，以增强推理剂的产生稳定性和质量。根据流行的Llava-Next模型和我们更强大的基础MLLM，我们在需要视觉推理的挑战性多模式基准测试中表现出显着的性能提高。从我们的多代理系统中受益，Insight-V还可以轻松地维护或改善以感知为中心的多模式任务的性能。

### Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation 
[[arxiv](https://arxiv.org/abs/2411.14423)] [[cool](https://papers.cool/arxiv/2411.14423)] [[pdf](https://arxiv.org/pdf/2411.14423)]
> **Authors**: Zhuoman Liu,Weicai Ye,Yan Luximon,Pengfei Wan,Di Zhang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: Homepage: https://zhuomanliu.github.io/PhysFlow/
- **标题**: 释放多模式基础模型的潜力和4D动态物理场景模拟的视频扩散
- **领域**: 计算机视觉和模式识别
- **摘要**: 动态场景的现实模拟需要准确捕获以物理原理为基础的复杂对象相互作用进行建模。但是，现有方法被限制为具有有限可预测参数的基本材料类型，因此不足以代表现实世界材料的复杂性。我们介绍了一种新型方法，该方法利用多模式的基础模型和视频扩散来实现增强的4D动态场景模拟。我们的方法利用多模式模型来识别材料类型并通过图像查询初始化材料参数，同时推断3D高斯夹层以详细的场景表示。我们使用视频扩散使用可区分的材料点法（MPM）和光流导指导进一步完善这些材料参数，而不是渲染损失或得分蒸馏采样（SDS）损失。这个集成的框架可以在现实世界中的动态相互作用进行准确的预测和现实模拟，从而在基于物理学的模拟中提高了准确性和灵活性。

### Multimodal Autoregressive Pre-training of Large Vision Encoders 
[[arxiv](https://arxiv.org/abs/2411.14402)] [[cool](https://papers.cool/arxiv/2411.14402)] [[pdf](https://arxiv.org/pdf/2411.14402)]
> **Authors**: Enrico Fini,Mustafa Shukor,Xiujun Li,Philipp Dufter,Michal Klein,David Haldimann,Sai Aitharaju,Victor Guilherme Turrisi da Costa,Louis Béthune,Zhe Gan,Alexander T Toshev,Marcin Eichner,Moin Nabi,Yinfei Yang,Joshua M. Susskind,Alaaeldin El-Nouby
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: https://github.com/apple/ml-aim
- **标题**: 大型视力编码器的多模式自回旋预训练
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 我们介绍了一种新的方法，用于预训练大型视觉编码器。基于视觉模型自回旋预培训的最新进步，我们将此框架扩展到多模式设置，即图像和文本。在本文中，我们提出了AIMV2，这是一个通才视觉的家族编码，其特征是在一系列下游任务中直接训练过程，可伸缩性和出色的性能。这是通过将视觉编码器与多模式解码器配对的，该解码器可自动加工生成原始图像补丁和文本令牌。我们的编码器不仅在多模式评估中，而且在视觉基准中，例如本地化，接地和分类。值得注意的是，我们的AIMV2-3B编码器在Imagenet-1k上具有89.5％的精度，并使用冷冻的躯干实现。此外，AIMV2始终优于跨不同设置的多模式图像理解中最先进的对比模型（例如，剪辑，siglip）。

### Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding 
[[arxiv](https://arxiv.org/abs/2411.14401)] [[cool](https://papers.cool/arxiv/2411.14401)] [[pdf](https://arxiv.org/pdf/2411.14401)]
> **Authors**: Yiming Zhang,Zhuokai Zhao,Zhaorun Chen,Zenghui Ding,Xianjun Yang,Yining Sun
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 超越训练：动态令牌合并以零拍摄视频理解
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 多模式大语言模型（MLLM）的最新进展开辟了新的途径，以供视频理解。但是，在零拍摄的视频任务中实现高保真度仍然具有挑战性。传统的视频处理方法在很大程度上依赖于微调来捕获细微的时空细节，从而产生了重要的数据和计算成本。相比之下，尽管效率高，但无训练的方法在保留复杂视频内容的上下文功能方面通常缺乏鲁棒性。为此，我们提出了Dyto，这是一个新型的动态令牌合并框架，用于零拍视频理解，可自适应优化令牌效率，同时保留关键的场景细节。 Dyto集成了分层框架的选择和两部分令牌合并策略，以动态群集键框架并有选择地压缩令牌序列，从而达到了与语义丰富度之间的计算效率之间的平衡。跨多个基准测试的广泛实验证明了Dyto的有效性，与微调和无训练方法相比，实现了卓越的性能，并为零拍视频理解设置了新的最新技术。

### Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance 
[[arxiv](https://arxiv.org/abs/2411.14279)] [[cool](https://papers.cool/arxiv/2411.14279)] [[pdf](https://arxiv.org/pdf/2411.14279)]
> **Authors**: Haozhe Zhao,Shuzheng Si,Liang Chen,Yichi Zhang,Maosong Sun,Mingjia Zhang,Baobao Chang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: 19 pages, 12 figures
- **标题**: 超越文本：通过多模式双重注意和软图像指导减少大型视觉模型中的语言偏见
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 大型视觉模型（LVLM）在各种视觉任务中取得了令人印象深刻的结果。然而，尽管表现出令人鼓舞的表现，但LVLMS仍遭受语言偏见引起的幻觉，导致对图像和无效的视觉理解的关注减少。我们确定了这种偏见的两个主要原因：1。在LLM和多模式比对阶段之间的训练数据的不同尺度。 2。由于文本数据的短期依赖性而引起的学习推理偏差。因此，我们提出了鞋带，这是一个旨在解决具有多模式双重注意机制（MDA）和软图像指导（IFG）的LVLM语言偏见的系统性框架。具体而言，MDA引入了平行的双重注意机制，该机制增强了整个模型的视觉输入的整合。 IFG在训练和推理过程中引入了可学习的柔和视觉提示，以替换旨在强迫LVLMS优先考虑文本输入的视觉输入。然后，IFG进一步提出了一种新型的解码策略，使用软视觉提示来减轻模型对相邻文本输入的过度依赖。全面的实验表明，我们的方法可以有效地从其语言偏见中进行LVLM，从而增强视觉理解和减少幻觉，而无需其他培训资源或数据。代码和模型可在[lacing-lvlm.github.io]（https://lacing-lvlm.github.io）上获得。

### FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression 
[[arxiv](https://arxiv.org/abs/2411.14228)] [[cool](https://papers.cool/arxiv/2411.14228)] [[pdf](https://arxiv.org/pdf/2411.14228)]
> **Authors**: Yuke Zhu,Chi Xie,Shuang Liang,Bo Zheng,Sheng Guo
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: FocusLava：一种粗到精细的方法，可高效有效的视觉令牌压缩
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型的最新进展表明，高分辨率图像输入对于模型功能至关重要，尤其是对于细粒度的任务。但是，高分辨率图像导致视力令牌输入到LLM的二次增加，从而导致了巨大的计算成本。当前的工作开发了视觉令牌压缩方法，以提高效率，通常是以牺牲性能为代价。我们认为，消除视觉冗余可以同时提高效率和性能。我们构建了一种粗略的视觉令牌压缩方法，具有视觉引导的采样器，用于压缩冗余区域，并具有低信息密度的冗余区域，以及一个用于选择与用户说明密切相关的视觉令牌的文本引导的采样器。在这两个模块中，拟议的FocusLava actie focusLava act inie focus ins cons in act in contins cosity a acter in cons in actie contins in actie confement in A效率和效率。我们在广泛的评估数据集中验证方法的有效性。

### FoPru: Focal Pruning for Efficient Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.14164)] [[cool](https://papers.cool/arxiv/2411.14164)] [[pdf](https://arxiv.org/pdf/2411.14164)]
> **Authors**: Lei Jiang,Weizhe Huang,Tongxuan Liu,Yuting Zeng,Jing Li,Lechao Cheng,Xiaohua Xu
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: 11 pages, 7 figures
- **标题**: Fopru：有效的大型视力模型的焦点修剪
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉模型（LVLMS）代表了通过启用强大的大型语言模型（LLM）来了解视觉输入来实现卓越多模式能力的重大进步。通常，LVLMS利用视觉编码器（例如夹子）将图像转换为视觉令牌，然后将其与文本令牌通过投影层对准，然后再将其输入到LLM中进行推理。尽管现有的LVLM取得了巨大的成功，但其推论效率仍然受到大量视觉令牌及其潜在冗余的限制。为了减轻此问题，我们提出了局灶性修剪（FOPRU），这是一种无训练的方法，该方法基于从视觉编码器中得出的基于注意力的令牌意义来修剪视觉令牌。具体而言，我们介绍了两种替代修剪策略：1）排名策略，该策略利用所有令牌意义得分在全球视图中保留更关键的代币； 2）行策略，重点是从本地角度保留图像中的连续关键信息。最后，重新排序选定的令牌以维持其原始的位置关系。各种LVLM和多模式数据集的广泛实验表明，我们的方法可以修剪大量冗余令牌，同时保持高精度，从而显着提高推理效率。

### VAGUE: Visual Contexts Clarify Ambiguous Expressions 
[[arxiv](https://arxiv.org/abs/2411.14137)] [[cool](https://papers.cool/arxiv/2411.14137)] [[pdf](https://arxiv.org/pdf/2411.14137)]
> **Authors**: Heejeong Nam,Jinwoo Ahn,Keummin Ka,Jiwan Chung,Youngjae Yu
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: 31 pages
- **标题**: 含糊：视觉上下文阐明了模棱两可的表达
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 人类交流通常依靠视觉提示来解决歧义。尽管人类可以直观地整合这些线索，但AI系统通常会发现参与复杂的多模式推理是挑战。我们介绍了Vague，这是一种评估多模式AI系统将视觉上下文整合以消除歧义的能力的基准测试。模糊由1.6k模棱两可的文本表达式组成，每个表达式与图像和多项选择解释配对，其中正确的答案只有在视觉上下文中才能明显。数据集跨越了上演，复杂的（视觉常识性推理）和自然的个人（EGO4D）场景，以确保多样性。我们的实验表明，现有的多模式AI模型难以推断说话者的真实意图。尽管绩效从引入更多的视觉提示中始终如一地提高，但总体准确性仍然远低于人类的性能，突出了多模式推理的关键差距。对故障案例的分析表明，当前模型无法将真实意图与视觉场景中的表面相关性区分开，这表明它们感知了图像，但并未有效地与之相关。我们在https://github.com/hazel-heejeong-nam/vague.git上发布代码和数据。

### MMGenBench: Fully Automatically Evaluating LMMs from the Text-to-Image Generation Perspective 
[[arxiv](https://arxiv.org/abs/2411.14062)] [[cool](https://papers.cool/arxiv/2411.14062)] [[pdf](https://arxiv.org/pdf/2411.14062)]
> **Authors**: Hailang Huang,Yong Wang,Zixuan Huang,Huaqiu Li,Tongwen Huang,Xiangxiang Chu,Richong Zhang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: This project is available at: https://github.com/lerogo/MMGenBench
- **标题**: MMGENBENCH：完全自动从文本到图像生成的角度自动评估LMM
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 大型多模型（LMM）表现出令人印象深刻的功能。但是，当前的基准主要集中在特定领域的图像理解上，这些基准是劳动密集型的构造。此外，他们的答案往往是简短的，因此很难评估LMM生成图像详细描述的能力。为了解决这些限制，我们提出了MMGENBENCH-PIPELINE，这是一种直接且完全自动化的评估管道。这涉及从输入映像中生成文本描述，使用这些描述通过文本到图像生成模型创建辅助图像，然后比较原始图像和生成的图像。此外，为了确保mmgenbench-Pipeline的有效性，我们设计了mmgenbench检验，评估了13个不同图像模式的LMM，以及MMGERBENCH域，重点是生成图像性能。涉及50多个流行LMM的彻底评估证明了管道和基准的有效性和可靠性。我们的观察结果表明，在现有基准中出色的许多LMM无法充分完成与图像理解和描述相关的基本任务。这一发现突出了当前LMM的性能提高的巨大潜力，并提出了未来模型优化的途径。同时，仅使用图像输入，MMGERBENCH-PIPELIN可以有效地评估不同域的LMM的性能。

### Safety Without Semantic Disruptions: Editing-free Safe Image Generation via Context-preserving Dual Latent Reconstruction 
[[arxiv](https://arxiv.org/abs/2411.13982)] [[cool](https://papers.cool/arxiv/2411.13982)] [[pdf](https://arxiv.org/pdf/2411.13982)]
> **Authors**: Jordan Vice,Naveed Akhtar,Mubarak Shah,Richard Hartley,Ajmal Mian
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: This research is supported by the NISDRG project #20100007, funded by the Australian Government
- **标题**: 没有语义中断的安全性：通过上下文的双重潜在重建来编辑安全的安全图像生成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 培训大型，未经切割的数据集中的多模式生成模型可能会导致用户暴露于有害，不安全和有争议的或有争议的或具有文化上不适当的输出。尽管已提出模型编辑来删除或过滤嵌入和潜在空间中的不良概念，但它可以无意间损坏学习的歧管，从而使概念在近距离近似。我们确定当前模型编辑技术中的局限性，表明即使是良性，近端概念也可能被错位。为了满足对安全内容生成的需求，我们利用安全嵌入和修改的扩散过程，在潜在空间中具有可调加权的求和来生成更安全的图像。我们的方法保留了全球环境，而不会损害学习流形的结构完整性。我们在安全图像生成基准测试基准上实现最新结果，并对模型安全级别提供直观的控制。我们确定安全和审查制度之间的权衡，这在道德AI模型的发展中提出了必要的观点。我们将发布我们的代码。关键字：文本到图像模型，生成AI，安全性，可靠性，模型编辑

### On the Fairness, Diversity and Reliability of Text-to-Image Generative Models 
[[arxiv](https://arxiv.org/abs/2411.13981)] [[cool](https://papers.cool/arxiv/2411.13981)] [[pdf](https://arxiv.org/pdf/2411.13981)]
> **Authors**: Jordan Vice,Naveed Akhtar,Richard Hartley,Ajmal Mian
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: This research is supported by the NISDRG project #20100007, funded by the Australian Government
- **标题**: 关于文本到图像生成模型的公平，多样性和可靠性
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式生成模型的广泛可用性引发了有关其公平性，可靠性和滥用潜力的重要讨论。尽管文本到图像模型可以产生高保真性，用户指导的图像，但它们也表现出不可预测的行为和脆弱性，可以利用这些行为和概念表示。为了解决这个问题，我们提出了一个评估框架，旨在通过对嵌入空间中全球和本地应用的“语义”扰动的响应来评估模型可靠性，从而指出了触发不可靠行为的输入。我们的方法为两个基本方面提供了更深入的见解：（i）生成多样性，评估学习概念的视觉表现广度，以及（ii）生成公平，研究从输入提示中删除概念如何影响语义指导。除了这些评估之外，我们的方法还为检测不可靠的，偏见的模型和偏见出处的检索奠定了基础。我们将发布我们的代码。关键字：公平，可靠性，AI伦理，偏见，文本对图像模型

### Separable Mixture of Low-Rank Adaptation for Continual Visual Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2411.13949)] [[cool](https://papers.cool/arxiv/2411.13949)] [[pdf](https://arxiv.org/pdf/2411.13949)]
> **Authors**: Ziqi Wang,Chang Che,Qi Wang,Yangyang Li,Zenglin Shi,Meng Wang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 低级适应的可分离混合物，用于连续视觉教学调整
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉指示调整（VIT）使多模式大语言模型（MLLM）通过将其作为基于语言的说明来有效地处理各种视觉任务。在此基础上，持续的视觉指导调整（CVIT）扩展了MLLM逐步学习新任务的能力，以适应不断发展的功能。尽管先前的工作通过开发新的基准和减轻灾难性遗忘的方法而提高了CVIT，但这些努力在很大程度上遵循了传统的持续学习范式，忽略了CVIT所带来的独特挑战。我们确定了CVIT中灾难性遗忘的双重形式，MLLM不仅忘记了以前学到的视觉理解，而且还经历了在获得新任务时的教学能力下降。为了解决这个问题，我们介绍了低级适应（Smolora）框架的可分离混合物，该框架通过两个不同的模块采用可分离的路由 - 一个用于视觉理解，另一种用于以下说明。这种双路由设计可在两个域中进行专门改编，从而在提高性能的同时忘记。此外，我们提出了一种新颖的CVIT基准测试，该基准通过评估模型概括地看不见任务并处理各种任务的不同指令的能力，超越了现有基准。广泛的实验表明，Smolora在减轻双重遗忘，改善对看不见的任务的概括以及确保遵循多种说明方面的鲁棒性方面优于现有方法。

### Multimodal 3D Reasoning Segmentation with Complex Scenes 
[[arxiv](https://arxiv.org/abs/2411.13927)] [[cool](https://papers.cool/arxiv/2411.13927)] [[pdf](https://arxiv.org/pdf/2411.13927)]
> **Authors**: Xueying Jiang,Lewei Lu,Ling Shao,Shijian Lu
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 具有复杂场景的多模式3D推理分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式学习的最新发展极大地推动了在体现AI等各种现实世界中的3D场景理解中的研究。但是，大多数现有工作都有两个典型的约束：1）它们缺乏对人类意义的互动和解释的推理能力，2）它们仅关注具有单类别对象的场景，这导致了由于对象之间的多模拟场景和空间关系的疏忽而导致过度简化的文本描述。我们通过为场景中的多个对象提出一个3D推理细分任务来弥合研究差距。该任务允许生成3D分割掩码和详细的文本解释，这些说明丰富了对象之间的3D空间关系。为此，我们创建了Reasonseseg3d，这是一种大规模且高质量的基准，将3D空间关系与生成的问答对和3D分割掩码集成在一起。此外，我们设计了MORE3D，这是一种简单而有效的方法，可以通过用户问题和文本输出进行多对象3D推理细分。广泛的实验表明，MORE3D在推理和细分复杂的多对象3D场景方面表现出色，而创建的ReasonseSegh3D为将来探索3D推理细分提供了一个宝贵的平台。数据集和代码将发布。

### Quantization without Tears 
[[arxiv](https://arxiv.org/abs/2411.13918)] [[cool](https://papers.cool/arxiv/2411.13918)] [[pdf](https://arxiv.org/pdf/2411.13918)]
> **Authors**: Minghao Fu,Hao Yu,Jie Shao,Junjie Zhou,Ke Zhu,Jianxin Wu
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 量化没有泪水
- **领域**: 计算机视觉和模式识别
- **摘要**: 深层神经网络，同时在各种任务中取得了巨大的成功，需要大量资源，包括计算，GPU内存，带宽，存储和能量。网络量化是一种标准压缩和加速技术，可降低存储成本，并通过将网络权重和激活离散为有限的整数值来实现潜在的推理加速度。但是，当前的量化方法通常是复杂且敏感的，需要广泛的特定于任务的超参数，即使单个错误配置也会损害模型性能，从而限制了不同模型和任务的一般性。在本文中，我们提出了量化而没有泪水（QWT），这是一种同时达到量化速度，准确性，简单性和一般性的方法。 QWT的关键见解是将轻巧的附加结构纳入量化网络中，以减轻量化过程中的信息损失。该结构仅由一小部分线性层组成，使方法保持简单有效。更重要的是，它提供了一个封闭形式的解决方案，使我们能够在2分钟以下轻松提高准确性。各种视野，语言和多模式任务进行的广泛实验表明，QWT既高效又有多功能。实际上，我们的方法为网络量化提供了强大的解决方案，结合了简单，准确性和适应性，为新的量化范式设计提供了新的见解。

### Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided Visual Prompts 
[[arxiv](https://arxiv.org/abs/2411.13909)] [[cool](https://papers.cool/arxiv/2411.13909)] [[pdf](https://arxiv.org/pdf/2411.13909)]
> **Authors**: Honglin Li,Yuting Gao,Chenglu Zhu,Jingdong Chen,Ming Yang,Lin Yang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: Panther：用指导引导的视觉提示照亮多模式LLM的视线
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大型语言模型（MLLM）正在迅速缩小与人类视觉感知能力的差距，而在关注微妙的图像细节或精确定位小物体等时仍然落后于落后。很少有研究集中于将文本指导提高到改进视觉表示，从而导致人们对以视觉为中心的任务失去了重点，这是我们在这里称为弱视的现象。在这项工作中，我们介绍了Panther，这是一个密切遵守用户指导的MLLM，并精确地定位了兴趣的目标，并以黑色豹的精致方式定位。具体而言，Panther包括三个积分组件：Panther-VE，Panther-Bridge和Panther-Decoder。 Panther-VE在视觉编码器的早期阶段集成了用户说明信息，从而提取了最相关和最有用的视觉表示。配备有力的过滤功能的Panther-Bridge模块大大降低了冗余的视觉信息，从而节省了大量的培训成本。 Panther-Decoder具有通用性，可以在没有歧视的情况下使用LLM的任何仅解码器架构。实验结果，尤其是以视觉为中心的基准，已经证明了豹的有效性。

### Leverage Task Context for Object Affordance Ranking 
[[arxiv](https://arxiv.org/abs/2411.16082)] [[cool](https://papers.cool/arxiv/2411.16082)] [[pdf](https://arxiv.org/pdf/2411.16082)]
> **Authors**: Haojie Huang,Hongchen Luo,Wei Zhai,Yang Cao,Zheng-Jun Zha
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 利用对象负担排名的任务上下文
- **领域**: 计算机视觉和模式识别
- **摘要**: 智能代理通过根据其负担能力利用各种对象来完成不同的任务，但是如何根据任务上下文选择适当的对象是没有充分探索的。当前的研究将负担能力类别中的对象视为等效的，而忽略该对象在不同的​​任务环境中优先考虑，从而阻碍了复杂环境中准确的决策。为了使代理商能够对执行任务所需的对象有更深入的了解，我们建议将任务上下文作为对象负担排名，即给定一个复杂场景的图像以及负担能力和任务上下文的文本描述，揭示任务对象关系并阐明检测对象的优先级等级。为此，我们提出了一个具有任务关系挖掘模块和图形组更新模块的新颖上下文所用的组排名框架，以深入整合任务上下文并执行全局相对关系传输。由于缺乏此类数据，我们构建了第一个大规模任务的负担能力排名数据集，其中25个常见任务，超过50k的图像和超过661k的对象。实验结果表明，基于任务上下文的负担能力学习范式的可行性以及我们模型比显着排名和多模式对象检测领域的最先进模型的优越性。源代码和数据集将提供给公众。

### Boosting 3D Object Generation through PBR Materials 
[[arxiv](https://arxiv.org/abs/2411.16080)] [[cool](https://papers.cool/arxiv/2411.16080)] [[pdf](https://arxiv.org/pdf/2411.16080)]
> **Authors**: Yitong Wang,Xudong Xu,Li Ma,Haoran Wang,Bo Dai
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: Accepted to SIGGRAPH Asia 2024 Conference Papers
- **标题**: 通过PBR材料增强3D对象生成
- **领域**: 计算机视觉和模式识别,人工智能,图形,机器学习
- **摘要**: 由于其在视频游戏，电影业和AR/VR等各种应用中的潜力，自动3D内容创建最近引起了人们的关注。鉴于单个RGB图像，扩散模型和多模型模型的最新进展显着提高了3D对象产生的质量和效率。但是，与人类创建的资产相比，即使是最先进的方法生成的3D对象仍然不令人满意。考虑仅纹理而不是材料，这些方法在照片真实性的渲染，重新效果和灵活的外观编辑中会遇到挑战。而且它们还遭受了几何和高频纹理细节之间的严重错位。在这项工作中，我们提出了一种新颖的方法，可以从基于物理学的渲染（PBR）材料的角度提高生成的3D对象的质量。通过分析PBR材料的组件，我们选择考虑反照率，粗糙度，金属和凸起图。对于反照率和凸起图，我们利用对合成数据进行微调的稳定扩散来提取这些值，并使用这些微调模型的新颖使用来获得生成的对象的3D一致的反照率和颠簸紫外线。就粗糙度和金属图而言，我们采用半自动过程为互动调整提供了空间，我们认为这是更实用的。广泛的实验表明，我们的模型通常对各种最新的生成方法有益，从而显着提高了其生成的3D对象的质量和现实性，具有自然的重新效果并大大改善了几何形状。

### ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration 
[[arxiv](https://arxiv.org/abs/2411.16044)] [[cool](https://papers.cool/arxiv/2411.16044)] [[pdf](https://arxiv.org/pdf/2411.16044)]
> **Authors**: Haozhan Shen,Kangjia Zhao,Tiancheng Zhao,Ruochen Xu,Zilun Zhang,Mingwei Zhu,Jianwei Yin
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: Zoomeye：通过基于树的图像探索增强具有类似人类的变焦功能的多模式LLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像，尤其是具有高分辨率的图像，通常由许多视觉元素组成，从主导的大物体到细粒的详细对象。当感知此类图像时，多模式的大语言模型〜（MLLM）由于预验证的视觉编码的输入分辨率而面临限制，图像的杂乱无章的密集上下文，导致着眼于主体，同时易于忽略详细的对象。在本文中，我们提出了Zoom Eye，这是一种旨在浏览图像的层次结构和视觉性质以捕获相关信息的树木搜索算法。缩放眼睛将图像概念化为树，每个孩子节点代表父节点的缩放子点，而根表示整体图像。此外，变焦眼是模型不足的且无训练的方法，因此它可以通过沿着图像树从根到叶子节点搜索沿图像树来模拟人类的缩放动作，从而找到相关信息，并准确响应相关查询。我们实验一系列精心的高分辨率基准，结果表明，Zoom Eye不仅可以始终提高利润率较大的串联基本MLLM的性能（例如，Llava-V1.5-7B在$ V^*上增加了34.57 \％，在$ V^*上增加了34.57 \％，而且在HR的较小模型上，也可以在HIR上增加7.88 \），也是7B的MIMM M.M M. GPT-4O。我们的代码可在\ href {https://github.com/om--ai-lab/zoomeye} {https://github.com/om--ai-lab/zoomeye}中获得。

### Bringing the Context Back into Object Recognition, Robustly 
[[arxiv](https://arxiv.org/abs/2411.15933)] [[cool](https://papers.cool/arxiv/2411.15933)] [[pdf](https://arxiv.org/pdf/2411.15933)]
> **Authors**: Klara Janouskova,Cristian Gavrus,Jiri Matas
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 将上下文重新恢复对象识别，稳健
- **领域**: 计算机视觉和模式识别
- **摘要**: 在对象识别中，感兴趣的主题（为简单起见，被称为前景，FG，FG）及其周围环境（背景，BG）都可能起重要作用。但是，标准的监督学习通常会导致对BG的过度依赖，从而限制了现实部署设置中的稳健性。该问题主要是通过抑制BG来解决的，从而牺牲了上下文信息以改善概括。我们建议“本地化稳健”（L2R2），这是一种新颖的识别方法，可以利用上下文感知分类的好处，同时保持分配变化的稳健性。 L2R2利用零摄像检测的进步来识别识别之前将FG定位。它通过有监督的培训来提高标准识别的性能，以及具有VLMS的多模式零照片识别，同时对长尾BGS和分配变化都有强大的态度。结果确认在广泛的数据集可能识别之前确认本地化，它们突出了对象检测其他数据集的限制

### VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding 
[[arxiv](https://arxiv.org/abs/2411.15839)] [[cool](https://papers.cool/arxiv/2411.15839)] [[pdf](https://arxiv.org/pdf/2411.15839)]
> **Authors**: Jiaqi Wang,Yifei Gao,Jitao Sang
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: 15 pages
- **标题**: 有效：通过视觉层融合对比度解码来减轻大视觉语言模型的幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉模型（LVLM）在多模式任务推理中表现出了出色的表现。但是，它们通常会产生看起来合理但不能准确反映视觉内容的响应，这种现象称为幻觉。最近的方法引入了无培训方法，通过在推理阶段调整解码策略来减轻幻觉，通常将幻觉归因于语言模型本身。但是，我们的分析表明，视觉编码过程中的扭曲显着影响模型的推理精度。具体而言，较早的视觉层可能会保留关键特征，但随着信息向输出层传播时，逐渐变形。在这些发现的基础上，我们从视觉编码的角度提出了一种新颖的幻觉缓解方法：\ textbf {v} isu \ textbf {a} l \ textbf {l} ayer fus \ textbf {i}在对比的\ textbf {d} ecoding（有效）上。此方法利用不确定性来指导视觉隐藏层的选择，校正视觉编码过程中的失真，从而提高生成的文本的可靠性。实验结果表明，与多种基线方法相比，有效地有效地降低了各种基准的幻觉，从而实现了最先进的性能。

### Modality Alignment Meets Federated Broadcasting 
[[arxiv](https://arxiv.org/abs/2411.15837)] [[cool](https://papers.cool/arxiv/2411.15837)] [[pdf](https://arxiv.org/pdf/2411.15837)]
> **Authors**: Yuting Ma,Shengeng Tang,Xiaohua Xu,Lechao Cheng
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 模态对齐符合联合广播
- **领域**: 计算机视觉和模式识别
- **摘要**: 联合学习（FL）已成为一种有力的方法，可以通过跨分布式边缘设备进行培训模型而不集中本地数据来保护数据隐私。尽管均质数据方案的进步，但由于数据分配变化降低模型收敛并增加了计算成本，因此在FL中保持全球和本地客户之间的性能仍然具有挑战性。本文介绍了一个新颖的FL框架，利用模式对齐，其中文本编码器位于服务器上，图像编码器在本地设备上操作。受剪辑等多模式学习范式的启发，这种设计通过处理类似于多模式广播的服务器客户通信来使跨客户学习保持一致。我们使用预先训练的模型初始化，以减轻过度拟合，通过低排名适应（LORA）更新选定的参数，以满足计算需求和性能效率。本地模型独立训练并传达了服务器的更新，该服务器通过基于查询的方法汇总了参数，从而促进了在极端异质性下的跨客户知识共享和性能改善。在基准数据集上进行的广泛实验证明了即使在高度异质的环境中，也可以保持概括和鲁棒性的功效。

### Text-Guided Coarse-to-Fine Fusion Network for Robust Remote Sensing Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2411.15770)] [[cool](https://papers.cool/arxiv/2411.15770)] [[pdf](https://arxiv.org/pdf/2411.15770)]
> **Authors**: Zhicheng Zhao,Changfu Zhou,Yu Zhang,Chenglong Li,Xiaoliang Ma,Jin Tang
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 文本指导的粗到融合网络，用于强大的遥感视觉问题回答
- **领域**: 计算机视觉和模式识别
- **摘要**: 遥感视觉问题回答（RSVQA）引起了重大的研究兴趣。但是，当前的RSVQA方法受到光传感器的成像机制的限制，尤其是在诸如云覆盖和弱光场景等挑战性条件下。鉴于合成孔径雷达（SAR）的历史和全天候成像能力，研究光学SAR图像的整合以提高RSVQA性能至关重要。在这项工作中，我们提出了一个文本引导的粗到精细融合网络（TGFNET），该网络利用问题文本和多源图像之间的语义关系指导网络在功能级别上互补融合。具体而言，我们开发了一种文本引导的粗到精细注意力完善（CFAR）模块，以专注于复杂遥感图像中与问题相关的关键领域。该模块通过关键区域路由逐渐将注意力从广泛领域转移到更细节，从而增强了模型专注于相关区域的能力。此外，我们提出了一个自适应多型融合（AMEF）模块，该模块动态整合了不同的专家，从而实现了光学和SAR特征的自适应融合。此外，我们创建了第一个用于评估光学SAR RSVQA方法的大规模基准数据集，其中包括6,008个良好的光学效果图像对和1,036,694个标签良好的问题 - 提问 - 答案对，包括16种不同的问题类型，包括复杂的关系理性问题。对拟议数据集进行的广泛实验表明，我们的TGFNET有效地整合了光学图像和SAR图像之间的互补信息，从而显着改善了该模型在具有挑战性的情况下的性能。该数据集可在以下网址提供：https：//github.com/mmic-lcl/。索引术语：遥感视觉问题回答，多源数据融合，多模式，遥感，OPT-SAR。

### MambaTrack: Exploiting Dual-Enhancement for Night UAV Tracking 
[[arxiv](https://arxiv.org/abs/2411.15761)] [[cool](https://papers.cool/arxiv/2411.15761)] [[pdf](https://arxiv.org/pdf/2411.15761)]
> **Authors**: Chunhui Zhang,Li Liu,Hao Wen,Xi Zhou,Yanfeng Wang
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: Preprint
- **标题**: Mambatrack：利用双重增强夜无人机跟踪
- **领域**: 计算机视觉和模式识别
- **摘要**: 夜间无人机（UAV）跟踪受到较差照明的挑战的阻碍，前日光优化的方法证明了在弱光条件下的次优性能，从而限制了无人机应用程序的实用性。为此，我们提出了一个高效的基于MAMBA的跟踪器，利用双重增强技术来增强夜间无人机跟踪。配备有照明估计器和损坏修复器的基于Mamba的低光增强剂，可实现全球图像增强功能，同时保留低光图像的细节和结构。此外，我们推进了一个跨模式的MAMBA网络，以在视力和语言方式之间实现有效的互动学习。广泛的实验表明，我们的方法可实现高级性能，并显示出显着提高的计算和记忆效率。例如，我们的方法比CiteTracker快2.8 $ \ times $，并减少了50.2 $ \％$ GPU内存。我们的代码可在\ url {https://github.com/983632847/awesome-multimodal-object-tracking}获得。

### Proceedings of the 6th International Workshop on Reading Music Systems 
[[arxiv](https://arxiv.org/abs/2411.15741)] [[cool](https://papers.cool/arxiv/2411.15741)] [[pdf](https://arxiv.org/pdf/2411.15741)]
> **Authors**: Jorge Calvo-Zaragoza,Alexander Pacha,Elona Shatri
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: Proceedings edited by Jorge Calvo-Zaragoza, Alexander Pacha and Elona Shatri
- **标题**: 关于阅读音乐系统的第六届国际研讨会论文集
- **领域**: 计算机视觉和模式识别,信息检索,机器学习
- **摘要**: 关于阅读音乐系统（蠕虫）的国际研讨会是一个研讨会，试图将开发阅读音乐系统的研究人员（例如在光学音乐识别领域）与其他可以从图书馆员或音乐学家等这样的系统中受益的研究人员和从业人员。讲习班感兴趣的相关主题包括但不限于：音乐阅读系统；光学识别；数据集和绩效评估；音乐分数的图像处理；作者身份证明；音乐分数的创作，编辑，存储和演示系统；多模式系统；音乐创作音乐的新颖输入方法；基于Web的音乐信息检索服务；申请和项目；与书面音乐有关的用例。这些是2024年11月22日在线举行的有关阅读音乐系统的第六届国际研讨会的会议记录。

### AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea 
[[arxiv](https://arxiv.org/abs/2411.15738)] [[cool](https://papers.cool/arxiv/2411.15738)] [[pdf](https://arxiv.org/pdf/2411.15738)]
> **Authors**: Qifan Yu,Wei Chow,Zhongqi Yue,Kaihang Pan,Yang Wu,Xiaoyang Wan,Juncheng Li,Siliang Tang,Hanwang Zhang,Yueting Zhuang
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: 41 pages, 24 figures
- **标题**: AnyEdit：掌握统一的高质量图像编辑
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于指令的图像编辑旨在通过自然语言说明修改特定的图像元素。但是，该域中的当前模型通常难以准确执行复杂的用户说明，因为它们接受了具有有限编辑类型的低质量数据培训。我们提出了AnyEdit，这是一种全面的多模式指令编辑数据集，包括250万个高质量编辑对，涵盖了20多种编辑类型和五个域。我们通过三个方面确保AnyEdit收集的多样性和质量：初始数据多样性，自适应编辑过程以及自动选择编辑结果。使用数据集，我们进一步训练了一个新颖的AnyEdit稳定扩散，并通过任务感知路由和可学习的任务嵌入统一图像编辑。三个基准数据集的全面实验表明，AnyEdit始终提高基于扩散的编辑模型的性能。这为开发以教学驱动的图像编辑模型的方式介绍了支持人类创造力的模型。

### Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks 
[[arxiv](https://arxiv.org/abs/2411.15720)] [[cool](https://papers.cool/arxiv/2411.15720)] [[pdf](https://arxiv.org/pdf/2411.15720)]
> **Authors**: Peng Xie,Yequan Bie,Jianda Mao,Yangqiu Song,Yang Wang,Hao Chen,Kani Chen
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 攻击链：基于视觉模型的鲁棒性，以防止基于转移的对抗攻击
- **领域**: 计算机视觉和模式识别
- **摘要**: 预训练的视觉模型（VLM）在图像和自然语言理解中表现出了显着的表现，例如图像字幕和响应产生。随着视觉模型的实际应用越来越普遍，它们的潜在安全性和鲁棒性问题引起了人们对对手可能逃避系统并导致这些模型通过恶意攻击产生有毒内容的担忧。因此，评估开源VLM对对抗攻击的鲁棒性已引起人们的注意，基于转移的攻击是代表性的黑盒攻击策略。但是，大多数现有基于转移的攻击忽略了视觉和文本方式之间语义相关性的重要性，从而导致了次优的对抗性示例生成和攻击性能。为了解决这个问题，我们提出了攻击链（COA），它使用一系列中间攻击步骤迭代地增强了基于多模式语义更新的对抗性示例的产生，从而实现了卓越的对抗性可传递性和效率。进一步提出了一种统一的攻击成功计算方法，以自动逃避评估。在最现实和高风险的情况下进行的广泛实验表明，我们的攻击策略可以有效地误导模型，以仅使用黑盒攻击而又不了解受害者模型就可以生成目标响应。我们的论文中的全面鲁棒性评估提供了对VLMS脆弱性的见解，并为未来模型发展的安全考虑提供了参考。

### Knowledge Transfer Across Modalities with Natural Language Supervision 
[[arxiv](https://arxiv.org/abs/2411.15611)] [[cool](https://papers.cool/arxiv/2411.15611)] [[pdf](https://arxiv.org/pdf/2411.15611)]
> **Authors**: Carlo Alberto Barbano,Luca Molinaro,Emanuele Aiello,Marco Grangetto
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: 21 pages, 7 figures, 17 tables
- **标题**: 通过自然语言监督跨模式的知识转移
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提供了一种仅使用其文本描述来学习新颖概念的方法。我们称此方法知识转移。与人类的看法类似，我们利用跨模式相互作用来引入新概念。我们假设在预先训练的视觉编码器中，已经学到了足够的低级特征（例如形状，外观，颜色），可用于描述以前未知的高级概念。我们的方法带有对新颖概念的文本描述，它通过将可视编码器的已知低级特征与其高级文本描述相提并论。我们表明，知识转移可以通过仅需要对目标概念的单个描述来成功地以非常有效的方式以非常有效的方式引入新颖概念。我们的方法与单独的文本编码器（例如剪辑）和跨模态共享参数兼容。我们还表明，遵循相同的原则，知识转移可以改善模型已经知道的概念。利用知识转移，我们改善了各个任务（例如分类，分割，图像文本检索和字幕）的零拍摄性能。

### How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking 
[[arxiv](https://arxiv.org/abs/2411.15600)] [[cool](https://papers.cool/arxiv/2411.15600)] [[pdf](https://arxiv.org/pdf/2411.15600)]
> **Authors**: Xuchen Li,Shiyu Hu,Xiaokun Feng,Dailing Zhang,Meiqi Wu,Jing Zhang,Kaiqi Huang
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: Preprint, Under Review
- **标题**: 文本如何帮助？精细的评估，以揭示语言在视觉跟踪中的作用
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: Vision语言跟踪（VLT）通过合并文本信息来扩展传统的单个对象跟踪，提供语义指导，以在诸如快速运动和变形之类的挑战条件下增强跟踪性能。但是，与多个基准上的单模式方法相比，当前的VLT跟踪器通常表现不佳，语义信息有时会成为“分心”。为了解决这个问题，我们提出了VLTverse，这是VLT跟踪器的第一个细粒度评估框架，该框架全面考虑了多个挑战因素和不同的语义信息，希望揭示语言在VLT中的作用。我们的贡献包括：（1）VLTVERSE介绍了10个序列级别挑战标签和6种类型的多粒性语义信息，为VLT创建了灵活的多维评估空间； （2）利用挑战因素和语义类型的组合形成的60个子空间，我们对三个主流SOTA VLT跟踪器进行了系统的细粒度评估，从而在复杂的场景中揭示了它们的性能瓶颈，并为VLT评估提供了新的观点； （3）通过对实验结果的分析分析，我们研究了各种语义类型对与不同算法相关的特定挑战因素的影响，从而为增强跨数据，评估和算法维度增强VLT的基本指南提供了指导。 VLTVERSE，工具包和结果将在\ url {http://metaverse.aitestunion.com}上获得。

### Improving Factuality of 3D Brain MRI Report Generation with Paired Image-domain Retrieval and Text-domain Augmentation 
[[arxiv](https://arxiv.org/abs/2411.15490)] [[cool](https://papers.cool/arxiv/2411.15490)] [[pdf](https://arxiv.org/pdf/2411.15490)]
> **Authors**: Junhyeok Lee,Yujin Oh,Dahyoun Lee,Hyon Keun Joh,Chul-Ho Sohn,Sung Hyun Baik,Cheol Kyu Jung,Jung Hyun Park,Kyu Sung Choi,Byung-Hoon Kim,Jong Chul Ye
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 通过配对的图像域检索和文本域增强，改善3D Brain MRI报告生成的事实
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: 急性缺血性中风（AIS）需要时间关键治疗，需要数小时的干预措施，导致患者不可逆地残疾。由于使用磁共振图像（MRI）扩散加权成像（DWI）在检测AIS中起着至关重要的作用，因此从DWI对AIS的自动预测一直是临床重要性的研究主题。虽然文本放射学报告包含图像发现中最相关的临床信息，但跨不同模式绘制的难度限制了传统直接DWI到报告生成方法的事实。在这里，我们提出了配对的图像域检索和文本域增强（PIRTA），这是一种跨模式检索型生成（RAG）框架，用于提供临床医生的促进AIS放射学报告，并改善了实际的实际情况。 PIRTA减轻了学习跨模式映射的需求，这在图像到文本生成方面构成了困难，这是通过将跨模式映射问题抛弃，作为对已经配对地面真实文本放射学报告的类似DWI图像的内域检索。通过利用检索到的放射学报告来增强查询图像的报告生成过程，我们通过具有广泛内部和公共数据集的实验显示PIRTA可以准确地从3D DWI图像中检索相关报告。与使用最先进的多模式模型的直接图像到文本生成相比，这种方法能够产生具有明显精度的放射学报告。

### MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking 
[[arxiv](https://arxiv.org/abs/2411.15459)] [[cool](https://papers.cool/arxiv/2411.15459)] [[pdf](https://arxiv.org/pdf/2411.15459)]
> **Authors**: Xinqi Liu,Li Zhou,Zikun Zhou,Jianqiu Chen,Zhenyu He
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: Mambavlt：视觉跟踪的时间不断变化的多模式状态空间模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉跟踪任务旨在根据各种模态参考执行对象跟踪。现有的基于变压器的视觉跟踪方法通过利用自我注意的全球建模能力取得了显着的进步。但是，当前的方法在有效利用时间信息并在跟踪过程中动态更新参考功能时仍面临挑战。最近，国家空间模型（SSM）（称为Mamba）在有效的长期建模方面表现出惊人的能力。尤其是，其状态空间不断发展的过程表明了具有线性复杂性的多模式时间信息的有希望的能力。目睹了它的成功，我们提出了一个基于Mamba的视觉跟踪模型，以利用其在时间空间中的状态空间不断发展的能力，用于强大的多模式跟踪，称为Mambavlt。特别是，我们的方法主要集成了随时间不断发展的混合状态空间块和选择性的局部性增强块，以捕获多模式建模和自适应参考功能更新的上下文信息。此外，我们引入了一个模态选择模块，该模块可以动态调整视觉和语言参考之间的权重，从而减轻任何参考类型的潜在歧义。广泛的实验结果表明，我们的方法对跨不同基准的最新跟踪器有利。

### Enhancing Instruction-Following Capability of Visual-Language Models by Reducing Image Redundancy 
[[arxiv](https://arxiv.org/abs/2411.15453)] [[cool](https://papers.cool/arxiv/2411.15453)] [[pdf](https://arxiv.org/pdf/2411.15453)]
> **Authors**: Te Yang,Jian Jia,Xiangyu Zhu,Weisong Zhao,Bo Wang,Yanhua Cheng,Yan Li,Shengyuan Liu,Quan Chen,Peng Jiang,Kun Gai,Zhen Lei
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 通过降低图像冗余来增强视觉语言模型的指导遵循能力
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型语言模型（LLMS）具有强大的指导遵循能力，可以按照人类命令指示解释和执行任务。与LLM相比，多模式大型语言模型（MLLM）具有较低的指令遵循能力。但是，MLLM和LLM之间的指令跟踪功能存在很大的差距。在这项研究中，我们进行了一个试验实验，该实验表明，空间下降的视觉令牌可显着增强MLLM的指导跟踪能力。这归因于视觉方式的实质性冗余。但是，这种直观的方法严重损害了MLLM的多模式理解能力。在本文中，我们提出了视觉模式令牌压缩（VMTC）和交叉模式的关注抑制（CMAI）策略，以减轻MLLM和LLMS之间的差距，通过抑制与内容产生过程中无关的视觉令牌的影响，增强MLLM的指导能力，同时保持其多模型理解的能力。在VMTC模块中，保留主要令牌，并通过令牌群集和合并来凝结冗余令牌。在CMAI过程中，我们通过文本到文本的注意力汇总文本到图像的关注，以获得文本对象焦点分数。注意抑制是在得分低的文本图像令牌对上进行的。我们对遵循指导能力的全面实验以及VQA-V2，GQA，TextVQA，MME和MMBENCH五个基准测试表明，提出的策略可显着增强MLLM的能力后的指导，同时保留理解和处理多模态输入的能力。

### freePruner: A Training-free Approach for Large Multimodal Model Acceleration 
[[arxiv](https://arxiv.org/abs/2411.15446)] [[cool](https://papers.cool/arxiv/2411.15446)] [[pdf](https://arxiv.org/pdf/2411.15446)]
> **Authors**: Bingxin Xu,Yuzhang Shang,Yunhao Ge,Qian Lou,Yan Yan
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: FreePruner：大型多模型加速的无训练方法
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型多模型模型（LMM）在视觉任务中表现出了令人印象深刻的功能，但由于其高计算需求，面临着重大部署挑战。虽然最近的降低方法显示出加速LMM的希望，但它们通常需要大量的重新训练或微调，这对于许多最先进的模型，尤其是具有专有培训数据的模型，这使得它们是不切实际的。我们提出了FreePruner，这是一种无训练的令牌减少方法，可以直接应用于任何开源LMM，而无需额外的培训。与现有的依赖令牌合并操作的现有方法不同，FreePruner采用了两阶段的代币选择策略：（1）识别使用我们设计的贡献学位度量的高级语义信息捕获高级语义信息的关键令牌，并且（2）通过注意通过注意力模式分析来保存基本的低级视觉详细信息。广泛的实验表明，FreePruner在无训练环境中保持主流视觉问题的基准测试基准，同时保持了2倍的加速度。此外，FreePruner是正交的，并且可以与其他训练后加速技术（例如训练后量化）结合使用，为有效的LMM部署提供了实用的解决方案。

### ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head Avatar with Temporal Guidance 
[[arxiv](https://arxiv.org/abs/2411.15436)] [[cool](https://papers.cool/arxiv/2411.15436)] [[pdf](https://arxiv.org/pdf/2411.15436)]
> **Authors**: Haijie Yang,Zhenyu Zhang,Hao Tang,Jianjun Qian,Jian Yang
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 一致的瓦塔塔尔：学会通过时间指导扩散完全一致的说话头像
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散模型在说话的脑海中显示出令人印象深刻的潜力。尽管实现了合理的外观和说话效果，但由于误差积累和单图生成能力的固有限制，这些方法仍然遭受时间，3D或表达不一致。在本文中，我们提出了一致的，这是一个新颖的框架，用于完全一致且高保真的化身产生。我们的方法没有直接在扩散过程中使用多模式条件，而是学会了首先建模相邻帧之间稳定性的时间表示。具体而言，我们提出了一个对时间敏感的细节（TSD）图，其中包含高频特征和轮廓沿时间轴差异很大。使用时间一致的扩散模块，我们学会将初始结果的TSD与视频框架接地真相相结合。最终的头像是由完全一致的扩散模块生成的，该模块在对齐的TSD，粗糙的头部正常和情绪及时嵌入。我们发现代表时间模式的对齐的TSD限制了扩散过程，以生成时间稳定的说话头。此外，其可靠的指导补充了其他条件的不准确性，抑制了累积错误，同时提高了各个方面的一致性。广泛的实验表明，一致的瓦尔塔尔在生成的外观，3D，表达和时间一致性上的最新方法优于最新方法。项目页面：https：//njust-yang.github.io/consistentavatar.github.io/

### What Makes a Scene ? Scene Graph-based Evaluation and Feedback for Controllable Generation 
[[arxiv](https://arxiv.org/abs/2411.15435)] [[cool](https://papers.cool/arxiv/2411.15435)] [[pdf](https://arxiv.org/pdf/2411.15435)]
> **Authors**: Zuyao Chen,Jinlin Wu,Zhen Lei,Chang Wen Chen
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 是什么使场景？基于场景图的评估和可控生成的反馈
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管已经对文本形象生成进行了广泛的研究，但从场景图生成图像仍然相对不受欢迎，这主要是由于准确建模空间关系和对象相互作用的挑战。为了填补这一空白，我们介绍了场景基础，这是一个综合基准，旨在评估和增强生成自然场景的事实一致性。场景基础包括Megasg，这是一个大规模的数据集，其中包含带有场景图的一百万张图像，从而促进了跨不同和复杂场景的模型的训练和公平比较。此外，我们提出了一种新型的评估指标SGSCORE，它利用多模式大语言模型（LLMS）的经过经过经过经过经过经过经验的推理能力来评估对象的存在和关系的准确性，从而比FID和ClipsCore（例如FID和ClipsCore）更有效地衡量了事实一致性。在此评估框架的基础上，我们开发了一个场景图形反馈管道，该管道通过识别和纠正场景图和图像之间的差异来迭代地完善生成的图像。广泛的实验表明，与现有基准相比，场景基础台面提供了更全面，更有效的评估框架，尤其是对于复杂的场景生成而言。此外，我们的反馈策略大大提高了图像生成模型的事实一致性，从而推进了可控图像生成的领域。

### FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity 
[[arxiv](https://arxiv.org/abs/2411.15411)] [[cool](https://papers.cool/arxiv/2411.15411)] [[pdf](https://arxiv.org/pdf/2411.15411)]
> **Authors**: Hang Hua,Qing Liu,Lingzhi Zhang,Jing Shi,Zhifei Zhang,Yilin Wang,Jianming Zhang,Jiebo Luo
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: Preprint
- **标题**: 罚款：组成图像字幕将重点放在您想要的任何粒度上
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉模型（VLM）的出现具有显着高级的多模式任务，可以在各种应用程序中更加复杂和准确的推理，包括图像和视频字幕，视觉询问答案以及跨模式检索。尽管具有出色的能力，但VLM与细粒度图像区域组成信息感知斗争。具体而言，它们很难准确地使分割掩模与相应的语义对齐，并精确地描述了引用区域的组成方面。但是，组成性 - 理解和生成已知视觉和文本组件的新型组合的能力 - 对于促进VLM跨模态的相干推理和理解至关重要。为了解决这个问题，我们提出了finecaption，这是一种新型的VLM，可以将任意掩码识别为参考输入和过程高分辨率图像，以在不同粒度级别上为组成图像字幕字幕。为了支持这项努力，我们引入了CompositionCap，这是一种用于多元区域组成图像字幕的新数据集，该数据集介绍了组成属性吸引的区域图像字幕的任务。经验结果证明了与其他最先进的VLM相比，我们提出的模型的有效性。此外，我们分析了当前VLM在识别各种视觉提示的组成区域图像字幕方面的功能，突出了改进VLM设计和培训的区域。

### A Constrast-Agnostic Method for Ultra-High Resolution Claustrum Segmentation 
[[arxiv](https://arxiv.org/abs/2411.15388)] [[cool](https://papers.cool/arxiv/2411.15388)] [[pdf](https://arxiv.org/pdf/2411.15388)]
> **Authors**: Chiara Mauri,Ryan Fritz,Jocelyn Mora,Benjamin Billot,Juan Eugenio Iglesias,Koen Van Leemput,Jean Augustinack,Douglas N Greve
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 14 pages, 10 figures, 3 tables
- **标题**: 超高分辨率claustrum分割的一种约束 - 敏捷方法
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: Claustrum是位于壳质和岛脉之间的带状灰质结构，其确切功能仍在积极研究中。它的板状结构使其在典型的分辨率和研究的神经成像工具（包括自动分割的方法）中几乎看不见，目前非常有限。在本文中，我们提出了一种在超高分辨率（0.35毫米各向同性）下进行claustrum分割的对比和分辨率的方法。该方法基于合成分割框架（Billot等，2023），该框架利用合成训练强度图像来实现出色的概括。特别是，由于相应的强度图像是通过随机对比和分辨率合成的，因此合成的图像仅需要训练标签图。我们使用了从18种超高分辨率MRI扫描（主要是离体）获得的Claustrum手动标签培训了一个自动隔离网络的深度学习网络。我们证明了使用这18个高分辨率案例（骰子得分= 0.632，平均表面距离= 0.458 mm，使用6倍交叉验证（CV））以及在典型的分辨率（〜1 mm同位素）下进行体内T1加权MRI扫描。我们还证明了该方法在重测设置中以及应用于多模式成像时（T2加权，质子密度和定量T1扫描）时具有鲁棒性。据我们所知，这是自动超高分辨率分割的第一种准确方法，这对于对比度和分辨率的变化是可靠的。该方法可在https://github.com/chiara-mauri/claustrum_segnementation上发布，并作为Neurotimaging软件包Freesurfer的一部分（Fischl，2012）。

### MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2411.15296)] [[cool](https://papers.cool/arxiv/2411.15296)] [[pdf](https://arxiv.org/pdf/2411.15296)]
> **Authors**: Chaoyou Fu,Yi-Fan Zhang,Shukang Yin,Bo Li,Xinyu Fang,Sirui Zhao,Haodong Duan,Xing Sun,Ziwei Liu,Liang Wang,Caifeng Shan,Ran He
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: Produced by MME+MMBench+LLaVA Teams. Project Page: https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks
- **标题**: mme-survey：一项关于多模式LLMS评估的综合调查
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 作为人工智能（AGI）的重要方向，多模式的大语言模型（MLLM）吸引了行业和学术界的关注。在预先训练的LLM的基础上，这个模型家族进一步发展了令人印象深刻的多模式感知和推理能力，例如给出流程图或基于图像创建故事的编写代码。在开发过程中，评估至关重要，因为它为改进模型提供了直观的反馈和指导。不同于传统的火车 - 测试范式，它仅倾向于单个任务，例如图像分类，MLLM的多功能性刺激了各种新基准和评估方法的兴起。在本文中，我们旨在介绍MLLM评估的全面调查，讨论四个关键方面：1）汇总的基准类型除以评估能力，包括基础能力，模型自我分析和范围应用； 2）基准集团的典型过程，包括数据收集，注释和预防措施； 3）由法官，公制和工具包组成的系统评估方式； 4）下一个基准测试的前景。这项工作旨在为研究人员轻松掌握如何根据不同的需求有效评估MLLM并激发更好的评估方法，从而推动MLLM研究的进步。

### Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training 
[[arxiv](https://arxiv.org/abs/2411.15207)] [[cool](https://papers.cool/arxiv/2411.15207)] [[pdf](https://arxiv.org/pdf/2411.15207)]
> **Authors**: Ameera Bawazir,Kebin Wu,Wenbin Li
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-25
> **comment**: 15 pages, 2 figures, accepted by BMVC'24
- **标题**: UNI-MLIP：统一的医学视觉语言预训练的自我训练
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 通过对比度学习，视觉预训练的最新进步已大大改善了计算机视觉任务的性能。但是，在医疗领域中，由于隐私，灵敏度和注释复杂性，获得多模式数据通常是昂贵且具有挑战性的。为了减轻数据稀缺性，在提高模型性能的同时，我们引入了\ textbf {uni-mlip}，这是一个专门设计用于增强医学视觉语言预训练的统一自学框架。 Uni-Mlip无缝集成了数据级和功能级别的交叉模式，Uni-Mododity和融合模式的自我设计技术。此外，Uni-Mlip裁缝Uni-Modal Image自学图像可容纳医学图像的独特特征。我们跨不同尺度数据集的实验表明，在下游三个关键任务中，Uni-MLIP显着超过了当前的最新方法：图像文本检索，图像分类和视觉询问答案（VQA）。

### Beyond Visual Understanding: Introducing PARROT-360V for Vision Language Model Benchmarking 
[[arxiv](https://arxiv.org/abs/2411.15201)] [[cool](https://papers.cool/arxiv/2411.15201)] [[pdf](https://arxiv.org/pdf/2411.15201)]
> **Authors**: Harsha Vardhan Khurdula,Basem Rizk,Indus Khaitan,Janit Anjaria,Aviral Srivastava,Rajvardhan Khaitan
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-25
> **comment**: 7 pages, 4 figures, Accepted at COLING 2025
- **标题**: 超越视觉理解：引入视觉语言模型基准的鹦鹉-360V
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 当前用于评估视觉语言模型（VLM）的基准通常在彻底评估模型能力以理解和处理复杂的视觉和文本内容的能力方面缺乏。他们通常专注于不需要深层推理或多种数据模式的简单任务来解决原始问题。为了解决这一差距，我们介绍了Parrot-360V基准，这是一种新颖而全面的基准，具有2487个具有挑战性的视觉难题，旨在测试复杂的视觉推理任务。我们使用Parrot-360V评估了领先的模型：GPT-4O，Claude-3.5-Sonnet和Gemini-1.5-Pro，评估其在视觉线索与语言技能相结合的能力，以类似于人类问题解决的方式解决任务。我们的发现表明了一个显着的性能差距：最先进的模型在我们的基准测试中得分在28至56个百分点之间，大大低于其在流行基准测试中的性能。这强调了当前VLM在处理复合物，多步推理任务中的局限性，并突出了需要更强大的评估框架以推进该领域的需求。

### Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion 
[[arxiv](https://arxiv.org/abs/2411.15113)] [[cool](https://papers.cool/arxiv/2411.15113)] [[pdf](https://arxiv.org/pdf/2411.15113)]
> **Authors**: Samarth N Ramesh,Zhixue Zhao
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 文本到图像模型的有效修剪：从修剪稳定扩散的见解
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 随着文本到图像模型的增长越来越强大和复杂，它们的迅速繁殖尺寸为广泛采用的重要障碍，尤其是在资源受限的设备上。本文介绍了一项关于稳定扩散后训练后修剪2的开创性研究，涉及文本到图像域中模型压缩的关键需求。我们的研究针对以前未开发的多模式生成模型解决了修剪技术，并特别研究对文本组件和图像生成组件的修剪影响。我们在各种稀疏性中对模型或模型的单个组成部分进行了全面比较。我们的结果产生了以前没有证件的发现。例如，与语言模型修剪的既定趋势相反，我们发现简单的修剪在文本到图像上下文中的表现优于更先进的技术。此外，我们的结果表明，稳定的扩散2可以修剪至38.5％的稀疏性，而质量损失最小，从而显着降低了模型尺寸。我们提出了一种最佳的修剪配置，将文本编码器修剪为47.5％，扩散发生器为35％。该配置保持图像生成质量，同时大大降低了计算要求。此外，我们的工作还发现了有关文本到图像模型中信息编码的有趣问题：我们观察到，修剪超出某些阈值会导致性能下降（不可读的图像），这表明特定权重编码关键语义信息。这一发现为在文本对图像模型中的模型压缩，互操作性和偏置识别方面的未来研究开辟了新的途径。通过提供对文本模型修剪行为的关键见解，我们的研究为开发更高效且易于访问的AI驱动图像生成系统奠定了基础

### About Time: Advances, Challenges, and Outlooks of Action Understanding 
[[arxiv](https://arxiv.org/abs/2411.15106)] [[cool](https://papers.cool/arxiv/2411.15106)] [[pdf](https://arxiv.org/pdf/2411.15106)]
> **Authors**: Alexandros Stergiou,Ronald Poppe
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 关于时间：进步，挑战和行动理解的观点
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 我们目睹了视频动作理解的令人印象深刻的进步。增加数据集大小，可变性和计算可用性已使性能和任务多样化的跨越。当前的系统可以提供视频场景的粗粒和细粒度描述，提取与查询相对应的段，合成视频的未观察到的部分以及预测上下文。这项调查全面审查了整个任务跨多型和多模式行动理解的进步。我们专注于普遍的挑战，概述广泛采用的数据集和调查开创性工作，重点是最新进展。我们广泛区分了三个时间范围：（1）完整观察到的动作的识别任务，（2）预测持续的部分观察到的动作的预测任务，以及（3）预测随后未观察到的动作的任务。该部门使我们能够确定特定的操作建模和视频表示挑战。最后，我们概述了未来解决当前缺点的指示。

### Context-Aware Multimodal Pretraining 
[[arxiv](https://arxiv.org/abs/2411.15099)] [[cool](https://papers.cool/arxiv/2411.15099)] [[pdf](https://arxiv.org/pdf/2411.15099)]
> **Authors**: Karsten Roth,Zeynep Akata,Dima Damen,Ivana Balažević,Olivier J. Hénaff
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 上下文感知的多模式预处理
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 大规模的多模式表示学习在测试时成功优化了零射击传输。然而，标准的预处理范式（大量图像文本数据上的对比度学习）并不能明确鼓励表示形式支持几次适应。在这项工作中，我们提出了一个简单但经过精心设计的多模式预处理的扩展，使表示能够适应其他背景。使用这个目标，我们表明可以训练视觉模型，以显着提高几次适应性：在21个下游任务中，我们发现测试时间样品效率的提高了四倍，平均几次适应性增长超过5％，同时在模型尺度和训练范围内保持了零发出的零孔概括性能。尤其是配备了简单，无训练，基于指标的适应机制，我们的表示很容易超越更复杂和昂贵的基于优化的方案，从而大大简化了对新领域的概括。

### HeadRouter: A Training-free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads 
[[arxiv](https://arxiv.org/abs/2411.15034)] [[cool](https://papers.cool/arxiv/2411.15034)] [[pdf](https://arxiv.org/pdf/2411.15034)]
> **Authors**: Yu Xu,Fan Tang,Juan Cao,Yuxin Zhang,Xiaoyu Kong,Jintao Li,Oliver Deussen,Tong-Yee Lee
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 头疗程：通过适应性的注意力头将MM-DIT的无训练图像编辑框架
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 扩散变压器（DIT）在图像生成任务中表现出强大的功能。但是，多模式DIT（MM-DITS）的准确文本引导的图像编辑仍然带来重大挑战。与可以利用自我/跨注意图进行语义编辑的基于UNET的结构不同，MM-dits固有地缺乏对明确且一致的合并文本指导的支持，从而导致编辑的结果和文本之间的语义不对对准。在这项研究中，我们揭示了不同注意力头对MM-Dits中不同图像语义的敏感性，并引入了HeadRouter，这是一个无训练的图像编辑框架，该框架通过将文本指导与MM-Dits中的不同注意力头进行自适应将源图像进行编辑。此外，我们提出了一个双重细化模块，以完善文本/图像令牌表示形式，以进行精确的语义指导和准确的区域表达。多个基准测试的实验结果表明，在编辑保真度和图像质量方面，头脑的表现。

### MSSF: A 4D Radar and Camera Fusion Framework With Multi-Stage Sampling for 3D Object Detection in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2411.15016)] [[cool](https://papers.cool/arxiv/2411.15016)] [[pdf](https://arxiv.org/pdf/2411.15016)]
> **Authors**: Hongsi Liu,Jun Liu,Guangfeng Jiang,Xin Jin
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: MSSF：一个4D雷达和摄像机融合框架，具有多个阶段抽样的3D对象检测
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 作为近年来出现的汽车传感器之一，4d毫米波雷达的分辨率高于常规3D雷达，并提供了精确的高程测量。但是它的点云仍然稀疏且嘈杂，这使得满足自动驾驶的要求具有挑战性。作为另一个常用传感器，相机可以捕获丰富的语义信息。结果，4D雷达和相机的融合可以为自动驾驶系统提供负担得起且可靠的感知解决方案。但是，与基于激光雷达的方法相比，尚未对以前的雷达相机融合方法进行彻底研究，从而产生了较大的性能差距。具体而言，他们忽略了特征性问题，并且不会与图像语义信息进行深入互动。为此，我们提出了一个基于4D雷达和相机的简单但有效的多阶段采样融合（MSSF）网络。一方面，我们设计了一个融合块，该融合块可以与图像特征深入相互作用的点云特征，并且可以以插件的方式应用于常用的单模式骨干。融合块包括两种类型，即简单特征融合（SFF）和多尺度可变形特征融合（MSDFF）。 SFF易于实现，而MSDFF具有更强的融合能力。另一方面，我们提出了一个语义引导的头部，以通过体素特征重新加权对体素进行前景分割，从而进一步减轻了特征模糊的问题。关于DEFELT（VOD）和TJ4DRADSET数据集的广泛实验证明了我们的MSSF的有效性。值得注意的是，与最先进的方法相比，MSSF在VOD和TJ4DRADSET数据集上的3D平均精度分别提高了7.0％和4.0％。它甚至超过了VOD数据集上的基于经典的LiDAR方法。

### Large Multi-modal Models Can Interpret Features in Large Multi-modal Models 
[[arxiv](https://arxiv.org/abs/2411.14982)] [[cool](https://papers.cool/arxiv/2411.14982)] [[pdf](https://arxiv.org/pdf/2411.14982)]
> **Authors**: Kaichen Zhang,Yifei Shen,Bo Li,Ziwei Liu
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 大型多模型模型可以解释大型多模式模型中的功能
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 大型多模型模型（LMM）的最新进展导致了学术界和工业的重大突破。出现的一个问题是，作为人类，我们如何理解他们的内部神经表现形式。本文通过提出一个多功能框架来识别和解释LMMS中的语义，迈出了第一步。具体来说，1）我们首先应用稀疏的自动编码器（SAE）将表示形式分解为人类可理解的特征。 2）然后，我们提出一个自动解释框架，以解释LMM自己在SAE中学到的开放语义特征。我们使用该框架使用LLAVA-OV-72B模型来分析LLAVA-NEXT-8B模型，以证明这些功能可以有效地引导模型的行为。我们的结果有助于更深入地了解为什么LMM在包括EQ测试在内的特定任务中表现出色，并阐明其错误的性质以及潜在的纠正策略。这些发现为LMM的内部机制提供了新的见解，并暗示了与人脑认知过程的相似之处。

### Design-o-meter: Towards Evaluating and Refining Graphic Designs 
[[arxiv](https://arxiv.org/abs/2411.14959)] [[cool](https://papers.cool/arxiv/2411.14959)] [[pdf](https://arxiv.org/pdf/2411.14959)]
> **Authors**: Sahil Goyal,Abhinav Mahajan,Swasti Mishra,Prateksha Udhayanan,Tripti Shukla,K J Joseph,Balaji Vasan Srinivasan
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: Accepted to WACV 2025. Project page: https://sahilg06.github.io/Design-o-meter/
- **标题**: 设计 - 米：旨在评估和完善图形设计
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **摘要**: 图形设计是视觉交流的有效媒介。它们的范围从贺卡到公司传单及以后。离线，机器学习技术能够生成此类设计，从而加速了内容生产速率。评估其质量的自动化方式变得至关重要。为此，我们介绍了Design-O-Meter，这是一种数据驱动的方法，以量化图形设计的优点。此外，我们的方法可以建议对这些设计进行修改，以提高其视觉吸引力。据我们所知，尽管设置的固有的主观性和模棱两可，但Design-O-Meter是在统一框架中得分和完善设计的第一种方法。我们对适合该任务的基线的方法（包括最近的多模式LLM方法）对基准的详尽定量和定性分析赋予了我们方法论的功效。我们希望我们的工作能使人们对这个重要且务实的问题设定更加兴趣。

### BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence 
[[arxiv](https://arxiv.org/abs/2411.14869)] [[cool](https://papers.cool/arxiv/2411.14869)] [[pdf](https://arxiv.org/pdf/2411.14869)]
> **Authors**: Xuewu Lin,Tianwei Lin,Lichao Huang,Hongyu Xie,Zhizhong Su
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: BIP3D：桥接2D图像和3D感知的具体智能
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 在体现的智能系统中，关键组件是3D感知算法，它使代理能够理解其周围环境。以前的算法主要依赖于点云，尽管提供了精确的几何信息，但由于固有的稀疏性，噪声和数据稀缺性，仍然限制了感知性能。在这项工作中，我们引入了一种新颖的以图像为中心的3D感知模型BIP3D，该模型利用具有显式3D位置的表达图像特征，以克服以点为中心方法的局限性。具体而言，我们利用预先训练的2D视觉基础模型来增强语义理解，并引入空间增强器模块以提高空间理解。这些模块共同使BIP3D能够实现多视图，多模式特征融合和端到端3D感知。在我们的实验中，BIP3D在体现基准测试中优于当前最新结果，在3D检测任务中，提高了5.69％，在3D视觉接地任务中获得了15.25％。

### High-Resolution Image Synthesis via Next-Token Prediction 
[[arxiv](https://arxiv.org/abs/2411.14808)] [[cool](https://papers.cool/arxiv/2411.14808)] [[pdf](https://arxiv.org/pdf/2411.14808)]
> **Authors**: Dengsheng Chen,Jie Hu,Tiezhu Yue,Xiaoming Wei,Enhua Wu
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 31 pages
- **标题**: 高分辨率图像通过下一步预测合成
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 最近，自回归模型在类条件图像生成中表现出了出色的性能。但是，在高分辨率的文本到图像生成中，下一步的预测在很大程度上尚未探索。在本文中，我们介绍了\ textbf {d-jepa $ \ cdot $ t2i}，这是一种基于连续代币的自动回归模型，在建筑和培训策略中纳入了创新，以在任意决议中生成高质量的，耐心的图像，最多4K。在架构上，我们采用了脱氧的关节嵌入预测架构（D-JEPA），同时利用多模式的视觉变压器来有效整合文本和视觉特征。此外，我们将流量匹配损失与所提出的视觉旋转位置嵌入（VOPE）一起引入，以实现连续分辨率学习。在培训策略方面，我们提出了一种数据反馈机制，该机制基于统计分析和在线学习评论家模型，动态调整采样程序。这鼓励该模型超越其舒适区，减少对良好的场景的冗余培训，并迫使其以次优的生成质量来解决更具挑战性的病例。我们首次通过下一步的预测实现了最新的高分辨率图像综合。

### VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection 
[[arxiv](https://arxiv.org/abs/2411.14794)] [[cool](https://papers.cool/arxiv/2411.14794)] [[pdf](https://arxiv.org/pdf/2411.14794)]
> **Authors**: Songhao Han,Wei Huang,Hairong Shi,Le Zhuo,Xiu Su,Shifeng Zhang,Xu Zhou,Xiaojuan Qi,Yue Liao,Si Liu
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 14 pages, 14 figures
- **标题**: VideoSpresso：通过核心框架选择的大规模思考数据集用于细粒度的视频推理
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 大型视觉语言模型（LVLM）的进步已经显着改善了多模式的理解，但由于缺乏高质量的大规模数据集，视频推理任务中仍然存在挑战。现有的视频提问（VideoQA）数据集通常依赖于具有较高的粒度或自动构造方法的昂贵的手动注释，并具有冗余的逐帧分析，从而限制了它们的可扩展性和有效性。为了应对这些挑战，我们介绍了VideoSpresso，这是一个新颖的数据集，其中包含VideoQA对保留基本空间细节和时间连贯性以及中间推理步骤的多模式注释。我们的施工管道采用语义感知方法来减少冗余，然后使用GPT-4O生成QA对。我们进一步开发了思想链（COT）注释以丰富推理过程，从而指导GPT-4O从QA对和视频内容中提取逻辑关系。为了利用高质量的视频配对的潜力，我们提出了一个混合LVLMS协作框架，该框架具有框架选择器和两阶段的指令微调的推理LVLM。该框架适应性地选择了核心框架，并使用多模式证据执行COT推理。在我们提出的基准测试中，通过14个任务对9个流行的LVLM进行了评估，我们的方法在大多数任务上都优于现有基准，展示了卓越的视频推理功能。我们的代码和数据集将在以下网址发布：https：//github.com/hshjerry/videoespresso

### ΩSFormer: Dual-Modal Ω-like Super-Resolution Transformer Network for Cross-scale and High-accuracy Terraced Field Vectorization Extraction 
[[arxiv](https://arxiv.org/abs/2411.17088)] [[cool](https://papers.cool/arxiv/2411.17088)] [[pdf](https://arxiv.org/pdf/2411.17088)]
> **Authors**: Chang Li,Yu Wang,Ce Zhang,Yongjun Zhang
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: ωsformer：用于跨尺度和高准确梯田场矢量化提取的双模式ω样超分辨率变压器网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 露台田是土壤和节水（SWC）的重要工程实践。从远程感知的图像中提取梯田场是监测和评估SWC的基础。 This study is the first to propose a novel dual-modal Ω-like super-resolution Transformer network for intelligent TFVE, offering the following advantages: (1) reducing edge segmentation error from conventional multi-scale downsampling encoder, through fusing original high-resolution features with downsampling features at each step of encoder and leveraging a multi-head attention mechanism; （2）通过提出类似ω的网络结构来提高TFVE的准确性，该结构完全集成了来自光谱和地形数据的丰富高级特征，以形成跨尺度的超级分辨率特征； （3）验证跨模式和跨尺度的最佳融合方案（即远程感知的图像和DEM之间的空间分辨率不一致）超分辨率特征提取； （4）通过粗到精细的和空间拓扑语义关系优化（Stsro）分割策略来减轻分割边缘像素之间的不确定性； （5）利用轮廓振动神经网络连续优化参数，并从语义分割结果中迭代矢量化梯田场。此外，首次创建了用于基于深度学习的TFVE的DMRVD，该DMRVD涵盖了中国四个省的9个研究区，总覆盖面积为22441平方公里。为了评估ωsformer的性能，比较了经典和SOTA网络。与最佳精度单模式远程感知的图像，单模式DEM和双模式结果相比，ωsformer的MIOU分别提高了0.165、0.297和0.128。

### Relations, Negations, and Numbers: Looking for Logic in Generative Text-to-Image Models 
[[arxiv](https://arxiv.org/abs/2411.17066)] [[cool](https://papers.cool/arxiv/2411.17066)] [[pdf](https://arxiv.org/pdf/2411.17066)]
> **Authors**: Colin Conwell,Rupert Tawiah-Quashie,Tomer Ullman
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 关系，否定和数字：在生成文本到图像模型中寻找逻辑
- **领域**: 计算机视觉和模式识别,计算语言学,符号计算
- **摘要**: 尽管在多模式AI研究中取得了显着的进展，但仍有一个突出的领域，现代AI继续落后于人类儿童：可靠的逻辑运营商的部署。在这里，我们检查了三种逻辑运营商：关系，否定和离散数字。我们要求人类受访者（总共n = 178）评估由最先进的图像生成的AI（dall-e 3）产生的图像，并提示了这些“逻辑探针”，并发现没有人会可靠地产生大于50 \％的人类一致性得分。否定探针和数字（超过3）最常失败。在第四个实验中，我们评估了一个“接地扩散”管道，该管道利用了针对性的及时工程和结构化的中间表示，以获得更大的组成控制，但发现其性能比在提示的dall-e 3中更糟糕。为了进一步清楚这些文本对图像系统中的潜在成功和失败的来源，我们通过多个辅助分析和示意图对4个核心实验进行了补充，直接量化了关系提示的n克频率与平均匹配的n克频率之间的关系；在否定提示的呈现中，三种不同的迅速修改策略的成功率；以及涉及整数的提示的标量变异性 /比率依赖性（“近似计算”）。我们通过讨论“基础”多模式学习系统所固有的局限性，这些系统的基础在很大程度上依赖于基于向量的语义（例如DALL-E 3）或未指定的句法约束（例如，“接地扩散”）（例如“接地扩散”），并提出了最小的修改（基于图像的构图），从而有助于缩放GR的构图。所有数据和代码均可在https://github.com/colinconwell/t2i-probology上获得

### Multimodal Alignment and Fusion: A Survey 
[[arxiv](https://arxiv.org/abs/2411.17040)] [[cool](https://papers.cool/arxiv/2411.17040)] [[pdf](https://arxiv.org/pdf/2411.17040)]
> **Authors**: Songtao Li,Hao Tang
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: 210+ references
- **标题**: 多模式对准和融合：调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 这项调查对机器学习中的多模式对齐和融合的最新进步进行了全面的综述，这是由于文本，图像，音频和视频等数据类型越来越多的多样性所激发的。多模式集成可以通过利用不同模式的互补信息，并促进知识转移的情况下，可以提高模型的准确性和更广泛的适用性。我们系统地对现有的一致性和融合技术进行了系统的分类和分析，从对200多篇相关论文的广泛审查中获取了见解。此外，这项调查解决了多模式数据集成的挑战 - 包括对齐问题，噪声弹性和功能表示差异 - 同时着重于社交媒体分析，医学成像和情感识别等领域中的应用。所提供的见解旨在指导未来的研究，以优化多模式学习系统，以增强其在各种应用程序中的可扩展性，鲁棒性和概括性。

### CMAViT: Integrating Climate, Managment, and Remote Sensing Data for Crop Yield Estimation with Multimodel Vision Transformers 
[[arxiv](https://arxiv.org/abs/2411.16989)] [[cool](https://papers.cool/arxiv/2411.16989)] [[pdf](https://arxiv.org/pdf/2411.16989)]
> **Authors**: Hamid Kamangir,Brent. S. Sams,Nick Dokoozlian,Luis Sanchez,J. Mason. Earles
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: Chavit：将气候，管理和遥感数据整合到农作物收益率估算的数据与多模式视觉变压器
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 作物产量预测对于农业规划至关重要，但由于天气，气候和管理实践之间的复杂相互作用，仍然具有挑战性。为了应对这些挑战，我们介绍了一个基于深度学习的多模型，称为“气候管理意识到变压器”（CMAVIT），该挑战是为像素级葡萄园的产量预测而设计的。 CMAVIT通过利用遥感图像和短期气象数据来整合空间数据和时间数据，从而捕获生长季节变化的影响。此外，它结合了以文本形式表示的管理实践，并使用跨注意编码器来对其与时间序列数据进行建模。这款创新的多模式变压器在2016  -  2019年的大型数据集上进行了测试，涵盖了2,200公顷和八个葡萄品种，包括超过500万藤，超过了传统模型，例如UNET-Convlstm，例如在空间可变性捕获和产量预测中都表现出色，尤其是在葡萄园中的极端价值。 CMAVIT在看不见的测试数据集上达到了R2为0.84，MAPE为8.22％。掩盖特定的方式降低了性能：不包括管理实践，气候数据以及R2降低至0.73、0.70和0.72，并将MAPE提高至11.92％，12.66％和12.39％，强调了每种模式对准确预测的重要性。代码可从https://github.com/plant-ai-biophysics-lab/cmavit获得。

### Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2411.16863)] [[cool](https://papers.cool/arxiv/2411.16863)] [[pdf](https://arxiv.org/pdf/2411.16863)]
> **Authors**: Federico Cocchi,Nicholas Moratelli,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 用自我反思令牌增强多模式LLM，以基于知识的视觉问题答案
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体
- **摘要**: 多模式LLM（MLLM）是大型语言模型的自然扩展，以处理多模式输入，结合文本和图像数据。由于能力解决涉及这两种方式的复杂任务的能力，他们最近引起了人们的关注。但是，它们的有效性仅限于培训期间获得的知识，这限制了其实际实用性。在这项工作中，我们介绍了一种新颖的方法来通过整合外部知识来源来增强MLLM的适应性。我们提出的模型Reflective Llava（Reflectiva）利用反射令牌来动态确定外部知识的需求，并预测从外部数据库中检索到的信息的相关性。在两阶段的两级模型培训配方之后，对令牌进行了培训。这最终使MLLM能够管理外部知识，同时在不需要外部知识的任务上保持流利性和性能。通过我们的实验，我们证明了Reflectiva对基于知识的视觉问题的回答的功效，与现有方法相比，它强调了其优越的性能。源代码和训练有素的模型可在https://github.com/aimagelab/reflectiva上公开获得。

### SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE 
[[arxiv](https://arxiv.org/abs/2411.16856)] [[cool](https://papers.cool/arxiv/2411.16856)] [[pdf](https://arxiv.org/pdf/2411.16856)]
> **Authors**: Yongwei Chen,Yushi Lan,Shangchen Zhou,Tengfei Wang,Xingang Pan
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: Project page: https://cyw-3d.github.io/projects/SAR3D/
- **标题**: SAR3D：通过多尺度3D VQVAE的自回旋3D对象产生和理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 自回归模型在各个领域都取得了巨大的成功，从大型语言模型（LLM）到大型多模式模型（LMM）和2D内容生成，更接近人工通用智能（AGI）。尽管有这些进展，但在3D对象产生和理解中采用自回归方法仍未得到探索。本文介绍了Scale AutoreSercrive 3D（SAR3D），这是一个新型框架，利用了多尺度3D矢量定量的变异自动编码器（VQVAE），以使3D对象具有有效的自动性产生和详细的理解。通过在多尺度潜在表示中预测下一个量表，而不是下一个单一代表，SAR3D大大减少了生成时间，从而在A6000 GPU上仅在0.82秒内实现了快速的3D对象生成。此外，考虑到具有层次3D感知信息的代币，我们对它们进行了预告片的LLM，从而可以对3D内容进行多模式理解。我们的实验表明，SAR3D超过了速度和质量的当前3D生成方法，并允许LLM全面解释和标题3D模型。

### Beyond Sight: Towards Cognitive Alignment in LVLM via Enriched Visual Knowledge 
[[arxiv](https://arxiv.org/abs/2411.16824)] [[cool](https://papers.cool/arxiv/2411.16824)] [[pdf](https://arxiv.org/pdf/2411.16824)]
> **Authors**: Yaqi Zhao,Yuanyang Yin,Lin Li,Mingan Lin,Victor Shea-Jay Huang,Siwei Chen,Weipeng Chen,Baoqun Yin,Zenan Zhou,Wentao Zhang
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 超越视觉：通过丰富的视觉知识在LVLM中朝着认知对齐
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 看到总是意味着知道吗？大型视觉模型（LVLMS）通常使用夹具作为视觉主链整合了预训练的视觉和语言组件。但是，这些模型经常遇到视觉编码器（VE）和大语言模型（LLM）之间“认知错位”的核心问题。具体而言，VE的视觉信息表示可能无法完全与LLM的认知框架一致，这导致视觉特征超过语言模型的解释范围的不匹配。为了解决这个问题，我们研究了VE表示的变化如何影响LVLM理解，尤其是当LLM面对VE VE的数据图像时，其模棱两可的视觉表示挑战了VE的解释精度。因此，我们构建了一个多范围的地标数据集，并系统地检查了知名和知名度数据对解释能力的影响。我们的结果表明，Ve-Nocknown的数据限制了LVLM准确理解的能力，而Ve-Ve-Newna（富含独特特征）有助于减少认知失误。在这些见解的基础上，我们提出了实体增强的认知对齐（EECA），这种方法采用多跨性监督来产生视觉丰富，良好的代币，不仅将其整合在LLM的嵌入空间中，而且还与LLM的认知框架保持一致。这种对齐明显提高了LVLM在具有里程碑意义的识别中的性能。我们的发现强调了Ve-Nonnwown数据所带来的挑战，并强调了认知一致性在推进多模式系统中的重要作用。

### FREE-Merging: Fourier Transform for Model Merging with Lightweight Experts 
[[arxiv](https://arxiv.org/abs/2411.16815)] [[cool](https://papers.cool/arxiv/2411.16815)] [[pdf](https://arxiv.org/pdf/2411.16815)]
> **Authors**: Shenghe Zheng,Hongzhi Wang
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: 16 pages, 5 figures
- **标题**: 自由合并：与轻型专家合并模型的傅立叶变换
- **领域**: 计算机视觉和模式识别
- **摘要**: 在当前的模型量表快速扩展时代，开源模型权重的可用性越来越大。但是，单个微调模型的功能通常无法满足各种部署需求。因此，模型合并已成为一种广泛集中的方法，用于有效地构建针对来自现有模型的多个任务量身定制的单个模型。然而，现有的模型合并方法面临绩效和部署成本之间的挑战权衡，这主要是由于合并网络内的任务冲突。我们对神经网络的分析表明，通过微调最小化引入的某些特定于任务的信息会增强性能，但会严重影响概括，从而导致任务冲突。为了减轻此信息的影响，我们提出了一种创新的方法，该方法利用频域信息有效地过滤有害的专业信息，从而最大程度地减少了任务冲突对主链的影响，而成本最低。由于无需成本的合并方法不可避免地绩效损失，因此我们引入了一个轻巧的特定任务专家，在推断过程中可以动态整合以补偿信息损失。这个框架，自由合并（与轻量级专家一起使用），在培训成本，推理速度，存储需求和性能之间取消了平衡的权衡。我们证明了FR-合并和自由合并对CV，NLP和多模式域的多个任务的有效性，并表明它们可以灵活地适应以满足特定需求。

### InTraGen: Trajectory-controlled Video Generation for Object Interactions 
[[arxiv](https://arxiv.org/abs/2411.16804)] [[cool](https://papers.cool/arxiv/2411.16804)] [[pdf](https://arxiv.org/pdf/2411.16804)]
> **Authors**: Zuhao Liu,Aleksandar Yanev,Ahmad Mahmood,Ivan Nikolov,Saman Motamed,Wei-Shi Zheng,Xi Wang,Luc Van Gool,Danda Pani Paudel
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: Interagen：对象交互的轨迹控制视频生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频生成的进步显着改善了创建场景的现实主义和质量。这引起了人们对开发直观工具的兴趣，使用户可以利用视频生成作为世界模拟器。文本到视频（T2V）的生成就是一种这样的方法，仅从文本描述中启用视频创建。然而，由于文本中的固有歧义和文本提示提供的有限的时间信息，研究人员探索了诸如轨迹引导的系统之类的其他控制信号，以使其更准确。但是，评估T2V模型是否可以在多个对象之间产生逼真的相互作用的方法。我们介绍了Intragen，这是一种用于改善基于轨迹的对象交互情况的管道。我们提出了4个新数据集和一个新型的轨迹质量指标，以评估所提出的内部的性能。为了实现对象相互作用，我们引入了一个多模式相互作用编码管道，该管道具有富含对象环境相互作用的对象ID注入机制。我们的结果表明，视觉保真度和定量性能的改善。代码和数据集可从https://github.com/insait-institute/intragen获得

### ST-Align: A Multimodal Foundation Model for Image-Gene Alignment in Spatial Transcriptomics 
[[arxiv](https://arxiv.org/abs/2411.16793)] [[cool](https://papers.cool/arxiv/2411.16793)] [[pdf](https://arxiv.org/pdf/2411.16793)]
> **Authors**: Yuxiang Lin,Ling Luo,Ying Chen,Xushi Zhang,Zihui Wang,Wenxian Yang,Mengsha Tong,Rongshan Yu
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: ST-Align：空间转录组学中图像 - 基因对齐的多模式基础模型
- **领域**: 计算机视觉和模式识别,基因组学
- **摘要**: 空间转录组学（ST）提供了高分辨率的病理图像和整个坡度范围内各个斑点上的全转录谱。此设置使其成为开发多模式基础模型的理想数据源。尽管最近的研究试图基于点级的可训练基因编码器来微调视觉编码器，但缺乏更广泛的幻灯片透视和空间内在关系限制了它们有效捕获ST特异性见解的能力。在这里，我们介绍了为ST设计的第一个基础模型，该模型是为ST设计的，它通过将空间环境结合在一起，有效地将病理成像与基因组特征融合在一起，从而深深地对齐图像 - 基因对。我们设计了一个新颖的预处理框架，采用三个目标对准策略，以跨图像 - 基因对构成（1）多尺度对齐，捕获了综合视角，以及（2）多媒体的跨层次和跨层次的互联层互联网，连接局部蜂窝细胞的架构和宽阔的构造，并将其跨层次保持一致。此外，ST-Align还采用了针对不同ST上下文的专业编码器，其次是基于注意力的融合网络（ABFN），用于增强多模式融合，有效地将域共享知识与病理学和基因组数据的ST特异性见解相结合。我们在130万个斑点成对上预先训练了ST-Align，并通过六个数据集的两个下游任务评估了其性能，这表明了优越的零射击和很少的射击功能。 ST-Align强调了降低ST成本并为人类组织内关键组成的区别提供有价值的见解的潜力。

### Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation 
[[arxiv](https://arxiv.org/abs/2411.16789)] [[cool](https://papers.cool/arxiv/2411.16789)] [[pdf](https://arxiv.org/pdf/2411.16789)]
> **Authors**: Jungeun Kim,Hyeongwoo Jeon,Jongseong Bae,Ha Young Kim
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 利用MLLM的力量用于无光泽的手语翻译
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 手语翻译（SLT）是一项具有挑战性的任务，涉及将手语图像翻译成口语。为了使SLT模型成功执行此任务，它们必须弥合模式差距并确定手语组件中的微妙变化，以准确理解其含义。为了应对这些挑战，我们提出了一个称为多模式手语翻译（MMSLT）的新型无光泽SLT框架，该框架利用了现成的多模式大型语言模型（MLLMS）的代表性功能。具体而言，我们使用MLLM生成了手语组件的详细文本描述。然后，通过我们提出的多模式预训练模块，我们将这些描述功能与符号视频功能集成在一起，以使其在口语句子空间中对齐。我们的方法可以在每日基准数据集和CSL上实现最新的性能，从而强调了MLLM在SLT中有效利用的潜力。

### UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing 
[[arxiv](https://arxiv.org/abs/2411.16781)] [[cool](https://papers.cool/arxiv/2411.16781)] [[pdf](https://arxiv.org/pdf/2411.16781)]
> **Authors**: Yiheng Li,Ruibing Hou,Hong Chang,Shiguang Shan,Xilin Chen
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: Unipose：人类姿势理解，生成和编辑的统一多模式框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 人姿势在数字时代起着至关重要的作用。尽管最近的作品在理解和产生人类姿势方面取得了令人印象深刻的进步，但它们通常仅支持一种控制信号的单一方式并孤立地运行，从而限制了他们在现实世界中的应用。本文介绍了使用大型语言模型（LLM）的框架，以理解，生成和编辑各种方式，包括图像，文本和3D SMPL姿势。具体而言，我们应用姿势令牌将3D姿势转换为离散的姿势令牌，从而使无缝集成在统一词汇中的LLM中。为了进一步增强细粒的姿势感知能力，我们促进了与视觉编码器的混合物的unipose，其中包括特定于姿势的视觉编码器。从统一的学习策略中受益，Unipose可以有效地将知识转移到不同姿势相关的任务中，适应看不见的任务，并展示了扩展的能力。这项工作是建立姿势理解，生成和编辑的通用框架的首次尝试。广泛的实验突出了Unipose在各种姿势相关任务中的竞争性甚至更高的表现。

### GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis 
[[arxiv](https://arxiv.org/abs/2411.16778)] [[cool](https://papers.cool/arxiv/2411.16778)] [[pdf](https://arxiv.org/pdf/2411.16778)]
> **Authors**: Bo Liu,Ke Zou,Liming Zhan,Zexin Lu,Xiaoyu Dong,Yidi Chen,Chengqiang Xie,Jiannong Cao,Xiao-Ming Wu,Huazhu Fu
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: This project is available at https://www.med-vqa.com/GEMeX
- **标题**: Gemex：用于胸部X射线诊断的大规模，可见且可解释的医疗VQA基准
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 医学视觉问题回答（VQA）是一项基本技术，它将计算机视觉和自然语言处理整合起来，以自动响应有关医学图像的临床查询。但是，当前的医学VQA数据集表现出两个重要的局限性：（1）他们通常缺乏视觉和文字解释，这阻碍了他们满足患者和初级医生的理解需求的能力； （2）他们通常提供狭窄的问题格式，不足以反映临床场景中遇到的各种要求。这些局限性对可靠且用户友好的Med-VQA系统的开发构成了重大挑战。为了应对这些挑战，我们引入了用于胸部X射线诊断（GEMEX）的大规模，可见且可解释的医学VQA基准（GEMEX），其中包含多种创新组件：（1）多模式的解释性机制，为每个问题提出的答案综合性提供了详细的视觉和文本解释； （2）四种不同的问题类型，开放式，封闭式，单选项和多项选择性，可以更好地反映各种临床需求。我们在Gemex上评估了10种代表性的大视觉语言模型，发现它们的表现不佳，突出了数据集的复杂性。但是，在使用训练集对基线模型进行了微调之后，我们观察到了显着的性能提高，证明了数据集的有效性。该项目可在www.med-vqa.com/gemex上获得。

### Is 'Right' Right? Enhancing Object Orientation Understanding in Multimodal Language Models through Egocentric Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2411.16761)] [[cool](https://papers.cool/arxiv/2411.16761)] [[pdf](https://arxiv.org/pdf/2411.16761)]
> **Authors**: Ji Hyeok Jung,Eun Tae Kim,Seo Yeon Kim,Joo Ho Lee,Bumsoo Kim,Buru Chang
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: “对”是对的吗？通过以自我为中心的指令调整，在多模式语言模型中增强对象取向理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式大语言模型（MLLM）充当基本接口，将人类与多模式应用中的AI技术联系起来。但是，由于训练数据中的方向注释不一致，当前的MLLMS在准确解释图像中的对象方向时面临挑战，从而阻碍了连贯的方向理解的发展。为了克服这一点，我们提出了以自我为中心的指令调整，该调整将MLLMS的方向理解与用户的观点保持一致，这是基于从用户的Egentric观点得出的一致注释标准。我们首先生成以自我为中心的指令数据，该数据利用MLLM识别对象细节并应用先验知识以进行方向理解的能力。使用此数据，我们执行指令调整以增强模型的准确方向解释能力。此外，我们推出了EgoorientBench，这是一种基准，该基准使用从不同域收集的图像来评估MLLMS在三个任务上的方向理解。该基准的实验结果表明，以自我为中心的教学调整可显着提高方向理解，而不会损害总体MLLM性能。指令数据和基准数据集可在我们的项目页面https://github.com/jhcor/egoorientbench上找到。

### AnySynth: Harnessing the Power of Image Synthetic Data Generation for Generalized Vision-Language Tasks 
[[arxiv](https://arxiv.org/abs/2411.16749)] [[cool](https://papers.cool/arxiv/2411.16749)] [[pdf](https://arxiv.org/pdf/2411.16749)]
> **Authors**: You Li,Fan Ma,Yi Yang
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: Anysynth：利用图像综合数据生成的力量来实现广义视觉任务
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近已经采用了扩散模型来生成高质量的图像，减少了手动数据收集的需求，并改善了诸如对象检测，实例分割和图像感知等任务中的模型概括。但是，由于对图像布局，内容和注释格式的各种要求，合成框架通常是针对每个任务进行精心设计的，因此限制了合成数据在更一般的情况下的应用。在本文中，我们提出了Anysyth，这是一个统一的框架，该框架整合了适应性，全面且高度可控的组件，能够产生不同的要求，以产生任意类型的合成数据。具体而言，首先引入了特定于任务的布局生成模块，以通过利用大语言模型和真实世界图像的布局先验的生成能力来为不同的任务生成合理的布局。然后开发一个Uni控制的图像生成模块，以创建可控制并基于生成的布局的高质量合成图像。此外，可以将特定于用户的参考图像和样式图像纳入任务要求中。最后，面向任务的注释模块为跨不同任务的生成图像提供精确而详细的注释。我们已经验证了框架在各种任务中的性能，包括很少的对象检测，跨域对象检测，零射击组成的图像检索以及多模式图像感知和接地。我们框架合成的特定数据可显着提高这些任务中的模型性能，证明了我们框架的一般性和有效性。

### LetsTalk: Latent Diffusion Transformer for Talking Video Synthesis 
[[arxiv](https://arxiv.org/abs/2411.16748)] [[cool](https://papers.cool/arxiv/2411.16748)] [[pdf](https://arxiv.org/pdf/2411.16748)]
> **Authors**: Haojie Zhang,Zhihao Liang,Ruibo Fu,Zhengqi Wen,Xuefei Liu,Chenxing Li,Jianhua Tao,Yaling Liang
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-26
> **comment**: 17 pages, 14 figures
- **标题**: LETSTALK：用于说话视频综合的潜扩散变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 使用音频的肖像图像动画已经快速提高，从而创建了越来越现实和表现力的动画面孔。这种多模式指导的视频生成任务的挑战涉及融合各种方式，同时确保时间和肖像的一致性。我们进一步寻求生动的说话头。为了应对这些挑战，我们提出了letstalk（会说话视频综合的潜在扩散变压器），这是一种扩散变压器，结合了模块化的时间和空间注意机制，以合并多模式并增强时空的一致性。为了处理多模式条件，我们首先总结了三个融合方案，范围从浅层到深融合紧凑，并彻底探索它们的影响和适用性。然后，根据图像，音频和视频生成的方式差异，我们提出了一个合适的解决方案。对于肖像来说，我们利用深层融合方案（共生融合）来确保肖像的一致性。对于音频，我们实施了浅融合计划（直接融合），以实现音频动画对准，同时保持多样性。我们广泛的实验表明，我们的方法在时间上产生了一致和现实的视频，并具有增强的多样性和活力。

### Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents 
[[arxiv](https://arxiv.org/abs/2411.16740)] [[cool](https://papers.cool/arxiv/2411.16740)] [[pdf](https://arxiv.org/pdf/2411.16740)]
> **Authors**: Jun Chen,Dannong Xu,Junjie Fei,Chun-Mei Feng,Mohamed Elhoseiny
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-26
> **comment**: the correct arxiv version
- **标题**: 文档干草堆：1000多个文档堆的视觉推理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型的多模型模型（LMM）在视觉理解方面取得了令人印象深刻的进步，但是它们在现实世界中的局限性上，需要在大量图像上进行复杂的推理。多图像提问的现有基准范围有限，每个问题只能与多达30张图像配对，这并不能完全捕获现实世界中遇到的大规模检索任务的需求。为了减少这些差距，我们介绍了两个文档的Haystack基准，该基准为Dochaystack和Infohaystack，旨在评估大规模视觉文档检索和理解的LMM性能。此外，我们提出了V-rag，这是一种新型，以视觉为中心的检索生成（RAG）框架，它利用了一套多模式视觉编码器，每个框架都针对特定的优势进行了优化，并具有专用的问题文档相关性模块。 V-RAG设定了一个新标准，与以前的最佳基线模型相比，在具有挑战性的Dochaystack-1000和Infohaystack-1000基准方面，Recell@1分别提高了9％和11％。此外，将V-rag与LMM集成，使它们能够在数千个图像上有效地操作，从而对我们的Dochaystack和InfohayStack基准进行了重大改进。我们的代码和数据集可从https://github.com/vision-cair/dochaystacks获得

### All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages 
[[arxiv](https://arxiv.org/abs/2411.16508)] [[cool](https://papers.cool/arxiv/2411.16508)] [[pdf](https://arxiv.org/pdf/2411.16508)]
> **Authors**: Ashmal Vayani,Dinura Dissanayake,Hasindri Watawana,Noor Ahsan,Nevasini Sasikumar,Omkar Thawakar,Henok Biadglign Ademtew,Yahya Hmaiti,Amandeep Kumar,Kartik Kuckreja,Mykola Maslych,Wafa Al Ghallabi,Mihail Mihaylov,Chao Qin,Abdelrahman M Shaker,Mike Zhang,Mahardika Krisna Ihsani,Amiel Esplana,Monil Gokani,Shachar Mirkin,Harsh Singh,Ashay Srivastava,Endre Hamerlik,Fathinah Asma Izzati,Fadillah Adamsyah Maani, et al. (44 additional authors not shown)
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: A MultilingualMultimodalcultural benchmark for 100 languages
- **标题**: 所有语言都很重要：评估文化多样的100种语言的LMM
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 现有的大型多模型模型（LMM）通常仅关注几个区域和语言。随着LMM的继续改善，确保他们了解文化背景，尊重当地敏感性并支持低资源语言的同时，同时有效地整合相应的视觉提示是越来越重要的。为了追求文化多样化的全球多模型模型，我们提出的所有语言都重要的基准（ALM-Bench）代表了迄今为止最大，最全面的努力，用于评估100种语言的LMM。 Alm Bench通过测试其理解和理论文化多样的图像与各种语言配对的文化图像的能力来挑战现有模型，其中包括传统上许多低资源的语言在LMM研究中代表性不足。该基准提供了一个强大而细微的评估框架，其中包括各种问题格式，包括True/fals，多项选择和开放式问题，这些问题将进一步分为简短和悠久的类别。 ALM基础设计可确保对模型处理视觉和语言推理难度不同的能力进行全面评估。为了捕捉丰富的全球文化挂毯，Alm Bench仔细地策划了13个不同文化方面的内容，从传统和仪式到著名人物和庆祝活动。通过此，ALM Bench不仅为最先进的开放和封闭式LMM提供了严格的测试场，而且还强调了文化和语言包容性的重要性，鼓励开发可以有效地为全球人口服务的模型的发展。我们的基准可公开可用。

### Interpreting Object-level Foundation Models via Visual Precision Search 
[[arxiv](https://arxiv.org/abs/2411.16198)] [[cool](https://papers.cool/arxiv/2411.16198)] [[pdf](https://arxiv.org/pdf/2411.16198)]
> **Authors**: Ruoyu Chen,Siyuan Liang,Jingzhi Li,Shiming Liu,Maosen Li,Zheng Huang,Hua Zhang,Xiaochun Cao
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 通过视觉精确搜索解释对象级基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 在视觉接地和对象检测等任务中，多模式预训练的进步具有推动的对象级基础模型，例如接地Dino和Florence-2。但是，解释这些模型\'决定越来越具有挑战性。对象级任务解释的现有可解释的归因方法具有明显的局限性：（1）基于梯度的方法由于基础模型中的视觉文本融合而缺乏精确的本地化，并且（2）基于扰动的方法产生嘈杂的显着性图，从而限制了细粒度的可解释性。为了解决这些问题，我们提出了一种视觉精确搜索方法，该方法在更少的区域生成准确的归因图。我们的方法绕过了内部模型参数，以克服从多模式融合，将输入分为稀疏子区域以及使用一致性和协作得分以准确识别关键决策区域的归因问题。我们还对我们方法的适用性边界保证和范围进行了理论分析。关于Refcoco，Coco和LVIS的实验表明，我们的方法增强了对对象级的任务可解释性，以跨各种评估指标接地Dino和Florence-2的对象级任务可解释性，忠诚的增长率为23.7 \％，31.6 \％\％\％\％，而MS Coco，LVIS和Refcoco的忠诚度为20.1 \％，以及699999。可可和佛罗伦萨-2的可可和reccoco。此外，我们的方法可以解释视觉接地和对象检测任务中的故障，从而超过了多个评估指标的现有方法。该代码将在\ url {https://github.com/ruoyuchen10/vps}发布。

### SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis 
[[arxiv](https://arxiv.org/abs/2411.16173)] [[cool](https://papers.cool/arxiv/2411.16173)] [[pdf](https://arxiv.org/pdf/2411.16173)]
> **Authors**: Junho Kim,Hyunjun Kim,Hosu Lee,Yong Man Ro
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: Project page: https://ivy-lvlm.github.io/SALOVA/
- **标题**: Salova：长期视频分析中针对目标检索和路由的段的长期视频助理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 尽管大型多模式模型的进步，但由于上下文长度的局限性和大量的内存开销，将它们应用于长而未修剪的视频内容仍然具有挑战性。这些限制通常会导致大量信息丢失并降低模型响应中的相关性。随着视频数据跨网平台的指数增长，理解长期视频对于推进通用智能至关重要。在本文中，我们介绍了Salova：片段演示的长期视频助手，这是一个新颖的视频-LLM框架，旨在通过有针对性的检索过程来增强冗长的视频内容的理解。我们解决了实现这一目标的两个主要挑战：（i）我们介绍了SceneWalk数据集，这是87.8K长的视频的高质量集合，每个视频都在细分市场级别上密集字幕，以使模型能够捕获场景连续性并保持丰富的描述性上下文。 （ii）我们开发了强大的体系结构设计，集成了动态路由机制和时空投影仪，以根据用户查询有效检索和处理相关的视频片段。我们的框架通过允许对查询的响应进行精确识别和检索，从而减轻当前视频LMM的局限性，从而改善生成的响应的上下文相关性。通过广泛的实验，Salova展示了在处理复杂的长形视频方面增强的能力，显示出在扩展序列之间保持上下文完整性的显着能力。

### ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality Images 
[[arxiv](https://arxiv.org/abs/2411.16096)] [[cool](https://papers.cool/arxiv/2411.16096)] [[pdf](https://arxiv.org/pdf/2411.16096)]
> **Authors**: Prithviraj Purushottam Naik,Rohit Agarwal
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: ENCLIP：结合和基于聚类的对比性语言图像 - 用于时尚多模式搜索的训练有限的数据和低质量图像
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **摘要**: 多模式搜索彻底改变了时装业，为用户发现和探索时尚物品提供了一种无缝且直观的方式。根据他们的偏好，样式或特定属性，用户可以通过组合文本和图像信息来搜索产品。文本对图像搜索使用户能够查找视觉上相似的项目或使用自然语言描述产品。本文提出了一种创新的方法，称为Anclip，以增强对比性语言图像预处理（剪辑）模型的性能，特别是针对时尚智能领域的多模式搜索。该方法着重于解决有限的数据可用性和低质量图像所带来的挑战。本文提出了一种算法，该算法涉及培训并结合剪辑模型的多个实例，并利用聚类技术将相似图像分组在一起。这项研究中提出的实验发现提供了该方法的有效性的证据。这种方法释放了剪辑在时尚智能领域的潜力，在这种智能领域中，数据稀缺和图像质量问题很普遍。总体而言，Enclip方法代表了对时尚智能领域的宝贵贡献，并提供了一种实用解决方案，可以在具有有限的数据和低质量图像的方案中优化剪辑模型。

### HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation 
[[arxiv](https://arxiv.org/abs/2411.18042)] [[cool](https://papers.cool/arxiv/2411.18042)] [[pdf](https://arxiv.org/pdf/2411.18042)]
> **Authors**: Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Khoa Luu
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: HyperGLM：视频场景图的超图生成和预期
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式LLM具有高级视觉语言任务，但仍在理解视频场景方面挣扎。为了弥合这一差距，已经出现了视频场景图（Vidsgg）以捕获视频框架的多对象关系。但是，先前的方法依赖于成对连接，从而限制了它们处理复杂的多对象交互和推理的能力。为此，我们在场景HyperGraph（HyperGLM）上提出了多模式LLM，从而促进了有关多路相互作用和高阶关系的推理。我们的方法唯一地集成了实体场景图，这些场景图捕获对象之间的空间关系，并与其因果过渡建模，形成统一的超图。值得注意的是，HyperGLM通过将此统一的超图注入LLM来实现推理。此外，我们介绍了一个新的视频场景图（VSGR）数据集，该数据集具有来自第三人称，中心和无人机视图的190万帧，并支持五个任务：场景图生成，场景图预期，视频问题响应，视频字幕字幕和关系推理。从经验上讲，HyperGLM始终在五个任务中胜过最先进的方法，从而有效地建模和推理了各种视频场景中的复杂关系。

### RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model 
[[arxiv](https://arxiv.org/abs/2411.17984)] [[cool](https://papers.cool/arxiv/2411.17984)] [[pdf](https://arxiv.org/pdf/2411.17984)]
> **Authors**: Huiyang Hu,Peijin Wang,Hanbo Bi,Boyuan Tong,Zhaozhi Wang,Wenhui Diao,Hao Chang,Yingchao Feng,Ziqi Zhang,Yaowei Wang,Qixiang Ye,Kun Fu,Xian Sun
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: 19 pages, 8 figures and 10 tables
- **标题**: RS-VEHEAT：热传导引导有效的遥感基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 遥感基础模型在很大程度上脱离了设计特定于任务模型的传统范式，从而在多个任务中提供了更大的可扩展性。但是，他们面临诸如低计算效率和有限的解释性之类的挑战，尤其是在处理大型遥感图像时。为了克服这些，我们从热传导中汲取灵感，这是对局部热扩散进行建模的物理过程。在这个想法的基础上，我们是第一个探索使用热传导的平行计算模型来模拟高分辨率遥感图像中局部区域相关性的潜力，并引入了有效的多模式遥感基础模型RS-Veheat。具体而言，RS-Veheat 1）应用$ O（N^{1.5}）$的复杂性和一个全球接收场的热传导操作员（HCO），在捕获遥感对象结构信息的同时减少了计算开销，以指导热扩散； 2）通过基于频域层次掩蔽和多域重建的自我监督策略来了解各种场景的频率分布表示； 3）显着提高了4个任务和10个数据集的最先进技术的效率和性能。与基于注意力的遥感基础模型相比，我们将记忆使用量减少84 \％，触及24 \％，并将吞吐量提高2.7倍。该代码将公开可用。

### Passive Deepfake Detection Across Multi-modalities: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2411.17911)] [[cool](https://papers.cool/arxiv/2411.17911)] [[pdf](https://arxiv.org/pdf/2411.17911)]
> **Authors**: Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: 26 pages
- **标题**: 跨多模式的被动深层检测：一项全面的调查
- **领域**: 计算机视觉和模式识别,密码学和安全
- **摘要**: 近年来，Deepfakes（DFS）被用于恶意目的，例如个人模仿，错误信息传播以及艺术家的风格模仿，引发了有关道德和安全问题的疑问。但是，现有的调查重点是用于单个模式的被动DF检测方法的准确性，例如图像，视频或音频。这项全面的调查探讨了多种方式的被动方法，包括图像，视频，音频和多模式域，并将我们的讨论扩展到超出检测准确性，包括概括，鲁棒性，属性和解释性。此外，我们讨论了被动方法的威胁模型，包括潜在的对抗策略以及不同水平的对手知识和能力。我们还强调了当前在DF检测中的挑战，包括在不同的生成模型中缺乏概括，对全面的可信度评估的需求以及现有多模式方法的局限性。最后，我们提出了未来的研究方向，该方向在被动DF检测领域中解决了这些未开发和新兴问题，例如自适应学习，动态基准，整体可信赖性评估以及用于说话的视频发电的多模式检测器。

### OracleSage: Towards Unified Visual-Linguistic Understanding of Oracle Bone Scripts through Cross-Modal Knowledge Fusion 
[[arxiv](https://arxiv.org/abs/2411.17837)] [[cool](https://papers.cool/arxiv/2411.17837)] [[pdf](https://arxiv.org/pdf/2411.17837)]
> **Authors**: Hanqi Jiang,Yi Pan,Junhao Chen,Zhengliang Liu,Yifan Zhou,Peng Shu,Yiwei Li,Huaqin Zhao,Stephen Mihm,Lewis C Howe,Tianming Liu
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: Oraclesage：通过跨模式知识融合，统一对甲骨骨文字的统一视觉语言理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 甲骨文骨文字（OBS）作为中国最早的成熟写作系统，由于其复杂的象形结构和与现代汉字的差异，在自动识别中提出了重大挑战。我们引入了Oraclesage，这是一种新型的跨模式框架，将层次视觉理解与基于图的语义推理集成在一起。具体而言，我们提出（1）一个层次的视觉语义理解模块，该模块通过逐步进行多种晶体性特征提取，通过逐步进行LLAVA视觉主链的逐步微调，（2）基于图的语义推理框架，可通过动态信息传递和（3）Oraclesimed obs toctripation，comped necation and compitation comped obs ned Datcocraphy，comped obs ned Datcocraphy，comped obs ned Datcoprication，捕获视觉组成部分和语义概念之间的关系。实验结果表明，Oraclesage明显胜过最先进的视觉模型。这项研究为古代文本解释建立了一个新的范式，同时为考古研究提供了宝贵的技术支持。

### NEMO: Can Multimodal LLMs Identify Attribute-Modified Objects? 
[[arxiv](https://arxiv.org/abs/2411.17794)] [[cool](https://papers.cool/arxiv/2411.17794)] [[pdf](https://arxiv.org/pdf/2411.17794)]
> **Authors**: Jiaxuan Li,Junwen Mo,MinhDuc Vo,Akihiro Sugimoto,Hideki Nakayama
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: NEMO：多模式LLM可以识别属性修饰的对象吗？
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在视觉理解方面取得了显着进步，但它们识别由特定属性修改的对象的能力仍然是一个悬而未决的问题。为了解决这个问题，我们探讨了MLLMS在对象识别中的推理能力，从常识到超声态度的场景。我们介绍了一种新颖的基准Nemo，其中包括900张原始水果及其相应属性修饰的图像。以及包括开放式，多项选择类型的2700个问题。我们使用我们的基准评估了26种新开源和商业模型。这些发现突出了明显的性能差距，在识别Nemo中的对象并揭示了不同模型之间的不同答案偏好。尽管更强大的视觉编码器提高了性能，但MLLM仍然落后于独立的视觉编码。有趣的是，扩大模型大小并不能始终如一地产生更好的结果，因为更深入的分析表明，较大的LLM可以在微调过程中削弱视力编码器。这些见解阐明了当前MLLM中的临界局限性，并提出了开发更具用途和弹性的多模型模型的潜在途径。

### Efficient Multi-modal Large Language Models via Visual Token Grouping 
[[arxiv](https://arxiv.org/abs/2411.17773)] [[cool](https://papers.cool/arxiv/2411.17773)] [[pdf](https://arxiv.org/pdf/2411.17773)]
> **Authors**: Minbin Huang,Runhui Huang,Han Shi,Yimeng Chen,Chuanyang Zheng,Xiangguo Sun,Xin Jiang,Zhenguo Li,Hong Cheng
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 通过视觉令牌分组有效的多模式大型语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的开发增强了大型语言模型（LLMS），能够感知到文本以外的数据格式，从而大大推动了一系列下游应用程序，例如视觉问答答案和图像字幕。但是，与处理高分辨率图像和视频相关的大量计算成本构成了更广泛采用的障碍。为了应对这一挑战，MLLM中的压缩视觉令牌已成为降低推理成本的一种有前途的方法。而现有方法在特征对齐阶段进行令牌减少。在本文中，我们介绍了Vistog，这是一种新型的分组机制，利用预训练的视力编码器的能力将相似的图像段分组而无需分割掩码。具体而言，我们加入语义令牌，以表示线性投影层之后的图像语义段，然后再插入视觉编码器。此外，通过我们采用的孤立注意力，Vistog可以利用预先训练的视觉编码器中的先验知识来识别和消除冗余的视觉令牌，从而有效地减少了计算需求。广泛的实验证明了Vistog的有效性，维持了原始性能的98.1％，同时减少了27 \％的推理时间。

### MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding 
[[arxiv](https://arxiv.org/abs/2411.17762)] [[cool](https://papers.cool/arxiv/2411.17762)] [[pdf](https://arxiv.org/pdf/2411.17762)]
> **Authors**: Rongchang Xie,Chen Du,Ping Song,Chang Liu
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: Muse-VL：通过语义离散编码对统一VLM进行建模
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们介绍了Muse-VL，这是一种统一的视觉语言模型，通过语义离散编码，用于多模式理解和产生。最近，研究界已经开始探索统一的视觉生成和理解模型。但是，现有的视觉标记器（例如，VQGAN）仅考虑低级信息，这使得很难与纹理语义特征保持一致。这会导致高训练的复杂性，并需要大量培训数据才能实现最佳性能。此外，他们的表现远非专门的理解模型。本文提出了语义离散编码（SDE），该编码（SDE）通过在视觉令牌中添加语义约束来有效地对齐视觉令牌和语言令牌的信息。这大大减少了训练难度并提高了统一模型的性能。所提出的模型在各种视觉语言基准中显着超过了先前的最先进，并且比专门的理解模型取得了更好的性能。

### OpenAD: Open-World Autonomous Driving Benchmark for 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2411.17761)] [[cool](https://papers.cool/arxiv/2411.17761)] [[pdf](https://arxiv.org/pdf/2411.17761)]
> **Authors**: Zhongyu Xia,Jishuo Li,Zhiwei Lin,Xinhao Wang,Yongtao Wang,Ming-Hsuan Yang
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: OpenAD：3D对象检测的开放世界自动驾驶基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 开放世界的自主驾驶涵盖域的概括和开放式视频计。域的概括是指在不同方案和传感器参数配置上自主驾驶系统的功能。开放词汇与识别训练过程中未遇到的各种语义类别的能力有关。在本文中，我们介绍了OpenAD，这是第一个用于3D对象检测的真实世界自动驾驶基准。 OpenAD建立在一个角案例发现和注释管道与多模式大型语言模型（MLLM）集成的情况下。提议的管道以统一格式注释Corner Case对象，用于五个具有2000个场景的自动驾驶感知数据集。此外，我们设计了评估方法，并评估各种2D和3D开放世界和专业模型。此外，我们提出了一个以视觉为中心的3D Open-World对象检测基线，并通过融合一般和专业模型来进一步引入合奏方法，以解决OpenAD基准测试现有的Open-World方法中较低精度的问题。注释，工具包代码和所有评估代码将发布。

### Video-Guided Foley Sound Generation with Multimodal Controls 
[[arxiv](https://arxiv.org/abs/2411.17698)] [[cool](https://papers.cool/arxiv/2411.17698)] [[pdf](https://arxiv.org/pdf/2411.17698)]
> **Authors**: Ziyang Chen,Prem Seetharaman,Bryan Russell,Oriol Nieto,David Bourgin,Andrew Owens,Justin Salamon
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: Project site: https://ificl.github.io/MultiFoley/
- **标题**: 视频引导的Foley Sound Generation具有多模式控件
- **领域**: 计算机视觉和模式识别,多媒体,声音,音频和语音处理
- **摘要**: 为视频产生声音效果通常需要创建艺术声音效果，这些效果与现实生活中的来源和声音设计中的灵活控制显着不同。为了解决这个问题，我们介绍了Multifoley，这是一种用于视频引导的声音生成的模型，该模型通过文本，音频和视频支持多模式调节。鉴于无声的视频和文本提示，多fireto允许用户创建干净的声音（例如，滑板车轮在没有风噪声的情况下旋转）或更异想天开的声音（例如，使狮子的咆哮声听起来像猫的喵喵叫声）。 Multifoley还允许用户从声音效果（SFX）库或部分视频中选择参考音频。我们模型的主要新颖性在于其在具有低质量音频和专业SFX录音的Internet视频数据集上的联合培训，从而使高质量的全频道（48kHz）音频产生。通过自动化评估和人类研究，我们证明了多叶成功在各种条件输入中成功地产生了同步的高质量声音，并且优于现有方法。请参阅我们的项目页面以获取视频结果：https：//ificl.github.io/multifoley/

### Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration 
[[arxiv](https://arxiv.org/abs/2411.17686)] [[cool](https://papers.cool/arxiv/2411.17686)] [[pdf](https://arxiv.org/pdf/2411.17686)]
> **Authors**: Yuhang Han,Xuyang Liu,Pengxiang Ding,Donglin Wang,Honggang Chen,Qingsen Yan,Siteng Huang
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 重新思考MLLMS的令牌减少：朝着无训练加速的统一范式
- **领域**: 计算机视觉和模式识别
- **摘要**: 为了加快重型多模式大型语言模型（MLLM）的推断，本研究重新考虑了当前无训练令牌减少研究的景观。我们很遗憾地发现，现有方法的关键组成部分紧密地交织在一起，它们的互连和效果仍不清楚，以进行比较，转移和扩展。因此，我们提出了一个统一的“滤波器 - 重音压缩”范式，该范式将令牌还原分解为管道内的三个不同阶段，在允许独特的实现的同时，保持了一致的设计目标和元素。我们还揭开了流行作品的神秘面纱，并将它们集成到我们的范式中以展示其普遍性。最后，我们提供了基于范式中的一组方法，在推理的不同阶段之间达到了速度和准确性之间的平衡。跨10个基准的实验结果表明，我们的方法可以达到82.4％的拖鞋降低，对性能的影响最小，同时超过了最新的无培训方法。我们的项目页面位于https://ficoco-accelerate.github.io/。

### SketchAgent: Language-Driven Sequential Sketch Generation 
[[arxiv](https://arxiv.org/abs/2411.17673)] [[cool](https://papers.cool/arxiv/2411.17673)] [[pdf](https://arxiv.org/pdf/2411.17673)]
> **Authors**: Yael Vinker,Tamar Rott Shaham,Kristine Zheng,Alex Zhao,Judith E Fan,Antonio Torralba
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: project page: https://sketch-agent.csail.mit.edu/
- **标题**: 草图：语言驱动的顺序草图生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 素描是一种用于外部想法的多功能工具，实现了跨越各种学科的快速探索和视觉交流。尽管人工系统在内容创建和人类计算机的互动方面取得了重大进展，但捕获人类素描的动态和抽象性质仍然具有挑战性。在这项工作中，我们介绍了Sketchagent，这是一种语言驱动的顺序草图生成方法，使用户能够通过动态的，对话性交互来创建，修改和完善草图。我们的方法不需要培训或微调。取而代之的是，我们利用了现成的多模式模型（LLM）的顺序性质和丰富的先验知识。我们提出了一种直观的素描语言，该语言是通过封闭式示例介绍给模型的，从而使其能够使用基于字符串的操作“绘制”。将它们处理到矢量图形中，然后渲染以在像素帆布上创建草图，可以再次访问该素描以进行进一步的任务。通过中风绘制中风，我们的经纪人捕获了素描固有的不断发展的动态品质。我们证明，Sketchagent可以从各种提示中生成草图，进行对话驱动的绘图，并与人类用户有意义地合作。

### SAMWISE: Infusing wisdom in SAM2 for Text-Driven Video Segmentation 
[[arxiv](https://arxiv.org/abs/2411.17646)] [[cool](https://papers.cool/arxiv/2411.17646)] [[pdf](https://arxiv.org/pdf/2411.17646)]
> **Authors**: Claudia Cuttano,Gabriele Trivigno,Gabriele Rosi,Carlo Masone,Giuseppe Averta
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: Samwise：在SAM2中注入文本驱动视频细分中的智慧
- **领域**: 计算机视觉和模式识别
- **摘要**: 引用视频对象细分（RVO）依赖于自然语言表达式来分割视频剪辑中的对象。现有方法将推理限制在独立的短片段，失去全球环境或离线处理整个视频，以流媒体方式损害其应用程序。在这项工作中，我们旨在超越这些局限性并设计一种能够在类似流的场景中有效运行的RVOS方法，同时保留过去框架的上下文信息。我们基于细分2（SAM2）模型，该模型提供了可靠的分割和跟踪功能，并且自然适合流处理。我们通过在功能提取阶段以自然的语言理解和明确的时间建模来使SAM2变得更加明智，而无需微调其权重，而没有将模态互动外包到外部模型。为此，我们引入了一个新型的适配器模块，该模块在特征提取过程中注入时间信息和多模式线索。我们进一步揭示了跟踪SAM2中跟踪偏差的现象，并提出了一个可学习的模块，以调整其跟踪焦点时，当当前帧功能提出与标题更加一致的新对象时。我们提出的方法Samwise通过添加仅4.2 m参数的顶部可忽略不计，从而在各种基准中实现了最先进的方法。该代码可从https://github.com/claudiacuttano/samwise获得

### Pre-training for Action Recognition with Automatically Generated Fractal Datasets 
[[arxiv](https://arxiv.org/abs/2411.17584)] [[cool](https://papers.cool/arxiv/2411.17584)] [[pdf](https://arxiv.org/pdf/2411.17584)]
> **Authors**: Davyd Svyezhentsev,George Retsinas,Petros Maragos
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 使用自动生成的分形数据集进行动作识别的预训练
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，对合成数据的兴趣已经增长，尤其是在预训练图像方式的背景下，以支持一系列计算机视觉任务，包括对象分类，医学成像等。这种方法解决了与实际数据相关的问题，例如收集和标签成本，版权和隐私。我们将这种趋势扩展到视频域将其应用于行动识别任务。使用分形几何形状，我们提出了自动生产大型数据集的简短合成视频剪辑的方法，该数据集可用于预训练的神经模型。生成的视频剪辑的特征是著名的变化，由分形生成复杂的多尺度结构的先天能力所造。为了缩小域间隙，我们进一步确定了真实视频的关键特性，并在预训练期间仔细模仿它们。通过彻底的消融，我们确定了加强下游结果的属性，并为合成视频提供预训练的一般指南。通过在既定的动作识别数据集HMDB51和UCF101以及与小组动作识别，细粒度的动作识别和动态场景相关的其他四个其他视频基准中，通过对既定的动作识别数据集HMDB51和UCF101进行微调预训练的模型进行了微调的预培训模型来评估所提出的方法。与标准动力学预训练相比，我们报告的结果接近，甚至在下游数据集的一部分中都出色。合成视频的代码和样本可在https://github.com/davidsvy/fractal_video上找到。

### FLEX-CLIP: Feature-Level GEneration Network Enhanced CLIP for X-shot Cross-modal Retrieval 
[[arxiv](https://arxiv.org/abs/2411.17454)] [[cool](https://papers.cool/arxiv/2411.17454)] [[pdf](https://arxiv.org/pdf/2411.17454)]
> **Authors**: Jingyou Xie,Jiayi Kuang,Zhenzhou Lin,Jiarui Ouyang,Zishuo Zhao,Ying Shen
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: Flex-CLIP：特征级生成网络增强剪辑，用于X-Shot Cross-Modal检索
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 给定一个模式的查询，很少射击的交叉模式检索（CMR）在另一个模式中检索具有目标域的语义上相似的实例，包括与源域不同的类别。与经典的少数CMR方法相比，视觉语言预处理方法（例如剪辑）显示出很大的射击或零射门学习性能。但是，由于（1）目标域中遇到的功能降解以及（2）极端数据不平衡，它们仍然遇到挑战。为了解决这些问题，我们提出了Flex-CLIP，这是一种新颖的功能级生成网络增强的剪辑。 Flex-CLIP包括两个训练阶段。在多模式特征生成中，我们提出了一个复合多模式VAE-GAN网络，以捕获实际特征分布模式并根据剪辑特征生成伪样品，从而解决数据不平衡。对于共同的空间投影，我们开发了一个栅极残留网络，以将剪辑功能与投影功能融合在一起，从而减少X-Shot场景中的功能降解。四个基准数据集的实验结果表明，与最先进的方法相比提高了7％-15％，消融研究表明夹子特征的增强。

### VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models 
[[arxiv](https://arxiv.org/abs/2411.17451)] [[cool](https://papers.cool/arxiv/2411.17451)] [[pdf](https://arxiv.org/pdf/2411.17451)]
> **Authors**: Lei Li,Yuancheng Wei,Zhihui Xie,Xuqing Yang,Yifan Song,Peiyi Wang,Chenxin An,Tianyu Liu,Sujian Li,Bill Yuchen Lin,Lingpeng Kong,Qi Liu
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: Project page: https://vl-rewardbench.github.io
- **标题**: Vlrewardbench：视觉生成奖励模型的具有挑战性的基准
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉生成奖励模型（VL-GENRMS）在对齐和评估多模式AI系统中起着至关重要的作用，但他们自己的评估仍然不足。当前的评估方法主要依赖于传统VL任务的AI-Antotated偏好标签，这些偏好标签可能引入偏见，并且通常无法有效地挑战最新模型。为了解决这些限制，我们引入了VL-RewardBench，这是一个跨越一般多模式查询，视觉幻觉检测和复杂推理任务的全面基准。通过我们的AI辅助注释管道将样本选择与人类验证相结合，我们策划了1,250个高质量示例，专门设计用于探测模型限制。在16个领先的大型视觉模型中进行的全面评估证明了VL-Rewardbench作为一个具有挑战性的测试台的有效性，即使GPT-4O也只能达到65.4％的精度，而最先进的开源模型（例如QWEN2-VL-72B）都难以超越随机驾驶。重要的是，使用VL-GENRMS使用最佳N采样，在VL-Reward-reward台上的性能与MMMU-PRO精确度密切相关（Pearson's R> 0.9）。分析实验发现了改善VL-GENRM的三个关键见解：（i）模型主要在基本的视觉感知任务而不是推理任务下失败； （ii）推理时间缩放效益的益处因模型容量而差异很大； （iii）培训VL-Genrms学会大大提高判断能力（7B VL-GENRM的精度为14.7％）。我们认为，VL-RewardBench以及实验见解将成为推进VL-Genrms的宝贵资源。

### DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters 
[[arxiv](https://arxiv.org/abs/2411.17423)] [[cool](https://papers.cool/arxiv/2411.17423)] [[pdf](https://arxiv.org/pdf/2411.17423)]
> **Authors**: Mingze Sun,Junhao Chen,Junting Dong,Yurun Chen,Xinyu Jiang,Shiwei Mao,Puhua Jiang,Jingbo Wang,Bo Dai,Ruqi Huang
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 驱动器：基于扩散的索具的赋予能力的多功能和表达性字符的生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 生成模型的最新进展已使多模式的高质量3D角色重建。但是，使这些生成的角色动画仍然是一项具有挑战性的任务，尤其是对于不足的大规模数据集和有效的索具方法，尤其是对于服装和头发等复杂元素。为了解决这一差距，我们策划Animerig，这是一个带有详细骨骼和皮肤注释的大型数据集。在此基础上，我们提出了Drive，这是一个新颖的框架，用于生成和索取具有复杂结构的3D人类角色。与现有方法不同，Drive使用3D高斯表示，促进有效的动画和高质量的渲染。我们进一步介绍了GSDIFF，这是一个基于3D高斯的扩散模块，该模块将关节位置作为空间分布，从而克服了基于回归的方法的局限性。广泛的实验表明，驱动器可以实现精确的索具结果，为衣服和头发实现了现实的动力，并超过了质量和多功能性的先前方法。该代码和数据集将在接受后公开供学术使用。

### Multimodal Outer Arithmetic Block Dual Fusion of Whole Slide Images and Omics Data for Precision Oncology 
[[arxiv](https://arxiv.org/abs/2411.17418)] [[cool](https://papers.cool/arxiv/2411.17418)] [[pdf](https://arxiv.org/pdf/2411.17418)]
> **Authors**: Omnia Alwazzan,Amaya Gallagher-Syed,Thomas O. Millner,Sebastian Brandner,Ioannis Patras,Silvia Marino,Gregory Slabaugh
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: Revised to 10 pages, with corrected typos, updated references (some added, others removed), improved figure quality, modified text for better method validation, added one more co-author, and identified the IEEE member
- **标题**: 多模式的外算术块双层次融合整个幻灯片图像和OMICS数据的精确肿瘤学
- **领域**: 计算机视觉和模式识别
- **摘要**: DNA甲基化数据与整个幻灯片图像（WSI）的整合为增强神经病理学中中枢神经系统（CNS）肿瘤分类的诊断精度提供了重要潜力。尽管现有方法通常在融合阶段或晚期融合阶段将编码的OMIC数据与组织学集成在一起，但通过双重融合重新引入OMIC数据的潜力仍未得到探索。在本文中，我们建议在早期和晚期融合期间使用OMIC嵌入，以捕获从局部（贴片级）到全局（幻灯片级）相互作用的互补信息，从而通过多模式集成来提高性能。在早期的融合阶段，OMIC嵌入在潜在空间的WSI斑块上，该嵌合物生成嵌入的嵌入，以封装每个斑点分子和形态学见解。这有效地将OMIC信息纳入了WSI的空间表示中。然后将这些嵌入使用多个实例学习的门控注意机制进行完善，该机制会在诊断贴上进行。在融合阶段晚期，我们通过使用多模式外算术块（MOAB）将其与幻灯片级的OMIC-WSI嵌入来重新引入OMIC数据，从而使两种模态的特征丰富，从而捕获其相关性和互补性。我们证明了在20个细粒亚型中的准确中枢神经系统肿瘤亚型，并在基准数据集上验证了我们的方法，与最先进的方法相比，在TCGA-BLCA上提高了TCGA-BLCA的生存预测和TCGA-BRCA的竞争性能。这种双重融合策略增强了可解释性和分类性能，强调了其临床诊断的潜力。

### Real-Time Multimodal Signal Processing for HRI in RoboCup: Understanding a Human Referee 
[[arxiv](https://arxiv.org/abs/2411.17347)] [[cool](https://papers.cool/arxiv/2411.17347)] [[pdf](https://arxiv.org/pdf/2411.17347)]
> **Authors**: Filippo Ansalone,Flavio Maiorana,Daniele Affinita,Flavio Volpi,Eugenio Bugli,Francesco Petri,Michele Brienza,Valerio Spagnoli,Vincenzo Suriani,Daniele Nardi,Domenico D. Bloisi
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: 11th Italian Workshop on Artificial Intelligence and Robotics (AIRO 2024), Published in CEUR Workshop Proceedings AI*IA Series
- **标题**: Robocup中HRI的实时多模式信号处理：了解人类裁判
- **领域**: 计算机视觉和模式识别,人机交互,机器人技术
- **摘要**: 推进人类机器人的交流对于在动态环境中运行的自主系统至关重要，在动态环境中，对人类信号的准确实时解释至关重要。 Robocup提供了一个令人信服的方案，用于测试这些功能，要求机器人了解裁判手势和吹口哨，并以最小的网络依赖性来了解。使用NAO机器人平台，本研究通过关键点提取和分类实现了两阶段的管道，以及连续的卷积神经网络（CCNN），以进行手势识别，以进行有效的哨声检测。提出的方法在诸如Robocup之类的竞争环境中增强了实时的人类机器人互动，提供了一些工具来推动能够与人类合作的自主系统的开发。

### MotionLLaMA: A Unified Framework for Motion Synthesis and Comprehension 
[[arxiv](https://arxiv.org/abs/2411.17335)] [[cool](https://papers.cool/arxiv/2411.17335)] [[pdf](https://arxiv.org/pdf/2411.17335)]
> **Authors**: Zeyu Ling,Bo Han,Shiyang Li,Hongdeng Shen,Jikang Cheng,Changqing Zou
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: Motionllama：统一的运动综合框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了Motionllama，这是一个统一的运动综合和理解框架，以及一种新型的全身运动令牌机，称为Homi Tokenizer。 Motionllama是根据三个核心原则开发的。首先，它通过Homi Tokenizer建立了强大的统一表示空间。使用单个代码簿，Homi Tokenizer在Motionllama中实现了重建精度，可与剩余矢量量化的量子量相媲美，利用六个代码簿，表现优于所有现有的单代码书籍，以优于所有现有的单代码书籍。其次，Motionllama集成了大型语言模型，以应对各种与运动有关的任务。这种整合桥接了各种方式，促进了全面和复杂的运动综合和理解。第三，Motionllama引入了MotionHub数据集，该数据集目前是最广泛的多模式，多任务运动数据集，该数据集可以对大型语言模型进行微调。广泛的实验结果表明，MotionLlama不仅涵盖了最广泛的运动相关任务范围，而且还涵盖了运动完成的最新表现（SOTA）表现，互动双人互动文本到运动以及所有理解任务，同时达到其余任务中与SOTA相当的绩效。代码和MotionHub数据集可公开使用。

### InsightEdit: Towards Better Instruction Following for Image Editing 
[[arxiv](https://arxiv.org/abs/2411.17323)] [[cool](https://papers.cool/arxiv/2411.17323)] [[pdf](https://arxiv.org/pdf/2411.17323)]
> **Authors**: Yingjing Xu,Jie Kong,Jiazhi Wang,Xiao Pan,Bo Lin,Qiang Liu
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: Insightedit：为图像编辑提供更好的教学
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们专注于基于教学的图像编辑的任务。以前的作品，例如ConstrumentPixPix，Dendert-Diffusion和SmartEdit，已经探索了端到端编辑。但是，仍然存在两个局限性：首先，现有数据集的分辨率低，背景一致性差和过于简单的说明。其次，当前方法主要是在文本上条件，而富裕的图像信息则不足，因此在遵循的复杂说明中较低并保持背景一致性。针对这些问题，我们首先使用新型的数据构建管道策划了高级EDEDIT数据集，从而制定了具有高视觉质量，复杂说明和良好背景一致性的大型数据集。然后，为了进一步注入丰富的图像信息，我们使用功能强大的多模式大型语言模型（MLLM）推理的文本和视觉特征引入了两条桥接机制，以更准确地指导图像编辑过程。广泛的结果表明，我们的方法，Insightedit，实现了最先进的表现，在随后的复杂教学中表现出色，并保持了与原始图像的高背景一致性。

### in-Car Biometrics (iCarB) Datasets for Driver Recognition: Face, Fingerprint, and Voice 
[[arxiv](https://arxiv.org/abs/2411.17305)] [[cool](https://papers.cool/arxiv/2411.17305)] [[pdf](https://arxiv.org/pdf/2411.17305)]
> **Authors**: Vedrana Krivokuca Hahn,Jeremy Maceiras,Alain Komaty,Philip Abbet,Sebastien Marcel
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: 8 pages, 13 figures, 4 tables
- **标题**: 用于驾驶员识别的车内生物识别（ICARB）数据集：面部，指纹和语音
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提供三个包含面部视频，指纹图像和语音样本的生物识别数据集（ICARB-FACE，ICARB指定，ICARB-VOICE），这些数据集从200名同意志愿者中收集到了汽车中。数据是使用近红外摄像头，两个指纹扫描仪和两个麦克风获得的，而志愿者则坐在汽车的驾驶员座椅上。数据收集是在汽车停在室内和室外时进行的，并且添加了不同的“噪声”，以模拟现实生活中驾驶员识别中可能遇到的非理想生物识别数据捕获。尽管这些数据集专门针对车载生物识别识别，但它们的效用不仅限于汽车环境。可用于研究社区的ICARB数据集可用于：（i）评估和基准的面部，指纹和语音识别系统（我们提供多种评估协议）； （ii）创建多模式伪身份，以训练/测试多模式融合算法； （iii）从生物识别数据中创建演示攻击，以评估演示攻击检测算法； （iv）使用提供的元数据研究生物识别系统中的人口统计学和环境偏见。据我们所知，我们的是最大，最多样化的公共载体生物识别数据集。大多数其他数据集仅包含一种生物识别方式（通常面对），而我们的数据集由三种模式组成，所有模式都在同一汽车环境中获得。此外，ICARB指纹似乎是第一个公开可用的车载指纹数据集。最后，ICARB数据集在200个数据主题中具有罕见的人口统计学多样性水平，包括50/50性别拆分，整个Fitzpatrick-Scale频谱中的肤色和广泛的年龄范围（18-60+）。因此，这些数据集对于推进生物识别技术研究将是有价值的。

### HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator 
[[arxiv](https://arxiv.org/abs/2411.17261)] [[cool](https://papers.cool/arxiv/2411.17261)] [[pdf](https://arxiv.org/pdf/2411.17261)]
> **Authors**: Fan Yang,Ru Zhen,Jianing Wang,Yanhao Zhang,Haoxiang Chen,Haonan Lu,Sicheng Zhao,Guiguang Ding
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: heie：基于MLLM的分层可解释的AIGC映像不可避免的评估器
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: AIGC图像在各个领域都普遍存在，但它们经常遭受诸如人工制品和不自然纹理等质量问题。专业模型旨在预测缺陷区域热图，但面临两个主要挑战：（1）缺乏解释性，无法提供细微缺陷的原因和分析，以及（2）无法利用常识和逻辑推理，导致普遍性不佳。多模式的大语言模型（MLLM）有望更好地理解和推理，但要面临自己的挑战：（1）由于捕获微小细节的局限性，难以在细粒度的缺陷本地化上进行。 （2）在提供精确热图生成所需的像素输出方面的限制。为了应对这些挑战，我们提出了HEIE：一种基于MLLM的新型层次可解释的图像令人难以置信的评估者。我们介绍了由COT驱动的可解释的三位一体评估器，该评估者集成了热图，分数和解释输出，并使用COT将复杂的任务分解为增加难度和增强可解释性的子任务。我们的自适应层次不可使用的映射器与来自LLMS的高级映射器令牌协同较低的图像特征，通过基于不确定性的自适应标记方法实现精确的局部到全球层次层次热图预测。此外，我们提出了一个新的数据集：Instel-Aigi-eval，旨在促进对AIGC图像的可解释性不可及性评估。我们的方法通过广泛的实验证明了最先进的性能。

### DiffSLT: Enhancing Diversity in Sign Language Translation via Diffusion Model 
[[arxiv](https://arxiv.org/abs/2411.17248)] [[cool](https://papers.cool/arxiv/2411.17248)] [[pdf](https://arxiv.org/pdf/2411.17248)]
> **Authors**: JiHwan Moon,Jihoon Park,Jungeun Kim,Jongseong Bae,Hyeongwoo Jeon,Ha Young Kim
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: Project page: https://diffslt.github.io/
- **标题**: DIFFSLT：通过扩散模型增强手语翻译中的多样性
- **领域**: 计算机视觉和模式识别
- **摘要**: 手语翻译（SLT）具有挑战性，因为它涉及将手语视频转换为自然语言。先前的研究优先于多样性。但是，多样性对于在机器翻译中处理词汇和句法歧义至关重要，这表明它可以同样受益于SLT。在这项工作中，我们提出了DIFFSLT，这是一种新型无光泽的SLT框架，它利用扩散模型，可以在保留手语语义语义的同时进行多种翻译。 DIFFSLT以输入视频的视觉特征为条件，将随机噪声转换为目标潜在表示。为了增强视觉调节，我们设计了指导融合模块，该模块充分利用了视觉特征的多级时空信息。我们还介绍了DIFFSLT-P，这是一种diffslt变体，该变体在伪光线和视觉特征上有条件，提供了关键的文本指导并减少了模态差距。结果，DIFFSLT和DIFFSLT-P显着改善了以前无光泽SLT方法的多样性，并在两个SLT数据集上实现最新性能，从而显着提高了翻译质量。

### Grounding-IQA: Multimodal Language Grounding Model for Image Quality Assessment 
[[arxiv](https://arxiv.org/abs/2411.17237)] [[cool](https://papers.cool/arxiv/2411.17237)] [[pdf](https://arxiv.org/pdf/2411.17237)]
> **Authors**: Zheng Chen,Xun Zhang,Wenbo Li,Renjing Pei,Fenglong Song,Xiongkuo Min,Xiaohong Liu,Xin Yuan,Yong Guo,Yulun Zhang
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: Code is available at: https://github.com/zhengchen1999/Grounding-IQA
- **标题**: IQA接地：图像质量评估的多峰语言接地模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的开发可以通过自然语言描述评估图像质量。这种进步允许进行更详细的评估。但是，这些基于MLLM的IQA方法主要依赖于一般上下文描述，有时会限制细粒度的质量评估。为了解决这一限制，我们引入了新的图像质量评估（IQA）任务范式，即IQA。该范式将多模式的参考和接地与IQA整合在一起，以实现更精细的质量感知。具体而言，接地-IQA包括两个子任务：接地-IQA-DESCRIPTION（GIQA-DES）和视觉问题答案（GIQA-VQA）。 GIQA-DES涉及具有精确位置（例如边界框）的详细描述，而GIQA-VQA专注于本地区域的质量质量质量质量。为了实现接地-IQA，我们通过提出的自动注释管道构建了一个相应的数据集GIQA-160K。此外，我们开发了精心设计的基准GIQA板凳。该基准从三个角度全面评估了模型接地-IQA性能：描述质量，VQA准确性和接地精度。实验表明，我们提出的任务范式，数据集和基准测试范围更细粒度的IQA应用程序。代码：https：//github.com/zhengchen1999/grounding-iqa。

### AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM 
[[arxiv](https://arxiv.org/abs/2411.17221)] [[cool](https://papers.cool/arxiv/2411.17221)] [[pdf](https://arxiv.org/pdf/2411.17221)]
> **Authors**: Jiarui Wang,Huiyu Duan,Guangtao Zhai,Juntong Wang,Xiongkuo Min
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: AIGV-Assessor：通过LMM进行基准测试和评估文本到视频的感知质量
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型（LMM）的快速发展导致人工智能生成的视频（AIGV）的快速扩展，这突出了针对AIGV设计的有效视频质量评估（VQA）模型的紧迫需求。当前的VQA模型通常由于存在独特的扭曲，例如不现实的对象，不自然运动或不一致的视觉元素而准确评估AIGV的感知质量。为了应对这一挑战，我们首先提出了AIGVQA-DB，这是一个大规模数据集，其中包括15个高级文本到视频模型生成的36,576个AIGV，使用1,048个不同的提示。使用这些AIGV，设计了一个系统的注释管道，包括评分和排名流程，迄今为止收集了37万专家评级。基于AIGVQA-DB，我们进一步介绍了AIGV-Assessor，这是一种新型的VQA模型，该模型利用时空特征和LMM框架来捕获AIGVS的复杂质量属性，从而准确地预测了精确的视频质量分数和视频配对偏好。通过对AIGVQA-DB和现有AIGV数据库的全面实验，AIGV-Assessor展示了最先进的性能，从多个感知质量维度来看，可以显着超过现有的评分或评估方法。

### Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment 
[[arxiv](https://arxiv.org/abs/2411.17188)] [[cool](https://papers.cool/arxiv/2411.17188)] [[pdf](https://arxiv.org/pdf/2411.17188)]
> **Authors**: Dongping Chen,Ruoxi Chen,Shu Pu,Zhaoyi Liu,Yanru Wu,Caixi Chen,Benlin Liu,Yue Huang,Yao Wan,Pan Zhou,Ranjay Krishna
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 交错的场景图，用于交织的文本和图像生成评估
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 许多现实世界中的用户查询（例如，“如何制作鸡蛋米饭？”）可以从能够通过两个文本步骤带有带有图像的文本步骤的系统中受益，类似于食谱。旨在生成交织的文本和图像的模型在确保这些方式内部和跨这些方式的一致性方面面临挑战。为了应对这些挑战，我们提出了ISG，这是一个致言交织的文本和图像生成的全面评估框架。 ISG利用场景图结构来捕获文本和图像块之间的关系，评估四个粒度级别的响应：整体，结构，块级和特定于图像。这种多层评估允许对一致性，连贯性和准确性进行细微的评估，并提供可解释的问题回馈。与ISG结合使用，我们引入了一个基准测试，ISG基础，包括8个类别和21个子类别的1,150个样本。该基准数据集包括复杂的语言视觉依赖性和金色答案，以有效地评估以视觉为中心的任务，例如样式转移，这是当前模型的挑战性领域。使用ISG基础，我们证明了最近的统一视觉模型在产生交织的内容方面表现不佳。结合单独的语言和图像模型的组成方法比整体级别的统一模型提高了111％，但它们的性能在块和图像级别上保持次优。为了促进未来的工作，我们开发了ISG-Agent，这是一种基线代理商，采用“计划前进”管道来调用工具，实现了122％的绩效提高。

### Learning Robust Anymodal Segmentor with Unimodal and Cross-modal Distillation 
[[arxiv](https://arxiv.org/abs/2411.17141)] [[cool](https://papers.cool/arxiv/2411.17141)] [[pdf](https://arxiv.org/pdf/2411.17141)]
> **Authors**: Xu Zheng,Haiwei Xue,Jialei Chen,Yibo Yan,Lutao Jiang,Yuanhuiyi Lyu,Kailun Yang,Linfeng Zhang,Xuming Hu
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: Work in progress
- **标题**: 通过单峰和跨模式蒸馏学习强大的任何模式分段
- **领域**: 计算机视觉和模式识别
- **摘要**: 同时使用来自多个传感器到训练分段的多模式输入是直观的，但实际上具有挑战性。一个关键的挑战是单峰偏见，多模式分段依赖某些模式，在其他人缺失时会导致性能下降，在现实世界中常见。为此，我们开发了学习可靠的分段器的第一个框架，该框架可以处理视觉方式的任何组合。具体来说，我们首先引入了一种平行的多模式学习策略来学习强大的老师。然后，通过将特征水平知识从多模态转移到任何模式的分段，旨在解决单峰偏置并避免过度依赖特定模式，从而在多量表表示空间中实现了跨模式和单峰蒸馏。此外，提出了预测水平的情态语义蒸馏，以实现分割的语义知识转移。对合成和现实世界多传感器基准的广泛实验表明，我们的方法可以达到卓越的性能。

### DOGE: Towards Versatile Visual Document Grounding and Referring 
[[arxiv](https://arxiv.org/abs/2411.17125)] [[cool](https://papers.cool/arxiv/2411.17125)] [[pdf](https://arxiv.org/pdf/2411.17125)]
> **Authors**: Yinan Zhou,Yuxin Chen,Haokun Lin,Shuyu Yang,Li Zhu,Zhongang Qi,Chen Ma,Ying Shan
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: 20 pages, 13 figures
- **标题**: Doge：朝着多功能的视觉文档接地和参考
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 近年来，多模式大语言模型（MLLM）越来越强调接地和参考能力，以实现详细的理解和灵活的用户互动。但是，在视觉文档理解的领域中，由于细粒度的数据集和全面的基准，这些功能落后于落后。为了填补这一空白，我们提出了文档接地和电子数据引擎（Doge-Ingine），该数据引擎产生了两种类型的高质量细颗粒文档数据：多细分解析数据，以增强基本文本本地化和识别能力；并指导数据来激活MLLM在对话和推理过程中的接地和参考功能。此外，使用我们的引擎，我们构建了Doge Bench，其中包括7个文档类型（图表，海报，PDF文档）的7个接地和参考任务，从而提供了全面的评估，以了解详细的文档理解。此外，利用引擎产生的数据，我们开发了强大的基线模型。此开创性的MLLM能够在文档图像中的多个粒度上准确地引用和接地文本。我们的代码，数据和模型将用于社区发展。

### CrossTracker: Robust Multi-modal 3D Multi-Object Tracking via Cross Correction 
[[arxiv](https://arxiv.org/abs/2411.18850)] [[cool](https://papers.cool/arxiv/2411.18850)] [[pdf](https://arxiv.org/pdf/2411.18850)]
> **Authors**: Lipeng Gu,Xuefeng Yan,Weiming Wang,Honghua Chen,Dingkun Zhu,Liangliang Nan,Mingqiang Wei
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: CrossTracker：通过交叉校正，可靠的多模式3D多模式多对象跟踪
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于摄像头和激光雷达的检测的融合为减轻3D多对象跟踪（MOT）中的跟踪故障提供了一种有希望的解决方案。但是，现有方法主要利用摄像头检测来纠正由潜在的LIDAR检测问题引起的跟踪故障，从而忽略了使用LIDAR数据提炼相机检测的相互益处。该限制植根于其单阶段体系结构，类似于单阶段对象检测器，缺少专用的轨迹改进模块来充分利用互补的多模式信息。为此，我们介绍了CrossTracker，这是一种新颖的在线多模式3D MOT的两阶段范式。 CrossTracker以粗略的方式进行操作，最初产生粗轨迹，然后通过独立的完善过程进行精炼。具体而言，CrossTracker结合了三个必需模块：i）多模式建模（M^3）模块，该模块通过融合多模式信息（图像，点云，甚至从图像中提取的平面几何形状）融合，为后续轨迹生成提供了强大的度量。 ii）一种粗轨迹生成（C-TG）模块，该模块生成初始粗两个流轨迹，iii）轨迹改进（TR）模块，该模块通过相机和激光射线之间的交叉校正来完善粗轨迹。全面的实验表明，我们的交叉越野车的表现优于18个竞争对手，强调了其在利用相机和LIDAR传感器的协同益处的有效性来实现强大的多模式3D MOT。

### Active Data Curation Effectively Distills Large-Scale Multimodal Models 
[[arxiv](https://arxiv.org/abs/2411.18674)] [[cool](https://papers.cool/arxiv/2411.18674)] [[pdf](https://arxiv.org/pdf/2411.18674)]
> **Authors**: Vishaal Udandarao,Nikhil Parthasarathy,Muhammad Ferjad Naeem,Talfan Evans,Samuel Albanie,Federico Tombari,Yongqin Xian,Alessio Tonioni,Olivier J. Hénaff
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: 主动数据策展有效提炼大规模多模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 知识蒸馏（KD）是将大规模模型压缩为较小模型的事实上的标准。先前的工作已经探索了涉及不同目标功能，教师尊重和体重继承的越来越复杂的KD策略。在这项工作中，我们探讨了一种替代但简单的方法 - 主动数据策展作为对比度多模式预处理的有效蒸馏。我们简单的在线选择方法，酸，优于各种模型，数据和计算配置的强大基准。此外，我们发现这种主动的数据策略实际上是与标准KD互补的，并且可以有效地组合以训练高性能的推理效率模型。我们简单且可扩展的预处理框架（Acded）在27个零射击分类和检索任务中取得了最先进的结果，推理掉了11％。我们进一步证明，我们的ACED模型在LIT-DECODER设置中训练生成的多模型模型产生了强大的视觉编码器，从而优于更大的视力编码器，用于捕捉图像和视觉提问的任务。

### DHCP: Detecting Hallucinations by Cross-modal Attention Pattern in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2411.18659)] [[cool](https://papers.cool/arxiv/2411.18659)] [[pdf](https://arxiv.org/pdf/2411.18659)]
> **Authors**: Yudong Zhang,Ruobing Xie,Jiansheng Chen,Xingwu Sun,Zhanhui kang,Yu Wang
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: 18 pages, 5 figures
- **标题**: DHCP：大视觉模型中通过跨模式注意模式检测幻觉
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉模型（LVLM）在复杂的多模式任务上表现出了出色的性能。但是，他们继续遭受重大幻觉问题的困扰，包括对象，属性和关系幻觉。为了准确检测这些幻觉，我们研究了幻觉和非隔离状态之间的跨模式注意模式的变化。利用这些区别，我们开发了一个轻巧的检测器，能够识别幻觉。我们提出的方法是通过跨模式注意模式（DHCP）检测幻觉，它很简单，不需要其他LVLM训练或额外的LVLM推理步骤。实验结果表明，DHCP在幻觉检测中取得了显着的性能。通过对LVLMS中幻觉的识别和分析提供新颖的见解，DHCP有助于提高这些模型的可靠性和可信度。

### GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation 
[[arxiv](https://arxiv.org/abs/2411.18499)] [[cool](https://papers.cool/arxiv/2411.18499)] [[pdf](https://arxiv.org/pdf/2411.18499)]
> **Authors**: Pengfei Zhou,Xiaopeng Peng,Jiajun Song,Chuanhao Li,Zhaopan Xu,Yue Yang,Ziyao Guo,Hao Zhang,Yuqi Lin,Yefei He,Lirui Zhao,Shuo Liu,Tianhua Li,Yuxuan Xie,Xiaojun Chang,Yu Qiao,Wenqi Shao,Kaipeng Zhang
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: 53 pages, 19 figures
- **标题**: 门开口：判断开放式交织的图像文本一代的综合基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在视觉理解和发电任务方面取得了长足的进步。但是，生成交织的图像文本内容仍然是一个挑战，需要集成的多模式理解和发电能力。尽管统一模型的进度提供了新的解决方案，但由于数据大小和多样性限制，现有基准不足以评估这些方法。为了弥合这一差距，我们介绍了大门开口（开口），这是一个全面的基准测试，其中包括56个现实世界中的5,400个高质量的人类宣传实例。开放涵盖了各种各样的每日场景，例如旅行指南，设计和头脑风暴，为挑战交织的生成方法提供了强大的平台。此外，我们提出了一个评估开放式多模式生成方法的法官模型。经过一条新的数据管道培训，我们的Intjudge的协议率为82。与人类判断的42％，表现优于基于GPT的评估者11.34％。开放的广泛实验表明，当前的交织生成方法仍然有很大的改进空间。进一步提出了有关交织图像文本生成的关键发现，以指导下一代模型的发展。开口是在https://opening-benchmark.github.io上开源的。

### Deep Fourier-embedded Network for RGB and Thermal Salient Object Detection 
[[arxiv](https://arxiv.org/abs/2411.18409)] [[cool](https://papers.cool/arxiv/2411.18409)] [[pdf](https://arxiv.org/pdf/2411.18409)]
> **Authors**: Pengfei Lyu,Pak-Hei Yeung,Xiaosheng Yu,Chengdong Wu,Jagath C. Rajapakse
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: 12 pages, 13 figures. Submitted to Journal on April 29, 2024
- **标题**: 用于RGB和热显着对象检测的深傅里叶框架网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 深度学习的快速发展已显着改善了RGB和热（RGB-T）图像的显着对象检测（SOD）。但是，现有的基于深度学习的RGB-T SOD模型遭受了两个主要局限性。首先，具有二次复杂性的基于变压器的模型在计算上昂贵且内存密集型，从而限制了它们在高分辨率双模式特征融合中的应用。其次，即使这些模型收敛到最佳解决方案，预测和地面真相之间仍然存在频率差距。为了克服这些局限性，我们提出了一个纯粹的基于傅立叶变换的模型，即深傅里叶安装的网络（DFENET），以进行准确的RGB-T SOD。为了解决计算复杂性时，我们在处理高分辨率图像时，我们利用了具有线性复杂性快速傅立叶变换的效率来设计三个关键组成部分：（1）模态配位感知的关注，它融合了RGB和热模态，并具有增强的多维表示； （2）频率分解的边缘块，通过深层分解和增强低级特征的频率成分来阐明对象边缘； （3）傅立叶残留通道注意块，该渠道优先考虑高频信息，同时使频道的全局关系对齐。为了减轻频率差距，我们提出了co焦点频率损失，该频率损失在边缘频率重建过程中通过在傅立叶域中的双模式边缘信息进行了动态加权硬度频率。在四个RGB-T SOD基准数据集上进行的广泛实验表明，Dfenet的表现优于15个现有的最先进的RGB-T SOD模型。全面的消融研究进一步验证了我们新提出的组件的价值和有效性。该代码可在https://github.com/joshualpf/dfenet上找到。

### XR-MBT: Multi-modal Full Body Tracking for XR through Self-Supervision with Learned Depth Point Cloud Registration 
[[arxiv](https://arxiv.org/abs/2411.18377)] [[cool](https://papers.cool/arxiv/2411.18377)] [[pdf](https://arxiv.org/pdf/2411.18377)]
> **Authors**: Denys Rozumnyi,Nadine Bertsch,Othman Sbai,Filippo Arcadu,Yuhua Chen,Artsiom Sanakoyeu,Manoj Kumar,Catherine Herold,Robin Kips
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: Accepted to WACV 2025
- **标题**: XR-MBT：通过学习深度点云注册的自学XR的多模式全身跟踪
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在XR（AR/VR）设备中跟踪用户的全身动作是带来真实社会存在感的基本挑战。由于缺乏专用的腿部传感器，目前可用的身体跟踪方法采用合成方法来产生合理的动作，鉴于头部和控制器跟踪的3点信号。为了启用混合现实功能，现代XR设备能够使用可用的传感器与专用的机器学习模型估算耳机周围环境的深度信息。这种自我的深度感应无法直接驱动身体，因为它没有注册，并且由于视野和身体自我闭合而不完整。我们首次提议利用可用的深度传感信号与自我划线相结合，以学习能够在XR设备上实时跟踪全身运动的多模式姿势估计模型。我们演示了如何使用语义点云编码器网络与残留网络进行多模式姿势估计的统一云网络结合使用当前的3点运动合成模型。这些模块以一种自制的方式共同训练，利用了实际未注册的点云和从运动捕获获得的模拟数据的组合。我们将我们的方法与用于XR身体跟踪的几个最新系统进行了比较，并表明我们的方法准确地跟踪了各种身体运动。 XR-MBT首次在XR中跟踪腿，而基于部分身体跟踪的传统合成方法是盲目的。

### ChatRex: Taming Multimodal LLM for Joint Perception and Understanding 
[[arxiv](https://arxiv.org/abs/2411.18363)] [[cool](https://papers.cool/arxiv/2411.18363)] [[pdf](https://arxiv.org/pdf/2411.18363)]
> **Authors**: Qing Jiang,Gen Luo,Yuqin Yang,Yuda Xiong,Yihao Chen,Zhaoyang Zeng,Tianhe Ren,Lei Zhang
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: 35 pages, 19 figures
- **标题**: Chatrex：驯服多模式LLM以供联合感知和理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 感知和理解是计算机视觉的两个支柱。虽然多模式的大语言模型（MLLM）表现出了显着的视觉理解能力，但它们可以说缺乏准确的感知能力，例如阶段的模型QWEN2-VL仅在可可数据集上达到43.9召回率，限制了许多需要认知和理解结合的任务。在这项工作中，我们旨在从模型设计和数据开发角度弥合这种感知差距。我们首先介绍Chatrex，这是一个带有脱钩的知觉设计的MLLM。我们没有让LLM直接预测框坐标，而是将输出框从通用提案网络馈入LLM，从而使其能够输出相应的框索引以表示其检测结果，从而将回归任务转换为LLM更熟练地处理的基于检索的任务。从数据的角度来看，我们构建了一个完全自动化的数据引擎，并构建了Rexverse-2M数据集，该数据集具有多种粒度来支持感知和理解的联合培训。经过三阶段的训练方法，ChatRex表现出强烈的看法和理解性能，这两个功能的结合也解锁了许多有吸引力的应用程序，证明了它们在MLLM中的互补作用。代码可在https://github.com/idea-research/chatrex上找到。

### MvKeTR: Chest CT Report Generation with Multi-View Perception and Knowledge Enhancement 
[[arxiv](https://arxiv.org/abs/2411.18309)] [[cool](https://papers.cool/arxiv/2411.18309)] [[pdf](https://arxiv.org/pdf/2411.18309)]
> **Authors**: Xiwei Deng,Xianchun He,Jiangfeng Bao,Yudan Zhou,Shuhui Cai,Congbo Cai,Zhong Chen
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: 11 pages, 10 figures
- **标题**: MVKETR：胸部CT报告生成具有多视觉感知和知识增强的生成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: CT报告生成（CTRG）旨在自动生成3D卷的诊断报告，以减轻临床医生的工作量并改善患者护理。尽管临床价值，现有作品未能有效地纳入来自多种解剖学观点的诊断信息，并且缺乏与准确可靠诊断至关重要的相关临床专业知识。为了解决这些局限性，我们提出了一种新型的多视图知觉知识增强变压器（MVKETR），以模仿临床医生的诊断工作流程。正如放射科医生首先检查了来自多个平面的CT扫描一样，具有观看感注意力的多视图感知聚合器（MVPA）有效地综合了来自多种解剖学观点的诊断信息。然后，受放射科医生如何进一步参考相关临床记录以指导诊断决策的启发，跨模式知识增强子（CMKE）基于查询量将最相似的报告检索，以将域知识纳入诊断程序。此外，我们使用具有可学习的非线性激活功能的Kolmogorov-Arnold网络（KANS），而不是传统的MLP，作为两个模块的基本构建块，以更好地捕获CT解释中的复杂诊断模式。公共CTRG-CHEST-548K数据集进行的广泛实验表明，我们的方法几乎在几乎所有指标上都超过了先前的最新模型（SOTA）模型。该代码将公开可用。

### Enhancing MMDiT-Based Text-to-Image Models for Similar Subject Generation 
[[arxiv](https://arxiv.org/abs/2411.18301)] [[cool](https://papers.cool/arxiv/2411.18301)] [[pdf](https://arxiv.org/pdf/2411.18301)]
> **Authors**: Tianyi Wei,Dongdong Chen,Yifan Zhou,Xingang Pan
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: 增强基于MMDIT的文本对图像模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 最新的多模式扩散变压器（MMDIT）代表文本到图像模型的尖端技术，很大程度上减轻了以前模型中存在的许多一代问题。但是，我们发现，当输入文本提示包含多个类似语义或外观的主题时，它仍然遭受主题忽视或混合的苦难。我们在MMDIT架构中确定了三个可能导致此问题的歧义：障碍，文本编码器歧义和语义歧义。为了解决这些问题，我们建议通过在早期剥离步骤中通过测试时间优化来修复模棱两可的潜在。详细说明，我们设计了三个损失功能：块对准损失，文本编码器对准损失和重叠损失，每个损失是为了减轻这些歧义而定制的。尽管有重大改进，我们观察到，在产生多个相似主题时，语义歧义仍然存在，因为重叠损失提供的指导不够明确。因此，我们进一步建议重叠在线检测和重新启动抽样策略，以减轻问题。针对类似受试者的新建挑战性数据集的实验结果验证了我们方法的有效性，比现有方法显示出了优越的发电质量和更高的成功率。我们的代码将在https://github.com/wtybest/enmmdit上找到。

### Grid-augmented vision: A simple yet effective approach for enhanced spatial understanding in multi-modal agents 
[[arxiv](https://arxiv.org/abs/2411.18270)] [[cool](https://papers.cool/arxiv/2411.18270)] [[pdf](https://arxiv.org/pdf/2411.18270)]
> **Authors**: Joongwon Chae,Zhenyu Wang,Lian Zhang,Dongmei Yu,Peiwu Qin
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: 14 pages, 11 figures
- **标题**: 网格提示的视觉：一种简单而有效的方法，用于增强多模式代理的空间理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式模型的最新进展表明在对象识别和场景理解中具有令人印象深刻的功能。但是，这些模型通常在精确的空间本地化方面遇到困难 - 现实世界应用的关键能力。受到人类如何使用基于网格的参考的启发，我们建议通过简单的网格覆盖方法介绍明确的视觉位置。通过在输入图像中添加9x9黑色网格模式，我们的方法提供了类似于位置编码在变压器中的工作方式的视觉空间引导，但以明确的视觉形式。可可2017年数据集的实验表明，我们基于网格的方法在本地化准确性方面取得了显着提高，与基线表现相比，IOU增长了107.4％（从0.27到0.56），GIOU（0.18至0.53）提高了194.4％（从0.18到0.53）。通过注意力可视化分析，我们展示了该视觉位置编码如何有助于建模更好的地面空间关系。我们的方法的简单性和有效性使其对于需要准确的空间推理的应用特别有价值，例如机器人操纵，医学成像和自主导航。

### TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability 
[[arxiv](https://arxiv.org/abs/2411.18211)] [[cool](https://papers.cool/arxiv/2411.18211)] [[pdf](https://arxiv.org/pdf/2411.18211)]
> **Authors**: Shimin Chen,Xiaohan Lan,Yitian Yuan,Zequn Jie,Lin Ma
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: timemarker：一种多功能视频llm，可用于长时间和简短的视频理解，具有出色的时间定位能力
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型语言模型（LLMS）的快速发展具有明显的高级多模式大型语言模型（LMM），尤其是在视力语言任务中。但是，现有的视频语言模型通常会忽略精确的时间定位，并在不同长度的视频中挣扎。我们介绍了TimeMarker，这是一种多功能视频LLM，旨在根据视频内容进行高质量对话，强调时间定位。 Timemarker集成了时间分离器令牌以增强时间意识，并准确地标记了视频中的特定时刻。它采用了动态框架采样和自适应令牌合并的任何实力机制，从而有效地处理了短视频和长视频。此外，Timemarker还利用了各种数据集，包括进一步转换与时间相关的视频QA数据集，以增强其时间理解功能。图像和交错数据还用于进一步增强模型的语义感知能力。评估表明，TimeMarker在多个基准测试中实现最先进的性能，在短视频类别中都表现出色。我们的项目页面位于\ url {https://github.com/timemarker-llm/timemarker/}。

### Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning 
[[arxiv](https://arxiv.org/abs/2411.18203)] [[cool](https://papers.cool/arxiv/2411.18203)] [[pdf](https://arxiv.org/pdf/2411.18203)]
> **Authors**: Di Zhang,Junxian Li,Jingdi Lei,Xunzhi Wang,Yujie Liu,Zonglin Yang,Jiatong Li,Weida Wang,Suorong Yang,Jianbo Wu,Peng Ye,Wanli Ouyang,Dongzhan Zhou
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: 16 pages, 11 figures
- **标题**: 评论家V：VLM评论家有助于捕获多模式推理中的VLM错误
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉语言模型（VLM）在多模式推理任务中显示出显着的进步。但是，由于幻觉图像理解或未精制的推理路径等问题，它们仍然经常会产生不准确或无关的响应。为了应对这些挑战，我们介绍了评论家V，这是一个受演员批评范式启发的新颖框架，以提高VLM的推理能力。该框架通过整合两个独立的组件来解开推理过程和批评过程：推理器基于视觉和文本输入生成推理路径，而评论家则提供了建设性的批评来完善这些路径。在这种方法中，推理者根据文本提示产生推理响应，可以根据评论家的反馈来迭代地迭代地演变为策略。从理论上讲，这种互动过程是由加强学习框架驱动的，在该框架中，评论家提供了自然语言批评而不是标量奖励，从而实现了更多细微的反馈，以提高推理者在复杂的推理任务上的能力。评论家模型是使用直接偏好优化（DPO）培训的，利用基于规则的奖励〜（RBR）排名的批评数据集以增强其评论家的能力。评估结果表明，在8个基准测试中，尤其是在推理准确性和效率方面，评论V框架在包括GPT-4V在内的现有方法（包括GPT-4V）大大胜过现有的方法。结合推理者的动态基于文本的策略和从偏好优化的评论家中的建设性反馈，可以实现一个更可靠和上下文敏感的多模式推理过程。我们的方法提供了一种有希望的解决方案，以提高VLM的可靠性，从而提高其在现实的推理多模式应用中的性能，例如自动驾驶和体现的智能。

### Enhancing Visual Reasoning with Autonomous Imagination in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.18142)] [[cool](https://papers.cool/arxiv/2411.18142)] [[pdf](https://arxiv.org/pdf/2411.18142)]
> **Authors**: Jingming Liu,Yumeng Li,Boyuan Xiao,Yichang Jian,Ziang Qin,Tianjia Shao,Yao-Xiang Ding,Kun Zhou
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: 通过多模式大语言模型来增强视觉推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，通过在输入场景中找到视觉线索，提高了MLLM的视觉推理能力，从而将思想链（COT）范式扩展到多模式大语言模型（MLLM）。但是，当前的方法是专门为线索查找在整个推理过程中起主要作用的任务而设计的，从而导致难以处理复杂的视觉场景，在这种情况下，线索发现实际上并没有简化整个推理任务。为了应对这一挑战，我们提出了一个新的视觉推理范式，使MLLM可以根据其推理状态自主将输入场景自主修改为新的场景，以便将COT重新构成，因为COT在一系列想象中的视觉场景中进行了简单的闭环决策和推理步骤，从而导致了自然和一般COT构建。为了实现此范式，我们引入了一个新颖的插件想象空间，其中MLLM通过焦点，忽略和基于其本地推理能力（而无需特定训练）进行操作进行视觉修改。我们通过跨越密集计数的基准测试，简单的拼图拼图解决和对象放置来验证我们的方法，从而挑战了线索发现以外的推理能力。结果验证了尽管现有技术缺乏，但我们的方法使MLLM能够通过自主想象力逐步推论。项目页面：https：//future-item.github.io/autoimagine-site。

## 计算机与社会(cs.CY:Computers and Society)

该领域共有 2 篇论文

### Evaluating GPT-4 at Grading Handwritten Solutions in Math Exams 
[[arxiv](https://arxiv.org/abs/2411.05231)] [[cool](https://papers.cool/arxiv/2411.05231)] [[pdf](https://arxiv.org/pdf/2411.05231)]
> **Authors**: Adriana Caraeni,Alexander Scarlatos,Andrew Lan
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Published in LAK 2025: The 15th International Learning Analytics and Knowledge Conference
- **标题**: 在数学考试中评估GPT-4的分级手写解决方案
- **领域**: 计算机与社会,计算语言学,机器学习
- **摘要**: 生成人工智能（AI）的最新进展已显示出准确对开放式学生的反应进行分级的希望。但是，由于缺乏数据以及将视觉和文本信息结合的挑战，很少有事实探讨了对手写的响应的探讨。在这项工作中，我们利用最先进的多模式AI模型，尤其是GPT-4O，可以自动对大学级数学考试的手写响应进行分级。在概率理论考试中，使用对问题的真正回答，我们使用各种提示技术评估了GPT-4O与人类毕业生的地面真实分数的一致性。我们发现，尽管提供标题可以改善对齐方式，但对于现实世界中的设置，模型的整体准确性仍然太低，这表明该任务的增长空间很大。

### Exploring Capabilities of Time Series Foundation Models in Building Analytics 
[[arxiv](https://arxiv.org/abs/2411.08888)] [[cool](https://papers.cool/arxiv/2411.08888)] [[pdf](https://arxiv.org/pdf/2411.08888)]
> **Authors**: Xiachong Lin,Arian Prabowo,Imran Razzak,Hao Xue,Matthew Amos,Sam Behrens,Flora D. Salim
> **First submission**: 2024-10-27
> **First announcement**: 2024-11-14
> **comment**: 7 pages, 1 figures, and 4 tables
- **标题**: 探索时间序列基础模型的构建分析能力
- **领域**: 计算机与社会,人工智能
- **摘要**: 数字化基础架构与物联网（IoT）网络的日益增长的整合已改变了建筑能源消耗的管理和优化。通过利用基于IoT的监测系统，诸如建筑经理，能源供应商和决策者等利益相关者可以做出以数据为导向的决策以提高能源效率。但是，准确的能量预测和分析面临着持续的挑战，这主要是由于建筑物的固有物理约束以及物联网生成的数据的多样性，异构性质。在这项研究中，我们对两个公开的物联网数据集进行了全面的基准测试，从而在建筑能源分析的背景下评估了时间序列基础模型的性能。我们的分析表明，单模式模型在克服建筑物中数据可变性和物理局限性的复杂性方面表现出巨大的希望，未来的工作着重于优化可持续能源管理的多模式模型。

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

该领域共有 1 篇论文

### Transforming the Hybrid Cloud for Emerging AI Workloads 
[[arxiv](https://arxiv.org/abs/2411.13239)] [[cool](https://papers.cool/arxiv/2411.13239)] [[pdf](https://arxiv.org/pdf/2411.13239)]
> **Authors**: Deming Chen,Alaa Youssef,Ruchi Pendse,André Schleife,Bryan K. Clark,Hendrik Hamann,Jingrui He,Teodoro Laino,Lav Varshney,Yuxiong Wang,Avirup Sil,Reyhaneh Jabbarvand,Tianyin Xu,Volodymyr Kindratenko,Carlos Costa,Sarita Adve,Charith Mendis,Minjia Zhang,Santiago Núñez-Corrales,Raghu Ganti,Mudhakar Srivatsa,Nam Sung Kim,Josep Torrellas,Jian Huang,Seetharami Seelam, et al. (19 additional authors not shown)
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: 70 pages, 27 figures
- **标题**: 将混合云转换为出现的AI工作负载
- **领域**: 分布式、并行和集群计算,人工智能,硬件架构,新兴技术,多代理系统
- **摘要**: 这份白皮书是通过IBM研究与IIDAI研究所内的UIUC研究人员之间的密切合作而开发的，它设想通过创新的，全堆栈的共同设计方法，强调可用性，可用性，可管理性，负担能力，适用性，适应性，效率，效率和可伸缩性，从而改变混合云系统，以满足AI工作负载日益增长的复杂性。通过整合诸如生成和代理AI，跨层自动化和优化，统一控制平面以及可组合和适应性系统体系结构等尖端技术，提议的框架解决了能源效率，性能和成本效益的关键挑战。在成熟时结合量子计算将启用材料科学，气候建模和其他高影响力域的量子加速模拟。学术界和工业之间的协作努力是这一愿景的核心，推动了材料设计和气候解决方案的基础模型，可扩展的多模式数据处理以及增强基于物理的AI模拟器，用于天气预报和碳序列化的应用。研究优先级包括推进AI代理系统，LLM作为抽象（LLMAAA），AI模型优化以及跨异构基础设施，端到端的Edge-Cloud转换，有效的编程模型，中间件和平台，安全基础设施，安全基础设施，应用程序适应性云系统和新量子量的合作工作。这些思想和解决方案涵盖了理论和实践研究问题，需要研究社区的协调意见和支持。这项联合计划旨在将混合云建立为安全，高效和可持续的平台，从而在AI驱动的应用程序和学术界，工业和社会的科学发现中促进突破性。

## 人机交互(cs.HC:Human-Computer Interaction)

该领域共有 9 篇论文

### Generative AI for Accessible and Inclusive Extended Reality 
[[arxiv](https://arxiv.org/abs/2410.23803)] [[cool](https://papers.cool/arxiv/2410.23803)] [[pdf](https://arxiv.org/pdf/2410.23803)]
> **Authors**: Jens Grubert,Junlong Chen,Per Ola Kristensson
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: Presented at the CHI 2024 Workshop "Building a Metaverse for All: Opportunities and Challenges for Future Inclusive and Accessible Virtual Environments", May 11, 2024, Honolulu, Hawaii
- **标题**: 用于可访问和包容性扩展现实的生成型AI
- **领域**: 人机交互,人工智能
- **摘要**: 人工智能生成的内容（AIGC）有可能改变人们如何与虚拟环境互动。在本文中，我们讨论了AIGC为创建包容性和可访问的虚拟环境所面临的潜在好处，但也挑战了挑战。具体而言，我们涉及对3D建模专业知识的需求减少，仅符号的好处以及多模式输入，3D内容编辑以及3D模型可访问性以及基础模型特定的挑战。

### AutoGameUI: Constructing High-Fidelity Game UIs via Multimodal Learning and Interactive Web-Based Tool 
[[arxiv](https://arxiv.org/abs/2411.03709)] [[cool](https://papers.cool/arxiv/2411.03709)] [[pdf](https://arxiv.org/pdf/2411.03709)]
> **Authors**: Zhongliang Tang,Mengchen Tan,Fei Xia,Qingrong Cheng,Hao Jiang,Yongxiang Zhang
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: 27 pages
- **标题**: AutoGameUI：通过多模式学习和基于交互式Web的工具构建高保真游戏UI
- **领域**: 人机交互,人工智能
- **摘要**: 我们介绍了一个创新的系统AutoGameUI，以有效地构建游戏开发中的凝聚力用户界面。我们的系统是第一个解决不一致的UI和UX设计引起的连贯性问题的系统，通常导致不匹配和效率低下。我们提出了一个两阶段的多模式学习管道，以获取UI和UX设计的全面表示，并建立它们的对应关系。通过对应关系，由成对设计自动构建了一个内聚物的用户界面。为了实现高保真效果，我们引入了一种通用数据协议，以进行精确的设计描述和跨平台应用程序。我们还为游戏开发人员开发了一种基于交互式网络的工具，以促进我们的系统使用。我们从实际的游戏项目中创建游戏UI数据集，并将其与公共数据集相结合以进行培训和评估。我们的实验结果证明了我们系统在保持构造界面和原始设计之间保持连贯性方面的有效性。

### Emotion-Aware Interaction Design in Intelligent User Interface Using Multi-Modal Deep Learning 
[[arxiv](https://arxiv.org/abs/2411.06326)] [[cool](https://papers.cool/arxiv/2411.06326)] [[pdf](https://arxiv.org/pdf/2411.06326)]
> **Authors**: Shiyu Duan,Ziyi Wang,Shixiao Wang,Mengmeng Chen,Runsheng Zhang
> **First submission**: 2024-11-09
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 智能用户界面中使用多模式深度学习的情感感知互动设计
- **领域**: 人机交互,机器学习
- **摘要**: 在用户与技术互动无处不在的时代，用户界面（UI）设计的重要性不能被夸大。精心设计的UI不仅可以增强可用性，而且还可以促进更自然，直觉和情感吸引人的体验，从而使技术在日常生活中更容易获得和影响。这项研究通过引入先进的情感识别系统来显着提高UI的情感反应性来解决这一需求。通过通过多支球菌变压器模型集成面部表情，语音和文本数据，该系统可以实时解释复杂的情绪提示，从而使UIS能够与用户更加有效地相互作用。使用公共键入数据集进行验证，我们的模型表明了情绪识别精度和F1分数的实质性改善，表现优于传统方法。这些发现强调了复杂的情感识别在UIS演变中起着至关重要的作用，从而使技术更适合用户需求和情感。这项研究强调了UIS中增强的情商如何不仅与技术创新有关，而且还涉及用户与数字世界之间的更深入，更有意义的联系，最终塑造人们如何在日常生活中与技术互动。

### Generative AI in Multimodal User Interfaces: Trends, Challenges, and Cross-Platform Adaptability 
[[arxiv](https://arxiv.org/abs/2411.10234)] [[cool](https://papers.cool/arxiv/2411.10234)] [[pdf](https://arxiv.org/pdf/2411.10234)]
> **Authors**: J. Bieniek,M. Rahouti,D. C. Verma
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: 13 pages, 4 figures
- **标题**: 多模式用户界面中的生成AI：趋势，挑战和跨平台适应性
- **领域**: 人机交互,人工智能
- **摘要**: 随着人类计算机交互的边界的扩大，生成AI成为重塑用户界面的关键驱动力，为个性化，多模式和跨平台交互引入了新的可能性。这种集成反映了人们对更自适应和直观的用户界面的需求不断增长，这些用户界面可以容纳多种输入类型，例如文本，语音和视频，并在设备上提供无缝的体验。本文探讨了生成AI在现代用户界面中的集成，研究了历史发展，并专注于多模式互动，跨平台的适应性和动态个性化。一个中心主题是界面困境，它解决了为多模式大语言模型设计有效相互作用的挑战，评估了图形，基于语音和沉浸式界面之间的权衡。该论文进一步评估了针对移动平台量身定制的轻量级框架，从而聚焦了移动硬件在启用可扩展多模式AI中的作用。彻底研究了技术和道德挑战，包括保留上下文，隐私问题以及平衡云和设备处理。最后，本文概述了未来的方向，例如情感自适应接口，预测性AI驱动的用户界面和实时协作系统，强调了生成的AI潜力，可以重新定义跨平台的自适应用户以用户为中心的接口。

### AdaptLIL: A Gaze-Adaptive Visualization for Ontology Mapping 
[[arxiv](https://arxiv.org/abs/2411.11768)] [[cool](https://papers.cool/arxiv/2411.11768)] [[pdf](https://arxiv.org/pdf/2411.11768)]
> **Authors**: Nicholas Chow,Bo Fu
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: The paper was submitted without the consent of all authors. It is being withdrawn until full consent is obtained
- **标题**: Adaptlil：本体映射的凝视自适应可视化
- **领域**: 人机交互,人工智能
- **摘要**: 本文展示了Adaptlil，这是一种实时自适应链接列表的列表映射可视化，将眼光视为主要输入源。通过实时系统，深度学习和Web开发应用程序的多模式组合，该系统独特地缩短了图形叠加层（适应），以对链接引入的列表本体论可视化的成对映射，仅基于眼睛的目光，为单个用户提供。

### SCOUT: A Situated and Multi-Modal Human-Robot Dialogue Corpus 
[[arxiv](https://arxiv.org/abs/2411.12844)] [[cool](https://papers.cool/arxiv/2411.12844)] [[pdf](https://arxiv.org/pdf/2411.12844)]
> **Authors**: Stephanie M. Lukin,Claire Bonial,Matthew Marge,Taylor Hudson,Cory J. Hayes,Kimberly A. Pollard,Anthony Baker,Ashley N. Foots,Ron Artstein,Felix Gervits,Mitchell Abrams,Cassidy Henry,Lucia Donatelli,Anton Leuski,Susan G. Hill,David Traum,Clare R. Voss
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: 14 pages, 7 figures
- **标题**: 侦察兵：位置和多模式的人类机器人对话语料库
- **领域**: 人机交互,计算语言学,机器人技术
- **摘要**: 我们介绍了理解交易的位置语料库（SCOUT），这是在协作探索任务领域中人机对话的多模式集合。该语料库是由多个向导的实验构建的，在该实验中，人类参与者对远程定位的机器人进行了口头指示，以移动和收集有关其周围环境的信息。侦察兵在278个对话中包含89,056个话语和310,095个单词，平均每对话320个话语。对话与实验期间可用的多模式数据流对齐：5,785张图像和30张地图。该语料库已被注释，以抽象的含义表示和对话-AMR来确定说话者在发言中的意图和意义，并通过交易单位和关系来跟踪讲话之间的关系以揭示对话结构的模式。我们描述了如何使用语料库及其注释来开发自主的人类机器人系统，并在开放的问题中对人类如何对机器人说话进行研究。我们发布该语料库以加快自主，位置，人类机器人对话的进展，尤其是在导航任务的背景下，需要发现有关环境的详细信息。

### Human-Robot Dialogue Annotation for Multi-Modal Common Ground 
[[arxiv](https://arxiv.org/abs/2411.12829)] [[cool](https://papers.cool/arxiv/2411.12829)] [[pdf](https://arxiv.org/pdf/2411.12829)]
> **Authors**: Claire Bonial,Stephanie M. Lukin,Mitchell Abrams,Anthony Baker,Lucia Donatelli,Ashley Foots,Cory J. Hayes,Cassidy Henry,Taylor Hudson,Matthew Marge,Kimberly A. Pollard,Ron Artstein,David Traum,Clare R. Voss
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: 52 pages, 14 figures
- **标题**: 多模式共同基础的人机对话注释
- **领域**: 人机交互,计算语言学,机器人技术
- **摘要**: 在本文中，我们描述了在人机对话数据上注释的符号表示的发展，以使参与协作，自然语言对话的自主系统可以访问含义的维度，并与人类合作伙伴建立共同点。建立共同基础的一个特殊挑战是在远程对话中（发生在救灾或搜索任务中），其中人和机器人从事陌生环境的联合导航和勘探任务，但由于有限的沟通约束，机器人无法立即共享高质量的视觉信息。进行对话提供了一种有效的沟通方式，可以补充按需或低质量的视觉信息来建立共同点。在这个范式中，我们通过对话-AMR注释来捕捉对话中单个话语的命题语义和单一话语的幻想力量，这是对抽象含义表示的增强。然后，我们捕获了在我们开发多层对话结构注释模式中，内部和跨越扬声器之间的不同话语之间的不同话语的模式。最后，我们开始注释并分析视觉方式为对话提供上下文信息的方式，以克服合作者对环境的理解中的差异。我们通过讨论从注释中实现的用例，架构和系统来结束，这些用例，架构和系统使物理机器人能够在双向对话和导航中自主与人自主互动。

### Lucia: A Temporal Computing Platform for Contextual Intelligence 
[[arxiv](https://arxiv.org/abs/2411.12778)] [[cool](https://papers.cool/arxiv/2411.12778)] [[pdf](https://arxiv.org/pdf/2411.12778)]
> **Authors**: Weizhe Lin,Junxiao Shen
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: Lucia：上下文智能的时间计算平台
- **领域**: 人机交互,人工智能
- **摘要**: 人工智能的快速发展，尤其是通过多模式大语言模型，它重新定义了用户互动，从而实现了上下文富裕和人类般的响应。随着AI成为日常生活不可或缺的一部分，出现了一个新的边界：开发系统不仅了解空间和感官数据，而且还可以解释时间上下文以建立长期的个性化记忆。该报告介绍了Lucia，Lucia是一个开源时间计算平台，旨在通过捕获和利用连续的上下文记忆来增强人类认知。卢西亚（Lucia）引入了一种轻巧，可穿戴的设备，在舒适性和实时数据可访问性中都表现出色，与现有设备区分开来，通常仅优先考虑可穿戴能力或感知功能。通过录制和解释日常活动，Lucia使用户能够访问稳健的时间内存，从而增强认知过程，例如决策和记忆回忆。

### Purrfessor: A Fine-tuned Multimodal LLaVA Diet Health Chatbot 
[[arxiv](https://arxiv.org/abs/2411.14925)] [[cool](https://papers.cool/arxiv/2411.14925)] [[pdf](https://arxiv.org/pdf/2411.14925)]
> **Authors**: Linqi Lu,Yifan Deng,Chuan Tian,Sijia Yang,Dhavan Shah
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 10 pages, 5 figures
- **标题**: Purrfessor：微调的多模式Llava Diet Health Chatbot
- **领域**: 人机交互,人工智能
- **摘要**: 这项研究介绍了Purrfessor，这是一种创新的AI聊天机器人，旨在通过互动，多模式参与提供个性化的饮食指导。 Purrfessor利用食物和营养数据的大型语言助手（LLAVA）模型进行了微调，并采用了人类的循环方法，将视觉膳食分析与上下文建议相结合，以增强用户体验和参与度。我们进行了两项研究，以评估聊天机器人的性能和用户体验：（a）进行了模拟评估和人类验证以检查微型模型的性能； （b）A 2（配置文件：bot vs. PET）通过3（型号：GPT-4对Llava vs. lllava）实验表明，Purrfessor显着增强了用户对护理的看法（$β= 1.59 $，$ p = 0.04 $）和利息（$β= 2.26 $，$ p = 0.001 $ p = 0.001 $ gpt）。此外，用户访谈强调了互动设计细节的重要性，强调需要响应能力，个性化和指导以改善用户参与度。

## 信息检索(cs.IR:Information Retrieval)

该领域共有 10 篇论文

### Combining Financial Data and News Articles for Stock Price Movement Prediction Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.01368)] [[cool](https://papers.cool/arxiv/2411.01368)] [[pdf](https://arxiv.org/pdf/2411.01368)]
> **Authors**: Ali Elahi,Fatemeh Taghvaei
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: 9 pages, 5 figures
- **标题**: 使用大语言模型结合财务数据和新闻文章，以进行股票价格变动预测
- **领域**: 信息检索,计算金融
- **摘要**: 预测金融市场和股票价格变动需要分析公司的绩效，历史性价格变动，特定于行业的事件，以及人为因素（例如社交媒体和新闻报道）的影响。我们假设财务报告（例如损益表，资产负债表和现金流报表），历史价格数据以及最近的新闻文章可以集体代表上述因素。我们以表格格式将财务数据与文本新闻文章相结合，并采用预先培训的大型语言模型（LLMS）来预测市场变动。 LLMS的最新研究表明，他们能够执行表格和文本分类任务，使其成为我们对多模式数据进行分类的主要模型。我们利用检索增强技术来检索并将相关的新闻文章附加到与公司有关的财务指标，并在零，两个和四局设置中提示LLMS。我们的数据集包含从不同来源，历史股票价格和财务报告数据收集的新闻文章，该数据为20家股票市场上不同行业交易量最高的公司提供了最高的新闻报道。我们利用了最近为基于LLM的分类器（包括GPT-3和4和Llama-2和3模型）发布的语言模型。我们介绍了一个基于LLM的分类器，能够使用表格（结构化）和文本（非结构化）数据的组合执行分类任务。通过使用该模型，我们预测了三个月和6个月的加权F1分数在数据集中的转移，加权F1分数为58.5％和59.1％，Matthews相关系数为0.175。

### Proactive Detection and Calibration of Seasonal Advertisements with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.00780)] [[cool](https://papers.cool/arxiv/2411.00780)] [[pdf](https://arxiv.org/pdf/2411.00780)]
> **Authors**: Hamid Eghbalzadeh,Shuai Shao,Saurabh Verma,Venugopal Mani,Hongnan Wang,Jigar Madia,Vitali Karpinchyk,Andrey Malevich
> **First submission**: 2024-10-16
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 具有多模式模型的季节性广告的主动检测和校准
- **领域**: 信息检索
- **摘要**: 无数的因素会影响大型广告交付系统，并影响用户体验和收入。这样的因素是主动检测和校准季节性广告，以帮助提高转换和用户满意度。在本文中，我们介绍了对季节性广告（PDCASA）的积极检测和校准，这是一个研究问题，对于广告排名和推荐社区都很感兴趣，无论是在工业环境还是在研究环境中。我们的论文从该问题的各个角度提供了详细的指南，并以大规模的工业广告排名系统进行了启发。我们分享了我们的发现，包括对问题的清晰陈述及其动机植根于现实世界系统，评估指标，并为现有的挑战，经验教训以及数据注释和机器学习模型的最佳实践赋予了灯光，以解决此问题。最后，我们提出了在这项研究探索期间采取的结论性解决方案：为了检测季节性，我们利用了多模式LLMS（MLMS），在我们的内部基准测试中获得了0.97的F1 TOP F1得分。根据我们的发现，我们设想MLM作为知识蒸馏的老师，机器标签，以及结合和分层的季节性检测系统的一部分，该系统可以通过丰富的季节性信息赋予广告排名系统的能力。

### Efficient and Effective Adaptation of Multimodal Foundation Models in Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2411.02992)] [[cool](https://papers.cool/arxiv/2411.02992)] [[pdf](https://arxiv.org/pdf/2411.02992)]
> **Authors**: Junchen Fu,Xuri Ge,Xin Xin,Alexandros Karatzoglou,Ioannis Arapakis,Kaiwen Zheng,Yongxin Ni,Joemon M. Jose
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: The extension of IISAN in SIGIR2024
- **标题**: 在顺序建议中有效有效适应多模式基础模型
- **领域**: 信息检索,计算机视觉和模式识别
- **摘要**: 多模式基础模型（MFM）通过高级表示学习彻底改变了顺序推荐系统。尽管参数有效的微调（PEFT）通常用于适应这些模型，但研究通常优先考虑参数效率，忽略GPU记忆和训练速度。为了解决这个问题，我们引入了IISAN框架，从而大大提高了效率。但是，IISAN仅限于对称的MFM，以及相同的文本和图像编码器，从而阻止了最先进的大语言模型。为了克服这一点，我们开发了Iisan-versa，这是一种与对称和不对称MFMS兼容的多功能插入式体系结构。 iisan-versa采用了脱钩的PEFT结构，并同时使用了模式间适应。它通过简单而有效的组合层和尺寸变换对准有效地处理不对称性。我们的研究表明，iisan-versa有效地适应了大型文本编码，我们进一步确定了较大编码器通常表现更好的缩放效果。 Iisan-versa在我们定义的多模式方案中还表现出强烈的多功能性，其中包括图像和视频产生的原始标题和字幕。此外，Iisan-versa在Microlens公共基准中取得了最先进的表现。我们将发布我们的代码和数据集，以支持未来的研究。

### Towards Automated Model Design on Recommender Systems 
[[arxiv](https://arxiv.org/abs/2411.07569)] [[cool](https://papers.cool/arxiv/2411.07569)] [[pdf](https://arxiv.org/pdf/2411.07569)]
> **Authors**: Tunhou Zhang,Dehua Cheng,Yuchen He,Zhengxing Chen,Xiaoliang Dai,Liang Xiong,Yudong Liu,Feng Cheng,Yufan Cao,Feng Yan,Hai Li,Yiran Chen,Wei Wen
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: Accepted in ACM Transactions on Recommender Systems. arXiv admin note: substantial text overlap with arXiv:2207.07187
- **标题**: 在推荐系统上迈向自动化模型设计
- **领域**: 信息检索
- **摘要**: 深度学习模型的日益普及为开发基于AI的推荐系统创造了新的机会。使用深层神经网络设计推荐系统需要仔细的体系结构设计，进一步的优化需要广泛的共同设计工作，共同优化模型架构和硬件。设计自动化（例如自动化机器学习（AUTOML））对于充分利用推荐模型设计的潜力，包括模型选择和模型硬件共同设计策略是必要的。我们引入了一种新颖的范式，该范式利用重量共享来探索丰富的解决方案空间。我们的范式创建了一个大型的超级网，以寻找最佳体系结构和共同设计策略，以应对推荐域中数据多模式和异质性的挑战。从模型的角度来看，超级网包括各种操作员，密集的连接性和维度搜索选项。从共同设计的角度来看，它包括内存中的多功能处理（PIM）配置，以产生硬件有效的模型。我们的解决方案空间的规模，异质性和复杂性构成了一些挑战，我们通过提出各种训练和评估超网的技术来解决这些挑战。我们精心设计的模型在三个点击率（CTR）预测基准方面显示出令人鼓舞的结果，在仅专注于建筑搜索时，具有最先进的性能的手动设计和自动制作的模型的表现优于手动设计和自动制作的模型。从共同设计的角度来看，我们在建议模型中实现了2倍的拖失效率，1.8倍的能源效率和1.5倍的性能提高。

### Residual Multi-Task Learner for Applied Ranking 
[[arxiv](https://arxiv.org/abs/2411.09705)] [[cool](https://papers.cool/arxiv/2411.09705)] [[pdf](https://arxiv.org/pdf/2411.09705)]
> **Authors**: Cong Fu,Kun Wang,Jiahua Wu,Yizhou Chen,Guangda Huzhang,Yabo Ni,Anxiang Zeng,Zhiming Zhou
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-15
> **comment**: No comments
- **标题**: 用于应用排名的剩余多任务学习者
- **领域**: 信息检索,机器学习
- **摘要**: 现代电子商务平台极大地依赖于建模各种用户反馈来提供个性化服务。因此，多任务学习已成为其排名系统中不可或缺的一部分。但是，现有的多任务学习方法遇到了两个主要挑战：有些人缺乏对任务关系的明确建模，导致绩效较低，而其他人则由于计算密集型，有可伸缩性问题或依靠强大的假设而具有有限的适用性。为了解决这些限制并更好地适合我们的现实情况，请在Shopee搜索中预先搜索，我们在本文Resflow中介绍了一个轻巧的多任务学习框架，该框架可以通过相应的任务网络层之间的残差连接来实现有效的交叉任务信息共享。来自各种场景和方式的数据集上的广泛实验表明了其优于最先进方法的性能和适应性。 Shopee搜索中的在线A/B测试在大规模工业应用中展示了其实用价值，这证明了OPU（pers-per-user）没有额外的系统延迟的1.29％。现在，RESFLOW已完全部署在Shopee搜索的预制模块中。为了促进有效的在线部署，我们提出了一个新颖的离线度量加权召回@K，它与我们的在线公制OPU保持一致，解决了长期存在的在线在线式公开指标未对准问题。此外，我们建议在对项目进行排名时，从多个任务中融合得分，这表现优于传统的乘法融合。该代码在https://github.com/brunotruthalliance/resflow上发布

### LLM-assisted Explicit and Implicit Multi-interest Learning Framework for Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2411.09410)] [[cool](https://papers.cool/arxiv/2411.09410)] [[pdf](https://arxiv.org/pdf/2411.09410)]
> **Authors**: Shutong Qiao,Chen Gao,Yong Li,Hongzhi Yin
> **First submission**: 2024-11-14
> **First announcement**: 2024-11-15
> **comment**: 10 pages
- **标题**: llm辅助的显式和隐式多功能学习框架，用于顺序推荐
- **领域**: 信息检索
- **摘要**: 当前推荐系统（RS）中的多关系建模主要基于用户行为数据，从多个维度捕获用户兴趣偏好。但是，由于行为数据是隐含的，而且通常很稀疏，因此了解用户复杂而多样的兴趣是一项挑战。最近的研究表明，文本中丰富的语义信息可以有效地补充行为数据的缺陷。尽管如此，小型模型仍然很难直接提取与用户深厚兴趣相关的语义功能。也就是说，如何有效地将语义与行为信息保持一致，以形成对用户利益的更全面和准确的理解已成为一个关键的研究问题。为了解决这个问题，我们提出了一个LLM辅助的显式和隐式多功能学习框架（命名为EIMF），以在两个级别上对用户兴趣进行建模：行为和语义。该框架由两个部分组成：隐式行为兴趣模块（IBIM）和显式语义兴趣模块（ESIM）。 IBIM中传统的多关系RS模型可以从与项目的交互中学习用户的隐性行为兴趣。在ESIM中，我们首先采用聚类算法来选择典型的样本并在LLM上设计促进策略以获得明确的语义兴趣。此外，在训练阶段，典型样本的语义兴趣可以根据语义预测和模态对准的多任务学习来增强行为兴趣的表示。因此，在推论阶段，只有用户的行为数据才能获得准确的建议。对现实世界数据集的广泛实验证明了拟议的EIMF框架的有效性，该框架有效，有效地将小型模型与LLM相结合，以提高多功能建模的准确性。

### Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule Retrieval 
[[arxiv](https://arxiv.org/abs/2411.11875)] [[cool](https://papers.cool/arxiv/2411.11875)] [[pdf](https://arxiv.org/pdf/2411.11875)]
> **Authors**: Zijun Min,Bingshuai Liu,Liang Zhang,Jia Song,Jinsong Su,Song He,Xiaochen Bo
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-19
> **comment**: BIBM 2024 Regular Paper
- **标题**: 探索基于文本分子检索的最佳基于运输的多元元素对齐
- **领域**: 信息检索,人工智能,计算语言学,生物分子
- **摘要**: 生物信息学领域已经取得了重大进展，使得跨模式文本分子检索任务越来越重要。该任务通过有效地对准文本描述和分子来帮助研究人员识别合适的分子候选者，从而基于文本描述准确检索分子结构。但是，许多现有方法忽略了分子子结构中固有的细节。在这项工作中，我们介绍了最佳的基于运输的多透明对准模型（ORMA），这是一种新型方法，可促进文本描述和分子之间的多透明对齐。我们的模型具有文本编码器和分子编码器。文本编码器处理文本描述以生成令牌级别和句子级的表示，而分子被建模为层次异质图，涵盖原子，基序和分子节点以在这三个级别上提取表示表示。 ORMA中的一个关键创新是将最佳传输（OT）应用于将令牌与图案保持一致的应用，从而创建了将多个令牌比对及其相应的图案整合的多型表示。此外，我们采用对比度学习以三个不同的尺度来完善交叉模式对齐：令牌原子，​​多语音 - 摩托福和句子 - 分子，以确保将正确匹配的文本 - 分子对之间的相似性最大化，而无与伦比的成对的成对的相似性被最小化。据我们所知，这是探索主题和多态级别的一致性的首次尝试。 CHEBI-20和PCDES数据集的实验结果表明，ORMA显着优于现有的最新模型（SOTA）模型。

### QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou 
[[arxiv](https://arxiv.org/abs/2411.11739)] [[cool](https://papers.cool/arxiv/2411.11739)] [[pdf](https://arxiv.org/pdf/2411.11739)]
> **Authors**: Xinchen Luo,Jiangxia Cao,Tianyu Sun,Jinkai Yu,Rui Huang,Wei Yuan,Hezheng Lin,Yichen Zheng,Shiyao Wang,Qigen Hu,Changqing Qiu,Jiaqi Zhang,Xu Zhang,Zhiheng Yan,Jingming Zhang,Simin Zhang,Mingxing Wen,Zhaojie Liu,Kun Gai,Guorui Zhou
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: Work in progress
- **标题**: QARM：Kuaishou的定量比对多模式建议
- **领域**: 信息检索,人工智能
- **摘要**: 近年来，随着多模式大型模型的显着演变，许多推荐的研究人员意识到了多模式信息在用户兴趣建模中的潜力。在行业中，广泛的建模体系结构是级联范式：（1）首先预培训多模式模型，以提供下游服务的无所不能表示； （2）下游推荐模型将多模式表示作为额外的输入，以适合真实的用户项目行为。尽管这种范式取得了显着的改进，但是仍然存在两个限制模型性能的问题：（1）表示不匹配：预训练的多模式模型始终受经典的NLP/CV任务的监督，而建议模型则由真实用户项目进行监督。结果，这两个根本上不同的任务的目标相对分开，并且缺乏一致的目标。 （2）表示：生成的多模式表示形式始终存储在缓存商店中，并作为推荐模型的额外固定输入，因此无法通过建议模型梯度更新，进一步不友好地进行下游培训。受到下游任务用法的两个困难挑战的启发，我们引入了一个定量的多模式框架，以自定义不同的下游模型的专业和可训练的多模式信息。

### Consistency Regularization for Complementary Clothing Recommendations 
[[arxiv](https://arxiv.org/abs/2411.12295)] [[cool](https://papers.cool/arxiv/2411.12295)] [[pdf](https://arxiv.org/pdf/2411.12295)]
> **Authors**: Shuiying Liao,P. Y. Mok,Li Li
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 互补服装建议的一致性正规化
- **领域**: 信息检索
- **摘要**: 本文报道了贝叶斯个性化排名（CR-BPR）的一致性正规模型的开发，这些模型解决了现有的互补服装建议方法中的缺点，即一致性有限，并且由多模式数据的特征量表引起的偏见和偏见。与其他产品类型相比，时尚偏好本质上是主观的，更个人化，而时尚通常不是由单个服装产品提供的，而是与其他互补产品相协调的时尚服装。当前的补充产品建议研究主要集中于用户偏好和产品匹配，这项研究进一步强调了在服装匹配的特定情况下，在用户产品交互和产品产品相互作用中观察到的一致性。大多数传统方法通常低估了现有衣柜项目对未来匹配选择的影响，从而导致偏好预测模型较低。此外，许多基于多模式信息的模型忽略了涉及各种特征量表引起的局限性。为了解决这些差距，CR-BPR模型集成了协作过滤技术，以合并用户的喜好和产品匹配建模，并独特地关注每个方面的一致性正则化。此外，合并功能缩放过程进一步解决了由不同特征量表引起的失衡，从而确保模型可以有效地处理多模式数据而不会被任何特定类型的功能偏斜。通过涉及两个基准数据集的详细分析来验证CR-BPR模型的有效性。结果证实，所提出的方法显着优于现有模型。

### Enabling Adoption of Regenerative Agriculture through Soil Carbon Copilots 
[[arxiv](https://arxiv.org/abs/2411.16872)] [[cool](https://papers.cool/arxiv/2411.16872)] [[pdf](https://arxiv.org/pdf/2411.16872)]
> **Authors**: Margaret Capetz,Swati Sharma,Rafael Padilha,Peder Olsen,Jessica Wolk,Emre Kiciman,Ranveer Chandra
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 通过土壤碳副驾驶采用再生农业
- **领域**: 信息检索,人工智能,新兴技术
- **摘要**: 缓解气候变化需要改变农业，以最大程度地减少环境影响并建立气候韧性。再生农业实践增强了土壤有机碳（SOC）水平，从而改善了土壤健康和隔离碳。增加再生农业实践的挑战是廉价地衡量SOC，并了解SOC如何受到再生农业实践以及其他环境因素和农场管理实践的影响。为了应对这一挑战，我们引入了AI驱动的土壤有机碳副铜，该土壤有机碳纤维副铜自动摄入复杂的多分辨率，多模式数据，以提供对土壤健康和再生实践的大规模见解。我们的数据包括极端天气事件数据（例如，干旱和野火事件），农场管理数据（例如农田信息和耕作预测）以及SOC预测。我们发现集成公共数据和专业模型可以实现可持续农业的大规模本地化分析。在比较加利福尼亚县的农业实践的比较中，我们发现有证据表明各种农业活动可能会减轻耕作的负面影响。尽管极端天气条件严重影响SOC，但堆肥可能会减轻SOC损失。最后，实施特定角色的角色赋予农艺师，农场顾问，政策制定者和其他利益相关者的能力，以实施循证策略，以促进可持续的农业并建立气候弹性。

## 机器学习(cs.LG:Machine Learning)

该领域共有 81 篇论文

### AdaFlow: Opportunistic Inference on Asynchronous Mobile Data with Generalized Affinity Control 
[[arxiv](https://arxiv.org/abs/2410.24028)] [[cool](https://papers.cool/arxiv/2410.24028)] [[pdf](https://arxiv.org/pdf/2410.24028)]
> **Authors**: Fenmin Wu,Sicong Liu,Kehao Zhu,Xiaochen Li,Bin Guo,Zhiwen Yu,Hongkai Wen,Xiangrui Xu,Lehao Wang,Xiangyu Liu
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: AdaFlow：具有广义亲和力控制的异步移动数据的机会性推断
- **领域**: 机器学习,人机交互
- **摘要**: 配备了许多传感器的移动设备的兴​​起，例如LiDAR和相机，促使采用多模式深智能来分布式传感任务，例如智能小屋和驾驶帮助。但是，移动感觉数据的到达时间因模态大小和网络动态而有所不同，这可能会导致延迟（如果等待较慢的数据）或准确性下降（如果推断不等待而进行）。此外，移动系统的多样性和动态性质加剧了这一挑战。作为响应，我们提出了异步分布式多模式数据的\ textit {机会}推断的转变，一旦部分数据到达，就可以推断。尽管现有方法着重于优化模态的一致性和互补性（称为模态亲和力），但它们缺乏在开放世界移动环境中控制这种亲和力的方法。 Adaflow先驱使用基于层次分析的归一化矩阵在移动环境中结构化跨模式亲和力的制定。这种方法适应方式的多样性和动态，从不同类型和数量的输入中概括。 AdaFlow采用基于亲和力注意的条件GAN（ACGAN），有助于灵活的数据插补，适应各种模式和下游任务而无需重新训练。实验表明，ADAFLOF将推理潜伏期显着降低79.9 \％，并提高准确性高达61.9 \％，表现优于状态的方法。

### An Information Criterion for Controlled Disentanglement of Multimodal Data 
[[arxiv](https://arxiv.org/abs/2410.23996)] [[cool](https://papers.cool/arxiv/2410.23996)] [[pdf](https://arxiv.org/pdf/2410.23996)]
> **Authors**: Chenyu Wang,Sharut Gupta,Xinyi Zhang,Sana Tonekaboni,Stefanie Jegelka,Tommi Jaakkola,Caroline Uhler
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: 多模式数据的控制分离的信息标准
- **领域**: 机器学习,人工智能,信息论
- **摘要**: 多模式表示学习旨在关联和分解多种方式固有的信息。通过将特定于模式的信息与跨模态共享的信息删除，我们可以提高解释性和鲁棒性，并启用下游任务，例如产生反事实结果。将两种类型的信息分开是具有挑战性的，因为它们通常在许多现实世界中被深深纠缠。我们提出了解开的自我监督学习（DisentangledSSL），这是一种新颖的自我监督方法，用于学习解开表示的表示。我们对每个分离表示的最优性进行了全面分析，尤其是专注于在先前工作中未涵盖的场景，而所谓的最低必要信息（MNI）无法达到无法达到的情况。我们证明，DisentangledSL成功地学习了多个合成和现实世界数据集上的共享和模式特异性特征，并且在各种下游任务上始终优于基准，包括视觉数据的预测任务，以及分子 - 音波型检索到生物学数据的任务。

### Classifier-guided Gradient Modulation for Enhanced Multimodal Learning 
[[arxiv](https://arxiv.org/abs/2411.01409)] [[cool](https://papers.cool/arxiv/2411.01409)] [[pdf](https://arxiv.org/pdf/2411.01409)]
> **Authors**: Zirun Guo,Tao Jin,Jingyuan Chen,Zhou Zhao
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: Accepted at NeurIPS 2024
- **标题**: 分类器引导的梯度调制，以增强多模式学习
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 近年来，多模式学习发展非常快。但是，在多模式训练过程中，该模型倾向于仅依靠一种基于它可以更快地学习的模式，从而导致其他模态的使用不足。平衡训练过程的现有方法始终对损失功能，优化者和模式数量有一定的限制，并且仅考虑调节梯度的大小，同时忽略梯度的方向。为了解决这些问题，在本文中，我们提出了一种新的方法，可以考虑梯度的大小和方向，以平衡多模式学习的梯度调制（CGGM）。我们对四个多模式数据集进行了广泛的实验：UPMC食品101，CMU-MOSI，IEMOCAP和BRATS 2021，涵盖分类，回归和分割任务。结果表明，CGGM始终如一地优于所有基准和其他最先进的方法，以表明其有效性和多功能性。我们的代码可从https://github.com/zrguo/cggm获得。

### Conformalized High-Density Quantile Regression via Dynamic Prototypes-based Probability Density Estimation 
[[arxiv](https://arxiv.org/abs/2411.01266)] [[cool](https://papers.cool/arxiv/2411.01266)] [[pdf](https://arxiv.org/pdf/2411.01266)]
> **Authors**: Batuhan Cengiz,Halil Faruk Karagoz,Tufan Kumbasar
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 通过基于动态原型的概率密度估计的结构化高密度分位回归
- **领域**: 机器学习,机器学习
- **摘要**: 分位数回归中的最新方法采用了分类的观点来处理异质，多模式或偏斜数据提出的挑战，通过将输出量化为固定箱。尽管这些回归 - 分类框架可以捕获高密度的预测区域和绕过凸凸的限制，但由于量化误差和尺寸的诅咒，它们由于每个维度的bin量恒定而受到限制。为了解决这些局限性，我们引入了一种与动态自适应的原型集合相结合的高密度分位回归方法。我们的方法通过在整个训练过程中自适应地添加，删除和重新定量箱来优化原型集。此外，我们的保形方案提供了有效的覆盖范围保证，重点是概率密度最高的区域。跨不同数据集和维度的实验证实，我们的方法始终如一地实现具有增强覆盖范围和鲁棒性的高质量预测区域，同时使用了更少的原型和内存，从而确保对更高维度的可伸缩性。该代码可在https://github.com/batuceng/max_quantile上找到。

### Privacy-Preserving Federated Learning with Differentially Private Hyperdimensional Computing 
[[arxiv](https://arxiv.org/abs/2411.01140)] [[cool](https://papers.cool/arxiv/2411.01140)] [[pdf](https://arxiv.org/pdf/2411.01140)]
> **Authors**: Fardin Jalil Piran,Zhiling Chen,Mohsen Imani,Farhad Imani
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: 28 Pages, 10 Figures
- **标题**: 通过差异私人高维计算
- **领域**: 机器学习,人工智能,密码学和安全,机器学习
- **摘要**: 联合学习（FL）对于物联网（IoT）环境中的有效数据交换至关重要，因为它在本地训练机器学习（ML）模型，并且仅分享模型更新。但是，FL容易受到模型反转和成员推理攻击等隐私威胁的影响，这些威胁可能会暴露敏感的培训数据。为了解决这些隐私问题，经常采用差异隐私（DP）机制。但是，在黑盒ML模型中添加DP噪声会降低性能，尤其是在动态的物联网系统中，随着时间的推移，连续的，终生的FL学习会积累过多的噪声。为了减轻此问题，我们将联合的超维度计算与隐私保护（Fedhdprivacy）一起引入了一个可解释的人工智能（XAI）框架，将神经符号范式与DP结合在一起。 Fedhdprivacy通过从理论上跟踪前一轮的累积噪声，仅添加必要的增量噪声以满足隐私要求，仔细地管理隐私与性能之间的平衡。 In a real-world case study involving in-process monitoring of manufacturing machining operations, FedHDPrivacy demonstrates robust performance, outperforming standard FL frameworks-including Federated Averaging (FedAvg), Federated Stochastic Gradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized Averaging (FedNova), and Federated Adam （Fedadam） - 最高38％。 Fedhdprivacy还显示了未来增强功能的潜力，例如多模式数据融合。

### Text2Freq: Learning Series Patterns from Text via Frequency Domain 
[[arxiv](https://arxiv.org/abs/2411.00929)] [[cool](https://papers.cool/arxiv/2411.00929)] [[pdf](https://arxiv.org/pdf/2411.00929)]
> **Authors**: Ming-Chih Lo,Ching Chang,Wen-Chih Peng
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: 7 pages, 3 figures, and be accepted by NeurIPS 2024 Workshop: Time Series in the Age of Large Models
- **标题**: text2freq：从文本通过频域的学习系列模式
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 传统的时间序列预测模型主要依赖于历史价值来预测未来的结果。尽管这些模型显示出令人鼓舞的结果，但它们通常会忽略其他方式中可获得的丰富信息，例如对特殊事件的文本描述，可以为未来的动态提供至关重要的见解。但是，这些研究可以与其他相关的次数相比，与其他相关的工作相比，该研究将文本纳入了与其他相关性相比。此外，时间序列数据和文本信息之间的模态差距为多模式学习带来了挑战。为了解决此任务，我们提出了Text2Freq，这是一个通过频域集成文本和时间序列数据的跨模式模型。具体而言，我们的方法将文本信息与时间序列数据的低频组件保持一致，从而在这两种方式之间建立了更有效和可解释的一致性。我们对现实世界中股票价格和合成文本的配对数据集进行的实验表明，Text2FReq具有适应性的体系结构，可以实现最新的性能，从而鼓励了该领域的未来研究。

### AAD-LLM: Adaptive Anomaly Detection Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.00914)] [[cool](https://papers.cool/arxiv/2411.00914)] [[pdf](https://arxiv.org/pdf/2411.00914)]
> **Authors**: Alicia Russell-Gilbert,Alexander Sommers,Andrew Thompson,Logan Cummins,Sudip Mittal,Shahram Rahimi,Maria Seale,Joseph Jaboure,Thomas Arnold,Joshua Church
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: AAD-LLM：使用大语言模型的自适应异常检测
- **领域**: 机器学习,人工智能,计算工程、金融和科学
- **摘要**: 对于数据约束，复杂和动态的工业环境，对于增强异常检测的可转移和多模式方法的迫切需要，因此可以防止与系统故障相关的成本。通常，传统的PDM方法是不可传输或多模式的。这项工作研究了在复杂和动态制造系统中使用大型语言模型（LLM）进行异常检测的使用。该研究旨在通过利用大型语言模型（LLM）来提高异常检测模型的可传递性，并试图验证拟议方法在数据 - 帕斯斯工业应用中的有效性增强。该研究还旨在通过允许使用语义来丰富输入系列数据，以在模型和工厂运营商之间实现更多的协作决策。此外，该研究旨在通过整合适应性机制来解决动态工业环境中概念漂移的问题。文献综述研究了LLM时间序列任务中的最新发展以及相关的自适应异常检测方法，以为所提出的体系结构建立强大的理论框架。本文提出了一个新颖的模型框架（AAD-LLM），该框架不需要在应用于且具有多模式的数据集上进行任何培训或填充。结果表明，在数据约束的工业应用中，可以将异常检测转换为“语言”任务，以提供有效的，上下文感知的检测。因此，这项工作为异常检测方法的进步做出了重大贡献。

### LLaMo: Large Language Model-based Molecular Graph Assistant 
[[arxiv](https://arxiv.org/abs/2411.00871)] [[cool](https://papers.cool/arxiv/2411.00871)] [[pdf](https://arxiv.org/pdf/2411.00871)]
> **Authors**: Jinyoung Park,Minseong Bae,Dohwan Ko,Hyunwoo J. Kim
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-04
> **comment**: NeurIPS 2024
- **标题**: Llamo：基于语言模型的大型分子图助手
- **领域**: 机器学习,人工智能,分子网络
- **摘要**: 大型语言模型（LLMS）表现出具有指导调整的概括性和指导遵循功能的显着概括。 LLM和指导调整的进步导致了大型视觉模型（LVLM）的发展。但是，在分子域中，LLM和指令调整的能力较少。因此，我们提出了Llamo：基于大语言模型的分子图助手，这是一个端到端训练的大分子图模型。为了弥合语言和图形模式之间的差异，我们介绍了多级图形投影仪，该投影仪通过将每个GNN层的输出表示和基序表示使用交叉意见机制来将图形表示变成图形令牌。我们还介绍了机器生成的分子图指导数据，以调整通用分子和语言理解的大分子图模型。我们的广泛实验表明，Llamo在各种任务上显示了最佳性能，例如分子描述生成，财产预测和IUPAC名称预测。 Llamo守则可从https://github.com/mlvlab/llamo获得。

### AI in Investment Analysis: LLMs for Equity Stock Ratings 
[[arxiv](https://arxiv.org/abs/2411.00856)] [[cool](https://papers.cool/arxiv/2411.00856)] [[pdf](https://arxiv.org/pdf/2411.00856)]
> **Authors**: Kassiani Papasotiriou,Srijan Sood,Shayleen Reynolds,Tucker Balch
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-04
> **comment**: 9 pages, 5 figures, ICAIF24: 5th ACM International Conference on AI in Finance
- **标题**: 投资分析中的AI：股票评级的LLMS
- **领域**: 机器学习,人工智能,计算金融
- **摘要**: 投资分析是金融服务行业的基石。高级机器学习技术的快速整合，尤其是大型语言模型（LLMS），提供了增强股票评级过程的机会。本文探讨了LLM通过摄入多种数据集来生成多类股票评级的应用。传统的股票评级方法在很大程度上取决于财务分析师的专业知识，并面临一些挑战，例如数据过载，备案不一致以及对市场事件的反应延迟。我们的研究通过利用LLM来提高股票评级的准确性和一致性来解决这些问题。此外，我们评估了将不同的数据模式与LLM一起用于金融领域的功效。我们利用各种数据集，包括2022年1月至2024年6月的基本金融，市场和新闻数据，以及GPT-4-32K（V0613）（V0613）（在2021年9月进行培训截止，以防止信息泄漏）。我们的结果表明，我们的基准方法在通过远期收益评估时，尤其是在合并财务基本面时，我们的基准方法的表现优于传统的库存评级方法。在整合新闻数据的同时，可以改善短期绩效，而用情感分数替换详细的新闻摘要会减少令牌的使用而不会降低绩效。在许多情况下，省略新闻数据完全通过减少偏见来提高性能。我们的研究表明，可以利用LLM有效地利用大量多模式财务数据，这是由于其在股票评级预测任务中的有效性所表明的。我们的工作为产生准确的库存评级提供了可再现和高效的框架，可作为传统方法的经济高效替代品。未来的工作将扩展到更长的时间范围，结合了不同的数据，并利用较新的模型来增强见解。

### Vision-Language Models Can Self-Improve Reasoning via Reflection 
[[arxiv](https://arxiv.org/abs/2411.00855)] [[cool](https://papers.cool/arxiv/2411.00855)] [[pdf](https://arxiv.org/pdf/2411.00855)]
> **Authors**: Kanzhi Cheng,Yantao Li,Fangzhi Xu,Jianbing Zhang,Hao Zhou,Yang Liu
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 视觉语言模型可以通过反思自我突出推理
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 事实证明，经过思考链（COT）可以提高大语言模型（LLMS）的推理能力。但是，由于多模式场景的复杂性以及收集高质量的COT数据的困难，多模式LLM中的COT推理在很大程度上被忽略了。为此，我们提出了一个简单而有效的自我训练框架R3V，它通过反思Cot Priendation来迭代地增强了模型的视觉推理。我们的框架由两个交错的部分组成：（1）迭代地引导启动推理数据集的正面和负面解决方案，以及（2）反思从错误中学习的理由。具体而言，我们介绍了自我挑选和自我选择的损失，使模型能够完善有缺陷的理由，并通过比较候选人的理由来得出正确的答案。对广泛视力语言任务进行的实验表明，R3V始终改善多模式LLM推理，相对于GPT-DISTISTILD填充的基线，相对提高了23％至60％。此外，我们的方法支持对生成的解决方案的自我反思，从而通过测试时间计算进一步提高性能。

### EF-LLM: Energy Forecasting LLM with AI-assisted Automation, Enhanced Sparse Prediction, Hallucination Detection 
[[arxiv](https://arxiv.org/abs/2411.00852)] [[cool](https://papers.cool/arxiv/2411.00852)] [[pdf](https://arxiv.org/pdf/2411.00852)]
> **Authors**: Zihang Qiu,Chaojie Li,Zhongyang Wang,Renyou Xie,Borui Zhang,Huadong Mo,Guo Chen,Zhaoyang Dong
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: EF-LLM：具有AI辅助自动化，增强稀疏预测，幻觉检测的能量预测LLM
- **领域**: 机器学习,人工智能
- **摘要**: 准确的预测有助于实现能源系统中的供需平衡，从而支持决策和调度。传统的模型缺乏AI辅助自动化，依靠专家，高昂的成本以及稀疏的数据预测斗争。为了应对这些挑战，我们提出了能量预测的大语言模型（EF-LLM），该模型集成了域知识和时间序列的时间序列数据，以预测，以支持预先归档前的操作和预先验证后的决策支持。 EF-LLM的人类互动功能降低了预测任务的进入障碍，从而减少了额外的专家参与的需求。为了实现这一目标，我们建议使用可更新的LORA和一个多通道体系结构进行连续的学习方法，以使异构多模式数据保持一致，从而使EF-LLM能够不断学习异质的多模式知识。此外，EF-LLM通过处理多模式数据的能力在稀疏数据条件下实现了准确的预测。我们提出了融合参数有效的微调（F-PEFT）方法，以有效利用时间序列数据和文本为此目的。 EF-LLM也是第一个通过多任务学习，语义相似性分析和ANOVA实现的检测幻觉并量化其发生率的第一个能量特异性LLM。我们在负载，光伏和风能预测的能源预测方案方面取得了成功。

### GWQ: Gradient-Aware Weight Quantization for Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.00850)] [[cool](https://papers.cool/arxiv/2411.00850)] [[pdf](https://arxiv.org/pdf/2411.00850)]
> **Authors**: Yihua Shao,Siyu Liang,Zijian Ling,Minxi Yan,Haiyang Liu,Siyu Chen,Ziyang Yan,Chenyu Zhang,Haotong Qin,Michele Magno,Yang Yang,Zhen Lei,Yan Wang,Jingcai Guo,Ling Shao,Hao Tang
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: GWQ：大语言模型的梯度感知权重量化
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 大型语言模型（LLM）在解决复杂的语言任务时表现出令人印象深刻的表现。但是，它的大量参数对模型在边缘设备上的部署和应用构成了重大挑战。将大型语言模型压缩到低位可以使它们能够在资源受限的设备上运行，这通常会导致性能降级。为了解决这个问题，我们提出了梯度感知的重量量化（GWQ），这是利用梯度定位异常值的低位重量量化方法的第一种量化方法，仅需要最少的校准数据才能进行异常检测。 GWQ以FP16精度优先保留对应于顶部1％异常值的权重，而其余的非外部权重则以低位格式存储。 GWQ在实验上发现，与利用Hessian矩阵定位模型中的敏感权重相比，使用梯度定位模型中的敏感权重更科学。与当前的量化方法相比，GWQ可以应用于多种语言模型，并在Wikitext2和C4数据集上实现较低的PPL。与其他量化方法相比，GWQ量化模型的准确性更高。 GWQ也适用于多模型模型量化，并且量化的QWEN-VL家族模型比其他方法更准确。零射击目标检测任务数据集reccoco的表现优于当前统计数据的方法SPQR。与原始模型相比，GWQ达到了1.2倍的推理速度，并有效地减少了推理内存。

### Exploring Multi-Modality Dynamics: Insights and Challenges in Multimodal Fusion for Biomedical Tasks 
[[arxiv](https://arxiv.org/abs/2411.00725)] [[cool](https://papers.cool/arxiv/2411.00725)] [[pdf](https://arxiv.org/pdf/2411.00725)]
> **Authors**: Laura Wenderoth
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 探索多模式动态：生物医学任务多模式融合的见解和挑战
- **领域**: 机器学习
- **摘要**: 本文研究了Han等人提出的MM动力学方法。 （2022）用于生物医学分类任务中的多模式融合。 MM Dynamics算法将功能级和模态级的信息集成到动态融合方式，以改善分类性能。但是，我们的分析揭示了复制和扩展MM动力学结果的几个局限性和挑战。我们发现，功能信息可以提高性能和解释性，而模式信息性并不能带来显着优势，并且会导致性能降级。基于这些结果，我们对图像数据扩展了特征信息，从而导致图像MM动力学的发展。尽管这种方法显示出有希望的定性结果，但它并没有定量胜过基线方法。

### PedSleepMAE: Generative Model for Multimodal Pediatric Sleep Signals 
[[arxiv](https://arxiv.org/abs/2411.00718)] [[cool](https://papers.cool/arxiv/2411.00718)] [[pdf](https://arxiv.org/pdf/2411.00718)]
> **Authors**: Saurav R. Pandey,Aaqib Saeed,Harlin Lee
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: Pedsleepmae：多模式小儿睡眠信号的生成模型
- **领域**: 机器学习
- **摘要**: 儿科睡眠是健康信息学中重要但经常被忽视的领域。我们提出了Pedsleepmae，这是一种生成模型，该模型完全利用多模式的小儿睡眠信号，包括多通道EEG，呼吸信号，EOG和EMG。这种掩盖的基于自动编码器的模型在睡眠评分和检测呼吸暂停，呼吸呼吸症，脑电图唤醒和氧气去饱和度中的监督学习模型相当。它的嵌入还显示出可捕获来自罕见遗传疾病的睡眠信号的细微差异。此外，Pedsleepmae生成可用于睡眠段检索，离群值检测和缺失通道插补的逼真的信号。这是第一个在多种类型的小儿睡眠信号训练的通用生成模型。

### CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis 
[[arxiv](https://arxiv.org/abs/2411.00696)] [[cool](https://papers.cool/arxiv/2411.00696)] [[pdf](https://arxiv.org/pdf/2411.00696)]
> **Authors**: Fuying Wang,Feng Wu,Yihan Tang,Lequan Yu
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: Technical report
- **标题**: CTPD：增强多模式电子健康记录分析的跨模式的时间模式发现
- **领域**: 机器学习,人工智能
- **摘要**: 整合多模式电子健康记录（EHR）数据，例如数值时间序列和自由文本临床报告，在预测临床结果方面具有很大的潜力。但是，先前的工作主要集中在捕获各个样本中的时间相互作用并融合多模式信息，忽略了患者的关键时间模式。这些模式，例如心率异常或血压等生命体征的趋势，可能表明健康状况恶化或即将发生的关键事件。同样，临床注释通常包含反映这些模式的文本描述。识别不同方式跨不同方式的相应时间模式对于提高临床结果预测的准确性至关重要，但这仍然是一项艰巨的任务。为了解决此差距，我们引入了一个跨模式的时间模式发现（CTPD）框架，该框架旨在从多模式EHR数据中有效提取有意义的跨模式时间模式。我们的方法介绍了共享的初始时间模式表示，这些表示，使用插槽注意来完善，以生成时间语义嵌入。为了确保在学习模式中确保丰富的跨模式时间语义，我们引入了基于对比度的TPNCE损失，以实现跨模式对准，并将两种重建损失丢失，以保留每种模态的核心信息。使用MIMIC-III数据库对两项临床关键任务，48小时的院内死亡率和24小时表型分类进行评估表明，我们方法比现有方法的优越性。

### Analyzing Multimodal Integration in the Variational Autoencoder from an Information-Theoretic Perspective 
[[arxiv](https://arxiv.org/abs/2411.00522)] [[cool](https://papers.cool/arxiv/2411.00522)] [[pdf](https://arxiv.org/pdf/2411.00522)]
> **Authors**: Carlotta Langer,Yasmin Kim Georgie,Ilja Porohovoj,Verena Vanessa Hafner,Nihat Ay
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 从信息理论的角度分析变异自动编码器中的多模式集成
- **领域**: 机器学习,信息论
- **摘要**: 人的感知本质上是多模式的。例如，我们将视觉，本体感受和触觉信息整合到一种体验中。因此，多模式学习对于构建旨在与现实世界进行牢固互动的机器人系统非常重要。多模式整合提出的一个潜在模型是多模式变异自动编码器。差异自动编码器（VAE）由两个网络组成，一个编码器将数据映射到随机潜在空间和一个解码器，从该潜在空间的元素重新构造了此数据。多模式VAE在潜在空间中的两个时间点集成了来自不同方式的输入，因此可以用作机器人剂的控制器。在这里，我们使用此体系结构并介绍信息理论措施，以分析不同模态对重建输入数据的集成的重要性。因此，我们计算了两种不同类型的度量，第一种类型称为单形式误差，并评估单个模式中的信息对于重建该模态或所有模态的重要性。其次，名为“精确度丢失”的度量计算出仅一种模式中缺少信息对这种模式或整个向量重建的影响。 VAE是通过证据下限对VAE进行训练的，可以将其写为两个不同术语的总和，即重建和潜在损失。潜在损失的影响可以通过一个额外的变量加权，该变量已引入以对抗后塌陷。在这里，我们使用四个不同的加权时间表培训网络，并就它们的多模式集成能力进行分析。

### Does the Definition of Difficulty Matter? Scoring Functions and their Role for Curriculum Learning 
[[arxiv](https://arxiv.org/abs/2411.00973)] [[cool](https://papers.cool/arxiv/2411.00973)] [[pdf](https://arxiv.org/pdf/2411.00973)]
> **Authors**: Simon Rampp,Manuel Milling,Andreas Triantafyllopoulos,Björn W. Schuller
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 难度的定义很重要？评分功能及其在课程学习中的作用
- **领域**: 机器学习
- **摘要**: 课程学习（CL）描述了一种机器学习培训策略，在该策略中，样本会根据其难度逐渐引入培训过程中。尽管文献中有部分矛盾的证据，但CL仍在深度学习研究中受欢迎，因为它有望利用人类启发的课程来实现更高的模型性能。然而，很少研究遵循难度的任何必要定义的主观性和偏见，尤其是对于从模型或培训统计数据中得出的订单中发现的偏见。为了更多地了解潜在的未解决问题，我们使用流行的基准数据集CIFAR-10和DCASE2020 2020的声学场景分类，分别是计算机视觉和计算机的代表，对最常见的评分功能的鲁棒性和相似性以及CL中的潜在好处进行了广泛的研究。我们报告了评分功能对训练设置的强烈依赖性，包括随机性，可以通过合奏评分来部分缓解。尽管我们没有发现CL比均匀抽样的总体优势，但我们观察到，在基于CL的训练中显示数据的顺序在模型性能中起着重要作用。此外，我们发现跨随机种子评分功能的鲁棒性与CL性能正相关。最后，我们发现了通过不同策略训练的模型通过延迟融合来增强预测能力，这可能是由于学习概念的差异所致。除了我们的发现外，我们还发布了Aucurriculum Toolkit（https://github.com/autrainer/aucurriculum），以模块化的方式实施样本难度和基于CL的培训。

### Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios 
[[arxiv](https://arxiv.org/abs/2411.02708)] [[cool](https://papers.cool/arxiv/2411.02708)] [[pdf](https://arxiv.org/pdf/2411.02708)]
> **Authors**: Yunkai Dang,Mengxi Gao,Yibo Yan,Xin Zou,Yanggan Gu,Aiwei Liu,Xuming Hu
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 探索MLLM中的反应不确定性：在误导场景下进行的经验评估
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 确保多模式大语言模型（MLLM）保持其响应的一致性对于发展值得信赖的多模式智能至关重要。但是，现有的基准包括许多样本，其中所有mllms \ textit {遇到误导信息时都表现出很高的响应不确定性}，每个样本都需要5-15次响应尝试以有效评估不确定性。因此，我们提出了一个两阶段的管道：首先，我们在没有误导信息的情况下收集MLLM的回答，然后通过特定的误导说明收集误导性的回答。通过计算误导率，并捕获两组响应之间的纠正到不正确的转移和不正确的转移，我们可以有效地衡量模型的响应不确定性。 Eventually, we establish a \textbf{\underline{M}}ultimodal \textbf{\underline{U}}ncertainty \textbf{\underline{B}}enchmark (\textbf{MUB}) that employs both explicit and implicit misleading instructions to comprehensively assess the vulnerability of MLLMs across diverse domains.我们的实验表明，所有开源和封闭源的MLLM都非常容易受到误导性说明的影响，平均误导率超过86 \％。为了增强MLLM的鲁棒性，我们通过合并明确和隐式的误导性数据来进一步调整所有开源MLLM，这表明误导率显着降低。我们的代码可在：\ href {https://github.com/yunkai696/mub} {https://github.com/yyunkai696/mub} {

### See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers 
[[arxiv](https://arxiv.org/abs/2411.02465)] [[cool](https://papers.cool/arxiv/2411.02465)] [[pdf](https://arxiv.org/pdf/2411.02465)]
> **Authors**: Jiaxin Zhuang,Leon Yan,Zhenwei Zhang,Ruiqi Wang,Jiawei Zhang,Yuantao Gu
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: Under review
- **标题**: 看到它，想一想，分类：大型多模型是很少的时间序列异常分析仪
- **领域**: 机器学习,人工智能,机器学习
- **摘要**: 由于各个部门的时间序列数据的快速增长，时间序列异常检测（TSAD）变得越来越重要。例如，Web服务数据中的异常情况可能会发出关键事件（例如系统故障或服务器故障），因此需要及时检测和响应。但是，大多数现有的TSAD方法论都在很大程度上依赖手动功能工程或需要广泛的标记培训数据，同时还提供有限的解释性。为了应对这些挑战，我们介绍了一个名为“时间序列异常多模式分析仪（TAMA）”的开创性框架，该框架利用大型多模型（LMMS）的力量来增强时间序列数据中异常的检测和解释。通过将时间序列转换为LMM可以有效处理的视觉格式，TAMA利用了很少的镜头学习能力来减少对广泛标记的数据集的依赖。通过在多个现实世界数据集上进行严格的实验来验证我们的方法，其中TAMA在TSAD任务中始终超过最先进的方法。此外，TAMA提供了丰富的基于自然语言的语义分析，为检测到的异常性质提供了更深入的见解。此外，我们贡献了最早的开源数据集之一，其中包括异常检测标签，异常类型标签和上下文描述，从而促进了此关键领域内更广泛的探索和进步。归根结底，TAMA不仅在异常检测中出色，而且还提供了一种全面的方法来理解异常的根本原因，从而通过创新的方法和见解推动了TSAD的前进。

### TableGPT2: A Large Multimodal Model with Tabular Data Integration 
[[arxiv](https://arxiv.org/abs/2411.02059)] [[cool](https://papers.cool/arxiv/2411.02059)] [[pdf](https://arxiv.org/pdf/2411.02059)]
> **Authors**: Aofeng Su,Aowen Wang,Chao Ye,Chen Zhou,Ga Zhang,Gang Chen,Guangcheng Zhu,Haobo Wang,Haokai Xu,Hao Chen,Haoze Li,Haoxuan Lan,Jiaming Tian,Jing Yuan,Junbo Zhao,Junlin Zhou,Kaizhe Shou,Liangyu Zha,Lin Long,Liyao Li,Pengzuo Wu,Qi Zhang,Qingyi Huang,Saisai Yang,Tao Zhang, et al. (8 additional authors not shown)
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: TableGPT2：具有表格数据集成的大型多模式模型
- **领域**: 机器学习,人工智能,数据库
- **摘要**: GPT，Claude，Llama和Qwen等模型的出现已经重塑了AI应用，在整个行业中呈现了巨大的新机会。然而，尽管表格数据的集成在众多现实世界中的基本作用，但表格数据的集成仍然显着不发达。由于三个主要原因，此差距至关重要。首先，数据库或数据仓库数据集成对于高级应用程序至关重要。其次，庞大而尚未开发的表格数据资源为分析提供了巨大的潜力。第三，商业智能领域特别要求许多当前LLM可能难以提供的适应性，精确的解决方案。作为响应，我们介绍了TableGPT2，这是一个严格的预先训练和微调的模型，具有超过593.8k的表和236m高质量的查询表释放元素，这是先前研究中前所未有的与桌子相关的数据的规模。这种广泛的培训使TableGpt2能够在以表中为中心的任务中表现出色，同时保持强大的一般语言和编码能力。 TableGPT2的关键创新之一是其新颖的表编码器，专门设计用于捕获模式级别和单元格级信息。该编码器增强了该模型处理模棱两可的查询，缺少列名和不规则表的能力。与视觉语言模型相似，这种开拓性方法与解码器集成在一起，形成了强大的大型多模式模型。我们认为结果令人信服：超过23个基准测量指标，TableGPT2在7B模型中的平均性能提高了35.20％，而72B模型中的平均性能在先前的基准测试中性LLMs中，具有强大的通用通用功能。

### An information-matching approach to optimal experimental design and active learning 
[[arxiv](https://arxiv.org/abs/2411.02740)] [[cool](https://papers.cool/arxiv/2411.02740)] [[pdf](https://arxiv.org/pdf/2411.02740)]
> **Authors**: Yonatan Kurniawan,Tracianne B. Neilsen,Benjamin L. Francis,Alex M. Stankovic,Mingjian Wen,Ilia Nikiforov,Ellad B. Tadmor,Vasily V. Bulatov,Vincenzo Lordi,Mark K. Transtrum
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 最佳实验设计和主动学习的信息匹配方法
- **领域**: 机器学习,材料科学,应用物理,计算物理,数据分析、统计和概率
- **摘要**: 数学模型的功效在很大程度上取决于培训数据的质量，但是收集足够的数据通常是昂贵且具有挑战性的。许多建模应用只需要推断参数作为预测其他含量感兴趣的手段（QOI）。由于模型通常包含许多无法识别的（草率）参数，因此QOI通常取决于相对较少的参数组合。因此，我们基于Fisher Information Matrix介绍了信息匹配标准，以从候选池中选择最有用的培训数据。此方法可确保所选数据包含足够的信息以仅学习约束下游QOI所需的参数。它被配置为凸优化问题，使其可扩展到大型模型和数据集。我们证明了这种方法在包括电力系统和水下声学在内的各种建模问题中的有效性。最后，我们将信息匹配用作材料科学应用的主动学习循环中的查询功能。在所有这些应用程序中，我们发现一组相对较小的最佳培训数据可以为实现精确预测提供必要的信息。这些结果令人鼓舞，尤其是在大型机器学习模型中的积极学习。

### Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level 
[[arxiv](https://arxiv.org/abs/2411.03562)] [[cool](https://papers.cool/arxiv/2411.03562)] [[pdf](https://arxiv.org/pdf/2411.03562)]
> **Authors**: Antoine Grosnit,Alexandre Maraval,James Doran,Giuseppe Paolo,Albert Thomas,Refinath Shahul Hameed Nabeezath Beevi,Jonas Gonzalez,Khyati Khandelwal,Ignacio Iacobacci,Abdelhakim Benechehab,Hamza Cherkaoui,Youssef Attia El-Hili,Kun Shao,Jianye Hao,Jun Yao,Balazs Kegl,Haitham Bou-Ammar,Jun Wang
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: No comments
- **标题**: 大型语言模型精心策划结构化推理达到Kaggle Grandmaster级别
- **领域**: 机器学习,人工智能
- **摘要**: 我们介绍了代理K V1.0，这是一种端到端的自动数据科学代理，旨在自动化，优化和推广到各种数据科学任务。完全自动化的代理K V1.0通过从经验中学习来管理整个数据科学生命周期。它利用一个高度灵活的结构化推理框架使其能够在嵌套结构中动态处理内存，从而有效地从存储的累积经验中学习以处理复杂的推理任务。它通过选择性存储和检索关键信息，根据环境奖励指导未来的决策来优化长期和短期内存。这种迭代方法使其可以在不进行微调或反向传播的情况下完善决策，从而通过体验式学习来持续改进。我们使用Kaggle竞赛作为案例研究来评估代理商的能力。按照全自动协议，代理K V1.0系统地解决了复杂和多模式的数据科学任务，采用贝叶斯优化进行超参数调整和功能工程。我们的新评估框架严格评估代理K V1.0的端到端功能，以从Kaggle竞争URL开始生成和发送提交。结果表明，Agent K V1.0在跨越表格，计算机视觉，NLP和多模式域的任务之间达到了92.5 \％的成功率。通过计算每个人的ELO-MMR得分，对5,856个人类Kaggle竞争对手进行基准测试时，Agent K v1.0在前38％的前38％中排名，表明总体技能水平与专家级用户相当。值得注意的是，它的ELO-MMR得分介于人类大师们达到的第一和第三四分位数之间。此外，我们的结果表明，Agent K V1.0达到了相当于Kaggle Grandmaster的性能水平，其记录是Kaggle的进度系统定义的6枚金，3枚银牌和7枚铜牌。

### Understanding Self-Supervised Learning via Gaussian Mixture Models 
[[arxiv](https://arxiv.org/abs/2411.03517)] [[cool](https://papers.cool/arxiv/2411.03517)] [[pdf](https://arxiv.org/pdf/2411.03517)]
> **Authors**: Parikshit Bansal,Ali Kavis,Sujay Sanghavi
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: No comments
- **标题**: 通过高斯混合模型了解自我监督的学习
- **领域**: 机器学习
- **摘要**: 自我监督的学习尝试从未标记的数据中学习表示形式；它通过损失功能来鼓励嵌入一个点接近其增强的函数。这个简单的想法表现出色，但是从理论上讲，这并不确切地理解为什么是这种情况。在本文中，我们在自然背景下分析了自我监督的学习：高斯混合模型的维度降低。至关重要的是，我们将数据点的增强定义为从同一基础混合物组件中的另一种独立绘制。我们表明，即使高斯人不是各向同性的，香草对比度学习（特别是Infonce损失）也能够找到最佳的低维子空间 - 这是Vanilla Spectral Techniques无法做到的。我们还证明了“非对抗性”自我监督学习（即Simsiam损失）的类似结果。我们进一步将分析扩展到多模式对比度学习算法（例如剪辑）。在这种情况下，我们表明对比度学习学习了Fisher-Timal-Timal子空间的子集，从而有效地滤除了从学习的表示形式中滤除所有噪声。最后，我们通过合成数据实验证实了理论发现。

### On the Comparison between Multi-modal and Single-modal Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2411.02837)] [[cool](https://papers.cool/arxiv/2411.02837)] [[pdf](https://arxiv.org/pdf/2411.02837)]
> **Authors**: Wei Huang,Andi Han,Yongqiang Chen,Yuan Cao,Zhiqiang Xu,Taiji Suzuki
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: 51pages, 1 figure, 1 table
- **标题**: 关于多模式和单模式对比学习之间的比较
- **领域**: 机器学习
- **摘要**: 通过语言监督进行多模式的对比学习，已经展示了现代机器学习的范式转变。通过对网络尺度数据集进行预培训，多模式对比度学习可以学习具有令人印象深刻的鲁棒性和可传递性的高质量表示。尽管取得了经验成功，但理论理解仍处于起步阶段，尤其是与单模式对比度学习的比较。在这项工作中，我们介绍了一个特征学习理论框架，该框架为理解多模式和单模式对比学习之间的差异提供了理论基础。基于由信号和噪声组成的数据生成模型，我们的分析是对具有Infomax目标函数训练的RELU网络进行的。通过基于轨迹的优化分析和对下游任务的概括表征，我们确定了关键因素，即信噪比（SNR），会影响多模式和单模式对比度学习的下游任务中的概括性。通过两种方式之间的合作，多模式学习可以实现更好的特征学习，从而与单模式学习相比，在下游任务的性能方面有所改善。我们的分析提供了一个统一的框架，可以表征单模式和多模式对比学习的优化和概括。合成和现实世界数据集的经验实验进一步巩固了我们的理论发现。

### Layer-Adaptive State Pruning for Deep State Space Models 
[[arxiv](https://arxiv.org/abs/2411.02824)] [[cool](https://papers.cool/arxiv/2411.02824)] [[pdf](https://arxiv.org/pdf/2411.02824)]
> **Authors**: Minseon Gwak,Seongrok Moon,Joohwan Ko,PooGyeon Park
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: NeurIPS 2024, Added missing arXiv information for one reference
- **标题**: 深度状态空间模型的层自适应状态修剪
- **领域**: 机器学习,系统与控制
- **摘要**: 由于缺乏状态维度优化方法，深度状态空间模型（SSM）牺牲了模型容量，培训搜索空间或稳定性，以减轻高状态维度引起的计算成本。在这项工作中，我们为SSM，层自适应状态修剪（最后）提供了一种结构化修剪方法，该方法通过扩展单个系统的模态截断来最小化模型级输出能量损失，从而降低了每一层的状态尺寸。使用$ \ Mathcal {h} _ {\ infty} $ subsystems和layer-wise wise formangranisation评估了最后的分数。该分数是全球修剪标准，从而实现了状态和自适应修剪的跨层比较。在各种序列基准中，最后一次优化了先前的SSM，揭示了其状态空间的冗余性和可压缩性。值得注意的是，我们证明，平均而言，修剪33％的州仍然保持绩效，在多输入多输出SSM中的精度损失为0.52％，而无需重新培训。代码可在https://github.com/msgwak/last上找到。

### Multimodal Structure-Aware Quantum Data Processing 
[[arxiv](https://arxiv.org/abs/2411.04242)] [[cool](https://papers.cool/arxiv/2411.04242)] [[pdf](https://arxiv.org/pdf/2411.04242)]
> **Authors**: Hala Hawashin,Mehrnoosh Sadrzadeh
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: 10 Pages, 16 Figures
- **标题**: 多模式结构感知量子数据处理
- **领域**: 机器学习
- **摘要**: 尽管大型语言模型（LLM）已推进了自然语言处理（NLP）的领域，但他们的“黑匣子”性质掩盖了他们的决策过程。为了解决这个问题，研究人员使用高阶张量开发了结构化方法。这些能够对语言关系进行建模，但是由于其大小过高，因此在经典计算机上训练时停滞了。张量是量子系统的天然居民，量子计算机上的培训通过将文本转换为变异量子电路提供了解决方案。在本文中，我们开发了多模式文本+图像数据的MultiQ-NLP：用于结构感知数据处理的框架。在这里，“结构”是指语言中的句法和语法关系，以及图像中视觉元素的层次结构组织。我们以新的类型和类型的同态丰富了翻译，并开发了代表结构的新型体系结构。当在主流图像分类任务（SVO探针）上进行测试时，我们的最佳模型显示出具有最先进模型的状态的PAR性能。而且，最佳模型是完全结构化的。

### Multi-Scale and Multimodal Species Distribution Modeling 
[[arxiv](https://arxiv.org/abs/2411.04016)] [[cool](https://papers.cool/arxiv/2411.04016)] [[pdf](https://arxiv.org/pdf/2411.04016)]
> **Authors**: Nina van Tiel,Robin Zbinden,Emanuele Dalsasso,Benjamin Kellenberger,Loïc Pellissier,Devis Tuia
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: Published at the CV4Ecology workshop at ECCV 2024 (https://cv4e.netlify.app/papers/06.pdf)
- **标题**: 多尺度和多模式物种分布建模
- **领域**: 机器学习
- **摘要**: 物种分布模型（SDM）旨在通过将发生数据与环境变量相关联，以预测物种的分布。深度学习到SDM的最新应用使新的途径，特别是将空间数据（环境横梁，卫星图像）纳入模型预测因子，使该模型可以考虑围绕每个物种观察结果的空间上下文。但是，图像的适当空间范围并不直接确定并可能影响模型的性能，因为量表被认为是SDMS中的重要因素。我们为SDM开发模块化结构，使我们能够测试单尺度和多尺度设置中比例的效果。此外，我们的模型可以使用晚期融合方法来考虑不同的尺度。 Geolifeclef 2023基准的结果表明，考虑多模式数据和学习多尺度表示形式会导致更准确的模型。

### Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning 
[[arxiv](https://arxiv.org/abs/2411.03978)] [[cool](https://papers.cool/arxiv/2411.03978)] [[pdf](https://arxiv.org/pdf/2411.03978)]
> **Authors**: Jiawei Yao,Qi Qian,Juhua Hu
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: Accepted by NeurIPS 2024
- **标题**: 通过多模式子空间代理学习定制多个聚类
- **领域**: 机器学习
- **摘要**: 多个聚类旨在从不同方面发现数据的各种潜在数据。通过利用数据中的复杂模式和关系，深层多种聚类方法实现了出色的性能。但是，现有的作品难以灵活地适应数据分组中各种用户特定的需求，这可能需要对每个集群进行手动了解。为了解决这些限制，我们引入了多sub，这是一种新颖的端到端多重聚类方法，该方法在这项工作中包含了多模式子空间代理学习框架。使用剪辑和GPT-4的协同功能，多sub将文本提示对准表达用户偏好及其相应的视觉表示。这是通过自动从大型语言模型中自动生成代理单词来实现的，这些词是用作子空间基础的大型语言模型，从而允许按照用户兴趣的术语进行自定义的数据表示。我们的方法始终在视觉多个聚类任务中的广泛数据集中胜过现有基线。我们的代码可在https://github.com/alexander-yao/multi-sub上找到。

### NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA 
[[arxiv](https://arxiv.org/abs/2411.03730)] [[cool](https://papers.cool/arxiv/2411.03730)] [[pdf](https://arxiv.org/pdf/2411.03730)]
> **Authors**: Marlon Tobaben,Mohamed Ali Souibgui,Rubèn Tito,Khanh Nguyen,Raouf Kerkouche,Kangsoo Jung,Joonas Jälkö,Lei Kang,Andrey Barsky,Vincent Poulain d'Andecy,Aurélie Joseph,Aashiq Muhamed,Kevin Kuo,Virginia Smith,Yusuke Yamasaki,Takumi Fukami,Kenta Niwa,Iifan Tyou,Hiro Ishii,Rio Yokota,Ragul N,Rintu Kutum,Josep Llados,Ernest Valveny,Antti Honkela, et al. (2 additional authors not shown)
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: 27 pages, 6 figures
- **标题**: Neurips 2023竞赛：保存联合学习文件的隐私VQA
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别
- **摘要**: 保留联合学习文件VQA（PFL-DOCVQA）竞争的隐私性挑战社区，在联邦设置中为现实生活中的用例开发了可证明的私人和沟通效率的解决方案：发票处理。竞争介绍了一个真实发票文档的数据集，以及需要信息提取和推理文档图像的相关问题和答案。因此，它从文档分析，隐私和联合学习社区中汇集了研究人员和专业知识。参与者对组织者为这个新领域提供的预先训练的，最先进的文档视觉询问模型进行了微调，模仿了典型的联合发票处理设置。基本模型是一种多模式的生成语言模型，可以通过视觉或文本输入模态传播敏感信息。参与者提出了优雅的解决方案，以降低沟通成本，同时保持轨道1中的最低公用事业门槛，并使用曲目2中的差异隐私保护所有信息提供者。该竞赛是开发和测试私人联邦学习方法的新测试台，同时提高了文档图像分析和认可社区中对隐私的认识。最终，竞争分析提供了最佳实践和建议，以成功地在未来以隐私为中心的联邦学习挑战。

### Aligning Large Language Models and Geometric Deep Models for Protein Representation 
[[arxiv](https://arxiv.org/abs/2411.05316)] [[cool](https://papers.cool/arxiv/2411.05316)] [[pdf](https://arxiv.org/pdf/2411.05316)]
> **Authors**: Dong Shu,Bingbing Duan,Kai Guo,Kaixiong Zhou,Jiliang Tang,Mengnan Du
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: 37 pages, 10 figures
- **标题**: 对齐大语言模型和蛋白质表示的几何深层模型
- **领域**: 机器学习,人工智能,计算工程、金融和科学,生物分子
- **摘要**: 潜在表示对准已成为通过将不同模式的嵌入到共享空间中的嵌入来构建多模式大语模型（MLLM）的基础技术，通常与大语言模型（LLMS）的嵌入空间保持一致，以启用有效的交叉模式理解。尽管已经出现了以蛋白质为中心的初步蛋白质MLLM，但它们主要依靠启发式方法，缺乏对跨表示的最佳一致性实践的基本理解。在这项研究中，我们探讨了蛋白质结构域中LLMS和几何深模型（GDM）之间多模式表示的比对。我们全面评估了三个最先进的LLM（Gemma2-2B，Llama3.1-8B和Llama3.1-70B），使用四个蛋白质特异性GDM（Gearnet，GVP，Scannet，GAT）。我们的工作从模型和蛋白质的角度研究了一致性因素，确定了当前一致性方法中的挑战，并提出了改善对齐过程的策略。我们的主要发现表明，将图形和3D结构信息均与LLMS更好地对齐，较大的LLM的GDM表现出提高的对齐能力，并且蛋白质稀有性会显着影响对齐性能。我们还发现，使用两层投影头的GDM嵌入尺寸增加，并且在蛋白质特异性数据上进行微调LLMS可增强对齐质量。这些策略为蛋白质相关多模型的性能提供了潜在的增强。我们的代码和数据可在https://github.com/tizzzzy/llm-gdm-arignment上获得。

### Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder 
[[arxiv](https://arxiv.org/abs/2411.05195)] [[cool](https://papers.cool/arxiv/2411.05195)] [[pdf](https://arxiv.org/pdf/2411.05195)]
> **Authors**: Siting Li,Pang Wei Koh,Simon Shaolei Du
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: 17 pages, 3 figures
- **标题**: 探索具有相同视觉编码器的生成性MLLM的感知比剪辑多得多
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 最近的研究表明，剪辑模型与需要接地构图，理解空间关系或捕获细粒细节的视觉推理任务斗争。一种自然的假设是，剪辑视觉编码器不会嵌入这些任务的基本信息。但是，我们发现并非总是如此：编码器收集与查询相关的视觉信息，而剪辑未能提取它。特别是，我们表明，视觉模型（VLM）的另一个分支，生成的多模式大语言模型（MLLMS），使用相同的视觉编码器和权重中的许多任务中的剪辑中的精度明显更高，这表明这些生成性MLLM的感知更多 - 因为它们提取和更有效地提取和利用了视觉信息。我们进行了一系列受控的实验，并揭示了它们的成功归因于多个关键设计选择，包括补丁令牌，位置嵌入和基于及时的加权。另一方面，单独增强培训数据或应用更强的文本编码器不足以解决任务，而其他文本令牌几乎没有好处。有趣的是，我们发现细粒度的视觉推理并不是由自回归损失训练的生成模型的独特之处：当通过对比较大的芬特键转换为夹子状编码器时，这些MLLM在基于余弦相似性的评估方案下仍然胜过剪辑。我们的研究强调了VLM架构选择的重要性，并提出了提高夹子样VLM的性能的方向。

### OneProt: Towards Multi-Modal Protein Foundation Models 
[[arxiv](https://arxiv.org/abs/2411.04863)] [[cool](https://papers.cool/arxiv/2411.04863)] [[pdf](https://arxiv.org/pdf/2411.04863)]
> **Authors**: Klemens Flöge,Srisruthi Udayakumar,Johanna Sommer,Marie Piraud,Stefan Kesselheim,Vincent Fortuin,Stephan Günneman,Karel J van der Weg,Holger Gohlke,Alina Bazarova,Erinc Merdivan
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: 28 pages, 15 figures, 7 tables
- **标题**: OneProt：朝向多模式蛋白基础模型
- **领域**: 机器学习,生物分子
- **摘要**: 最近的AI进步使多模式系统能够建模和翻译各种信息空间。扩展了文本和视觉之外，我们引入了OneProt，这是一种集成结构，序列，比对和结合位​​点数据的蛋白质的多模式AI。使用ImageBind框架，OneProt将模态编码的潜在空间对准蛋白质序列。它在检索任务中表现出强大的性能，并超过了各种下游任务中的最新方法，包括金属离子结合分类，基因 - 主体学注释和酶功能预测。这项工作扩大了蛋白质模型中的多模式能力，为在药物发现，生物催化反应计划和蛋白质工程中的应用铺平了道路。

### Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2411.04708)] [[cool](https://papers.cool/arxiv/2411.04708)] [[pdf](https://arxiv.org/pdf/2411.04708)]
> **Authors**: Chengxin Hu,Hao Li,Yihe Yuan,Jing Li,Ivor Tsang
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: 9 pages, 4 tables, 1 figure, paper under review
- **标题**: 探索多模式LLM中的分层分子图表示
- **领域**: 机器学习
- **摘要**: 遵循大型语言模型（LLM）和多模型模型的里程碑，我们看到将LLMS应用于生化任务的激增。利用图特征和分子文本表示，LLM可以解决各种任务，例如预测化学反应结果并描述分子特性。但是，即使不同的化学任务可能受益于不同的特征级别，大多数当前工作都忽略了图形模式的 *多层次性质 *。在这项工作中，我们首先研究了特征粒度的效果，并揭示了甚至将所有GNN生成的特征令牌减少到单个的效果也不会显着影响模型性能。然后，我们研究了各种图形特征水平的效果，并证明了LLM生成的分子的质量和跨不同任务的模型性能都取决于不同的图形水平。因此，我们以两个关键见解得出结论：（1）当前与分子相关的多模式LLM缺乏对图形特征的全面理解，并且（2）静态处理不足以使其足以层次结构图。我们详细分享我们的发现，希望为社区开发更先进的多模式LLM的道路，以融合分子图。

### Semantic-Aware Resource Management for C-V2X Platooning via Multi-Agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2411.04672)] [[cool](https://papers.cool/arxiv/2411.04672)] [[pdf](https://arxiv.org/pdf/2411.04672)]
> **Authors**: Zhiyu Shao,Qiong Wu,Pingyi Fan,Kezhi Wang,Qiang Fan,Wen Chen,Khaled B. Letaief
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: This paper has been submitted to IEEE Journal. The source code has been released at:https://github.com/qiongwu86/Semantic-Aware-Resource-Management-for-C-V2X-Platooning-via-Multi-Agent-Reinforcement-Learning
- **标题**: 通过多代理增强学习，用于C-V2X编排的语义感知资源管理
- **领域**: 机器学习,多代理系统,网络和互联网架构,信号处理
- **摘要**: 本文提出了使用多代理增强学习（MARL）的多模式资源分配（SAMRA），用于多任务（MARL），称为SAMRAMARL，利用了排队系统，其中采用了细胞车辆到每次通信（C-V2X）通信。提出的方法利用语义信息来优化通信资源的分配。通过整合分布式多机构增强学习（MARL）算法，Samramarl可以根据传输信息的上下文重要性来实现每辆车的自主决策，频道分配优化，功率分配和语义符号长度。这种语义意识可确保车辆到车辆（V2V）和车辆到基础设施（V2I）通信优先确定数据对维持安全有效的排队操作至关重要的数据。该框架还引入了用于语义通信的量身定制的体验质量（QOE）指标，旨在在V2V链接中最大化QOE，同时提高语义信息传输的成功率（SRS）。广泛的模拟表明，Samramarl的表现优于现有方法，在C-V2X排队场景中实现了QOE和通信效率的显着提高。

### wav2sleep: A Unified Multi-Modal Approach to Sleep Stage Classification from Physiological Signals 
[[arxiv](https://arxiv.org/abs/2411.04644)] [[cool](https://papers.cool/arxiv/2411.04644)] [[pdf](https://arxiv.org/pdf/2411.04644)]
> **Authors**: Jonathan F. Carter,Lionel Tarassenko
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Accepted to Machine Learning for Health (ML4H) 2024
- **标题**: WAV2Sleep：一种从生理信号的统一的多模式方法进行睡眠阶段分类
- **领域**: 机器学习,人工智能
- **摘要**: 从较少令人难以置信的传感器测量值（例如心电图）（ECG）或光摄像机图（PPG）中对睡眠阶段进行准确分类，可以在睡眠医学中实现重要的应用。现有的该问题方法通常使用了设计和培训以在一个或多个特定的输入信号上操作的深度学习模型。但是，用于开发这些模型的数据集通常不包含相同的输入信号集。某些信号，尤其是PPG，比其他信号少得多，此前已经用转移学习等技术来解决。此外，只有对一种或多种固定模式的培训排除了其他来源的跨模式信息转移，这在其他问题域中被证明是有价值的。为了解决这个问题，我们介绍了Wav2Sleep，这是一个统一模型，旨在在训练和推理过程中操作变量的输入信号集。在共同培训了来自六个公开可用的多个多摄影数据集的10,000多个过夜录音之后，包括SHHS和MESA，WAV2SLEEP在包括ECG，PPG和呼吸信号在内的测试时间输入组合跨测试时间输入组合的现有睡眠阶段分类模型。

### Causal Representation Learning from Multimodal Biological Observations 
[[arxiv](https://arxiv.org/abs/2411.06518)] [[cool](https://papers.cool/arxiv/2411.06518)] [[pdf](https://arxiv.org/pdf/2411.06518)]
> **Authors**: Yuewen Sun,Lingjing Kong,Guangyi Chen,Loka Li,Gongxu Luo,Zijian Li,Yixuan Zhang,Yujia Zheng,Mengyue Yang,Petar Stojanov,Eran Segal,Eric P. Xing,Kun Zhang
> **First submission**: 2024-11-10
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 从多模式生物学观察中学习的因果表示
- **领域**: 机器学习,定量方法,方法论
- **摘要**: 多模式数据集在生物学应用（例如人类表型测量）中普遍存在，可以为基本的生物学机制提供宝贵的见解。但是，目前旨在分析此类数据集的机器学习模型仍然缺乏可解释性和理论保证，这对于生物应用至关重要。因果代表学习的最新进展表明，有望通过正式的理论证书揭示可解释的潜在因果变量。不幸的是，现有用于多模式分布的作品要么依赖于限制性参数假设，要么提供相当粗糙的识别结果，从而限制了其适用于生物学研究，从而有利于对机制的详细理解。在这项工作中，我们旨在为多模式数据和原则方法开发灵活的识别条件，以促进对生物数据集的理解。从理论上讲，我们考虑了灵活的非参数潜在分布（C.F.，先前工作中的参数假设），该分布允许跨潜在不同模态的因果关系。我们为每个潜在组件建立可识别性保证，从而扩展了先前工作的子空间识别结果。我们的关键理论成分是不同模式之间因果关系的结构稀疏性，正如我们将要讨论的那样，对于大量的生物系统而言，这是很自然的。从经验上讲，我们提出了一个实用框架来实例化我们的理论见解。我们通过对数值和合成数据集的广泛实验来证明方法的有效性。现实世界中人类表型数据集的结果与既定的医学研究一致，从而验证了我们的理论和方法论框架。

### A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time Series via Visualization 
[[arxiv](https://arxiv.org/abs/2411.06018)] [[cool](https://papers.cool/arxiv/2411.06018)] [[pdf](https://arxiv.org/pdf/2411.06018)]
> **Authors**: Haoxin Liu,Chenghao Liu,B. Aditya Prakash
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 图片值得一千个数字：通过可视化使LLM关于时间序列的原因
- **领域**: 机器学习,人工智能
- **摘要**: 大型语言模型（LLMS）具有多个领域的推理能力，在很大程度上尚未进行时间序列推理（TSR），这在现实世界中无处不在。在这项工作中，我们提出了TimerBed，这是第一个评估LLMS TSR性能的全面测试床。具体而言，计时型包括具有实际任务的分层推理模式，LLM和推理策略的全面组合以及各种监督模型作为比较锚。我们对计时，测试多个当前信念进行广泛的实验，并验证LLM在TSR中的初始失败，这证明了零射击（ZST）的无效性（ZST）和少数射击内部文本学习（ICL）的性能降解。此外，我们确定了一个可能的根本原因：数据的数值建模。为了解决这个问题，我们使用可视化模型数据和语言指导推理提出了一个基于及时的解决方案VL时间。实验结果表明，VL时间使多模式LLMS成为时间序列的非平凡的ZST和强大的ICL推理器，达到了约140％的平均绩效提高，平均代币成本降低了99％。

### Longitudinal Ensemble Integration for sequential classification with multimodal data 
[[arxiv](https://arxiv.org/abs/2411.05983)] [[cool](https://papers.cool/arxiv/2411.05983)] [[pdf](https://arxiv.org/pdf/2411.05983)]
> **Authors**: Aviad Susman,Rupak Krishnamurthy,Yan Chak Li,Mohammad Olaimat,Serdar Bozdag,Bino Varghese,Nasim Sheikh-Bahaei,Gaurav Pandey
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: 11 pages, submitted to ICLR 2025
- **标题**: 纵向集成集成，用于与多模式数据顺序分类
- **领域**: 机器学习,人工智能
- **摘要**: 有效地对多模式纵向数据进行建模是各种应用领域，尤其是生物医学的紧迫需求。尽管如此，文献中对于此问题的文献中很少有方法，大多数方法不足以考虑到数据的多模式。在这项研究中，我们开发了一种新型的多模式和纵向学习框架的多种配置，即纵向集成整合（LEI），以进行顺序分类。我们评估了Lei的性能，并将其与现有方法进行了比较，以便早期发现痴呆症，这是研究最多的多模式顺序分类任务之一。 Lei的表现优于这些方法，因为它使用了由个别数据模式产生的中间基础预测，从而使它们能够随着时间的流逝而更好地集成。 Lei的设计还可以识别在有效预测与痴呆相关诊断的有效预测中始终重要的功能。总体而言，我们的工作证明了LEI对纵向多模式数据的顺序分类的潜力。

### Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model 
[[arxiv](https://arxiv.org/abs/2411.05903)] [[cool](https://papers.cool/arxiv/2411.05903)] [[pdf](https://arxiv.org/pdf/2411.05903)]
> **Authors**: Ben Koska,Mojmír Horváth
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 迈向多模式的精通：4.5b参数真正的多模式小语言模型
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 我们提出了一个新颖的4.5b参数小语言模型，该模型可以处理多个输入和输出方式，包括文本，图像，视频和音频。尽管尺寸很小，但该模型仍在各种任务上达到了几乎最先进的表现，这表明了多模式模型解决复杂的现实世界问题的潜力。我们的方法利用了语言建模和多任务学习的最新进步，以创建一个多功能且高性能的模型，甚至可以将其部署用于边缘推理。实验结果表明该模型在多个基准测试中的出色表现，为多模式人工智能进一步进步铺平了道路。

### Multimodal Fusion Balancing Through Game-Theoretic Regularization 
[[arxiv](https://arxiv.org/abs/2411.07335)] [[cool](https://papers.cool/arxiv/2411.07335)] [[pdf](https://arxiv.org/pdf/2411.07335)]
> **Authors**: Konstantinos Kontras,Thomas Strypsteen,Christos Chatzichristos,Paul Pu Liang,Matthew Blaschko,Maarten De Vos
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: 21 pages, 6 figures, 4 tables, 1 algorithm
- **标题**: 通过游戏理论正则化的多模式融合平衡
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,计算机科学与博弈论,多媒体
- **摘要**: 多模式学习可以通过发现数据源之间的关键依赖性来完成信息提取的图像。但是，当前系统无法完全利用多种方式来获得最佳性能。这归因于模态竞争，在这种竞争中，这种方式努力训练资源，使一些人不满意。我们表明，当前的平衡方法难以训练超过简单基线的多模式模型，例如合奏。这就提出了一个问题：我们如何确保对多模式训练的所有模式进行了足够的培训，并且从新方式中学习始终如一地提高性能？本文提出了多模式竞争规则器（MCR），这是一个新的损失组件，灵感来自共同信息（MI）分解，旨在防止竞争在多模式训练中的不利影响。我们的主要贡献是：1）在多模式学习中引入游戏理论原则，在这种学习中，每种模态都充当竞争，以最大程度地影响其对最终结果的影响，从而使MI术语自动平衡。 2）精炼每个MI术语的下限和上限，以增强与任务相关的独特和共享信息的提取。 3）提出有条件MI估计的潜在空间排列，从而显着提高了计算效率。 MCR的表现都胜过所有先前建议的培训策略，并且是第一个始终如一地改善集合基线以外的多模式学习的人，这清楚地表明，结合方式会导致合成和大型现实世界数据集的显着性能提高。

### Dockformer: A transformer-based molecular docking paradigm for large-scale virtual screening 
[[arxiv](https://arxiv.org/abs/2411.06740)] [[cool](https://papers.cool/arxiv/2411.06740)] [[pdf](https://arxiv.org/pdf/2411.06740)]
> **Authors**: Zhangfan Yang,Junkai Ji,Shan He,Jianqiang Li,Tiantian He,Ruibin Bai,Zexuan Zhu,Yew Soon Ong
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: 15 pages, 10 figures
- **标题**: DockFormer：用于大规模虚拟筛选的基于变压器的分子对接范式
- **领域**: 机器学习,人工智能
- **摘要**: 分子对接是药物开发的关键步骤，它使化合物文库的虚拟筛选能够鉴定靶向目标蛋白的潜在配体。但是，随着复合库的大小增加，传统对接模型的计算复杂性增加。最近，深度学习算法可以提供数据驱动的研发模型，以提高对接过程的速度。不幸的是，与传统模型相比，很少有模型可以实现出色的筛选性能。因此，本研究引入了一种新颖的基于学习的对接方法，名为Dockformer。 DockFormer利用多模式信息来捕获分子的几何拓扑和结构知识，并可以直接以端到端方式与相应的置信度度量产生结合构象。实验结果表明，DockFormer在PDBBIND核心集和PoseBusters基准的基准分别达到90.53％和82.71％的成功率，推理过程速度的增长率分别超过100倍，超过100倍以上，几乎超过了所有目的是所有先进的停靠方法。此外，在现实世界虚拟筛查方案中证明了DockFormer鉴定冠状病毒主要蛋白酶抑制剂的能力。考虑到其较高的对接精度和筛选效率，可以将DockFormer视为药物设计领域中强大而强大的工具。

### Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.07559)] [[cool](https://papers.cool/arxiv/2411.07559)] [[pdf](https://arxiv.org/pdf/2411.07559)]
> **Authors**: Tiejin Chen,Kaishen Wang,Hua Wei
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: Accepted to Neurips SafeGenAi Workshop 2024
- **标题**: ZER0-JACK：一种基于内存梯度的越狱方法，用于黑盒多模式大型语言模型
- **领域**: 机器学习,人工智能
- **摘要**: 越狱的方法引起了多模式大型语言模型（MLLM）来输出有害的响应，这引起了严重的安全问题。在这些方法中，基于梯度的方法（使用梯度来产生恶意提示），由于其在白色盒子设置中的高成功率，在该设置中可以完全访问该模型，因此已经广泛研究了其恶意提示。但是，这些方法具有明显的局限性：它们需要白色框访问，这并不总是可行的，并且涉及高内存使用情况。为了解决白色框访问不可用的方案，攻击者经常求助于转移攻击。在转移攻击中，使用白色框模型生成的恶意输入应用于黑盒模型，但这通常会导致攻击性能降低。为了克服这些挑战，我们提出了ZER0杰克，这种方法绕开了通过利用零件订单优化的白盒访问的需求。我们提出贴片坐标下降，以有效地生成恶意图像输入以直接攻击黑盒MLLM，从而大大降低了内存使用情况。通过广泛的实验，ZER0杰克在各种模型上取得了很高的攻击成功率，超过了先前的基于转移的方法，并使用现有的白盒越狱技术进行了相当的攻击。值得注意的是，ZER0-JACK在Minigpt-4上实现了95 \％的攻击成功率，而有害行为在黑色盒子设置上具有多模式数据集，证明了其有效性。此外，我们表明ZER0杰克可以直接攻击商业MLLM，例如GPT-4O。补充剂中提供代码。

### Efficiently learning and sampling multimodal distributions with data-based initialization 
[[arxiv](https://arxiv.org/abs/2411.09117)] [[cool](https://papers.cool/arxiv/2411.09117)] [[pdf](https://arxiv.org/pdf/2411.09117)]
> **Authors**: Frederic Koehler,Holden Lee,Thuy-Duong Vuong
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: 有效地学习和对基于数据初始化的多模式分布进行采样
- **领域**: 机器学习,数据结构和算法,可能性,机器学习
- **摘要**: 我们考虑了用马尔可夫链采样多模式分布的问题，只有固定度量中的少量样品。 Although mixing can be arbitrarily slow, we show that if the Markov chain has a $k$th order spectral gap, initialization from a set of $\tilde O(k/\varepsilon^2)$ samples from the stationary distribution will, with high probability over the samples, efficiently generate a sample whose conditional law is $\varepsilon$-close in TV distance to the stationary measure.特别是，这适用于满足庞加莱不平等的$ k $分布的混合物，当它们满足log-sobolev不平等时，融合更快。我们的边界对马尔可夫链的扰动稳定，尤其是在$ \ mathbb r^d $上进行分数估计误差以及GLAUBER动力学以及Pseudolikelihood估计中的近似误差的工作。尽管对数据分布的混合缓慢，但基于数据的初始化对于得分匹配方法的成功是合理的，并改善并推广了Koehler和Vuong（2023）的结果，即具有线性而不是指数级的依赖性，并适用于$ K $并适用于任意的半群。由于我们的结果，我们首次表明，可以从样本中有效地学习一类天然的低复杂性措施。

### MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics Classification 
[[arxiv](https://arxiv.org/abs/2411.08703)] [[cool](https://papers.cool/arxiv/2411.08703)] [[pdf](https://arxiv.org/pdf/2411.08703)]
> **Authors**: Shan Cong,Zhiling Sang,Hongwei Liu,Haoran Luo,Xin Wang,Hong Liang,Jie Hao,Xiaohui Yao
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: MVKTRANS：鲁棒多组学分类的多视图知识转移
- **领域**: 机器学习,人工智能
- **摘要**: 多组学数据的独特特征，包括生物学层和疾病异质性内部和跨性别的复杂相互作用（例如，病因学和临床症状的异质性）促使我们开发了新颖的设计，以应对多组学预测的独特挑战。在本文中，我们提出了多视图知识转移学习（MVKTRANS）框架，该框架通过审查数据异质性并抑制偏见转移，从而以适应性的方式转移内部和间词间知识，从而增强分类性能。具体而言，我们设计了一个图形对比模块，该模块在未标记的数据上进行了训练，以有效地学习和将基本的内部词内模式转移到监督任务中。这种无监督的预处理促进了每种方式的学习通用和无偏见的表示，无论下游任务如何。鉴于不同疾病和/或样本之间的歧视能力的不同歧视能力，我们引入了适应性和双向交叉摩西蒸馏模块。该模块自动确定了更丰富的方式，并促进了动态知识的转移，从更详细的信息到少信息，从而实现了更强大和更广泛的整合。在四个实际生物医学数据集上进行的广泛实验表明，与最先进的艺术相比，MVKTRAN的卓越性能和鲁棒性。代码和数据可在https://github.com/yaolab-fantastic/mvktrans上找到。

### Measuring similarity between embedding spaces using induced neighborhood graphs 
[[arxiv](https://arxiv.org/abs/2411.08687)] [[cool](https://papers.cool/arxiv/2411.08687)] [[pdf](https://arxiv.org/pdf/2411.08687)]
> **Authors**: Tiago F. Tavares,Fabio Ayres,Paris Smaragdis
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: 使用感应邻域图测量嵌入空间之间的相似性
- **领域**: 机器学习
- **摘要**: 深度学习技术在生成嵌入空间以捕获项目之间的语义相似性方面表现出色。通常，这些表示是配对的，可以与类比（在同一域内成对）和交叉模式（跨域对）进行实验。这些实验基于有关嵌入空间几何形状的特定假设，这些假设可以通过推断训练数据集中的嵌入对之间的位置关系来查找配对项目，从而允许查找新的类似物和多模式零发射分类等任务。在这项工作中，我们提出了一个度量，以评估配对项目表示之间的相似性。我们的建议是根据每个表示的最接近邻居诱导图之间的结构相似性构建的，并且可以配置为基于不同距离指标和不同邻域大小的空间。我们证明我们的建议可用于在不同尺度上识别类似的结构，这很难使用诸如中心内核比对（CKA）等内核方法实现。我们通过两个案例研究进一步说明了我们的方法：使用手套嵌入的一个类比任务，而使用夹嵌入的CIFAR-100数据集中的零摄像分类。我们的结果表明，类比和零击分类任务的准确性与嵌入相似性相关。这些发现可以帮助解释这些任务的性能差异，并可能在将来改进配对模型的设计。

### UniMat: Unifying Materials Embeddings through Multi-modal Learning 
[[arxiv](https://arxiv.org/abs/2411.08664)] [[cool](https://papers.cool/arxiv/2411.08664)] [[pdf](https://arxiv.org/pdf/2411.08664)]
> **Authors**: Janghoon Ock,Joseph Montoya,Daniel Schweigert,Linda Hung,Santosh K. Suram,Weike Ye
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: Unimat：通过多模式学习统一材料嵌入
- **领域**: 机器学习,材料科学
- **摘要**: 材料科学数据集本质上是异质的，并且具有不同的方式，例如表征光谱，原子结构，微观图像和基于文本的合成条件。多模式学习的进步，尤其是在视觉和语言模型中，为以不同形式集成数据开辟了新的途径。在这项工作中，我们评估了统一材料科学中一些最重要的方式：原子结构，X射线衍射模式（XRD）和组成中的多模式学习（对齐和融合）中的共同技术。我们表明，通过与XRD模式对齐可以增强结构图模式。此外，我们表明，比对齐和融合更多的实验可访问的数据格式，例如XRD模式和组成，可以创造出比各种任务中的单个模态更强大的关节嵌入。这为未来的研究奠定了基础，旨在利用材料科学中多模式数据的全部潜力，从而促进了材料设计和发现中更明智的决策。

### Material Property Prediction with Element Attribute Knowledge Graphs and Multimodal Representation Learning 
[[arxiv](https://arxiv.org/abs/2411.08414)] [[cool](https://papers.cool/arxiv/2411.08414)] [[pdf](https://arxiv.org/pdf/2411.08414)]
> **Authors**: Chao Huang,Chunyan Chen,Ling Shi,Chen Chen
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: 具有元素属性知识图和多模式表示学习的材料属性预测
- **领域**: 机器学习,材料科学,人工智能
- **摘要**: 机器学习已成为预测晶体材料特性的关键工具。但是，现有方法主要通过构建晶体结构的多边图来表示材料信息，通常忽视元素的化学和物理特性（例如原子半径，电负性，熔点和电离能量），从而对材料性能产生重大影响。为了解决此限制，我们首先构建了元素属性知识图，并利用了嵌入模型来编码知识图内的元素属性。此外，我们提出了一个多模式融合框架ESNET，该框架将元素属性与晶体结构特征集成在一起，以生成关节多模式表示。这为预测结晶材料的性能提供了更全面的观点，使模型能够同时考虑材料的微结构组成和化学特征。我们对材料项目基准数据集进行了实验，该数据集在带隙预测任务中显示了领先的性能，并以编队能量预测任务中现有基准测试的结果取得了结果。

### Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment 
[[arxiv](https://arxiv.org/abs/2411.10919)] [[cool](https://papers.cool/arxiv/2411.10919)] [[pdf](https://arxiv.org/pdf/2411.10919)]
> **Authors**: Arushi Gupta,Rafal Kocielnik,Jiayun Wang,Firdavs Nasriddinov,Cherine Yang,Elyssa Wong,Anima Anandkumar,Andrew Hung
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: Accepted as a spotlight proceedings paper at Machine Learning for Health 2024
- **标题**: 多模式自学学习，用于手术反馈有效性评估
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 在手术培训中，从培训师到学员的实时反馈对于预防错误和增强长期技能的重要性很重要。准确地预测反馈的有效性，特别是它是否导致了学员行为的改变，对于开发改善手术培训和教育的方法至关重要。但是，依靠人类注释来评估反馈有效性是费力的，容易出现偏见，强调了对自动化，可扩展和客观方法的需求。创建这样的自动化系统会带来挑战，因为它需要了解培训师提供的口头反馈和实时手术场景的视觉上下文。为了解决这个问题，我们提出了一种方法，该方法将来自转录的言语反馈和相应外科视频的信息整合在一起，以预测反馈效果。我们的发现表明，转录的反馈和外科视频都可以单独预测受训者行为的变化，并且它们的组合达到了0.70 +/- 0.02的AUROC，提高了预测准确性高达6.6％。此外，我们介绍了自我监督的微调作为增强外科视频表示学习的策略，这是可扩展的，并进一步增强了预测性能。我们的结果表明，多模式学习的潜力可以推进手术反馈的自动评估。

### GeomCLIP: Contrastive Geometry-Text Pre-training for Molecules 
[[arxiv](https://arxiv.org/abs/2411.10821)] [[cool](https://papers.cool/arxiv/2411.10821)] [[pdf](https://arxiv.org/pdf/2411.10821)]
> **Authors**: Teng Xiao,Chao Cui,Huaisheng Zhu,Vasant G. Honavar
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: BIBM 2024
- **标题**: 地理策略：分子的对比几何 - 文本预训练
- **领域**: 机器学习,生物分子
- **摘要**: 预处理分子表示对于药物和材料发现至关重要。最近的方法着重于从几何结构中学习表示，有效地捕获了3D位置信息。然而，他们忽略了生物医学文本中丰富的信息，这些信息详细介绍了分子的特性和子结构。考虑到这一点，我们为200k对基地几何结构和生物医学文本设置了数据收集工作，从而产生了PubChem3D数据集。基于此数据集，我们提出了GeomClip框架，以增强从分子结构和生物医学文本学习的多模式表示。在预训练期间，我们设计了两种类型的任务，即多模式表示对准和单峰deNOSING训练预处理，以使3D几何编码器与文本信息保持一致，并同时保留其原始表示能力。实验结果表明，GeomClip在各种任务中的有效性，例如分子性质预测，零射击文本 - 分子检索和3D分子字幕。我们的代码和收集的数据集可在\ url {https://github.com/xiaocui3737/geomclip}上找到

### Hybrid Attention Model Using Feature Decomposition and Knowledge Distillation for Glucose Forecasting 
[[arxiv](https://arxiv.org/abs/2411.10703)] [[cool](https://papers.cool/arxiv/2411.10703)] [[pdf](https://arxiv.org/pdf/2411.10703)]
> **Authors**: Ebrahim Farahmand,Shovito Barua Soumma,Nooshin Taheri Chatrudi,Hassan Ghasemzadeh
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: updated results
- **标题**: 使用特征分解和知识蒸馏进行葡萄糖预测的混合注意模型
- **领域**: 机器学习,信号处理
- **摘要**: 连续葡萄糖监测器的可用性，因为非处方商品创造了一个独特的机会，可以监测一个人的血糖水平，预测血糖轨迹并提供自动干预措施，以防止不良的葡萄糖控制产生毁灭性的慢性并发症。然而，预测血糖水平是具有挑战性的，因为血糖的摄入量，药物摄入，体育锻炼，睡眠和压力始终如一地变化。从多模式和不规则采样数据以及长期预测范围中准确预测BGL特别困难。此外，这些预测模型必须在边缘设备上实时运行，以提供时刻的干预措施。为了应对这些挑战，我们提出了Gluconet，Gluconet是一种由AI驱动的传感器系统，用于不断监测对血糖模式的行为和生理健康以及可靠的预测。 Gluconet设计了一个基于特征分解的变压器模型，该模型结合了患者的行为和生理数据，并使用数学模型将稀疏和不规则的患者数据（例如饮食和药物摄入数据）转化为连续特征，从而促进与BGL数据更好地集成。鉴于BG信号的非线性和非平稳性，我们提出了一种分解方法，以从BGL信号中提取低频和高频组件，从而提供准确的预测。为了降低计算复杂性，我们还建议采用知识蒸馏来压缩变压器模型。 Gluconet使用涉及12名T1糖尿病的参与者获得的数据，将RMSE提高了60％，参数数量减少了21％，RMSE和MAE提高了51％和57％。这些结果强调了Gluconet作为预防和管理的紧凑而可靠的工具的潜力。

### Patient-Specific Models of Treatment Effects Explain Heterogeneity in Tuberculosis 
[[arxiv](https://arxiv.org/abs/2411.10645)] [[cool](https://papers.cool/arxiv/2411.10645)] [[pdf](https://arxiv.org/pdf/2411.10645)]
> **Authors**: Ethan Wu,Caleb Ellington,Ben Lengerich,Eric P. Xing
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: Findings paper presented at Machine Learning for Health (ML4H) symposium 2024, December 15-16, 2024, Vancouver, Canada, 4 pages
- **标题**: 患者特定的治疗效果模型解释了结核病的异质性
- **领域**: 机器学习,机器学习
- **摘要**: 结核病（TB）是全球重大的健康挑战，诸如HIV，糖尿病和贫血等合并症使治疗结果复杂化并有助于异质性患者反应。 TB的传统模型通常通过关注广泛的，预先定义的患者群体来忽略这种异质性，从而缺少各个患者环境的细微效果。我们建议通过使用上下文化建模超出粗大的亚组分析，这是一种多任务学习方法，该方法将患者环境编码为个性化的治疗效果模型，从而揭示了特定于患者的治疗益处。我们的模型应用于TB门户网站数据集，并针对3,000多名结核病患者进行多模式测量，揭示了合并症，治疗和患者结果之间的结构性相互作用，确定贫血，发病年龄和HIV是治疗效率的影响力。通过提高异质种群的预测准确性并提供特定于患者的见解，情境化模型有望实现新的个性化治疗方法。

### Weakly-Supervised Multimodal Learning on MIMIC-CXR 
[[arxiv](https://arxiv.org/abs/2411.10356)] [[cool](https://papers.cool/arxiv/2411.10356)] [[pdf](https://arxiv.org/pdf/2411.10356)]
> **Authors**: Andrea Agostini,Daphné Chopard,Yang Meng,Norbert Fortin,Babak Shahbaba,Stephan Mandt,Thomas M. Sutter,Julia E. Vogt
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: Findings paper presented at Machine Learning for Health (ML4H) symposium 2024, December 15-16, 2024, Vancouver, Canada, 13 pages. arXiv admin note: text overlap with arXiv:2403.05300
- **标题**: 对模拟CXR的弱监督多模式学习
- **领域**: 机器学习
- **摘要**: 多模式数据集成和标签稀缺对医疗环境中的机器学习构成了重大挑战。为了解决这些问题，我们对充满挑战的MIMIC-CXR数据集对新提出的多模式变化混合物（MMVM）VAE进行了深入的评估。我们的分析表明，MMVM VAE始终优于其他多模式VAE和完全监督的方法，从而强调了其在现实世界中的强大潜力。

### PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse 
[[arxiv](https://arxiv.org/abs/2411.10087)] [[cool](https://papers.cool/arxiv/2411.10087)] [[pdf](https://arxiv.org/pdf/2411.10087)]
> **Authors**: Einari Vaaras,Manu Airaksinen,Okko Räsänen
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: PFML：无代表崩溃的时间序列数据的自我监督学习
- **领域**: 机器学习,人工智能
- **摘要**: 自我监督学习（SSL）是一种数据驱动的学习方法，它利用数据的先天结构来指导学习过程。与依赖外部标签的监督学习相反，SSL利用数据的固有特征来产生其自身的监督信号。但是，SSL方法的一个常见问题是表示崩溃，该模型在其中输出恒定的输入特征表示。此问题阻碍了SSL方法在新的数据模式中的潜在应用，因为试图避免代表性崩溃，废除了研究人员的时间和精力。本文介绍了一种新颖的SSL算法，用于时间序列数据，称为蒙版潜伏期（PFML）功能的预测。 PFML不是直接预测掩盖的输入信号或其潜在表示，而是通过预测与掩盖的嵌入的输入信号的统计功能（给定一系列未掩盖的嵌入）来运行。该算法旨在避免表示崩溃，使其直接适用于不同时间序列数据域，例如临床数据中的新传感器方式。我们通过三种不同数据模式的复杂，现实生活中的分类任务来证明PFML的有效性：来自多传感器惯性测量单元数据的婴儿姿势和运动分类，语音数据的情绪识别以及EEG数据中的睡眠阶段分类。结果表明，PFML优于概念上类似的SSL方法和基于对比学习的SSL方法。此外，PFML与当前的最新SSL方法相提并论，同时在概念上也更简单，没有代表性崩溃。

### Jal Anveshak: Prediction of fishing zones using fine-tuned LlaMa 2 
[[arxiv](https://arxiv.org/abs/2411.10050)] [[cool](https://papers.cool/arxiv/2411.10050)] [[pdf](https://arxiv.org/pdf/2411.10050)]
> **Authors**: Arnav Mejari,Maitreya Vaghulade,Paarshva Chitaliya,Arya Telang,Lynette D'mello
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: Jal Anveshak：使用微调美洲驼的预测捕鱼区2
- **领域**: 机器学习,人工智能
- **摘要**: 近年来，全球和印度政府在监测和收集与渔业行业有关的数据方面的努力取得了重大进步。尽管有大量数据，但仍存在利用基于人工智能的技术系统使印度渔民在沿海地区受益的潜力。为了在印度技术生态系统中填补这一空白，作者介绍了Jal Anveshak。这是一个用飞镖和颤音编写的应用程序框架，它使用了基于Llama 2的大型语言模型，该模型对与捕鱼产量和可用性有关的预处理和增强的政府数据进行了微调。其主要目的是帮助印度渔民安全从沿海地区获得鱼类的最大产量，并以多语言和多模式的方式解决与捕鱼相关的查询。

### Just KIDDIN: Knowledge Infusion and Distillation for Detection of INdecent Memes 
[[arxiv](https://arxiv.org/abs/2411.12174)] [[cool](https://papers.cool/arxiv/2411.12174)] [[pdf](https://arxiv.org/pdf/2411.12174)]
> **Authors**: Rahul Garg,Trilok Padhi,Hemang Jain,Ugur Kursuncu,Ponnurangam Kumaraguru
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: Just Kiddin：知识输液和蒸馏以检测不雅模因
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 在线多模式环境中的毒性识别仍然是一项具有挑战性的任务，因为跨模态的上下文连接的复杂性（例如，文本和视觉）。在本文中，我们提出了一个新颖的框架，该框架整合了大型视觉语言模型（LVLM）和知识输注的知识蒸馏（KD），以增强可恨模因中毒性检测的性能。我们的方法从ConceptNet提取了子知识图，这是一个大规模常识性知识图（kg），该图被注入紧凑的VLM框架中。标题和模因中的有毒短语以及模因中的视觉概念之间的关系背景增强了模型的推理能力。我们对两个仇恨言语基准数据集的研究结果的实验​​结果表明，在AU-ROC，F1和召回率的最先进基准和召回率上的表现分别为1.1％，7％和35％。鉴于毒性检测任务的上下文复杂性，我们的方法展示了从显式（即kg）学习的重要性以及通过混合神经符号方法纳入的隐式（即LVLMS）上下文提示。这对于对有毒内容的准确且可扩展的识别对于创建更安全的在线环境至关重要的现实应用程序至关重要。

### MMBind: Unleashing the Potential of Distributed and Heterogeneous Data for Multimodal Learning in IoT 
[[arxiv](https://arxiv.org/abs/2411.12126)] [[cool](https://papers.cool/arxiv/2411.12126)] [[pdf](https://arxiv.org/pdf/2411.12126)]
> **Authors**: Xiaomin Ouyang,Jason Wu,Tomoyoshi Kimura,Yihan Lin,Gunjan Verma,Tarek Abdelzaher,Mani Srivastava
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: MMBIND：在物联网中释放分布式和异质数据的潜力
- **领域**: 机器学习
- **摘要**: 在各种现实世界中，多模式传感系统越来越普遍。大多数现有的多模式学习方法在很大程度上依赖于大量同步，完整的多模式数据的培训。但是，这种设置在现实世界IoT传感应用程序中是不切实际的，这些应用程序通常通过具有异质数据模式的分布式节点收集数据，并且也很少标记。在本文中，我们提出了MMBIND，这是一种用于分布式和异质IoT数据多模式学习的新数据绑定方法。 MMBIND的关键思想是通过通过不同的描述性共享模态来构造伪配对的多模式数据集，以构建模型训练的模型训练和不完整的模态。我们还提出了一种加权对比度学习方法，以处理不同数据之间的域移位，并与适应性的多模式学习体系结构相结合，能够培训具有异质模态组合的训练模型。对十个现实世界多模式数据集的评估要点，即MMBIND在不同程度的数据不完整性和域移动下优于最先进的基准，并保持有望在IOT应用中推进多模式基础模型培训\ footNote（可以通过https：//github.com/nesl/nesslel/mult）获得多模式的模型培训（可以通过源代码获得源代码。

### ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling 
[[arxiv](https://arxiv.org/abs/2411.11911)] [[cool](https://papers.cool/arxiv/2411.11911)] [[pdf](https://arxiv.org/pdf/2411.11911)]
> **Authors**: Zikang Zhou,Hengjian Zhou,Haibo Hu,Zihao Wen,Jianping Wang,Yung-Hui Li,Yu-Kai Huang
> **First submission**: 2024-11-17
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: MODESEQ：使用顺序模式建模的稀疏多模式运动预测
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器人技术
- **摘要**: 预期未来事件的多模式为安全的自动驾驶奠定了基础。但是，由于缺乏多模式地面真相，交通代理的多模式运动预测已蒙上阴影。现有作品主要采用赢家全力训练策略来应对这一挑战，但仍遭受有限的轨迹多样性和未对准模式的信心。尽管某些方法通过产生过多的轨迹候选者来解决这些局限性，但它们需要一个后处理阶段来识别最具代表性的模式，这一过程缺乏普遍的原理和损害轨迹的准确性。因此，我们有动力推出Modeseq，这是一种模型为序列的新的多模式预测范式。与一击解码多个合理轨迹的常见实践不同，MODESEQ要求运动解码器逐步推断下一个模式，从而更明确地捕获模式之间的相关性，并显着增强了对多模态推理的能力。利用顺序模式预测的电感偏差，我们还提出了早期匹配的全部训练策略，以进一步多样化轨迹。 Modeseq在不依赖密集模式预测或基于规则的轨迹选择的情况下大大提高了多模式输出的多样性，同时达到令人满意的轨迹精度，从而在运动预测基准上产生了平衡的性能。此外，Modeseq自然而然地出现了模式外推的能力，该功能支持预测未来的预测更多的行为模式。

### Dissecting Representation Misalignment in Contrastive Learning via Influence Function 
[[arxiv](https://arxiv.org/abs/2411.11667)] [[cool](https://papers.cool/arxiv/2411.11667)] [[pdf](https://arxiv.org/pdf/2411.11667)]
> **Authors**: Lijie Hu,Chenyang Ren,Huanyi Xie,Khouloud Saadi,Shu Yang,Zhen Tan,Jingfeng Zhang,Di Wang
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: 33 pages
- **标题**: 通过影响功能在对比度学习中剖析表示未对准
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 通常应用于大规模多模型的对比学习通常依赖于来自不同且通常不可靠的来源的数据，这些数据可能包括未对准或标记的文本图像对。这通常会导致健壮的问题和幻觉，最终导致性能降解。数据评估是检测和追踪这些未对准的有效方法。然而，对于大规模型号而言，现有方法在计算上很昂贵。尽管在计算上有效，但对于对比度学习模型而言，经典的影响功能不足，因为它们最初是为了丢失的损失而设计的。此外，对比度学习涉及最大程度地降低样品阳性方式之间的距离，同时最大程度地提高阴性样本方式之间的距离。这需要从这两个角度评估样品的影响。为了应对这些挑战，我们引入了对比度损失（ECIF）的扩展影响功能，这是为对比度损失而设计的影响功能。 ECIF考虑了正样本和负面样本，并提供了对比度学习模型的封闭形式近似，从而消除了重新培训的需求。在ECIF的基础上，我们开发了一系列用于数据评估，未对准检测和错误预测的痕量折线任务的算法。实验结果表明，与传统的基线方法相比，通过更准确地评估数据影响和模型对齐方式，我们的ECIF提高了夹式嵌入模型的透明度和解释性。

### Heuristic-Free Multi-Teacher Learning 
[[arxiv](https://arxiv.org/abs/2411.12724)] [[cool](https://papers.cool/arxiv/2411.12724)] [[pdf](https://arxiv.org/pdf/2411.12724)]
> **Authors**: Huy Thong Nguyen,En-Hung Chu,Lenord Melvix,Jazon Jiao,Chunglin Wen,Benjamin Louie
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: 无启发式的多教师学习
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 我们介绍了Teacher2Task，这是一个多教老师学习的新型框架，它消除了对手动汇总启发式方法的需求。现有的多教学方法通常依赖于这种启发式方法来结合多个教师的预测，通常会导致优化的聚合标签和聚集错误的传播。 Texter2Task通过引入特定于教师的输入令牌并重新提出培训过程来解决这些局限性。该框架不依赖汇总标签，而是将培训数据转换为训练数据，包括基础真相标签和n师的注释，n+1个不同的任务：n个辅助任务，这些任务预测了n个个人老师的标签风格，以及一个重点介绍地面真相标签的主要任务。这种方法借鉴了多个学习范式的原则，在各种架构，模式和任务中都表现出了强烈的经验结果。

### FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data 
[[arxiv](https://arxiv.org/abs/2411.14717)] [[cool](https://papers.cool/arxiv/2411.14717)] [[pdf](https://arxiv.org/pdf/2411.14717)]
> **Authors**: Binqian Xu,Xiangbo Shu,Haiyang Mei,Guosen Xie,Basura Fernando,Jinhui Tang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: FEDMLLM：在多模式异质性数据上联合的微调MLLM
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）取得了重大进步，证明了在处理和理解多模式数据方面的强大功能。通过联合学习（FL）进行微调MLLM，可以通过包括私人数据源来扩展培训数据范围，从而增强其在对隐私敏感域中的实际适用性。但是，当前的研究仍处于早期阶段，特别是在解决现实世界应用中的\ textbf {多模式异质性}时。在本文中，我们介绍了一个基准，以评估MLLM在各种多模式异质场景中的联合微调的性能，为该领域的未来研究奠定了基础。我们的基准包括两个轻巧的MLLM，两个下游任务，三个评估指标和五个域中的五个数据集，以及六个比较基线，涵盖了四种多模式场景中的十种类型的模态异质性。为了应对多模式异质性带来的挑战，我们开发了一个通用的FEDMLLM框架，该框架将经典的FL方法与两种模态性不足的策略一起集成在一起。广泛的实验结果表明，我们提出的FL范式通过扩大训练数据的范围并减轻多模式异质性来提高MLLM的性能。代码在补充材料中可用。

### FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers 
[[arxiv](https://arxiv.org/abs/2411.14507)] [[cool](https://papers.cool/arxiv/2411.14507)] [[pdf](https://arxiv.org/pdf/2411.14507)]
> **Authors**: Zehua Pei,Hui-Ling Zhen,Xianzhi Yu,Sinno Jialin Pan,Mingxuan Yuan,Bei Yu
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: Fusegpt：可学习的层融合生成的预训练变压器
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 通过广泛的模型参数的广泛缩放，生成的预训练变压器（GPT）表现出在不同领域的表现出色。最近的作品观察到整个变压器块的冗余，并通过结构化块的结构化修剪来开发压缩方法。但是，这种简单的消除将始终提供不可逆转的性能降级。在本文中，我们提出了Fusegpt，这是一种回收修剪变压器块的新方法，以进一步恢复模型性能。首先，我们引入了一种新的重要性检测指标，宏观影响（MI），以通过计算其删除后的信息丢失来检测每个变压器块的长期影响。然后，我们提出了组级层融合，该层采用了不重要块的层中的参数，并将它们注入相邻块内的相应层中。融合不是一次性的，而是通过迭代参数更新通过轻巧的组级微调来更新。具体而言，这些注入的参数被冷冻，但用可学习的等级分解矩阵加权，以减少微调过程中的开销。我们的方法不仅可以在大型语言模型上运作良好，而且在大型多模型模型上也很好地运行。实验表明，通过使用适度的数据，Fusegpt可以在困惑和零击任务性能方面胜过以前的作品。

### From Complexity to Parsimony: Integrating Latent Class Analysis to Uncover Multimodal Learning Patterns in Collaborative Learning 
[[arxiv](https://arxiv.org/abs/2411.15590)] [[cool](https://papers.cool/arxiv/2411.15590)] [[pdf](https://arxiv.org/pdf/2411.15590)]
> **Authors**: Lixiang Yan,Dragan Gašević,Linxuan Zhao,Vanessa Echeverria,Yueqiao Jin,Roberto Martinez-Maldonado
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 从复杂性到简约：整合潜在的班级分析，以在协作学习中揭示多模式学习模式
- **领域**: 机器学习,人机交互,方法论
- **摘要**: 多模式学习分析（MMLA）利用高级传感技术和人工智能来捕获复杂的学习过程，但是将各种数据源整合到凝聚力的见解中仍然具有挑战性。这项研究介绍了一种在MMLA中整合潜在类别分析（LCA）的新方法，以将单形成行为指标映射到旁皮的多模式中。使用高保真的医疗保健模拟环境，我们收集了位置，音频和生理数据，得出了17个单体指标。 LCA确定了四个不同的潜在类别：协作沟通，体现的协作，遥远的互动和单独的参与度，每种都捕获了独特的单色模式。认知网络分析将这些多模式指标与原始单座指标进行了比较，发现多模式方法更为简约，同时为学生的任务和协作表演提供了更高的解释能力。这些发现突出了LCA在简化复杂多模式数据的分析的同时，捕获细微差别的跨模式行为，为教育工作者提供可行的见解并增强协作学习干预设计的设计。这项研究提出了一种推进MMLA的途径，使其更加简约和可管理，并与以学习者为中心的教育原则保持一致。

### Sampling with Adaptive Variance for Multimodal Distributions 
[[arxiv](https://arxiv.org/abs/2411.15220)] [[cool](https://papers.cool/arxiv/2411.15220)] [[pdf](https://arxiv.org/pdf/2411.15220)]
> **Authors**: Björn Engquist,Kui Ren,Yunan Yang
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-25
> **comment**: 26 pages, 6 figures
- **标题**: 具有自适应差异的多模式分布的采样
- **领域**: 机器学习,数值分析,计算,机器学习
- **摘要**: 我们建议和分析有界域上多模式分布的一类自适应采样算法，这些算法与经典过度引导的Langevin Dynamics具有结构性相似。我们首先证明，可以将具有自适应扩散系数和矢量场的这类线性动力学解释和分析，因为加权的瓦斯汀梯度梯度流动流的梯度流动流量 -  leibback-leibler（kl）当前分布与目标gibb之间的分布之间的差异，这直接导致了对KL和$ $ $ $ $ $ $ $ $ $ $ $ qub^2 $ qub的指示率的速度依赖的速度。公制和吉布斯的潜力。然后，我们证明，无衍生版本的动力学可以用于采样，而无需gibbs电位的梯度信息，并且对于具有非convex电位的Gibbs分布，这种方法可以比经典的过度低矮的langevin动力学更快地获得收敛速度。比较非凸电势的局部最小值之间的平均过渡时间进一步突出了无衍生物动力学在采样中的效率。

### M2oE: Multimodal Collaborative Expert Peptide Model 
[[arxiv](https://arxiv.org/abs/2411.15208)] [[cool](https://papers.cool/arxiv/2411.15208)] [[pdf](https://arxiv.org/pdf/2411.15208)]
> **Authors**: Zengzhu Guo,Zhiqi Ma
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-25
> **comment**: accepted by bibm 2024
- **标题**: M2OE：多模式协作专家肽模型
- **领域**: 机器学习,人工智能,生物分子
- **摘要**: 肽是由氨基酸组成的生物分子，它们在我们体内起重要作用。近年来，肽在药物设计和合成方面受到了广泛的关注，肽预测任务有助于我们更好地寻找功能性肽。通常，我们将肽的主要序列和结构信息用于模型编码。但是，最近的研究更多地集中于没有多模式方法的预测的单模式信息（结构或序列）。我们发现，单模式模型不擅长处理该特定模式中的信息更少的信息。因此，本文提出了M2OE多模式协作专家肽模型。基于先前的工作，通过整合序列和空间结构信息，采用专家模型和跨注意机制，模型的功能得到了平衡和改进。实验结果表明，M2OE模型在复杂的任务预测中表现出色。

### Multimodal large language model for wheat breeding: a new exploration of smart breeding 
[[arxiv](https://arxiv.org/abs/2411.15203)] [[cool](https://papers.cool/arxiv/2411.15203)] [[pdf](https://arxiv.org/pdf/2411.15203)]
> **Authors**: Guofeng Yang,Yu Li,Yong He,Zhenjiang Zhou,Lingzhen Ye,Hui Fang,Yiqi Luo,Xuping Feng
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 小麦育种的多模式大语言模型：智能育种的新探索
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 无人机遥感技术已成为农作物育种的关键技术，该技术可以实现高通量和无损的作物表型数据收集。但是，育种的多学科性质为知识开采带来了技术障碍和效率挑战。因此，重要的是开发智能繁殖目标工具来开采跨域多模式数据。 Based on different pre-trained open-source multimodal large language models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used supervised fine-tuning (SFT), retrieval-augmented generation (RAG), and reinforcement learning from human feedback (RLHF) technologies to inject cross-domain knowledge into MLLMs, thereby constructing multiple multimodal large language models for小麦育种（wblms）。在本研究中，使用新创建的评估基准评估了上述WBLM。结果表明，使用SFT，RAG和RLHF Technologies和InternVL2-8B构建的WBLM具有领先的性能。然后，使用WBLM进行了随后的实验。消融实验表明，SFT，RAG和RLHF技术的结合可以提高整体生成性能，提高生成的质量，平衡生成的答案的及时性和适应性，并减少幻觉和偏见。 WBLM同时使用跨域数据（遥感，表型，天气，种质）在小麦产量预测中表现最好，R2和RMSE分别为0.821和489.254 kg/ha。此外，WBLM可以为表型估计，环境压力评估，目标种质筛查，培养技术建议和种子价格查询任务产生专业决策支持答案。

### Forecasting Application Counts in Talent Acquisition Platforms: Harnessing Multimodal Signals using LMs 
[[arxiv](https://arxiv.org/abs/2411.15182)] [[cool](https://papers.cool/arxiv/2411.15182)] [[pdf](https://arxiv.org/pdf/2411.15182)]
> **Authors**: Md Ahsanul Kabir,Kareem Abdelfatah,Shushan He,Mohammed Korayem,Mohammad Al Hasan
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 人才获取平台中的预测应用程序计数：使用LMS利用多模式信号
- **领域**: 机器学习,人工智能
- **摘要**: 随着招聘和人才的获取变得越来越有竞争力，招聘公司在使用机器学习（ML）方法方面变得越来越复杂，以优化其日常活动。但是，该领域的大多数基于ML的方法都仅限于候选人匹配，技能匹配，工作分类和归一化等任务。在这项工作中，我们讨论了招聘领域中的一项新任务，即应用数量预测，其动机来自设计有效的外展活动以吸引合格的申请人。我们表明，现有的基于自动回归的时间序列预测方法在此任务上的表现较差。从此以后，我们提出了一个基于多模式的模型，该模型通过简单的编码器融合了各种模式的工作启动元数据。来自CareerBuilder LLC的大型现实数据集的实验显示了该方法对现有最新方法的有效性。

### Random Forest-Supervised Manifold Alignment 
[[arxiv](https://arxiv.org/abs/2411.15179)] [[cool](https://papers.cool/arxiv/2411.15179)] [[pdf](https://arxiv.org/pdf/2411.15179)]
> **Authors**: Jake S. Rhodes,Adam G. Rustad
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-25
> **comment**: 4 pages, 3 figures, Accepted at MMAI 2024 (BigData 2024)
- **标题**: 随机森林监督歧管对准
- **领域**: 机器学习,机器学习
- **摘要**: 流动对齐是一种数据融合技术，可以创建从多个域收集的数据共享的低维表示，从而实现跨域学习并改善下游任务的性能。本文提出了一种使用随机森林作为半监督对准算法的基础进行对齐的方法，利用该模型的固有优势。我们专注于通过从随机森林中得出的几何形状邻近度集成类标签，从而增强两个最近开发的基于对齐图。这些接近是构建维持当地邻里结构的跨域关系的监督初始化，从而促进对齐。我们的方法解决了多种局限性中的一个共同限制，其中现有方法通常无法生成嵌入，以捕获足够的信息以进行下游分类。相比之下，我们发现使用随机森林接近或类标签信息的对齐模型可提高下游分类任务的准确性，表现优于单域基线。多个数据集的实验表明，我们的方法通常会增强跨域特征集成和预测性能，这表明随机森林接近为需要多模式数据一致性的任务提供了实用的解决方案。

### PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision 
[[arxiv](https://arxiv.org/abs/2411.15127)] [[cool](https://papers.cool/arxiv/2411.15127)] [[pdf](https://arxiv.org/pdf/2411.15127)]
> **Authors**: Arnav M. Das,Chi Ian Tang,Fahim Kawsar,Mohammad Malekzadeh
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: Presented at ICASSP 2025. Also presented under the title "PRIMUS: Pretraining IMU Encoders withMultimodaland Self-Supervised Learning" at NeurIPS 2024 TSALM Workshop (Time Series in the Age of Large Models)
- **标题**: Primus：用多模式的自我判断进行预训练的IMU编码
- **领域**: 机器学习
- **摘要**: 通过嵌入在个人设备中的惯性测量单元（IMU）来传感人类运动，从而实现了健康和保健方面的重要应用。标记的IMU数据稀缺，但是，未标记或弱标记的IMU数据可用于建模人类运动。对于视频或文本模式，“预处理和适应”方法利用大量未标记或弱标记的数据来构建一个强功能提取器，然后使用有限的标记数据适应特定任务。但是，对于IMU数据而言，预处理的方法对较少了解，并且很少在室外任务上评估管道。我们提出了Primus：一种预处理IMU编码的方法，该方法使用了一个新颖的预处理目标，该目标是基于在域内和室外数据集的下游性能进行经验验证的。 Primus客观通过结合自我统计，多模式和最近的邻里监督有效地增强了下游性能。与最先进的基准相比，Primus每类标记的样品少于500个样品，提高了高达15％的测试准确性。为了使更广泛的社区受益，我们已经在github.com/nokia-bell-labs/pretrataining-imu-engoders开了开源。

### Continual SFT Matches Multimodal RLHF with Negative Supervision 
[[arxiv](https://arxiv.org/abs/2411.14797)] [[cool](https://papers.cool/arxiv/2411.14797)] [[pdf](https://arxiv.org/pdf/2411.14797)]
> **Authors**: Ke Zhu,Yu Wang,Yanpeng Sun,Qiang Chen,Jiangjiang Liu,Gang Zhang,Jingdong Wang
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 连续的SFT与负面监督匹配多模式RLHF
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式RLHF通常发生在监督的芬特（SFT）阶段之后，以不断改善视觉模型（VLMS）的理解。在这个偏好一致性阶段，传统的智慧比连续的SFT具有优势。在本文中，我们观察到多模式RLHF的固有价值在于其负面监督，即被拒绝的响应的logit。因此，我们提出了一种新型的负面监督者（NSFT）方法，该方法完全发掘了这些信息。我们的NSFT在RLHF范式中解散了这种负面的监督，并不断使VLM与简单的SFT损失保持一致。与多模式RLHF相比，严格需要2（例如DPO）或4（例如PPO）大VLM的多模式RLHF。通过将NSFT与各种数据集源，基本VLM和评估指标之间的各种多模式RLHF方法进行比较，可以严格证明NSFT的有效性。此外，还提供了卓有成效的消融来支持我们的假设。我们希望本文能够刺激进一步的研究，以正确地调整大型视力语言模型。

### Grid and Road Expressions Are Complementary for Trajectory Representation Learning 
[[arxiv](https://arxiv.org/abs/2411.14768)] [[cool](https://papers.cool/arxiv/2411.14768)] [[pdf](https://arxiv.org/pdf/2411.14768)]
> **Authors**: Silin Zhou,Shuo Shang,Lisi Chen,Peng Han,Christian S. Jensen
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: This paper is accepted by KDD2025(August Cycle)
- **标题**: 网格和道路表达是轨迹表示学习的补充
- **领域**: 机器学习,人工智能
- **摘要**: 轨迹表示学习（TRL）地图轨迹轨迹可用于许多下游任务。现有的TRL方法使用网格轨迹，在自由空间中捕获运动或道路轨迹，在道路网络中捕获运动作为输入。我们观察到两种类型的轨迹是互补的，提供了区域和位置信息或提供道路结构和运动规律性。因此，我们提出了一种称为绿色的新型多模式TRL方法，以共同利用网格和道路轨迹表达来进行有效的表示学习。特别是，我们将RAW GPS轨迹转换为网格和道路轨迹，并量身定制两个编码器以捕获其各自的信息。为了使两个编码器相结合，以使它们相互补充，我们采取了对比损失，以鼓励他们为相同的原始轨迹产生类似的嵌入，并设计蒙版语言模型（MLM）损失使用网格轨迹来帮助重建掩盖掩盖的道路轨迹。为了了解最终的轨迹表示形式，使用双模式相互作用器通过交叉注意来融合两个编码器的输出。我们将绿色与7种最先进的TRL方法进行比较，用于3个下游任务，发现绿色始终优于所有基准，并将表现最佳基线的准确性平均提高15.99 \％。

### Probing the limitations of multimodal language models for chemistry and materials research 
[[arxiv](https://arxiv.org/abs/2411.16955)] [[cool](https://papers.cool/arxiv/2411.16955)] [[pdf](https://arxiv.org/pdf/2411.16955)]
> **Authors**: Nawaf Alampara,Mara Schilling-Wilhelmi,Martiño Ríos-García,Indrajeet Mandal,Pranav Khetarpal,Hargun Singh Grover,N. M. Anoop Krishnan,Kevin Maik Jablonka
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 探测化学和材料研究多模式模型的局限性
- **领域**: 机器学习,材料科学
- **摘要**: 人工智能的最新进展引发了人们对科学助理的兴趣，这些助理可以支持整个科学工作流程的研究人员，从文献综述到实验设计和数据分析。这种系统的关键能力是能够在视觉和文本形式中处理科学信息的能力 - 从解释光谱数据到了解实验室设置。在这里，我们介绍了MacBench，这是一种综合基准，用于评估视觉模型如何处理三个核心方面的现实化学化学和材料科学任务：数据提取，实验理解和结果解释。通过对领先模型的系统评估，我们发现，尽管这些系统在基本的感知任务中表现出了有希望的能力 - 在设备识别和标准化数据提取方面实现了几乎完美的性能，但它们在空间推理，跨模式信息综合和多步逻辑率上表现出基本的限制。除了化学和材料科学之外，我们的见解具有重要的含义，这表明开发可靠的多模式AI科学助手可能需要进步策划合适的培训数据和培训这些模型的方法。

### Scaling Laws for Black box Adversarial Attacks 
[[arxiv](https://arxiv.org/abs/2411.16782)] [[cool](https://papers.cool/arxiv/2411.16782)] [[pdf](https://arxiv.org/pdf/2411.16782)]
> **Authors**: Chuan Liu,Huanran Chen,Yichi Zhang,Yinpeng Dong,Jun Zhu
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 黑匣子对抗攻击的缩放法律
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 深度学习模型的一个长期问题是它们易受对抗性例子的脆弱性，通常通过对自然示例应用不可察觉的扰动来产生。对抗性示例具有跨模型的可传递性，从而能够攻击有关其体系结构和参数的有限信息的黑盒模型。模型结合是通过同时攻击多个替代模型来提高可转移性的有效策略。但是，由于先前的研究通常在集合中采用的模型很少，因此仍然存在一个开放的问题，即规模模型数量是否可以进一步改善黑框攻击。受到大型基础模型的发现的启发，我们研究了这项工作中黑盒对抗攻击的缩放定律。通过分析替代模型的数量与对抗性示例的可转移性之间的关系，我们以清晰的缩放定律结论，强调使用更多的替代模型来增强对抗性转移性的潜力。广泛的实验验证了对标准图像分类器，多模式大语模型甚至GPT-4O等专有模型的主张，并以更多的代孕模型显示了一致的缩放效果和令人印象深刻的攻击成功率。通过可视化的进一步研究表明，缩放攻击在语义上带来了更好的解释性，表明捕获了模型的共同特征。

### Federated Learning in Chemical Engineering: A Tutorial on a Framework for Privacy-Preserving Collaboration Across Distributed Data Sources 
[[arxiv](https://arxiv.org/abs/2411.16737)] [[cool](https://papers.cool/arxiv/2411.16737)] [[pdf](https://arxiv.org/pdf/2411.16737)]
> **Authors**: Siddhant Dutta,Iago Leal de Freitas,Pedro Maciel Xavier,Claudio Miceli de Farias,David Esteban Bernal Neira
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-26
> **comment**: 53 Pages, 8 figures, Under review in ACS Industrial & Engineering Chemistry Research Journal
- **标题**: 化学工程中的联合学习：关于跨分布式数据源的隐私协作框架的教程
- **领域**: 机器学习,分布式、并行和集群计算,神经和进化计算
- **摘要**: 联邦学习（FL）是一种分散的机器学习方法，它引起了人们的关注，因为它有可能在保护数据隐私的同时，在客户跨客户培训中进行协作模型培训，从而使其成为化学工业的有吸引力的解决方案。这项工作旨在为化学工程社区提供对学科的可访问介绍。在动手教程和全面示例集合的支持下，它探讨了FL在制造优化，多模式数据集成和药物发现等任务中的应用，同时解决了保护专有信息和管理分布式数据集的独特挑战。该教程是使用关键框架构建的，例如$ \ texttt {flower} $和$ \ texttt {tensorflow fiferated} $，旨在为化学工程师提供正确的工具，以在其特定需求下采用FL。我们将FL与化学工程应用相关的三个不同数据集与集中学习的表现进行了比较，这表明FL通常会维持或改善分类性能，尤其是对于复杂和异质数据。最后，我们对要解决联合学习的开放挑战的前景和目前的方法旨在补救和改善此框架。

### Video-Text Dataset Construction from Multi-AI Feedback: Promoting Weak-to-Strong Preference Learning for Video Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.16201)] [[cool](https://papers.cool/arxiv/2411.16201)] [[pdf](https://arxiv.org/pdf/2411.16201)]
> **Authors**: Hao Yi,Qingyang Li,Yulan Hu,Fuzheng Zhang,Di Zhang,Yong Liu
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 来自多版反馈的视频文本数据集构建：促进视频模型的弱至较强的偏好学习
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 高质量的视频介绍偏好数据对于多模式大语言模型（MLLM）对准至关重要。但是，现有的偏好数据非常稀缺。获得偏好训练的VQA偏好数据是昂贵的，并且手动注释响应高度不可靠，这可能会导致低质量对。同时，由温度调节控制的AI生成的反应缺乏多样性。要解决这些问题，我们提出了一个高质量的VQA首选项数据集，称为\ textIt {\ textbf {m} untiple \ textbf {m} ult-imodal \ textbf {a} rtveration \ textbf \ textbf \ textbf {i} （\ textbf {mmaip-v}），它是通过从响应分布集中采样并使用外部评分函数来构建的，以进行响应评估。此外，为了充分利用MMAIP-V中的偏好知识并确保足够的优化，我们提出\ textIt {\ textbf {iter} ative \ textbf {w} eak-tok-toe-t-t-eak to-textbf {s} s} \ textbf {f}视频mllms}（\ textbf {iter-w2s-rlaif}），该框架逐渐通过迭代更新参考模型和执行参数突破来逐渐增强MLLM的对齐功能。最后，我们在VQA评估中提出了一个公正的信息完整评估方案。实验表明，MMAIP-V在偏好学习中对MLLM有益，并且ITER-W2S-RLAIF完全利用MMAIP-V中的对齐信息。我们认为，基于AI反馈的拟议的自动VQA偏好数据生成管道可以极大地促进MLLMS对齐中的未来工作。 \ textbf {代码和数据集可用} \ href {https://anonymon.4open.science/r/mmaip-v_iter-w2s-rlaif-702f} {mmaip-v \ _iter-_iter-_iter-_iter-_iter-_Iter-_Iter-_Iter-W2S-w2s-rlaif-702f}。

### BlendServe: Optimizing Offline Inference for Auto-regressive Large Models with Resource-aware Batching 
[[arxiv](https://arxiv.org/abs/2411.16102)] [[cool](https://papers.cool/arxiv/2411.16102)] [[pdf](https://arxiv.org/pdf/2411.16102)]
> **Authors**: Yilong Zhao,Shuo Yang,Kan Zhu,Lianmin Zheng,Baris Kasikci,Yang Zhou,Jiarong Xing,Ion Stoica
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: Blendserve：通过资源感知批处理优化自动回归大型型号的离线推理
- **领域**: 机器学习
- **摘要**: 离线批处理推断利用请求批处理以实现更高的吞吐量和较低的成本的灵活性，这对于延迟不敏感的应用程序变得越来越流行。同时，模型能力和方式的最新进展使请求在计算和内存需求方面更加多样化，从而为通过资源重叠而改善吞吐量的独特机会。但是，最大化资源重叠的请求时间表可能与最大化前缀共享的时间表发生冲突，该计划是一种广泛使用的性能优化，从而导致了次优的推理吞吐量。我们提出Blendserve，该系统通过使用资源感知的前缀树结合资源重叠和前缀共享的好处，从而最大程度地利用了离线批处理推断的资源利用。 Blendserve在离线批处理推理中利用了放松的延迟要求，以重新订购和重叠的资源要求，同时确保高前缀共享。我们在各种合成的多模式工作负载上评估Blendserve，并表明它提供了高达$ 1.44 \ times $ $吞吐量的提升，与广泛使用的行业标准，VLLM和SGLANG相比。

### Data-driven development of cycle prediction models for lithium metal batteries using multi modal mining 
[[arxiv](https://arxiv.org/abs/2411.17625)] [[cool](https://papers.cool/arxiv/2411.17625)] [[pdf](https://arxiv.org/pdf/2411.17625)]
> **Authors**: Jaewoong Lee,Junhee Woo,Sejin Kim,Cinthya Paulina,Hyunmin Park,Hee-Tak Kim,Steve Park,Jihan Kim
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: 30 pages, 7 figures
- **标题**: 使用多模态挖掘的锂金属电池的数据驱动的循环预测模型开发
- **领域**: 机器学习
- **摘要**: 数据驱动研究的最新进展表明，在理解材料及其性能之间的复杂关系方面具有很大的潜力。本文中，我们介绍了一种新型的多模态数据驱动方法，该方法采用自动电池数据收集器（ABC），该方法将大型语言模型（LLM）与自动图挖掘工具，材料图数字化器（MATGD）集成在一起。该平台可从各种文本和图形数据源中准确提取电池材料数据和可环保性能指标的最新精确提取。从通过ABC平台得出的数据库，我们开发了机器学习模型，这些模型可以准确预测锂金属电池的容量和稳定性，锂金属电池是有史以来第一个用于实现此类预测的模型。我们的模型还经过实验验证，确认了我们数据驱动方法的实际适用性和可靠性。

### Multiscale spatiotemporal heterogeneity analysis of bike-sharing system's self-loop phenomenon: Evidence from Shanghai 
[[arxiv](https://arxiv.org/abs/2411.17555)] [[cool](https://papers.cool/arxiv/2411.17555)] [[pdf](https://arxiv.org/pdf/2411.17555)]
> **Authors**: Yichen Wang,Qing Yu,Yancun Song
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 自行车共享系统的自行车自行车现象的多尺度时空异质性分析：上海的证据
- **领域**: 机器学习,计算机与社会
- **摘要**: 自行车共享是一种环保的共享移动性模式，但是其自动浮动现象，在几次使用时间后，自行车将自行车返回同一电台，对访问其服务的股权产生了重大影响。因此，这项研究通过空间自回归模型和双重机​​器学习框架进行了多尺度分析，以评估社会经济特征以及地理空间位置对地铁站和街头规模上自动浮动现象的影响。结果表明，自行车共享的自环强度在街道上表现出显着的空间滞后效应，并且与住宅用地的使用呈正相关。在中年居民，高固定就业和低汽车所有权的街道上，住宅用地使用的边际治疗效果更高。多模式的公共过境条件揭示了两个尺度上显着的正边缘治疗效果。为了增强自行车共享合作，我们主张在高层使用率和低公共汽车覆盖范围的地区增加自行车的可用性，并实施适应性的重新分配策略。

### APT: Architectural Planning and Text-to-Blueprint Construction Using Large Language Models for Open-World Agents 
[[arxiv](https://arxiv.org/abs/2411.17255)] [[cool](https://papers.cool/arxiv/2411.17255)] [[pdf](https://arxiv.org/pdf/2411.17255)]
> **Authors**: Jun Yu Chen,Tao Gao
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: 8 pages
- **标题**: APT：使用大型语言模型为开放世界代理商使用大型语言模型的建筑规划和文本到蓝图构造
- **领域**: 机器学习,人工智能
- **摘要**: 我们提出了APT，这是一种高级大型语言模型（LLM）驱动的框架，使自主代理能够在Minecraft环境中构建复杂而创造性的结构。与以前主要集中于基于技能的开放世界任务或依赖基于图像的扩散模型生成基于体素的结构的方法不同，我们的方法利用了LLMS的固有空间推理能力。通过采用经过思考链的分解以及多模式输入，该框架生成了详细的体系结构布局和蓝图，代理可以在零射击或少数射击学习方案下执行这些布局。我们的代理商都结合了内存和反射模块，以促进整个建筑过程中终身学习，适应性改进和误差校正。为了严格评估代理在这个新兴研究领域的表现，我们介绍了一个全面的基准，该基准由旨在测试创造力，空间推理，遵守游戏中规则的多种施工任务组成，并有效地集成了多模式指令。使用各种基于GPT的LLM后端和代理配置的实验结果证明了该代理的能力准确解释涉及许多项目，其位置和方向的广泛指令。代理成功地产生了具有内部功能（例如红石供电系统）的复杂结构。 A/B测试表明，包括内存模块的包含导致性能的显着提高，强调了其在持续学习和重复使用累积经验中的作用。此外，代理商意外的脚手架行为出现突出了未来LLM驱动的代理使用子例程计划的潜力，并利用LLMS自主发展类似人类的问题解决问题的技术的出现能力。

### Stratified Non-Negative Tensor Factorization 
[[arxiv](https://arxiv.org/abs/2411.18805)] [[cool](https://papers.cool/arxiv/2411.18805)] [[pdf](https://arxiv.org/pdf/2411.18805)]
> **Authors**: Alexander Sietsema,Zerrin Vural,James Chapman,Yotam Yaniv,Deanna Needell
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: 5 pages. Will appear in IEEE Asilomar Conference on Signals, Systems, and Computers 2024
- **标题**: 分层的非负张量分解
- **领域**: 机器学习,数值分析
- **摘要**: 非负矩阵分解（NMF）和非负张量分解（NTF）将非负高维数据分解为非负低级别组件。 NMF和NTF方法因其对大规模数据的内在解释性和有效性而受欢迎。最近的工作开发了Stratified-NMF，该NMF适用于数据可能来自不同来源（层）具有不同基础分布的政权，并试图恢复依赖地层的信息和跨层共享的全球主题。将分层-NMF应用于多模式数据需要跨模式平坦，因此失去张量内包含的几何结构。为了解决此问题，我们通过制定乘法更新规则并演示文本和图像数据的方法，将分层-NMF扩展到张量设置。我们发现，分层-NTF可以识别比分层-NMF较低的可解释的主题。我们还引入了该方法的正则化版本，并演示了其对图像数据的影响。

### MM-Path: Multi-modal, Multi-granularity Path Representation Learning -- Extended Version 
[[arxiv](https://arxiv.org/abs/2411.18428)] [[cool](https://papers.cool/arxiv/2411.18428)] [[pdf](https://arxiv.org/pdf/2411.18428)]
> **Authors**: Ronghui Xu,Hanyin Cheng,Chenjuan Guo,Hongfan Gao,Jilin Hu,Sean Bin Yang,Bin Yang
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: This is an extended version of the paper accepted by KDD 2025
- **标题**: MM路径：多模式，多晶格路径表示学习 - 扩展版本
- **领域**: 机器学习,人工智能
- **摘要**: 在智能运输中，开发有效的路径表示已变得越来越重要。尽管预训练的路径表示模型显示出改善的性能，但它们主要集中在单个模态数据（即道路网络）的拓扑结构上，俯瞰与路径相关图像相关的几何和上下文特征，例如遥感图像。与人类的理解相似，从多种方式整合信息可以提供更全面的观点，从而提高表示准确性和概括。然而，信息粒度的变化阻碍了基于道路网络的路径（路径）和基于图像的路径（图像路径）的语义一致性，而多模式数据的异质性为有效的融合和利用带来了重大挑战。在本文中，我们提出了一种新型的多模式，多晶格路径表示框架（MM-Path），该框架可以通过从道路路径和图像路径中整合模态来学习通用路径表示。为了增强多模式数据的一致性，我们制定了一种多跨性比对策略，该策略系统地将节点，道路子路径和道路路径与相应的图像贴片相关联，从而确保详细的本地信息和更广泛的全局环境的同步。为了有效地解决多模式数据的异质性，我们介绍了一种基于图的跨模式残留融合成分，旨在全面融合不同的方式和粒度范围。最后，我们在两个下游任务下对两个大型现实世界数据集进行了广泛的实验，从而验证了所提出的MM-Path的有效性。该代码可在以下网址获得：https：//github.com/decisionIntelligence/mm-path。

### Multimodal Integration of Longitudinal Noninvasive Diagnostics for Survival Prediction in Immunotherapy Using Deep Learning 
[[arxiv](https://arxiv.org/abs/2411.18253)] [[cool](https://papers.cool/arxiv/2411.18253)] [[pdf](https://arxiv.org/pdf/2411.18253)]
> **Authors**: Melda Yeghaian,Zuhir Bodalal,Daan van den Broek,John B A G Haanen,Regina G H Beets-Tan,Stefano Trebeschi,Marcel A J van Gerven
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: 使用深度学习的纵向非侵入性诊断进行免疫疗法生存预测的多模式整合
- **领域**: 机器学习,人工智能,定量方法
- **摘要**: 目的：使用人工智能分析非侵入性纵向和多模式数据可能会改变癌症患者的免疫疗法，从而铺平了精确医学的道路。方法：在这项研究中，我们整合了预处理和治疗的血液测量，开处方的药物和基于CT的基于CT的器官，由694名接受免疫疗法治疗的694例患者的大型泛滥群体，以预测短期和长期的整体生存期。通过利用最近的发展的结合，我们端对端训练了扩展多模式变压器的简单暂时关注（MMTSIMTA）网络的不同变体，以预测三，六，九和十二个月的死亡率。还将这些模型与结合了基于融合的集成方法的基线方法进行了比较。 Results: The strongest prognostic performance was demonstrated using the extended transformer-based multimodal model with area under the curves (AUCs) of $0.84 \pm $0.04, $0.83 \pm $0.02, $0.82 \pm $0.02, $0.81 \pm $0.03 for 3-, 6-, 9-, and 12-month survival prediction, respectively.结论：我们的发现表明，分析综合的早期治疗数据有可能预测免疫疗法患者的存活。使用我们基于变压器的扩展体系结构，将互补的非侵入性模式整合到一个联合训练的模型中，表现出改进的多模式预后性能，尤其是在短期生存预测中。

## 多代理系统(cs.MA:Multiagent Systems)

该领域共有 2 篇论文

### CPIG: Leveraging Consistency Policy with Intention Guidance for Multi-agent Exploration 
[[arxiv](https://arxiv.org/abs/2411.03603)] [[cool](https://papers.cool/arxiv/2411.03603)] [[pdf](https://arxiv.org/pdf/2411.03603)]
> **Authors**: Yuqian Fu,Yuanheng Zhu,Haoran Li,Zijie Zhao,Jiajun Chai,Dongbin Zhao
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: No comments
- **标题**: CPIG：利用一致性政策，并通过意图指导多代理探索
- **领域**: 多代理系统
- **摘要**: 有效的探索对于合作的多代理增强学习（MARL）至关重要，尤其是在稀疏回报的环境中。但是，由于依赖单峰政策，现有方法容易陷入当地的最佳状态，从而阻碍了对更好政策的有效探索。此外，在稀疏的奖励环境中，每个代理都倾向于获得稀缺的奖励，这对合作间合作提出了重大挑战。这不仅增加了政策学习的困难，而且还会降低多代理任务的整体绩效。为了解决这些问题，我们提出了意图指导（CPIG）的一致性政策，其中有两个主要组成部分：（a）引入多模式政策以增强代理商的勘探能力，以及（b）在代理商之间共享意图以促进代理商合作。对于组件（a），CPIG将一致性模型作为策略，利用其多模式性质和随机特征来促进探索。关于组成部分（b），我们介绍了一个意图学习者，从每个代理人的本地观察中推断出全球状态的意图。然后，这种意图是一致性政策的指导，促进了代理之间的合作。在多代理粒子环境（MPE）和多代理Mujoco（Mamujoco）中评估所提出的方法。经验结果表明，我们的方法不仅可以在密集的回报环境中实现与各种基准相当的性能，而且还可以显着提高稀疏回报的性能，而且表现优于最先进的（SOTA）算法20％。

### Agent-Based Modeling for Multimodal Transportation of $CO_2$ for Carbon Capture, Utilization, and Storage: CCUS-Agent 
[[arxiv](https://arxiv.org/abs/2411.14438)] [[cool](https://papers.cool/arxiv/2411.14438)] [[pdf](https://arxiv.org/pdf/2411.14438)]
> **Authors**: Majbah Uddin,Robin Clark,Michael Hilliard,Joshua Thompson,Matthew Langholtz,Erin Webb
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: $ CO_2 $的多模式运输的基于代理的建模用于碳捕获，利用和存储：CCUS-AGENT
- **领域**: 多代理系统
- **摘要**: 为了了解碳捕获，利用率和存储中实体之间的系统级相互作用（CCUS），基于代理的基础建模工具CCUS-Encent是为了大规模研究美国的运输流和基础设施的大规模研究。该工具的关键特征包括（i）模块化设计，（ii）多种运输模式，（iii）扩展功能以及（iv）测试针对各种系统组件和大小的各种系统组件和网络。探索了二种二氧化碳供应剂（例如，动力装置和工业设施）和需求代理（例如存储和利用网站）的五种匹配算法：最多的第一年（MPFY），全年最有利可图的（MPAY），最短的总距离（SDFY）（SDFY）（SDFY）（SDFY）（SDFY），整个距离（SDFY），整个距离（short Levers），以及所有的时间（sand short depand tess Day tess tess Day tess tess tess tess tess toess tess toess tess toess tess toess tess toess tess toess tess toess toess tess toess-ac a。在匹配之前，必须可用供应代理，需求代理和路线，并且连接必须有利可图。有利可图的连接意味着45Q税收抵免额的供应代理一部分必须支付供应代理成本和所有运输成本，而需求代理收入部分必须支付所有需求代理的费用。在美国连续的美国使用了5500多名供求代理和多模式CCUS运输基础设施的案例研究。结果表明，从2025年到2043年，有可能捕获超过90亿吨的二氧化碳（GT），如果捕获成本降低40％，这将显着增加到22 GT。 MPFY和SDFY算法在时间范围内捕获更多的CO2，而MPay和Sday算法在时间范围内捕获更多的二氧化碳。

## 多媒体(cs.MM:Multimedia)

该领域共有 9 篇论文

### Multimodal Graph Neural Network for Recommendation with Dynamic De-redundancy and Modality-Guided Feature De-noisy 
[[arxiv](https://arxiv.org/abs/2411.01561)] [[cool](https://papers.cool/arxiv/2411.01561)] [[pdf](https://arxiv.org/pdf/2411.01561)]
> **Authors**: Feng Mo,Lin Xiao,Qiya Song,Xieping Gao,Eryao Liang
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 多式模式图神经网络，用于推荐，具有动态降值和模态引导的特征DE-NOISY
- **领域**: 多媒体,信息检索
- **摘要**: 图形神经网络（GNN）在多模式推荐任务中变得至关重要，因为它们具有捕获相邻节点之间复杂关系的强大能力。但是，增加GNN中传播层的数量可能会导致冗余，这可能会对整体建议性能产生负面影响。此外，现有的建议任务方法将预处理的多模式特征映射到低维空间，这将使噪声无关，从而影响模型的表示能力。为了应对上述挑战，我们提出了多模式图神经网络的推荐（MGNM），并具有动态的降值和模态引导的特征De-noisy，该特征被分为局部和全球互动。最初，在局部交互过程中，我们集成了动态降低（DDR）损耗函数，该损失函数是通过利用特征系数矩阵的乘积和特征矩阵作为惩罚因素来实现的。它减少了由多个GNN层堆叠引起的多模式和行为特征的特征冗余效应。随后，在全球互动过程中，我们为每种模态开发了模态引导的全球特征净化器，以减轻模态噪声的影响。这是一种两倍的指导机制，消除了与用户偏好无关的模态特征，并在模式中捕获了复杂的关系。实验结果表明，与最先进的方法相比，MGNM在多模式信息的多模式信息上取得了卓越的性能。

### Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation 
[[arxiv](https://arxiv.org/abs/2411.00813)] [[cool](https://papers.cool/arxiv/2411.00813)] [[pdf](https://arxiv.org/pdf/2411.00813)]
> **Authors**: Sixu An,Xiangguo Sun,Yicong Li,Yu Yang,Guandong Xu
> **First submission**: 2024-10-25
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 来自在线短视频平台的人格分析具有多域适应性
- **领域**: 多媒体,人工智能,计算语言学,计算机视觉和模式识别,计算机与社会,机器学习,社交和信息网络,音频和语音处理
- **摘要**: 在线简短视频中的人格分析由于其在个性化推荐系统，情感分析和人为计算机互动中的应用而获得了突出。传统的评估方法，例如基于五大人格框架的问卷，受到自我报告偏见的限制，对于大规模或实时分析是不切实际的。利用简短视频中存在的丰富的多模式数据为更准确的个性推断提供了一种有希望的替代方法。但是，整合这些多样化和异步模式会带来重大挑战，尤其是在使时间变化的数据保持一致，并确保模型可以很好地推广到具有有限标记的数据的新领域。在本文中，我们提出了一个新型的多模式人格分析框架，该框架通过同步和整合多种模式的特征并通过域适应来增强模型概括来解决这些挑战。我们引入了基于时间戳的模态对准机制，该机制基于口语单词时间戳同步数据，确保跨模态的准确对应关系并促进有效的特征集成。为了捕获时间依赖性和模式间相互作用，我们采用双向长期的短期记忆网络和自我发项机制，从而使模型可以专注于人格预测的最有用的特征。此外，我们开发了一种基于梯度的域适应方法，该方法将知识从多个源域转移，以通过稀缺标记的数据来提高目标域中的性能。对现实世界数据集的广泛实验表明，我们的框架在人格预测任务中的表现显着优于现有方法，突出了其在捕获复杂行为提示和适应新领域的鲁棒性方面的有效性。

### Harmful YouTube Video Detection: A Taxonomy of Online Harm and MLLMs as Alternative Annotators 
[[arxiv](https://arxiv.org/abs/2411.05854)] [[cool](https://papers.cool/arxiv/2411.05854)] [[pdf](https://arxiv.org/pdf/2411.05854)]
> **Authors**: Claire Wonjeong Jo,Miki Wesołowska,Magdalena Wojcieszak
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 有害YouTube视频检测：在线危害和MLLM作为替代注释者的分类法
- **领域**: 多媒体,人工智能,计算机视觉和模式识别,计算机与社会
- **摘要**: 全球数十亿个用户都使用了简短的视频平台，例如YouTube，Instagram或Tiktok。这些平台将用户暴露于有害内容，范围从点击诱饵或身体危害到错误信息或在线仇恨。然而，由于对人类注释中涉及的损害以及有限的资源和心理伤害的了解不一致，检测有害视频仍然具有挑战性。因此，这项研究采取了检测视频内容危害的措施和方法。首先，我们为视频平台上的在线危害开发了全面的分类法，将其分为六类：信息，仇恨和骚扰，上瘾，点击诱饵，性和身体伤害。接下来，我们将多模式大语模型建立为有害视频的可靠注释者。我们使用14个图像帧，1个缩略图和文本元数据分析了19,422个YouTube视频，将人群工人（MTURK）和GPT-4-Turbo的准确性与域专家注释作为黄金标准进行了比较。我们的结果表明，GPT-4-Turbo在二进制分类（有害与无害）和多标签危害分类任务中的表现都优于众议员。从方法上讲，这项研究扩展了LLM在文本注释和二进制分类之外的多标签和多模式上下文中的应用。实际上，我们的研究通过指导视频平台上有害内容的定义和确定有害内容来促进在线危害。

### Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML, CNN, and GCN Models using Audio-Visual Features 
[[arxiv](https://arxiv.org/abs/2411.08885)] [[cool](https://papers.cool/arxiv/2411.08885)] [[pdf](https://arxiv.org/pdf/2411.08885)]
> **Authors**: Abdelrahman Abdelwahab,Akshaj Vishnubhatla,Ayaan Vaswani,Advait Bharathulwar,Arnav Kommaraju
> **First submission**: 2024-10-26
> **First announcement**: 2024-11-14
> **comment**: 11 pages, 18 figures
- **标题**: 增强谎言检测准确性：使用视听功能对经典ML，CNN和GCN模型的比较研究
- **领域**: 多媒体,人工智能,计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 测谎仪测试中的不准确性通常会导致错误的信念，虚假信息和偏见，所有这些都会对法律和政治制度产生重大影响。最近，分析面部微表达是一种检测欺骗的方法。但是，当前模型尚未达到高准确性和可推广性。这项研究的目的是帮助解决这些问题。本研究中使用的独特多模式变压器体系结构通过使用听觉输入，视觉面部微表达和手动转录的手势注释来改善以前的方法，从而更接近可靠的非侵入性谎言检测模型。分别使用视觉变压器和开毛模型提取视觉和听觉特征，然后将其与参与者微表达和手势的转录串联。使用这些加工和串联的特征对各种模型进行了分类和真理的分类。 CNN Conv1D多模型模型的平均准确度为95.4％。但是，仍需要进一步的研究来创建更高质量的数据集，甚至需要更广泛的模型来为更多样化的应用程序。

### A Novel Multimodal System to Predict Agitation in People with Dementia Within Clinical Settings: A Proof of Concept 
[[arxiv](https://arxiv.org/abs/2411.08882)] [[cool](https://papers.cool/arxiv/2411.08882)] [[pdf](https://arxiv.org/pdf/2411.08882)]
> **Authors**: Abeer Badawi,Somayya Elmoghazy,Samira Choudhury,Sara Elgazzar,Khalid Elgazzar,Amer Burhan
> **First submission**: 2024-10-25
> **First announcement**: 2024-11-14
> **comment**: No comments
- **标题**: 一种新型的多模式系统，可预测临床环境中痴呆症患者的躁动：概念证明
- **领域**: 多媒体,人工智能,计算机视觉和模式识别
- **摘要**: 痴呆症是一种神经退行性疾病，结合了几种疾病，并影响了世界各地数百万人及其周围的疾病。尽管认知障碍是严重残疾的，但它是痴呆症的非认知特征，被称为神经精神症状（NPS），与生活质量降低最紧密相关。患有痴呆症患者（PWD）患者的躁动和侵略（AA）有助于困扰和增加医疗保健需求。当前的评估方法依赖于护理人员干预和事件报告，引入主观性和偏见。人工智能（AI）和预测算法为实时使用时可检测PWD中的AA发作的潜在解决方案。我们提出了一个为期5年的研究系统，该系统利用EmbracePlus腕带和视频检测系统整合了多模式方法，以预测严重痴呆症患者的AA。我们对安大略省海岸心理健康研究所的三名参与者进行了一项试点研究，以验证系统的功能。该系统从EmbracePlus腕带中收集和流程原始生物标志物，以准确预测AA。该系统还至少在AA事件发生前六分钟检测到了启动模式，这是以前从EmbracePlus腕带中发现的。此外，保护隐私的视频系统使用掩盖工具将人们的特征隐藏在框架中，并采用深度学习模型进行AA检测。视频系统还有助于确定标签的煽动事件的实际开始和结束时间。初步数据分析的有希望的结果强调了系统预测AA事件的能力。提议的系统实时自主运行并在没有外部辅助的情况下识别AA和凝时症状的能力代表了该研究领域的重要里程碑。

### CMATH: Cross-Modality Augmented Transformer with Hierarchical Variational Distillation for Multimodal Emotion Recognition in Conversation 
[[arxiv](https://arxiv.org/abs/2411.10060)] [[cool](https://papers.cool/arxiv/2411.10060)] [[pdf](https://arxiv.org/pdf/2411.10060)]
> **Authors**: Xiaofei Zhu,Jiawei Cheng,Zhou Yang,Zhuo Chen,Qingyang Wang,Jianfeng Yao
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: No comments
- **标题**: CMATH：跨模式增强变压器，具有分层变分蒸馏，以在对话中进行多模式情绪识别
- **领域**: 多媒体,计算语言学
- **摘要**: 对话（MER）中的多模式情感识别旨在通过整合多模式信息来准确地识别对话说法中的情绪。以前的方法通常将多模式信息视为均等的质量，并采用对称架构进行多模式融合。但是，实际上，不同方式的质量通常会大不相同，在处理不平衡的模态信息时，使用对称架构很难准确地识别对话情绪。此外，单个粒度中的多模式信息融合可能无法充分整合模态信息，从而加剧了情绪识别中的不准确性。在本文中，我们提出了一种新型的交叉模式增强变压器，其层次变化蒸馏称为cmath，该变化物由两个主要成分组成，即多模式相互作用融合和分层变分蒸馏。前者由两个子模型组成，包括模态重建和跨模型增强变压器（CMA变形器），其中模式重建侧重于获得每种模态的高质量压缩表示，而CMA-变形型将采用一种不对称融合策略，以一种不对称的融合策略将一种模式视为中心模式，并将其视为一个模式。后者首先设计了一个变分融合网络，以将CMA-Transformer学到的细粒度表示融合为粗粒表示。然后，它引入了一个分层蒸馏框架，以保持具有不同粒度的模态表示之间的一致性。 IEMOCAP和MELD数据集的实验表明，我们提出的模型的表现优于先前的最新基线。可以在https://github.com/ cjw-mer/cmath上获得实现代码。

### MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity Prediction 
[[arxiv](https://arxiv.org/abs/2411.15455)] [[cool](https://papers.cool/arxiv/2411.15455)] [[pdf](https://arxiv.org/pdf/2411.15455)]
> **Authors**: Jiacheng Lu,Mingyuan Xiao,Weijian Wang,Yuxin Du,Yi Cui,Jingnan Zhao,Cheng Hua
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: 14 pages,9 figures
- **标题**: MUFM：微型视频流行度预测的Mamba增强反馈模型
- **领域**: 多媒体,人工智能
- **摘要**: 微观视频的激增正在改变受欢迎程度的概念。随着研究人员深入研究大量的多模式数据集，人们对了解这种受欢迎程度的起源以及推动其快速扩张的力量越来越兴趣。最近的研究表明，简短视频的病毒性不仅与它们固有的多模式内容有关，而且还受到受众反馈驱动的平台建议的强度的影响。在本文中，我们介绍了一个框架，用于根据Mamba Hawkes流程来捕获用户反馈和动态事件交互中的长期依赖。我们在大规模开源多模式数据集上的实验表明，我们的模型在各种指标上的最先进方法显着超过23.2％。我们认为，我们的模型能够绘制用户反馈行为序列中关系的能力不仅有助于下一代建议算法和平台应用程序的发展，而且还增强了我们对微观视频传播及其更广泛的社会影响的理解。

### Gotta Hear Them All: Sound Source Aware Vision to Audio Generation 
[[arxiv](https://arxiv.org/abs/2411.15447)] [[cool](https://papers.cool/arxiv/2411.15447)] [[pdf](https://arxiv.org/pdf/2411.15447)]
> **Authors**: Wei Guo,Heng Wang,Jianbo Ma,Weidong Cai
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 18 pages, 13 figures, source code available at https://github.com/wguo86/SSV2A
- **标题**: 一定会听到他们的全部声音：声音来源的愿景对音频产生
- **领域**: 多媒体,计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 愿景对审计（V2A）合成在多媒体中具有广泛的应用。 V2A方法的最新进展使得从视频或静止图像的输入中生成相关音频成为可能。但是，这一代人的沉浸感和表现力是有限的。一个可能的问题是，现有方法仅依赖于全局场景，并忽略了本地发声对象的细节（即声源）。为了解决此问题，我们提出了一个声音源吸引V2A（SSV2A）发电机。 SSV2A能够从具有视觉检测和交叉模式翻译的场景中局部感知的多模式声源。然后，它对比学习了一个跨模式声源（CMS）歧管，以使每个源歧义每个源。最后，我们认真地将其CMSS语义混合到丰富的音频表示形式中，验证的音频发电机从中输出声音。为了建模CMSS歧管，我们从VGGSound策划了一种新颖的单声音源视觉ADIO数据集VGGS3。我们还设计了一个声源匹配分数，以衡量局部音频相关性。通过在声音级别上解决V2A的生成，SSV2A超过了一代忠诚度和相关性的最新方法，这是广泛的实验所证明的。我们进一步证明了SSV2A通过组合视觉，文本和音频条件来实现直观的V2A控制能力。可以在https://ssv2a.github.io/ssv2a-demo上尝试和听到我们这一代。

### Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis 
[[arxiv](https://arxiv.org/abs/2411.17690)] [[cool](https://papers.cool/arxiv/2411.17690)] [[pdf](https://arxiv.org/pdf/2411.17690)]
> **Authors**: Akshita Gupta,Tatiana Likhomanenko,Karren Dai Yang,Richard He Bai,Zakaria Aldeneh,Navdeep Jaitly
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: Visatronic：语音合成的仅多模式解码器模型
- **领域**: 多媒体,计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 在本文中，我们提出了一项新任务 - 从人及其成绩单（VTTS）的视频中产生演讲，以激发多模式语音生成的新技术。这项任务概括了从裁剪唇部视频中生成语音的任务，并且比从视频和文字中生成通用音频剪辑（例如，狗吠叫）的任务还要复杂。任务的多语言版本可能会导致跨语性配音的新技术。我们还为此任务提供了一个仅解码器的多模式模型，我们称之为Visatronic。该模型将视觉，文本和语音直接嵌入变压器模型的常见子空间中，并使用自回归损失来学习以扬声器视频和语音的成绩单为条件的离散MEL光谱图的生成模型。通过将所有模式嵌入一个共同的子空间中，Visatronic可以比仅使用文本或视频作为输入的模型获得改进的结果。此外，与依靠唇部检测器和复杂的体系结构融合方式的同时，在产生更好的结果的同时，它为多模式语音生成提供了一种更简单的方法。由于该模型足够灵活，可以容纳不同的订购输入方式作为顺序，因此我们仔细探索了不同的策略，以更好地了解将信息传播到生成步骤的最佳方法。为了促进对VTTS的进一步研究，我们将发布（i）大规模voxceleb2数据集的代码，（ii）清理转录，以及（iii）为VTTS的标准化评估协议，该协议既包含客观和主观指标。

## 机器人技术(cs.RO:Robotics)

该领域共有 18 篇论文

### 3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing 
[[arxiv](https://arxiv.org/abs/2410.24091)] [[cool](https://papers.cool/arxiv/2410.24091)] [[pdf](https://arxiv.org/pdf/2410.24091)]
> **Authors**: Binghao Huang,Yixuan Wang,Xinyi Yang,Yiyue Luo,Yunzhu Li
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: Accepted at Conference on Robot Learning (CoRL) 2024
- **标题**: 3D-VITAC：通过视觉触觉感应学习细粒度的操纵
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 触觉和视觉感知对于人类与环境进行细粒度的相互作用至关重要。为机器人开发类似的多模式传感功能可以显着提高和扩展其操纵技巧。本文介绍了\ textbf {3d-vitac}，这是一种用于灵巧双人操作的多模式感应和学习系统。我们的系统具有配备密集传感单元的触觉传感器，每个传感器覆盖3 $ mm^2 $。这些传感器是低成本且灵活的，可提供详细且广泛的物理接触覆盖范围，从而有效地补充了视觉信息。为了整合触觉和视觉数据，我们将它们融合到一个统一的3D表示空间中，以保留其3D结构和空间关系。然后可以将多模式表示与模仿学习的扩散策略结合在一起。通过具体的硬件实验，我们证明，即使是低成本的机器人也可以执行精确的操作，并显着超过视力的策略，尤其是在与脆弱项目的安全互动和执行涉及涉及手柄操纵的长途任务的安全互动中。我们的项目页面可在\ url {https://binghao-huang.github.io/3d-vitac/}中找到。

### Know Where You're Uncertain When Planning with Multimodal Foundation Models: A Formal Framework 
[[arxiv](https://arxiv.org/abs/2411.01639)] [[cool](https://papers.cool/arxiv/2411.01639)] [[pdf](https://arxiv.org/pdf/2411.01639)]
> **Authors**: Neel P. Bhatt,Yunhao Yang,Rohan Siva,Daniel Milan,Ufuk Topcu,Zhangyang Wang
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-04
> **comment**: Fine-tuned models, code, and datasets are available at https://tinyurl.com/uncertainty-disentanglement
- **标题**: 知道在使用多模式基础模型计划时不确定的位置：正式框架
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 多模式基础模型通过处理感官输入来生成可行的计划，为机器人感知和计划提供了一个有希望的框架。但是，解决感知（感觉解释）和决策（计划生成）中的不确定性仍然是确保任务可靠性的关键挑战。我们提出了一个全面的框架，以解开，量化和减轻这两种形式的不确定性。我们首先引入了一个不确定性分解的框架，隔离了与生成计划的鲁棒性有关的视觉理解和决策不确定性的限制引起的感知不确定性。为了量化每种类型的不确定性，我们提出了针对感知和决策的独特特性量身定制的方法：我们使用共形预测来校准感知不确定性，并引入正式使用方法驱动的预测（FMDP）来量化决策不确定性，利用形式验证技术来获得理论保证。在此量化的基础上，我们实施了两种有针对性的干预机制：一个主动感应过程，动态地重新观察高确定性场景，以增强视觉输入质量和一个自动化的修复程序，以对高确定性数据进行微调模型，从而提高其满足任务规格的能力。现实世界和模拟机器人任务中的经验验证表明，与基准相比，我们的不确定性分离框架可将可变性降低多达40％，并将任务成功率提高5％。这些改进归因于干预措施的综合作用，并强调了不确定性分解的重要性，这有助于有针对性的干预措施，从而增强了自主系统的鲁棒性和可靠性。

### Vocal Sandbox: Continual Learning and Adaptation for Situated Human-Robot Collaboration 
[[arxiv](https://arxiv.org/abs/2411.02599)] [[cool](https://papers.cool/arxiv/2411.02599)] [[pdf](https://arxiv.org/pdf/2411.02599)]
> **Authors**: Jennifer Grannen,Siddharth Karamcheti,Suvir Mirchandani,Percy Liang,Dorsa Sadigh
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: Published at CoRL 2024. 24 pages, 8 figures. Project Page: https://vocal-sandbox.github.io
- **标题**: 声带沙盒：持续学习和适应人类机器人协作
- **领域**: 机器人技术,人工智能,计算语言学,人机交互,机器学习
- **摘要**: 我们介绍了声乐沙盒，这是一个框架，用于在位置环境中实现无缝的人类机器人协作。我们框架中的系统的特点是它们能够从多种教学方式，例如口语对话，对象关键点和动力学示范等各种教学方式中适应和不断学习的能力。为了启用这种适应，我们设计了轻巧且可解释的学习算法，使用户可以实时建立理解并与机器人的能力共同适应新的行为。例如，在演示了“跟踪”对象的新的低级技能之后，当被要求跟踪新对象时，向用户提供了机器人预期运动的轨迹可视化。同样，用户通过口语对话教授高级计划行为，使用验证的语言模型来综合行为，例如“包装对象”作为低级技能$  -  $ $概念的组成，可以重复使用和构建。我们在两种设置中评估声带沙盒：协作礼品袋组件和乐高定格动画。在第一个环境中，我们与8位非专家参与者进行系统的消融和用户研究，强调了多层次教学的影响。在23个小时的总机器人交互时间中，用户教授17种新的高级行为，平均16个新型的低级技能，与基准相比，需要减少22.1％的主动监督，并且产生更复杂的自主性能（+19.7％），失败较少（-67.1％）。在定性上，由于易于使用（+20.6％）和整体性能（+13.9％），用户非常喜欢声带系统。最后，我们将经验丰富的系统用户与机器人配对，以拍摄定格动画。在两个小时的持续协作中，用户逐渐教授更复杂的运动技能，以拍摄52秒（232帧）电影。

### Digitizing Touch with an Artificial Multimodal Fingertip 
[[arxiv](https://arxiv.org/abs/2411.02479)] [[cool](https://papers.cool/arxiv/2411.02479)] [[pdf](https://arxiv.org/pdf/2411.02479)]
> **Authors**: Mike Lambeta,Tingfan Wu,Ali Sengul,Victoria Rose Most,Nolan Black,Kevin Sawyer,Romeo Mercado,Haozhi Qi,Alexander Sohn,Byron Taylor,Norb Tydingco,Gregg Kammerer,Dave Stroud,Jake Khatha,Kurt Jenkins,Kyle Most,Neal Stein,Ricardo Chavira,Thomas Craven-Bartle,Eric Sanchez,Yitian Ding,Jitendra Malik,Roberto Calandra
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: 28 pages
- **标题**: 用人工多模式指尖数字化触摸
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 触摸是一种至关重要的感应方式，可提供有关对象属性以及与物理环境相互作用的丰富信息。人类和机器人都受益于使用触摸来感知和与周围环境相互作用（Johansson和Flanagan，2009; Li等，2020; Calandra等，2017）。但是，没有现有的系统通过半球形的实施例提供丰富的多模式数字触摸感应功能。在这里，我们描述了一些概念和技术创新，以改善触摸的数字化。这些进步体现在具有先进感应功能的人造手指形传感器中。值得注意的是，该指尖包含高分辨率传感器（约830万个符号），这些传感器响应全向触摸，捕获多模式信号，并使用设备上的人工智能实时处理数据。评估表明，人工指尖可以解决较小至7 um的空间特征，觉得正常和剪切力分别分辨率为1.01 mn和1.27 mn，可感知的振动最多可达10 kHz，感官热，甚至是感性的气味。此外，它嵌入了设备的AI神经网络加速器，该加速器在机器人上充当外周神经系统，并模仿人类中发现的反射弧。这些结果证明了通过超人性能进行数字化触摸的可能性。这些含义是深远的，我们预计机器人技术（工业，医学，农业和消费者级别），虚拟现实和远程敏感，假肢和电子商务的潜在应用。为了大规模数字化触摸，我们开源一个模块化平台，以促进对触摸性质的未来研究。

### DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution 
[[arxiv](https://arxiv.org/abs/2411.02359)] [[cool](https://papers.cool/arxiv/2411.02359)] [[pdf](https://arxiv.org/pdf/2411.02359)]
> **Authors**: Yang Yue,Yulin Wang,Bingyi Kang,Yizeng Han,Shenzhi Wang,Shiji Song,Jiashi Feng,Gao Huang
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: 25 pages, 6 figures, NeurIPS 2024
- **标题**: 鹿VLA：多模式大语言模型的动态推断，以进行有效的机器人执行
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: MLLM通过复杂的语言和视觉数据表现出了非凡的理解和推理能力。这些进步刺激了建立通才的机器人MLLM熟练理解复杂的人类指示并完成各种具体任务的愿景。但是，由于在机器人平台上提供的计算和内存能力通常有限，为现实世界机器人开发MLLM是具有挑战性的。相比之下，MLLM的推断涉及存储数十亿个参数并执行巨大的计算，从而施加了重大的硬件需求。在我们的论文中，我们提出了一个动态的早期效果框架，用于机器人视觉 - 语言动作模型（deer-vla或Simply Deer），该框架会根据手头的每种情况自动调整激活的MLLM的大小。该方法利用MLLM中的多EXIT体系结构，该模型一旦在特定情况下激活了模型的适当大小，从而允许模型终止处理，从而避免了进一步的冗余计算。此外，我们开发了新的算法，这些算法以鹿的早期终止标准为基于预定义的需求，例如平均计算成本（即功耗）以及峰值计算消耗（即延迟）和GPU存储器的使用。这些增强能力确保鹿在不同的资源限制下有效地运行，同时保持竞争性能。在加尔文机器人操纵基准上，鹿表明，LLM的计算成本显着降低了5.2-6.5x，而LLM的GPU记忆则以2-6倍的速度降低，而不会损害性能。代码和检查点可在https://github.com/yueyang130/deer-vla上找到。

### The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare 
[[arxiv](https://arxiv.org/abs/2411.03287)] [[cool](https://papers.cool/arxiv/2411.03287)] [[pdf](https://arxiv.org/pdf/2411.03287)]
> **Authors**: Souren Pashangpour,Goldie Nejat
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: ef:MDPI Robotics 2024, 13(8)
- **标题**: 智能医疗保健的未来：对机器人使用大语言模型进行医疗保健的整合和影响的系统分析和讨论
- **领域**: 机器人技术,人工智能,新兴技术,人机交互,系统与控制
- **摘要**: 大型语言模型（LLM）在医疗保健机器人技术中的潜在用途可以帮助解决全球医疗保健系统的巨大需求，以了解衰老的人口统计学和医疗保健专业人员的短缺。即使LLM已经纳入医学以协助临床医生和患者，但尚未探索LLM在医疗保健机器人中的整合。在此观点论文中，我们研究了机器人和LLMS的开创性发展，以唯一确定通过人类机器人互动（HRIS），语义推理和任务计划来设计基于健康特定LLM的机器人所需的系统要求。此外，我们讨论了这一新兴创新领域的道德问题，公开挑战以及潜在的未来研究方向。

### DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation 
[[arxiv](https://arxiv.org/abs/2411.04999)] [[cool](https://papers.cool/arxiv/2411.04999)] [[pdf](https://arxiv.org/pdf/2411.04999)]
> **Authors**: Peiqi Liu,Zhanqiu Guo,Mohit Warke,Soumith Chintala,Chris Paxton,Nur Muhammad Mahi Shafiullah,Lerrel Pinto
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Website: https://dynamem.github.io
- **标题**: Demandem：开放世界移动操作的在线动态时空声音记忆
- **领域**: 机器人技术,机器学习
- **摘要**: 开放式移动操作已经取得了重大进展，该目标是使机器人在自然语言描述的任何环境中执行任务。但是，大多数当前系统都采用静态环境，这限制了系统在实际情况下的适用性，在现实情况下，由于人类干预或机器人自己的行动，环境经常发生变化。在这项工作中，我们提出了Demanceem，这是一种使用动态空间语义内存来代表机器人环境的开放世界移动操作的新方法。 Dynamem构建了3D数据结构，以维持点云的动态内存，并使用多模式LLMS或由先进的视觉语言模型生成的多模式LLM或开放式唱片代理特征来访问开放式摄物对象本地化查询。由Demanceem提供支持，我们的机器人可以探索新颖的环境，搜索在内存中找不到的对象，并在对象移动，出现或消失时不断更新内存。我们在三个真实和九个离线场景中在拉伸SE3机器人上进行了广泛的实验，并在非平稳对象上获得了70％的平均拾音成功率，这比最先进的静态系统的增长超过2倍。我们的代码以及我们的实验和部署视频是开源的，可以在我们的项目网站上找到：https：//dynamem.github.io/

### Vision Language Models are In-Context Value Learners 
[[arxiv](https://arxiv.org/abs/2411.04549)] [[cool](https://papers.cool/arxiv/2411.04549)] [[pdf](https://arxiv.org/pdf/2411.04549)]
> **Authors**: Yecheng Jason Ma,Joey Hejna,Ayzaan Wahid,Chuyuan Fu,Dhruv Shah,Jacky Liang,Zhuo Xu,Sean Kirmani,Peng Xu,Danny Driess,Ted Xiao,Jonathan Tompson,Osbert Bastani,Dinesh Jayaraman,Wenhao Yu,Tingnan Zhang,Dorsa Sadigh,Fei Xia
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: Project website and demo: https://generative-value-learning.github.io/
- **标题**: 视觉语言模型是文化值学习者
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 预测视觉轨迹的时间进步对于可以学习，适应和改进的智能机器人很重要。但是，在不同的任务和域中学习此类进度估计器或时间价值函数需要大量可以扩展和概括的不同数据和方法。为了应对这些挑战，我们提出了生成价值学习（\ GVL），这是一种通用价值函数估计器，它利用嵌入视觉模型（VLMS）中的世界知识来预测任务进度。天真地要求VLM预测视频序列的值，因为连续帧之间的时间相关性很强。取而代之的是，GVL将价值估计作为时间订购问题提出，而不是洗牌视频帧；这项看似更具挑战性的任务鼓励VLM更完全利用其基本的语义和时间基础能力，以根据其感知的任务进度来区分框架，从而产生明显的更好的价值预测。如果没有任何机器人或任务特定培训，GVL可以在零射击中进行零射击，而几乎没有射击可以预测各种机器人平台的300多个不同的现实世界任务的有效值，包括具有挑战性的双层操纵任务。此外，我们证明GVL允许通过异构任务和实施方案（例如人类视频）的示例进行灵活的多模式中的学习学习。 GVL的一般性可以使各种与视觉运动策略学习有关的下游应用程序，包括数据集过滤，成功检测和优势加权回归 - 所有这些都没有任何模型培训或填充。

### Querying Perception Streams with Spatial Regular Expressions 
[[arxiv](https://arxiv.org/abs/2411.05946)] [[cool](https://papers.cool/arxiv/2411.05946)] [[pdf](https://arxiv.org/pdf/2411.05946)]
> **Authors**: Jacob Anderson,Georgios Fainekos,Bardh Hoxha,Hideki Okamoto,Danil Prokhorov
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: This work has been submitted to the International Journal on Software Tools for Technology Transfer
- **标题**: 带有空间正则表达式的查询感知流
- **领域**: 机器人技术,计算机视觉和模式识别,形式语言和自动机理论
- **摘要**: 机器人技术，制造和数据分析等领域的感知会产生大量的时间和空间数据，以有效捕获其环境。但是，对于特定方案进行分类是一个细致且容易出错的过程，通常取决于应用程序，并且缺乏通用性和可重复性。在这项工作中，我们将Spress作为一种新颖的查询语言引入，用于模式匹配，以匹配感知流，其中包含来自多模式动态环境的空间和时间数据。为了强调Spres的功能，我们开发了Strem工具作为感知数据的离线和在线模式匹配框架。我们通过对公开可用的AV数据集（Woven Planet感知）的案例研究来证明Strem的离线功能及其在线功能，通过将ROS中的Strem与Carla Simulator集成的案例研究。我们还对各种SPRE查询进行了性能基准实验。使用我们的匹配框架，我们能够在296毫秒内找到20,000多次匹配，从而使Strem适用于运行时监视应用程序。

### Prediction of Acoustic Communication Performance for AUVs using Gaussian Process Classification 
[[arxiv](https://arxiv.org/abs/2411.07933)] [[cool](https://papers.cool/arxiv/2411.07933)] [[pdf](https://arxiv.org/pdf/2411.07933)]
> **Authors**: Yifei Gao,Harun Yetkin,McMahon James,Daniel J. Stilwell
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: No comments
- **标题**: 使用高斯流程分类的AUV的声学沟通性能预测
- **领域**: 机器人技术,机器学习
- **摘要**: 合作自动水下车辆（AUV）通常依靠声学交流来有效地协调其行动。但是，随着车辆之间的通信范围的增加，水下声学通信的可靠性降低了。因此，合作AUV的团队通常会对他们可靠交流的最大范围进行保守的假设。为了解决这一限制，我们提出了一种新颖的方法，该方法涉及学习一张基于发射和接收车辆位置的成功沟通可能性的地图。该概率通信图解释了诸如在给定位置在车辆，环境噪声和多路径效应之间的范围。为了实现这一目标，我们研究了高斯流程二进制分类以生成所需的通信图的应用。我们将现有结果专门用于此特定的二进制分类问题，并探索将车辆位置中不确定性纳入映射过程的方法。此外，我们将使用二进制分类生成的概率通信图的预测性能与使用高斯过程回归生成的信噪比（SNR）通信图生成的概率通信图。使用一对弗吉尼亚理工学院690 AUVS在试验期间收集的通信和导航数据对我们的方法进行了实验验证。

### ClevrSkills: Compositional Language and Visual Reasoning in Robotics 
[[arxiv](https://arxiv.org/abs/2411.09052)] [[cool](https://papers.cool/arxiv/2411.09052)] [[pdf](https://arxiv.org/pdf/2411.09052)]
> **Authors**: Sanjay Haresh,Daniel Dijkman,Apratim Bhattacharyya,Roland Memisevic
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: To appear at NeurIPS 2024 (D&B track)
- **标题**: CLEVRSKILLS：机器人技术中的组成语言和视觉推理
- **领域**: 机器人技术,机器学习
- **摘要**: 机器人技术本质上是高度组成的。例如，要执行一项高级任务，例如清洁桌子，机器人必须采用将效应子移动到桌子上的对象的低级功能，请捡起它们，然后将它们逐一将桌子移开，同时重新评估此过程中随之而来的动态场景。鉴于大型视觉语言模型（VLM）在需要高水平，类似人类的推理的许多任务上显示出进展，我们提出一个问题：如果教授模型的必要低级功能，它们是否可以通过新颖的方法来完成有趣的高级任务，例如清洁桌子，而无需明确的教学？为此，我们介绍了Clevrskills-用于机器人技术组成推理的基准套件。 CLEVRSKILLS是在Maniskill2模拟器和随附数据集的顶部开发的环境套件。该数据集包含在一系列具有语言和视觉注释的机器人任务上生成的轨迹以及多模式提示作为任务规范。该套件包括一系列具有三个层次理解的任务课程，从需要基本运动技能的简单任务开始。我们在CLEVRSKILLS上基准了多个不同的VLM基准，并表明，即使在大量任务上进行了预先培训之后，这些模型在机器人任务中的组成推理中都失败了。

### Lo-MARVE: A Low Cost Autonomous Underwater Vehicle for Marine Exploration 
[[arxiv](https://arxiv.org/abs/2411.08605)] [[cool](https://papers.cool/arxiv/2411.08605)] [[pdf](https://arxiv.org/pdf/2411.08605)]
> **Authors**: Karl Mason,Daniel Kelly
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: This paper was presented at the 12th International Conference on Control, Mechatronics and Automation (ICCMA 2024), held in London, UK, from November 11-13, 2024
- **标题**: Lo-Marve：一辆低成本的自动驾驶水下汽车用于海洋勘探
- **领域**: 机器人技术,人工智能
- **摘要**: 本文介绍了低成本的海洋自动驾驶机器人车辆探险器（LO-MARVE），这是一种新型的自动水下车辆（AUV），旨在为浅水环境中的水下勘探和环境监测提供低成本解决方案。 Lo-Marve为现有AUV提供了一种具有成本效益的替代方法，具有模块化设计，低成本传感器和无线通信功能。 Lo-Marve的总成本约为500欧元。使用Raspberry Pi 4B微处理器开发了Lo-Marve，并用Python编写了控制软件。拟议的AUV通过实验室环境外，在爱尔兰戈尔韦河里河的淡水环境中的现场测试得到了验证。这证明了其自主导航，收集数据并在受控实验室环境之外进行有效交流的能力。 Lo-Marve在现实世界环境中的成功部署验证了其概念证明。

### Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies 
[[arxiv](https://arxiv.org/abs/2411.10171)] [[cool](https://papers.cool/arxiv/2411.10171)] [[pdf](https://arxiv.org/pdf/2411.10171)]
> **Authors**: Anant Garg,K Madhava Krishna
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: Submitted to IROS 2025
- **标题**: 想象2驱动器：通过多模式扩散政策利用高保真世界模型
- **领域**: 机器人技术,人工智能
- **摘要**: 基于世界模型的强化学习（WMRL）可以通过减少可能是昂贵和不安全的在线互动的需求，尤其是对于自动驾驶，可以实现样本有效的政策学习。但是，现有的世界模型通常会遭受低预测的保真度和复杂的一步错误，从而导致政策降级在远距离上。此外，传统的RL政策（通常是确定性或基于高斯的单一政策）未能捕获复杂驾驶场景中决策的多模式性质。为了应对这些挑战，我们提出了一个新颖的WMRL框架Imagine-2-Drive，该框架将高保真世界模型与基于多模式扩散的政策参与者集成在一起。它由两个关键组成部分：Diffreamer组成，这是一个基于扩散的世界模型，同时生成未来的观察，减轻误差积累和DPA（扩散策略参与者），这是一种基于扩散的策略，模拟了多样化和多模式轨迹分布的模型。通过在Diffreamer中培训DPA，我们的方法可以通过最少的在线互动来实现强大的策略学习。我们使用标准驾驶基准评估了Carla中的方法，并证明它表现优于先前的世界模型基线，将路线的完成和成功率分别提高了15％和20％。

### GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping 
[[arxiv](https://arxiv.org/abs/2411.12286)] [[cool](https://papers.cool/arxiv/2411.12286)] [[pdf](https://arxiv.org/pdf/2411.12286)]
> **Authors**: Teli Ma,Zifan Wang,Jiaming Zhou,Mengmeng Wang,Junwei Liang
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: Glover：可推广的开放式视频计师负担能力推理，以任务为导向的抓握
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 根据人体规格推断任意物体的负担得起的部分（即可抓地）部分对于前进的开放式摄影操作至关重要。然而，目前的掌握计划者受到有限的视觉理解和耗时的3D辐射模型的阻碍，从而限制了实时的，开放式唱片的互动与对象。为了解决这些局限性，我们建议Glover是一个统一的可推广的开放式摄影量负担推理框架，该框架对大型语言模型（LLMS）进行了微调，以预测RGB特征空间内的可移动物体零件的视觉负担能力。我们从人类对象相互作用中编译了一个超过10,000张图像的数据集，并用统一的视觉和语言负担能标记注释，以实现多模式微调。 Glover从LLMS继承了世界知识和常识性推理，从而促进了更细粒度的对象理解和复杂的工具使用推理。为了实现有效的现实世界部署，我们提出了负担得起的掌握估计（AGE），这是一种非参数抓握的策划者，将握把姿势与来自负担能力数据得出的超等分表面保持一致。在30个现实世界中的评估中，格洛弗（Glover）在零件识别中达到了86.0％的成功率，握把的成功率为76.3％，速度的负担能力推理速度约为330倍，抓握姿势估计的速度比以前的先前的速度快40倍。

### Target Height Estimation Using a Single Acoustic Camera for Compensation in 2D Seabed Mosaicking 
[[arxiv](https://arxiv.org/abs/2411.12338)] [[cool](https://papers.cool/arxiv/2411.12338)] [[pdf](https://arxiv.org/pdf/2411.12338)]
> **Authors**: Xiaoteng Zhou,Yusheng Wang,Katsunori Mizuno
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: 8 pages,conference
- **标题**: 使用单个声学摄像机进行目标高度估算，以在2D海底摩擦下进行补偿
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 这封信提出了一种新的方法，用于补偿二维海底摩西摩西的目标高度数据，以使水下知觉低可见性。声学摄像机是通过高分辨率成像功能以及对黑暗和浊度的鲁棒性，是传感器的有效传感器。但是，在成像过程中，高程角的丧失导致原始声学摄像头图像中缺乏目标高度信息，从而导致海底镶嵌的2D表示。在感知杂乱无章的海洋环境时，目标高度数据对于避免与海洋机器人发生碰撞至关重要。这项研究提出了一种新的方法，可以使用单个声学摄像头估算海床目标高度，并将高度数据整合到2D海床上，以补偿海床目标缺失的3D维度。与建模高架角度损失以实现海底3D重建的经典方法不同，本研究的重点是利用可用的声学铸造阴影线索和简单的传感器运动来快速估计目标高度。通过水箱实验和模拟实验来验证我们的提案的可行性。

### A Benchmark Dataset for Collaborative SLAM in Service Environments 
[[arxiv](https://arxiv.org/abs/2411.14775)] [[cool](https://papers.cool/arxiv/2411.14775)] [[pdf](https://arxiv.org/pdf/2411.14775)]
> **Authors**: Harin Park,Inha Lee,Minje Kim,Hyungyu Park,Kyungdon Joo
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 8 pages, 6 figures, Accepted to IEEE RA-L
- **标题**: 服务环境中协作大满贯的基准数据集
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 随着服务环境变得多样化，他们已经开始要求复杂的任务，这些任务很难完成。这种更改导致对多个机器人的兴趣，而不是一个机器人。 C-Slam作为多个服务机器人的基本技术，需要处理诸如均匀场景和动态对象之类的各种挑战，以确保机器人能够平稳运行并安全执行其任务。但是，现有的C-SLAM数据集并不包括具有上述挑战的各种室内服务环境。为了缩小此差距，我们在各种室内服务环境中为多个服务机器人提供了一个新的多模式C-SLAM数据集，在服务环境（CSE）中称为C-SLAM数据集。我们使用NVIDIA ISAAC SIM在各种室内服务环境中生成数据，并在现实服务环境中可能会出现挑战。通过使用仿真，我们可以提供准确且精确的时间同步传感器数据，例如立体声RGB，立体声深度，IMU和地面真实（GT）构成。我们配置了三个通用的室内服务环境（医院，办公室和仓库），每个环境都包含各种动态对象，这些动力对象适用于每个环境。此外，我们驾驶三个机器人来模仿真实服务机器人的动作。通过这些因素，我们为多个服务机器人生成了更现实的C-SLAM数据集。我们通过评估各种最先进的单机器人大满贯和多机器人大满贯方法来演示我们的数据集。我们的数据集可在https://github.com/vision3d-lab/cse_dataset上找到。

### Inference-Time Policy Steering through Human Interactions 
[[arxiv](https://arxiv.org/abs/2411.16627)] [[cool](https://papers.cool/arxiv/2411.16627)] [[pdf](https://arxiv.org/pdf/2411.16627)]
> **Authors**: Yanwei Wang,Lirui Wang,Yilun Du,Balakumar Sundaralingam,Xuning Yang,Yu-Wei Chao,Claudia Perez-D'Arpino,Dieter Fox,Julie Shah
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: No comments
- **标题**: 推理时间政策通过人类互动来转向
- **领域**: 机器人技术,人工智能,人机交互,机器学习
- **摘要**: 接受人类示威训练的生成政策可以自主完成多模式的长途任务。但是，在推断期间，通常将人类从政策执行循环中删除，从而限制了在多个预测中指导预训练的策略朝着特定的次目标或轨迹形状的能力。天真的人干预可能会无意间加剧分布的转移，从而导致违规或处决失败。为了更好地使政策输出与人类的意图保持一致，而无需诱发分布错误，我们提出了一个推理时间策略转向（ITP）框架，该框架利用人类的交互来偏向生成的采样过程，而不是对交互数据进行微调。我们评估了三个模拟和现实基准测试的ITP，测试了三种形式的人类相互作用和相关的对齐距离指标。在六种抽样策略中，我们提出的随机抽样具有扩散策略，可以实现对齐和分配变化之间的最佳权衡。视频可在https://yanweiw.github.io/itps/上找到。

### A comparison of extended object tracking with multi-modal sensors in indoor environment 
[[arxiv](https://arxiv.org/abs/2411.18476)] [[cool](https://papers.cool/arxiv/2411.18476)] [[pdf](https://arxiv.org/pdf/2411.18476)]
> **Authors**: Jiangtao Shuai,Martin Baerveldt,Manh Nguyen-Duc,Anh Le-Tuan,Manfred Hauswirth,Danh Le-Phuoc
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: No comments
- **标题**: 扩展对象跟踪与室内环境中多模式传感器的比较
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 本文提出了对有效的对象跟踪方法的初步研究，比较了两个不同的3D点云感觉源的性能：LIDAR和立体声摄像机，它们的价格差异很大。在这项初步工作中，我们专注于单个对象跟踪。我们首先开发了一个快速的启发式对象检测器，该对象检测器利用了有关环境和目标的先前信息。随后将所得目标点馈入扩展对象跟踪框架，其中目标形状使用Star-Convex Hypersurface模型进行参数化。实验结果表明，我们使用立体声摄像机的对象跟踪方法的性能类似于激光雷达传感器的性能，其成本差超过十倍。

## 声音(cs.SD:Sound)

该领域共有 10 篇论文

### Improving snore detection under limited dataset through harmonic/percussive source separation and convolutional neural networks 
[[arxiv](https://arxiv.org/abs/2410.23796)] [[cool](https://papers.cool/arxiv/2410.23796)] [[pdf](https://arxiv.org/pdf/2410.23796)]
> **Authors**: F. D. Gonzalez-Martinez,J. J. Carabias-Orti,F. J. Canadas-Quesada,N. Ruiz-Reyes,D. Martinez-Munoz,S. Garcia-Galan
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: ef:AppliedAcoustics, vol. 216, 15 January 2024, 109811
- **标题**: 通过谐波/打击源分离和卷积神经网络改善有限数据集中的SNORE检测
- **领域**: 声音,人工智能,新兴技术,音频和语音处理,信号处理
- **摘要**: 打nor是一种在阻塞性睡眠呼吸暂停综合征（OSA）中通常观察到的声学生物标志物，具有诊断和监测这种公认的临床疾病的巨大潜力。不论打呼式类型如何，大多数打呼式实例表现出可识别的谐波模式，随着时间的流逝而表现出独特的能量分布。在这项工作中，我们提出了一种新颖的方法，可以通过使用谐波/打击声源分离（HPSS）分析输入声音的谐波内容来区分单声呼吸与非磁性声音。最终的功能基于HPS的谐波频谱图作为传统神经网络体系结构的输入数据，旨在即使在有限的数据学习框架下，也旨在增强打nore检测性能。为了评估提案的绩效，我们研究了两种不同的情况：1）使用大型打s和干扰声音数据集，以及2）使用由大约1％的数据材料组成的减少训练集。在前一种情况下，与文献的其他输入功能相比，提出的基于HPSS的功能提供了竞争性结果。但是，所提出的方法的主要优势在于在有限的数据学习环境中源自HPS的谐波谱图的出色性能。在这种特殊情况下，与现有文献中记录的经典输入功能相比，使用所提出的谐波功能可显着提高所有研究体系结构的性能。这一发现清楚地表明，结合谐波内容可以更可靠地学习基本的时频特征，这些特征在大多数打呼式声音中都普遍存在，即使在训练数据量有限的情况下也是如此。

### Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM 
[[arxiv](https://arxiv.org/abs/2411.00774)] [[cool](https://papers.cool/arxiv/2411.00774)] [[pdf](https://arxiv.org/pdf/2411.00774)]
> **Authors**: Xiong Wang,Yangze Li,Chaoyou Fu,Yunhang Shen,Lei Xie,Ke Li,Xing Sun,Long Ma
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: Project Page: https://freeze-omni.github.io/
- **标题**: Freeze-Omni：使用Frozen LLM的智能和低延迟语音对话模型
- **领域**: 声音,人工智能,计算语言学,音频和语音处理
- **摘要**: 快速开发的大型语言模型（LLM）带来了巨大的智能应用程序。特别是，GPT-4O出色的双工语音互动能力为用户带来了令人印象深刻的经验。研究人员最近在这个方向上提出了几个多模式LLM，可以实现用户代理语音到语音对话。本文提出了一种新型的语音文本多模式LLM架构，称为Freeze-Omni。我们的主要贡献是，语音输入和输出方式可以轻松连接到文本LLM，同时在整个培训过程中冻结LLM的参数。我们设计了一个三阶段的培训策略，用于对语音输入和输出进行建模，从而使Freeze-OMNI使用文本语音配对数据（例如ASR和TTS数据）获得语音到语音对话能力，并且只有60,000个多轮文本Q＆A在8 GPUS上。此外，我们可以有效地确保语音模式中的冻结智能与其骨干LLM的文本模式相比，在语音方式中的智能处于相同的水平，同时达到了低潜伏期端到端的口语响应。此外，我们还设计了一种通过多任务训练来实现双工对话能力的方法，从而为用户和代理商之间的冻结 -  OMNI具有更自然的对话能力。总而言之，Freeze-Omni在冻结LLM的条件下基于多模式LLM进行语音到语音对话的潜力很大，从而避免了由于有限的数据和培训资源引起的灾难性遗忘问题。

### Multi Modal Information Fusion of Acoustic and Linguistic Data for Decoding Dairy Cow Vocalizations in Animal Welfare Assessment 
[[arxiv](https://arxiv.org/abs/2411.00477)] [[cool](https://papers.cool/arxiv/2411.00477)] [[pdf](https://arxiv.org/pdf/2411.00477)]
> **Authors**: Bubacarr Jobarteh,Madalina Mincu,Gavojdian Dinu,Suresh Neethirajan
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: 31 pages, 22 figures, 2 tables
- **标题**: 在动物福利评估中解码奶牛发声的声学和语言数据的多模态信息融合
- **领域**: 声音,人工智能,音频和语音处理,定量方法
- **摘要**: 通过多源数据融合来了解动物的发声对于评估情绪状态和增强精确牲畜养殖中的动物福利至关重要。这项研究旨在通过采用多模式数据融合技术，集成转录，语义分析，上下文和情感评估以及声学特征提取来解码乳制牛接触调用。我们利用自然语言处理模型将牛发声的录音转录为书面形式。通过将多个声学特征频率，持续时间和强度与转录文本数据融合在一起，我们开发了牛发声的全面表示。利用数据融合在自定义的本体论中，我们将发声分为与遇险或唤醒相关的高频调用，以及与满足或平静相关的低频呼叫。分析融合的多维数据，我们确定了指示情绪困扰的焦虑相关特征，包括特定的频率测量和声音频谱结果。评估来自20个单独的母牛的发声的情感和声学特征，使我们能够确定呼叫模式和情绪状态的差异。我们采用先进的机器学习算法，随机森林，支持向量机和经常性的神经网络，有效地处理和融合了多源数据，以对牛发声进行分类。这些模型经过优化，以应对实际农场环境中固有的计算需求和数据质量挑战。我们的发现证明了多源数据融合和智能处理技术在动物福利监测中的有效性。这项研究代表了动物福利评估的重大进步，强调了创新融合技术在理解和改善奶牛情绪健康中的作用。

### PIAST: A Multimodal Piano Dataset with Audio, Symbolic and Text 
[[arxiv](https://arxiv.org/abs/2411.02551)] [[cool](https://papers.cool/arxiv/2411.02551)] [[pdf](https://arxiv.org/pdf/2411.02551)]
> **Authors**: Hayeon Bang,Eunjin Choi,Megan Finch,Seungheon Doh,Seolhee Lee,Gyeong-Hoon Lee,Juhan Nam
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: Accepted for publication at the 3rd Workshop on NLP for Music and Audio (NLP4MusA 2024)
- **标题**: Piast：带有音频，符号和文本的多模式钢琴数据集
- **领域**: 声音,人工智能,多媒体,音频和语音处理
- **摘要**: 虽然钢琴音乐已成为音乐信息检索的重要研究领域（MIR），但缺少带有文本标签的钢琴独奏的数据集。为了解决此差距，我们介绍了Piast（带有音频，符号和文本的钢琴数据集），钢琴音乐数据集。利用语义标签的钢琴特异性分类法，我们从YouTube收集了9,673条曲目，并为音乐专家添加了2,023首曲目的人类注释，从而有两个子集：Piast-Yt和Piast-at。两者都包括音频，文本，标记注释以及使用最先进的钢琴转录和击败跟踪模型的转录MIDI。在使用多模式数据集的许多可能的任务中，我们使用音频和MIDI数据进行音乐标签和检索，并报告基线性能，以证明其潜力是MIR研究的宝贵资源。

### Advancing Robust Underwater Acoustic Target Recognition through Multi-task Learning and Multi-Gate Mixture-of-Experts 
[[arxiv](https://arxiv.org/abs/2411.02787)] [[cool](https://papers.cool/arxiv/2411.02787)] [[pdf](https://arxiv.org/pdf/2411.02787)]
> **Authors**: Yuan Xie,Jiawei Ren,Junfeng Li,Ji Xu
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 通过多任务学习和多门的混合物来推进强大的水下声学目标识别
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 水下声学目标识别已成为水下声学领域内的一个重要研究领域。但是，当前真实的水下声学信号记录仍然有限，这阻碍了数据驱动的声学识别模型，这些模型从有限的一组复杂的水下信号中学习了可靠的目标模式，从而损害了它们在实际应用中的稳定性。为了克服这些局限性，本研究提出了一个称为M3（多任务，多门，多expert）的识别框架，以通过使其意识到目标的固有属性来增强模型捕获强大模式的能力。在此框架中，设计了一个专注于目标属性（例如估计目标大小）的辅助任务。然后，辅助任务与识别任务共享参数，以实现多任务学习。该范式使模型可以专注于跨任务的共享信息，并以正规化的方式识别目标的强大模式，从而增强模型的概括能力。此外，M3结合了多型专家和多门机制，从而使不同的参数空间分配给各种水下信号。这使模型能够以细粒度和分化的方式处理复杂的信号模式。为了评估M3的有效性，在水下船舶辐射噪声数据集上实施了广泛的实验。结果证明了M3能够胜过最先进的单任务识别模型，从而实现最先进的性能。

### DEMONet: Underwater Acoustic Target Recognition based on Multi-Expert Network and Cross-Temporal Variational Autoencoder 
[[arxiv](https://arxiv.org/abs/2411.02758)] [[cool](https://papers.cool/arxiv/2411.02758)] [[pdf](https://arxiv.org/pdf/2411.02758)]
> **Authors**: Yuan Xie,Xiaowei Zhang,Jiawei Ren,Ji Xu
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: Demonet：基于多专家网络和跨时空变异自动编码器的水下声学目标识别
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 由于水下环境和目标的动态运动状态，在实际情况下建立强大的水下声学识别系统是具有挑战性的。一种有希望的优化方法是利用目标的内在物理特征（无论环境条件如何）提供强大的见解。但是，我们的研究表明，尽管物理特征表现出强大的特性，但它们可能缺乏特定于类别的歧视性模式。因此，将物理特征直接纳入模型训练中可能会引入意外的感应偏见，从而导致性能降解。为了利用物理特征的益处，同时减轻可能的有害影响，我们在这项研究中提出了Demonet，该研究利用了噪声（Demon）的包膜调节（DEMON）的检测来提供对目标轴频率或目标刀片计数的强大见解。 Demonet是一个多专家网络，它基于Demon Spectra将各种水下信号分配给其最匹配的专家层，以进行细粒度的信号处理。在那里，恶魔光谱完全负责提供隐性的物理特征，而无需与目标类别建立映射关系。此外，为了减轻恶魔特征中的噪声和虚假调制光谱，我们引入了跨时空的对准策略，并采用各种自动编码器（VAE）来重建耐噪声的恶魔光谱以替换原始的恶魔特征。在Deepship数据集和我们的专有数据集中，主要评估了跨时期VAE的拟议Demonet的有效性。实验结果表明，我们的方法可以在两个数据集上实现最先进的性能。

### Adversarial multi-task underwater acoustic target recognition: towards robustness against various influential factors 
[[arxiv](https://arxiv.org/abs/2411.02848)] [[cool](https://papers.cool/arxiv/2411.02848)] [[pdf](https://arxiv.org/pdf/2411.02848)]
> **Authors**: Yuan Xie,Ji Xu,Jiawei Ren,Junfeng Li
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: No comments
- **标题**: 对抗性多任务水下声学目标识别：朝着各种影响因素的鲁棒性
- **领域**: 声音,机器学习
- **摘要**: 基于被动声纳的水下声学目标识别在实际海事应用中面临许多挑战。主要挑战之一在于信号特征对各种环境条件和数据采集配置的敏感性，这可能导致识别系统中的不稳定性。尽管已经致力于解决水下声学其他领域中这些影响因素的重大努力，但在水下声学目标识别领域通常会忽略它们。为了克服这一局限性，本研究设计了基于可用注释的影响因素（例如，源范围，水柱深度或风速）的辅助任务，并采用多任务框架将这些因素连接到识别任务。此外，我们将对抗性学习机制集成到多任务框架中，以促使模型提取对影响因素的强大表示形式。通过对Shipsear数据集的广泛实验和分析，我们提出的对抗性多任务模型展示了其有效地模拟影响因素并在12级识别任务上实现最新性能的能力。

### Acoustic-based 3D Human Pose Estimation Robust to Human Position 
[[arxiv](https://arxiv.org/abs/2411.07165)] [[cool](https://papers.cool/arxiv/2411.07165)] [[pdf](https://arxiv.org/pdf/2411.07165)]
> **Authors**: Yusuke Oumi,Yuto Shibata,Go Irie,Akisato Kimura,Yoshimitsu Aoki,Mariko Isogawa
> **First submission**: 2024-11-08
> **First announcement**: 2024-11-11
> **comment**: Accepted at BMVC2024
- **标题**: 基于声学的3D人类姿势估计可靠到人类位置
- **领域**: 声音,人工智能,计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 本文探讨了仅来自低级声学信号的3D人姿势估计的问题。现有的基于3D人体姿势估计的基于主动声传感的方法隐含地假设目标用户位于扬声器和麦克风之间的一条线上。由于人体的反射和声音的反射和衍射会导致与声音阻塞相比的微妙声学信号变化，因此当受试者偏离这一线时，现有模型会显着降低其准确性，从而限制了其在现实世界中的实用性。为了克服这一限制，我们提出了一种由位置鉴别和抗辩模型组成的新方法。前者可以预测受试者的立场，并应用对抗性学习以提取受试者位置不变特征。后者在估计目标时间之前利用声学信号作为提高鲁棒性，以增强由于衍射和反射而导致声音到达时间变化的鲁棒性。我们构建了一个声学姿势估计数据集，该数据集涵盖了各种各样的人类位置，并通过实验证明我们所提出的方法的表现优于现有方法。

### Generative AI for Music and Audio 
[[arxiv](https://arxiv.org/abs/2411.14627)] [[cool](https://papers.cool/arxiv/2411.14627)] [[pdf](https://arxiv.org/pdf/2411.14627)]
> **Authors**: Hao-Wen Dong
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: PhD Dissertation
- **标题**: 音乐和音频的生成AI
- **领域**: 声音,人工智能,机器学习,多媒体,音频和语音处理
- **摘要**: 生成的AI一直在改变我们与技术互动和消费内容的互动方式。在接下来的十年中，AI技术将重塑我们如何在各种媒体中创建音频内容，包括音乐，戏剧，电影，游戏，播客和短视频。在本文中，我介绍了我的研究的三个主要方向，该方向围绕着音乐和音频的生成AI：1）多音阶音乐生成，2）辅助音乐创作工具，以及3）多模式学习，用于音频和音乐。通过我的研究，我的目标是回答以下两个基本问题：1）AI如何帮助专业人士或业余爱好者创建音乐和音频内容？ 2）AI可以学会以类似于人类学习音乐的方式创作音乐吗？我的长期目标是降低音乐创作的入境障碍并使创建音频内容民主化

### HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset 
[[arxiv](https://arxiv.org/abs/2411.14207)] [[cool](https://papers.cool/arxiv/2411.14207)] [[pdf](https://arxiv.org/pdf/2411.14207)]
> **Authors**: Shivam Saini,Jürgen Peissig
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: Accepted at ICASSP 2025 Workshop. Code to generate uploaded at: https://github.com/whojavumusic/HARP
- **标题**: 竖琴：一个大规模的高阶Ambisonic Room Impulse响应数据集
- **领域**: 声音,人工智能,多媒体,音频和语音处理
- **摘要**: 此贡献介绍了使用图像源方法创建的7阶Ambisonic Room脉冲响应（HOA-RIRS）的数据集。通过使用高阶Ambisonics，我们的数据集可以实现精确的空间音频复制，这是对现实的沉浸式音频应用的关键要求。利用虚拟仿真，我们基于叠加原理提出了独特的麦克风配置，旨在优化声场覆盖范围，同时解决传统麦克风阵列的局限性。提出的64个微载体配置使我们能够直接在球形谐波域中捕获RIR。该数据集具有广泛的房间配置，包括房间几何形状，声学吸收材料和源接收器距离的变化。提供了模拟设置的详细说明，以进行准确的复制。该数据集是研究空间音频的研究人员的重要资源，尤其是在涉及机器学习以改善房间声学建模和声场综合的应用中。它进一步提供了很高的空间分辨率和现实主义，对于诸如源定位，混响预测和沉浸式声音复制等任务至关重要。

## 软件工程(cs.SE:Software Engineering)

该领域共有 2 篇论文

### Interaction2Code: Benchmarking MLLM-based Interactive Webpage Code Generation from Interactive Prototyping 
[[arxiv](https://arxiv.org/abs/2411.03292)] [[cool](https://papers.cool/arxiv/2411.03292)] [[pdf](https://arxiv.org/pdf/2411.03292)]
> **Authors**: Jingyu Xiao,Yuxuan Wan,Yintong Huo,Zixin Wang,Xinyi Xu,Wenxuan Wang,Zhiyao Xu,Yuhang Wang,Michael R. Lyu
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: 21 pages,14 figures
- **标题**: 互动2代码：通过交互式原型制作基于MLLM的交互式网页代码生成基准测试
- **领域**: 软件工程,人工智能,人机交互
- **摘要**: 多模式大语言模型（MLLM）在设计对代码任务（即从UI模型生成UI代码）上表现出了显着的性能。但是，现有基准仅包含用于评估的静态网页，并忽略了动态交互，从而限制了生成的网页的实用性，可用性和用户参与度。为了弥合这些差距，我们介绍了MLLM在生成交互式网页时的首次系统研究。具体而言，我们制定了交互对代码任务并建立交互2代码基准，其中包含127个独特的网页和374个不同的相互作用，从15个网页类型和31个交互类别中。 Through comprehensive experiments utilizing state-of-the-art (SOTA) MLLMs, evaluated via both automatic metrics and human assessments, we identify four critical limitations of MLLM on Interaction-to-Code task: (1) inadequate generation of interaction compared with full page, (2) prone to ten types of failure, (3) poor performance on visually subtle interactions, and (4) insufficient undestanding on interaction when limited to single-modality visual描述。为了解决这些限制，我们提出了四种增强策略：交互式元素突出显示，故障软件提示（FAP），视觉显着性增强和视觉文本描述组合，所有这些组合都旨在提高MLLMS在交互作用量体上的任务上的性能。相互作用2代码基准和代码可在https：// github中找到。 com/webpai/互动2代码。

### The importance of visual modelling languages in generative software engineering 
[[arxiv](https://arxiv.org/abs/2411.17976)] [[cool](https://papers.cool/arxiv/2411.17976)] [[pdf](https://arxiv.org/pdf/2411.17976)]
> **Authors**: Roberto Rossi
> **First submission**: 2024-11-26
> **First announcement**: 2024-11-27
> **comment**: 9 pages, working paper
- **标题**: 视觉建模语言在生成软件工程中的重要性
- **领域**: 软件工程,人工智能
- **摘要**: 多模式GPT代表软件工程与生成人工智能之间的相互作用中的流域。 GPT-4接受图像和文本输入，而不是仅仅是自然语言。我们研究了来自GPT-4的这些增强功能的相关用例。据我们所知，没有其他工作研究类似的用例，涉及通过多模式GPT执行的软件工程任务，并结合了图表和自然语言。

## 音频和语音处理(eess.AS:Audio and Speech Processing)

该领域共有 3 篇论文

### Blind Estimation of Sub-band Acoustic Parameters from Ambisonics Recordings using Spectro-Spatial Covariance Features 
[[arxiv](https://arxiv.org/abs/2411.03172)] [[cool](https://papers.cool/arxiv/2411.03172)] [[pdf](https://arxiv.org/pdf/2411.03172)]
> **Authors**: Hanyu Meng,Jeroen Breebaart,Jeremy Stoddard,Vidhyasaharan Sethu,Eliathamby Ambikairajah
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-06
> **comment**: Accepted by ICASSP2025
- **标题**: 使用光谱空间协方差特征对副频段的声学参数的盲目估计
- **领域**: 音频和语音处理,机器学习,声音,信号处理
- **摘要**: 估计频率相变的声学参数对于增强现实的空间音频创建中的沉浸感至关重要。在本文中，我们提出了一个统一的框架，该框架使用一阶Ambisonics（FOA）语音记录作为输入来盲目估算10个频段的混响时间（T60），直接到返带比（DRR）和清晰度（C50）。该提出的框架利用了一种名为Spectro-Spatial协方差矢量（SSCV）的新型特征，有效地表示FOA信号的时间，光谱以及空间信息。我们的模型大大优于仅使用光谱信息的现有单渠道方法，这三个声学参数将估计误差降低了一半以上。此外，我们引入了FOA-CONV3D，这是一个新型的后端网络，用于使用3D卷积编码器有效地利用SSCV功能。 FOA-CONV3D优于卷积神经网络（CNN）和经常性卷积神经网络（CRNN）后端，达到较低的估计错误，并考虑了所有3个声学参数的较高差异（POV）。

### Analyzing Multimodal Features of Spontaneous Voice Assistant Commands for Mild Cognitive Impairment Detection 
[[arxiv](https://arxiv.org/abs/2411.04158)] [[cool](https://papers.cool/arxiv/2411.04158)] [[pdf](https://arxiv.org/pdf/2411.04158)]
> **Authors**: Nana Lin,Youxiang Zhu,Xiaohui Liang,John A. Batsis,Caroline Summerour
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-07
> **comment**: No comments
- **标题**: 分析自发语音助手命令的多模式特征，以进行轻度认知障碍检测
- **领域**: 音频和语音处理,计算语言学,机器学习,声音
- **摘要**: 轻度认知障碍（MCI）是一个主要的公共卫生问题，因为它的痴呆症患者的高风险。这项研究调查了在受控环境中以35名老年人的自发语音助手（VA）命令检测MCI的潜力。具体而言，命令生成任务是针对参与者自由生成与认知能力更相关的命令的预定意图的设计。我们开发具有音频，文本，意图和多模式融合功能的MCI分类和回归模型。我们发现，命令生成任务的表现优于命令阅读任务，平均分类精度为82％，通过利用多模式融合功能实现。此外，生成的命令与内存和注意子域更加密切，而不是阅读命令。我们的结果证实了指挥生成任务的有效性，并暗示了使用纵向内部命令进行MCI检测的承诺。

### SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation 
[[arxiv](https://arxiv.org/abs/2411.18138)] [[cool](https://papers.cool/arxiv/2411.18138)] [[pdf](https://arxiv.org/pdf/2411.18138)]
> **Authors**: Wenyi Yu,Siyin Wang,Xiaoyu Yang,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Guangzhi Sun,Lu Lu,Yuxuan Wang,Chao Zhang
> **First submission**: 2024-11-27
> **First announcement**: 2024-11-28
> **comment**: Technical report
- **标题**: Salmonn-Omni：一种无编码的LLM，用于全双工语音理解和发电
- **领域**: 音频和语音处理,人工智能,计算语言学,声音
- **摘要**: 全双工多模式大型语言模型（LLMS）提供了一个统一的框架，用于解决多样化的语音理解和发电任务，从而实现了更自然和无缝的人机对话。与传统的模块化对话AI系统不同，该系统将语音识别，理解和语音到语音生成分为不同的组件不同，多模式LLMS作为单端到端模型运行。这种简化的设计消除了跨组件的错误传播，并充分利用了嵌入在输入语音信号中的丰富的非语言信息。我们介绍了Salmonn-Omni，这是一种无编解码的，完整的语音理解和生成模型，能够同时聆听其在讲话时自己生成的语音和背景声音。为了支持这种能力，我们提出了一个新颖的双工口语对话框架，该框架结合了``思维''机制，该机制促进异步文本和语音产生，依靠嵌入而不是编解码器（量化的语音和音频图表）。实验结果表明，在各种流语音任务中，鲑鱼 - 奥姆尼的多功能性，包括语音识别，语音增强和口头答案。此外，Salmonn-Omni在管理转弯，驳船中和回声取消方案方面表现出色，从而确立了其作为全双工对话性AI系统的强大原型的潜力。据我们所知，Salmonn-Omni是同类产品的第一个无编码模型。完整的技术报告以及模型检查站将很快发布。

## 图像和视频处理(eess.IV:Image and Video Processing)

该领域共有 31 篇论文

### Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis 
[[arxiv](https://arxiv.org/abs/2410.24046)] [[cool](https://papers.cool/arxiv/2410.24046)] [[pdf](https://arxiv.org/pdf/2410.24046)]
> **Authors**: Junliang Du,Yiru Cang,Tong Zhou,Jiacheng Hu,Weijie He
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: No comments
- **标题**: 使用HM-VGG深度学习：多模式图像分析的AI策略
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 这项研究介绍了混合多模式VGG（HM-VGG）模型，这是一种早期诊断青光眼的尖端深度学习方法。 HM-VGG模型利用注意机制来处理视野（VF）数据，从而提取了对识别青光眼早期迹象至关重要的关键特征。尽管对大型注释数据集有共同的依赖，但在数据有限的情况下，HM-VGG模型在方案中表现出色，从而获得了较小的样本量的显着效果。该模型的性能在精确，准确性和F1得分方面的高指标强调，这表明其在青光眼检测中使用现实世界的潜力。本文还讨论了与眼科图像分析相关的挑战，尤其是获得大量注释数据的困难。它突出了超出单模式数据的重要性，例如VF或光学连贯性层析成像（OCT）图像，即可以提供更丰富，更全面的数据集的多模式方法。这种不同数据类型的集成显示可显着提高诊断准确性。 HM-VGG模型为医生提供了有希望的工具，简化了诊断过程并改善了患者的结果。此外，其适用性扩展到远程医疗和移动医疗保健，使诊断服务更容易获得。本文提出的研究是医学图像处理领域迈出的重要一步，对临床眼科具有深远的影响。

### Deep Multi-contrast Cardiac MRI Reconstruction via vSHARP with Auxiliary Refinement Network 
[[arxiv](https://arxiv.org/abs/2411.01291)] [[cool](https://papers.cool/arxiv/2411.01291)] [[pdf](https://arxiv.org/pdf/2411.01291)]
> **Authors**: George Yiasemis,Nikita Moriakov,Jan-Jakob Sonke,Jonas Teuwen
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: 11 pages, 1 figure, 3 tables, CMRxRecon Challenge 2024
- **标题**: 通过辅助精炼网络通过VSHARP进行深度对比度心脏MRI重建
- **领域**: 图像和视频处理,计算机视觉和模式识别,医学物理
- **摘要**: 心脏MRI（CMRI）是一种基石成像方式，可深入了解心脏结构和功能。多对比度CMRI（MCCMRI）获得了不同对比度重量的序列，通过捕获广泛的心脏组织特征来显着增强诊断能力。但是，MCCMRI通常受到冗长的获取时间和对运动伪像的敏感性的限制。为了缓解这些挑战，已经开发出通过不同采样方案在加速因子上使用K空间不足采样的加速成像技术，以缩短扫描持续时间。在这种情况下，我们提出了一种基于深度学习的重建方法，用于2D动态多对比度，多速率和多激系MRI。我们的方法集成了最先进的VSHARP模型，该模型利用半季度的变量分裂和ADMM优化，并用作辅助修复网络（ARN）来更好地适应MCCMRI数据的多样性。具体而言，次采样的k空间数据被馈入ARN，该数据对VSHARP使用的降解步骤产生了初始预测。随后，VSHARP将其与二键采样的K空间一起生成高质量的2D序列预测。我们的方法优于传统的重建技术和其他基于VSHARP的模型。

### Unsupervised Training of a Dynamic Context-Aware Deep Denoising Framework for Low-Dose Fluoroscopic Imaging 
[[arxiv](https://arxiv.org/abs/2411.00830)] [[cool](https://papers.cool/arxiv/2411.00830)] [[pdf](https://arxiv.org/pdf/2411.00830)]
> **Authors**: Sun-Young Jeon,Sen Wang,Adam S. Wang,Garry E. Gold,Jang-Hwan Choi
> **First submission**: 2024-10-29
> **First announcement**: 2024-11-04
> **comment**: 15 pages, 10 figures
- **标题**: 低剂量透视成像的动态背景感知的深层denoising框架的无监督培训
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 荧光镜检查对于医学成像中的实时X射线可视化至关重要。但是，低剂量图像会因噪声而损害，可能会影响诊断准确性。降噪对于维持图像质量至关重要，尤其是考虑到诸如运动伪影等挑战和医学成像中干净数据的有限可用性。为了解决这些问题，我们提出了一个无监督的培训框架，用于动态上下文感知荧光镜检查图像序列。首先，我们在不需要干净的数据来解决初始噪声的情况下训练多尺度复发的U-NET（MSR2AU-NET）。其次，我们结合了基于知识蒸馏的不相关的噪声抑制模块，并通过运动补偿增强了基于递归的相关噪声抑制模块，以进一步改善运动补偿并实现出色的脱氧性能。最后，我们通过将这些模块与像素动态对象运动交叉融合矩阵相结合，介绍了一种新颖的方法，该矩阵旨在适应运动，并为精确的细节保留而进行边缘保留损失。为了验证所提出的方法，我们在医学图像数据集上进行了广泛的数值实验，包括来自动态幻像的3500次透视图像（2400张培训图像，1,100张测试）和350个脊柱手术患者的临床图像。此外，我们使用4,800张图像进行培训和1,136个测试，通过对2016年公开可用的2016年低剂量CT Grand Challenge数据集进行测试，证明了我们的方法在不同的成像方式上的鲁棒性。结果表明，所提出的方法在视觉质量和定量评估中的最先进的算法优于最先进的算法，同时在低剂量荧光镜和CT成像中实现了可比的性能与良好的监督学习方法。

### Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract 
[[arxiv](https://arxiv.org/abs/2411.00726)] [[cool](https://papers.cool/arxiv/2411.00726)] [[pdf](https://arxiv.org/pdf/2411.00726)]
> **Authors**: Fan Xiao,Junlin Hou,Ruiwei Zhao,Rui Feng,Haidong Zou,Lina Lu,Yi Xu,Juzhao Zhang
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: 10 pages, 4 figures
- **标题**: 与白内障的多模式糖尿病性视网膜病变分级的交叉式变压器
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 糖尿病性视网膜病（DR）是全球失明的主要原因，也是糖尿病常见并发症。作为用于DR分级的两种不同的成像工具，Color Fellus Photography（CFP）和红外眼底摄影（IFP）在临床应用中高度相关且互补。据我们所知，这是第一项探索一种新型的多模式深度学习框架的研究，将信息从CFP和IFP融合到更准确的DR分级。具体而言，我们构建了双流式体系结构交叉式变压器（CFT），以融合两个底面图像模式的基于VIT的特征。特别是，引入了精心设计的交叉汇票注意力（CFA）模块，以捕获CFP和IFP图像之间的对应关系。此外，我们同时采用单模式和多模式的监督，以最大程度地提高DR分级的整体性能。由1,713对多模式底面图像组成的临床数据集进行了广泛的实验，证明了我们提出的方法的优越性。我们的代码将发布以供公众访问。

### Tumor Location-weighted MRI-Report Contrastive Learning: A Framework for Improving the Explainability of Pediatric Brain Tumor Diagnosis 
[[arxiv](https://arxiv.org/abs/2411.00609)] [[cool](https://papers.cool/arxiv/2411.00609)] [[pdf](https://arxiv.org/pdf/2411.00609)]
> **Authors**: Sara Ketabi,Matthias W. Wagner,Cynthia Hawkins,Uri Tabori,Birgit Betina Ertl-Wagner,Farzad Khalvati
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 肿瘤位置加权MRI报告对比学习：改善小儿脑肿瘤诊断的解释性的框架
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 尽管通过磁共振成像（MRI）在脑肿瘤诊断中卷积神经网络（CNN）的表现有望，但它们与临床工作流程的整合受到限制。这主要是由于放射科医生不清楚导致模型预测的特征尚不清楚，因此在临床上无关紧要，即缺乏解释性。作为放射学家知识和专业知识的宝贵来源，可以将放射学报告与MRI集成在对比度学习（CL）框架中，从而使图像报告协会学习以提高CNN的可解释性。在这项工作中，我们培训了3D Brain MRI扫描和放射学报告的多模式CL架构，以学习信息丰富的MRI表示。此外，我们将肿瘤的位置集成到了几个脑肿瘤分析任务中，以提高其普遍性。然后，我们应用学习的图像表示来改善小儿低度神经胶质瘤的遗传标记分类的解释性和性能，这是儿童中最普遍的脑肿瘤，作为下游任务。我们的结果表明，模型的注意图和手动肿瘤分割（作为一种解释性度量）的骰子得分为31.1％，测试分类性能为87.7％，显着胜过基准。这些增强功能可以在放射科医生中建立对我们模型的信任，从而促进其整合到临床实践中，以进行更有效的肿瘤诊断。

### MAROON: A Framework for the Joint Characterization of Near-Field High-Resolution Radar and Optical Depth Imaging Techniques 
[[arxiv](https://arxiv.org/abs/2411.00527)] [[cool](https://papers.cool/arxiv/2411.00527)] [[pdf](https://arxiv.org/pdf/2411.00527)]
> **Authors**: Vanessa Wirth,Johanna Bräunig,Martin Vossiek,Tim Weyrich,Marc Stamminger
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: 栗色：近场高分辨率雷达和光学深度成像技术的联合表征的框架
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 利用特定波长范围或深度传感器的互补优势对于强大的计算机辅助任务（例如自动驾驶）至关重要。尽管如此，在光学深度传感器和雷达近距离运行近距离的交点上仍然很少进行研究，该目标远离传感器。随着对在近场中运行的高分辨率成像雷达的兴趣日益兴趣，这些传感器与传统的光学对应物相比的行为如何。在这项工作中，我们采取了使用多模式的空间校准，从光学和射频域中共同表征深度成像仪的独特挑战。我们从四个深度成像器中收集数据，其中三个光学传感器的操作原理和一个成像雷达。我们对它们的深度测量值进行了全面的评估，相对于不同的对象材料，几何形状和对象传感器距离。具体而言，我们揭示了部分传播材料的散射效应，并研究了射频信号的响应。所有对象测量将以多模式数据集的形式公开，称为Maroon。

### Multi-modal deformable image registration using untrained neural networks 
[[arxiv](https://arxiv.org/abs/2411.02672)] [[cool](https://papers.cool/arxiv/2411.02672)] [[pdf](https://arxiv.org/pdf/2411.02672)]
> **Authors**: Quang Luong Nhat Nguyen,Ruiming Cao,Laura Waller
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: No comments
- **标题**: 使用未经训练的神经网络的多模式可变形图像注册
- **领域**: 图像和视频处理,机器学习
- **摘要**: 图像注册技术通常假定要注册的图像具有某种类型（例如，单模式与多模式，2D与3D，刚性与变形），并且缺乏在所有条件下可以适用于数据的一般方法。我们提出了一种使用神经网络进行图像表示的注册方法。我们的方法将未经训练的网络使用有限的表示能力作为隐式，以进行良好的注册指南。与以前专门针对特定数据类型的方法不同，我们的方法处理刚性和非刚性的方法，以及单模式和多模式的注册，而无需更改模型或目标函数。我们已经使用各种数据集进行了全面的评估研究，并证明了有希望的性能。

### Multi-modal Spatial Clustering for Spatial Transcriptomics Utilizing High-resolution Histology Images 
[[arxiv](https://arxiv.org/abs/2411.02534)] [[cool](https://papers.cool/arxiv/2411.02534)] [[pdf](https://arxiv.org/pdf/2411.02534)]
> **Authors**: Bingjun Li,Mostafa Karami,Masum Shah Junayed,Sheida Nabavi
> **First submission**: 2024-10-30
> **First announcement**: 2024-11-05
> **comment**: 9 pages
- **标题**: 用于空间转录组学的多模式空间聚类利用高分辨率组织学图像
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 了解生物组织中复杂的细胞环境对于发现对复杂生物学功能的见解至关重要。尽管单细胞RNA测序显着增强了我们对细胞态的理解，但它缺乏完全理解细胞环境所需的空间环境。空间转录组学（ST）通过在保持空间上下文的同时实现全转录组基因表达分析来解决这一局限性。 ST数据分析中的主要挑战之一是空间聚类，该聚类揭示了基于组织中斑点的空间域。现代的ST测序程序通常包括高分辨率的组织学图像，在先前的研究中已显示与基因表达谱密切相关。但是，当前的空间聚类方法通常无法与基因表达数据完全整合高分辨率的组织学图像特征，从而限制了它们捕获关键的空间和细胞相互作用的能力。在这项研究中，我们提出了空间转录组学多模式聚类（STMMC）模型，这是一种基于对比度学习的新型深度学习方法，该方法通过多模式平行图自动编码器将基因表达数据与组织学图像特征集成在一起。我们针对四种最新基线模型测试了STMMC：Leiden，Graphst，SpagCN和Stlearn在两个公共ST数据集上，总共有13个样本切片。实验表明，STMMC在ARI和NMI方面优于所有基线模型。一项消融研究进一步验证了对比学习和组织学图像特征的融合的贡献。

### MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network 
[[arxiv](https://arxiv.org/abs/2411.01896)] [[cool](https://papers.cool/arxiv/2411.01896)] [[pdf](https://arxiv.org/pdf/2411.01896)]
> **Authors**: Longfeng Shen,Yanqi Hou,Jiacong Chen,Liangjin Diao,Yaxi Duan
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: Brain tumor segmentation, lightweight model, Brain Tumor Segmentation (BraTS) Challenge, group convolution
- **标题**: MBDRES-U-NET：多尺度轻质脑肿瘤分割网络
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 精确分割脑肿瘤在脑肿瘤疾病的诊断和治疗中起关键作用。它是量化肿瘤并提取其特征的关键技术。随着深度学习方法的越来越多的应用，计算负担越来越重。为了实现具有良好分割性能的轻量级模型，本研究建议使用三维（3D）U-NET编解码器框架MBDRES-U-NET模型，该模型将多基金会残留块集成并融合到模型中。分支策略减少了模型的计算负担，该分支策略有效地使用了多模式图像中的丰富局部特征并增强了子肿瘤区域的分割性能。此外，在编码过程中，将自适应加权扩展卷积层引入多支分支残留块中，从而丰富了特征表达式并提高了模型的分割精度。关于脑肿瘤细分（BRAT）挑战2018和2019年数据集的实验表明，该体系结构可以保持很高的脑肿瘤细分精度，同时大大减少了计算开销。

### A Novel Deep Learning Tractography Fiber Clustering Framework for Functionally Consistent White Matter Parcellation Using Multimodal Diffusion MRI and Functional MRI 
[[arxiv](https://arxiv.org/abs/2411.01859)] [[cool](https://papers.cool/arxiv/2411.01859)] [[pdf](https://arxiv.org/pdf/2411.01859)]
> **Authors**: Jin Wang,Bocheng Guo,Yijie Li,Junyi Wang,Yuqian Chen,Jarrett Rushmore,Nikos Makris,Yogesh Rathi,Lauren J O'Donnell,Fan Zhang
> **First submission**: 2024-11-04
> **First announcement**: 2024-11-05
> **comment**: 5 pages, 3 figures
- **标题**: 一种新型的深度学习拖拉学纤维聚类框架，用于使用多模式扩散MRI和功能性MRI，在功能上保持一致的白质。
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 使用扩散MRI（DMRI）是白质（WM）分析的关键策略。当前方法主要使用纤维（即空间轨迹）的几何信息将相似的纤维分组为簇，从而忽略了沿纤维区域中存在的重要功能信号。越来越多的证据表明，可以使用功能性MRI（fMRI）来测量WM中的神经活动，从而为纤维聚类提供潜在的有价值的多模式信息。在本文中，我们开发了一种新型的深度学习纤维聚类框架，即深层多视图纤维聚类（DMVFC），该框架使用联合DMRI和FMRI数据来启用功能一致的WM parcellation。 DMVFC可以有效地将WM纤维的几何特性与沿纤维区域的fMRI BOLD信号整合在一起。它包括两个主要组件：1）单独计算纤维几何信息和功能信号的嵌入功能的多视图预审计模块，以及2）协作微调模块，以同时完善两种嵌入。在实验中，我们将DMVFC与两种最先进的纤维聚类方法进行了比较，并在实现功能有意义且一致的WM分析结果方面表现出了卓越的性能。

### TexLiverNet: Leveraging Medical Knowledge and Spatial-Frequency Perception for Enhanced Liver Tumor Segmentation 
[[arxiv](https://arxiv.org/abs/2411.04595)] [[cool](https://papers.cool/arxiv/2411.04595)] [[pdf](https://arxiv.org/pdf/2411.04595)]
> **Authors**: Xiaoyan Jiang,Zhi Zhou,Hailing Wang,Guozhong Wang,Zhijun Fang
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-08
> **comment**: No comments
- **标题**: Texlivernet：利用医学知识和空间频率感知来增强肝肿瘤分段
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 将文本数据与肝肿瘤分割中的成像整合在一起对于提高诊断准确性至关重要。但是，当前的多模式医疗数据集仅提供一般文本注释，缺少特定于病变的细节对于提取细微的特征至关重要，尤其是用于细分肿瘤边界和小病变的细粒度分割。为了解决这些局限性，我们开发了针对肝肿瘤的病变特异性文本注释的数据集，并引入了Texlivernet模型。 Texlivernet采用了一个基于代理的跨注意模块，该模块将文本功能有效地与视觉特征集成在一起，从而大大降低了计算成本。另外，提出了增强的空间和自适应频域感知，以精确描绘病变边界，减少背景干扰并在小病变中恢复细节。对公共和私人数据集的全面评估表明，与当前的最新方法相比，Texlivernet的性能优越。

### PRISM: Privacy-preserving Inter-Site MRI Harmonization via Disentangled Representation Learning 
[[arxiv](https://arxiv.org/abs/2411.06513)] [[cool](https://papers.cool/arxiv/2411.06513)] [[pdf](https://arxiv.org/pdf/2411.06513)]
> **Authors**: Sarang Galada,Tanurima Halder,Kunal Deo,Ram P Krish,Kshitij Jadhav
> **First submission**: 2024-11-10
> **First announcement**: 2024-11-11
> **comment**: This work has been submitted to ISBI 2025
- **标题**: Prism：通过分离表示的学习
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 多站点的MRI研究通常会遭受特定地点的变化，这是由于方法论，硬件和获取协议的差异而产生的，从而损害了临床AI/ML任务的准确性和可靠性。我们提出PRISM（保护隐私的站点间MRI协调），这是一个新颖的深度学习框架，可在保留数据隐私的同时，在多个站点进行协调结构性大脑MRI。 Prism采用双分支自动编码器，具有对比度学习和变异推断，从而将解剖学特征从样式和特定于网站的变化中解散，从而实现了不配对的图像翻译，而无需旅行主题或多个MRI模式。我们的模块化设计允许对任何目标站点进行协调，并无需再进行重新调整或微调。使用多站点的结构MRI数据，我们证明了Prism在下游任务（例如脑组织分割）中的有效性，并通过多个实验验证其协调性能。我们的框架解决了医疗AI/ML的关键挑战，包括数据隐私，分配变化，模型可推广性和解释性。代码可从https://github.com/saranggalada/prism获得

### Exploring the Feasibility of Affordable Sonar Technology: Object Detection in Underwater Environments Using the Ping 360 
[[arxiv](https://arxiv.org/abs/2411.05863)] [[cool](https://papers.cool/arxiv/2411.05863)] [[pdf](https://arxiv.org/pdf/2411.05863)]
> **Authors**: Md Junayed Hasan,Somasundar Kannan,Ali Rohan,Mohd Asif Shah
> **First submission**: 2024-11-07
> **First announcement**: 2024-11-11
> **comment**: This work is currently under review. This is a pre-print
- **标题**: 探索负担得起的声纳技术的可行性：使用PING 360在水下环境中的对象检测
- **领域**: 图像和视频处理,计算机视觉和模式识别,新兴技术
- **摘要**: 这项研究探讨了PING 360声纳设备（主要用于导航）的潜力，用于检测复杂的水下障碍物。这项研究背后的主要动机是该设备的负担能力和开源性质，为更昂贵的成像声纳系统提供了具有成本效益的替代方案。该调查的重点是了解Ping 360在受控环境中的行为并评估其对物体检测的适用性，尤其是在人类操作员无法检查浅水区近海结构的情况下。通过一系列精心设计的实验，我们研究了浅水水下环境中表面反射和对象阴影的影响。此外，我们开发了一个手动注释的声纳图像数据集来训练U-NET分割模型。我们的发现表明，尽管Ping 360声纳在更简单的设置中表现出潜力，但除非应用大量数据预处理和注释，否则在更混乱或反射的环境中其性能受到限制。据我们所知，这是评估Ping 360的复杂对象检测功能的第一项研究。通过调查低成本声纳设备的可行性，这项研究为其局限性和对未来基于AI的解释的潜力提供了宝贵的见解，标志着对该领域的独特贡献。

### Robust Divergence Learning for Missing-Modality Segmentation 
[[arxiv](https://arxiv.org/abs/2411.08305)] [[cool](https://papers.cool/arxiv/2411.08305)] [[pdf](https://arxiv.org/pdf/2411.08305)]
> **Authors**: Runze Cheng,Zhongao Sun,Ye Zhang,Chun Li
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: No comments
- **标题**: 强大的分歧学习缺失模式细分
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 多模式磁共振成像（MRI）提供了分析脑肿瘤子区域的必要互补信息。尽管使用四种常见的MRI模式进行自动分割的方法已显示出成功，但由于图像质量问题，协议不一致，过敏反应或成本因素，它们经常面临缺失模式的挑战。因此，开发一种处理缺失模式的细分范式在临床上是有价值的。引入了基于HölderDivergence和相互信息的新型单模式并行处理网络框架。每种模态都独立输入到共享网络骨架中，以进行并行处理，从而保留独特的信息。此外，还引入了一个动态共享框架，该框架根据模式可用性调整网络参数。 Hölder差异和基于信息的损失函数用于评估预测和标签之间的差异。对Brats 2018和Brats 2020数据集进行了广泛的测试表明，我们的方法在处理缺失模式方面优于现有技术，并验证每个组件的有效性。

### LapGSR: Laplacian Reconstructive Network for Guided Thermal Super-Resolution 
[[arxiv](https://arxiv.org/abs/2411.07750)] [[cool](https://papers.cool/arxiv/2411.07750)] [[pdf](https://arxiv.org/pdf/2411.07750)]
> **Authors**: Aditya Kasliwal,Ishaan Gakhar,Aryan Kamani,Pratinav Seth,Ujjwal Verma
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-13
> **comment**: No comments
- **标题**: LAPGSR：用于指导热超分辨率的Laplacian重建网络
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 在过去的几年中，多模式数据的融合已被广泛研究，例如机器人技术，手势识别和自动导航。实际上，高质量的视觉传感器很昂贵，消费级传感器会产生低分辨率图像。研究人员开发了将RGB颜色图像与非视觉数据（例如热量）相结合的方法，以克服这种限制以改善分辨率。融合多种模态以产生视觉吸引力的高分辨率图像通常需要具有数百万参数和繁重的计算负载的密集模型，这通常归因于模型的复杂架构。我们提出了LAPGSR，这是一种多模式，轻巧的生成模型，其中融合了用于指导热超分辨率的Laplacian图像金字塔。这种方法在RGB颜色图像上使用Laplacian金字塔来提取重要边缘信息，然后将其用来绕过模型较高层中的重型特征图计算，并以组合的像素和对抗性损失结合。 LAPGSR保留图像的空间和结构细节，同时也有效而紧凑。这导致一个模型的参数明显少于其他SOTA模型，同时在两个跨域数据集中证明了出色的结果。 ULB17-VT和VGTSR数据集。

### MpoxVLM: A Vision-Language Model for Diagnosing Skin Lesions from Mpox Virus Infection 
[[arxiv](https://arxiv.org/abs/2411.10888)] [[cool](https://papers.cool/arxiv/2411.10888)] [[pdf](https://arxiv.org/pdf/2411.10888)]
> **Authors**: Xu Cao,Wenqian Ye,Kenny Moise,Megan Coffee
> **First submission**: 2024-11-16
> **First announcement**: 2024-11-18
> **comment**: Accepted by ML4H 2024
- **标题**: MPOXVLM：一种视觉模型，用于诊断MPOX病毒感染的皮肤病变
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 在199日大流行和加速气候变化之后，新兴的传染病，尤其是由人畜共患病产生的疾病，仍然是全球威胁。 MPOX（由Monkeypox病毒引起）是人畜共患感染的一个显着例子，通常无法诊断，尤其是随着其皮疹在各个阶段进行，使跨不同呈现不同人群的检测变得复杂。 2024年8月，世卫组织董事宣布MPOX第二次爆发国际公共卫生紧急情况。尽管部署了用于从皮肤病变图像中检测疾病的深度学习技术，但由于开源MPOX皮肤病变图像，多模式临床数据以及专门的训练管道的不可用而缺乏强大且公共可访问的MPOX诊断基础模型。为了解决这一差距，我们提出了MPOXVLM，这是一种视觉语言模型（VLM），旨在通过分析皮肤病变图像和患者临床信息来检测MPOX。 MPOXVLM集成了夹子视觉编码器，这是一种增强的视觉变压器（VIT）分类器，用于皮肤病变，Llama-2-7B模型，从我们新发布的MPOX SKION病变数据集中进行了预训练和微调。我们的工作达到了90.38％的MPOX检测精度，为提高对抗MPOX的早期诊断准确性提供了有希望的途径。

### HIST-AID: Leveraging Historical Patient Reports for Enhanced Multi-Modal Automatic Diagnosis 
[[arxiv](https://arxiv.org/abs/2411.10684)] [[cool](https://papers.cool/arxiv/2411.10684)] [[pdf](https://arxiv.org/pdf/2411.10684)]
> **Authors**: Haoxu Huang,Cem M. Deniz,Kyunghyun Cho,Sumit Chopra,Divyam Madaan
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: In Proceedings of Machine Learning for Health
- **标题**: 历史称：利用历史患者报告以增强多模式自动诊断
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 胸部X射线成像是一种可检测胸部异常的广泛访问和非侵入性诊断工具。尽管许多AI模型有助于放射科医生解释这些图像，但大多数忽略了患者的历史数据。为了弥合这一差距，我们引入了时间模拟数据集，该数据集整合了五年的患者病史，包括射线照相扫描和模仿CXR和模仿IV的报告，包括12,221例患者和十三个病理。在此基础上，我们提出了Hist-Aid，该框架可以使用历史报告提高自动诊断准确性。 Hist-Aid模拟了放射科医生的全面方法，利用历史数据来提高诊断准确性。我们的实验表明，与仅依赖放射线照相扫描的模型相比，AUROC增加了6.56％，AUPRC增加了9.51％。这些收益始终在不同的人口组中观察到，包括性别，年龄和种族类别的变化。我们表明，尽管最近的数据提高了性能，但由于患者状况的变化，较旧的数据可能会降低准确性。我们的工作铺平了将历史数据纳入更可靠的自动诊断的潜力，为临床决策提供了重要的支持。

### EyeDiff: text-to-image diffusion model improves rare eye disease diagnosis 
[[arxiv](https://arxiv.org/abs/2411.10004)] [[cool](https://papers.cool/arxiv/2411.10004)] [[pdf](https://arxiv.org/pdf/2411.10004)]
> **Authors**: Ruoyu Chen,Weiyi Zhang,Bowen Liu,Xiaolan Chen,Pusheng Xu,Shunming Liu,Mingguang He,Danli Shi
> **First submission**: 2024-11-15
> **First announcement**: 2024-11-18
> **comment**: 28 pages, 2 figures
- **标题**: Eyediff：文本到图像扩散模型改善了罕见的眼病诊断
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 威胁性视网膜疾病的不断增长对全球医疗保健系统造成了重大负担。深度学习（DL）为自动疾病筛查提供了有希望的解决方案，但需要大量数据。在各种方式上收集和标记大量的眼镜图像遇到了几种现实世界中的挑战，尤其是对于稀有疾病。在这里，我们介绍了Eyediff，这是一种文本对图像模型，旨在从自然语言提示中生成多模式眼科图像，并评估其在诊断常见和罕见疾病中的适用性。使用高级潜在扩散模型对八个大型数据集进行了培训，涵盖了14个眼科图像方式和80多种眼部疾病，并适用于十个多国外部数据集。生成的图像准确地捕获了基本的病变特征，并通过客观指标和人类专家评估的文本提示达到了高度对齐。此外，整合产生的图像可显着提高检测少数群体和罕见的眼部疾病的准确性，超过传统的过采样方法来解决数据不平衡。 Eyediff有效地解决了罕见疾病中通常遇到的数据失衡和不足的问题，并解决了收集大规模注释的图像的挑战，提供了一种变革性的解决方案，以增强眼科领域的专家级疾病诊断模型的发展。

### Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical Image Fusion 
[[arxiv](https://arxiv.org/abs/2411.11799)] [[cool](https://papers.cool/arxiv/2411.11799)] [[pdf](https://arxiv.org/pdf/2411.11799)]
> **Authors**: Meng Zhou,Yuxuan Zhang,Xiaolan Xu,Jiayi Wang,Farzad Khalvati
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: An extended version of the paper accepted at IEEE BIBM 2024
- **标题**: 用于多模式医学图像融合的边缘增强的扩张剩余注意网络
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 多模式医学图像融合是一项至关重要的任务，将不同成像方式的互补信息结合到统一表示，从而增强了诊断准确性和治疗计划。尽管深度学习方法，尤其是卷积神经网络（CNN）和变压器具有明显的高级融合性能，但现有的一些基于CNN的方法在捕获细粒的多尺度和边缘特征方面缺乏，从而导致了次优特征集成。另一方面，基于变压器的模型在训练和融合阶段都在计算范围内进行了密集型，这使得它们对于实时临床使用不切实际。此外，融合图像的临床应用仍未开发。在本文中，我们提出了一种基于CNN的新型体系结构，该结构通过引入扩大的残留注意网络模块来解决这些局限性，以进行有效的多尺度特征提取，并与梯度操作员相结合，以增强边缘细节的学习。为了确保快速有效的融合，我们基于SoftMax的加权核标准提出了无参数的融合策略，该策略在训练或推理过程中不需要其他计算。包括下游脑肿瘤分类任务在内的广泛实验表明，我们的方法在视觉质量，纹理保存和融合速度方面优于各种基线方法，这使其成为现实世界中临床应用的实用解决方案。该代码将在https://github.com/simonzhou86/en_dran上发布。

### LMM-driven Semantic Image-Text Coding for Ultra Low-bitrate Learned Image Compression 
[[arxiv](https://arxiv.org/abs/2411.13033)] [[cool](https://papers.cool/arxiv/2411.13033)] [[pdf](https://arxiv.org/pdf/2411.13033)]
> **Authors**: Shimon Murai,Heming Sun,Jiro Katto
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: IEEE VCIP 2024 poster
- **标题**: LMM驱动的语义图像文本编码超低 - 二尖学学到的图像压缩
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 在强大的生成模型的支持下，使用感知指标的低二晶型学术图像压缩（LIC）模型变得可行。一些最先进的模型通过将图像标题作为亚信息实现高压率和出色的感知质量。本文表明，使用大型多模式模型（LMM），可以生成字幕并在单个模型中压缩它们。我们还提出了一种适用于任何LIC网络的新型语义感知的微调方法，与现有方法相比，LPIPS BD率有41.58％的改善。我们的实施和预训练的权重可在https://github.com/tokkiwa/imagetextcoding上找到。

### SAM-I2I: Unleash the Power of Segment Anything Model for Medical Image Translation 
[[arxiv](https://arxiv.org/abs/2411.12755)] [[cool](https://papers.cool/arxiv/2411.12755)] [[pdf](https://arxiv.org/pdf/2411.12755)]
> **Authors**: Jiayu Huo,Sebastien Ourselin,Rachel Sparks
> **First submission**: 2024-11-12
> **First announcement**: 2024-11-20
> **comment**: No comments
- **标题**: SAM-i2i：释放段的力量，任何模型的医学图像翻译模型
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 医疗图像翻译对于减少临床领域中冗余且昂贵的多模式成像的需求至关重要。但是，基于卷积神经网络（CNN）和变压器的当前方法通常无法捕获细粒语义特征，从而导致次优质量。为了应对这一挑战，我们提出了SAM-I2I，这是一个基于段的任何模型2（SAM2）的新型图像到图像翻译框架。 SAM-I2I利用预先训练的图像编码器从源图像中提取多尺度语义特征，基于掩码单元注意模块的解码器来综合目标模态图像。我们对多对比度MRI数据集的实验表明，SAM-I2I的表现优于最先进的方法，提供了更有效，更准确的医学图像翻译。

### Large-scale cross-modality pretrained model enhances cardiovascular state estimation and cardiomyopathy detection from electrocardiograms: An AI system development and multi-center validation study 
[[arxiv](https://arxiv.org/abs/2411.13602)] [[cool](https://papers.cool/arxiv/2411.13602)] [[pdf](https://arxiv.org/pdf/2411.13602)]
> **Authors**: Zhengyao Ding,Yujian Hu,Youyao Xu,Chengchen Zhao,Ziyu Li,Yiheng Mao,Haitao Li,Qian Li,Jing Wang,Yue Chen,Mengjia Chen,Longbo Wang,Xuesen Chu,Weichao Pan,Ziyi Liu,Fei Wu,Hongkun Zhang,Ting Chen,Zhengxing Huang
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-21
> **comment**: 23 pages, 8 figures
- **标题**: 大规模的跨模式预读的模型增强了心电图中的心血管状态估计和心肌病检测：AI系统开发和多中心验证研究
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 心血管疾病（CVD）对早期和准确的诊断提出了重大挑战。尽管心脏磁共振成像（CMR）是评估心脏功能和诊断CVD的金标准，但其高成本和技术复杂性限制了可访问性。相反，心电图（ECG）为大规模早期筛查提供了希望。这项研究介绍了心脏网络，这是一种创新的模型，通过通过跨模式对比度学习和生成预处理利用CMR的诊断优势来增强ECG分析。心脏网络具有两个主要功能：（1）使用ECG输入，它评估了潜在的CVD的详细心脏功能指标和潜在CVD的筛查，包括冠状动脉疾病，心肌病，心肌病，心脏病，心力衰竭和肺动脉高压； （2）它通过从ECG数据中生成高质量的CMR图像来增强可解释性。我们在两个大规模公共数据集（英国生物银行（拥有41,519个个人和包括501,172个样本的模拟IV-ECG）以及三个私人数据集以及三个私人数据集（Fahzu）（410个个人，与464个个人和QPH的人）和DISDNACT的训练，并验证提议的心脏网络（英国生物银行和包括501,172个样本的Mimic-IV-ECG）以及三个私人数据集（Fahzu），以及与338个个人的Sahzu一起演示。仅ECG模型，大大提高了筛选精度。此外，生成的CMR图像为所有经验水平的医生提供了宝贵的诊断支持。这项概念验证的研究强调了心电图如何促进对心脏功能评估的跨模式见解，为在人群水平上增强CVD筛查和诊断铺平了道路。

### Cross Group Attention and Group-wise Rolling for Multimodal Medical Image Synthesis 
[[arxiv](https://arxiv.org/abs/2411.14684)] [[cool](https://papers.cool/arxiv/2411.14684)] [[pdf](https://arxiv.org/pdf/2411.14684)]
> **Authors**: Tao Song,Yicheng Wu,Minhao Hu,Xiangde Luo,Linda Wei,Guotai Wang,Yi Guo,Feng Xu,Shaoting Zhang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 多模式医学图像综合的跨小组注意力和小组滚动
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 多模式MR图像合成旨在通过融合和映射一些可用的MRI数据来生成缺失的模态图像。大多数现有方法通常采用图像到图像翻译方案。但是，这些方法通常由于不同模态之间的空间错位而通常被视为输入通道时的空间错位。因此，在本文中，我们提出了一个自适应群体相互作用网络（AGI-NET），该网络探索了多模态MR图像合成的模式间和模式内关系。具体而言，首先是沿通道维度预定的组，然后我们对标准卷积内核进行自适应滚动以捕获模式间空间对应关系。同时，引入了一个跨组注意模块以跨不同通道组的融合信息，从而提供了更好的特征表示。我们评估了模型对公开可用的IXI和BRATS2023数据集的有效性，在该数据集中，AGI-NET在其中实现了多模式MR图像合成的最新性能。代码将发布。

### Multimodal 3D Brain Tumor Segmentation with Adversarial Training and Conditional Random Field 
[[arxiv](https://arxiv.org/abs/2411.14418)] [[cool](https://papers.cool/arxiv/2411.14418)] [[pdf](https://arxiv.org/pdf/2411.14418)]
> **Authors**: Lan Jiang,Yuchao Zheng,Miao Yu,Haiqing Zhang,Fatemah Aladwani,Alessandro Perelli
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: 13 pages, 7 figures, Annual Conference on Medical Image Understanding and Analysis (MIUA) 2024
- **标题**: 通过对抗训练和条件随机场进行多模式3D脑肿瘤分割
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 由于结构上的复杂性和神经胶质瘤的个体差异，准确的脑肿瘤分割仍然是一项具有挑战性的任务。利用CRF的显着细节弹性和V-NET的空间特征提取能力，我们提出了一个多模式3D体积生成对抗网络（3D-VGAN），以进行精确的分割。该模型利用伪3D进行V-NET改进，在发电机后添加条件随机场，并使用原始图像作为补充指导。结果使用BRATS-2018数据集，表明3D-VGAN的表现优于经典分割模型，包括U-NET，GAN，FCN和3D V-NET，其特异性超过99.8％。

### A Multimodal Approach to The Detection and Classification of Skin Diseases 
[[arxiv](https://arxiv.org/abs/2411.13855)] [[cool](https://papers.cool/arxiv/2411.13855)] [[pdf](https://arxiv.org/pdf/2411.13855)]
> **Authors**: Allen Yang,Edward Yang
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 一种多模式检测和分类皮肤疾病的方法
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 根据PBS的数据，将近三分之一的美国人无法获得初级保健服务，另外四十％的延迟将避免医疗费用。结果，即使该疾病在皮肤上表现出许多身体症状，许多疾病也未经诊断和没有治疗。随着人工智能的兴起，自我诊断和疾病识别的改善变得比以往任何时候都更有希望。尽管如此，现有方法还是缺乏大规模的患者数据库和过时的研究方法，导致研究仅限于几种疾病或方式。这项研究结合了随时可用的，可以通过图像和文本易于获取的患者信息，以在26种皮肤病类型的新数据集上进行皮肤病分类，其中包括皮肤病图像（37K）和相关的患者叙述。使用此数据集，确定了各种图像模型的基线，以优于现有方法。最初，Resnet-50模型只能达到70％的精度，但是，在各种优化技术之后，将其精度提高到80％。此外，这项研究还提出了一种针对序列分类的新型微调策略（LLMS），选项链，该策略将复杂的推理任务分解为训练时间中的中间步骤，而不是推断。通过图像模型的一系列选择和初步疾病的建议，这种方法在诊断患者皮肤病方面达到了91％的诊断状态，只有患者皮肤疾病的形象以及患者对症状的描述（例如瘙痒或头晕）。通过这项研究，可能会发生对皮肤疾病的较早诊断，临床医生可以使用深度学习模型来进行更准确的诊断，改善生活质量并挽救生命。

### Image Compression Using Novel View Synthesis Priors 
[[arxiv](https://arxiv.org/abs/2411.13862)] [[cool](https://papers.cool/arxiv/2411.13862)] [[pdf](https://arxiv.org/pdf/2411.13862)]
> **Authors**: Luyuan Peng,Mandar Chitre,Hari Vishnu,Yuen Min Too,Bharath Kalyan,Rajat Mishra,Soo Pieng Tan
> **First submission**: 2024-11-21
> **First announcement**: 2024-11-22
> **comment**: Preprint submitted to IEEE Journal of Oceanic Engineering
- **标题**: 使用新视图合成先验的图像压缩
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器人技术
- **摘要**: 实时视觉反馈对于对远程操作的车辆的无限控制至关重要，尤其是在检查和操纵任务期间。尽管声学通信是水下中等范围通信的首选选择，但其有限的带宽使实时传输图像或视频是不切实际的。为了解决这个问题，我们提出了一种基于模型的图像压缩技术，该技术利用先前的任务信息。我们的方法采用训练有素的基于机器学习的新型视图合成模型，并使用梯度下降优化来完善潜在表示，以帮助在相机图像和渲染图像之间产生可压缩的差异。我们使用来自人造海洋盆地的数据集评估了提出的压缩技术，显示出优于现有技术的较高压缩比和图像质量。此外，我们的方法表现出鲁棒性，可以在场景中引入新物体，这突出了其推进无绳远程操作的车辆操作的潜力。

### Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and Pediatrics 
[[arxiv](https://arxiv.org/abs/2411.15872)] [[cool](https://papers.cool/arxiv/2411.15872)] [[pdf](https://arxiv.org/pdf/2411.15872)]
> **Authors**: Sarim Hashmi,Juan Lugo,Abdelrahman Elsayed,Dinesh Saggurthi,Mohammed Elseiagy,Alikhan Nurkamal,Jaskaran Walia,Fadillah Adamsyah Maani,Mohammad Yaqub
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: No comments
- **标题**: 用Mednext优化脑肿瘤分割：Brats 2024 SSA和儿科
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 鉴定脑MRI中的关键病理特征对于神经胶质瘤患者的长期存活至关重要。但是，手动细分是耗时的，需要专家干预，并且容易受到人为错误的影响。因此，大量研究致力于开发机器学习方法，该方法可以准确地分割3D多模式脑MRI扫描中的肿瘤。尽管取得了进步，但最新的模型通常受到培训的数据的限制，这引起了对可能引入分配变化的不同人群的可靠性的担忧。这种转变可能源于较低的MRI技术（例如，在撒哈拉以南非洲）或患者人口统计学（例如儿童）的变化。 BRATS-2024挑战提供了解决这些问题的平台。这项研究介绍了我们使用MEDNEXT，全面模型结合和彻底的后处理方法来分割BRATS-2024 SSA和小儿肿瘤任务中肿瘤的方法。我们的方法在看不见的验证集上表现出很强的性能，在BRATS-2024 SSA数据集上达到了平均骰子相似性系数（DSC）为0.896，而Brats小儿肿瘤数据集的平均DSC为0.830。此外，我们的方法在BRATS-2024 SSA数据集上达到了14.682的Hausdorff距离（HD95），而Brats儿科数据集的平均HD95为37.508。我们的GitHub存储库可以在此处访问：项目存储库：https：//github.com/python-arch/biombz-optimizing-brain-tumor-tumor-segentation-with-mednext-mednext-brats-2024-ssa-ssa-ssa-ssa-and-petiatrics

### M3-CVC: Controllable Video Compression with Multimodal Generative Models 
[[arxiv](https://arxiv.org/abs/2411.15798)] [[cool](https://papers.cool/arxiv/2411.15798)] [[pdf](https://arxiv.org/pdf/2411.15798)]
> **Authors**: Rui Wan,Qi Zheng,Yibo Fan
> **First submission**: 2024-11-24
> **First announcement**: 2024-11-25
> **comment**: Accepted to ICASSP 2025
- **标题**: M3-CVC：具有多模式生成模型的可控视频压缩
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 传统和神经视频编解码器通常在超低 - 二核编码方案下遇到可控性和通用性的局限性。为了克服这些挑战，我们提出了M3-CVC，这是一个可控的视频压缩框架，结合了多模式生成模型。该框架利用语义 - 动作复合策略来选择关键信息。对于每个密钥帧及其相应的视频剪辑，基于对话的大型多模型模型（LMM）方法提取了层次的时空详细信息，从而启用了框架间和框架内表示，以改善视频保真度，同时增强编码解释性。 M3-CVC进一步采用了条件扩散的，文本引导的钥匙帧压缩方法，在框架重建中实现了高保真度。在解码过程中，从LMMS得出的文本描述指导扩散过程，以准确恢复原始视频的内容。实验结果表明，M3-CVC在超低比特率方案中的最新VVC标准显着胜过，尤其是在保留语义和感知忠诚度方面。

### MulModSeg: Enhancing Unpaired Multi-Modal Medical Image Segmentation with Modality-Conditioned Text Embedding and Alternating Training 
[[arxiv](https://arxiv.org/abs/2411.15576)] [[cool](https://papers.cool/arxiv/2411.15576)] [[pdf](https://arxiv.org/pdf/2411.15576)]
> **Authors**: Chengyin Li,Hui Zhu,Rafi Ibn Sultan,Hassan Bagher Ebadian,Prashant Khanduri,Chetty Indrin,Kundan Thind,Dongxiao Zhu
> **First submission**: 2024-11-23
> **First announcement**: 2024-11-25
> **comment**: Accepted by WACV-2025
- **标题**: Mulmodseg：使用模态条件嵌入和交替培训增强未配对的多模式医学图像分割
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 在医学成像的各种领域，自动分割具有许多应用，必须处理各种输入域，例如不同类型的计算机断层扫描（CT）扫描和磁共振（MR）图像。这种异质性挑战自动分割算法，由于需要空间对齐和配对的图像，因此在不同模态上保持一致的性能。通常，使用单一模态对细分模型进行培训，这限制了它们在不使用传输学习技术的情况下将其推广到其他类型的输入数据的能力。此外，利用不同模式的互补信息来增强细分精度通常需要对流行的编码器编码设计进行实质性修改，例如为每种方式引入多个分支编码或解码路径。在这项工作中，我们提出了一种简单的多模式分割（Mulmodseg）策略，以增强跨多种模式，特别是CT和MR的医学图像分割。它结合了两个关键设计：通过冷冻文本编码器的模式调节文本嵌入框架，该框架为现有的分割框架增加了模态意识，而无需进行重大结构修改或计算开销，以及交替的培训程序，以促进不属于CT和MR输入的基本功能集成。通过对完全卷积网络和基于变压器的主机进行的广泛实验，Mulmodseg始终优于CT和MR模态的腹部多器官和心脏子结构方面的先前方法。该代码可在此{\ href {https://github.com/chengyinlee/mulmodseg_2024} {link}}}中获得。

### Optimized Vessel Segmentation: A Structure-Agnostic Approach with Small Vessel Enhancement and Morphological Correction 
[[arxiv](https://arxiv.org/abs/2411.15251)] [[cool](https://papers.cool/arxiv/2411.15251)] [[pdf](https://arxiv.org/pdf/2411.15251)]
> **Authors**: Dongning Song,Weijian Huang,Jiarun Liu,Md Jahidul Islam,Hao Yang,Shanshan Wang
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 12 pages, 7 figurres, submitted to TIP
- **标题**: 优化的容器分割：一种结构不足的方法，具有小血管增强和形态校正
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 血管的准确分割对于各种临床评估和术后分析至关重要。然而，血管成像的固有挑战，例如稀疏性，细粒度，低对比度，数据分布可变性以及保留拓扑结构的关键需求，使广义血管分割变得特别复杂。尽管已经为特定的解剖区域开发了专门的分割方法，但它们对量身定制的模型的过度依赖性阻碍了更广泛的适用性和泛化。医学成像中引入的通用分割模型通常无法解决关键的血管特征，包括分割结果的连通性。为了克服这些局限性，我们提出了一个优化的容器分割框架：一种结构不合时宜的方法，该方法结合了小血管增强和多模式血管分割的形态校正。为了训练和验证该框架，我们编制了一个跨越17个数据集的全面的多模式数据集，并根据六种基于SAM的方法和17个专家模型对模型进行了基准测试。结果表明，我们的方法达到了卓越的分割准确性，概括和连通性提高34.6％，从而强调了其临床潜力。一项消融研究进一步验证了拟议的改进的有效性。这项工作发布后，我们将在GitHub发布代码和数据集。

### RankByGene: Gene-Guided Histopathology Representation Learning Through Cross-Modal Ranking Consistency 
[[arxiv](https://arxiv.org/abs/2411.15076)] [[cool](https://papers.cool/arxiv/2411.15076)] [[pdf](https://arxiv.org/pdf/2411.15076)]
> **Authors**: Wentao Huang,Meilong Xu,Xiaoling Hu,Shahira Abousamra,Aniruddha Ganguly,Saarthak Kapse,Alisa Yurovsky,Prateek Prasanna,Tahsin Kurc,Joel Saltz,Michael L. Miller,Chao Chen
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 17 pages, 8 figures
- **标题**: Rankbygene：基因引导的组织病理学表示通过跨模式排名一致性学习
- **领域**: 图像和视频处理,计算机视觉和模式识别,定量方法
- **摘要**: 空间转录组学（ST）通过绘制组织中的基因表达来提供必不可少的空间环境，从而详细研究了细胞异质性和组织组织。但是，将ST数据与组织学图像对齐构成固有的空间扭曲和特定于模态的变化引起的挑战。现有方法在很大程度上取决于直接对齐，而直接对齐通常无法捕获复杂的跨模式关系。为了解决这些局限性，我们提出了一个新颖的框架，该框架使用基于排名的对准损失来对齐基因和图像特征，从而保留跨模态的相对相似性并实现强大的多规模比对。为了进一步增强对齐的稳定性，我们使用教师的网络体系结构采用了自我监督的知识蒸馏，从而有效地减轻了基因表达数据中高维度，稀疏性和噪声的破坏。关于基因表达预测和生存分析的广泛实验证明了我们的框架的有效性，表明对现有方法的一致性和预测性能得到了改善，并为数字病理学中的基因引导图像表示学习建立了强大的工具。

## 信号处理(eess.SP:Signal Processing)

该领域共有 7 篇论文

### Demo: Multi-Modal Seizure Prediction System 
[[arxiv](https://arxiv.org/abs/2411.05817)] [[cool](https://papers.cool/arxiv/2411.05817)] [[pdf](https://arxiv.org/pdf/2411.05817)]
> **Authors**: Ali Saeizadeh,Pietro Brach del Prever,Douglas Schonholtz,Raffaele Guida,Emrecan Demirors,Jorge M. Jimenez,Pedram Johari,Tommaso Melodia
> **First submission**: 2024-11-01
> **First announcement**: 2024-11-11
> **comment**: 1 page, 1 figure, Proceedings of the IEEE 20th International Conference on Body Sensor Networks (BSN), October 2024
- **标题**: 演示：多模式癫痫发作预测系统
- **领域**: 信号处理,机器学习
- **摘要**: 该演示介绍了Seiznet，这是一种创新的系统，用于预测从多模式传感器网络中受益的癫痫发作并利用深度学习（DL）技术。癫痫病影响了全球约6500万人，其中许多人经历了耐药性癫痫发作。 Seiznet旨在提供高度准确的警报，使个人能够采取预防措施而不会被错误警报打扰。 SEIZNET结合了通过侵入性（颅内脑电图（IEEG））或无创（脑电图（EEG）（EEG）和心电图（ECG）传感器（ECG））收集的数据组合，并通过高级DL算法进行处理，这些算法可在Edge上进行实时投入到EDGE上，以实现Edge，以实现隐私范围，以实现隐私范围。 Seiznet在癫痫发作预测方面的精度> 97％，同时保持植入设备的尺寸和能量限制。

### A Multi-Modal Unsupervised Machine Learning Approach for Biomedical Signal Processing in CPR 
[[arxiv](https://arxiv.org/abs/2411.11869)] [[cool](https://papers.cool/arxiv/2411.11869)] [[pdf](https://arxiv.org/pdf/2411.11869)]
> **Authors**: Saidul Islam,Jamal Bentahar,Robin Cohen,Gaith Rjoub
> **First submission**: 2024-11-03
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: CPR中生物医学信号处理的多模式无监督的机器学习方法
- **领域**: 信号处理,人工智能,机器学习
- **摘要**: 心肺复苏（CPR）是一种关键的挽救生命的干预措施，旨在恢复患有心脏骤停或呼吸衰竭的人的血液循环和呼吸。从院前阶段到重症监护室（ICU），对CPR期间生物医学信号的准确和实时分析对于监测和决策至关重要。但是，CPR信号通常被噪音和文物损坏，使精确的解释具有挑战性。传统的剥离方法，例如过滤器，难以适应CPR信号中存在的不同且复杂的噪声模式。鉴于CPR的高风险性质，快速，准确的反应可以决定生存，因此迫切需要更强大和适应性的脱氧技术。在这种情况下，无监督的机器学习（ML）方法特别有价值，因为它消除了对标记数据的依赖，在紧急情况下，这可能是稀缺或不切实际的。本文介绍了一种新型的无监督的ML方法，用于使用多模式框架来降级CPR信号，该框架利用多种信号源来增强该过程。所提出的方法不仅可以改善降低降噪和信号保真度，而且还保留了关键的信号间相关性（0.9993），这对于下游任务至关重要。此外，就信噪比（SNR）和峰值信噪比（PSNR）而言，它在无监督的上下文中胜过现有方法，这对于实时应用程序非常有效。多模式的整合进一步增强了该系统对CPR以外的各种生物医学信号的适应性，从而改善了自动化的CPR系统和临床决策。

### AI Flow at the Network Edge 
[[arxiv](https://arxiv.org/abs/2411.12469)] [[cool](https://papers.cool/arxiv/2411.12469)] [[pdf](https://arxiv.org/pdf/2411.12469)]
> **Authors**: Jiawei Shao,Xuelong Li
> **First submission**: 2024-11-19
> **First announcement**: 2024-11-20
> **comment**: This paper has been accepted to IEEE Network Magazine
- **标题**: 网络边缘的AI流动
- **领域**: 信号处理,人工智能,机器学习,网络和互联网架构
- **摘要**: 大型语言模型（LLM）及其多模式变体的最新进展导致了各个领域的显着进步，表现出令人印象深刻的能力和前所未有的潜力。在无处不在的连接时代，利用通信网络分发情报是一个变革性的概念，它设想了在网络边缘访问的AI驱动服务。但是，将大型模型从云到受资源受限的环境面临着关键的挑战。低端设备的模型推断会导致过度的潜伏期和性能瓶颈，而在有限的带宽网络上的原始数据传输会导致高通信开销。本文介绍了AI Flow，该框架通过共同利用跨设备，边缘节点和云服务器可用的异质资源来简化推理过程，从而使智能流跨网络。为了促进多个计算节点之间的合作，提出的框架探讨了通信网络系统从传输信息流到智能流的设计范式转移，在这种流程中，通信的目标是面向任务并将其折叠成推理过程。实验结果证明了通过图像字幕的用例表明了所提出的框架的有效性，展示了减少响应潜伏期的能力，同时保持高质量的标题。本文是确定AI流动的动机，挑战和原则的位置论文。

### Physically Parameterized Differentiable MUSIC for DoA Estimation with Uncalibrated Arrays 
[[arxiv](https://arxiv.org/abs/2411.15144)] [[cool](https://papers.cool/arxiv/2411.15144)] [[pdf](https://arxiv.org/pdf/2411.15144)]
> **Authors**: Baptiste Chatelier,José Miguel Mateos-Ramos,Vincent Corlay,Christian Häger,Matthieu Crussière,Henk Wymeersch,Luc Le Magoarou
> **First submission**: 2024-11-06
> **First announcement**: 2024-11-22
> **comment**: No comments
- **标题**: 带有未校准阵列的DOA估计的物理参数化的可区分音乐
- **领域**: 信号处理,人工智能,信息论,机器学习
- **摘要**: 到达方向（DOA）估计是雷达，声纳，音频和无线通信系统中的常见感应问题。随着综合传感和通信范式的出现，它已重新获得了重视。为了充分利用此类传感系统的潜力，要考虑可能对所获得的性能产生负面影响的潜在硬件障碍至关重要。这项研究介绍了基于模型的方法，介绍了DOA估计和硬件损伤学习方案。具体而言，得出了多个信号分类（音乐）算法的可区分版本，从而有效地学习了所考虑的障碍。所提出的方法支持受监督和无监督的学习策略，展示其实际潜力。仿真结果表明，所提出的方法成功地学习了天线位置和复杂增长的明显不准确性。此外，所提出的方法在DOA估计任务中优于古典音乐算法。

### CardioLab: Laboratory Values Estimation and Monitoring from Electrocardiogram Signals -- A Multimodal Deep Learning Approach 
[[arxiv](https://arxiv.org/abs/2411.14886)] [[cool](https://papers.cool/arxiv/2411.14886)] [[pdf](https://arxiv.org/pdf/2411.14886)]
> **Authors**: Juan Miguel Lopez Alcaraz,Nils Strodthoff
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-25
> **comment**: 7 pages, 1 figure, code under https://github.com/AI4HealthUOL/CardioLab
- **标题**: Cardiolab：心电图信号的实验室值估计和监视 - 一种多模式深度学习方法
- **领域**: 信号处理,机器学习
- **摘要**: 背景：实验室价值是医学诊断和管理的基础，但是获取这些价值可能是昂贵，侵入性和耗时的。尽管心电图（ECG）模式已与某些实验室异常相关，但这些关系的全面建模仍然没有得到充实的态度。方法：我们利用模拟IV数据集开发多模式深度学习模型，以证明来自ECG波形，人口统计学，生物透镜和生命体征的估计（实时）和监视（实时）和监视（预测）实验室价值异常。结果：模型在估计设置中的23个实验室值和在监视设置中的23个实验室值以统计学意义的方式表现出强大的预测性能，而AUROC得分以统计学意义的方式高于0.70。最值得注意的是，跨多种生理类别（例如心脏，肾脏，血液学，代谢，免疫学和凝结）的准确预测值包含异常。命名示例，用于0.882的估计NTPROBNP（> 353 pg/ml），而在30分钟的尿素氮（<6 mg/dl）为0.851，以0.851的速度进行监测，在60分钟，肌酐（<0.5 mg/dl），含0.85，含0.85，以及120分钟的hemogoglobin（> 17.5 g/dl）。结论：这项研究提供了首先证明使用ECG数据以及临床常规数据以及对实验室价值异常的实时估计和监测的可行性，这可以为传统的实验室测试提供非侵入性，具有成本效益的补充，对增强患者监测和早期干预具有很大的影响。进一步的验证可以促进其整合到常规临床实践中。

### Deciphering Acoustic Emission with Machine Learning 
[[arxiv](https://arxiv.org/abs/2411.17755)] [[cool](https://papers.cool/arxiv/2411.17755)] [[pdf](https://arxiv.org/pdf/2411.17755)]
> **Authors**: Dénes Berta,Balduin Katzer,Katrin Schulz,Péter Dusán Ispánovity
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 通过机器学习解密的声发射
- **领域**: 信号处理,材料科学,机器学习
- **摘要**: 声发射信号已显示出材料中的雪崩样事件，例如晶体固体中的脱位雪崩，多孔物质中空隙的崩溃或铁族中的域壁运动。声发射测量提供的数据非常丰富，但是将其与触发雪崩的特征相关联是一项挑战。在我们的工作中，我们提出了一种基于机器学习的方法，可以通过该方法从仅声发射数据中的微柱压缩测试中推断出脱位雪崩的微观细节。正如本文中所证明的那样，这种方法适用于对力时间响应的预测，因为它可以为雪崩的时间位置提供出色的预测，还可以预测单个变形事件的幅度。在我们的机器学习方法中使用了各种描述符（包括频率依赖和独立的描述符），并分析了它们在预测中的重要性。还展示了该方法向其他标本大小的可传递性，并讨论了在更通用的设置中的可能应用。

### Comparison of Tiny Machine Learning Techniques for Embedded Acoustic Emission Analysis 
[[arxiv](https://arxiv.org/abs/2411.17733)] [[cool](https://papers.cool/arxiv/2411.17733)] [[pdf](https://arxiv.org/pdf/2411.17733)]
> **Authors**: Uditha Muthumala,Yuxuan Zhang,Luciano Sebastian Martinez-Rau,Sebastian Bader
> **First submission**: 2024-11-22
> **First announcement**: 2024-11-27
> **comment**: Conference Presentations (Accepted) at IEEE 10th World Forum on Internet of Things. "https://wfiot2024.iot.ieee.org/program/technical-paper-program"
- **标题**: 用于嵌入声发射分析的微型机器学习技术的比较
- **领域**: 信号处理,机器学习,音频和语音处理
- **摘要**: 本文将机器学习方法与不同的输入数据格式进行了比较，以分类声发射（AE）信号。在许多结构性健康监测应用中，AE信号是一种有希望的监测技术。机器学习已被证明是一种有效的数据分析方法，并根据其代表的损坏机制对不同的AE信号进行了分类。这些分类可以基于从中提取的整个AE波形或特定功能执行。但是，目前未知这些方法是首选。为了在资源约束的物联网（IoT）系统上进行模型部署，这项工作评估并比较了分类准确性，内存需求，处理时间和能源消耗的两种方法。为此，提取并精心选择了功能，针对每个输入数据方案设计和优化了神经网络模型，并且模型部署在低功率IoT节点上。比较分析表明，所有模型均可达到超过99 \％的高分类精度，但是嵌入式特征提取在计算上是昂贵的。因此，利用RAW AE信号作为输入的模型具有最快的处理速度，因此是最低的能量消耗，这是以更大的内存要求为代价的。

## 优化与控制(math.OC:Optimization and Control)

该领域共有 1 篇论文

### A Learned Proximal Alternating Minimization Algorithm and Its Induced Network for a Class of Two-block Nonconvex and Nonsmooth Optimization 
[[arxiv](https://arxiv.org/abs/2411.06333)] [[cool](https://papers.cool/arxiv/2411.06333)] [[pdf](https://arxiv.org/pdf/2411.06333)]
> **Authors**: Yunmei Chen,Lezhi Liu,Lei Zhang
> **First submission**: 2024-11-09
> **First announcement**: 2024-11-11
> **comment**: No comments
- **标题**: 一种学习的近端交流算法及其诱导的网络，用于一类两块非凸和非滑动优化的网络
- **领域**: 优化与控制,机器学习
- **摘要**: 这项工作提出了一种一般学到的近端交替最小化算法LPAM，用于解决可学习的两块块非滑动和非convex优化问题。我们通过适当的平滑技术来解决非平滑度，并自动减少平滑效果。对于平滑的非凸问题，我们通过合并残留学习体系结构来修改近端交替的线性最小化（PALM）方案（PALM）方案，该结构已被证明在深层网络训练方面非常有效，并采用了块坐标（BCD）迭代，以作为算法融合的保障措施。我们证明，LPAM生成的迭代物具有一个子序列，LPAM至少有一个累积点，每个累积点都是Clarke固定点。我们的方法广泛适用，因为人们可以采用各种学习问题作为两个障碍优化，并且也很容易扩展用于求解多块非块和非convex优化问题。该网络的体系结构完全遵循LPAM，即LPAM-NET，它继承了该算法的收敛属性，以使网络可解释。作为LPAM-NET的示例应用，我们介绍了LPAM-NET在联合多模式MRI重建中的应用的数值和理论结果，并具有显着采样不足的K-Space数据。实验结果表明，与某些最新方法相比，提出的LPAM-NET具有参数效率，并且具有优惠的性能。

## 物理与社会(physics.soc-ph:Physics and Society)

该领域共有 1 篇论文

### Mobility-based Traffic Forecasting in a Multimodal Transport System 
[[arxiv](https://arxiv.org/abs/2411.08052)] [[cool](https://papers.cool/arxiv/2411.08052)] [[pdf](https://arxiv.org/pdf/2411.08052)]
> **Authors**: Henock M. Mboko,Mouhamadou A. M. T. Balde,Babacar M. Ndiaye
> **First submission**: 2024-11-05
> **First announcement**: 2024-11-13
> **comment**: 17 pages, 18 figures
- **标题**: 在多模式运输系统中基于机动性的交通预测
- **领域**: 物理与社会,机器学习,社交和信息网络,应用领域,机器学习
- **摘要**: 我们根据人口从一个节点到另一个节点的流动性研究对人口的所有运动的分析，以观察，衡量和预测该流动性的交通影响。在道路上的拥堵频率直接或间接影响我们的经济或社会福利。我们的工作重点是探索一些机器学习方法，以通过人口移动数据的多模式运输网络预测（具有一定概率）流量。我们根据这种观察（历史基础）分析了人们运动对运输网络的影响的观察，并可能在网络上预测拥堵。

## 空间物理学(physics.space-ph:Space Physics)

该领域共有 1 篇论文

### Probabilistic Forecasting of Radiation Exposure for Spaceflight 
[[arxiv](https://arxiv.org/abs/2411.17703)] [[cool](https://papers.cool/arxiv/2411.17703)] [[pdf](https://arxiv.org/pdf/2411.17703)]
> **Authors**: Rutuja Gurav,Elena Massara,Xiaomei Song,Kimberly Sinclair,Edward Brown,Matt Kusner,Bala Poduval,Atilim Gunes Baydin
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-27
> **comment**: No comments
- **标题**: 太空飞行的辐射暴露的概率预测
- **领域**: 空间物理学,机器学习
- **摘要**: 在月球和火星任务期间，在低地球轨道（BLEO）之外的人类存在将在不久的将来构成重大挑战。与这些任务相关的主要健康风险是辐射暴露，主要来自Galatic宇宙射线（GCR）和太阳质子事件（SPE）。尽管GCR表现出更加一致的威胁，但SPE很难预测，并且可以在短时间内提供急性剂量。目前，NASA利用分析工具来监视太空辐射环境，以便立即决定庇护宇航员。但是，可以通过预测辐射暴露的预测模型可以显着增强这种反应性方法，理想情况下是在重大事件之前数小时，同时提供了预测不确定性以改善决策的估计。在这项工作中，我们提出了一种使用多模式的时间序列数据来预测BLEO的机器学习方法，包括来自太阳能动力学天文台的直接太阳图像，从GOOS任务中测量的X射线通量测量以及从Biosentinel卫星的辐射剂量测量结果，该卫星是作为Artemis〜1任务的一部分发射的。据我们所知，这是第一次使用全盘太阳图像来预测辐射暴露。我们证明，我们的模型可以预测由于SPE事件引起的辐射增加以及事件发生后的辐射衰减曲线的开始。

## 生物分子(q-bio.BM:Biomolecules)

该领域共有 1 篇论文

### Structure-Based Molecule Optimization via Gradient-Guided Bayesian Update 
[[arxiv](https://arxiv.org/abs/2411.13280)] [[cool](https://papers.cool/arxiv/2411.13280)] [[pdf](https://arxiv.org/pdf/2411.13280)]
> **Authors**: Keyue Qiu,Yuxuan Song,Jie Yu,Hongbo Ma,Ziyao Cao,Zhilong Zhang,Yushuai Wu,Mingyue Zheng,Hao Zhou,Wei-Ying Ma
> **First submission**: 2024-11-20
> **First announcement**: 2024-11-21
> **comment**: 27 pages, 17 figures
- **标题**: 通过梯度引导的贝叶斯更新优化基于结构的分子
- **领域**: 生物分子,人工智能
- **摘要**: 基于结构的分子优化（SBMO）旨在优化具有连续坐标和离散类型的分子针对蛋白质靶标。一个有希望的方向是在图像上取得了显着的成功，对生成模型进行梯度指南，但是指导模式之间的离散数据和风险不一致是一项挑战。为此，我们利用了通过贝叶斯推断得出的连续且可区分的空间，呈现了分子关节优化（MOLJO），这是第一个基于梯度的SBMO框架，促进了跨不同模态的关节指导信号，同时保留SE（3）等级。我们引入了一种新颖的后退校正策略，该策略在过去历史的滑动窗口中进行了优化，从而在优化过程中可以在探索和探索之间进行无缝的权衡。我们提出的Moljo在Crossdocked2020基准中实现了最先进的性能（成功率为51.3％，Vina Dock -9.05和SA 0.78），与基于梯度的同步组相比，成功率提高了4倍，而“ Me-Better”比率高于3D Baselines。此外，我们将Moljo扩展到广泛的优化设置，包括多目标优化和药物设计中的具有挑战性的任务，例如R组优化和脚手架跳跃，进一步强调了其多功能性和潜力。

## 基因组学(q-bio.GN:Genomics)

该领域共有 1 篇论文

### RNA-GPT: Multimodal Generative System for RNA Sequence Understanding 
[[arxiv](https://arxiv.org/abs/2411.08900)] [[cool](https://papers.cool/arxiv/2411.08900)] [[pdf](https://arxiv.org/pdf/2411.08900)]
> **Authors**: Yijia Xiao,Edward Sun,Yiqiao Jin,Wei Wang
> **First submission**: 2024-10-29
> **First announcement**: 2024-11-14
> **comment**: Machine Learning for Structural Biology Workshop, NeurIPS 2024
- **标题**: RNA-GPT：用于RNA序列理解的多模式生成系统
- **领域**: 基因组学,人工智能,计算工程、金融和科学,机器学习,生物分子
- **摘要**: RNA是具有对生命至关重要的遗传信息的重要分子，对药物开发和生物技术产生了深远的影响。尽管很重要，但RNA研究通常受到有关该主题的大量文献的阻碍。为了简化此过程，我们引入了RNA-GPT，这是一种多模式RNA聊天模型，旨在通过利用广泛的RNA文献来简化RNA发现。 RNA-GPT与线性投影层和最先进的大语言模型（LLMS）集成了RNA序列编码器，以进行精确的表示对齐，从而使其能够处理用户更上而成的RNA序列并提供简洁，准确的响应。 RNA-GPT建立在可扩展的培训管道上，使用RNA-QA，这是一种自动化系统，使用rnacentral收集RNA注释，使用divide and-concequer方法使用GPT-4O和潜在的DIRICHLET分配（LDA）来有效处理大型数据集并生成指导样品。我们的实验表明，RNA-GPT有效地解决了复杂的RNA查询，从而促进了RNA研究。此外，我们提出RNA-QA，这是一个407,616个RNA样品的数据集，用于模态对准和指导调整，进一步推动了RNA研究工具的潜力。

## 定量方法(q-bio.QM:Quantitative Methods)

该领域共有 2 篇论文

### GramSeq-DTA: A grammar-based drug-target affinity prediction approach fusing gene expression information 
[[arxiv](https://arxiv.org/abs/2411.01422)] [[cool](https://papers.cool/arxiv/2411.01422)] [[pdf](https://arxiv.org/pdf/2411.01422)]
> **Authors**: Kusal Debnath,Pratip Rana,Preetam Ghosh
> **First submission**: 2024-11-02
> **First announcement**: 2024-11-04
> **comment**: No comments
- **标题**: GRAMSEQ-DTA：一种基于语法的药物目标亲和力预测方法融合基因表达信息
- **领域**: 定量方法,机器学习
- **摘要**: 药物目标亲和力（DTA）预测是药物发现的关键方面。药物和靶标的有意义的表示对于准确的预测至关重要。对于药物和靶标，使用基于1D字符串的表示形式是一种常见的方法，它在药物目标亲和力预测中表现出了良好的结果。但是，这些方法缺乏有关原子和键相对位置的信息。为了解决此限制，基于图的表示已在某种程度上使用。但是，仅考虑药物和靶标的结构方面可能不足以进行准确的DTA预测。在遗传水平上整合这些药物的功能方面可以增强模型的预测能力。为了填补这一空白，我们提出了GRAMSEQ-DTA，将化学扰动信息与药物和靶标的结构信息相结合。我们将语法变异自动编码器（GVAE）应用于药物特征提取，并利用两种不同的方法进行蛋白质特征提取：卷积神经网络（CNN）和复发性神经网络（RNN）。化学扰动数据是从L1000项目中获得的，该项目提供了有关选定药物引起的基因上调和下调的信息。处理该化学扰动信息，并准备紧凑的数据集，作为药物的功能特征集。通过在模型中整合药物，基因和靶特征，我们的方法在广泛使用的DTA数据集（BindingDB，Davis和Kiba）上进行验证时，我们的方法优于当前最新的DTA预测模型。这项工作通过融合生物实体的结构和功能方面，为DTA预测提供了一种新颖而实用的方法，并鼓励对多模式DTA预测进行进一步的研究。

### A quantum inspired predictor of Parkinsons disease built on a diverse, multimodal dataset 
[[arxiv](https://arxiv.org/abs/2411.18640)] [[cool](https://papers.cool/arxiv/2411.18640)] [[pdf](https://arxiv.org/pdf/2411.18640)]
> **Authors**: Diya Vatsavai,Anya Iyer,Ashwin A. Nair
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-28
> **comment**: 20 pages, 3 figures, 1 table
- **标题**: 帕克森氏病的量子启发的预测因子，建立在多样化的多模式数据集上
- **领域**: 定量方法,机器学习
- **摘要**: 帕金森氏病是全球增长最快的神经退行性疾病，在短短两年内，病例增加了50％。随着言语，记忆和运动症状随着时间的流逝而恶化，早期诊断对于保留患者的生活质量至关重要。尽管基于机器学习的检测已显示出希望，但由于患者之间症状的差异，依靠单个特征进行分类可能容易出错。为了解决此限制，我们利用了MPower数据库，该数据库包括四个关键生物标志物的15万个样本：语音，步态，敲击和人口统计数据。从这些测量值中，我们提取了64个功能并训练了基线随机森林模型，以选择高于80％的特征。对于分类，我们设计了一个可模拟的量子支持向量机（QSVM），该量子机（QSVM）检测高维模式，利用了量子机学习的最新进步。借助可以在标准硬件而不是资源密集型量子计算机上运行的新颖，可模拟的体系结构，我们的模型的精度为90％，AUC的精度为0.98，超过了基准模型。通过利用建立在各种功能的创新分类框架上，我们的模型为全球帕金森氏症筛查提供了途径。

## 计算金融(q-fin.CP:Computational Finance)

该领域共有 1 篇论文

### Quantifying Qualitative Insights: Leveraging LLMs to Market Predict 
[[arxiv](https://arxiv.org/abs/2411.08404)] [[cool](https://papers.cool/arxiv/2411.08404)] [[pdf](https://arxiv.org/pdf/2411.08404)]
> **Authors**: Hoyoung Lee,Youngsoo Choi,Yuhee Kwon
> **First submission**: 2024-11-13
> **First announcement**: 2024-11-14
> **comment**: 7 pages, 4 figures
- **标题**: 量化定性见解：利用LLM到市场预测
- **领域**: 计算金融,机器学习
- **摘要**: 大语言模型（LLMS）的最新进步有可能通过整合数值和文本数据来转化财务分析。但是，在融合多模式信息时不足的挑战以及LLMs作为文本产生的定性产出的效用时的难度限制了其在财务预测等任务中的有效性。这项研究通过利用证券公司的日常报告来解决这些挑战，以创建高质量的上下文信息。这些报告分为基于文本的关键因素，并与数值数据（例如价格信息）结合在一起，以形成上下文集。通过根据查询时间动态更新几示示例，这些集合包含了最新信息，形成了与查询点紧密一致的高度相关的集合。此外，精心设计的提示旨在将分数分配给关键因素，从而将定性见解转化为定量结果。派生的分数经历了缩放过程，将它们转换为用于预测的现实世界值。我们的实验表明，LLMS在市场预测中的表现超过了时间序列模型，尽管诸如不完善的可重复性和有限的解释性之类的挑战仍然存在。

## 交易和市场微观结构(q-fin.TR:Trading and Market Microstructure)

该领域共有 1 篇论文

### FinVision: A Multi-Agent Framework for Stock Market Prediction 
[[arxiv](https://arxiv.org/abs/2411.08899)] [[cool](https://papers.cool/arxiv/2411.08899)] [[pdf](https://arxiv.org/pdf/2411.08899)]
> **Authors**: Sorouralsadat Fatemi,Yuheng Hu
> **First submission**: 2024-10-29
> **First announcement**: 2024-11-14
> **comment**: Accepted at ICAIF 2024
- **标题**: Finvision：股票市场预测的多代理框架
- **领域**: 交易和市场微观结构,人工智能
- **摘要**: 金融交易一直是一项具有挑战性的任务，因为它需要从各种模式中整合大量数据。传统的深度学习和强化学习方法需要大量的培训数据，并且通常涉及将各种数据类型编码为模型输入的数值格式，这限制了模型行为的解释性。最近，基于LLM的代理商在处理多模式数据方面表现出了显着的进步，使他们能够执行复杂的多步骤决策任务，同时为他们的思维过程提供见解。这项研究介绍了专门为财务交易任务设计的多模式多模式多代理系统。我们的框架采用了一个专业的基于LLM的代理团队，每个人都擅长处理和解释各种形式的财务数据，例如文本新闻报告，烛台图表和交易信号图表。我们方法的一个关键特征是整合反射模块，该模块对历史交易信号及其结果进行了分析。这种反思过程有助于增强系统的决策能力，以实现未来的交易方案。此外，消融研究表明，视觉反射模块在增强我们框架的决策能力方面起着至关重要的作用。

## 机器学习(stat.ML:Machine Learning)

该领域共有 4 篇论文

### EigenVI: score-based variational inference with orthogonal function expansions 
[[arxiv](https://arxiv.org/abs/2410.24054)] [[cool](https://papers.cool/arxiv/2410.24054)] [[pdf](https://arxiv.org/pdf/2410.24054)]
> **Authors**: Diana Cai,Chirag Modi,Charles C. Margossian,Robert M. Gower,David M. Blei,Lawrence K. Saul
> **First submission**: 2024-10-31
> **First announcement**: 2024-11-01
> **comment**: 25 pages, 9 figures. Advances in Neural Information Processing Systems (NeurIPS), 2024
- **标题**: EIGENVI：基于分数的变异推理，具有正交函数的扩展
- **领域**: 机器学习,机器学习,计算
- **摘要**: 我们开发了eigenvi，这是一种基于特征值的黑盒变化推理（BBVI）的方法。 Eigenvi从正交函数扩展中构建其变异近似值。对于$ \ Mathbb {r}^d $上方的分布，这些扩展中的最低订单项提供了高斯变分近似，而高阶项则提供了一种模型非高斯性的系统方法。这些近似值足够灵活，可以对复杂分布进行建模（多模式，不对称），但是它们很简单，可以计算出其低阶矩并从中绘制样品。 EIGENVI还可以通过构造来自正交函数的不同家族的变异近似值来对其他类型的随机变量（例如，非负，有限）建模。在这些家族中，EIGENVI通过最大程度地减少Fisher Divergence的随机估计值来计算最能与目标分布的得分函数相匹配的变异近似。值得注意的是，这种优化减少了解决最低特征值问题，因此特征有效地避开了许多其他BBVI算法所需的基于迭代梯度的优化。 （基于梯度的方法可以对学习率，终止标准和其他可调的超参数敏感。）我们使用eigenvi近似各种目标分布，包括来自Postteriordb的基准贝叶斯模型。在这些分布中，我们发现eigenvi比高斯BBVI的现有方法更准确。

### Unified Bayesian representation for high-dimensional multi-modal biomedical data for small-sample classification 
[[arxiv](https://arxiv.org/abs/2411.07043)] [[cool](https://papers.cool/arxiv/2411.07043)] [[pdf](https://arxiv.org/pdf/2411.07043)]
> **Authors**: Albert Belenguer-Llorens,Carlos Sevilla-Salcedo,Jussi Tohka,Vanessa Gómez-Verdejo
> **First submission**: 2024-11-11
> **First announcement**: 2024-11-12
> **comment**: 36 pages, 3 figures and 3 tables
- **标题**: 统一的贝叶斯表示，用于用于小样本分类的高维多模式生物医学数据
- **领域**: 机器学习,机器学习
- **摘要**: 我们提出了Baldur，这是一种新型的贝叶斯算法，旨在处理高维设置中的多模式数据集和小样本量，同时提供可解释的解决方案。为此，所提出的模型结合了共同的潜在空间中不同的数据视图，以提取相关信息以求解分类任务并修剪无关/冗余功能/数据视图。此外，为了在较小的样本量场景中提供可推广的解决方案，Baldur有效地将视图上的双内核与较小的样品与功能比率集成在一起。最后，其线性性质确保了模型结果的解释性，从而允许其用于生物标志物识别。该模型在两个不同的神经变性数据集上进行了测试，表现优于最先进的模型，并检测与科学文献中已经描述的标记对齐的特征。

### Parallelly Tempered Generative Adversarial Networks 
[[arxiv](https://arxiv.org/abs/2411.11786)] [[cool](https://papers.cool/arxiv/2411.11786)] [[pdf](https://arxiv.org/pdf/2411.11786)]
> **Authors**: Jinwon Sohn,Qifan Song
> **First submission**: 2024-11-18
> **First announcement**: 2024-11-19
> **comment**: No comments
- **标题**: 一致的恢复生成的对抗网络
- **领域**: 机器学习,机器学习
- **摘要**: 生成的对抗网络（GAN）一直是生成人工智能（AI）的代表性骨干模型，因为它在捕获复杂的数据生成过程方面具有强大的性能。但是，GAN训练以其臭名昭著的训练不稳定而闻名，通常以模式崩溃的发生为特征。通过梯度方差的镜头，这项工作特别分析了模式崩溃的训练不稳定性和效率低下，通过将其与目标分布中的多模态联系起来。为了缓解严重多模式的培训问题，我们引入了一个新型的GAN训练框架，该框架利用了通过凸插值产生的一系列钢化分布。借助我们新开发的GAN目标函数，发电机可以同时学习所有钢化分布，从概念上与统计中的平行回火产生共鸣。我们的仿真研究表明，在图像和表格数据合成中，我们的方法比现有的流行培训策略的优越性。从理论上讲，我们可以通过使用矫正分布来降低梯度估计的方差而产生这种显着的改进。最后，我们进一步开发了旨在生成公平合成数据的拟议框架的变体，这是值得信赖的AI领域日益增长的兴趣之一。

### Conformalised Conditional Normalising Flows for Joint Prediction Regions in time series 
[[arxiv](https://arxiv.org/abs/2411.17042)] [[cool](https://papers.cool/arxiv/2411.17042)] [[pdf](https://arxiv.org/pdf/2411.17042)]
> **Authors**: Eshant English,Christoph Lippert
> **First submission**: 2024-11-25
> **First announcement**: 2024-11-26
> **comment**: Workshop on Bayesian Decision-making and Uncertainty, 38th Conference on Neural Information Processing Systems (NeurIPS 2024)
- **标题**: 时间序列中关节预测区域的共形条件归一流流
- **领域**: 机器学习,机器学习
- **摘要**: 共形预测提供了一个有力的框架，用于量化机器学习模型中的不确定性，从而可以构建具有有限样本有效性保证的预测集。虽然易于适应非稳定模型，但将共形预测应用于概率生成模型，但诸如标准化流程之类的范围并不简单。这项工作提出了一种新颖的方法，可以将有条件地归一化流量归一化，专门解决了获得多步时间序列预测的预测区域的问题。我们的方法利用了将流量标准化的灵活性产生潜在的脱节预测区域，从而在存在潜在的多模式预测分布的情况下提高了预测效率。

## 其他论文

共有 66 篇其他论文

- [Analysing the Interplay of Vision and Touch for Dexterous Insertion Tasks](https://arxiv.org/abs/2410.23860)
  - **标题**: 分析视觉和触摸的触觉插入任务
  - **Filtered Reason**: none of cs.RO in whitelist
- [MACE: Leveraging Audio for Evaluating Audio Captioning Systems](https://arxiv.org/abs/2411.00321)
  - **标题**: 狼牙棒：利用音频评估音频字幕系统
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Neurobench: DCASE 2020 Acoustic Scene Classification benchmark on XyloAudio 2](https://arxiv.org/abs/2410.23776)
  - **标题**: Neurobench：Dcase 2020声学场景分类基准在Xyloaudio 2上
  - **Filtered Reason**: none of cs.NE,eess.AS,cs.SD in whitelist
- [Leveraging LLM and Text-Queried Separation for Noise-Robust Sound Event Detection](https://arxiv.org/abs/2411.01174)
  - **标题**: 利用LLM和文本引起的分离进行噪声稳定声音事件检测
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence](https://arxiv.org/abs/2411.01805)
  - **标题**: MOMU扩散：在学习长期运动音乐同步和对应方面
  - **Filtered Reason**: none of eess.AS,cs.SD,cs.MM in whitelist
- [Estimating the Number and Locations of Boundaries in Reverberant Environments with Deep Learning](https://arxiv.org/abs/2411.02609)
  - **标题**: 估计带有深度学习回响环境中边界的数量和位置
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation](https://arxiv.org/abs/2411.03540)
  - **标题**: VLA-3D：用于3D语义场景的数据集理解和导航
  - **Filtered Reason**: none of cs.RO in whitelist
- [Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments](https://arxiv.org/abs/2411.02938)
  - **标题**: 共享和动态环境的多模式3D场景图形更新器
  - **Filtered Reason**: none of cs.RO in whitelist
- [RT-Grasp: Reasoning Tuning Robotic Grasping via Multi-modal Large Language Model](https://arxiv.org/abs/2411.05212)
  - **标题**: RT-Grasp：通过多模式大型语言模型进行推理调整机器人抓握
  - **Filtered Reason**: none of cs.RO in whitelist
- [STEM: Soft Tactile Electromagnetic Actuator for Virtual Environment Interactions](https://arxiv.org/abs/2411.05114)
  - **标题**: STEM：虚拟环境相互作用的软触觉电磁执行器
  - **Filtered Reason**: none of cs.HC in whitelist
- [A Comprehensive Review of Multimodal XR Applications, Risks, and Ethical Challenges in the Metaverse](https://arxiv.org/abs/2411.04508)
  - **标题**: 对元评估中多模式XR应用，风险和道德挑战的全面审查
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Visuotactile-Based Learning for Insertion with Compliant Hands](https://arxiv.org/abs/2411.06408)
  - **标题**: 基于视觉acti骨的学习，用于插入符合的手
  - **Filtered Reason**: none of cs.RO in whitelist
- [MIPD: A Multi-sensory Interactive Perception Dataset for Embodied Intelligent Driving](https://arxiv.org/abs/2411.05881)
  - **标题**: MIPD：用于具体智能驾驶的多感官互动感知数据集
  - **Filtered Reason**: none of cs.RO in whitelist
- [Fast Stochastic Subspace Identification of Densely Instrumented Bridges Using Randomized SVD](https://arxiv.org/abs/2411.05510)
  - **标题**: 使用随机SVD快速随机子空间鉴定密集的仪器桥
  - **Filtered Reason**: none of cs.CE in whitelist
- [Development of a Human-Robot Interaction Platform for Dual-Arm Robots Based on ROS and Multimodal Artificial Intelligence](https://arxiv.org/abs/2411.05342)
  - **标题**: 开发基于ROS和多模式人工智能的双臂机器人的人机交互平台
  - **Filtered Reason**: none of cs.RO in whitelist
- [Acoustic Volume Rendering for Neural Impulse Response Fields](https://arxiv.org/abs/2411.06307)
  - **标题**: 神经脉冲反应场的声量渲染
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [EROAS: 3D Efficient Reactive Obstacle Avoidance System for Autonomous Underwater Vehicles using 2.5D Forward-Looking Sonar](https://arxiv.org/abs/2411.05516)
  - **标题**: EROAS：使用2.5D前瞻性声纳
  - **Filtered Reason**: none of cs.RO in whitelist
- [High-Fidelity Cellular Network Control-Plane Traffic Generation without Domain Knowledge](https://arxiv.org/abs/2411.07345)
  - **标题**: 高保真蜂窝网络控制平面流量生成没有域知识
  - **Filtered Reason**: none of cs.NI in whitelist
- [AV-PedAware: Self-Supervised Audio-Visual Fusion for Dynamic Pedestrian Awareness](https://arxiv.org/abs/2411.06789)
  - **标题**: AV-PEDAWARE：动态行人意识的自我监管的视听融合
  - **Filtered Reason**: none of cs.RO in whitelist
- [Learning from Feedback: Semantic Enhancement for Object SLAM Using Foundation Models](https://arxiv.org/abs/2411.06752)
  - **标题**: 从反馈中学习：使用基础模型的对象猛击的语义增强
  - **Filtered Reason**: none of cs.RO in whitelist
- [SoundSil-DS: Deep Denoising and Segmentation of Sound-field Images with Silhouettes](https://arxiv.org/abs/2411.07517)
  - **标题**: Soundil-DS：用轮廓的深层降级和分割声场图像
  - **Filtered Reason**: none of cs.SD,eess.IV,physics.optics,eess.SP,eess.AS in whitelist
- [Integrating Chaotic Evolutionary and Local Search Techniques in Decision Space for Enhanced Evolutionary Multi-Objective Optimization](https://arxiv.org/abs/2411.07860)
  - **标题**: 在决策空间中整合混乱的进化和本地搜索技术，以增强进化多目标优化
  - **Filtered Reason**: none of cs.NE in whitelist
- [Generative AI in Self-Directed Learning: A Scoping Review](https://arxiv.org/abs/2411.07677)
  - **标题**: 自我指导学习中的生成AI：范围评论
  - **Filtered Reason**: none of cs.CY in whitelist
- [WristSonic: Enabling Fine-grained Hand-Face Interactions on Smartwatches Using Active Acoustic Sensing](https://arxiv.org/abs/2411.08217)
  - **标题**: 手腕：使用主动声感应在智能手表上实现精细颗粒的手面相互作用
  - **Filtered Reason**: none of cs.HC in whitelist
- [LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing Online Learners](https://arxiv.org/abs/2411.09873)
  - **标题**: LLM驱动的AI导师，具有D/聋和听力障碍的在线学习者的角色
  - **Filtered Reason**: none of cs.HC in whitelist
- [SimTube: Generating Simulated Video Comments through Multimodal AI and User Personas](https://arxiv.org/abs/2411.09577)
  - **标题**: Simtube：通过多模式AI和用户角色生成模拟的视频评论
  - **Filtered Reason**: none of cs.HC in whitelist
- [SmartInv: Multimodal Learning for Smart Contract Invariant Inference](https://arxiv.org/abs/2411.09217)
  - **标题**: SmartInv：智能合同不变推理的多模式学习
  - **Filtered Reason**: none of cs.CR,cs.PL,cs.SE in whitelist
- [ParaLBench: A Large-Scale Benchmark for Computational Paralinguistics over Acoustic Foundation Models](https://arxiv.org/abs/2411.09349)
  - **标题**: Paralbench：关于声学基础模型的计算副语言学的大规模基准
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [BlueME: Robust Underwater Robot-to-Robot Communication Using Compact Magnetoelectric Antennas](https://arxiv.org/abs/2411.09241)
  - **标题**: Blueme：使用紧凑的磁电天线的强大的水下机器人到机器人通信
  - **Filtered Reason**: none of eess.SP,cs.RO in whitelist
- [Noncontact Multi-Point Vital Sign Monitoring with mmWave MIMO Radar](https://arxiv.org/abs/2411.09201)
  - **标题**: 使用mmwave mimo雷达监测非接触性多点生命体征
  - **Filtered Reason**: none of eess.SP,cs.AR in whitelist
- [Optimization free control and ground force estimation with momentum observer for a multimodal legged aerial robot](https://arxiv.org/abs/2411.11216)
  - **标题**: 优化的自由控制和地面力量估计具有多模式的空中机器人的动量观察者
  - **Filtered Reason**: none of eess.SY,cs.RO in whitelist
- [SoK: Unifying Cybersecurity and Cybersafety of Multimodal Foundation Models with an Information Theory Approach](https://arxiv.org/abs/2411.11195)
  - **标题**: SOK：通过信息理论方法统一多模式基础模型的网络安全和网络安全
  - **Filtered Reason**: none of cs.CR in whitelist
- [D-Flow: Multi-modality Flow Matching for D-peptide Design](https://arxiv.org/abs/2411.10618)
  - **标题**: D-Flow：D肽设计的多模式流量匹配
  - **Filtered Reason**: none of cs.CE in whitelist
- [A Novel MLLM-based Approach for Autonomous Driving in Different Weather Conditions](https://arxiv.org/abs/2411.10603)
  - **标题**: 在不同天气条件下自动驾驶的一种新型基于MLLM的方法
  - **Filtered Reason**: none of eess.SY,cs.RO in whitelist
- ['What did the Robot do in my Absence?' Video Foundation Models to Enhance Intermittent Supervision](https://arxiv.org/abs/2411.10016)
  - **标题**: “在我缺席的情况下，机器人做了什么？”视频基础模型以增强间歇性监督
  - **Filtered Reason**: none of cs.RO in whitelist
- [Practitioner Paper: Decoding Intellectual Property: Acoustic and Magnetic Side-channel Attack on a 3D Printer](https://arxiv.org/abs/2411.10887)
  - **标题**: 从业者纸：解码知识产权：对3D打印机的声学和磁侧通道攻击
  - **Filtered Reason**: none of cs.CR in whitelist
- [NeuMaDiff: Neural Material Synthesis via Hyperdiffusion](https://arxiv.org/abs/2411.12015)
  - **标题**: Neumadiff：神经材料通过过度扩散综合
  - **Filtered Reason**: none of cs.GR in whitelist
- [InstruGen: Automatic Instruction Generation for Vision-and-Language Navigation Via Large Multimodal Models](https://arxiv.org/abs/2411.11394)
  - **标题**: Instrugen：通过大型多模型的视觉和语言导航的自动指导生成
  - **Filtered Reason**: none of cs.RO in whitelist
- [Wideband Ultrasonic Acoustic Underwater Channels: Measurements and Characterization](https://arxiv.org/abs/2411.11726)
  - **标题**: 宽带超声声音水下通道：测量和表征
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [Using voice analysis as an early indicator of risk for depression in young adults](https://arxiv.org/abs/2411.11541)
  - **标题**: 使用语音分析作为年轻人抑郁症风险的早期指标
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Validation of Tumbling Robot Dynamics with Posture Manipulation for Closed-Loop Heading Angle Control](https://arxiv.org/abs/2411.12970)
  - **标题**: 通过姿势操纵进行闭环角度控制的验证机器人动力学的验证
  - **Filtered Reason**: none of eess.SY,cs.RO in whitelist
- [I Can Tell What I am Doing: Toward Real-World Natural Language Grounding of Robot Experiences](https://arxiv.org/abs/2411.12960)
  - **标题**: 我可以说出我在做什么：建立机器人体验的现实世界自然语言基础
  - **Filtered Reason**: none of cs.RO in whitelist
- [Identifying patterns of proprioception and target matching acuity in healthy humans](https://arxiv.org/abs/2411.12664)
  - **标题**: 识别健康人的本体感受和目标匹配的模式
  - **Filtered Reason**: none of cs.RO in whitelist
- [I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial Perception](https://arxiv.org/abs/2411.13314)
  - **标题**: I2TT：图像指示的沉浸式文本到语音综合，并具有空间感知
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [PairSonic: Helping Groups Securely Exchange Contact Information](https://arxiv.org/abs/2411.13693)
  - **标题**: Pairsonic：帮助小组安全交换联系信息
  - **Filtered Reason**: none of cs.NI,cs.CR,cs.HC in whitelist
- [TrojanEdit: Backdooring Text-Based Image Editing Models](https://arxiv.org/abs/2411.14681)
  - **标题**: Trojanedit：基于后门的基于文本的图像编辑模型
  - **Filtered Reason**: none of cs.CR in whitelist
- [Conjugate momentum based thruster force estimate in dynamic multimodal robot](https://arxiv.org/abs/2411.14596)
  - **标题**: 基于共轭动量的推进器力估计在动态多模式机器人中
  - **Filtered Reason**: none of cs.RO in whitelist
- [Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by Harnessing AI](https://arxiv.org/abs/2411.14299)
  - **标题**: Masala-chai：通过利用AI的大规模香料网表数据集用于模拟电路
  - **Filtered Reason**: none of cs.AR in whitelist
- [Generalizing End-To-End Autonomous Driving In Real-World Environments Using Zero-Shot LLMs](https://arxiv.org/abs/2411.14256)
  - **标题**: 使用零击LLMS在现实环境中端到端自动驾驶概括
  - **Filtered Reason**: none of cs.RO in whitelist
- [Dehazing-aided Multi-Rate Multi-Modal Pose Estimation Framework for Mitigating Visual Disturbances in Extreme Underwater Domain](https://arxiv.org/abs/2411.13988)
  - **标题**: 除去的多率多模式姿势估计框架，用于减轻极端水下域中的视觉干扰
  - **Filtered Reason**: none of cs.RO in whitelist
- [SpikEmo: Enhancing Emotion Recognition With Spiking Temporal Dynamics in Conversations](https://arxiv.org/abs/2411.13917)
  - **标题**: Spikemo：通过对话中的时间动态增强情绪识别
  - **Filtered Reason**: none of cs.MM in whitelist
- [FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation](https://arxiv.org/abs/2411.15753)
  - **标题**: 泡沫：富含接触式机器人操作的力感知的反应性政策
  - **Filtered Reason**: none of cs.RO in whitelist
- [Robustifying Long-term Human-Robot Collaboration through a Multimodal and Hierarchical Framework](https://arxiv.org/abs/2411.15711)
  - **标题**: 通过多模式和层次结构框架来稳健地增强长期的人类机器人协作
  - **Filtered Reason**: none of cs.RO in whitelist
- [Medillustrator: Improving Retrospective Learning in Physicians' Continuous Medical Education via Multimodal Diagnostic Data Alignment and Representation](https://arxiv.org/abs/2411.15593)
  - **标题**: Medillustrator：通过多模式诊断数据一致性和表示，改善医师持续医学教育中的回顾性学习
  - **Filtered Reason**: none of cs.HC in whitelist
- [Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models](https://arxiv.org/abs/2411.15525)
  - **标题**: botfip-llm：增强的多模式科学计算框架利用大语模型的知识蒸馏
  - **Filtered Reason**: none of cs.SC in whitelist
- [MOANA: Multi-Objective Ant Nesting Algorithm for Optimization Problems](https://arxiv.org/abs/2411.15157)
  - **标题**: Moana：用于优化问题的多目标蚂蚁嵌套算法
  - **Filtered Reason**: none of cs.NE in whitelist
- [Learning-based Trajectory Tracking for Bird-inspired Flapping-Wing Robots](https://arxiv.org/abs/2411.15130)
  - **标题**: 基于学习的轨迹跟踪，用于鸟类风格的拍打翼机器人
  - **Filtered Reason**: none of eess.SY,cs.RO in whitelist
- [Aim My Robot: Precision Local Navigation to Any Object](https://arxiv.org/abs/2411.14770)
  - **标题**: 对准我的机器人：精密本地导航到任何对象
  - **Filtered Reason**: none of cs.RO in whitelist
- [FinML-Chain: A Blockchain-Integrated Dataset for Enhanced Financial Machine Learning](https://arxiv.org/abs/2411.16277)
  - **标题**: FINML链：一个区块链集成的数据集，用于增强金融机器学习
  - **Filtered Reason**: none of econ.GN,cs.CR,q-fin.CP,stat.ML,cs.CE in whitelist
- [Cross-modal Medical Image Generation Based on Pyramid Convolutional Attention Network](https://arxiv.org/abs/2411.17420)
  - **标题**: 基于金字塔卷积注意网络的跨模式医学图像生成
  - **Filtered Reason**: none of eess.IV,cs.CE in whitelist
- [TRIP: Terrain Traversability Mapping With Risk-Aware Prediction for Enhanced Online Quadrupedal Robot Navigation](https://arxiv.org/abs/2411.17134)
  - **标题**: 旅行：通过风险了解预测的地形遍历映射，用于增强的在线四足机器人导航
  - **Filtered Reason**: none of cs.RO in whitelist
- [DexGrip: Multi-modal Soft Gripper with Dexterous Grasping and In-hand Manipulation Capacity](https://arxiv.org/abs/2411.17124)
  - **标题**: Dexgrip：具有灵巧的抓握和手持操作能力的多模式软抓手
  - **Filtered Reason**: none of cs.RO in whitelist
- [BESTAnP: Bi-Step Efficient and Statistically Optimal Estimator for Acoustic-n-Point Problem](https://arxiv.org/abs/2411.17521)
  - **标题**: BESTANP：声性和统计学上的Bi-Step效率和统计最佳估计量
  - **Filtered Reason**: none of cs.RO in whitelist
- [DuetML: Human-LLM Collaborative Machine Learning Framework for Non-Expert Users](https://arxiv.org/abs/2411.18908)
  - **标题**: Duetml：非专家用户的人类合作机器学习框架
  - **Filtered Reason**: none of cs.HC in whitelist
- [EEG-Based Analysis of Brain Responses in Multi-Modal Human-Robot Interaction: Modulating Engagement](https://arxiv.org/abs/2411.18587)
  - **标题**: 基于EEG的多模式人类机器人相互作用中大脑反应的分析：调节参与度
  - **Filtered Reason**: none of eess.SP,q-bio.NC,cs.HC in whitelist
- [At First Contact: Stiffness Estimation Using Vibrational Information for Prosthetic Grasp Modulation](https://arxiv.org/abs/2411.18507)
  - **标题**: 首先接触：使用振动信息进行假肢调制的刚度估算
  - **Filtered Reason**: none of cs.RO in whitelist
