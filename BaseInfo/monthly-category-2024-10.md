# 2024-10 月度论文分类汇总

共有769篇相关领域论文, 另有74篇其他

## 太阳和恒星天体物理学(astro-ph.SR:Solar and Stellar Astrophysics)

该领域共有 2 篇论文

### A Foundation Model for the Solar Dynamics Observatory 
[[arxiv](https://arxiv.org/abs/2410.02530)] [[cool](https://papers.cool/arxiv/2410.02530)] [[pdf](https://arxiv.org/pdf/2410.02530)]
> **Authors**: James Walsh,Daniel G. Gass,Raul Ramos Pollan,Paul J. Wright,Richard Galvez,Noah Kasmanoff,Jason Naradowsky,Anne Spalding,James Parr,Atılım Güneş Baydin
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: 太阳动力学天文台的基础模型
- **领域**: 太阳和恒星天体物理学,计算机视觉和模式识别
- **摘要**: SDO-FM是使用NASA太阳能天文台（SDO）航天器的数据的基础模型；集成三种独立的仪器，以将太阳的复杂物理相互作用封装到多模式嵌入空间中。该模型可用于简化涉及SDO的科学研究，通过使庞大的数据集更加计算机上用于热物理研究，并启用需要仪器融合的研究。我们讨论了四个关键组件：摄入管道，以创建机器学习准备就绪数据集，模型体系结构和培训方法，结果嵌入和微调模型，最后是下游微调应用程序。这项工作的关键组成部分是在开发的每个阶段都包括主题专家。审查科学价值并为模型架构，数据集和培训范式决策提供指导。本文标志着我们验证的模型和嵌入数据集的发布，该数据集可供社区提供有关Hugging Face和Sdofm.org的信息。

### Multimodal Flare Forecasting with Deep Learning 
[[arxiv](https://arxiv.org/abs/2410.16116)] [[cool](https://papers.cool/arxiv/2410.16116)] [[pdf](https://arxiv.org/pdf/2410.16116)]
> **Authors**: Grégoire Francisco,Sabrina Guastavino,Teresa Barata,João Fernandes,Dario Del Moro
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 深度学习的多模式耀斑预测
- **领域**: 太阳和恒星天体物理学,天体物理学仪器和方法,人工智能,计算机视觉和模式识别
- **摘要**: 太阳耀斑预测主要依赖于光电图和相关的物理特征来预测即将发生的耀斑。然而，据信耀斑起始机制通常起源于染色体和下电晕。在这项研究中，我们将深度学习用作纯粹的数据驱动方法，以比较不同波长的色球和冠状紫外线和EUV排放的预测能力与光泽镜磁力图的预测能力。我们的发现表明，单个EUV波长可以提供与视线磁力图相当或更好的歧视功率。此外，我们确定了一个简单的多模式神经网络体系结构，这些架构始终超过单输入模型，显示了可以从太阳大气的不同层中提取的耀斑前体之间的互补性。为了减轻活跃区域耀斑目录中已知错误归因的潜在偏差，使用全盘图像和全盘级别的全面火炬事件目录对我们的模型进行了训练和评估。我们介绍了一个深入学习的架构，适合从全盘视频中提取时间功能。

## 材料科学(cond-mat.mtrl-sci:Materials Science)

该领域共有 2 篇论文

### Rapid and Automated Alloy Design with Graph Neural Network-Powered LLM-Driven Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2410.13768)] [[cool](https://papers.cool/arxiv/2410.13768)] [[pdf](https://arxiv.org/pdf/2410.13768)]
> **Authors**: Alireza Ghafarollahi,Markus J. Buehler
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,无序系统和神经网络,介观和纳米物理,人工智能,多代理系统
- **摘要**: 多代理AI模型用于自动化新的金属合金的发现，集成了多模式数据和外部知识，包括通过原子模拟从物理学的见解。我们的多代理系统具有三个关键组件：（a）一套负责推理和计划等任务的LLM套件，（b）具有动态协作的一组具有独特角色和专业知识的AI代理，以及（c）新开发的图形神经网络（GNN）模型，以快速检索关键物理属性的快速检索。一组由LLM驱动的AI代理合作，以自动化MPEA的巨大设计空间的探索，并在GNN的预测下进行。我们专注于使用基于ML的原子质电位建模的NBMOTA家族合金（BCC）合金，并靶向两个关键特性：PEIERLS屏障和溶质/螺钉脱位相互作用能量。我们的GNN模型准确地预测了这些原子级特性，为昂贵的蛮力计算提供了更快的替代方法，并减轻了物理检索多代理系统的计算负担。该AI系统通过减少对人类专业知识的依赖并克服直接全原子模拟的局限性来彻底改变材料的发现。通过通过基于LLM的代理的动态协作协同，该系统自动浏览庞大的合金设计空间，识别原子尺度材料属性的趋势并预测宏观机械强度的趋势，如多个计算实验所证明。这种方法加速了高级合金的发现，并有望在其他复杂系统中进行更广泛的应用，这标志着自动化材料设计的重要一步。

### Interpretable Multimodal Machine Learning Analysis of X-ray Absorption Near-Edge Spectra and Pair Distribution Functions 
[[arxiv](https://arxiv.org/abs/2410.17467)] [[cool](https://papers.cool/arxiv/2410.17467)] [[pdf](https://arxiv.org/pdf/2410.17467)]
> **Authors**: Tanaporn Na Narong,Zoe N. Zachko,Steven B. Torrisi,Simon J. L. Billinge
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: No comments
- **标题**: X射线吸收近边光谱和配对分布功能的X射线吸收的可解释的多模式机器学习分析
- **领域**: 材料科学,机器学习
- **摘要**: 我们使用可解释的机器学习将来自多个异质光谱的信息结合在一起：X射线吸收近边光谱（XANES）和原子对分布函数（PDFS），以在氧化物中提取过渡金属阳离子的局部结构和化学环境。对随机森林模型进行了模拟的XANE，PDF的训练，并且都合并以提取氧化态，协调数和平均最接近的邻居键长。尽管使用金属的差异PDF（DPDF）而不是总PDF，但仅XANES模型即使在结构任务上也通常优于仅PDF模型，即使是结构任务的模型也缩小了这一间隙。当与PDF结合使用时，来自XANES的信息通常主导预测。我们的结果表明，Xanes包含丰富的结构信息，并突出了物种特异性的实用性。这种可解释的多模式方法很快就可以与合适的数据库实施，并为不同方式的相对优势提供了宝贵的见解，指导研究人员实验设计并确定互补技术在结合互补技术时为科学研究增加了有意义的信息。

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 55 篇论文

### BabelBench: An Omni Benchmark for Code-Driven Analysis of Multimodal and Multistructured Data 
[[arxiv](https://arxiv.org/abs/2410.00773)] [[cool](https://papers.cool/arxiv/2410.00773)] [[pdf](https://arxiv.org/pdf/2410.00773)]
> **Authors**: Xuwu Wang,Qiwen Cui,Yunzhe Tao,Yiran Wang,Ziwei Chai,Xiaotian Han,Boyi Liu,Jianbo Yuan,Jing Su,Guoyin Wang,Tingkai Liu,Liyu Chen,Tianyi Liu,Tao Sun,Yufeng Zhang,Sirui Zheng,Quanzeng You,Yang Yang,Hongxia Yang
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: Babelbench：用于代码驱动的多模式和多结构数据的OMNI基准
- **领域**: 人工智能,计算语言学
- **摘要**: 大型语言模型（LLM）在各个领域中变得越来越关键，尤其是在处理复杂的数据类型方面。这包括结构化数据处理，如ChartQa和ChatGpt-ADA所示，以及在视觉询问答案（VQA）中看到的多模式的非结构化数据处理。这些领域引起了行业和学术界的极大关注。尽管如此，对于这些不同的数据处理方案仍缺乏统一的评估方法。作为回应，我们介绍了Babelbench，这是一个创新的基准框架，该框架评估了LLMS在管理多模式多结构数据中使用代码执行的熟练程度。 Babelbench结合了一个数据集，其中包括247个精心策划的问题，这些问题通过感知，常识性推理，逻辑推理等方面的任务来挑战模型。除了多模式理解，结构化数据处理以及代码生成的基本功能外，这些任务还需要探索，计划，推理和调试的高级功能。我们在Babelbench上的实验发现表明，即使像Chatgpt 4这样的尖端模型也有大量改进的空间。我们的全面分析得出的见解为社区内的未来研究提供了宝贵的指导。可以在https://github.com/ffd8ffe/babelbench上找到基准数据。

### Multimodal Auto Validation For Self-Refinement in Web Agents 
[[arxiv](https://arxiv.org/abs/2410.00689)] [[cool](https://papers.cool/arxiv/2410.00689)] [[pdf](https://arxiv.org/pdf/2410.00689)]
> **Authors**: Ruhana Azam,Tamer Abuelsaad,Aditya Vempaty,Ashish Jagmohan
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: Web代理中自我翻新的多模式自动验证
- **领域**: 人工智能,软件工程
- **摘要**: 随着我们的世界数字化，可以自动化复杂和单调任务的网络代理在简化工作流程中变得至关重要。本文介绍了一种通过多模式验证和自我进行提高Web代理性能的方法。我们介绍了一项关于不同模式（文本，愿景）的全面研究以及层次结构对Web代理自动验证的影响，并在最先进的代理-E Web自动化框架上。我们还使用已开发的自动validator引入了一种自动进行自动化的自发机制，该机制使Web代理能够检测和自我校正工作流量故障。我们的结果表明，Agent-E（SOTA Web代理）先前的最先进绩效上有很大的收益，在WebVoyager基准的子集中将任务完成率从76.2 \％提高到81.24 \％。本文提出的方法为在复杂的现实世界情景中更可靠的数字助理铺平了道路。

### ReXplain: Translating Radiology into Patient-Friendly Video Reports 
[[arxiv](https://arxiv.org/abs/2410.00441)] [[cool](https://papers.cool/arxiv/2410.00441)] [[pdf](https://arxiv.org/pdf/2410.00441)]
> **Authors**: Luyang Luo,Jenanan Vairavamurthy,Xiaoman Zhang,Abhinav Kumar,Ramon R. Ter-Oganesyan,Stuart T. Schroff,Dan Shilo,Rydhwana Hossain,Mike Moritz,Pranav Rajpurkar
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: 12 pages. The project page is https://www.rajpurkarlab.hms.harvard.edu/rexplain
- **标题**: 解释：将放射学转化为患者友好的视频报告
- **领域**: 人工智能,图像和视频处理
- **摘要**: 放射学报告是为了在医学专家之间有效沟通而设计的，通常对患者仍然无法理解。这种无法获得的性可能会导致焦虑，减少治疗决策的参与以及较差的健康结果，从而破坏以患者为中心的护理。我们提出Rexplain（放射学说明），这是一种创新的AI驱动系统，将放射学发现转化为患者友好的视频报告。 Rexplain唯一地集成了用于医学文本简化和文本解剖关联的大型语言模型，一种用于解剖区域识别的图像分割模型，以及用于吸引界面可视化的头像生成工具。 Rexplain可以以视频报告的形式使用简单的语言，突出显示的图像和3D器官渲染产生全面的解释。为了评估Rexplain生成的解释的效用，我们从六位经过董事会认证的放射科医生那里进行了两轮用户反馈收集。这项概念证明研究的结果表明，Rexplain可以准确地提供放射学信息并有效地模拟一对一的咨询，从而阐明了通过潜在的临床用法增强以患者为中心的放射学。这项工作展示了AI辅助医学沟通的新范式，有可能改善放射学护理中患者的参与度和满意度，并为多模式医学通信研究开辟了新的途径。

### Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2410.00255)] [[cool](https://papers.cool/arxiv/2410.00255)] [[pdf](https://arxiv.org/pdf/2410.00255)]
> **Authors**: Weitai Kang,Haifeng Huang,Yuzhang Shang,Mubarak Shah,Yan Yan
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: 8 pages
- **标题**: ROBIN3D：通过强大的说明调整改进3D大语言模型
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 3D大语言模型（3DLLM）的最新进展强调了它们在3D现实世界中构建通用药物的潜力，但由于缺乏高质量的可靠指导跟踪数据，挑战仍然存在，从而导致歧视性有限和3DLLM的概括。在本文中，我们介绍了Robin3D，这是一种强大的3DLLM，该3DLLM在我们的新型数据引擎（ROBSUST TENSTRINION INSERTION（RIG）发动机）生成的大规模指令中培训。钻机生成了两个关键的指令数据：1）遵循的对抗指令数据，该数据具有混合的负和阳性样品，以增强模型的歧视性理解。 2）包含各种指令样式以增强模型的概括的方式，其中包含各种指令数据。结果，我们构建了100万个指导遵循的数据，包括344K对抗样本，508K多种样本和165K基准训练套件。为了更好地处理这些复杂的说明，Robin3D首先结合了关系增强的投影仪，以增强空间理解，然后通过ID-Feature Bonding增强对象参考和接地能力。 Robin3d始终优于五个广泛使用的3D多模式学习基准的先前方法，而无需特定于任务的微调。值得注意的是，我们在接地任务（Multi3DRefer）方面取得了7.8％的改进，字幕任务（SCAN2CAP）的改进为6.9 \％。

### From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities 
[[arxiv](https://arxiv.org/abs/2410.02155)] [[cool](https://papers.cool/arxiv/2410.02155)] [[pdf](https://arxiv.org/pdf/2410.02155)]
> **Authors**: Wanpeng Zhang,Zilong Xie,Yicheng Feng,Yijiang Li,Xingrun Xing,Sipeng Zheng,Zongqing Lu
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 从像素到代币：字节对编码量化的视觉方式
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式的大型语言模型在整合视觉和文本信息方面取得了重大进步，但是他们经常在有效地对齐这些方式方面挣扎。我们引入了一种新型的图像令牌，该图像令牌通过将字节对编码（BPE）的原理应用于视觉数据来弥合此差距。与依靠单独的视觉编码器的常规方法不同，我们的方法将结构性先验信息直接纳入图像令牌，从而反映了仅在文本大型语言模型中使用的成功的令牌化策略。这种创新的方法使变压器模型能够更有效地学习和理性。通过理论分析和广泛的实验，我们证明了我们的BPE图像令牌显着增强了MLLM的多模式理解能力，即使训练数据有限。利用这种方法，我们开发了VL-0，该模型在各种基准测试中表现出卓越的性能，并显示出有希望的可伸缩性，并有可能为更高效，更有能力的多模式基础模型铺平道路。

### Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics 
[[arxiv](https://arxiv.org/abs/2410.02026)] [[cool](https://papers.cool/arxiv/2410.02026)] [[pdf](https://arxiv.org/pdf/2410.02026)]
> **Authors**: Yuan Zhou,Peng Zhang,Mengya Song,Alice Zheng,Yiwen Lu,Zhiheng Liu,Yong Chen,Zhaohan Xi
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 十二生肖：心脏病专家级LLM多代理诊断框架
- **领域**: 人工智能,计算语言学
- **摘要**: 大型语言模型（LLM）在医疗保健方面表现出了显着的进步。但是，关于LLMS在特定领域的临床实践中的专业精神的差距仍然存在很大的差距，从而限制了其在现实世界诊断中的应用。在这项工作中，我们介绍了Zodiac，这是一个具有LLM驱动的框架，具有心脏病专家级专业精神，旨在使LLMS参与心脏病诊断。十二生肖通过从患者数据中提取临床相关的特征，检测严重的心律不齐，并生成初步报告，以供心脏病专家进行审查和改进。为了获得心脏病专家级的专业精神，十二生肖建立在多代理协作框架的基础上，从而使能够跨多种方式处理患者数据。每个LLM代理都使用心脏病专家裁定的现实世界患者数据进行微调，从而增强了模型的专业精神。十二生肖对独立心脏病专家进行了严格的临床验证，对八个指标进行了评估，这些指标衡量了临床有效性并解决了安全问题。结果表明，十二生肖优于行业领先的模型，包括OpenAI的GPT-4O，Meta的Llama-3.1-405B和Google的Gemini-Pro，以及Microsoft的Microsoft Miogpt等医学专家LLM。 Zodiac通过提供满足严格的医疗实践需求的领域特异性解决方案，展示了专业LLM在医疗保健中的变革潜力。值得注意的是，十二生肖已成功整合到心电图（ECG）设备中，体现了将LLM嵌入软件 -  AS-Medical-Device（SAMD）的增长趋势。

### Image First or Text First? Optimising the Sequencing of Modalities in Large Language Model Prompting and Reasoning Tasks 
[[arxiv](https://arxiv.org/abs/2410.03062)] [[cool](https://papers.cool/arxiv/2410.03062)] [[pdf](https://arxiv.org/pdf/2410.03062)]
> **Authors**: Grant Wardle,Teo Susnjak
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: 首先图像还是文字？优化大语言模型中的模式的测序提示和推理任务
- **领域**: 人工智能
- **摘要**: 本文研究了多模式提示中图像和文本的测序如何影响大语言模型（LLMS）的推理性能。我们使用三个商业LLM进行了经验评估。我们的结果表明，出现方式的顺序可以显着影响性能，尤其是在不同复杂性的任务中。对于涉及单个图像的更简单任务，模态测序对准确性有明显的影响。但是，在涉及多个图像和复杂的推理步骤的更复杂的任务中，测序的效果减少了，这可能是由于任务的认知需求增加所致。我们的发现还突出了问题/及时结构的重要性。在嵌套和多步推理任务中，模态测序在塑造模型性能中起着关键作用。尽管LLM在推理的最初阶段表现出色，但他们努力地重新组合了早期的信息，强调了变压器体系结构中多跳推理的挑战。这表明，将模态的顺序与推理步骤的逻辑流相一致比单独的模态顺序更为关键。这些见解为改善多模式及时设计的有价值的影响提供了宝贵的含义，并在教育，医学成像和跨模式学习等领域进行了更广泛的应用。

### Plots Unlock Time-Series Understanding in Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.02637)] [[cool](https://papers.cool/arxiv/2410.02637)] [[pdf](https://arxiv.org/pdf/2410.02637)]
> **Authors**: Mayank Daswani,Mathias M. J. Bellaiche,Marc Wilson,Desislav Ivanov,Mikhail Papkov,Eva Schnider,Jing Tang,Kay Lamerigts,Gabriela Botea,Michael A. Sanchez,Yojan Patel,Shruthi Prabhakara,Shravya Shetty,Umesh Telang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: 57 pages
- **标题**: 情节在多模型中解锁时间序列的理解
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 尽管多模式的基础模型现在可以与文本超出文本的数据生存合作，但它们在分析医疗保健，金融和社会科学等领域的大量多维时序数据方面仍然不足，这代表了富裕，数据驱动的见解的错过机会。本文提出了一种简单但有效的方法，该方法利用这些模型的现有视觉编码来通过图“查看”时间序列数据，从而避免了需要额外的，可能成本高昂的模型培训。我们的经验评估表明，这种方法的表现优于提供原始的时间序列数据作为文本，其额外好处是，视觉时间序列表示的模型API成本降低了90％。我们通过增加复杂性的综合数据任务来验证我们的假设，从清洁数据上的简单功能形式识别到从嘈杂的散点图中提取趋势。为了从具有明确的推理步骤到更复杂的现实世界情景的综合任务中证明概括性，我们将方法应用于消费者健康任务（特别是掉落检测，活动识别和准备就绪评估） - 涉及异质性，嘈杂的数据和多步推理。在GPT和GEMINI模型系列中，零摄像的合成任务上的整体成功与文本性能相比（在零射合合成任务上的性能提高了120％，在现实世界任务上的性能提高），强调了我们的方法可以充分利用基础模型的本机能力。

### End-to-end Driving in High-Interaction Traffic Scenarios with Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2410.02253)] [[cool](https://papers.cool/arxiv/2410.02253)] [[pdf](https://arxiv.org/pdf/2410.02253)]
> **Authors**: Yueyuan Li,Mingyang Jiang,Songan Zhang,Wei Yuan,Chunxiang Wang,Ming Yang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: 10 pages, 3 figures, experiment under progress, only to demonstrate the originality of the method
- **标题**: 通过加强学习的高度交流交通情况的端到端驾驶
- **领域**: 人工智能,机器学习,机器人技术
- **摘要**: 动态和交互式交通情况对自动驾驶系统构成了重大挑战。强化学习（RL）通过探索驾驶政策，超出了预采用的数据集和预定义条件的限制，尤其是在复杂的环境中，提供了一种有希望的方法。但是，一个关键的挑战在于有效地从高维，多模式观察的序列中提取空间和时间特征，同时最大程度地减少了随着时间的推移误差的积累。此外，在训练过程中，有效指导大规模的RL模型在没有频繁失败的情况下收敛于最佳驾驶策略，这仍然很棘手。我们提出了一种基于端到端模型的RL算法，旨在解决这些问题。漫步过程将多视图的RGB图像和LiDar Point云到低维的潜在特征，以在每个时间步骤中捕获流量场景的上下文。然后，采用基于变压器的架构来建模时间依赖性并预测未来状态。通过学习环境的动态模型，Ramble可以预见到即将举行的交通事件，并做出更明智的战略决策。我们的实施表明，先前在功能提取和决策方面的经验在加速RL模型向最佳驾驶政策的融合方面起着关键作用。 Ramble在Carla排行榜2.0的路线完成率和驾驶得分方面取得了最先进的表现，展示了其在管理复杂而动态的交通状况方面的有效性。

### DAMMI:Daily Activities in a Psychologically Annotated Multi-Modal IoT dataset 
[[arxiv](https://arxiv.org/abs/2410.04152)] [[cool](https://papers.cool/arxiv/2410.04152)] [[pdf](https://arxiv.org/pdf/2410.04152)]
> **Authors**: Mohsen Falah Rad,Kamrad Khoshhal Roudposhti,Mohammad Hassan Khoobkar,Mohsen Shirali,Zahra Ahmadi,Carlos Fernandez-Llatas
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: 14 pages
- **标题**: DAMMI：心理注释的多模式IOT数据集中的日常活动
- **领域**: 人工智能
- **摘要**: 老年人口的增长和金字塔的变化增加了对医疗保健和福祉服务的需求。为了解决这一问题，除了医疗服务的成本上升外，在医疗和技术解决方案的最新进展中，出现了家里衰老的概念。计算机科学，通信技术和医疗保健方面的专家通过在生活环境，可穿戴设备和智能手机中使用传感器，与高级数据挖掘和具有学习能力的智能系统合作，通过学习能力，监控，分析和预测老年人的健康状况，以开发负担得起的健康解决方案。但是，实施智能医疗保健系统并开发分析技术需要对现实世界数据进行测试和评估算法。尽管需要，但仍有满足这些要求的公开数据集缺乏。为了解决这一差距，我们在这项工作中介绍了DAMMI数据集，旨在支持该领域的研究人员。该数据集包括通过家庭安装的传感器，智能手机数据和腕带收集的老年人的日常活动数据，超过146天。它还包含心理学家团队提供的日常心理报告。此外，数据收集涵盖了重大事件，例如Covid-19大流行，新年假期和斋月宗教月份，提供了更多的分析机会。在本文中，我们概述了有关数据收集系统，记录的数据类型以及预处理的事件日志的详细信息。该数据集旨在协助物联网和数据挖掘的专业人员评估和实施其研究思想。

### Gamified crowd-sourcing of high-quality data for visual fine-tuning 
[[arxiv](https://arxiv.org/abs/2410.04038)] [[cool](https://papers.cool/arxiv/2410.04038)] [[pdf](https://arxiv.org/pdf/2410.04038)]
> **Authors**: Shashank Yadav,Rohan Tomar,Garvit Jain,Chirag Ahooja,Shubham Chaudhary,Charles Elkan
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 高质量数据的游戏化众包用于视觉微调
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 本文介绍了游戏化对抗提示（GAP），这是一个框架，该框架可为大型多峰模型的视觉指导调整高质量数据。 GAP将数据收集过程转换为引人入胜的游戏，激励玩家提供针对模型知识中差距的细粒度，具有挑战性的问题和答案。我们的贡献包括（1）一种直接解决模型知识中的弱点的问题解答对的方法，（2）一种评估和奖励玩家的方法，该方法成功激励了他们以提供高质量的提交，以及（3）在几周内从50,000个参与者中收集这些数据的可扩展，游戏化平台。我们的差距的实施显着提高了小型多模型模型的准确性，即Minicpm-llama3-V-2.5-8B，将其GPT得分从我们的数据集中的0.147提高到0.477，接近由大量较大的GPT-4V设定的基准。此外，我们证明了使用minicpm-llama3-v-2.5-8b生成的数据也提高了其在其他基准测试中的性能，并展示了跨模型的好处。具体而言，相同的数据改善了在相同多个基准测试上的QWEN2-VL-2B和QWEN2-VL-7B的性能。

### A Unified Framework for Motion Reasoning and Generation in Human Interaction 
[[arxiv](https://arxiv.org/abs/2410.05628)] [[cool](https://papers.cool/arxiv/2410.05628)] [[pdf](https://arxiv.org/pdf/2410.05628)]
> **Authors**: Jeongeun Park,Sungjoon Choi,Sangdoo Yun
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: https://vim-motion-language.github.io/
- **标题**: 人类互动中运动推理和产生的统一框架
- **领域**: 人工智能
- **摘要**: 大型语言模型（LLM）的最新进展已大大提高了它们产生自然和上下文相关文本的能力，从而实现了更类似人类的AI相互作用。然而，由于对这些相互作用进行建模的复杂性，产生和理解多个人参与协调运动的人类互动运动的运动仍然具有挑战性。此外，还需要一个统一的通用模型来处理各种交互式场景，例如动态适应用户指令和分配角色的聊天系统。为了应对这些挑战，我们介绍了VIM，这是一种多功能的交互式运动语言模型，该模型同时集成了语言和运动方式，以在多转交流环境中有效理解，生成和控制交互运动。与以前主要关注单向任务（例如文本到动作或运动对文本）的研究不同，VIM采用了统一的体系结构，能够同时理解和产生运动和文本方式。鉴于没有适当的数据集来支持此任务，我们引入了Inter-Mt2，这是一个大规模指令调用数据集，其中包含82.7k多转交互式运动指令，涵盖153K交互式运动样本。 Inter-MT2涵盖了各种教学方案，包括运动编辑，问答和故事的产生，利用现成的大型语言模型和运动扩散模型来构建一系列的交互式运动指令。我们广泛评估了VIM在多个交互式运动相关任务中的多功能性，包括运动对文本，文本到动作，反应产生，运动编辑以及有关运动序列的推理。

### Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents 
[[arxiv](https://arxiv.org/abs/2410.05243)] [[cool](https://papers.cool/arxiv/2410.05243)] [[pdf](https://arxiv.org/pdf/2410.05243)]
> **Authors**: Boyu Gou,Ruohan Wang,Boyuan Zheng,Yanan Xie,Cheng Chang,Yiheng Shu,Huan Sun,Yu Su
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Accepted to ICLR 2025 (Oral)
- **标题**: 像人类一样浏览数字世界：GUI代理的通用视觉基础
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）正在转换图形用户界面（GUI）代理的功能，从而促进了它们从受控仿真到各个平台上复杂的现实世界应用程序的过渡。但是，这些代理的有效性取决于其接地能力的鲁棒性。当前的GUI代理主要利用基于文本的表示，例如HTML或可访问性树，尽管它们的实用性通常会引入噪音，不完整性和增加的计算开销。在本文中，我们主张为GUI代理的类似人类的实施例，该体现完全在视觉上感知环境并直接在GUI上进行像素级操作。关键是视觉接地模型，可以准确地将各种GUI元素的表达式映射到跨不同平台上的GUI上的坐标。我们表明，一个简单的食谱包括基于Web的合成数据和LLAVA架构的轻微适应，对于训练此类视觉接地模型非常有效。到目前为止，我们收集了最大的GUI视觉接地数据集，其中包含10m GUI元素及其超过130万屏幕截图的表达式，并使用它来训练Uground，这是GUI代理的强大通用视觉接地模型。跨越三类基准的实证结果（接地，离线代理和在线代理）表明，1）Uground在高达20％的绝对值中大大优于GUI代理的现有视觉接地模型，而2）具有Ugground Enterprofer Enforment Entermant Enthart Agratents的代理，尽管现有代理商仅使用了其他基于文本的输入，但它仅使用了其他基于文本的输入。这些结果为GUI代理人的可行性和承诺提供了强有力的支持，这些GUI代理像人类一样在数字世界中浏览。

### Does Spatial Cognition Emerge in Frontier Models? 
[[arxiv](https://arxiv.org/abs/2410.06468)] [[cool](https://papers.cool/arxiv/2410.06468)] [[pdf](https://arxiv.org/pdf/2410.06468)]
> **Authors**: Santhosh Kumar Ramakrishnan,Erik Wijmans,Philipp Kraehenbuehl,Vladlen Koltun
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 边境模型中是否出现空间认知？
- **领域**: 人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 还没有。我们提出空间，这是一个系统地评估边境模型中空间认知的基准。我们的基准基于认知科学的数十年研究。它评估了当生物体穿越物理环境，对物体形状和布局的较小规模的推理以及认知基础架构（如空间注意力和记忆）时，它会带来的大规模映射能力。对于许多任务，我们通过文本和图像实例化并行演示，使我们能够基准大型语言模型和大型多模式模型。结果表明，当代边境模型未达到动物的空间智能，在许多经典的动物认知测试中进行了接近机会水平。

### Multimodal Situational Safety 
[[arxiv](https://arxiv.org/abs/2410.06172)] [[cool](https://papers.cool/arxiv/2410.06172)] [[pdf](https://arxiv.org/pdf/2410.06172)]
> **Authors**: Kaiwen Zhou,Chengzhi Liu,Xuandong Zhao,Anderson Compalas,Dawn Song,Xin Eric Wang
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 多模式情境安全
- **领域**: 人工智能,计算语言学
- **摘要**: 多模式大语言模型（MLLM）正在迅速发展，表现出令人印象深刻的能力，作为与人类及其环境相互作用的多模式助手。但是，这种提高的复杂性引入了重大的安全问题。在本文中，我们介绍了称为多模式的新型安全挑战的首次评估和分析，该挑战探讨了安全考虑如何根据用户或代理参与的特定情况而变化。我们认为，要使MLLM安全地响应，无论是通过语言还是动作，它通常都需要评估语言查询在相应的视觉上下文中的安全含义。为了评估这种能力，我们开发了多模式的情境安全基准（MSSBench），以评估当前MLLM的情境安全性能。数据集包含1,820个语言查询图像对，其中一半的图像上下文是安全的，另一半是不安全的。我们还开发了一个评估框架，该框架分析了关键的安全方面，包括明确的安全推理，视觉理解以及至关重要的情况，即情境安全推理。我们的发现表明，当前的MLLM在遵循教学的设置中与这个细微的安全问题斗争，并努力立即应对这些情况安全挑战，这突出了未来研究的关键领域。此外，我们开发了多代理管道来协调解决安全挑战，这比原始的MLLM响应显示出安全性的一致性提高。代码和数据：mssbench.github.io。

### Bottom-up Anytime Discovery of Generalised Multimodal Graph Patterns for Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2410.05839)] [[cool](https://papers.cool/arxiv/2410.05839)] [[pdf](https://arxiv.org/pdf/2410.05839)]
> **Authors**: Xander Wilcke,Rick Mourits,Auke Rijpma,Richard Zijdeman
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: :68T10ACM Class:I.5.1
- **标题**: 自下而上，任何时间都会发现知识图的广义多模式图模式
- **领域**: 人工智能,数据库
- **摘要**: 大量的异质知识以知识图的形式公开获得，通常将多个从未在一起的数据来源联系起来，从而使学者能够回答许多新的研究问题。但是，通常不知道数据可能会有答案，这可能会留下许多有趣且新颖的见解，以保持未被发现。为了支持学者在这个科学工作流程中，我们引入了一种随时随地的算法，以自下而上发现知识图中的广义多模式图模式。每种模式都是二进制语句与（数据 - ）类型变量，常数和/或值模式的结合。发现后，模式将转换为SPARQL查询，并在交互式浏览器中介绍，以及元数据和出处信息，使学者可以探索，分析和共享查询。我们从用户的角度评估我们的方法，并在人文科学领域专家的帮助下。

### COMMA: A Communicative Multimodal Multi-Agent Benchmark 
[[arxiv](https://arxiv.org/abs/2410.07553)] [[cool](https://papers.cool/arxiv/2410.07553)] [[pdf](https://arxiv.org/pdf/2410.07553)]
> **Authors**: Timothy Ossowski,Jixuan Chen,Danyal Maqbool,Zefan Cai,Tyler Bradshaw,Junjie Hu
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 逗号：交流多模式的多模式基准
- **领域**: 人工智能
- **摘要**: 建立在大型基础模型上的多模式代理的快速进步在很大程度上忽略了它们在协作任务中基于语言的沟通的潜力。这种疏忽在理解其在现实世界部署中的有效性方面尤其是在与人类沟通时的关键差距。现有的代理基准无法解决代理间沟通和协作的关键方面，尤其是在代理商无法获得信息不平等的情况下，必须共同努力以实现超出个人功能范围的任务。为了填补这一空白，我们介绍了一种新颖的基准测试，旨在通过语言交流评估多模式多代理系统的协作性能。我们的基准有各种方案，在交流协作环境中对四个关键的代理能力进行了全面评估。通过使用开源和封闭源模型测试代理商和代理人人类的协作，我们的发现显示了最新模型中的令人惊讶的弱点，包括GPT-4O等专有模型。这些模型中的一些努力甚至在代理商协作中超过一个简单的随机代理基线，并且只有在涉及人类时才超过随机基线。

### The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks 
[[arxiv](https://arxiv.org/abs/2410.07391)] [[cool](https://papers.cool/arxiv/2410.07391)] [[pdf](https://arxiv.org/pdf/2410.07391)]
> **Authors**: Isaac R. Galatzer-Levy,David Munday,Jed McGiffin,Xin Liu,Danny Karmon,Ilia Labzovsky,Rivka Moroshko,Amir Zait,Daniel McDuff
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 生成AI的认知能力：与人类基准的比较分析
- **领域**: 人工智能
- **摘要**: 追踪通用情报基础模型的功能的兴趣越来越大。这项研究基准了针对人类成人智力量表（WAIS-IV）的大型语言模型和视觉语言模型，这是对潜在的人类认知和智力能力的全面，人口统计的评估，重点介绍了语言综合性（VCI）（VCI），工作记忆（WMI）和感知理由（PRI（PRI））。大多数模型在存储，检索和操纵方面表现出了特殊的功能，例如任意字母和数字序列，与人口规范能力相比，工作记忆指数（WMI）的性能大于或等于第99.5个百分位。言语理解指数（VCI）的表现，该指数衡量了获取信息的检索以及对单词含义及其彼此关系的语言理解，在第98个百分位数中也表现出一致的表现。尽管有这些广泛的优势，但我们从多模型模型中观察到的感知推理指数（PRI；范围为0.1-10％）的性能一直持续差，表明无法解释和理性的视觉信息。较小和旧的模型版本始终如一地执行较差，表明训练数据，参数计数和调整的进步导致认知能力的显着进步。

### InstructG2I: Synthesizing Images from Multimodal Attributed Graphs 
[[arxiv](https://arxiv.org/abs/2410.07157)] [[cool](https://papers.cool/arxiv/2410.07157)] [[pdf](https://arxiv.org/pdf/2410.07157)]
> **Authors**: Bowen Jin,Ziqi Pang,Bingjun Guo,Yu-Xiong Wang,Jiaxuan You,Jiawei Han
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: 16 pages
- **标题**: 指令2i：从多模式属性图合成图像
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,机器学习,社交和信息网络
- **摘要**: 在本文中，我们接触了一个被忽略但关键的任务图2IMAGE：从多模式归因图（MMAGS）生成图像。由于图形大小的爆炸，图形实体之间的依赖性以及在图条件下需要可控性，因此此任务构成了重大挑战。为了应对这些挑战，我们提出了一个称为Constrictg2i的图表条件条件的扩散模型。 ConstructG2i首先利用图形结构和多模式信息来通过组合个性化页面级别并根据视觉语言特征进行重新排列来进行信息丰富的邻居采样。然后，图形Qumerer编码器将图节节点自适应地编码为一组图形提示，以指导扩散过程。最后，我们提出了无图形分类器的指导，通过改变图形指导的强度和多个连接的边缘来实现可控制的生成。在来自不同领域的三个数据集上进行的广泛实验证明了我们方法的有效性和可控性。该代码可在https://github.com/petergriffinjin/instructg2i上找到。

### Agent S: An Open Agentic Framework that Uses Computers Like a Human 
[[arxiv](https://arxiv.org/abs/2410.08164)] [[cool](https://papers.cool/arxiv/2410.08164)] [[pdf](https://arxiv.org/pdf/2410.08164)]
> **Authors**: Saaket Agashe,Jiuzhou Han,Shuyu Gan,Jiachen Yang,Ang Li,Xin Eric Wang
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: 23 pages, 16 figures, 9 tables
- **标题**: 代理S：使用像人类这样的计算机的开放代理框架
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 我们提出了代理S，这是一个开放的代理框架，可以通过图形用户界面（GUI）实现与计算机的自主交互，旨在通过自动化复杂的多步任务来转换人类计算机的交互。 Agent S的目的是应对自动化计算机任务的三个关键挑战：获取特定于域的知识，在长期任务范围内进行计划以及处理动态，不均匀的接口。为此，Agent S介绍了经验丰富的层次结构计划，该规划从外部知识搜索和内部经验以多个级别的方式学习，从而促进了有效的任务计划和子任务。此外，它使用代理计算机界面（ACI）更好地基于多模式大语模型（MLLM）更好地引起GUI代理的推理和控制能力。 OSWORLD基准的评估表明，代理S的成功率（相对改善83.6％）优于基线9.37％，并实现了新的最先进。全面的分析强调了各个组件的有效性，并为未来的改进提供了见解。此外，Agent S在新发行的Windowsagentarena基准测试中证明了对不同操作系统的广泛概括性。代码可在https://github.com/simular-ai/agent-s上找到。

### MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2410.09453)] [[cool](https://papers.cool/arxiv/2410.09453)] [[pdf](https://arxiv.org/pdf/2410.09453)]
> **Authors**: Xi Jiang,Jian Li,Hanqiu Deng,Yong Liu,Bin-Bin Gao,Yifeng Zhou,Jialin Li,Chengjie Wang,Feng Zheng
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: Accepted by ICLR 2025. The code and data are available at https://github.com/jam-cc/MMAD
- **标题**: MMAD：工业异常检测中多模式大语言模型的全面基准
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 在工业检查领域，多模式大语言模型（MLLM）由于其强大的语言能力和概括能力而具有较高的实际应用范式的潜力。但是，尽管在许多领域中具有令人印象深刻的解决问题的技能，但MLLM在工业异常检测中的能力尚未系统地研究。为了弥合这一差距，我们提出了MMAD，这是工业异常检测中有史以来首个全光谱MLLMS基准。我们在工业检查中定义了MLLM的七个关键子任务，并设计了一条新颖的管道，以生成MMAD数据集，其中39,672个问题用于8,366个工业图像。使用MMAD，我们对各种最新MLLM进行了全面的定量评估。商业模型表现最好，GPT-4O型号的平均准确性达到74.9％。但是，这一结果远远远没有工业要求。我们的分析表明，当前的MLLM在回答与工业异常和缺陷有关的问题方面仍然有很大的改进空间。我们进一步探索了两种无培训的绩效增强策略，以帮助模型改善工业场景，突出了它们未来研究的潜力。

### Declarative Knowledge Distillation from Large Language Models for Visual Question Answering Datasets 
[[arxiv](https://arxiv.org/abs/2410.09428)] [[cool](https://papers.cool/arxiv/2410.09428)] [[pdf](https://arxiv.org/pdf/2410.09428)]
> **Authors**: Thomas Eiter,Jan Hadl,Nelson Higuera,Johannes Oetsch
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)
- **标题**: 来自大型语言模型的声明知识蒸馏回答数据集
- **领域**: 人工智能,计算语言学
- **摘要**: 视觉问题回答（VQA）是回答有关图像的问题，并需要处理多模式输入和推理以获取答案。在推理组件中使用声明性表示的模块化解决方案比端到端训练的系统在解释性方面具有明显的优势。不利的一面是，制定此类组件的规则可能是开发人员的额外负担。我们通过提出一种从大语言模型（LLM）中提出一种声明性知识蒸馏的方法来应对这一挑战。我们的方法是提示LLM扩展有关VQA推理的初始理论，作为答案集程序，以满足VQA任务的要求。 VQA数据集中的示例用于指导LLM，验证结果并通过使用ASP求解器的反馈来正确修补规则。我们证明了我们的方法在著名的CLEVR和GQA数据集上起作用。我们的结果证实，除了数据驱动的规则学习方法外，从LLM中提取知识实际上是一个有前途的方向。

### Baichuan-Omni Technical Report 
[[arxiv](https://arxiv.org/abs/2410.08565)] [[cool](https://papers.cool/arxiv/2410.08565)] [[pdf](https://arxiv.org/pdf/2410.08565)]
> **Authors**: Yadong Li,Haoze Sun,Mingan Lin,Tianpeng Li,Guosheng Dong,Tao Zhang,Bowen Ding,Wei Song,Zhenglin Cheng,Yuqi Huo,Song Chen,Xu Li,Da Pan,Shusen Zhang,Xin Wu,Zheng Liang,Jun Liu,Tao Zhang,Keer Lu,Yaqi Zhao,Yanjun Shen,Fan Yang,Kaicheng Yu,Tao Lin,Jianhua Xu, et al. (2 additional authors not shown)
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: Baichuan-Omni技术报告
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: GPT-4O的显着多模式功能和互动体验突出了其在实际应用中的关键作用，但缺乏表现出色的开源对应物。在本文中，我们介绍了Baichuan-Omni，这是第一个开源7B多模式大型语言模型（MLLM），同时处理和分析图像，视频，音频和文本的方式，同时提供先进的多模式互动体验和强大的性能和强大的性能。我们提出了一个有效的多模式训练架构，从7B模型开始，然后通过音频，图像，视频和文本模态的多模式对齐和多任务进行微调阶段。这种方法使语言模型具有有效处理视觉和音频数据的能力。在各种Omni-Modal和多模式基准测试中表现出强劲的性能，我们的目标是为开源社区提供竞争性基线，以促进多模式理解和实时互动。

### Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2410.10735)] [[cool](https://papers.cool/arxiv/2410.10735)] [[pdf](https://arxiv.org/pdf/2410.10735)]
> **Authors**: Kuofeng Gao,Huanqia Cai,Qingyao Shuai,Dihong Gong,Zhifeng Li
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 将自我纠正嵌入大语模型中的固有能力，以增强数学推理
- **领域**: 人工智能,计算语言学
- **摘要**: 大型语言模型（LLM）的准确数学推理对于革命性依赖这种推理的革命领域至关重要。但是，LLM在数学推理的某些方面经常遇到困难，从而导致理由和错误的结果。为了减轻这些问题，我们介绍了一种新颖的机制，即自校正链（COSC），该链被设计为将自我纠正嵌入LLMS中的固有能力，使它们能够验证和纠正自己的结果。 COSC机制通过一系列自我纠正阶段运行。在每个阶段，LLMS生成一个解决给定问题的程序，使用基于程序的工具获得输出，然后验证此输出。根据验证，LLMS要么进入下一个更正阶段或最终确定答案。这种迭代的自我纠正过程使LLM可以完善其推理步骤并提高其数学推理的准确性。我们使用两阶段微调方法实施COSC。首先，LLMS经过培训的，该LLM的播种数据相对较少，从GPT-4产生。然后，我们通过使用大量的自我生成数据训练来增强COSC，而无需依赖GPT-4。实验表明，与现有的开源LLM相比，COSC显着提高了标准数学数据集的性能。值得注意的是，我们的COSC-CODE-34B模型在具有挑战性的数学数据集上获得了53.5％的分数，胜过ChatGPT，GPT-4和多模式LLM，例如GPT-4V和Gemini-1.0。重要的是，COSC以零拍的方式运行，而无需示范。

### Context-Enhanced Multi-View Trajectory Representation Learning: Bridging the Gap through Self-Supervised Models 
[[arxiv](https://arxiv.org/abs/2410.13196)] [[cool](https://papers.cool/arxiv/2410.13196)] [[pdf](https://arxiv.org/pdf/2410.13196)]
> **Authors**: Tangwen Qian,Junhe Li,Yile Chen,Gao Cong,Tao Sun,Fei Wang,Yongjun Xu
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 上下文增强的多视图轨迹表示学习：通过自我监督模型弥合差距
- **领域**: 人工智能,机器学习
- **摘要**: 具有通用用途密度表示的建模轨迹数据已成为各种下游应用的普遍范式，例如轨迹分类，旅行时间估计和相似性计算。但是，现有方法通常依赖于单个空间视图的轨迹，从而限制了它们捕获丰富的上下文信息的能力，这些信息对于获得对不同地理空间环境中运动模式的更深入了解至关重要。为此，我们提出了MVTRAJ，这是一种用于轨迹表示学习的新型多视图建模方法。 MVTRAJ整合了从GP到道路网络和利益点的各种上下文知识，以提供对轨迹数据的更全面的理解。为了使学习过程跨越多种视图，我们利用GPS轨迹作为桥梁，并采用自我监督的借口任务来捕获和区分不同空间视图的运动模式。之后，我们将轨迹从不同的观点视为不同的模态，并应用层次结构的跨模式相互作用模块来融合表示形式，从而丰富了从多种来源得出的知识。对现实世界数据集的广泛实验表明，MVTRAJ在与各种空间视图相关的任务中的现有基准显着优于现有基准，从而验证了其在时空建模中的有效性和实用性。

### PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking 
[[arxiv](https://arxiv.org/abs/2410.12375)] [[cool](https://papers.cool/arxiv/2410.12375)] [[pdf](https://arxiv.org/pdf/2410.12375)]
> **Authors**: Markus J. Buehler
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 预手机：基于偏好的递归语言建模，用于推理和代理思维的探索性优化
- **领域**: 人工智能,无序系统和神经网络,介观和纳米物理,材料科学,计算语言学
- **摘要**: 预手机（用于推理探索性优化的基于偏好的递归语言建模）将偏好优化与从强化学习到使模型通过迭代推理改进能够进行自学的概念相结合。我们提出了一种递归学习方法，该方法将模型参与多步推理，重新审视和完善中间步骤，然后再产生训练和推理阶段的最终输出。通过多个训练阶段，该模型首先学会通过优化优先响应和非优先响应之间的对数赔率来使其推理与准确的决策路径保持一致。在此过程中，Preflexor通过产生从随机文本块和检索仪器来构建动态知识图，从而使整个培训语料库的相关细节上下文相关细节。在第二阶段，偏好优化通过使用拒绝采样来通过不断产生原位训练数据来掩盖推理步骤，从而增强了模型性能。思维令牌框架内的递归优化引入了迭代反馈循环，该模型可以完善推理，实现更深的连贯性，一致性和适应性。在只有30亿个参数的小语言模型中实施，即使是微小的模型也可以迭代地教自己以更深入和反射率来推理。我们的实施很简单，可以将其纳入任何现有的验证LLM中。我们将示例集中在生物材料科学中的应用上，并在各种案例研究中演示了从内域到跨域应用的方法。使用包括思考和反思方式的推理策略，我们构建了一种多代理递归的自我提出推理方法，通过在推理时间中重复采样来依次改善响应。

### OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities 
[[arxiv](https://arxiv.org/abs/2410.12219)] [[cool](https://papers.cool/arxiv/2410.12219)] [[pdf](https://arxiv.org/pdf/2410.12219)]
> **Authors**: Lichang Chen,Hexiang Hu,Mingda Zhang,Yiwen Chen,Zifeng Wang,Yandong Li,Pranav Shyam,Tianyi Zhou,Heng Huang,Ming-Hsuan Yang,Boqing Gong
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: 19 pages, 6 figures, 12 tables
- **标题**: Omnixr：评估OMNI模式语言模型有关跨模式的推理
- **领域**: 人工智能,计算语言学,多媒体
- **摘要**: 我们介绍了Omnixr，这是一个评估套件，旨在基准基准SOTA OMNI模式语言模型，例如GPT-4O和Gemini。评估整合多种模式（例如文本，视觉和音频）的OLMS提出了独特的挑战。特别是，用户消息通常包括多种模式，以便OLM必须建立跨模态的整体理解和推理才能完成任务。现有的基准测试仅限于单个模式或双模式任务，从而忽略了模型推理的全面多模式评估。为了解决这个问题，Omnixr提供了两个评估变体：（1）合成子集：通过将文本转换为多种模态（Audio，图像，视频和混合体）自动生成的合成数据集（Omnify）。 （2）现实的子集：专家手动策划和注释的现实世界数据集，用于评估自然设置中的跨模式推理。 Omnixr提出了一种独特的评估，以评估OLM在各种方式中，例如涉及视频，音频和文本的问题，提供了严格的跨模式推理测试台，与任何现有基准不同。我们的实验发现，所有最新的OLMS都在争夺Omnixr问题，这些问题需要整合来自多种方式的信息才能回答。进一步的分析强调了推理行为的差异，强调了Omni-Modal AI对齐的挑战。

### Utilizing Large Language Models for Event Deconstruction to Enhance Multimodal Aspect-Based Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2410.14150)] [[cool](https://papers.cool/arxiv/2410.14150)] [[pdf](https://arxiv.org/pdf/2410.14150)]
> **Authors**: Xiaoyong Huang,Heli Sun,Qunshu Gao,Wenjie Huang,Ruichen Cao
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 利用大型语言模型进行事件解构来增强基于多模式的情感分析
- **领域**: 人工智能,计算语言学
- **摘要**: 随着互联网的快速发展，用户生成的内容量的丰富性增加了，这使得基于多模式的情感分析（MABSA）成为研究热点。现有的研究在MABSA中取得了某些结果，但它们尚未有效地解决了多个实体和情感共存的方案中的分析挑战。本文创新地介绍了用于事件分解的大型语言模型（LLMS），并为多模式的基于方面的情感分析（MABSA-RL）框架提出了强化学习框架。该框架使用LLMS将原始文本分解为一组事件，从而降低了分析的复杂性，从而引入了增强学习以优化模型参数。实验结果表明，在两个基准数据集上，MABSA-RL优于现有的高级方法。本文提供了一种新的研究观点和多模式方面情感分析的方法。

### A Pattern to Align Them All: Integrating Different Modalities to Define Multi-Modal Entities 
[[arxiv](https://arxiv.org/abs/2410.13803)] [[cool](https://papers.cool/arxiv/2410.13803)] [[pdf](https://arxiv.org/pdf/2410.13803)]
> **Authors**: Gianluca Apriceno,Valentina Tamma,Tania Bailoni,Jacopo de Berardinis,Mauro Dragoni
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 20 pages, 6 figures
- **标题**: 将它们对齐的模式：整合不同的模态以定义多模式实体
- **领域**: 人工智能
- **摘要**: 与不同的感官输入进行推理和整合的能力是基础人类智能的基础，这是对在知识图中建模多模式信息的兴趣日益兴趣的原因。多模式知识图通过将实体与其可能的模态表示（包括文本，图像，音频和视频）相关联来扩展传统知识图，所有这些都用于传达实体的语义。尽管越来越关注多模式知识图已经收到的，但关于模式的定义和建模缺乏共识，其定义通常由应用域确定。在本文中，我们提出了一种新颖的本体设计模式，该模式捕获了实体（以及它传达的信息）之间关注的分离，其语义可以在不同的媒体上具有不同的表现，并在物理信息实体方面实现了其实现。通过引入该抽象模型，我们旨在促进与不同的多模式本体论的统一和整合，这对于从医学到数字人文科学的不同领域的许多智能应用至关重要。

### MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures 
[[arxiv](https://arxiv.org/abs/2410.13754)] [[cool](https://papers.cool/arxiv/2410.13754)] [[pdf](https://arxiv.org/pdf/2410.13754)]
> **Authors**: Jinjie Ni,Yifan Song,Deepanway Ghosal,Bo Li,David Junhao Zhang,Xiang Yue,Fuzhao Xue,Zian Zheng,Kaichen Zhang,Mahir Shah,Kabir Jain,Yang You,Michael Shieh
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: MixeVal-X：来自现实世界数据混合物的任何一对一评估
- **领域**: 人工智能,机器学习,多媒体
- **摘要**: 感知和产生多种方式对于AI模型有效地学习和互动现实信号至关重要，因此需要对其发展进行可靠的评估。我们在当前评估中确定了两个主要问题：（1）不一致的标准，由不同协议和成熟度的不同社区塑造； （2）明显的查询，分级和泛化偏见。为了解决这些问题，我们介绍了MixeVal-X，这是第一个对任何对任何现实世界的基准测试，旨在优化和标准化各种输入和输出模式的评估。我们提出了多模式的基准混合物和适应性调整管道，以重建现实世界的任务分布，从而确保评估有效地推广到现实世界中的用例。广泛的元评估表明，我们的方法有效地将基准样品与现实世界任务分布保持一致。同时，MixeVal-X的模型排名与众群体的现实世界评估（高达0.98）密切相关，同时更加有效。我们为Rerank现有模型和组织提供全面的排行榜，并提供见解，以增强对多模式评估的理解并为未来的研究提供信息。

### SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation 
[[arxiv](https://arxiv.org/abs/2410.15164)] [[cool](https://papers.cool/arxiv/2410.15164)] [[pdf](https://arxiv.org/pdf/2410.15164)]
> **Authors**: Jingxuan Chen,Derek Yuen,Bin Xie,Yuhao Yang,Gongwei Chen,Zhihao Wu,Li Yixing,Xurui Zhou,Weiwen Liu,Shuai Wang,Kaiwen Zhou,Rui Shao,Liqiang Nie,Yasheng Wang,Jianye Hao,Jun Wang,Kun Shao
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: ICLR 2025 Spotlight
- **标题**: 水疗基础：智能手机代理评估的全面基准
- **领域**: 人工智能
- **摘要**: 智能手机代理对于用户（多模式）大语言模型（MLLM）的方法作为关键竞争者而越来越重要，以帮助用户有效地控制设备。相当比较这些代理是必不可少的，但具有挑战性，需要各种任务范围，具有不同实现的代理的整合以及可评估其优势和劣势的可普遍评估管道。在本文中，我们提出了Spa-b Ench，这是一种全面的智能手机代理基准测试，旨在在模拟现实世界中的交互环境中评估（M）基于LLM的代理。 SPA-B ENCH提供了三个关键贡献：（1）涵盖英语和中文的第三方应用程序的各种任务，重点介绍了日常工作中常用的功能； （2）一个插件框架，启用了与Android设备的实时代理相互作用，使十多个代理集成了添加更多的代理； （3）一条新的评估管道，自动评估跨多个维度的代理性能，其中包括与任务完成和资源消耗有关的七个指标。我们跨任务和代理商进行的广泛实验揭示了挑战，例如解释移动用户界面，操作接地，记忆保留和执行成本。我们建议未来的研究方向缓解这些困难，更接近现实世界智能手机代理应用程序。 SPA-B ENCH可在https://ai-agent-2030.github.io/spa-bench/上找到。

### BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation 
[[arxiv](https://arxiv.org/abs/2410.14971)] [[cool](https://papers.cool/arxiv/2410.14971)] [[pdf](https://arxiv.org/pdf/2410.14971)]
> **Authors**: Jilong Li,Zhenxi Song,Jiaqi Wang,Min Zhang,Zhiguo Zhang
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: brainecho：语义大脑信号通过矢量定量的频谱图解码，用于耳语增强文本生成
- **领域**: 人工智能,计算语言学,声音,音频和语音处理
- **摘要**: 从大脑信号（EEG和MEG）解码语言的最新进展已受到预先训练的语言模型的显着驱动，从而在公开可用的无侵性脑电图/MEG数据集上取得了显着进展。但是，先前的作品主要利用教师在文本生成期间的强迫，导致绩效大幅下降而无需使用。一个基本问题是无法建立统一的特征空间将文本数据与相应的诱发大脑信号相关联。尽管最近的一些研究试图使用音频文本预训练的模型Whisper来减轻这一差距，但由于其信号输入模式而受到青睐，但它们仍然很大程度上忽略了直接应用Whisper来解码大脑信号的音频信号和大脑信号之间的固有差异。为了解决这些局限性，我们提出了一种新的多阶段策略，用于通过矢量定量的频谱图重建，以解码语言，以解码耳语增强文本生成，称为brainecho。具体而言，brainecho依次进行：1）音频谱图的离散自动编码； 2）大脑原告潜在空间对齐； 3）语义文本生成通过耳语登录。通过这种自动编码 - 对准过程，在两个广泛接受的资源上，BraineCho在相同数据拆分设置下优于最先进的方法：EEG数据集（Brennan）和MEG DataSet（Gwilliams）。 Brainecho的创新以及其在公共数据集的句子，会话和主题无关的级别上的稳健性和优越性，强调了其对基于语言的脑部计算机接口的重要性。

### Polymath: A Challenging Multi-modal Mathematical Reasoning Benchmark 
[[arxiv](https://arxiv.org/abs/2410.14702)] [[cool](https://papers.cool/arxiv/2410.14702)] [[pdf](https://arxiv.org/pdf/2410.14702)]
> **Authors**: Himanshu Gupta,Shreyas Verma,Ujjwala Anantheswaran,Kevin Scaria,Mihir Parmar,Swaroop Mishra,Chitta Baral
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-21
> **comment**: 49 pages, (10 pages paper, 9 pages references, 30 pages appendix)
- **标题**: Polymath：具有挑战性的多模式数学推理基准
- **领域**: 人工智能,计算语言学
- **摘要**: 多模式的大语言模型（MLLM）在各个领域都表现出令人印象深刻的解决问题的能力，但是它们的视觉理解和抽象推理技能仍然不足。为此，我们提出了Polymath，这是一个具有挑战性的基准，旨在评估MLLM的一般认知推理能力。 Polymath包括5,000个手动收集的10个不同类别的认知文本和视觉挑战的高质量图像，包括模式识别，空间推理和相对推理。我们使用四种不同的提示策略（包括思想链和后卫）对15 MLLM进行了全面的定量评估。 Claude-3.5十四行诗，GPT-4O和Gemini-1.5分别获得了〜41％，〜36％和〜27％，分别为〜41％，〜36％和27％，分别为〜41％，〜36％和27％ - 突出了这些问题的逻辑和视觉复杂性。进一步的细粒错误分析表明，这些模型难以理解空间关系并执行引人入胜的高级推理。我们的消融研究估计MLLM性能代替图表，从而进一步加强了这一点。与文本描述相比，与实际图像相比，大约4％的改进证明了这一点，我们发现模型并未真正理解视觉图和其中的空间信息，因此很容易出现逻辑错误。最后，我们评估了OpenAI O1模型，发现它们的性能仅与人类基线相匹配，从而强调了基准的难度。 Polymath的结果突出了改进多模式推理的空间，并提供了独特的见解来指导未来MLLM的发展。

### MCSFF: Multi-modal Consistency and Specificity Fusion Framework for Entity Alignment 
[[arxiv](https://arxiv.org/abs/2410.14584)] [[cool](https://papers.cool/arxiv/2410.14584)] [[pdf](https://arxiv.org/pdf/2410.14584)]
> **Authors**: Wei Ai,Wen Deng,Hongyi Chen,Jiayi Du,Tao Meng,Yuntao Shou
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: 6 pages, 1 figures
- **标题**: MCSFF：实体对齐的多模式一致性和特异性融合框架
- **领域**: 人工智能
- **摘要**: 多模式实体对齐（MMEA）对于增强知识图和改善信息检索和提问系统至关重要。现有的方法通常集中于通过互补性整合方式，但忽略每种方式的特异性，这可能会掩盖关键特征并降低对齐精度。为了解决这个问题，我们提出了多模式的一致性和特异性融合框架（MCSFF），该框架是创新地整合了模态的互补和特定方面。我们利用规模计算的超融合基础架构在大规模数据处理中优化IT管理和资源分配。我们的框架首先使用模态嵌入来计算每种方式的相似性矩阵，以保留其独特的特征。然后，一种迭代更新方法降低并增强了模态特征，以完全表达关键信息。最后，我们整合了来自所有模式的更新信息，以创建丰富而精确的实体表示。实验表明，我们的方法优于MMKG数据集上的当前最新MMEA基准，这表明其有效性和实践潜力。

### Large Body Language Models 
[[arxiv](https://arxiv.org/abs/2410.16533)] [[cool](https://papers.cool/arxiv/2410.16533)] [[pdf](https://arxiv.org/pdf/2410.16533)]
> **Authors**: Saif Punjwani,Larry Heck
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 大型肢体语言模型
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 随着虚拟代理在人类计算机的互动中变得越来越普遍，实时产生现实且上下文适当的手势仍然是一个重大挑战。尽管神经渲染技术在静态脚本方面取得了长足的进步，但它们对人类计算机相互作用的适用性仍然有限。为了解决这个问题，我们介绍了大型肢体语言模型（LBLM），并介绍LBLM-AVA，这是一种新颖的LBLM体系结构，将变形金刚-XL大语言模型与并行扩散模型相结合，以从多模态输入（文本，音频和视频）中产生类似人类的手势。 LBLM-AVA结合了几个关键组件，可增强其手势产生能力，例如多模式对置式嵌入，通过重新定义注意力机制增强的序列到序列映射，一种用于手势序列相干性的时间平滑模块，以及基于注意力的细化模块，以增强现实主义。该模型经过我们的大规模专有开源数据集Allo-Ava的培训。 LBLM-AVA在产生寿命和上下文适当的手势方面取得了最先进的表现，与现有方法相比，Fréchet手势距离（FGD）降低了30％，而FréchetInception距离提高了25％。

### Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for Allocentric Avatar Gesture Animation 
[[arxiv](https://arxiv.org/abs/2410.16503)] [[cool](https://papers.cool/arxiv/2410.16503)] [[pdf](https://arxiv.org/pdf/2410.16503)]
> **Authors**: Saif Punjwani,Larry Heck
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: Allo-ava：一个大规模的多模式会话AI数据集
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 高质量，多模式训练数据的稀缺性极大地阻碍了在虚拟环境中为对话型AI的栩栩如生的化身动画的创建。现有的数据集通常缺乏言语，面部表情和体现自然交流的身体运动之间的复杂同步。为了解决这个关键的差距，我们介绍了Allo-Ava，这是一个专门为文本和音频驱动的化身手势动画而设计的大规模数据集，该数据集中是在中心（第三人称观察点）上下文中。 Allo-ava由$ \ sim $ 1,250小时的不同视频内容组成，并配有音频，成绩单和提取的关键点。 Allo-ava独特地将这些关键点映射到精确的时间戳，从而可以准确地复制人类运动（身体和面部手势）与语音同步。这种综合资源可以开发和评估更自然的，背景感知的头像动画模型，从虚拟现实到数字​​助手，可能会改变应用程序。

### Towards a Reliable Offline Personal AI Assistant for Long Duration Spaceflight 
[[arxiv](https://arxiv.org/abs/2410.16397)] [[cool](https://papers.cool/arxiv/2410.16397)] [[pdf](https://arxiv.org/pdf/2410.16397)]
> **Authors**: Oliver Bensch,Leonie Bensch,Tommy Nilsson,Florian Saling,Wafa M. Sadri,Carsten Hartmann,Tobias Hecking,J. Nathan Kutz
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: 75th International Astronautical Congress (IAC), Milan, Italy, 14-18 October 2024
- **标题**: 长时间迈向可靠的离线个人AI助手
- **领域**: 人工智能,新兴技术
- **摘要**: 当人类为月球和火星的新任务做准备时，鉴于沟通延迟使地球的实时支持变得困难，宇航员将需要以更大的自主权运作。例如，火星和地球之间的消息最多可能需要24分钟，这使得不可能快速回复。对于必须依靠现场工具来访问航天器传感器，流浪者和卫星的大量数据的宇航员，这种限制对宇航员构成了挑战，这些数据通常是零散且难以使用的。为了弥合这一差距，正在开发诸如火星探索遥测驱动的信息系统（METIS）之类的系统。 METIS是一名AI助手，旨在处理常规任务，监视航天器系统并检测异常，同时减少对任务控制的依赖。当前的生成预审预测的变压器（GPT）模型，虽然在安全至关重要的环境中进行了强大的斗争。它们可以产生合理但不正确的反应，这一现象称为“幻觉”，可能会危害宇航员。为了克服这些局限性，本文提出了通过整合GPT，检索授课的生成（RAG），知识图（KGS）和增强现实（AR）来增强METIS等系统。这个想法是允许宇航员使用自然语言查询和通过AR可视化实时信息，以更直观地与数据进行交互。 KGS将用于轻松访问实时遥测和多模式数据，以确保宇航员在正确的时间拥有正确的信息。通过将AI，KGS和AR相结合，该新系统将使宇航员能够在未来的太空任务中更自主，安全，有效地工作。

### MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays, ECGs, and Diagnostic Report 
[[arxiv](https://arxiv.org/abs/2410.16239)] [[cool](https://papers.cool/arxiv/2410.16239)] [[pdf](https://arxiv.org/pdf/2410.16239)]
> **Authors**: Samrajya Thapa,Koushik Howlader,Subhankar Bhattacharjee,Wei le
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: 10 pages, 5 figures, 9 tables. Supplementary detail in Appendix. Code made available in Github for reproducibility
- **标题**: 更多：在X射线，ECG和诊断报告上与变压器的多模式对比预训练
- **领域**: 人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 在本文中，我们介绍了一个新型的多模式对比前训练框架，该框架协同结合了X射线，心电图（ECG）和放射学/心脏病学报告。我们的方法利用变压器将这些不同的模式编码为统一的表示空间，旨在提高诊断准确性并促进全面的患者评估。我们利用Lora-Peft可以显着减少LLM中的可训练参数，并在视觉变压器（VIT）中纳入最新的线性注意策略，以使注意力更平滑。此外，我们为模型提供了新颖的多模式注意解释和检索。据我们所知，我们是第一个提出一个集成模型的人，将X射线，ECG和放射学/心脏病学报告与这种方法相结合。通过利用对比度损失，更有效地将特定于模态特征的特征与连贯的嵌入在一起，该特征支持各种下游任务，例如零照片分类和多模式检索。通过我们提出的方法，我们在MIMIC-IV，CHEXPERT，水肿严重性和PTBXL下游数据集上实现了最新的（SOTA），超过了现有的多模式方法。我们提出的框架在捕获复杂的模式间关系及其在医学诊断中的鲁棒性方面显示出显着改善，这为医疗保健领域多模式学习的未来研究建立了框架。

### Multi-Sensor Fusion for UAV Classification Based on Feature Maps of Image and Radar Data 
[[arxiv](https://arxiv.org/abs/2410.16089)] [[cool](https://papers.cool/arxiv/2410.16089)] [[pdf](https://arxiv.org/pdf/2410.16089)]
> **Authors**: Nikos Sakellariou,Antonios Lalas,Konstantinos Votis,Dimitrios Tzovaras
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: 10 pages, 6 figures
- **标题**: 基于图像和雷达数据的特征图，用于无人机分类的多传感器融合
- **领域**: 人工智能,信号处理
- **摘要**: 现代无人机的独特成本，灵活性，速度和效率使它们在当代社会的许多应用中成为有吸引力的选择。然而，这会导致越来越多的报告的恶意或意外事件，因此需要开发无人机检测和分类机制。我们提出了一种开发一种系统的方法，该系统将已经处理的多传感器数据融合到新的深神经网络中，以提高其对无人机检测的分类准确性。 DNN模型融合了从与热，optronic和雷达数据相关的单个对象检测和分类模型中提取的高级特征。此外，强调基于模型的卷积神经网络（CNN）结构，该体系结构通过堆叠与单独单独的每个传感器相比，可实现更高分类精度的三个传感器模式的特征，从而结合了三种传感器模式的特征。

### How to Build a Pre-trained Multimodal model for Simultaneously Chatting and Decision-making? 
[[arxiv](https://arxiv.org/abs/2410.15885)] [[cool](https://papers.cool/arxiv/2410.15885)] [[pdf](https://arxiv.org/pdf/2410.15885)]
> **Authors**: Zuojin Tang,Bin Hu,Chenyang Zhao,De Ma,Gang Pan,Bin Liu
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 如何构建预先训练的多模型模型以同时聊天和决策？
- **领域**: 人工智能
- **摘要**: 现有的大型预训练模型通常以端到端的方式将文本输入映射到文本输出，例如chatgpt，或将文本输入段映射到操作决策的层次结构，例如OpenVLA。但是，在接收特定输入信号时，人类可以同时生成文本和动作。例如，驾驶员可以在乘客座位上与朋友交谈时做出精确的驾驶决策。在这项工作中，我们考虑了以下问题：是否可以构建可以在动态开放场景中提供语言互动和精确决策能力的预训练模型。我们通过开发一个新的模型体系结构，称为“视觉语言动作”模型（VLA4CD），并进一步证明其在挑战自动驾驶任务中的表现，从而为这个问题提供了确定的答案。具体来说，我们利用洛拉（Lora）来微调预先训练的LLM，并具有涵盖语言，视觉和动作的多种模式的数据。与LLM微调的现有LORA操作不同，我们为VLA4CD设计了新的计算模块和培训成本功能。这些设计使VLA4CD能够在输出文本响应时提供连续值的动作决策。相反，现有的LLM只能输出文本响应，而当前的VLA模型只能输出动作决策。此外，这些VLA模型通过离散化然后对离散的动作进行象征来处理动作数据，这是一种不适用于复杂的决策任务的方法，该任务涉及涉及高维连续值的动作向量，例如自动驾驶。卡拉的实验结果验证了：（1）我们提出的模型构建方法是有效的； （2）与SOTA VLA模型相比，VLA4CD可以在保留LLMS固有的文本相互作用能力的同时提供更准确的实时决策。

### Voice-Enabled AI Agents can Perform Common Scams 
[[arxiv](https://arxiv.org/abs/2410.15650)] [[cool](https://papers.cool/arxiv/2410.15650)] [[pdf](https://arxiv.org/pdf/2410.15650)]
> **Authors**: Richard Fang,Dylan Bowman,Daniel Kang
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 语音启用AI代理可以执行常见的骗局
- **领域**: 人工智能
- **摘要**: 多模式，高功能的LLM的最新进展使语音启用了AI代理。这些代理正在启用新应用程序，例如支持语音的自主客户服务。但是，凭借所有AI功能，这些新功能具有双重使用。在这项工作中，我们表明启用语音的AI代理可以执行执行常见骗局所需的动作。为此，我们选择了政府收集的常见骗局列表，并构建了具有执行这些骗局的语音代理商。我们对支持语音的代理进行实验，并表明他们确实可以执行自主执行此类骗局所需的措施。我们的结果提出了有关语音启用AI代理的广泛部署的问题。

### Insights on Disagreement Patterns in Multimodal Safety Perception across Diverse Rater Groups 
[[arxiv](https://arxiv.org/abs/2410.17032)] [[cool](https://papers.cool/arxiv/2410.17032)] [[pdf](https://arxiv.org/pdf/2410.17032)]
> **Authors**: Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Zoe Ashwood,Aida Mostafazadeh Davani,Mark Diaz,Michela Paganini,Alicia Parrish,Ding Wang,Vinodkumar Prabhakaran,Lora Aroyo,Verena Rieser
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: 20 pages, 7 figures
- **标题**: 关于各种评估者组的多模式安全感知的分歧模式的见解
- **领域**: 人工智能
- **摘要**: AI系统至关重要的是人类评级，但是这些评级通常是汇总的，从而掩盖了现实现象中固有的观点多样性。在评估生成AI的安全性时，这一点尤其令人担忧，在社会文化背景下，感知和相关的危害可能会有很大差异。尽管最近的研究研究了人口统计学差异对注释文本的影响，但对这些主观变化如何影响生成AI中的多模式安全性的理解有限。为了解决这个问题，我们进行了一项大规模研究，采用了高度平行的安全等级，该评级是大约1000个文本对图像（T2i）世代，该人口具有多样化的评估者库，该库在跨年龄，性别和种族之间在30个交叉组中平衡的630个评估者。我们的研究表明，（1）关于他们评估危害的严重程度，各种差异在不同类型的安全违规行为之间存在明显差异，（2）各种评估者捕获的注释模式捕获了与在特定的安全策略中培训的专家评估者的差异，（3）以前的安全性差异与以前的差异有关。为了进一步了解这些不同的观点，我们对评估者提供的开放解释进行了定性分析。该分析揭示了不同群体在T2i世代感知危害的原因中的核心差异。我们的发现强调了将各种观点纳入生成AI的安全评估的迫切需要，确保了这些系统真正包容并反映了所有用户的价值。

### An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and Geometric Reasoning Skills Using Computer Graphics Questions 
[[arxiv](https://arxiv.org/abs/2410.16991)] [[cool](https://papers.cool/arxiv/2410.16991)] [[pdf](https://arxiv.org/pdf/2410.16991)]
> **Authors**: Tony Haoran Feng,Paul Denny,Burkhard C. Wünsche,Andrew Luxton-Reilly,Jacqueline Whalley
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: 8 pages, 8 figures, 1 table, to be published in SIGGRAPH Asia 2024 Educator's Forum
- **标题**: 对AI的关注：使用计算机图形问题评估GPT-4O的视觉感知技能和几何推理技能
- **领域**: 人工智能,图形
- **摘要**: CG（计算机图形）是CS（计算机科学）的流行领域，但是由于需要大量技能，例如数学，编程，几何推理和创造力，因此许多学生认为这个主题很困难。在过去的几年中，研究人员研究了利用Genai（生成人工智能）改善教学的力量的方法。在CS中，许多研究都集中在入门计算上。一项评估LLM（大语言模型）GPT-4（仅文本）的性能的最新研究，在CG问题上表明表现不佳，并且依赖图像内容的详细描述，这通常需要用户的大量见解才能返回合理的结果。到目前为止，尚未研究LMM（大型多模型）或多模式LLM的能力来解决CG问题，以及如何使用这些能力来改善教学。在这项研究中，我们构建了两个数据集的CG问题数据集，需要不同程度的视觉感知技能和几何推理技能，并在两个数据集中评估当前最新的LMM GPT-4O。我们发现，尽管GPT-4O在独立解决视觉信息的问题方面表现出巨大的潜力，但主要限制仍然存在生成结果的准确性和质量。尽管存在这些局限性，但我们提出了几种新颖的方法，使CG教育者将Genai纳入CG教学。我们希望我们的准则进一步鼓励在CG教室中学习和参与。

### Order Matters: Exploring Order Sensitivity in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.16983)] [[cool](https://papers.cool/arxiv/2410.16983)] [[pdf](https://arxiv.org/pdf/2410.16983)]
> **Authors**: Zhijie Tan,Xu Chu,Weiping Li,Tong Mo
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: No comments
- **标题**: 订单事项：探索多模式大语言模型中的顺序灵敏度
- **领域**: 人工智能
- **摘要**: 多模式的大语言模型（MLLM）利用由文本，图像或视频组成的多模式上下文来解决各种多模式任务。但是，我们发现更改多模式输入的顺序会导致模型的性能在高级性能和随机猜测之间波动。这种现象存在于单模式（仅文本或仅图像）和混合模式（图像文本对）上下文中。此外，我们证明了流行的MLLM特别关注某些多模式上下文位置，尤其是开始和终点。利用这一特殊关注，我们将关键的视频框架和重要的图像/文本内容放在上下文中的特殊位置，并将其提交给MLLM进行推理。此方法可导致视频匹配的平均性能增长14.7％，视觉问题回答任务的平均性能增长17.8％。此外，我们提出了一种新的度量，位置不变的精度（PIA），以解决MLLM评估中的顺序偏差。我们的研究发现有助于更好地理解多模式中的内在学习（MMICL），并提供了实用的策略来增强MLLM性能而不增加计算成本。

### Scene-Aware Explainable Multimodal Trajectory Prediction 
[[arxiv](https://arxiv.org/abs/2410.16795)] [[cool](https://papers.cool/arxiv/2410.16795)] [[pdf](https://arxiv.org/pdf/2410.16795)]
> **Authors**: Pei Liu,Haipeng Liu,Xingyu Liu,Yiqun Li,Junlan Chen,Yangfan He,Jun Ma
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: No comments
- **标题**: 场景意识可解释的多模式轨迹预测
- **领域**: 人工智能
- **摘要**: 通过增强环境感知和自动化车辆的轨迹预测，智能技术的进步在复杂的交通环境中已大大改善了导航。但是，当前的研究通常忽略了场景代理的共同推理，并且缺乏轨迹预测模型中的解释性，从而限制了它们在现实情况下的实际使用。为了解决这个问题，我们介绍了可解释的基于条件扩散的多模式轨迹预测（DMTP）模型，该模型旨在阐明影响预测的环境因素并揭示基本机制。我们的模型集成了修改的条件扩散方法，以捕获多模式轨迹模式，并采用修订后的Shapley价值模型来评估全球和方案特异性特征的重要性。使用Waymo Open Motion数据集的实验表明，我们的可解释模型在识别关键输入并在精度上显着优于基线模型方面表现出色。此外，这些因素确定与人类驾驶经验保持一致，强调该模型在学习准确的预测中的有效性。代码可在我们的开源存储库中提供：https：//github.com/ocean-luna/explainable-prediction。

### R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.17885)] [[cool](https://papers.cool/arxiv/2410.17885)] [[pdf](https://arxiv.org/pdf/2410.17885)]
> **Authors**: Linger Deng,Yuliang Liu,Bohan Li,Dongliang Luo,Liang Wu,Chengquan Zhang,Pengyuan Lyu,Ziyang Zhang,Gang Zhang,Errui Ding,Yingying Zhu,Xiang Bai
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: R-COT：大型多模型中的几何推理的反向链链生成的问题产生
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 由于缺乏高质量的图像文本配对数据，现有的大型多模型模型（LMM）与数学几何推理斗争。当前的几何数据生成方法，这些方法应用预设模板来生成几何数据或使用大型语言模型（LLMS）来重塑问题和答案（Q＆A），不可避免地限制了数据的准确性和多样性。为了综合高质量的数据，我们提出了一个两阶段的反向链链（R-COT）几何形状生成管道。首先，我们介绍地球字体来产生高保真的几何图像和相应的描述，突出了几何元素之间的关系。然后，我们设计了一种反向A和Q方法，该方法根据描述逐步推荐，并与推理结果相反。实验表明，所提出的方法在多个LMM基准线上带来了显着和一致的改进，从而在2B，7B和8B设置中实现了新的性能记录。值得注意的是，R-COT-8B在MathVista上显着优于先前的最先前的开源数学模型，而GEOQA的开源模型则高于9.2％，而在两个数据集中，GEOQA的开源模型平均超过了封闭源模型GPT-4O。该代码可在https://github.com/dle666/r-cot上找到。

### Lightweight Neural App Control 
[[arxiv](https://arxiv.org/abs/2410.17883)] [[cool](https://papers.cool/arxiv/2410.17883)] [[pdf](https://arxiv.org/pdf/2410.17883)]
> **Authors**: Filippos Christianos,Georgios Papoudakis,Thomas Coste,Jianye Hao,Jun Wang,Kun Shao
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: ICLR 2025 (spotlight)
- **标题**: 轻型神经应用程序控制
- **领域**: 人工智能
- **摘要**: 本文介绍了一种新颖的手机控制体系结构，轻质多模式应用程序控制（LIMAC），以进行各种Android应用程序的有效交互和控制。 LIMAC将其作为输入文本目标和一系列过去的移动观测值，例如屏幕截图和相应的UI树，以生成精确的动作。为了解决智能手机固有的计算约束，我们引入了与微调视觉模型（VLM）集成的小动作变压器（ACT），以实时决策和任务执行。我们在两个开源移动控制数据集上评估了LIMAC，这证明了我们的小型方法方法与诸如Florence2和QWEN2-VL等微调版本的小型版本相比。它还大大优于促使工程基线利用诸如GPT-4O之类的封闭源基础模型。更具体地说，与迅速工程基线相比，LIMAC的总体动作准确性最多可提高19％，高达42％。

### RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models for Multi-label Chest X-ray Classification 
[[arxiv](https://arxiv.org/abs/2410.17827)] [[cool](https://papers.cool/arxiv/2410.17827)] [[pdf](https://arxiv.org/pdf/2410.17827)]
> **Authors**: Marco Mistretta,Andrew D. Bagdanov
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: Accepted for publication at Medical Imaging meets NeurIPS (NeurIPS23)
- **标题**: 重新调整：用于多标签胸部X射线分类的生物医学视觉语言模型的增量微调
- **领域**: 人工智能
- **摘要**: 在本文中，我们介绍了Re-Tune，这是一种在增量学习场景中进行微调预训练的多模式生物医学视觉模型（VLMS）的新型方法，用于多标签的胸部疾病诊断。重新调整冻结骨干，仅在图像顶部和VLM的文本编码器上训练简单的适配器。通过工程为疾病的正面和负面文本提示，我们利用大型语言模型引导训练轨迹的能力。我们在三种现实的增量学习方案中评估重新调整：类新型，标签信息和数据信息。我们的结果表明，生物医学VLM是自然的持续学习者，并防止灾难性遗忘。重新调整不仅可以实现准确的多标签分类结果，而且还优先考虑患者隐私，并通过出色的计算效率来区分自己，使其非常适合在现实世界中的医疗保健环境中广泛采用。

### OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning 
[[arxiv](https://arxiv.org/abs/2410.18963)] [[cool](https://papers.cool/arxiv/2410.18963)] [[pdf](https://arxiv.org/pdf/2410.18963)]
> **Authors**: Xiaoqiang Wang,Bang Liu
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Work in progress
- **标题**: 奥斯卡：通过国家感知推理和重新计划的操作系统控制
- **领域**: 人工智能,计算语言学
- **摘要**: 大型语言模型（LLM）和大型多模式模型（LMM）在自动化复杂任务（例如Web浏览和游戏）方面具有巨大潜力。但是，它们在各种应用程序中概括的能力仍然有限，阻碍了更广泛的公用事业。为了应对这一挑战，我们提出了奥斯卡奖：通过国家感知推理和重新计划进行操作系统控制。奥斯卡（Oscar）是一种通才代理，旨在通过标准化控件（例如鼠标和键盘输入）自主浏览和与各种桌面和移动应用程序进行交互，同时处理屏幕图像以满足用户命令。奥斯卡将人类说明转换为可执行的Python代码，从而对图形用户界面（GUIS）进行精确控制。为了提高稳定性和适应性，奥斯卡（Oscar）作为状态机，配备了错误处理机制和动态任务重新计划，从而使其可以有效地适应实时反馈和异常。我们通过对台式机和移动平台的各种基准测试的广泛实验来展示奥斯卡的有效性，在那里它将复杂的工作流程转换为简单的自然语言命令，从而大大提高了用户的生产率。我们的代码将在出版后开源。

### Guiding Empowerment Model: Liberating Neurodiversity in Online Higher Education 
[[arxiv](https://arxiv.org/abs/2410.18876)] [[cool](https://papers.cool/arxiv/2410.18876)] [[pdf](https://arxiv.org/pdf/2410.18876)]
> **Authors**: Hannah Beaux,Pegah Karimi,Otilia Pop,Rob Clark
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: 9 pages, 1 Figure, 1 Table, Accepted in FIE 2024
- **标题**: 指导授权模型：在线高等教育中解放神经多样性
- **领域**: 人工智能,人机交互
- **摘要**: 在此创新的实践全文中，我们通过识别影响学习和功能的动态因素来解决神经差异和处境有限的学习者的公平差距。教育工作者表现出对确定学习者的认知能力和学习偏好的兴趣，以衡量他们对学术成就的影响。通常，机构采用千篇一律的方法，给残疾学生带来负担，以自我审核或容忍不足的支持。新兴框架通过教学方法（例如在线教育）指导神经化学习者。但是，这些框架无法满足整体环境需求或建议技术干预措施，尤其是对于那些未公开的学习或发育障碍和情境限制的人。在本文中，我们通过对大约100篇文章的二级研究进行了神经异常的观点，以引入一个指导授权模型，涉及关键的认知和情境因素，从而将影响学习者能力的日常经历背景相关。我们合成了三个示例学生资料，这些样本概况突出了功能中的用户问题。我们使用此模型来评估样本学习平台功能和其他支持技术解决方案。所提出的方法增加了框架，例如通用设计，用于考虑考虑各种感官处理差异，社会联系挑战和环境局限性在内的因素。我们建议，通过采用技术支持的功能，例如可自定义的任务管理，指导性的各种内容访问以及指导性的多模式协作，将删除神经病和情境有限的学习者的主要学习障碍，以激活其学术目标的成功追求。

### LLMs Can Evolve Continually on Modality for X-Modal Reasoning 
[[arxiv](https://arxiv.org/abs/2410.20178)] [[cool](https://papers.cool/arxiv/2410.20178)] [[pdf](https://arxiv.org/pdf/2410.20178)]
> **Authors**: Jiazuo Yu,Haomiao Xiong,Lu Zhang,Haiwen Diao,Yunzhi Zhuge,Lanqing Hong,Dong Wang,Huchuan Lu,You He,Long Chen
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: LLM可以以X模式推理的方式不断发展
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 多模式的大型语言模型（MLLM）由于其在多模式理解中令人印象深刻的能力而引起了极大的关注。但是，现有方法在很大程度上依赖于广泛的模态特异性预处理和联合模式调整，从而在扩展到新模式时会导致巨大的计算负担。在本文中，我们提出了PathWeave，这是一个具有模态路径开关和扩展能力的灵活且可扩展的框架，使MLLM能够在$ \ Mathbb {X} $模态推理的模态上不断发展。我们利用持续学习的概念并在预先训练的MLLM上制定了增量培训策略，从而在不执行联合模式预处理的情况下，可以使用单模式数据扩展到新模式。详细介绍，引入了一种新颖的适配器（ANA）框架，其中单峰和跨模式适配器无缝集成以促进有效的模态对准和协作。此外，在两种类型的适配器之间应用了基于MOE的门控模块，以进一步增强多模式相互作用。为了研究提出的方法，我们建立了一个具有挑战性的基准，称为持续学习模态（MCL），该基准由五种不同模式的高质量质量质量数据组成：图像，视频，音频，深度和点云。广泛的实验证明了拟议的ANA框架在持续学习过程中学习可塑性和记忆稳定性的有效性。此外，PathEave的性能与最先进的MLLM相当，同时将参数训练负担减少了98.73％。我们的代码位于https://github.com/jiazuoyu/pathweave

### LLM-Consensus: Multi-Agent Debate for Visual Misinformation Detection 
[[arxiv](https://arxiv.org/abs/2410.20140)] [[cool](https://papers.cool/arxiv/2410.20140)] [[pdf](https://arxiv.org/pdf/2410.20140)]
> **Authors**: Kumud Lakara,Georgia Channing,Juil Sock,Christian Rupprecht,Philip Torr,John Collomosse,Christian Schroeder de Witt
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: LLM-Consensus：视觉错误信息检测的多代理辩论
- **领域**: 人工智能
- **摘要**: 最具挑战性的错误信息形式之一涉及使用与误导性文本配对的图像的外观（OOC），从而创造了错误的叙述。现有的AI驱动检测系统缺乏解释性，需要昂贵的登录。我们通过LLM-Consensus解决了这些问题，LLM-Consensus是一个用于OOC错误信息检测的多代理辩论系统。 LLM-Consensus介绍了一个新颖的多代理辩论框架，其中多模式代理协作以评估上下文一致性并请求外部信息以增强跨文本推理和决策。我们的框架即使没有特定领域的微调，也可以使用最先进的准确性来解释检测。广泛的消融研究证实，外部检索显着提高了检测准确性，用户研究表明，LLM-Consensus可以提高专家和非专家的性能。这些结果将LLM-Consensus定位为自治和公民情报应用的强大工具。

### Multi-modal AI for comprehensive breast cancer prognostication 
[[arxiv](https://arxiv.org/abs/2410.21256)] [[cool](https://papers.cool/arxiv/2410.21256)] [[pdf](https://arxiv.org/pdf/2410.21256)]
> **Authors**: Jan Witowski,Ken G. Zeng,Joseph Cappadona,Jailan Elayoubi,Khalil Choucair,Elena Diana Chiru,Nancy Chan,Young-Joon Kang,Frederick Howard,Irina Ostrovnaya,Carlos Fernandez-Granda,Freya Schnabel,Zoe Steinsnyder,Ugur Ozerdem,Kangning Liu,Waleed Abdulsattar,Yu Zong,Lina Daoud,Rafic Beydoun,Anas Saad,Nitya Thakore,Mohammad Sadic,Frank Yeung,Elisa Liu,Theodore Hill, et al. (26 additional authors not shown)
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 多模式AI用于全面的乳腺癌预后
- **领域**: 人工智能,计算机视觉和模式识别,图像和视频处理
- **摘要**: 乳腺癌的治疗选择是由分子亚型和临床特征指导的。但是，包括基因组测定在内的当前工具缺乏最佳临床决策所需的准确性。我们开发了一种新型的人工智能（AI）方法，该方法将数字病理图像与临床数据相结合，为预测乳腺癌患者的癌症复发风险提供了一种更强大，更有效的方法。具体而言，我们利用了一个视觉变压器泛滥的基础模型，该模型接受了自我监督的学习训练，以从数字化的H＆E染色幻灯片中提取功能。这些特征与临床数据集成在一起，形成了多模式AI测试，以预测癌症复发和死亡。该测试是使用来自七个国家的15个同类人群中总共8,161名女性乳腺癌患者的数据开发和评估的。其中，来自五个队列的3,502名患者仅用于评估，而其余患者则用于训练。我们的测试准确地预测了我们的主要终点，无疾病的间隔，在五个评估队列中（C-指数：0.71 [0.68-0.75]，HR：3.63 [3.02-4.37，p <0.001]）。在直接比较（n = 858）中，AI测试比分别为0.67 [0.61-0.74]的C-指数[0.61-0.74]的AI测试更准确，分别为0.61-0.74]，而0.61 [0.49-0.73]。此外，AI测试在多元分析中为Oncotype DX添加了独立的预后信息（HR：3.11 [1.91-5.09，p <0.001）]）。该测试表明，包括TNBC在内的主要分子乳腺癌亚型（C-INDEX：0.71 [0.62-0.81]，HR：3.81 [2.35-6.17，P = 0.02]）的测试表现出了良好的精度，其中目前没有通过Clinical GuideLine建议使用诊断工具。这些结果表明，我们的AI测试提高了现有预后测试的准确性，同时适用于更广泛的患者。

### ADAM: An Embodied Causal Agent in Open-World Environments 
[[arxiv](https://arxiv.org/abs/2410.22194)] [[cool](https://papers.cool/arxiv/2410.22194)] [[pdf](https://arxiv.org/pdf/2410.22194)]
> **Authors**: Shu Yu,Chaochao Lu
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: 亚当：在开放世界环境中的一个具体因果因素
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 在Minecraft等开放世界环境中，现有代理商在不断学习的结构化知识（尤其是因果关系）方面面临挑战。这些挑战源于黑匣子模型固有的不透明性以及在培训期间对先验知识的过度依赖，从而损害了它们的可解释性和泛化能力。为此，我们介绍了Minecraft中一种体现的因果代理Adam，可以自主地浏览开放世界，感知多模式环境，学习因果世界知识，并通过终身学习解决复杂的任务。亚当被四个关键组成部分授权：1）一个交互模块，使代理在记录交互过程时可以执行操作； 2）一个因果模块，其任务是从头开始构建不断增长的因果图，可增强可解释性并降低对先验知识的依赖； 3）一个由计划者，演员和内存池组成的控制器模块，该模块使用学习的因果图来完成任务； 4）一个感知模块，由多模式大语言模型提供动力，使亚当能够像人类玩家一样感知。广泛的实验表明，亚当从头开始构建了几乎完美的因果图，从而具有有效的任务分解和执行，具有强大的解释性。值得注意的是，在我们修改过的Minecraft游戏中，Adam保持其性能，并表现出出色的鲁棒性和概括能力。亚当开拓了一种新颖的范式，该范式以协同的方式整合了因果方法和体现的药物。我们的项目页面在https://opencausalab.github.io/adam上。

### A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment 
[[arxiv](https://arxiv.org/abs/2410.23242)] [[cool](https://papers.cool/arxiv/2410.23242)] [[pdf](https://arxiv.org/pdf/2410.23242)]
> **Authors**: Matteo G. Mecattaf,Ben Slater,Marko Tešić,Jonathan Prunty,Konstantinos Voudouris,Lucy G. Cheke
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: 25 pages, 4 figures; v2: Added AFMR Acknowledgment
- **标题**: 请少一些对话，请采取更多的动作：在3D体现的环境中调查LLM的物理常识
- **领域**: 人工智能
- **摘要**: 作为通用工具，大型语言模型（LLM）通常必须理解日常的物理环境。以问答的能力，可能需要了解物理对象的相互作用以做出适当的反应。此外，LLM越来越多地用作代理系统中的推理引擎，设计和控制其动作序列。绝大多数研究都使用静态基准来解决此问题，包括有关物理世界的文本或基于图像的问题。但是，这些基准并不能捕获现实生活过程的复杂性和细微差别。在这里，我们提倡第二个相对尚未探索的方法：通过在3D环境中授予对代理的控制来“体现” LLMS。我们介绍了对LLM中物理常识性推理的第一个体现和认知有意义的评估。我们的框架可以直接比较LLM与其他具体体现的药物，例如基于深度强化学习的药物以及人类和非人类动物。我们采用模拟的3D虚拟实验室的动物AAI（AAI）环境来研究LLMS中的物理常识性推理。为此，我们使用AAI测试床，这是一套实验，这些实验可以通过非人类动物复制实验室研究，研究包括距离估计，跟踪视野对象和工具使用在内的物理推理能力。我们证明，没有填充的最先进的多模式模型可以完成这种任务风格，从而可以与2019年动物Ai-Ai Olympics竞赛的参赛者和人类儿童进行有意义的比较。我们的结果表明，在这些任务上，LLM当前胜过人类儿童。我们认为，这种方法允许使用直接从认知科学绘制的生态有效实验来研究物理推理，从而提高了LLM的可预测性和可靠性。

## 硬件架构(cs.AR:Hardware Architecture)

该领域共有 2 篇论文

### Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective 
[[arxiv](https://arxiv.org/abs/2410.04466)] [[cool](https://papers.cool/arxiv/2410.04466)] [[pdf](https://arxiv.org/pdf/2410.04466)]
> **Authors**: Jinhao Li,Jiaming Xu,Shan Huang,Yonghua Chen,Wen Li,Jun Liu,Yaoxiu Lian,Jiayi Pan,Li Ding,Hao Zhou,Yu Wang,Guohao Dai
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: 51 pages, 19 figures. Update the discussion about the future trends of LLM
- **标题**: 大型语言模型推理加速度：全面的硬件观点
- **领域**: 硬件架构,机器学习
- **摘要**: 大型语言模型（LLMS）在各个领域都表现出了出色的功能，从自然语言理解到文本生成。与Bert和Deberta等非生成LLM相比，GPT系列和Llama系列（如GPT系列）的生成LLM目前是其出色的算法性能，目前是主要重点。生成LLM的进步与硬件功能的开发紧密相关。各种硬件平台具有不同的硬件特性，可以帮助提高LLM推理性能。因此，本文在不同的硬件平台上全面调查了有效的生成LLM推断。首先，我们概述了主流生成LLM的算法体系结构，并深入研究推理过程。然后，我们总结了不同平台（例如CPU，GPU，FPGA，ASIC和PIM/NDP）的不同优化方法，并为生成LLM提供推理结果。此外，我们通过考虑硬件功耗，绝对推理速度（代币/s）和能源效率（令牌/j）来对不同硬件平台上的推理性能与批量1和8进行定性和定量比较。我们比较了不同硬件平台上相同优化方法的性能，不同硬件平台之间的性能以及在同一硬件平台上的不同方法的性能。这通过集成软件优化方法和硬件平台，提供了现有推理加速工作的系统，全面的摘要。我们指出，三个趋势（多模式，推理时间计算和更高的推理能源效率）有望重新定义边缘人工智能系统的能力。我们的项目可在https://dai.sjtu.edu.cn/project.html上找到。

### A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.07265)] [[cool](https://papers.cool/arxiv/2410.07265)] [[pdf](https://arxiv.org/pdf/2410.07265)]
> **Authors**: Cong Guo,Feng Cheng,Zhixu Du,James Kiessling,Jonathan Ku,Shiyu Li,Ziru Li,Mingyuan Ma,Tergel Molom-Ochir,Benjamin Morris,Haoxuan Shan,Jingwei Sun,Yitu Wang,Chiyue Wei,Xueying Wu,Yuhao Wu,Hao Frank Yang,Jingyang Zhang,Junyao Zhang,Qilin Zheng,Guanglei Zhou,Hai,Li,Yiran Chen
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-10
> **comment**: Accepted by IEEE Circuits and Systems Magazine
- **标题**: 调查：大语模型时代的协作硬件和软件设计
- **领域**: 硬件架构,人工智能,机器学习,软件工程
- **摘要**: 大型语言模型（LLM）的快速发展显着改变了人工智能领域，表现出了自然语言处理的显着能力，并朝着多模式功能迈进。这些模型越来越多地整合到各种应用中，从而影响了研究和行业。但是，他们的开发和部署面临着重大挑战，包括需要广泛的计算资源，高能消耗和复杂的软件优化。与传统的深度学习系统不同，LLM需要独特的优化策略来培训和推理，重点是系统级效率。本文调查了专门针对大型语言模型的独特特征和约束的专门定制的硬件和软件共同设计方法。这项调查分析了LLMS对硬件和算法研究的挑战和影响，探索算法优化，硬件设计和系统级创新。它旨在在以LLM中心计算系统中的权衡和考虑方面提供全面的理解，从而指导AI中未来的进步。最后，我们总结了该领域的现有努力，并概述了未来的指示，以实现下一代大型语言模型和AI系统的生产级共同设计方法。

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

该领域共有 2 篇论文

### Modeling News Interactions and Influence for Financial Market Prediction 
[[arxiv](https://arxiv.org/abs/2410.10614)] [[cool](https://papers.cool/arxiv/2410.10614)] [[pdf](https://arxiv.org/pdf/2410.10614)]
> **Authors**: Mengyu Wang,Shay B. Cohen,Tiejun Ma
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Accepted by EMNLP 2024
- **标题**: 建模新闻互动和金融市场预测的影响
- **领域**: 计算工程、金融和科学,人工智能,计算语言学,计算金融
- **摘要**: 金融新闻扩散到市场价格上是一个复杂的过程，这使得评估新闻事件与市场变动之间的联系具有挑战性。本文介绍了Finin（金融互连的新闻影响网络），这是一个新颖的市场预测模型，不仅捕获了新闻与价格之间的联系，而且还捕获了新闻项目本身之间的互动。 Finin有效地整合了来自市场数据和新闻文章的多模式信息。我们在两个数据集上进行了广泛的实验，其中包括15年期间的标准普尔500指数和纳斯达克100指数，以及超过270万本新闻文章。结果表明，Finin的有效性优于先进的市场预测模型，两家市场的每日夏普比率分别提高了0.429和0.341。此外，我们的结果揭示了对财务新闻的见解，包括新闻的延迟市场定价，新闻的长期记忆效应以及财务情感分析的局限性，从而充分从新闻数据中提取预测能力。

### Drone Acoustic Analysis for Predicting Psychoacoustic Annoyance via Artificial Neural Networks 
[[arxiv](https://arxiv.org/abs/2410.22208)] [[cool](https://papers.cool/arxiv/2410.22208)] [[pdf](https://arxiv.org/pdf/2410.22208)]
> **Authors**: Andrea Vaiuso,Marcello Righi,Oier Coretti,Moreno Apicella
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: 20 Pages, 10 Figures, 4 Tables
- **标题**: 无人机声学分析，用于通过人工神经网络预测精神声音烦恼
- **领域**: 计算工程、金融和科学,人工智能
- **摘要**: 由于其低运营成本，紧凑的尺寸和广泛的可访问性，无人驾驶飞机（UAV）已在各个领域和工业应用中广泛使用。但是，无人机螺旋桨产生的噪声已成为一个重大问题。这可能会影响公众愿意在需要靠近居民区运营的服务中实施这些车辆的意愿。应对这一挑战的标准方法包括音压测量和噪声特征分析。近年来，人工智能模型的整合通过增强无人机声学数据中的复杂特征检测进一步简化了这一过程。这项研究基于先前的研究，通过研究各种深度学习模型在预测心理声学烦恼方面的功效，这是基于多个无人机特征作为输入的多个无人机特征来衡量人耳的烦恼的有效指数。这是通过使用具有多个麦克风的各种无人机模型的精确度量来构建训练数据集并分析飞行数据，操纵，无人机物理特征以及在现实条件下感知到的烦恼来实现的。这项研究的目的是提高我们对无人机噪声的理解，有助于降低降噪技术的发展，并鼓励在公共场所接受无人机使用。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 102 篇论文

### Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models 
[[arxiv](https://arxiv.org/abs/2410.00363)] [[cool](https://papers.cool/arxiv/2410.00363)] [[pdf](https://arxiv.org/pdf/2410.00363)]
> **Authors**: Shitian Zhao,Renrui Zhang,Xu Luo,Yan Wang,Shanghang Zhang,Peng Gao
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: 释放多模式模型的可能性组成的潜力
- **领域**: 计算语言学
- **摘要**: 模型融合一直是一个重要的主题，尤其是在一个具有不同体系结构，参数大小和训练管道的大型语言模型（LLM）和多模式语言模型（MLM）的时代。在这项工作中，我们提出了一个事后框架，旨在融合异质模型，我们称之为\ textit {样可能的组成}，而基本思想是在执行多种选择性的视觉效果 - 问题 - 求和任务时构成多个模型的可能性分布。在这里，核心概念\ textit {可能性}实际上是候选人答案的对数概率。在\ textIt {似然组成}中，我们介绍了一些基本操作：\ textit {debias}，\ textit {亮点{亮点}，\ textit {Martial-vote}和\ textit {ensemble}。通过组合（组成）这些基本元素，我们获得了混合组成方法：\ textit {mix-composition}。通过对9个VQA数据集和10毫升的全面实验，我们证明了\ textIt {mix-composition}的有效性，与Simple \ textit {Ensemble}或\ textit {Martial-vote}方法相比。在此框架中，人们可以提出新的基本构图方法，并将它们组合起来以获取新的混合构图方法。我们希望我们提出的\ textit {似然成分}可以提供融合异质模型的新观点，并在此框架下激发探索。

### Insight: A Multi-Modal Diagnostic Pipeline using LLMs for Ocular Surface Disease Diagnosis 
[[arxiv](https://arxiv.org/abs/2410.00292)] [[cool](https://papers.cool/arxiv/2410.00292)] [[pdf](https://arxiv.org/pdf/2410.00292)]
> **Authors**: Chun-Hsiao Yeh,Jiayun Wang,Andrew D. Graham,Andrea J. Liu,Bo Tan,Yubei Chen,Yi Ma,Meng C. Lin
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: Accepted to MICCAI 2024. Project Webpage: https://danielchyeh.github.io/MDPipe/
- **标题**: 洞察力：使用LLMS进行眼表疾病诊断的多模式诊断管道
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 眼表疾病的准确诊断在验光和眼科中至关重要，这在整合临床数据源（例如，免生体成像和临床元数据）方面取决于副作用。传统的人类评估在量化临床观察方面缺乏精确性，而当前基于机器的方法通常将诊断视为多类分类问题，将诊断限制在预定义的固定固定的策划答案中，而无需推理每个变量与诊断的临床相关性。为了应对这些挑战，我们通过使用大型语言模型（LLMS）进行眼表面疾病诊断来引入创新的多模式诊断管道（MDPIPE）。我们首先采用视觉翻译器来解释约物图像，通过将它们转换为可量化的形态数据，促进它们与临床元数据的整合，并使细微的医学见解与LLMS的沟通。为了进一步促进这种交流，我们引入了基于LLM的摘要器，以将合并形态和临床元数据的见解背景下，并产生临床报告摘要。最后，我们从现实生活中的临床医生诊断中使用了LLMS的推理能力。我们对各种眼表疾病诊断的评估基准表明，MDPipe的表现优于包括GPT-4在内的现有标准，并为诊断提供了临床上合理的理由。

### Visual Perception in Text Strings 
[[arxiv](https://arxiv.org/abs/2410.01733)] [[cool](https://papers.cool/arxiv/2410.01733)] [[pdf](https://arxiv.org/pdf/2410.01733)]
> **Authors**: Qi Jia,Xiang Yue,Shanshan Huang,Ziheng Qin,Yizhu Liu,Bill Yuchen Lin,Yang You
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 文本字符串中的视觉感知
- **领域**: 计算语言学
- **摘要**: 了解嵌入连续字符的视觉语义是大型语言模型（LLM）和多模式大语言模型（MLLM）的关键能力。这种类型的人工制品具有独特的特征，即在文本和图像中都可以很容易地提出相同的信息，从而使它们成为分析现代LLM和MLLMS在模态 - 语言视觉理解中的重要代理。在这项工作中，我们选择ASCII艺术作为代表性的人工制品，在该工件中，用来描绘每个概念的线条和亮度是由字符呈现的，我们将问题作为ASCII艺术识别任务。我们通过使用精细的分类树构建评估数据集，并收集一个训练集来激发模型的视觉感知能力，从而基于此任务的模型性能。通过对数十种模型的全面分析，结果表明，尽管人类可以达到近100％的精度，但最先进的LLM和MLLM落后于远方。模型能够识别ASCII艺术中所描绘的概念，仅给出了某些概念的60％精度所示的文本输入，但是当在所有类别中平均时，大多数概念仅能达到30％的精度。当提供图像作为输入时，GPT-4O获得了82.68％，表现优于最强的开源MLLM 21.95％。尽管模型根据所提供的方式有利于不同种类的ASCII艺术，但是当同时提供两种模态时，MLLM都没有成功。此外，监督的微调有助于提高模型的准确性，尤其是在提供图像方式时，但也强调需要更好地训练技术来增强模式之间的信息融合。

### Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Unveiling AI's Potential Through Tools, Techniques, and Applications 
[[arxiv](https://arxiv.org/abs/2410.01268)] [[cool](https://papers.cool/arxiv/2410.01268)] [[pdf](https://arxiv.org/pdf/2410.01268)]
> **Authors**: Pohsun Feng,Ziqian Bi,Yizhu Wen,Xuanhe Pan,Benji Peng,Ming Liu,Jiawei Xu,Keyu Chen,Junyu Liu,Caitlyn Heqi Yin,Sen Zhang,Jinlang Wang,Qian Niu,Ming Li,Tianyang Wang
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: This book contains 155 pages and 9 figures
- **标题**: 深度学习和机器学习，推进大数据分析和管理：通过工具，技术和应用程序揭示AI的潜力
- **领域**: 计算语言学,机器学习
- **摘要**: 人工智能（AI），机器学习和深度学习已成为大数据分析和管理中的变革力量，从而在各种行业之间取得了突破性的进步。本文深入研究了这些领域的基础概念和尖端发展，特别关注大型语言模型（LLM）及其在自然语言处理，多模式推理和自主决策中的作用。讨论突出了Chatgpt，Claude和Gemini等工具，探讨了他们在数据分析，模型设计和优化中的应用。神经网络，增强学习和生成模型等高级算法的集成增强了AI系统处理，可视化和解释复杂数据集的功能。此外，Edge计算和自动化机器学习（AutoML）等技术的出现使对AI的访问人士民主化，从而赋予用户跨技能水平的用户能够与智能系统互动。这项工作还强调了道德考虑，透明度和公平性在AI技术部署中的重要性，为负责任的创新铺平了道路。通过对硬件配置，软件环境和现实世界应用程序的实践见解，本文为研究人员和从业人员提供了综合资源。通过将理论的基础与可行的策略桥接，它展示了AI和LLM的潜力彻底改变大数据管理并推动跨医疗保健，金融和自主系统等领域的有意义的进步。

### Unified Multimodal Interleaved Document Representation for Retrieval 
[[arxiv](https://arxiv.org/abs/2410.02729)] [[cool](https://papers.cool/arxiv/2410.02729)] [[pdf](https://arxiv.org/pdf/2410.02729)]
> **Authors**: Jaewoo Lee,Joonho Ko,Jinheon Baek,Soyeong Jeong,Sung Ju Hwang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Preprint
- **标题**: 取回的统一多模式交织文档表示
- **领域**: 计算语言学,人工智能,信息检索
- **摘要**: 信息检索（IR）方法旨在识别与查询相关的文档，这些文档已广泛应用于各种自然语言任务。但是，现有方法通常仅考虑文档中的文本内容，从而忽略了文档可以包含多种模式（包括图像和表）的事实。同样，他们经常将每个长文档分为多个离散段落以嵌入，这使他们无法捕获整体文档上下文和段落之间的交互。为了应对这两个挑战，我们提出了一种方法，该方法通过利用最近的视觉模型的能力来整体嵌入文档与多种方式交织在一起，该模型可以将文本，图像和表的处理和集成到统一的格式和表示中。此外，为了减轻将文档分割为段落的信息损失，而不是分别表示和检索段落，我们将分段段落的表示形式合并为一个单个文档表示形式，同时我们还会引入重新融合策略，以使文档中的相关段落在文档中确定。然后，通过考虑文本和多模式查询的各种红外场景的广泛实验，我们表明我们的方法基本上优于相关的基线，这要归功于文档中的多模式信息。

### MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences 
[[arxiv](https://arxiv.org/abs/2410.02381)] [[cool](https://papers.cool/arxiv/2410.02381)] [[pdf](https://arxiv.org/pdf/2410.02381)]
> **Authors**: Genta Indra Winata,David Anugraha,Lucky Susanto,Garry Kuwanto,Derry Tanti Wijaya
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Accepted to ICLR 2025
- **标题**: iNaMetrics：使用人类偏好校准生成任务的指标
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 了解绩效评估度量的质量对于确保模型输出与人类偏好保持一致至关重要。但是，尚不清楚每个指标如何捕获这些偏好的各个方面，因为指标通常在一个特定领域中表现出色，但在所有方面都不是。为了解决这个问题，必须系统地将指标校准到人类偏爱的特定方面，以适应每个方面的独特特征。我们介绍了Entametrics，这是一种经过校准的元元素，旨在以监督方式评估不同方式的发电任务。 Entametrics优化了现有指标的组合，以增强其与人类偏好的一致性。我们的度量标准在下游任务中表现出灵活性和有效性，在各种多语言和多域场景中都显示出显着的好处。元素与人类的偏好紧密一致，并且高度可扩展且易于整合到任何应用中。这使得元学成为改善生成任务评估的强大工具，从而确保指标更能代表各种环境中人类的判断。

### Towards Comprehensive Detection of Chinese Harmful Memes 
[[arxiv](https://arxiv.org/abs/2410.02378)] [[cool](https://papers.cool/arxiv/2410.02378)] [[pdf](https://arxiv.org/pdf/2410.02378)]
> **Authors**: Junyu Lu,Bo Xu,Xiaokun Zhang,Hongbo Wang,Haohao Zhu,Dongyu Zhang,Liang Yang,Hongfei Lin
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: 旨在全面检测中国有害模因
- **领域**: 计算语言学,人工智能
- **摘要**: 本文已在2024 D＆B曲目中接受。有害模因在中国互联网上激增，而研究中国有害模因的研究显着落后，因为没有可靠的数据集和有效的探测器。为此，我们专注于对中国有害模因的全面发现。我们构建了第一个中国有害模因数据集有毒MM，该数据集由12,000个样品组成，并具有针对各种模因类型的细粒注释。此外，我们提出了一个基线探测器，多模式知识增强（MKE），并结合了LLM生成的模因内容的上下文信息，以增强对中国模因的理解。在评估阶段，我们对包括LLMS和我们的MKE在内的多个基准进行了广泛的定量实验和定性分析。实验结果表明，检测中国有害模因对于现有模型而言是具有挑战性的，同时证明了MKE的有效性。本文的资源可在https://github.com/dut-lujunyu/toxicn_mm上获得。

### From Concrete to Abstract: A Multimodal Generative Approach to Abstract Concept Learning 
[[arxiv](https://arxiv.org/abs/2410.02365)] [[cool](https://papers.cool/arxiv/2410.02365)] [[pdf](https://arxiv.org/pdf/2410.02365)]
> **Authors**: Haodong Xie,Rahul Singh Maharjan,Federico Tavella,Angelo Cangelosi
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: 从具体到摘要：一种多模式的生成方法，用于抽象概念学习
- **领域**: 计算语言学,人工智能
- **摘要**: 理解和操纵具体和抽象的概念是人类智能的基础。然而，对于人造代理人来说，它们仍然具有挑战性。本文介绍了一种多模式生成方法，用于高阶抽象概念学习，该方法将视觉和分类语言信息从混凝土中整合在一起。我们的模型最初将下级具体概念扎根，结合起来，形成基本级别的概念，最后通过基本级别概念的基础将其抽象到上级级别的概念。我们通过高阶抽象概念通过语言对视觉和视觉到语言测试评估模型语言学习能力。实验结果证明了该模型在语言理解和语言命名任务中的熟练程度。

### Can LLMs Improve Multimodal Fact-Checking by Asking Relevant Questions? 
[[arxiv](https://arxiv.org/abs/2410.04616)] [[cool](https://papers.cool/arxiv/2410.04616)] [[pdf](https://arxiv.org/pdf/2410.04616)]
> **Authors**: Alimohammad Beigi,Bohan Jiang,Dawei Li,Zhen Tan,Pouya Shaeri,Tharindu Kumarage,Amrita Bhattacharjee,Huan Liu
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: LLM可以通过提出相关问题来改善多模式事实检查吗？
- **领域**: 计算语言学
- **摘要**: 传统的事实核对依靠人类来提出相关和有针对性的事实检查问题（FCQ），寻找证据并验证索赔的事实。尽管大型语言模型（LLMS）通常用于自动化证据检索和事实验证，但由于缺乏FCQ公式而阻碍了事实检查的有效性。为了弥合这一差距，我们试图回答两个研究问题：（1）LLM可以产生相关的FCQ吗？ （2）LLM生成的FCQ可以改善多模式事实检查吗？因此，我们引入了一个框架LRQ-FACT，用于使用LLM生成相关的FCQ来促进证据检索并通过探索多种方式的信息来促进并增强事实检查。通过广泛的实验，我们验证LRQ-FACT是否可以生成不同类型的相关FCQ，并且LRQ-FACT是否可以在多模式事实检查中始终超过基线方法。进一步的分析说明了LRQ-FACT中的每个组件如何致力于改善事实检查的性能。

### FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering 
[[arxiv](https://arxiv.org/abs/2410.04526)] [[cool](https://papers.cool/arxiv/2410.04526)] [[pdf](https://arxiv.org/pdf/2410.04526)]
> **Authors**: Siqiao Xue,Tingting Chen,Fan Zhou,Qingyang Dai,Zhixuan Chu,Hongyuan Mei
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: FAMMA：金融领域多语言多模式问题的基准回答
- **领域**: 计算语言学,人工智能
- **摘要**: 在本文中，我们介绍了Famma，Famma是财务多语言多模式问答（QA）的开源基准。我们的基准旨在评估多模式大语言模型（MLLM）的能力，以回答需要先进的财务知识和复杂推理的问题。它包括从大学教科书和考试中精心收集的提问对的1,758对，涵盖了8个金融领域的主要子领域，包括公司财务，资产管理和金融工程。其中一些质量检查是用中文或法语编写的，而其中大多数是英语。这些问题以混合格式结合，结合了文本和异质图像类型，例如图表，表格和图表。我们在基准上评估了一系列最新的MLLM，我们的分析表明，Famma对这些模型构成了重大挑战。即使是GPT-4O和Claude-35-Sonnet等高级系统，也仅达到42 \％的精度。此外，开源QWEN2-VL特别落后于其专有对应物。最后，我们探索GPT O1式推理链以增强模型的推理能力，从而显着改善了误差校正。我们的FAMMA基准将促进未来的研究，以开发金融质量检查中的专家系统。排行榜可在https://famma-bench.github.io/famma/上找到。

### ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal Large Language Models Via Error Detection 
[[arxiv](https://arxiv.org/abs/2410.04509)] [[cool](https://papers.cool/arxiv/2410.04509)] [[pdf](https://arxiv.org/pdf/2410.04509)]
> **Authors**: Yibo Yan,Shen Wang,Jiahao Huo,Hang Li,Boyan Li,Jiamin Su,Xiong Gao,Yi-Fan Zhang,Tianlong Xu,Zhendong Chu,Aoxiao Zhong,Kun Wang,Hui Xiong,Philip S. Yu,Xuming Hu,Qingsong Wen
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: ERRORRADAR：通过错误检测进行多模式大语言模型的复杂数学推理基准测试
- **领域**: 计算语言学
- **摘要**: 随着多模式大语言模型（MLLM）的领域继续发展，它们彻底改变人工智能的潜力尤其有希望，尤其是在解决数学推理任务时。当前的数学基准主要集中在评估MLLM的解决问题能力上，但是在解决更复杂的情况（例如错误检测）方面存在至关重要的差距，以增强复杂设置中的推理能力。为了填补这一空白，我们正式制定了新任务：多模式错误检测，并引入Errorradar，这是第一个旨在评估MLLMS在此类任务中的功能的基准。 Errorradar评估了两个子任务：错误步骤标识和错误分类，为评估MLLMS复杂的数学推理能力提供了全面的框架。它由2,500个高质量的多模式K-12数学问题组成，这些问题是从教育组织中的现实世界学生互动中收集的，具有严格的注释和富裕的元数据，例如问题类型和错误类别。通过广泛的实验，我们评估了开源和闭合代表代表MLLM，对他们针对教育专家评估人员进行了基准测试。结果表明仍然存在重大挑战，因为具有最佳性能的GPT-4O仍然落后于人类评估的10％。该数据集将在接受后可用。

### Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2410.04491)] [[cool](https://papers.cool/arxiv/2410.04491)] [[pdf](https://arxiv.org/pdf/2410.04491)]
> **Authors**: Xinyu Feng,Yuming Lin,Lihua He,You Li,Liang Chang,Ya Zhou
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: Accepted to EMNLP Findings 2024
- **标题**: 知识引导的动态方式注意融合框架多模式情感分析
- **领域**: 计算语言学,人工智能,多媒体
- **摘要**: 多模式情感分析（MSA）利用多模式数据来推断用户的情感。先前的方法着重于平等地对待每种方式或静态上使用文本作为进行互动的主要方式的贡献，这忽略了每种方式可能成为主导的情况。在本文中，我们提出了一种知识引导的动态情感注意融合框架（KUDA），以进行多模式情感分析。 Kuda使用情感知识来指导模型动态选择主要模式并调整每种模式的贡献。此外，使用获得的多模式表示，该模型可以进一步强调通过相关评估损失来占主导地位的贡献。在四个MSA基准数据集上进行的广泛实验表明，Kuda可以实现最先进的性能，并能够适应不同的主导方式。

### Fine-Grained Prediction of Reading Comprehension from Eye Movements 
[[arxiv](https://arxiv.org/abs/2410.04484)] [[cool](https://papers.cool/arxiv/2410.04484)] [[pdf](https://arxiv.org/pdf/2410.04484)]
> **Authors**: Omer Shubi,Yoav Meiri,Cfir Avraham Hadar,Yevgeni Berzak
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: Accepted to EMNLP
- **标题**: 从眼动的读取理解的细粒度预测
- **领域**: 计算语言学
- **摘要**: 可以根据阅读的眼动评估人类阅读理解吗？在这项工作中，我们使用大规模的眼影数据来解决这个长期存在的问题，这些数据针对阅读理解的行为分析，这些材料旨在进行文本材料。我们专注于一项细粒度且在很大程度上没有解决的任务，即从单个问题的级别上预测眼动的阅读理解。我们使用三种新的多模式模型以及文献中的一系列先前模型来解决此任务。我们评估了模型将新文本项目，新参与者的推广能力以及两种不同的阅读制度（普通阅读和信息寻求信息）组合的能力。评估表明，尽管任务极具挑战性，但眼睛运动包含有用的信号，用于良好的阅读理解预测。代码和数据将公开可用。

### ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities 
[[arxiv](https://arxiv.org/abs/2410.03907)] [[cool](https://papers.cool/arxiv/2410.03907)] [[pdf](https://arxiv.org/pdf/2410.03907)]
> **Authors**: Ying Su,Zhan Ling,Haochen Shi,Jiayang Cheng,Yauwai Yim,Yangqiu Song
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: 13 pages, 9 figures, 8 tables, accepted to EMNLP 2024 main conference
- **标题**: ACTPLAN-1K：基准在家庭活动中视觉语言模型的程序计划能力
- **领域**: 计算语言学
- **摘要**: 大型语言模型〜（LLM）已被用于处理文本任务描述并在体现的AI任务中完成程序计划，因为它们具有强大的推理能力。但是，仍然缺乏研究视觉语言模型〜（VLM）在考虑多模式任务输入时的行为。评估模型在替代任务情况下评估模型推理能力的反事实规划也未被利用。为了评估多模式和反事实方面的计划能力，我们提出了ACTPLAN-1K。 ACTPLAN-1K是一种基于CHATGPT和家庭活动模拟器Igibson2构建的多模式计划基准。基准包括153个活动和1,187个实例。描述一个活动的每个实例都有一个自然语言任务描述和来自模拟器的多个环境图像。每个实例的黄金计划是在提供场景中对象上的动作序列。在典型的VLM上评估了正确性和常识性满意度。事实证明，目前的VLM仍在为制定正常活动和反事实活动的人类水平的程序计划而努力。我们通过对BLEURT模型进行填充，以促进我们的基准研究，进一步提供自动评估指标。

### SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains? 
[[arxiv](https://arxiv.org/abs/2410.03859)] [[cool](https://papers.cool/arxiv/2410.03859)] [[pdf](https://arxiv.org/pdf/2410.03859)]
> **Authors**: John Yang,Carlos E. Jimenez,Alex L. Zhang,Kilian Lieret,Joyce Yang,Xindi Wu,Ori Press,Niklas Muennighoff,Gabriel Synnaeve,Karthik R. Narasimhan,Diyi Yang,Sida I. Wang,Ofir Press
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: SWE基础多模式：AI系统是否概括为视觉软件域？
- **领域**: 计算语言学,人工智能,软件工程
- **摘要**: 现在，用于软件工程的自主系统能够修复错误和开发功能。这些系统通常在SWE-Bench（Jimenez等，2024a）上进行评估，该系统评估了他们从GitHub存储库中解决软件问题的能力。但是，SWE-Bench仅使用Python存储库，而问题陈述主要以文本为主，并且缺少图像等视觉元素。这种有限的覆盖范围激发了我们对现有系统在未代表的软件工程域（例如前端，游戏开发，DevOps）的询问，该系统使用不同的编程语言和范式。因此，我们提出了SWE基础多模式（SWE-Bench M），以评估其在视觉，面向用户的JavaScript软件中修复错误的能力。 SWE-Bench M具有从17个用于Web界面设计，图表，数据可视化，语法突出显示和交互式映射的JavaScript库收集的617个任务实例。每个SWE基础M任务实例在其问题语句或单元测试中至少包含一个图像。我们的分析发现，表现最佳的SWE板台系统与SWE基础M障碍，从而揭示了视觉问题解决和跨语言概括的局限性。最后，我们表明Swe-Agent的灵活语言敏捷的功能使其能够在SWE-Bench M上大大优于替代方案，从而解决了12％的任务实例，而下一个最佳系统的替代方案为6％。

### Self-Powered LLM Modality Expansion for Large Speech-Text Models 
[[arxiv](https://arxiv.org/abs/2410.03798)] [[cool](https://papers.cool/arxiv/2410.03798)] [[pdf](https://arxiv.org/pdf/2410.03798)]
> **Authors**: Tengfei Yu,Xuebo Liu,Zhiyi Hou,Liang Ding,Dacheng Tao,Min Zhang
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: Accepted to EMNLP 2024
- **标题**: 大型语音文本模型的自我驱动的LLM模式扩展
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 大型语言模型（LLMS）在各种任务中表现出色，表明它们通过整合语音能力扩展到大型语音文本模型（LSMS）的潜力。尽管统一的语音文本预培训和多模式数据指导可以提供可观的好处，但这些方法通常需要大量资源需求，并且倾向于过度适合特定的任务。这项研究旨在通过解决香草教学调整的局限性来完善语音数据集用于LSM培训。我们探讨了LSM中的指令跟随动力学，确定了一个关键问题，称为语音锚定偏见 -  LSM倾向于过度依赖语音输入，错误地将整个语音模式解释为指令，从而忽略了文本指令。为了抵消这种偏见，我们引入了一个自动的LSM，该LSM利用该模型本身生成的自动语音识别数据来增强自动语音识别数据，以进行更有效的说明调整。我们在一系列基于语音的任务中进行的实验表明，自源的LSM减轻语音锚偏见，并改善LSMS中语音和文本方式的融合。数据，代码和脚本可在https://github.com/ytf-philp/self-powered-lsm上免费获得。

### CalliffusionV2: Personalized Natural Calligraphy Generation with Flexible Multi-modal Control 
[[arxiv](https://arxiv.org/abs/2410.03787)] [[cool](https://papers.cool/arxiv/2410.03787)] [[pdf](https://arxiv.org/pdf/2410.03787)]
> **Authors**: Qisheng Liao,Liang Li,Yulang Fei,Gus Xia
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-07
> **comment**: 11 pages, 7 figures
- **标题**: CalliffusionV2：具有灵活的多模式控制的个性化自然书法生成
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,多媒体
- **摘要**: 在本文中，我们介绍了CalliffusionV2，这是一种新型系统，旨在生产具有灵活的多模式控制的天然中国书法。与以前仅依赖图像或文本输入并且缺乏细粒度控制的方法不同，我们的系统利用这两个图像都以细粒度的水平和自然语言文本指导世代来描述世代的特征。 CalliffusionV2擅长创建广泛的角色，并可以通过几种学习方法快速学习新样式。它还能够在没有事先培训的情况下生成非中国角色。综合测试证实，我们的系统会产生书法，既是神经网络分类器和人类评估者的风格准确又可以识别的书法。

### ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation 
[[arxiv](https://arxiv.org/abs/2410.03738)] [[cool](https://papers.cool/arxiv/2410.03738)] [[pdf](https://arxiv.org/pdf/2410.03738)]
> **Authors**: Fillipe dos Santos Silva,Gabriel Kenzo Kakimoto,Julio Cesar dos Reis,Marcelo S. Reis
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-07
> **comment**: 15 pages, 10 figures, published in BRACIS 2024 conference
- **标题**: Erasmo：利用大型语言模型来增强聚类分割
- **领域**: 计算语言学,人工智能
- **摘要**: 集群分析在各种领域和应用程序中起着至关重要的作用，例如在营销中的客户细分。这些上下文通常涉及多模式数据，包括表格数据集和文本数据集，这使得代表获得有意义的群集的隐藏模式具有挑战性。这项研究介绍了Erasmo，该框架旨在在文本编码的表格数据上微调一个验证的语言模型，并从微调模型中生成嵌入。 Erasmo使用文本转换器将表格数据转换为文本格式，从而使语言模型能够更有效地处理和理解数据。此外，伊拉斯莫（Erasmo）通过随机特征序列改组和数字言语来产生上下文富含和结构上代表性的嵌入。使用多个数据集和基线方法进行了广泛的实验评估。我们的结果表明，Erasmo完全利用每个表格数据集的特定上下文，从而导致更精确和细微的嵌入以进行准确的聚类。这种方法通过捕获各种表格数据中的复杂关系模式来增强聚类性能。

### Realtime, multimodal invasive ventilation risk monitoring using language models and BoXHED 
[[arxiv](https://arxiv.org/abs/2410.03725)] [[cool](https://papers.cool/arxiv/2410.03725)] [[pdf](https://arxiv.org/pdf/2410.03725)]
> **Authors**: Arash Pakbin,Aaron Su,Donald K. K. Lee,Bobak J. Mortazavi
> **First submission**: 2024-09-29
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 实时，使用语言模型的多模式侵入性通风风险监测并进行框架
- **领域**: 计算语言学
- **摘要**: 目的：重症监护病房（ICU）中对侵入性通气（IV）的实时监测在确保迅速干预和更好的患者结果方面起着至关重要的作用。但是，常规方法通常忽略了仅依靠表格数据的临床注释中嵌入的宝贵见解。在这项研究中，我们提出了一种创新的方法来通过使用语言模型进行文本摘要，通过将临床注释纳入监测管道中来增强静脉风险监测。结果：我们在最新的IV风险监测中报告的所有指标中都取得了卓越的性能，即：AUROC为0.86，AUC-PR为0.35，auct auct为0.86。我们还证明，我们的方法可以在特定时间桶中标记IV时有更多的提前时间。结论：我们的研究强调了将临床笔记和语言模型整合到实时IV风险监测中的潜力，为改善患者护理和知情的ICU环境中的临床决策铺平了道路。

### Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas 
[[arxiv](https://arxiv.org/abs/2410.03181)] [[cool](https://papers.cool/arxiv/2410.03181)] [[pdf](https://arxiv.org/pdf/2410.03181)]
> **Authors**: Seungjong Sun,Eungu Lee,Seo Yeon Baek,Seunghyun Hwang,Wonbyung Lee,Dongyan Nan,Bernard J. Jansen,Jang Hyun Kim
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: EMNLP 2024
- **标题**: 亲吻，踢下：探索具有分配的视觉角色的多模式大语言模型的行为变化
- **领域**: 计算语言学
- **摘要**: 这项研究是第一个探索多模式的大语言模型（LLMS）是否可以使其行为与视觉角色保持一致，从而解决了文献中主要集中于基于文本的角色的显着差距。我们开发了一个新颖的数据集，该数据集是5K虚构的化身图像，以作为LLM的视觉角色分配，并根据这些图像中描述的视觉特征分析了他们的谈判行为，并特别着眼于攻击性。结果表明，LLMS以类似于人类的方式评估图像的侵略性，并在以侵略性的视觉角色提示时输出更具侵略性的谈判行为。有趣的是，当对手的形象看起来不如自己的侵略性时，LLM表现出更具侵略性的谈判行为，而当对手形象显得更具侵略性时，并且侵略性较低。

### Grammar Induction from Visual, Speech and Text 
[[arxiv](https://arxiv.org/abs/2410.03739)] [[cool](https://papers.cool/arxiv/2410.03739)] [[pdf](https://arxiv.org/pdf/2410.03739)]
> **Authors**: Yu Zhao,Hao Fei,Shengqiong Wu,Meishan Zhang,Min Zhang,Tat-seng Chua
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 视觉，语音和文本的语法归纳
- **领域**: 计算语言学,人工智能
- **摘要**: 语法诱导可以受益于丰富的异质信号，例如文本，视觉和声学。在此过程中，来自不同方式的特征本质上彼此扮演着互补的角色。有了这样的直觉，这项工作引入了一种新颖的\ emph {无监督的视觉audio-text语法诱导}任务（名为\ textbf {vat-gi}），从并行图像，文本和语音输入中诱导构成语法树。受到语言语法本地存在于文本以外的事实的启发，我们认为文本不必成为语法诱导中主要的方式。因此，我们进一步介绍了增值税的\ emph {nextless}设置，其中任务仅依赖于视觉和听觉输入。为了完成任务，我们提出了一个视觉审计文本内部外部递归自动编码器（\ textbf {vatiora}）框架，该框架利用丰富的模态特异性和互补的特征来实现有效的语法解析。此外，构建了更具挑战性的基准数据来评估增值税系统的概括能力。两个基准数据集的实验表明，我们提出的Vatiora系统在合并各种多模式信号方面更有效，并且还提出了VAT-GI的新最新性能。

### FluentEditor2: Text-based Speech Editing by Modeling Multi-Scale Acoustic and Prosody Consistency 
[[arxiv](https://arxiv.org/abs/2410.03719)] [[cool](https://papers.cool/arxiv/2410.03719)] [[pdf](https://arxiv.org/pdf/2410.03719)]
> **Authors**: Rui Liu,Jiatian Xi,Ziyue Jiang,Haizhou Li
> **First submission**: 2024-09-28
> **First announcement**: 2024-10-07
> **comment**: submitted for an IEEE publication
- **标题**: fluenteditor2：通过建模多尺度声学和韵律一致性，基于文本的语音编辑
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 基于文本的语音编辑（TSE）允许用户通过直接修改相应的文本而无需更改原始记录来编辑语音。当前的TSE技术通常集中于最大程度地减少训练过程中编辑区域内生成的语音与参考之间的差异，以实现流利的TSE性能。但是，在编辑区域中产生的语音应与未经编辑的地区以及本地和全球层面的原始演讲保持声学和韵律的一致性。 To maintain speech fluency, we propose a new fluency speech editing scheme based on our previous \textit{FluentEditor} model, termed \textit{\textbf{FluentEditor2}}, by modeling the multi-scale acoustic and prosody consistency training criterion in TSE training.具体而言，对于局部声学的一致性，我们提出\ textit {层次局部声学平滑度约束}，以使语音框架，音素和单词的声学特性与编辑的区域中生成的语音与未经编辑的地区的言语之间的边界保持一致。对于全球韵律的一致性，我们提出\ textit {对比全球韵律一致性约束}，以使语音保持在编辑的区域，这与原始话语的韵律保持一致。在VCTK和Libritts数据集上进行的大量实验表明，\ textit {fluentEditor2}超过了基于神经网络的TSE方法，包括Editspeech，CampNet，$^3 $ t，FluentsPeech和我们的流利者，以及我们的流利者，主体和客观。消融研究进一步强调了每个模块对系统整体有效性的贡献。语音演示可在：\ url {https://github.com/ai-s2-lab/fluentededitor2}中获得。

### Multimodal Large Language Models and Tunings: Vision, Language, Sensors, Audio, and Beyond 
[[arxiv](https://arxiv.org/abs/2410.05608)] [[cool](https://papers.cool/arxiv/2410.05608)] [[pdf](https://arxiv.org/pdf/2410.05608)]
> **Authors**: Soyeon Caren Han,Feiqi Cao,Josiah Poon,Roberto Navigli
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Accepted at ACM-MM 2024
- **标题**: 多模式大型语言模型和调谐：视觉，语言，传感器，音频以及其他
- **领域**: 计算语言学
- **摘要**: 本教程探讨了多模式预处理和大型模型的最新进展，该模型能够整合和处理各种数据形式，例如文本，图像，音频和视频。参与者将了解多模式的基础概念，多模式研究的发展以及这些模型所面临的关键技术挑战。我们将介绍最新的多模式数据集和预处理的模型，包括超越视觉和语言的模型。此外，该教程将深入研究多模式大型模型和指令调整策略的复杂性，以优化特定任务的性能。动手实验室将提供最先进的多模式模型的实践经验，展示现实世界中的应用程序，例如视觉讲故事和视觉问题的回答。该教程旨在为研究人员，从业者和新移民提供知识和技能，以利用多模式AI。 ACM Multimedia 2024是本教程的理想场所，与我们理解多模式预处理和大型语言模型及其调整机制的目标完全一致。

### On Instruction-Finetuning Neural Machine Translation Models 
[[arxiv](https://arxiv.org/abs/2410.05553)] [[cool](https://papers.cool/arxiv/2410.05553)] [[pdf](https://arxiv.org/pdf/2410.05553)]
> **Authors**: Vikas Raunak,Roman Grundkiewicz,Marcin Junczys-Dowmunt
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: WMT'24
- **标题**: 关于教学 - 重注神经机器翻译模型
- **领域**: 计算语言学,人工智能
- **摘要**: 在这项工作中，我们介绍了用于神经机器翻译（NMT）模型的指令列，该模型将指令从大语言模型（LLMS）（LLMS）中提取到较小的NMT模型中。我们的NMT模型的指令 - 重新调整配方可以为有限但不同的特定于翻译特定任务集的翻译自定义。我们表明，NMT模型能够同时遵循多个指令，并演示指令零摄像组成的功能。我们还表明，通过指令填充，传统上不同的任务，例如形式控制的机器翻译，多域的适应以及多模式翻译，可以通过单个指令列出的NMT模型共同处理，以与诸如GPT-3.5-TURBO等LLMS相当的性能水平。据我们所知，我们的工作是最早展示传统NMT模型的指导性功能的工作之一，该模型可以更快，更便宜，更有效地提供自定义翻译。

### Grounding Partially-Defined Events in Multimodal Data 
[[arxiv](https://arxiv.org/abs/2410.05267)] [[cool](https://papers.cool/arxiv/2410.05267)] [[pdf](https://arxiv.org/pdf/2410.05267)]
> **Authors**: Kate Sanders,Reno Kriz,David Etter,Hannah Recknor,Alexander Martin,Cameron Carpenter,Jingyang Lin,Benjamin Van Durme
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Preprint; 9 pages; 2024 EMNLP Findings
- **标题**: 在多模式数据中接地部分定义的事件
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 我们如何仅从简短的视频片段就能了解复杂的时事？虽然自然语言可以直接地代表未指定的，部分可观察到的事件，但视觉数据并不能促进类似方法，因此在事件理解中引入了独特的挑战。随着具有视觉能力的AI代理的越来越流行，这些系统必须能够从非结构化视频数据的集合中建模事件。为了在多模式设置中处理强大的事件建模，我们引入了一种多模式的公式，以部分定义的事件，并将这些事件的提取作为三阶段跨度检索任务。我们为此任务提出了一个相应的基准测试，该任务由14.5小时的密集注释的当前事件视频和1,168个文本文档组成，其中包含22.8k标记为中心事件的实体。我们建议通过LLM驱动的方法来完成多模式事件分析的任务，并在多人G上对其进行评估。结果说明了抽象事件理解所带来的挑战，并证明了以事件为中心的视频语言系统中的希望。

### MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.04819)] [[cool](https://papers.cool/arxiv/2410.04819)] [[pdf](https://arxiv.org/pdf/2410.04819)]
> **Authors**: Kaichen Huang,Jiahao Huo,Yibo Yan,Kun Wang,Yutao Yue,Xuming Hu
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: 矿工：在多模式模型中挖掘特定于模式的神经元的基本模式
- **领域**: 计算语言学
- **摘要**: 近年来，多模式的大语言模型（MLLM）已显着提高，将更多的模式集成到各种应用中。但是，缺乏解释性仍然是其在需要决策透明度的情况下使用的主要障碍。当前的神经元级解释范例主要集中于知识本地化或特定于域的分析，从而使多模式的探索在很大程度上没有解决。为了应对这些挑战，我们提出了Miner，这是MLLM中采矿模式特异性神经元（MSN）的可转移框架，该框架包括四个阶段：（1）模态分离，（2）重要性得分计算，（3）重要性得分汇总，（4）模态特异性神经元选择。 Extensive experiments across six benchmarks and two representative MLLMs show that (I) deactivating ONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for Qwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly converge in the lower layers, (III) MSNs influence how key information from various modalities converges to the last令牌，（iv）两个有趣的现象值得进一步研究，即语义探测和语义端粒。源代码可在此URL上找到。

### How Does Vision-Language Adaptation Impact the Safety of Vision Language Models? 
[[arxiv](https://arxiv.org/abs/2410.07571)] [[cool](https://papers.cool/arxiv/2410.07571)] [[pdf](https://arxiv.org/pdf/2410.07571)]
> **Authors**: Seongyun Lee,Geewook Kim,Jiyeon Kim,Hyunji Lee,Hoyeon Chang,Sue Hyun Park,Minjoon Seo
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: Work in Progress
- **标题**: 视力语言适应如何影响视觉语言模型的安全性？
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 视觉语言适应性（VL适应）将大型语言模型（LLMS）转化为多模式任务的大型视觉模型（LVLM），但是此过程通常会损害原始LLM中嵌入的固有的安全能力。尽管安全措施降低了潜在的有害性，但对适应对安全性的影响的深入分析仍然不足。这项研究研究了VL适应如何影响安全性并评估安全微调方法的影响。我们的分析表明，即使训练数据是安全的，在VL适应过程中，安全降解也会发生。尽管安全调整技术，例如通过安全数据集进行微调或从人类反馈中进行的强化学习减轻一些风险，但由于过度拒绝问题，它们仍然会导致安全性下降和帮助下的帮助。对内部模型权重的进一步分析表明，VL适应可能会影响某些与安全相关的层，从而有可能降低总体安全水平。此外，我们的发现表明，VL适应性和安全性调整的目标是不同的，这通常会导致其同时应用是次优的。为了解决这个问题，我们建议使用权重合并方法作为一种最佳解决方案有效地降低了安全性降解，同时保持了帮助。这些见解有助于指导开发更可靠和安全的LVLM，以实现现实世界的应用程序。

### Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs) 
[[arxiv](https://arxiv.org/abs/2410.07507)] [[cool](https://papers.cool/arxiv/2410.07507)] [[pdf](https://arxiv.org/pdf/2410.07507)]
> **Authors**: Abhijit Mishra,Shreya Shukla,Jose Torres,Jacek Gwizdka,Shounak Roychowdhury
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: Accepted to Findings of NAACL 2025
- **标题**: Think2Text：使用大语言模型（LLM）从脑电图信号发电的文本生成
- **领域**: 计算语言学
- **摘要**: 以可理解的形式解码和表达大脑活动是AI中具有挑战性的边界。本文介绍了Thought2Text，该文本使用了用EEG数据微调的指令调整的大语言模型（LLMS）来实现此目标。该方法涉及三个阶段：（1）训练脑电图编码器以进行视觉特征提取，（2）在图像和文本数据上进行微调LLMS，启用多模式描述生成，以及（3）在EEG嵌入式上进行进一步的微调，以在推理过程中直接从EEG中直接从EEG生成文本。对六个具有图像刺激和文本字幕的六名受试者收集的公共脑电图数据集进行的实验证明了多模式LLMS（Llama-V3，Mismtral-V0.3，Qwen2.5）的有效性，使用传统语言生成评估指标，以及易于衡量的度量。这种方法标志着在神经科学和自然语言处理中使用潜在应用的便携式，低成本的“思想到文本”技术的重大进步。

### From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.06795)] [[cool](https://papers.cool/arxiv/2410.06795)] [[pdf](https://arxiv.org/pdf/2410.06795)]
> **Authors**: Yuying Shang,Xinyi Zeng,Yutao Zhu,Xiao Yang,Zhengwei Fang,Jingyuan Zhang,Jiawei Chen,Zinan Liu,Yu Tian
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 从像素到代币：在大型视觉模型中重新审视对象幻觉
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 大型视觉模型（LVLM）中的幻觉是一个重大挑战，即产生视觉输入中未呈现的对象，从而损害了它们的可靠性。最近的研究通常将幻觉归因于对视觉输入的缺乏理解，但忽略了一个更基本的问题：该模型无法有效提取或解除视觉特征。在本文中，我们从架构的角度重新审视LVLM中的幻觉，研究主要原因是视觉编码器（特征提取）还是模态对准模块（特征解耦）。在我们对初步调查的发现的激励下，我们提出了一种新颖的调整策略，以减轻LVLMS的幻觉。可以将这种插件方法集成到各种LVLMS中，利用自适应虚拟令牌从边界框中提取对象功能，从而解决由于视觉功能的脱钩不足而引起的幻觉。补丁在多个多模式幻觉数据集上实现最新性能。我们希望这种方法可以为研究人员提供对LVLMS幻觉的根本原因的更深入的见解，从而促进了该领域的进一步进步和创新。

### To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.06765)] [[cool](https://papers.cool/arxiv/2410.06765)] [[pdf](https://arxiv.org/pdf/2410.06765)]
> **Authors**: Junyan Lin,Haoran Chen,Dawei Zhu,Xiaoyu Shen
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: Accepted to EMNLP 2024 Main Conference
- **标题**: 保存或压缩：多模式模型中连接器选择的深入研究
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 近年来，多模式大语模型（MLLM）引起了行业和学术界的极大关注。但是，关于构建MLLM体系结构的辩论仍然存在很大的争论，尤其是在选择适当的连接器以进行不同粒度的感知任务方面。本文系统地研究了连接器对MLLM性能的影响。具体而言，我们将连接器分类为功能保护和功能压缩类型。利用统一的分类标准，我们将三个综合基准，MMBench，MME和种子基础的子任务分类为三种任务类型：粗粒感，细粒度的感知和推理，并评估表现。我们的发现表明，功能保护连接器在\ emph {细粒感知}任务中出色，因为它们可以保留详细的视觉信息。相比之下，功能压缩连接器虽然在细粒度的感知任务中有效较低，但提供了显着的速度优势，并在\ emph {粗粒感知}和\ emph {clinice}任务中表现出色。这些见解对于指导MLLM体系结构设计和推进MLLM架构的优化至关重要。

### ING-VP: MLLMs cannot Play Easy Vision-based Games Yet 
[[arxiv](https://arxiv.org/abs/2410.06555)] [[cool](https://papers.cool/arxiv/2410.06555)] [[pdf](https://arxiv.org/pdf/2410.06555)]
> **Authors**: Haoran Zhang,Hangyu Guo,Shuyue Guo,Meng Cao,Wenhao Huang,Jiaheng Liu,Ge Zhang
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: 49 pages, 12 figures
- **标题**: ING-VP：MLLM还不能玩简单的基于视觉的游戏
- **领域**: 计算语言学
- **摘要**: 随着多模式大型语言模型（MLLM）继续在各种各样的任务中证明竞争性的越来越具竞争力，因此已经开发出更复杂和全面的基准来评估这些尖端模型。这些基准测试将新的挑战引入核心能力，例如感知，推理和计划。但是，现有的多模式基准在基于图像中的空间关系的多步规划提供了重点评估时缺乏。为了弥合这一差距，我们提出了ING-VP，这是第一个基于游戏的视觉计划基准，该基准是专门设计用于评估MLLM的空间想象力和多步推理能力的。 ING-VP具有6种不同的游戏，包括300个级别，每个级别都有6种独特的配置。单个模型进行了超过60,000轮的互动。基准框架允许进行多个比较设置，包括图像文本与仅文本输入，单步与多步推理，以及历史悠久的与没有历史的条件，为模型功能提供了宝贵的见解。我们评估了众多最先进的MLLM，具有表现最高的模型Claude-3.5十四行诗，平均准确度仅为3.37％，远低于预期的标准。这项工作旨在提供专门的评估框架，以推动MLLM的复杂空间推理和计划的能力提高。该代码可在https://github.com/thisisus7/ing-vp.git上公开获取。

### Chip-Tuning: Classify Before Language Models Say 
[[arxiv](https://arxiv.org/abs/2410.06541)] [[cool](https://papers.cool/arxiv/2410.06541)] [[pdf](https://arxiv.org/pdf/2410.06541)]
> **Authors**: Fangwei Zhu,Dian Li,Jiajun Huang,Gang Liu,Hui Wang,Zhifang Sui
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 芯片调整：在语言模型之前进行分类
- **领域**: 计算语言学,人工智能
- **摘要**: 大语言模型（LLM）表现的快速发展伴随着模型大小的升级，从而导致模型培训和推断的成本不断上升。先前的研究发现，LLMS中的某些层表现出冗余，并且去除这些层仅带来模型性能的边际损失。在本文中，我们采用探测技术来解释LLMS中的层冗余，并证明可以通过探测分类器有效地修剪语言模型。我们提出了Chip-Tuning，这是一个简单有效的结构化修剪框架，专门用于分类问题。芯片调整将名为芯片的微小探测分类器附加到不同的LLM层，并用骨干模型训练芯片。选择分类的芯片后，可以在边际性能损失的情况下去除附件之后的所有层。各种LLM和数据集的实验结果表明，芯片调整的准确性和修剪比显着优于先前的最新基准，可实现高达50％的修剪比率。我们还发现，可以将芯片调整应用于多模式模型，并且可以与模型登录结合使用，证明其出色的兼容性。

### Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.08174)] [[cool](https://papers.cool/arxiv/2410.08174)] [[pdf](https://arxiv.org/pdf/2410.08174)]
> **Authors**: Qingni Wang,Tiantian Geng,Zhiyuan Wang,Teng Wang,Bo Fu,Feng Zheng
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 然后识别样本：在多模式大语言模型中进行风险控制和评估的一般框架
- **领域**: 计算语言学,人工智能,机器学习,多媒体
- **摘要**: 多模式的大语言模型（MLLM）在各种任务中都具有有希望的进步，但它们仍然遇到重大的可信度问题。先前的研究在语言建模中应用拆分保形预测（SCP）来构建具有统计保证的预测集。但是，这些方法通常依赖于内部模型逻辑或仅限于多项选择设置，这会妨碍其在动态的开放式环境中的推广性和适应性。在本文中，我们介绍了Tron，这是一个两步的风险控制和评估框架，适用于任何支持开放式和封闭情况下采样的MLLM。 TRON包括两个主要组成部分：（1）针对最小大小的样本响应集的新型保形得分，以及（2）基于自稳态理论识别高质量响应的非符合性评分，将错误率通过两个特定风险水平控制。此外，我们首次研究了在开放式上下文中预测集中的语义冗余，从而导致基于平均设置大小的MLLM的有希望的评估度量。我们在使用八个MLLM的四个视频提问（VideoQA）数据集中进行的全面实验表明，TRON达到了所需的错误率，这些错误率受两个用户指定的风险级别界定。此外，删除的预测集保持适应性，同时在不同的风险水平下更有效和稳定，以评估风险评估。

### Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2410.08145)] [[cool](https://papers.cool/arxiv/2410.08145)] [[pdf](https://arxiv.org/pdf/2410.08145)]
> **Authors**: Xiaoyuan Liu,Wenxuan Wang,Youliang Yuan,Jen-tse Huang,Qiuzhi Liu,Pinjia He,Zhaopeng Tu
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 洞察视线？探索多模式LLM中的视力知识冲突
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 本文探讨了多模式大语模型（MLLMS）中常识级级视力 - 知识冲突的问题，其中视觉信息与模型的内部共识知识相矛盾（见图1）。为了研究这个问题，我们引入了一条自动管道，并以人类的质量控制增强，以建立一个旨在模拟和评估MLLM冲突的基准。利用这条管道，我们制作了一个诊断基准，其中包括374张原始图像和1,122个高质量的提问（QA）对。该基准涵盖了两种类型的冲突目标和三个问题难度水平，提供了彻底的评估工具。通过这个基准，我们评估了各个模型家族中九个代表性MLLM的冲突解决能力，并发现对文本查询的过分依赖。利用这些发现，我们提出了一种新颖的提示策略，即“ vision”（FOV），该策略显着增强了MLLM的能力，即偏爱视觉数据而不是相互冲突的文本知识。我们的详细分析和新提出的策略大大提高了对MLLM中视力知识冲突的理解和减轻。数据和代码可公开可用。

### InstructBioMol: Advancing Biomolecule Understanding and Design Following Human Instructions 
[[arxiv](https://arxiv.org/abs/2410.07919)] [[cool](https://papers.cool/arxiv/2410.07919)] [[pdf](https://arxiv.org/pdf/2410.07919)]
> **Authors**: Xiang Zhuang,Keyan Ding,Tianwen Lyu,Yinuo Jiang,Xiaotong Li,Zhuoyi Xiang,Zeyuan Wang,Ming Qin,Kehua Feng,Jike Wang,Qiang Zhang,Huajun Chen
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 教学biomol：根据人类的指示推进生物分子的理解和设计
- **领域**: 计算语言学,生物分子
- **摘要**: 了解和设计生物分子，例如蛋白质和小分子，对于推进药物发现，合成生物学和酶工程至关重要。人工智能（AI）的最新突破已经彻底改变了生物分子研究，在生物分子预测和设计方面取得了显着的准确性。但是，使用自然语言将分子复杂性与人类意图保持一致，AI的计算能力和研究人员的直觉之间仍然存在关键差距。大型语言模型（LLMS）显示出可能解释人类意图的潜力，但由于挑战，包括专业知识要求，多模式数据整合以及自然语言和生物分子之间的语义一致性，它们在生物分子研究中的应用仍然偏生。为了解决这些局限性，我们提出了TenchBiomol，这是一种新颖的LLM，旨在通过全面的自然语言，分子和蛋白质的任何一致性来弥合自然语言和生物分子。该模型可以将多模式的生物分子作为输入整合，并使研究人员能够以自然语言阐明设计目标，从而提供满足精确生物学需求的生物分子输出。实验结果表明，指导性植物可以按照人类的指示理解和设计生物分子。值得注意的是，它可以生成具有10％的结合亲和力和设计酶提高10％的药物分子，使ESP得分为70.4，这使其成为超过ESP开发人员建议的60.0酶相互作用阈值的唯一方法。这突出了它改变现实世界生物分子研究的潜力。

### Leveraging Customer Feedback for Multi-modal Insight Extraction 
[[arxiv](https://arxiv.org/abs/2410.09999)] [[cool](https://papers.cool/arxiv/2410.09999)] [[pdf](https://arxiv.org/pdf/2410.09999)]
> **Authors**: Sandeep Sricharan Mukku,Abinesh Kanagarajan,Pushpendu Ghosh,Chetan Aggarwal
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: NAACL 2024
- **标题**: 利用客户反馈进行多模式洞察提取
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,信息检索
- **摘要**: 企业可以从不同方式（例如文本和图像）的客户反馈中受益，以增强其产品和服务。但是，很难在单个通行证中提取可操作和相关的文本段和图像。在本文中，我们提出了一种新型的多模式方法，该方法将图像和文本信息融合在潜在空间中，并使用图像文本接地的文本解码器解码以提取相关的反馈段。我们还引入了一种弱监督的数据生成技术，该技术为这项任务生成培训数据。我们在看不见的数据上评估了我们的模型，并证明它可以从多模式客户的反馈中有效地挖掘可操作的见解，从而超过现有基准的F1分数$ 14美元。

### M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought 
[[arxiv](https://arxiv.org/abs/2410.09220)] [[cool](https://papers.cool/arxiv/2410.09220)] [[pdf](https://arxiv.org/pdf/2410.09220)]
> **Authors**: Gitanjali Kumari,Kirtan Jain,Asif Ekbal
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: 34 Pages. Accepted in The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024). Main Conference
- **标题**: M3HOP-COT：具有多模式多跳线链的厌恶模因识别
- **领域**: 计算语言学,计算机与社会,机器学习
- **摘要**: 近年来，在社交媒体平台上，对女性的仇恨现象显着增加，尤其是通过使用厌恶的模因。这些模因通常针对具有微妙和晦涩的暗示的女性，这使其对自动化系统的检测成为艰巨的任务。最近，大型语言模型（LLMS）在使用思想链（COT）的推理方面显示出令人鼓舞的结果，促使产生中间推理链作为促进多模式任务的理由，但通常会忽略文化多样性和关键方面，例如情感和情感和上下文知识，隐藏在视觉模态中。为了解决这一差距，我们引入了一个多模式的多模式婴儿床（M3HOP-COT）框架，以识别厌恶的模因识别，将基于夹的分类器和一个多模式的COT模块与Entity-Object-Recotation-Recotiation-Recoothip Integnation相结合。 M3HOP-COT采用三步的多模式提示原理来诱导情绪，目标意识和上下文知识进行模因分析。我们的经验评估，包括定性和定量分析，都验证了M3HOP-COT框架在Semeval-2022 Task 5（MAMI任务）数据集中的功效，突出了其在宏F1分数中的强劲性能。此外，我们通过在各种基准模因数据集上对模型进行评估，从而评估了该模型的可推广性，从而对我们在不同数据集中方法的有效性提供了详细的见解。

### Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.09047)] [[cool](https://papers.cool/arxiv/2410.09047)] [[pdf](https://arxiv.org/pdf/2410.09047)]
> **Authors**: Qin Liu,Chao Shang,Ling Liu,Nikolaos Pappas,Jie Ma,Neha Anna John,Srikanth Doss,Lluis Marquez,Miguel Ballesteros,Yassine Benajiba
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: Preprint
- **标题**: 视觉模型的解开和缓解安全对齐降解
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 与LLM骨架相比，视觉模型的安全对准能力（VLM）容易被视觉模块的整合降低。我们在本文中调查了这种现象，称为“安全一致性降解”，并表明挑战是源于在将视觉模式引入VLM时出现的代表差距。特别是，我们表明，多模式输入的表示形式从仅文本输入的表示，这些输入代表了LLM骨架优化的分布。同时，最初在文本嵌入空间内开发的安全对齐功能不会成功地转移到这个新的多模式表示空间。为了减少安全比对降解，我们引入了跨模式表示操作（CMRM），这是一种推理时间表示干预方法，用于恢复VLM的LLM骨架固有的安全对准能力，同时保留VLMS的功能功能。经验结果表明，我们的框架显着恢复了从LLM主链继承的对齐能力，即使没有额外的培训，对预训练的VLM的流利度和语言能力也很小。具体而言，仅在推理时间干预的情况下，多模式输入上LLAVA-7B的不安全率可以从61.53％降低到低至3.15％。警告：本文包含有毒或有害语言的例子。

### RoRA-VLM: Robust Retrieval-Augmented Vision Language Models 
[[arxiv](https://arxiv.org/abs/2410.08876)] [[cool](https://papers.cool/arxiv/2410.08876)] [[pdf](https://arxiv.org/pdf/2410.08876)]
> **Authors**: Jingyuan Qi,Zhiyang Xu,Rulin Shao,Yang Chen,Jin Di,Yu Cheng,Qifan Wang,Lifu Huang
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: RORA-VLM：强大的检索型视觉语言模型
- **领域**: 计算语言学
- **摘要**: 当前的视觉模型（VLMS）在知识密集型任务上仍然表现出劣等的性能，这主要是由于准确地编码视觉对象和场景之间的所有关联的挑战与其相应的实体和背景知识。尽管检索增强方法提供了一种有效的方法来整合外部知识，但将它们扩展到视觉领域，在（1）（1）精确地从外部来源中检索了相关信息，这是由于多模式查询中固有的差异而从外部来源检索相关信息，并且（2）对不相关的，间隔和努力的知识均及时回顾了次数。在这项工作中，我们介绍了Rora-vlm，这是一个专门针对VLMS量身定制的小说且强大的检索增强框架，并具有两个关键的创新：（1）一个2阶段的检索过程，具有图像锚定的文本广播扩展，以协同结合Query和Textual Informity Query和Textual Infortain Query Query和最相关的MultiModal Sneipsalssippers snippets snippets; and (2) a robust retrieval augmentation method that strengthens the resilience of VLMs against irrelevant information in the retrieved multimodal knowledge by injecting adversarial noises into the retrieval-augmented training process, and filters out extraneous visual information, such as unrelated entities presented in images, via a query-oriented visual token refinement strategy.我们进行了广泛的实验，以验证三个广泛采用的基准数据集对我们提出的方法的有效性和鲁棒性。我们的结果表明，RORA-VLM的训练实例最少，使基本模型能够在所有基准测试中实现显着的性能改善，并不断地超过最先进的检索效果VLM，同时还表现出新颖的零击域传递能力。

### Emphasis Rendering for Conversational Text-to-Speech with Multi-modal Multi-scale Context Modeling 
[[arxiv](https://arxiv.org/abs/2410.09524)] [[cool](https://papers.cool/arxiv/2410.09524)] [[pdf](https://arxiv.org/pdf/2410.09524)]
> **Authors**: Rui Liu,Zhenqi Jia,Jie Yang,Yifan Hu,Haizhou Li
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: submitted to IEEE Transaction
- **标题**: 通过多模式多尺度上下文建模的对话文本到语音的强调渲染
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 对话文本到语音（CTTS）旨在准确表达用适当风格的对话环境中的话语，如今吸引了更多的关注。在认识到CTTS任务的重要性的同时，先前的研究尚未彻底调查语音强调表达，这对于传达人机相互作用情景中的潜在意图和态度至关重要，这是由于缺乏对话性强调数据集以及上下文理解中的难度。在本文中，我们为CTTS模型（称为er-ctts）提出了一种新颖的重点渲染方案，其中包括两个主要组成部分：1）我们同时考虑了文本和声学环境，包括全球和本地语义建模，以全球和本地语义建模来全面地了解对话上下文； 2）我们深入整合了多模式和多尺度上下文，以了解上下文对当前话语的强调表达的影响。最后，推断的重点特征被馈入神经语音合成器以产生对话性语音。为了解决数据稀缺性，我们在现有的对话数据集（DailyTalk）上创建强调强度注释。客观评估和主观评估都表明，我们的模型在对话环境中的强调呈现方面优于基线模型。代码和音频示例可在https://github.com/codestoretts/er-ctts上找到。

### GT2Vec: Large Language Models as Multi-Modal Encoders for Text and Graph-Structured Data 
[[arxiv](https://arxiv.org/abs/2410.11235)] [[cool](https://papers.cool/arxiv/2410.11235)] [[pdf](https://arxiv.org/pdf/2410.11235)]
> **Authors**: Jiacheng Lin,Kun Qian,Haoyu Han,Nurendra Choudhary,Tianxin Wei,Zhongruo Wang,Sahika Genc,Edward W Huang,Sheng Wang,Karthik Subbian,Danai Koutra,Jimeng Sun
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: GT2VEC：大型语言模型作为文本和图形结构数据的多模式编码器
- **领域**: 计算语言学
- **摘要**: 图形结构的信息提供了丰富的上下文信息，可以通过提供结构化的关系和层次结构来增强语言模型，从而为各种应用程序（例如检索，问题答案和分类）提供更具表现力的嵌入。但是，现有的集成图形和文本嵌入的方法通常基于多层感知器（MLP）或浅变压器，其能力限制了它​​们完全利用这些模态异质性质的能力。为了克服这一点，我们提出了GT2VEC，这是一个简单而有效的框架，利用大型语言模型（LLMS）共同编码文本和图形数据。具体而言，GT2VEC使用MLP适配器将嵌入项目图嵌入与文本嵌入相同的空间中，从而使LLM可以共同处理这两种模态。与先前的工作不同，我们还引入了对比度学习以更有效地对齐图形和文本空间，从而提高了学习的联合嵌入的质量。跨越三个任务的六个数据集的经验结果：知识将图形上下文化的问题回答，图形对对分类和检索，表明GT2VEC始终胜过现有的基准，从而在多个数据集中实现了重大改进。这些结果突出了GT2VEC在集成图形和文本数据中的有效性。消融研究进一步验证了我们方法的有效性。

### Core Knowledge Deficits in Multi-Modal Language Models 
[[arxiv](https://arxiv.org/abs/2410.10855)] [[cool](https://papers.cool/arxiv/2410.10855)] [[pdf](https://arxiv.org/pdf/2410.10855)]
> **Authors**: Yijiang Li,Qingying Gao,Tianwei Zhao,Bingyang Wang,Haoran Sun,Haiyun Lyu,Dezhi Luo,Hokin Deng
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-15
> **comment**: Website with this $\href{https://growing-ai-like-a-child.github.io/}{link}$
- **标题**: 多模式模型中的核心知识缺陷
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 虽然多模式的大语言模型（MLLM）表现出令人印象深刻的能力，但在高水平的感知和推理上，它们在野外的稳健性仍然落后于人类，并且在对人类直观的简单任务上表现出效力降低。我们研究了这些缺陷源于核心知识的缺乏，从幼儿开始对人类天生的基本认知能力造成的假设。为了探究MLLM中的核心知识表示形式，我们从发展认知科学中汲取灵感，并开发出大规模的基准测试，即cor识别数据集，其中包含12个核心认知概念。我们用10个不同的提示评估了219个模型，从而导致2409个数据点进行分析。我们的发现揭示了早期发展的核心知识缺陷，而模型在高水平认知中表现出了人类可比的表现。此外，我们发现低水平的能力几乎没有缩放，与高级能力形成鲜明对比。最后，我们介绍了一种评估技术，概念黑客，通过该技术，我们证明了MLLM并不能真正迈向核心知识，而是依靠虚幻的理解和捷径学习。 $ \ href {https://growing-ai-like-a-child.github.io/} {link} $的网站。

### Generative AI and Its Impact on Personalized Intelligent Tutoring Systems 
[[arxiv](https://arxiv.org/abs/2410.10650)] [[cool](https://papers.cool/arxiv/2410.10650)] [[pdf](https://arxiv.org/pdf/2410.10650)]
> **Authors**: Subhankar Maity,Aniket Deroy
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Scientific Report (Under Review)
- **标题**: 生成的AI及其对个性化智能辅导系统的影响
- **领域**: 计算语言学,人工智能
- **摘要**: 生成人工智能（AI）正在通过在智能辅导系统（ITS）中启用高度个性化和适应性的学习环境来彻底改变教育技术。该报告深入研究了生成AI的集成，尤其是GPT-4等大型语言模型（LLM），以通过动态内容产生，实时反馈和自适应学习途径来增强个性化教育。我们探索关键应用程序，例如自动化问题的生成，自定义反馈机制以及响应个人学习者需求的交互式对话系统。该报告还解决了重大挑战，包括确保教学准确性，减轻AI模型中的固有偏见以及保持学习者的参与度。未来的方向强调了多模式AI整合，辅导系统中的情商以及AI驱动教育的道德含义的潜在进步。通过综合当前的研究和实际实施，本报告强调了生成AI在创造更有效，公平和引人入胜的教育经验方面的变革潜力。

### MMCFND: Multimodal Multilingual Caption-aware Fake News Detection for Low-resource Indic Languages 
[[arxiv](https://arxiv.org/abs/2410.10407)] [[cool](https://papers.cool/arxiv/2410.10407)] [[pdf](https://arxiv.org/pdf/2410.10407)]
> **Authors**: Shubhi Bansal,Nishit Sushil Singh,Shahid Shafi Dar,Nagendra Kumar
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: MMCFND：多模式的多语言字幕 - 意识到的虚假新闻检测低资源指示语言
- **领域**: 计算语言学
- **摘要**: 通过结合欺骗性文本和图像的操纵策略对虚假信息的广泛传播威胁着可靠的信息来源的完整性。尽管已经进行了使用多模式方法以高资源语言检测假新闻的研究，但低资源的方法指示语言主要依赖于文本分析。这种差异突出了对强大方法的需求，这些方法专门针对多模式假新闻，其中缺乏广泛的数据集和工具给出了一个重要的进步障碍。为此，我们介绍了用于指示假新闻检测（MMIFND）的多模式多语言数据集。精心策划的数据集由28,085个实例组成，分布在印地语，孟加拉语，马拉地语，马拉雅拉姆语，泰米尔语，古吉拉特语和旁遮普语。我们进一步提出了假新闻检测的多模式多语言字幕感知框架（MMCFND）。 MMCFND利用了预先训练的非模态编码器和成对编码器，该模型与视觉和语言保持一致，从而从新闻文章的视觉和文本组成部分中提取了深层表示。基础模型中的多模式融合编码器集成了从其成对编码器得出的文本和图像表示，以生成全面的交叉模态表示。此外，我们生成描述性图像标题，这些图像标题可提供其他上下文来检测不一致和操纵。然后将检索的功能融合并馈入分类器，以确定新闻文章的真实性。策划的数据集可能会在低资源环境中有可能加速研究和开发。对MMIFND进行的彻底实验表明，我们提出的框架优于提取相关的假新闻检测功能的方法。

### OMCAT: Omni Context Aware Transformer 
[[arxiv](https://arxiv.org/abs/2410.12109)] [[cool](https://papers.cool/arxiv/2410.12109)] [[pdf](https://arxiv.org/pdf/2410.12109)]
> **Authors**: Arushi Goel,Karan Sapra,Matthieu Le,Rafael Valle,Andrew Tao,Bryan Catanzaro
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: Demo page: https://om-cat.github.io
- **标题**: omcat：Omni上下文意识到变压器
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 大型语言模型（LLM）在文本生成和理解方面取得了重大进步，最近的进步扩展到了整合视觉和音频输入的多模式LLM。但是，这些模型继续与细粒度，跨模式的时间理解相处，尤其是在将音频和视频流的事件相关联时。我们通过两个关键贡献解决这些挑战：一个新的数据集和模型，分别称为Octav和Omcat。 Octav（Omni上下文和时间音频视频）是一个新颖的数据集，旨在捕获跨音频和视频的事件过渡。其次，omcat（Omni上下文意识变压器）是一个强大的模型，该模型利用了绳索的创新扩展（旋转时间嵌入），以增强时间锚定任务的时间接地和计算效率。通过强大的三阶段训练管线 - 特征对齐，指导调整和八度特异性训练 - 莫卡特在跨模式的时间理解方面表现出色。我们的模型展示了视听问题答案（AVQA）任务和OctAV基准测试的最新性能，并通过全面的实验和消融研究验证了时间推理和跨模式对齐的显着增长。我们的数据集和代码将公开可用。我们演示页面的链接是https://om-cat.github.io。

### MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation 
[[arxiv](https://arxiv.org/abs/2410.11779)] [[cool](https://papers.cool/arxiv/2410.11779)] [[pdf](https://arxiv.org/pdf/2410.11779)]
> **Authors**: Chenxi Wang,Xiang Chen,Ningyu Zhang,Bozhong Tian,Haoming Xu,Shumin Deng,Huajun Chen
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: ICLR 2025
- **标题**: MLLM可以看到吗？减轻幻觉的动态校正解码
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习,多媒体
- **摘要**: 多模式的大语言模型（MLLM）经常表现出幻觉现象，但根本的原因仍然很少理解。在本文中，我们提出了经验分析，发现尽管MLLM错误地在最终输出中生成对象，但它们实际上能够识别前面层中的视觉对象。我们推测这可能是由于语言模型的强大知识培训抑制了视觉信息，从而导致了幻觉。在此激励的情况下，我们为MLLMS Deco提出了一种新型的动态校正解码方法，该方法可自适应地选择适当的前面层，并将知识按比例地集成到最终层以调整输出逻辑。请注意，Deco是模型不可知论，可以与各种经典的解码策略无缝融合并应用于不同的MLLM。我们在广泛使用的基准测试基准上评估了DECO，这表明与基线相比，它可以将幻觉速度降低很大，从而强调了其减轻幻觉的潜力。代码可在https://github.com/zjunlp/deco上找到。

### Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple Instructions 
[[arxiv](https://arxiv.org/abs/2410.11701)] [[cool](https://papers.cool/arxiv/2410.11701)] [[pdf](https://arxiv.org/pdf/2410.11701)]
> **Authors**: Yuhan Fu,Ruobing Xie,Jiazhen Liu,Bangxiang Lan,Xingwu Sun,Zhanhui Kang,Xirong Li
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: The proposed method does not work for up-to-date MLLMs.
- **标题**: 放大镜提示：通过极其简单的说明来应对多模式幻觉
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,多媒体
- **摘要**: 多模式大语言模型（MLLM）的幻觉阻碍了其实际应用。为了解决这个问题，我们提出了一个放大器提示（MAGPROMPT），这是一种简单而有效的方法，可以通过非常简单的说明来解决MLLMS中的幻觉。 MagPrompt基于以下两个关键原则，该原理指导各种有效提示的设计，以证明鲁棒性：（1）MLLM应该更多地关注图像。 （2）当图像与模型的内部知识之间存在冲突时，MLLM应优先考虑图像。 MagPrompt是无训练的，可以应用于开源和封闭源型号，例如GPT-4O和Gemini-Pro。它在许多数据集中的性能都很好，其有效性比VCD等更复杂的方法可比性甚至更好。此外，我们的及时设计原理和实验分析为多模式幻觉提供了宝贵的见解。

### Unveiling the Mystery of Visual Attributes of Concrete and Abstract Concepts: Variability, Nearest Neighbors, and Challenging Categories 
[[arxiv](https://arxiv.org/abs/2410.11657)] [[cool](https://papers.cool/arxiv/2410.11657)] [[pdf](https://arxiv.org/pdf/2410.11657)]
> **Authors**: Tarun Tater,Sabine Schulte im Walde,Diego Frassinelli
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 揭示具体和抽象概念的视觉属性的奥秘：可变性，最近的邻居和具有挑战性的类别
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 一个概念的视觉表示根据其含义及其发生的上下文有很大变化。这对视觉和多模式模型构成了多个挑战。我们的研究着重于具体性，这是一个经过精心研究的词汇 - 语义变量，它用作案例研究来检查视觉表示的可变性。我们依靠与从两个不同数据集提取的大约1,000个抽象和具体概念相关的图像：Bing和YFCC。我们的目标是：（i）评估概念描述中的视觉多样性是否可以可靠地区分具体和抽象概念； （ii）通过最近的邻居分析分析相同概念的多个图像的视觉特征的可变性； （iii）通过对图像进行分类和注释图像来确定促成这种变异性的具有挑战性的因素。我们的发现表明，对于对抽象和具体概念的图像进行分类，诸如颜色和​​纹理之类的基本视觉特征的组合比视觉变压器（VIT）等更复杂的模型所提取的功能更有效。但是，VIT在最近的邻居分析中显示出更好的性能，强调在通过文本以外的模式分析概念变量时，需要仔细选择视觉特征。

### Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2410.11437)] [[cool](https://papers.cool/arxiv/2410.11437)] [[pdf](https://arxiv.org/pdf/2410.11437)]
> **Authors**: Sihang Zhao,Youliang Yuan,Xiaoying Tang,Pinjia He
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: EMNLP 2024 Findings
- **标题**: 艰巨的任务是，但是简单的任务否：揭示了多模式LLMS中的懒惰
- **领域**: 计算语言学,人工智能
- **摘要**: 多模式的大语言模型（MLLM）表现出对现实世界的强烈了解，甚至可以处理复杂的任务。但是，他们仍然在一些直接的视觉提问（VQA）问题上失败。本文深入研究了这个问题，表明在回答有关图像的简单问题（例如是/否问题）时，模型即使可以正确地描述图像也会出现错误。我们将困难和简单问题之间的模型行为差异称为模型懒惰。为了系统地研究模型懒惰，我们手动构建了LazyBench，这是一种基准，其中包括是/否，多项选择，简短答案问题以及与图像中相同主题相关的图像描述任务。基于Lazybench，我们观察到懒惰广泛存在于当前的高级MLLM中（例如GPT-4O，Gemini-1.5-Pro，Claude 3和Llava-V1.5-13B），并且在更强的模型上更为明显。我们还分析了VQA V2（LLAVA-V1.5-13B）基准测试，发现其大约一半的故障情况是由模型懒惰引起的，这进一步强调了确保模型充分利用其功能的重要性。为此，我们对如何减轻懒惰并发现思想链（COT）有效地解决了这个问题的初步探索。

### Self-adaptive Multimodal Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2410.11321)] [[cool](https://papers.cool/arxiv/2410.11321)] [[pdf](https://arxiv.org/pdf/2410.11321)]
> **Authors**: Wenjia Zhai
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 自适应的多模式检索生成一代
- **领域**: 计算语言学
- **摘要**: 传统的检索生成生成（RAG）方法受其依赖固定数量检索的文档的限制，通常会导致破坏任务绩效的不完整或嘈杂的信息。尽管最近的自适应方法缓解了这些问题，但它们在复杂和现实世界中的多模式任务中的应用仍然有限。为了解决这些问题，我们提出了一种新方法，称为自适应多模式检索生成（SAM-RAG），该生成专为多模式上下文量身定制。 SAM-RAG不仅基于输入查询动态过滤相关文档，包括在需要时包括图像标题，还可以验证已检索的文档和输出的质量。广泛的实验结果表明，SAM-RAG在检索准确性和响应产生方面都超过了现有的最新方法。通过进一步的消融实验和有效性分析，SAM-RAG保持了高召回质量，同时改善了多模式抹布任务的整体任务性能。我们的代码可在https://github.com/sam-rag/sam_rag上找到。

### Titanic Calling: Low Bandwidth Video Conference from the Titanic Wreck 
[[arxiv](https://arxiv.org/abs/2410.11434)] [[cool](https://papers.cool/arxiv/2410.11434)] [[pdf](https://arxiv.org/pdf/2410.11434)]
> **Authors**: Fevziye Irem Eyiokur,Christian Huber,Thai-Binh Nguyen,Tuan-Nam Nguyen,Fabian Retkowski,Enes Yavuz Ugan,Dogucan Yaman,Alexander Waibel
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 泰坦尼克号：泰坦尼克号沉船的低带宽视频会议
- **领域**: 计算语言学
- **摘要**: 在本文中，我们报告了2022年夏天进行的交流实验，深入研究泰坦尼克号的残骸。深海水中无法进行无线电传输，并且通信链接依赖于声纳信号。由于声纳信号的带宽较低，并且需要传达可读数据，因此在深海任务中使用文本消息传递。在本文中，我们报告了一个消息传递系统的结果和经验，该系统将语音转换为潜艇中的文本，将短信发送到表面，并将这些消息作为扬声器的合成唇部同步视频重建。在2022年夏天实际潜入泰坦尼克号期间，对所得系统进行了测试。对于这种复杂性和良好质量的系统，我们达到了可接受的延迟。可以在以下链接上找到系统演示视频：https：//youtu.be/c4lym86-5ig

### Empowering Dysarthric Speech: Leveraging Advanced LLMs for Accurate Speech Correction and Multimodal Emotion Analysis 
[[arxiv](https://arxiv.org/abs/2410.12867)] [[cool](https://papers.cool/arxiv/2410.12867)] [[pdf](https://arxiv.org/pdf/2410.12867)]
> **Authors**: Kaushal Attaluri,Anirudh CHVS,Sireesha Chittepu
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-17
> **comment**: 19 pages, 6 figures, 3 tables
- **标题**: 授权违反语音：利用高级LLM进行准确的语音校正和多模式分析
- **领域**: 计算语言学,人工智能
- **摘要**: 构造障碍是由神经系统损害引起的一种运动言语障碍，会影响用于语音产生的肌肉，导致言语含糊，缓慢或难以理解。它影响了全世界数百万个个人，包括患有中风，脑损伤，脑瘫，帕金森氏病和多发性硬化症等状况的人。构造障碍带来了主要的沟通障碍，影响了生活质量和社会互动。本文介绍了一种新颖的方法，可以识别和翻译符号障碍语音，使患有这种情况的人能够更有效地进行交流。我们利用先进的大型语言模型来进行准确的语音校正和多模式情绪分析。首先，使用OpenAi Whisper模型将命运语音转换为文本，然后使用微型开源模型和基准模型（如GPT-4.O，Llama 3.1 70B和Groq AI加速器上的Mismtral 8x7b）进行句子预测。使用的数据集将Torgo数据集与Google语音数据相结合，并为情感上下文手动标记。我们的框架确定了诸如幸福，悲伤，中立，惊喜，愤怒和恐惧之类的情绪，同时以高准确的方式重建了扭曲的言语句子。这种方法在鉴定和解释违反语音的识别和解释方面取得了重大进步。

### WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation 
[[arxiv](https://arxiv.org/abs/2410.12722)] [[cool](https://papers.cool/arxiv/2410.12722)] [[pdf](https://arxiv.org/pdf/2410.12722)]
> **Authors**: João Matos,Shan Chen,Siena Placino,Yingya Li,Juan Carlos Climent Pardo,Daphna Idan,Takeshi Tohyama,David Restrepo,Luis F. Nakayama,Jose M. M. Pascual-Leone,Guergana Savova,Hugo Aerts,Leo A. Celi,A. Ian Wong,Danielle S. Bitterman,Jack Gallifant
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: submitted for review, total of 14 pages
- **标题**: WorldMedQA-V：多模式模型评估的多语言，多模式医学检查数据集
- **领域**: 计算语言学
- **摘要**: 多模式/视觉语言模型（VLM）越来越多地在全球的医疗保健环境中部署，需要坚固的基准，以确保其安全性，功效和公平性。从国家医学检查中得出的多项选择问题和答案（QA）数据集长期以来一直是有价值的评估工具，但是现有的数据集在很大程度上是仅文本的，并且在有限的语言和国家 /地区可用。为了应对这些挑战，我们提出了WorldMedQA-V，这是一种更新的多语言，多模式基准数据集，旨在评估医疗保健中的VLM。 WorldMedqa-V包括568个标记为多项选择QA，并配对来自四个国家（巴西，以色列，日本和西班牙）的568张医学图像，分别涵盖了原始语言，并由本地临床医生进行了验证的英语翻译。在本地语言和英语翻译中提供了常见开放和封闭源模型的基线性能，并提供和没有图像提供的模型。 WorldMedQA-V基准旨在使AI系统与部署的各种医疗保健环境更好地匹配，从而促进更公平，有效和代表性的应用。

### Prompt Compression for Large Language Models: A Survey 
[[arxiv](https://arxiv.org/abs/2410.12388)] [[cool](https://papers.cool/arxiv/2410.12388)] [[pdf](https://arxiv.org/pdf/2410.12388)]
> **Authors**: Zongqian Li,Yinhong Liu,Yixuan Su,Nigel Collier
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 大型语言模型的及时压缩：调查
- **领域**: 计算语言学
- **摘要**: 利用大型语言模型（LLMS）进行复杂的自然语言任务通常需要长期提示来传达详细的要求和信息，从而增加内存使用和推理成本。为了缓解这些挑战，已经提出了多种有效的方法，并迅速压缩具有重大的研究兴趣。该调查概述了及时的压缩技术，分为硬提示方法和软提示方法。首先，比较了这些方法的技术方法，然后对了解其机制的各种方法进行了探索，包括注意优化的观点，参数有效的微调（PEFT），模态积分和新的合成语言。我们还检查了各种及时压缩技术的下游改编。最后，分析了当前及时压缩方法的局限性，并概述了几个未来的方向，例如优化压缩编码器，结合硬和软提示方法以及利用多模式的见解。

### Understanding the Role of LLMs in Multimodal Evaluation Benchmarks 
[[arxiv](https://arxiv.org/abs/2410.12329)] [[cool](https://papers.cool/arxiv/2410.12329)] [[pdf](https://arxiv.org/pdf/2410.12329)]
> **Authors**: Botian Jiang,Lei Li,Xiaonan Li,Zhaowei Li,Xiachong Feng,Lingpeng Kong,Qi Liu,Xipeng Qiu
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 了解LLM在多模式评估基准中的作用
- **领域**: 计算语言学,人工智能
- **摘要**: 多模式大语言模型（MLLM）的快速发展伴随着各种基准测试以评估其能力的发展。但是，这些评估的真实性质以及它们评估多模式推理的程度与仅利用潜在的大语言模型（LLM）骨干的程度尚不清楚。本文对LLM主链在MLLM评估中的作用进行了全面研究，重点介绍了两个关键方面：当前基准真正评估多模式推理的程度以及LLM先验知识对性能的影响。具体而言，我们引入了修改的评估协议，以将LLM骨架的贡献从多模式集成中解散，以及一种自动知识识别技术，用于诊断LLMS是否为相应的多模态问题配备了必要的知识。我们的研究包括四个不同的MLLM基准和八个最先进的MLLM。主要发现表明，即使没有视觉输入，某些基准也可以允许高性能，并且最多50 \％的错误率可以归因于LLM骨架中的世界知识不足，这表明对语言能力非常依赖。为了解决知识缺陷，我们提出了一条知识增强管道，该管道可实现大量绩效的增长，在某些数据集上的提高高达60 \％，导致性能提高了约4倍。我们的工作为LLM主链在MLLM中的作用提供了重要的见解，并强调了对更细微的基准测试方法的需求。

### Learning Multimodal Cues of Children's Uncertainty 
[[arxiv](https://arxiv.org/abs/2410.14050)] [[cool](https://papers.cool/arxiv/2410.14050)] [[pdf](https://arxiv.org/pdf/2410.14050)]
> **Authors**: Qi Cheng,Mert İnan,Rahma Mbarki,Grace Grmek,Theresa Choi,Yiming Sun,Kimele Persaud,Jenny Wang,Malihe Alikhani
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: SIGDIAL 2023
- **标题**: 学习儿童不确定性的多模式提示
- **领域**: 计算语言学,计算机视觉和模式识别,计算机与社会,人机交互
- **摘要**: 理解不确定性在实现共同基础方面起着关键作用（Clark等，1983）。这对于与用户协作以解决问题或通过挑战性概念引导用户的多模式AI系统尤其重要。在这项工作中，我们第一次提出了一个与发展和认知心理学家合作的数据集，目的是研究不确定性的非语言提示。然后，我们对数据进行分析，研究不确定性的不同作用及其与任务难度和性能的关系。最后，我们提出了一个多模式的机器学习模型，该模型在参与者的实时视频剪辑的情况下，可以预测不确定性，我们发现基线多模式变压器模型的改进。这项工作为人类与人类和人类之间的认知协调性研究提供了研究，对手势的理解和产生具有广泛的影响。完成所需的同意书和数据表后，我们的数据和代码的匿名版本将公开可用。

### Learning Metadata-Agnostic Representations for Text-to-SQL In-Context Example Selection 
[[arxiv](https://arxiv.org/abs/2410.14049)] [[cool](https://papers.cool/arxiv/2410.14049)] [[pdf](https://arxiv.org/pdf/2410.14049)]
> **Authors**: Chuhong Mai,Ro-ee Tal,Thahir Mohamed
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: Accepted to NeurIPS 2024 Table Representation Learning workshop
- **标题**: 学习元数据 - 敏捷表示，用于文本到sql中文示例选择
- **领域**: 计算语言学
- **摘要**: 内部文化学习（ICL）是一个强大的范式，其中大语言模型（LLMS）受益于提示中的任务演示。但是，选择最佳演示并不是一件容易的事，尤其是对于在输入和输出分布不同的复杂或多模式任务中。我们假设形成输入的特定任务表示是关键。在本文中，我们提出了一种将自然语言问题和SQL查询的表示形式相结合的方法。我们的技术被称为MARLO-METADATA-AGNOSTIC表示文本到SQL的学习 - 使用查询结构来建模意图，而不会在基础数据库元数据上过度索引（即表，列，列或域或域特异性实体的数据库或查询中引用的数据库特定实体）。这使Marlo可以选择与任务相关的结构和语义上相关的示例，而不是与某个域或问题相关的示例。当使用问题相似性检索示例时，与蜘蛛基准上的通用嵌入模型（平均+2.9 \％pt。在执行精度中+2.9 \％pt。）相比，MARLO显示出卓越的性能。它还胜过将元数据信息掩盖+0.8 \％pt的下一个最佳方法。在执行准确性中，同时施加明显降低的推理潜伏期。

### Generating Signed Language Instructions in Large-Scale Dialogue Systems 
[[arxiv](https://arxiv.org/abs/2410.14026)] [[cool](https://papers.cool/arxiv/2410.14026)] [[pdf](https://arxiv.org/pdf/2410.14026)]
> **Authors**: Mert İnan,Katherine Atwell,Anthony Sicilia,Lorna Quandt,Malihe Alikhani
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2024) Industry Track
- **标题**: 在大规模对话系统中生成签名的语言说明
- **领域**: 计算语言学,人工智能,计算机与社会,人机交互
- **摘要**: 我们介绍了一种以目标为导向的对话性AI系统，并通过美国手语（ASL）说明增强，并在全球多模式对话AI平台上介绍了此类系统的首次实现。通过基于触摸的接口访问，我们的系统从用户接收输入，并通过利用检索方法和基于认知的光泽翻译来无缝生成ASL指令。我们设计的核心是由大型语言模型提供动力的标志翻译模块，以及基于令牌的视频检索系统，用于传递食谱和Wikihow指南的教学内容。我们的发展过程深深植根于对社区参与的承诺，结合了聋哑和听力障碍社区的见解以及认知和ASL学习科学专家。我们的签名指令的有效性通过用户反馈验证，与系统中的不签名变体中的评级相当。此外，我们的系统在检索准确性和文本生成质量方面表现出了出色的性能，该质量由BertScore等指标衡量。我们已经在https://github.com/merterm/signed-dialogue上公开访问代码库和数据集，并且可以通过https://huggingface.co/spaces/merterm/signed-signed-instructions获得我们签名的指令视频检索系统的演示。

### Can MLLMs Understand the Deep Implication Behind Chinese Images? 
[[arxiv](https://arxiv.org/abs/2410.13854)] [[cool](https://papers.cool/arxiv/2410.13854)] [[pdf](https://arxiv.org/pdf/2410.13854)]
> **Authors**: Chenhao Zhang,Xi Feng,Yuelin Bai,Xinrun Du,Jinchang Hou,Kaixin Deng,Guangzeng Han,Qinrui Li,Bingli Wang,Jiaheng Liu,Xingwei Qu,Yifei Zhang,Qixuan Zhao,Yiming Liang,Ziqiang Liu,Feiteng Fang,Min Yang,Wenhao Huang,Chenghua Lin,Ge Zhang,Shiwen Ni
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 32 pages,18 figures. Project Page: https://cii-bench.github.io/ Code: https://github.com/MING_X/CII-Bench Dataset: https://huggingface.co/datasets/m-a-p/CII-Bench
- **标题**: MLLM可以理解中国图像背后的深刻含义吗？
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,计算机与社会
- **摘要**: 随着多模式大语言模型（MLLM）的能力继续提高，对MLLM的高阶能力评估的需求正在增加。但是，缺乏评估MLLM高阶感知和对中国视觉内容的理解的工作。为了填补空白，我们介绍了** c ** hinese ** i ** mage ** i ** mplication理解**板凳** mark，** cii-bench **，旨在评估中国图像MLLM的高阶感知和理解能力。与现有基准相比，CII板凳以几种方式脱颖而出。首先，为了确保中国背景的真实性，CII基础上的图像来自中国互联网并手动审查，还可以手动制作相应的答案。此外，CII板凳还结合了代表中国传统文化的图像，例如著名的中国传统绘画，可以深刻地反映模型对中国传统文化的理解。通过对多个MLLM的CII基础台的广泛实验，我们做出了重要的发现。最初，在CII基础上的MLLM和人类的性能之间观察到了一个很大的差距。 MLLM的最高精度达到64.4％，在人类的准确性平均为78.2％，达到令人印象深刻的81.0％的峰值。随后，MLLM在中国传统文化图像上的表现较差，这表明其理解高级语义的能力和缺乏中国传统文化的深刻知识基础的能力有限。最后，观察到，当将图像情感提示纳入提示时，大多数模型都表现出增强的精度。我们认为，CII板凳将使MLLM能够更好地了解中国语义和中文特定的图像，从而促进迈向人工通用情报（AGI）的旅程。我们的项目可在https://cii-bench.github.io/上公开获取。

### Retrospective Learning from Interactions 
[[arxiv](https://arxiv.org/abs/2410.13852)] [[cool](https://papers.cool/arxiv/2410.13852)] [[pdf](https://arxiv.org/pdf/2410.13852)]
> **Authors**: Zizhao Chen,Mustafa Omer Gul,Yiwei Chen,Gloria Geng,Anne Wu,Yoav Artzi
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 回顾性从互动中学习
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 大语言模型（LLM）和用户之间的多转交互作用自然包含隐式反馈信号。如果LLM以意想不到的方式对指令做出响应，则用户可能会通过重新调整请求，表达挫败感或转向替代任务来发出信号。此类信号是独立于任务的，并且占据了相对受约束的语言子空间，即使LLM在实际任务上失败，LLM也可以识别它们。这为在没有其他注释的情况下不断学习的途径创造了一种途径。我们引入尊重，这是一种通过回顾从过去的交互中学习的方法。我们在新的多模式互动场景中部署尊重，在其中人类指示LLM使用组合解决方案空间解决抽象的推理任务。通过与人类的数千互动，我们展示了尊重如何逐渐将任务完成率从31％提高到82％，而无需任何外部注释。

### GeoCoder: Solving Geometry Problems by Generating Modular Code through Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.13510)] [[cool](https://papers.cool/arxiv/2410.13510)] [[pdf](https://arxiv.org/pdf/2410.13510)]
> **Authors**: Aditya Sharma,Aman Dalmia,Mehran Kazemi,Amal Zouaq,Christopher J. Pal
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 地理编码器：通过视觉模型生成模块化代码来解决几何问题
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 几何解决问题需要高级推理能力来处理多模式输入并有效地采用数学知识。视觉模型（VLM）在各种多模式任务中取得了重大进展。然而，它们仍然在几何问题上挣扎，并且由于无法执行在预训练期间看不到的数学操作而受到显着限制，例如计算任意角度的余弦以及在正确应用相关几何公式方面的困难。为了克服这些挑战，我们提出了地理编码器，该地理编码器利用模块化代码 - 字符来使用预定义的几何函数库生成和执行代码。通过执行代码，我们实现了准确和确定性的计算，与自回归令牌预测的随机性质进行了对比，而函数库将公式使用中的错误最小化。我们还提出了一个名为rag-Geocoder的多模式检索仪式的变体，该变体称为rag-geocoder，该变体结合了一个非参数存储器模块，用于从几何库中检索功能，从而降低了对参数存储的依赖。我们的模块化代码 - 触发方法增强了VLM的几何推理能力，与其他填充方法相比，在Geomverse数据集上的各种问题复杂性中，平均提高了16％以上。

### Parameter-efficient Adaptation of Multilingual Multimodal Models for Low-resource ASR 
[[arxiv](https://arxiv.org/abs/2410.13445)] [[cool](https://papers.cool/arxiv/2410.13445)] [[pdf](https://arxiv.org/pdf/2410.13445)]
> **Authors**: Abhishek Gupta,Amruta Parulekar,Sameep Chattopadhyay,Preethi Jyothi
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 低资源ASR的多语言多模型的参数改编
- **领域**: 计算语言学,人工智能,机器学习,音频和语音处理
- **摘要**: 由于培训数据的稀缺性，低资源语言的自动语音识别（ASR）仍然是一个挑战。参数有效的微调和仅文本适应是两种流行的方法，用于解决此类低资源设置。在这项工作中，我们研究了如何使用多语言多模型（如SeamlessM4T）有效地组合这些技术。多模式模型能够通过仅文本适应来利用未标记的文本，并进一步的参数效率ASR微调，从而提高ASR性能。我们还从高资源语言中展示了跨语性的转移，在零拍设置中，在没有任何标记的语音的情况下，在基线上实现了相对17％的降低。

### Representation Learning of Structured Data for Medical Foundation Models 
[[arxiv](https://arxiv.org/abs/2410.13351)] [[cool](https://papers.cool/arxiv/2410.13351)] [[pdf](https://arxiv.org/pdf/2410.13351)]
> **Authors**: Vijay Prakash Dwivedi,Viktor Schlegel,Andy T. Liu,Thanh-Tung Nguyen,Abhinav Ramesh Kashyap,Jeng Wei,Wei-Hsian Yin,Stefan Winkler,Robby T. Tan
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: NeurIPS 2024 Workshop on Unifying Representations in Neural Models (UniReps 2024)
- **标题**: 医学基础模型的结构化数据的代表性学习
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 大型语言模型（LLMS）在包括医疗保健在内的各个领域都表现出了出色的表现。但是，它们有效地表示结构化非文本数据的能力，例如ICD-10或Snomed-CT等记录中使用的字母数字医学法规，在最近的研究中尤其受到暴露。本文研究了由于当前的令牌方法的缺点，LLMS处理医疗法规所面临的挑战。结果，我们介绍了统一的体系结构，以设计一个多模式的医学基础基础模型，该模型是非结构化的文本和结构化数据，该模型通过专门针对结构化的医疗代码调整子词令牌技术来解决这些挑战。我们的方法是通过模型预培训在广泛的内部医疗数据库和结构性医疗记录的公共存储库中验证的。在内部医疗数据库上接受了超过10亿个代币的培训，提议的模型在评估指标上的提高了23％，归因于我们提出的令牌化大约2％。此外，当对Ehrshot公共基准进行评估，并使用1/1000个前培训数据进行评估时，统一模型可改善42％的下游任务的性能。我们的方法不仅增强了以患者为中心的模型的表示和概括能力，而且还弥合了表示模型​​的能力，即在表示复杂的结构化医疗数据以及非结构化文本以及非结构化文本的能力中占据了关键的差距。

### AMPLE: Emotion-Aware Multimodal Fusion Prompt Learning for Fake News Detection 
[[arxiv](https://arxiv.org/abs/2410.15591)] [[cool](https://papers.cool/arxiv/2410.15591)] [[pdf](https://arxiv.org/pdf/2410.15591)]
> **Authors**: Xiaoman Xu,Xiangrun Li,Taihang Wang,Ye Jiang
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 足够：情绪感知的多模式融合提示促进假新闻检测
- **领域**: 计算语言学,人工智能
- **摘要**: 由于其多样性和复杂性，在大型数据集中检测虚假新闻是具有挑战性的，传统方法通常集中在文本功能上，同时不足以说明语义和情感元素。当前的方法还严重依赖大型注释数据集，从而在更细微的分析中限制了它们的有效性。为了应对这些挑战，本文介绍了情感 -  \ textbf {a} Ware \ textbf {M} Ultodal Fusion \ textbf {p} rompt \ rompt \ textbf {l} \ textbf {l} \ textbf {e} arnning（\ textbf {ample}）框架以上录制了综合词组的框架和综合序列。该框架通过利用情绪分析工具从文本中提取情感元素。然后，它采用多头交叉注意（MCA）机制和相似性感知的融合方法来整合多模式数据。提出的足够的框架表明，在少数图和数据范围内的两个公共数据集上表现出强劲的性能，结果表明了虚假新闻检测中情感方面的潜力。此外，该研究探讨了将大型语言模型与这种方法进行文本情感提取的影响的影响，从而揭示了大量进一步改进的空间。代码可以在：\ url {https://github.com/xxm1215/mmm2025_few-shot/

### SceneGraMMi: Scene Graph-boosted Hybrid-fusion for Multi-Modal Misinformation Veracity Prediction 
[[arxiv](https://arxiv.org/abs/2410.15517)] [[cool](https://papers.cool/arxiv/2410.15517)] [[pdf](https://arxiv.org/pdf/2410.15517)]
> **Authors**: Swarang Joshi,Siddharth Mavani,Joel Alex,Arnav Negi,Rahul Mishra,Ponnurangam Kumaraguru
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: SceneGrammi：用于多模式错误信息真实性预测的场景增强图形的混合融合
- **领域**: 计算语言学
- **摘要**: 错误信息破坏了个人知识并影响更广泛的社会叙事。尽管对研究界对多模式错误信息检测的兴趣日益兴趣，但现有方法在捕获多模式数据集中的语义提示，关键区域和跨模式相似性方面表现出局限性。我们提出了SceneGrammi，这是一种用于多模式错误信息真实性预测的场景增强的混合融合方法，该方法集成了跨不同模态的场景图以提高检测性能。四个基准数据集的实验结果表明，场景格雷米始终优于最先进的方法。在一项全面的消融研究中，我们强调了每个组件的贡献，而shapley值则用于检查模型决策过程的解释性。

### RoMemes: A multimodal meme corpus for the Romanian language 
[[arxiv](https://arxiv.org/abs/2410.15497)] [[cool](https://papers.cool/arxiv/2410.15497)] [[pdf](https://arxiv.org/pdf/2410.15497)]
> **Authors**: Vasile Păiş,Sara Niţă,Alexandru-Iulius Jerpelea,Luca Pană,Eric Curea
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: 12 pages, 7 tables, 1 figure, submitted to The 19th International Conference on Linguistic Resources and Tools for Natural Language Processing (ConsILR 2024)
- **标题**: romemes：罗马尼亚语言的多模式模因语料库
- **领域**: 计算语言学
- **摘要**: 模因在在线媒体中越来越受欢迎，尤其是在社交网络中。他们通常将图形表示（图像，图纸，动画或视频）与文本结合在一起，以传达强大的消息。为了提取，处理和理解消息，AI应用程序需要采用多模式算法。在本文中，我们在罗马尼亚语言中介绍了一个策划的真实模因的数据集，具有多个注释级别。使用基线算法来证明数据集的可用性。结果表明，需要进一步的研究以提高面对Internet模因的AI工具的处理能力。

### CROPE: Evaluating In-Context Adaptation of Vision and Language Models to Culture-Specific Concepts 
[[arxiv](https://arxiv.org/abs/2410.15453)] [[cool](https://papers.cool/arxiv/2410.15453)] [[pdf](https://arxiv.org/pdf/2410.15453)]
> **Authors**: Malvina Nikandrou,Georgios Pantazopoulos,Nikolas Vitsakis,Ioannis Konstas,Alessandro Suglia
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: CROPE：评估视觉和语言模型对特定文化概念的文化适应
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 随着视觉和语言模型（VLM）正在吸引全球用户，评估他们的文化理解已成为一个关键挑战。在本文中，我们介绍了Crope，这是一个视觉问题，回答旨在探究特定文化概念知识的基准测试，并通过上下文信息评估文化适应能力。这使我们能够区分训练期间获得的参数知识以及通过视觉和文本描述推断期间提供的上下文知识。我们对几种最新开放式VLM的评估显示了参数设置中特定文化和共同概念之间的巨大绩效差异。此外，具有上下文知识的实验表明，模型难以有效利用多模式信息并将特定于文化的概念与其描述结合在一起。我们的发现揭示了当前VLM的文化理解和适应性的局限性，这些VLM需要解决更多文化包容模型。

### Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant 
[[arxiv](https://arxiv.org/abs/2410.15316)] [[cool](https://papers.cool/arxiv/2410.15316)] [[pdf](https://arxiv.org/pdf/2410.15316)]
> **Authors**: Alan Dao,Dinh Bach Vu,Huy Hoang Ha
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: Ichigo：混合模式的早期融合实时语音助手
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 大型语言模型（LLM）彻底改变了自然语言处理，但是由于音频和文本模式的复杂性，它们对基于语音的任务的应用仍然具有挑战性。本文介绍了Ichigo，这是一种混合模型，无缝处理语音和文本的交织序列。 Ichigo利用令牌化的早期融合方法，将语音量化为离散令牌，并采用基于统一的变压器架构来进行语音和文本方式。此方法使跨模态的联合推理和生成无需单独的适配器。我们提出了一种全面的培训方法，包括对多语言语音识别数据集进行预训练以及在精选的指令数据集上进行微调。 Ichigo在语音提问基准测​​试方面展示了最先进的表现，表现优于现有的开源语音语言模型，并取得与级联系统的可比结果。值得注意的是，Ichigo的潜伏期仅为第一个令牌生成111毫秒，明显低于当前模型。我们的方法不仅可以发展多模式AI领域，而且还为较小的研究团队提供了一个有效贡献开源语音语言模型的框架。

### Enhancing Multimodal Sentiment Analysis for Missing Modality through Self-Distillation and Unified Modality Cross-Attention 
[[arxiv](https://arxiv.org/abs/2410.15029)] [[cool](https://papers.cool/arxiv/2410.15029)] [[pdf](https://arxiv.org/pdf/2410.15029)]
> **Authors**: Yuzhe Weng,Haotian Wang,Tian Gao,Kewei Li,Shutong Niu,Jun Du
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 通过自我验证和统一的跨注意力增强多模式分析，以丢失模式
- **领域**: 计算语言学,人工智能
- **摘要**: 在多模式情感分析中，由于注释成本更高，自动语音识别（ASR）质量不一致，收集文本数据通常比视频或音频更具挑战性。为了应对这一挑战，我们的研究开发了一个强大的模型，该模型即使在没有文本模式的情况下，也可以有效整合多模式情感信息。具体而言，我们已经开发了一个双流式自我验证框架，包括统一的模态交叉注意（UMCA）和模态想象自动编码器（MIA），它在处理这两种情况方面都具有完整的方式以及缺少文本方式的情况。详细说明，当缺少文本模式时，我们的框架使用基于LLM的模型来模拟音频模式中的文本表示，而MIA模块为其他两种模式提供了信息，以使模拟文本表示形式类似于实际文本表示。为了进一步调整模拟和真实表示形式，并使模型能够捕获情感回归任务中样本顺序的连续性质，我们还引入了Rank-N对比度（RNC）损耗函数。在CMU-Mosei进行测试时，我们的模型在MAE上取得了出色的性能，并且在缺少文本模式时大大优于其他模型。该代码可在以下网址找到：https：//github.com/warmcongee/sdumc

### DM-Codec: Distilling Multimodal Representations for Speech Tokenization 
[[arxiv](https://arxiv.org/abs/2410.15017)] [[cool](https://papers.cool/arxiv/2410.15017)] [[pdf](https://arxiv.org/pdf/2410.15017)]
> **Authors**: Md Mubtasim Ahasan,Md Fahim,Tasnim Mohiuddin,A K M Mahbubur Rahman,Aman Chadha,Tariq Iqbal,M Ashraful Amin,Md Mofijul Islam,Amin Ahsan Ali
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: DM-CODEC：蒸馏多模式表示语音令牌化
- **领域**: 计算语言学,人工智能,声音,音频和语音处理
- **摘要**: 语音语言模型的最新进展已在语音令牌化和综合方面取得了重大改善。但是，有效地将语音的多维属性映射到离散代币中仍然具有挑战性。此过程需要声音，语义和上下文信息，以进行精确的语音表示。现有的语音表示形式通常分为两类：来自音频编解码器和语义令牌的声学令牌，来自语音自我监督学习模型。尽管最近的努力统一了声学和语义令牌以提高性能，但它们忽略了上下文表示在综合语音建模中的关键作用。我们的实证研究表明，缺乏上下文表示会导致单词错误率升高（WER）和语音转录中的单词信息丢失（WIL）。 To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. DM-Codec体系结构采用了带有残差矢量量化器（RVQ）的简化编码器框架，并在训练过程中结合了LM和SM。实验表明，DM-Codec的表现明显优于最先进的语音象征化模型，将WER降低了13.46％，WIL下降了9.82％，并将语音质量提高了5.84％，并且在LibrisPeech基准基准Dataset上将语音质量提高了1.85％。代码，样本和模型检查点可在https://github.com/mubtasimahasan/dm-codec上找到。

### SemiHVision: Enhancing Medical Multimodal Models with a Semi-Human Annotated Dataset and Fine-Tuned Instruction Generation 
[[arxiv](https://arxiv.org/abs/2410.14948)] [[cool](https://papers.cool/arxiv/2410.14948)] [[pdf](https://arxiv.org/pdf/2410.14948)]
> **Authors**: Junda Wang,Yujan Ting,Eric Z. Chen,Hieu Tran,Hong Yu,Weijing Huang,Terrence Chen
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: Semihvision：使用半人类注释数据集和微调指令生成增强医疗多模型
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）取得了长足的进步，但由于有限的专业知识，它们在医疗领域面临挑战。尽管最近的医疗MLLM在实验室环境中表现出强劲的表现，但它们经常在现实世界中的应用中挣扎，这突出了研究和实践之间的巨大差距。在本文中，我们试图在端到端学习管道的各个阶段解决此差距，包括数据收集，模型微调和评估。在数据收集阶段，我们介绍了Semihvision，该数据集将人类注释与自动增强技术相结合，以改善医学知识表示和诊断推理。对于模型进行微调，我们培训了PMC-Cambrian-8B-AN超过2400 H100 GPU小时，导致性能超过了Huatuogpt-Vision-34b（79.0％vs. 66.7％），例如Claude3-Opus（55.7％）等公共医疗模型（79.0％vs.66.7％），例如SLAKE和VQ，slake and VQ均在传统的基础上进行。在评估阶段，我们观察到传统的基准不能准确反映现实的临床任务功能。为了克服这一局限性并为模型评估提供了更有针对性的指导，我们引入了JAMA临床挑战，这是一种专门旨在评估诊断推理的新型基准。在此基准上，PMC-Cambrian-AN以1.29的GPT-4分数实现最先进的性能，表现明显优于Huatuogpt-Vision-34b（1.13）和Claude3-Opus（1.17），表明其出色的诊断能力。

### MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps 
[[arxiv](https://arxiv.org/abs/2410.14668)] [[cool](https://papers.cool/arxiv/2410.14668)] [[pdf](https://arxiv.org/pdf/2410.14668)]
> **Authors**: Xiongtao Zhou,Jie He,Lanyu Chen,Jingyu Li,Haojing Chen,Víctor Gutiérrez-Basulto,Jeff Z. Pan,Hanjie Chen
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: NAACL 2025
- **标题**: Miceval：通过图像描述和推理步骤揭示思想质量的多模式链
- **领域**: 计算语言学
- **摘要**: 多模式的思想链（MCOT）是一种流行的提示策略，可在一系列复杂的推理任务中改善多模式大语模型（MLLM）的性能。尽管它很受欢迎，但仍有显着的自动化方法来评估MCOT中推理步骤的质量。为了解决这一差距，我们提出了多模式链评估（Miceval），该框架旨在通过评估描述的质量和每个推理步骤来评估推理链的正确性。描述组件的评估侧重于图像描述的准确性，而推理步骤评估每个步骤的质量，因为它是根据上述步骤有条件生成的。 Miceval建立在一个细粒度数据集的基础上，并具有根据正确性，相关性和信息性对每个步骤进行评分的注释。对四个最先进的MLLM的广泛实验表明，与基于余弦相似性或微调方法的现有方法相比，使用Miceval的逐步评估与人类判断更加紧密地对齐。可以在https://github.com/alenai97/miceval中找到miceval数据集和代码。

### SwaQuAD-24: QA Benchmark Dataset in Swahili 
[[arxiv](https://arxiv.org/abs/2410.14289)] [[cool](https://papers.cool/arxiv/2410.14289)] [[pdf](https://arxiv.org/pdf/2410.14289)]
> **Authors**: Alfred Malengo Kondoro
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: Swaquad-24：Swahili的QA基准数据集
- **领域**: 计算语言学,人工智能
- **摘要**: 本文提出了斯瓦希里语问题回答（QA）基准数据集的创建，旨在解决斯瓦希里语在自然语言处理（NLP）中的代表性不足。该数据集从诸如小队，胶水，肯斯夸夸德和克鲁（Klue）之类的既定基准中得出，这些基准将着重于提供高质量的，带注释的问答对，以捕获斯瓦希里语的语言多样性和复杂性。该数据集旨在支持各种应用程序，包括机器翻译，信息检索和医疗保健聊天机器人等社会服务。道德考虑，例如数据隐私，缓解和包容性，对数据集开发至关重要。此外，本文概述了未来的扩展计划，包括特定于域的内容，多模式集成以及更广泛的众包工作。斯瓦希里语质量保证数据集旨在促进东非的技术创新，并为NLP研究和低资源语言的应用提供必不可少的资源。

### Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model 
[[arxiv](https://arxiv.org/abs/2410.14225)] [[cool](https://papers.cool/arxiv/2410.14225)] [[pdf](https://arxiv.org/pdf/2410.14225)]
> **Authors**: Li Yuan,Yi Cai,Junsheng Huang
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: accepted by ACM MM 2024
- **标题**: 通过知识增强的跨模式及时模型，几乎没有射击的联合多模式关系提取
- **领域**: 计算语言学,人工智能
- **摘要**: 联合多模式实体关系提取（JMere）是一项艰巨的任务，旨在从社交媒体帖子中的文本图像对中提取实体及其关系。现有的Jmere方法需要大量的标记数据。但是，收集和注释的Jmere的细粒多模式数据构成了重大挑战。最初，我们构建了安装在原始数据分布的多种模式的多模式的多式联运数据集。要在几次设置中解决不足的信息，我们介绍了\ textbf {k} nowledge- \ textbf {e} nhanged \ textbf {c} ross-modal \ textbf {p} rompt \ textbf \ textbf \ textbf {m} odel（kecpm）for Jmere for Jmere。通过指导大型语言模型来产生补充背景知识，该方法可以有效地解决几次弹奏设置中信息的问题。我们提出的方法包括两个阶段：（1）知识摄入阶段，该阶段基于语义相似性，动态提示提示，指南Chatgpt生成相关知识并采用自我反思来完善知识； （2）一个知识增强的语言模型阶段，将辅助知识与原始输入合并，并利用基于变压器的模型与Jmere所需的输出格式保持一致。我们在Jmere数据集中得出的几个弹出数据集上广泛评估了我们的方法，从微基准和宏F $ _1 $分数方面，证明了其优于强基础的优势。此外，我们提出定性分析和案例研究，以阐明我们的模型的有效性。

### MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems 
[[arxiv](https://arxiv.org/abs/2410.14179)] [[cool](https://papers.cool/arxiv/2410.14179)] [[pdf](https://arxiv.org/pdf/2410.14179)]
> **Authors**: Zifeng Zhu,Mengzhao Jia,Zhihan Zhang,Lang Li,Meng Jiang
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: NAACL 2025, 19 pages, 10 figures
- **标题**: Multichartqa：关于多创态问题的基准测试视觉模型
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在各种任务中都表现出了令人印象深刻的能力，包括视觉问题回答和图表理解，但与图表相关任务的现有基准测试在捕获现实世界多创态场景的复杂性方面缺乏。当前的基准主要集中在单局任务上，忽略了从多个图表中提取和集成信息所需的多跳推理，这在实际应用中至关重要。为了填补这一空白，我们介绍了Multichartqa，这是一种评估MLLM在四个关键领域的功能的基准：直接问题回答，并行问题回答，比较推理和顺序推理。我们对广泛的MLLM的评估揭示了与人类相比的显着性能差距。这些结果突出了多制定理解的挑战以及多章QA推动该领域进步的潜力。我们的代码和数据可从https://github.com/zivenzhu/multi-chart-qa获得

### DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding 
[[arxiv](https://arxiv.org/abs/2410.16472)] [[cool](https://papers.cool/arxiv/2410.16472)] [[pdf](https://arxiv.org/pdf/2410.16472)]
> **Authors**: Manan Suri,Puneet Mathur,Franck Dernoncourt,Rajiv Jain,Vlad I Morariu,Ramit Sawhney,Preslav Nakov,Dinesh Manocha
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: EMNLP 2024 (Main)
- **标题**: 文档V2：通过多模式LLM接地编辑文档结构
- **领域**: 计算语言学
- **摘要**: 文档结构编辑涉及根据用户的请求操纵文档图像中本地化的文本，视觉和布局组件。过去的作品表明，文档图像中用户请求的多模式接地，并识别准确的结构组件及其相关属性仍然是该任务的关键挑战。为了解决这些问题，我们介绍了文档V2，这是一个新颖的框架，该框架通过利用大型多模型（LMM）来执行端到端文档编辑。它由三个新颖组成部分组成：（1）DOC2COMMAND，它们同时本地定位了关注区域（ROI），并将用户编辑请求删除到编辑命令中； （2）基于LLM的命令重新印象促使最初针对专业软件定制编辑命令，以适用于通才LMM的编辑说明。 （3）此外，文档V2通过GPT-4V和GEMINI（例如，解析文档布局）对接地区域（ROI）执行编辑并生成编辑的文档图像来处理这些输出，以解析文档的布局。文档数据集上的广泛实验表明，文档V2在编辑命令生成（2-33％），ROI边界框检测（12-31％）和整体文档编辑（1-12 \％）任务上的文档V2明显优于强大的基准。

### Enhancing Multimodal Affective Analysis with Learned Live Comment Features 
[[arxiv](https://arxiv.org/abs/2410.16407)] [[cool](https://papers.cool/arxiv/2410.16407)] [[pdf](https://arxiv.org/pdf/2410.16407)]
> **Authors**: Zhaoyuan Deng,Amith Ananthram,Kathleen McKeown
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 通过学习的实时评论功能增强多模式的情感分析
- **领域**: 计算语言学,人工智能,多媒体
- **摘要**: 实时评论（也称为Danmaku）是用户生成的消息，与视频内容同步。这些评论直接覆盖在流视频上，实时捕获观众的情绪和反应。尽管先前的工作利用了实时评论在情感分析中，但由于在不同的视频平台上实时评论的相对稀有性，其使用受到限制。为了解决这个问题，我们首先构建了情感分析的现场评论（LCACTECT）数据集，其中包含涵盖各种流派的英语和中文视频的实时评论，这些视频引起了各种各样的情感。然后，使用此数据集，我们使用对比度学习来训练视频编码器来生成合成的实时评论功能，以增强多模式情感内容分析。通过在英语和中文中对广泛的情感分析任务（情感，情感认识和讽刺检测）进行全面的实验，我们证明了这些合成的实时评论特征可显着提高与最先进方法相比的性能。

### Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages 
[[arxiv](https://arxiv.org/abs/2410.16153)] [[cool](https://papers.cool/arxiv/2410.16153)] [[pdf](https://arxiv.org/pdf/2410.16153)]
> **Authors**: Xiang Yue,Yueqi Song,Akari Asai,Seungone Kim,Jean de Dieu Nyandwi,Simran Khanuja,Anjali Kantharuban,Lintang Sutawika,Sathyanarayanan Ramamoorthy,Graham Neubig
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: 54 pages, 27 figures
- **标题**: Pangea：一种全面的多语言多模式LLM，适用于39种语言
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 尽管多模式大语言模型（MLLM）最近取得了进步，但它们的发展主要集中在英语和西方的数据集和任务上，这使世界上大多数语言以及多种文化背景的代表性不足。本文介绍了Pangea，这是一种在Pangeains上训练的多语言多模式LLM，这是一种跨越39种语言的6M指令数据集。 pangeains特征：1）高质量的英语说明，2）精心的机器翻译说明和3）与文化相关的多模式任务，以确保跨文化覆盖。为了严格评估模型的功能，我们介绍了Pangeabench，这是一个整体评估套件，其中包含14个涵盖47种语言的数据集。结果表明，Pangea在多语言环境和各种文化环境中的表现明显优于现有的开源模型。消融研究进一步揭示了英语数据比例，语言知名度以及多模式培训样本对整体表现的重要性。我们完全开源的数据，代码和训练有素的检查站，以促进包容性和强大的多语言MLLM的发展，从而在更广泛的语言和文化范围内促进公平和可及性。

### Did somebody say "Gest-IT"? A pilot exploration of multimodal data management 
[[arxiv](https://arxiv.org/abs/2410.15825)] [[cool](https://papers.cool/arxiv/2410.15825)] [[pdf](https://arxiv.org/pdf/2410.15825)]
> **Authors**: Ludovica Pannitto,Lorenzo Albanesi,Laura Marion,Federica Maria Martines,Carmelo Caruso,Claudia S. Bianchini,Francesca Masini,Caterina Mauri
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 有人说“ gest-it”？多模式数据管理的试点探索
- **领域**: 计算语言学
- **摘要**: 本文介绍了对多模式语料库的构建，管理和分析的试点探索。通过提供拼字，韵律和手势转录的三层注释，Gest-IT资源可以研究视力障碍者与视觉障碍者之间的对话中的手势制作模式的变化。在讨论了我们的研究中采用的转录方法和技术程序之后，我们提出了一个统一的Conll-U语料库，并指出了我们的未来步骤

### VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2410.17485)] [[cool](https://papers.cool/arxiv/2410.17485)] [[pdf](https://arxiv.org/pdf/2410.17485)]
> **Authors**: Yifan Peng,Krishna C. Puvvada,Zhehuai Chen,Piotr Zelasko,He Huang,Kunal Dhawan,Ke Hu,Shinji Watanabe,Jagadeesh Balam,Boris Ginsburg
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: Accepted at NAACL 2025 main conference
- **标题**: VoiceTextBlender：通过单级联合语音 - 监督微调来增强语音能力的大型语言模型
- **领域**: 计算语言学,音频和语音处理
- **摘要**: 最近的研究增强了具有语音能力的大型语言模型（LLMS），从而导致语言模型（SpeechLMS）的发展。较早的SpeechLMS专注于基于单转的问题答案（QA），用户输入包括语音上下文和文本问题。最近的研究将其扩展到了多转化的对话，尽管它们通常需要具有不同数据的复杂，多阶段监督的微调（SFT）。 SpeechLMS的另一个关键挑战是灾难性的遗忘，在此灾难性的遗忘中，针对语音任务进行优化的模型在仅文本性能方面遭受了重大降级。为了减轻这些问题，我们提出了一种新型的单阶段的联合语音 - 文本SFT方法，该方法对LLM主链的低级适应（LORA）。我们的联合SFT将仅使用文本的SFT数据与三种类型的语音相关数据相结合：语音识别和翻译，基于语音的QA和混合模式SFT。与以前具有7B或13B参数的SpeechLM相比，我们的3B模型在各种语音基准中表现出卓越的性能，同时保留了仅文本任务上的原始功能。此外，我们的模型显示出有效处理以前看不见的提示和任务的紧急能力，包括多转，混合模式输入。

### Captions Speak Louder than Images (CASLIE): Generalizing Foundation Models for E-commerce from High-quality Multimodal Instruction Data 
[[arxiv](https://arxiv.org/abs/2410.17337)] [[cool](https://papers.cool/arxiv/2410.17337)] [[pdf](https://arxiv.org/pdf/2410.17337)]
> **Authors**: Xinyi Ling,Bo Peng,Hanwen Du,Zhihui Zhu,Xia Ning
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: Xinyi Ling and Bo Peng contributed equally to this paper
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **摘要**: 利用多模式数据通过多模式基础模型（MFMS）在电子商务应用程序中推动突破正在引起研究社区的越来越多的关注。但是，存在着巨大的挑战，它阻碍了基础模型最佳使用多模式电子商务数据：（1）缺乏大规模的高质量多模式基准数据集； （2）缺乏有效的多模式信息整合方法。为了解决这些挑战，在本文中，我们介绍了MmeCinstruct，这是有关电子商务的第一个，大规模和高质量的多模式指令数据集。我们还开发了Caslie，这是一个简单，轻巧但有效的框架，用于集成电子商务的多模式信息。在利用MMeCinstruct，我们在Caslie中微调了一系列电子商务MFM，称为Caslie模型。我们的全面评估表明，Caslie模型在内域评估中大大优于5类高级基线模型。此外，Caslie模型显示出对室外设置的强大概括性。通过https://ninglab.github.io/caslie/可以公开访问Mmecinstruct和Caslie模型。

### JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation 
[[arxiv](https://arxiv.org/abs/2410.17250)] [[cool](https://papers.cool/arxiv/2410.17250)] [[pdf](https://arxiv.org/pdf/2410.17250)]
> **Authors**: Shota Onohara,Atsuyuki Miyai,Yuki Imajuku,Kazuki Egashira,Jeonghun Baek,Xiang Yue,Graham Neubig,Kiyoharu Aizawa
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: Project page: https://mmmu-japanese-benchmark.github.io/JMMMU/
- **标题**: JMMMU：日本大规模的多学科多模式理解基准，用于文化感​​知评估
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 对非英语语言的大型多模型模型（LMM）的加速研究对于增强广泛人群的用户体验至关重要。在本文中，我们介绍了JMMMU（日本MMMU），这是第一个大规模的日本基准，旨在评估基于日本文化背景的专家级任务的LMM。为了促进全面的文化感知评估，JMMMU具有两个互补的子集：（i）培养 - 反应式（CA）子集，其中选择了与文化无关的受试者（例如，数学）并将其转化为日语，可以一对一地比较其英语对方MMMU； （ii）特定于文化的（CS）子集，包括反映日本文化背景的新制作的主题。使用CA子集，我们在用日语评估时观察到许多LMM的性能下降，这纯粹归因于语言变化。使用CS子集，我们揭示了他们的日本文化理解不足。此外，通过结合两个子集，我们确定某些LMM在CA子集上的表现良好，但在CS子集上表现不佳，揭示了对文化理解中缺乏深度的日语的浅理解。我们希望这项工作不仅可以帮助提高日语的LMM表现，而且还可以作为为多语言LMM开发创建高标准，文化上不同基准的指南。项目页面是https://mmmu-japanese-benchmark.github.io/jmmmu/。

### IPL: Leveraging Multimodal Large Language Models for Intelligent Product Listing 
[[arxiv](https://arxiv.org/abs/2410.16977)] [[cool](https://papers.cool/arxiv/2410.16977)] [[pdf](https://arxiv.org/pdf/2410.16977)]
> **Authors**: Kang Chen,Qingheng Zhang,Chengbao Lian,Yixin Ji,Xuwei Liu,Shuguang Han,Guoqiang Wu,Fei Huang,Jufeng Chen
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: No comments
- **标题**: IPL：利用多模式大语模型用于智能产品清单
- **领域**: 计算语言学
- **摘要**: 与专业的企业对消费者（B2C）电子商务平台（例如，亚马逊）不同，消费者到消费者（C2C）平台（例如Facebook市场）主要是针对通常缺乏足够经验在电子商务经验的个人卖家。个别卖家通常很难为销售产品撰写适当的描述。随着多模式大语言模型（MLLM）的最新进展，我们试图将这种最新生成的AI技术集成到产品上市过程中。为此，我们开发了IPL，这是一种智能产品上市工具，该工具量身定制，用于使用各种产品属性（例如类别，品牌，颜色，条件等）生成描述。IPL使用户可以通过仅上传销售产品的照片来构成产品描述。更重要的是，它可以模仿我们C2C平台Xianyu的内容样式。这是通过在MLLM上采用特定领域的指令调整并采用多模式检索生成（RAG）过程来实现的。全面的经验评估表明，IPL的基本模型在域特定的任务中显着优于基本模型，同时产生较少的幻觉。 IPL已成功部署在我们的生产系统中，其中72％的用户根据生成的内容拥有其发布的产品列表，并且这些产品列表被证明比没有AI帮助的产品的质量得分高5.6％。

### Monolingual and Multilingual Misinformation Detection for Low-Resource Languages: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2410.18390)] [[cool](https://papers.cool/arxiv/2410.18390)] [[pdf](https://arxiv.org/pdf/2410.18390)]
> **Authors**: Xinyu Wang,Wenbo Zhang,Sarah Rajtmajer
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: 低资源语言的单语和多语言错误信息检测：一项全面的调查
- **领域**: 计算语言学
- **摘要**: 在当今的全球数字景观中，错误信息超越了语言界限，对适度系统构成了重大挑战。尽管在错误信息检测方面已取得了重大进展，但重点主要保留在单语高资源的环境上，而低资源的语言经常被忽略。这项调查旨在通过对当前关于单语和多语言环境中低资源语言错误信息检测的研究进行全面概述来弥合差距。我们回顾了这些领域中使用的现有数据集，方法和工具，并确定与：数据资源，模型开发，文化和语言环境，现实世界应用以及研究工作相关的关键挑战。我们还研究了新兴方法，例如语言不足的模型和多模式技术，同时强调需要改进数据收集实践，跨学科协作以及对社会负责人AI研究的更强大的激励措施。我们的发现强调了能够解决各种语言和文化背景之间的误解的强大，包容性系统的需求。

### OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation 
[[arxiv](https://arxiv.org/abs/2410.17799)] [[cool](https://papers.cool/arxiv/2410.17799)] [[pdf](https://arxiv.org/pdf/2410.17799)]
> **Authors**: Qinglin Zhang,Luyao Cheng,Chong Deng,Qian Chen,Wen Wang,Siqi Zheng,Jiaqing Liu,Hai Yu,Chaohong Tan,Zhihao Du,Shiliang Zhang
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: Work in progress
- **标题**: Omniflatten：无缝语音对话的端到端GPT模型
- **领域**: 计算语言学,人工智能,声音,音频和语音处理
- **摘要**: 全双工说话对话系统显着超过了传统的基于转弯的对话系统，因为它们允许同时进行双向交流，紧密反映人类人类的互动。但是，在全双工对话系统中实现低潜伏期和自然相互作用仍然是一个重大挑战，尤其是考虑到人类对话动态，例如中断，回音和演讲重叠。在本文中，我们介绍了一种新型的端到端基于GPT的模型OmniFlatten，以进行全双工对话，能够有效地建模自然对话固有的复杂行为，而潜伏期低。为了获得全双工对话功能，我们提出了一种多阶段的训练后方案，该方案逐渐将文本大语模型（LLM）主链适应语音文本对话LLM，能够实时生成文本和语音，而无需修改Backbone LLM的体系结构。培训过程包括三个阶段：模态对准，半封面对话学习和全双工对话学习。在所有培训阶段，我们都使用扁平操作对数据进行标准化，这使培训方法和GPT主链跨不同的方式和任务统一。我们的方法提供了一种简单的建模技术和有前途的研究方向，用于开发高效，自然的端到端全双工对话系统。 OmniFlatten生成的对话的音频样本可以在此网站（https://omniflatten.github.io/）上找到。

### A Survey of Multimodal Sarcasm Detection 
[[arxiv](https://arxiv.org/abs/2410.18882)] [[cool](https://papers.cool/arxiv/2410.18882)] [[pdf](https://arxiv.org/pdf/2410.18882)]
> **Authors**: Shafkat Farabi,Tharindu Ranasinghe,Diptesh Kanojia,Yu Kong,Marcos Zampieri
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Published in the Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence Survey Track. Pages 8020-8028
- **标题**: 多模式讽刺检测的调查
- **领域**: 计算语言学
- **摘要**: 讽刺是一种修辞手段，用于传达话语的字面意义的相反。讽刺广泛用于社交媒体和其他形式的计算机介导的通信，促使使用计算模型自动识别它。尽管仅在文本上进行了讽刺检测的明显方法，但讽刺检测通常需要以音调，面部表达和上下文图像中存在的其他信息。这导致了多模式的引入，开放了以多种模式（例如音频，图像，文本和视频）来检测讽刺的可能性。在本文中，我们介绍了有关多模式讽刺检测的首次综合调查 - 迄今为止，MSD。我们调查了2018年至2023年之间关于该主题的论文，并讨论用于此任务的模型和数据集。我们还提出了MSD的未来研究方向。

### Distill Visual Chart Reasoning Ability from LLMs to MLLMs 
[[arxiv](https://arxiv.org/abs/2410.18798)] [[cool](https://papers.cool/arxiv/2410.18798)] [[pdf](https://arxiv.org/pdf/2410.18798)]
> **Authors**: Wei He,Zhiheng Xi,Wanxu Zhao,Xiaoran Fan,Yiwen Ding,Zifei Shan,Tao Gui,Qi Zhang,Xuanjing Huang
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Under review. The code and dataset are publicly available at https://github.com/hewei2001/ReachQA
- **标题**: 从LLM到MLLM的蒸馏视觉图表推理能力
- **领域**: 计算语言学
- **摘要**: 解决复杂的图表问答任务需要多模式大语模型（MLLM）中的高级视觉推理能力。最近的研究强调，这些能力包括两个主要部分：识别来自视觉输入的关键信息并对其进行推理。因此，增强MLLM的有前途的方法是构建关注这两个方面的相关培训数据。但是，收集和注释复杂的图表和问题是昂贵且耗时的，并且确保带注释的答案的质量仍然是一个挑战。在本文中，我们提出了代码 - 插入式转换（CIT），这是一种具有成本效益，有效且易于扩展的数据合成方法，用于将视觉推理能力从LLMS提取到MLLM。该代码用作中介，将视觉图表表示为文本表示形式，使LLMS能够了解跨模式信息。具体而言，我们采用基于文本的合成技术来构建图表编码代码并生成ressionqa，这是一个包含3K推理密集型图表和20K Q＆A对的数据集，以增强识别能力和推理能力。实验表明，当对我们的数据进行微调时，模型不仅在与图表相关的基准上表现良好，而且还可以在Mathvista（如Mathvista）等一般数学基准上展示提高的多模式推理能力。代码和数据集可在https://github.com/hewei2001/reachqa上公开获得。

### Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data 
[[arxiv](https://arxiv.org/abs/2410.18558)] [[cool](https://papers.cool/arxiv/2410.18558)] [[pdf](https://arxiv.org/pdf/2410.18558)]
> **Authors**: Shuhao Gu,Jialing Zhang,Siyuan Zhou,Kevin Yu,Zhaohu Xing,Liangdong Wang,Zhou Cao,Jintao Jia,Zhuoyi Zhang,Yixuan Wang,Zhenchong Hu,Bo-Wen Zhang,Jijie Li,Dong Liang,Yingli Zhao,Songjing Wang,Yulong Ao,Yiming Ju,Huanhuan Ma,Xiaotong Li,Haiwen Diao,Yufeng Cui,Xinlong Wang,Yaoqi Liu,Fangxiang Feng, et al. (1 additional authors not shown)
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: Infinity-MM：通过大规模和高质量指令数据缩放多模式性能
- **领域**: 计算语言学
- **摘要**: 最近，视觉模型（VLM）在多模式任务中取得了显着进步，多模式指令数据是增强VLM功能的基础。尽管有几个开源的多模式数据集可用，但开源指令数据的规模和质量的限制阻碍了这些数据集中训练的VLM的性能，与对封闭源数据进行培训的模型相比，在这些数据集上训练有明显的差距。为了应对这一挑战，我们引入了Infinity-MM，这是一个大规模的多模式指令数据集。我们收集了可用的多模式指令数据集并进行了统一的预处理，从而产生了一个超过4000万个样本的数据集，可确保多样性和准确性。此外，为了实现指令数据的大规模扩展并支持连续获取高质量数据，我们提出了一种基于标记系统和开源VLMS的合成指令生成方法。通过建立不同类型的图像和相关指令类型之间的对应关系，该方法可以在数据综合期间提供基本的指导。利用这种高质量数据，我们培训了一个20亿亿六十亿杆的视觉语言模型Aquila-VL-2B，该模型在类似规模的模型中实现了最先进的（SOTA）性能。数据可在以下网址提供：https：//huggingface.co/datasets/baai/infinity-mm。

### MatViX: Multimodal Information Extraction from Visually Rich Articles 
[[arxiv](https://arxiv.org/abs/2410.20494)] [[cool](https://papers.cool/arxiv/2410.20494)] [[pdf](https://arxiv.org/pdf/2410.20494)]
> **Authors**: Ghazal Khalighinejad,Sharon Scott,Ollie Liu,Kelly L. Anderson,Rickard Stureborg,Aman Tyagi,Bhuwan Dhingra
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 矩阵：从视觉上丰富的文章中提取多模式信息
- **领域**: 计算语言学
- **摘要**: 多模式信息提取（MIE）对于科学文献至关重要，在科学文献中，有价值的数据通常分布在文本，数字和表格中。在材料科学中，从研究文章中提取结构化信息可以加速发现新材料。但是，科学内容的多模式性质和复杂的互连为传统的基于文本的方法带来了挑战。我们介绍了\ textsc {matvix}，这是一个由$ 324 $全长研究文章和$ 1,688 $复杂的结构化JSON文件组成的基准，由域专家仔细策划。这些JSON文件是从全长文档中的文本，表和数字中提取的，为MIE提供了全面的挑战。我们介绍了一种评估方法，以评估曲线相似性的准确性和层次结构的比对。此外，我们以零拍的方式对视觉模型（VLM）进行基准测试，能够处理长上下文和多模式输入，并表明使用专用模型（DEPLOT）可以改善提取曲线的性能。我们的结果表明，当前模型中有很大的改进空间。我们的数据集和评估代码可用\ footNote {\ url {https://matvix-bench.github.io/}}。

### What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration 
[[arxiv](https://arxiv.org/abs/2410.20482)] [[cool](https://papers.cool/arxiv/2410.20482)] [[pdf](https://arxiv.org/pdf/2410.20482)]
> **Authors**: Libo Qin,Qiguang Chen,Hao Fei,Zhi Chen,Min Li,Wanxiang Che
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-28
> **comment**: Accepted at NeurIPS 2024
- **标题**: 哪些因素会影响多模式的内在学习学习？深入探索
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 最近，多模式中文本学习（MM-ICL）的快速进步取得了显着的成功，它能够在无需其他参数调整的情况下实现各种任务的卓越性能。但是，MM-ICL有效性的基本规则仍然不足。为了填补这一空白，这项工作旨在调查研究问题：“哪些因素会影响MM-ICL的性能？''到达这一目的，我们研究了MM-ICL的三个核心步骤，包括示范检索，演示顺序，使用6个视觉大语模型和20个策略的策略，我们的发现重点（1），我们的发现（1）示例（1）内部示意图对示意图的顺序排序，以及（3）通过提示中的介绍性指令增强任务理解。

### Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation 
[[arxiv](https://arxiv.org/abs/2410.20336)] [[cool](https://papers.cool/arxiv/2410.20336)] [[pdf](https://arxiv.org/pdf/2410.20336)]
> **Authors**: Maohao Shen,Shun Zhang,Jilong Wu,Zhiping Xiu,Ehab AlBadawy,Yiting Lu,Mike Seltzer,Qing He
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 准备大型语言模型说话：一种后期融合的语音方法
- **领域**: 计算语言学,人工智能,声音,音频和语音处理
- **摘要**: 大型语言模型（LLMS）彻底改变了自然语言处理（NLP），并在各种基于文本的任务中表现出色。但是，通过语音生成任务扩展到文本主导的LLM尚未探索。在这项工作中，我们介绍了一个文本对语音（TTS）系统，该系统由一个名为TTS-llama的微调Llama模型提供动力，该模型可实现最新的语音综合性能。在TTS-LALA上，我们进一步提出了Mole-lalama，这是一种通过纯粹的后期融合参数效率微调（PEFT）和expert架构的混合物开发的文本和语音多模式LLM。广泛的经验结果表明，Mole-Lalla在仅文本提问（QA）和TTS任务上的竞争表现，从而减轻了这两种方式中的灾难性遗忘问题。最后，我们进一步探索了文本中的QA任务中的molela lalama，这表明了它作为能够发电的多模式对话系统的巨大潜力。

### Improving Multimodal Large Language Models Using Continual Learning 
[[arxiv](https://arxiv.org/abs/2410.19925)] [[cool](https://papers.cool/arxiv/2410.19925)] [[pdf](https://arxiv.org/pdf/2410.19925)]
> **Authors**: Shikhar Srivastava,Md Yousuf Harun,Robik Shrestha,Christopher Kanan
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: NeurIPS 2024 Workshop on Scalable Continual Learning for Lifelong Foundation Models
- **标题**: 使用持续学习改善多模式大语模型
- **领域**: 计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 生成的大语言模型（LLM）具有令人印象深刻的功能，可以通过将预训练的视觉模型集成到原始LLM中以创建多模式LLM（MLLM）来进一步增强。但是，与原始LLM相比，这种整合通常会大大降低自然语言理解和发电任务的表现。这项研究使用LLAVA MLLM研究了这个问题，将整合视为持续学习问题。我们评估了五种连续的学习方法，以减轻遗忘并确定一种增强视觉理解的技术，同时最大程度地减少语言性能丧失。我们的方法在LLAVA食谱上最多将语言性能降解降低了15 \％，同时保持了高的多模式准确性。我们还通过在一系列视觉任务上进行学习来证明我们的方法的鲁棒性，从而有效地保留了语言技能，同时获得了新的多模式能力。

### OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization 
[[arxiv](https://arxiv.org/abs/2410.19609)] [[cool](https://papers.cool/arxiv/2410.19609)] [[pdf](https://arxiv.org/pdf/2410.19609)]
> **Authors**: Hongliang He,Wenlin Yao,Kaixin Ma,Wenhao Yu,Hongming Zhang,Tianqing Fang,Zhenzhong Lan,Dong Yu
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: OpenWebVoyager：通过迭代现实世界探索，反馈和优化构建多模式网络代理
- **领域**: 计算语言学,人工智能
- **摘要**: 大型语言和多模型模型的快速发展引发了人们对使用专有模型（例如GPT-4O）的重大兴趣，以开发能够处理像Web导航等真实世界场景的自主剂。尽管最近的开源努力试图使代理商能够探索环境并随着时间的流逝不断改善，但它们正在在合成环境中构建纯文本代理，这些环境明确定义了奖励信号。这样的代理商努力将需要多模式感知能力的现实环境概括和缺乏地面信号。在本文中，我们介绍了一个开源框架，旨在促进多模式Web代理的开发，该框架可以自主进行现实世界探索并改善自身。我们首先通过模仿学习来训练基本模型以获得基本能力。然后，我们让代理商探索开放的网络并收集有关其轨迹的反馈。之后，它通过从另一个通用模型判断的良好表现轨迹中学习进一步改善了政策。这种探索反馈优化周期可以继续进行多次迭代。实验结果表明，我们的Web代理在每次迭代后都成功地改善了自身，证明了多个测试集的强劲性能。

### Graph Linearization Methods for Reasoning on Graphs with Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.19494)] [[cool](https://papers.cool/arxiv/2410.19494)] [[pdf](https://arxiv.org/pdf/2410.19494)]
> **Authors**: Christos Xypolopoulos,Guokan Shang,Xiao Fei,Giannis Nikolentzos,Hadi Abdine,Iakovos Evdaimon,Michail Chatzianastasis,Giorgos Stamou,Michalis Vazirgiannis
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 图形线性化方法用于与大语言模型的图形上的推理
- **领域**: 计算语言学,机器学习
- **摘要**: 大型语言模型已经演变为处理文本以外的多种模式，例如图像和音频，这激发了我们探索如何有效利用它们用于图形机器学习任务的方式。因此，关键问题是如何将图形转换为令牌的线性序列，这是一个过程，我们称为图形线性化，以便LLMS可以自然处理图形。我们认为应该有意义地对图进行线性性化，以反映自然语言文本的某些属性，例如本地依赖性和全球一致性，以便简化当代LLM，并接受了数万亿个文本代币的培训，可以更好地理解图形。为了实现这一目标，我们基于图中心性，退化性和节点重新标记方案开发了几种图形线性化方法。然后，我们研究了它们对图形推理任务中LLM性能的影响。合成图的实验结果证明了与随机线性化基准相比，我们方法的有效性。我们的工作介绍了适合LLM的新型图表表示，这有助于使用统一变压器模型的多模式处理的趋势，这有助于图形机学习的潜在整合。

### CT2C-QA: Multimodal Question Answering over Chinese Text, Table and Chart 
[[arxiv](https://arxiv.org/abs/2410.21414)] [[cool](https://papers.cool/arxiv/2410.21414)] [[pdf](https://arxiv.org/pdf/2410.21414)]
> **Authors**: Bowen Zhao,Tianhao Cheng,Yuejie Zhang,Ying Cheng,Rui Feng,Xiaobo Zhang
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: 10 pages, 6 figures
- **标题**: CT2C-QA：通过中文文本，表和图表回答的多模式问题
- **领域**: 计算语言学,人工智能
- **摘要**: 多模式问答（MMQA）至关重要，因为它通过整合了来自表，图表和文本等不同数据表示的见解来实现全面的理解和准确的回答。 MMQA中的大多数现有研究仅着眼于两种模式，例如Image-Text QA，Table-Text QA和Chart-Text QA，并且在研究文本，表和图表的联合分析的研究中仍然存在显着稀缺性。在本文中，我们介绍了C $ \ text {t}^2 $ c-qa，这是一个开创性的基于中国推理的QA数据集，其中包括大量的文本，表格和图表，并从200个选择性源接的网页中精心编译。我们的数据集模拟了真实的网页，并为模型使用多模式数据分析和推理的能力提供了很好的测试，因为问题的答案可能以各种方式出现，甚至根本不存在。此外，我们提出AED（\ textbf {a} llosating，\ textbf {e} xpert和\ textbf {d} esicion），这是一种通过协作部署，信息交互，信息交互和集体决策制定的多代理系统。具体而言，分配代理人负责选择和激活专家代理，包括精通文本，表格和图表的代理。决策代理人负责作出最终裁决，并利用这些专家代理提供的分析见解。我们进行了全面的分析，将AED与MMQA中的各种最新模型进行了比较，包括GPT-4。实验结果表明，包括GPT-4在内的当前方法尚未符合我们数据集设置的基准。

### Large Language Model Benchmarks in Medical Tasks 
[[arxiv](https://arxiv.org/abs/2410.21348)] [[cool](https://papers.cool/arxiv/2410.21348)] [[pdf](https://arxiv.org/pdf/2410.21348)]
> **Authors**: Lawrence K. Q. Yan,Qian Niu,Ming Li,Yichao Zhang,Caitlyn Heqi Yin,Cheng Fei,Benji Peng,Ziqian Bi,Pohsun Feng,Keyu Chen,Tianyang Wang,Yunze Wang,Silin Chen,Ming Liu,Junyu Liu
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: 25 pages, 5 tables
- **标题**: 大型语言模型基准在医疗任务中
- **领域**: 计算语言学,人工智能
- **摘要**: 随着大语模型（LLM）在医疗领域的越来越多，使用基准数据集评估这些模型的性能已经变得至关重要。本文对医疗LLM任务中采用的各种基准数据集进行了全面调查。这些数据集涵盖了多种模式，包括文本，图像和多模式基准测试，重点关注医学知识的不同方面，例如电子健康记录（EHR），医生患者对话，医疗问答和医疗图像字幕。该调查按模式对数据集进行了分类，讨论了其意义，数据结构以及对临床任务（例如诊断，报告生成和预测性决策支持）的LLM开发的影响。关键基准包括Mimic-III，Mimic-IV，Bioasq，PubMedQA和Chexpert，它们促进了医疗报告生成，临床总结和合成数据生成等任务的进步。本文总结了利用这些基准推进多模式医学智能的挑战和机遇，强调了对具有更大程度的语言多样性，结构化的OMICS数据以及合成的创新方法的数据集的需求。这项工作还为LLM在医学中应用的未来研究奠定了基础，这为医学人工智能的发展领域做出了贡献。

### NeuGPT: Unified multi-modal Neural GPT 
[[arxiv](https://arxiv.org/abs/2410.20916)] [[cool](https://papers.cool/arxiv/2410.20916)] [[pdf](https://arxiv.org/pdf/2410.20916)]
> **Authors**: Yiqian Yang,Yiqun Duan,Hyejeong Jo,Qiang Zhang,Renjing Xu,Oiwi Parker Jones,Xuming Hu,Chin-teng Lin,Hui Xiong
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: Neugpt：统一的多模式神经GPT
- **领域**: 计算语言学
- **摘要**: 本文介绍了Neugpt，这是一种开创性的多模式语言生成模型，旨在协调神经记录研究的碎片景观。传统上，该领域的研究已通过信号类型进行了分界化，而EEG，MEG，ECOG，SEEG，fMRI和FNIRS数据被分别分析。认识到未开发的交叉授粉潜力和在不同实验条件下神经信号的适应性，我们着手开发能够与多种方式接口的统一模型。 Neugpt从NLP，计算机视觉和语音处理中的预训练大型模型的成功中汲取灵感，构建了处理各种神经记录的阵列，并与语音和文本数据互动。我们的模型主要集中于脑对文本解码，在BLEU-1上将SOTA从6.94提高到12.92，在Rouge-1F上将SOTA从6.94和6.93到13.06。它还可以模拟大脑信号，从而充当新的神经界面。代码可在\ href {https://github.com/neuspeech/neugpt} {neuspeech/neugpt（https://github.com/neuspeech/neugpt/neugpt）。

### ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding 
[[arxiv](https://arxiv.org/abs/2410.22211)] [[cool](https://papers.cool/arxiv/2410.22211)] [[pdf](https://arxiv.org/pdf/2410.22211)]
> **Authors**: Kimihiro Hasegawa,Wiradee Imrattanatrai,Zhi-Qi Cheng,Masaki Asada,Susan Holm,Yuran Wang,Ken Fukuda,Teruko Mitamura
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: 18 pages, 11 figures
- **标题**: promqa：用于多模式程序活动理解的问题回答数据集
- **领域**: 计算语言学
- **摘要**: 多模式系统具有帮助人类进行程序活动的巨大潜力，人们遵循指示以实现其目标。尽管采用了多种应用方案，但通常会根据传统的分类任务进行评估系统，例如行动识别或时间动作细分。在本文中，我们提出了一个新颖的评估数据集Promqa，以测量面向应用程序的方案中的系统进步。 Promqa由401个多模式程序质量质量质量QA对组成，以记录程序活动的用户记录以及相应的指令。对于QA注释，我们采用了一种具有成本效益的人类协作方法，在该方法中，现有的注释通过LLM生成的QA对增强，后来由人类进行了验证。然后，我们提供基准结果，以在Promqa上设置基线性能。我们的实验揭示了人类绩效与当前系统的差距，包括竞争性专有多模型。我们希望我们的数据集能够阐明模型多模式理解能力的新方面。

### Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench 
[[arxiv](https://arxiv.org/abs/2410.22108)] [[cool](https://papers.cool/arxiv/2410.22108)] [[pdf](https://arxiv.org/pdf/2410.22108)]
> **Authors**: Zheyuan Liu,Guangyao Dou,Mengzhao Jia,Zhaoxuan Tan,Qingkai Zeng,Yongle Yuan,Meng Jiang
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: NAACL Main 2025
- **标题**: 使用MLLMU板凳保护多模式大语模型的隐私
- **领域**: 计算语言学,人工智能
- **摘要**: 诸如大型语言模型（LLM）和多模式大语模型（MLLM）之类的生成模型可以记住并披露个人的机密和私人数据，从而引发法律和道德问题。尽管许多以前的作品已经通过计算机在LLM中解决了此问题，但对于MLLM来说，它仍然没有探索。为了应对这一挑战，我们介绍了多模式的大语言模型学习基准（MLLMU基准），这是一种旨在促进对多模式机器学习的理解的新型基准。 MLLMU基础由500个虚构的配置文件和153个公共名人配置文件组成，每个配置文件都在14个自定义的问题 - 答案对上，并从多模式（图像+文本）和单峰（文本）观点进行了评估。基准分为四组，以评估功效，概括性和模型效用方面评估未学习算法。最后，我们使用现有的生成模型未学习算法提供基线结果。令人惊讶的是，我们的实验表明，单形成算法在生成和锁定任务中表现出色，而多模态划定方法在具有多模态输入的分类任务中表现更好。

### Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications 
[[arxiv](https://arxiv.org/abs/2410.21943)] [[cool](https://papers.cool/arxiv/2410.21943)] [[pdf](https://arxiv.org/pdf/2410.21943)]
> **Authors**: Monica Riedler,Stefan Langer
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: 超越文本：用多模式输入来优化工业应用的抹布
- **领域**: 计算语言学,人工智能
- **摘要**: 大型语言模型（LLMS）在回答问题方面表现出了令人印象深刻的能力，但它们缺乏特定领域的知识，并且容易受到幻觉的影响。检索增强生成（RAG）是解决这些挑战的一种方法，而多模型模型则成为有希望的AI助手来处理文本和图像。在本文中，我们描述了一系列旨在确定如何最好地将多模式模型整合到工业领域的抹布系统中的实验。实验的目的是确定工业领域内文档中文本的图像是否增加了抹布的性能，并找到了这种多模式抹布系统的最佳配置。我们的实验包括两种用于图像处理和检索的方法，以及两个用于答案合成的LLMS（GPT4-Vision和Llava）。这些图像处理策略涉及使用多模式嵌入以及图像中文本摘要的生成。我们通过LLM-AS-A-A-Gudge方法评估我们的实验。我们的结果表明，多模式的抹布可以优于单模式的抹布设置，尽管图像检索比文本检索构成了更大的挑战。此外，与使用多模式嵌入相比，从图像中利用文本摘要提出了一种更有希望的方法，为未来的进步提供了更多机会。

### Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs 
[[arxiv](https://arxiv.org/abs/2410.23478)] [[cool](https://papers.cool/arxiv/2410.23478)] [[pdf](https://arxiv.org/pdf/2410.23478)]
> **Authors**: Sireesh Gururaja,Yueheng Zhang,Guannan Tang,Tianhao Zhang,Kevin Murphy,Yu-Tsen Yi,Junwon Seo,Anthony Rollett,Emma Strubell
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: No comments
- **标题**: 拼贴：可分解的快速原型用于科学PDF的信息提取
- **领域**: 计算语言学,人机交互
- **摘要**: 近年来，NLP的持续开发了针对科学文档的域特异性信息提取工具，以及越来越多的多模式预审预周化的变压器模型的释放。虽然NLP以外的科学家将这些系统评估和应用于自己的领域的机会从未更清楚，但这些模型很难比较：他们接受不同的输入格式，通常是黑框，几乎没有对处理失败的见解，很少处理PDF文档，这是最常见的科学刊登格式。在这项工作中，我们提出了拼贴画，该工具旨在快速原型制作，可视化和评估科学PDF上不同信息提取模型。拼贴允许使用和评估任何拥抱面代币分类器，几个LLM和其他多个特定于任务的模型，并提供可扩展的软件接口，以通过新模型加速实验。此外，我们使开发人员和基于NLP的工具的用户都可以通过提供处理中间状态的颗粒状视图来检查，调试和更好地理解建模管道。我们在信息提取的背景下演示了我们的系统，以协助材料科学中的文献综述。

### Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model 
[[arxiv](https://arxiv.org/abs/2410.22736)] [[cool](https://papers.cool/arxiv/2410.22736)] [[pdf](https://arxiv.org/pdf/2410.22736)]
> **Authors**: Keito Sasagawa,Koki Maeda,Issa Sugiura,Shuhei Kurita,Naoaki Okazaki,Daisuke Kawahara
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: 15 pages, 7 figures
- **标题**: 从头开始构建多模式数据集，以快速开发日语视觉语言模型
- **领域**: 计算语言学
- **摘要**: 要开发高性能的视觉语言模型（VLM），必须准备多模式资源，例如图像文本对，交织的数据和指令数据。虽然英语的多模式资源丰富，但对于非英语语言（例如日语）的相应资源严重缺乏相应的资源。为了解决这个问题，我们将日语作为一种非英语语言，并提出了一种从头开始快速创建日本多模式数据集的方法。我们从Web档案中收集日本图像文本对，并从Web档案中收集交错数据，并使用现有VLM直接从图像中生成日语指令数据。我们的实验结果表明，在这些天然数据集上训练的VLM优于依靠机器翻译内容的VLM。

## 密码学和安全(cs.CR:Cryptography and Security)

该领域共有 8 篇论文

### PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI System 
[[arxiv](https://arxiv.org/abs/2410.02828)] [[cool](https://papers.cool/arxiv/2410.02828)] [[pdf](https://arxiv.org/pdf/2410.02828)]
> **Authors**: Gary D. Lopez Munoz,Amanda J. Minnich,Roman Lutz,Richard Lundeen,Raja Sekhar Rao Dheekonda,Nina Chikanov,Bolor-Erdene Jagdagdorj,Martin Pouliot,Shiven Chawla,Whitney Maxwell,Blake Bullwinkel,Katherine Pratt,Joris de Gruyter,Charlotte Siska,Pete Bryan,Tori Westerhoff,Chang Kawaguchi,Christian Seifert,Ram Shankar Siva Kumar,Yonatan Zunger
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: Pyrit：生成AI系统中安全风险标识和红色团队的框架
- **领域**: 密码学和安全,人工智能,计算语言学
- **摘要**: 生成人工智能（Genai）在我们的日常生活中变得无处不在。计算能力和数据可用性的增加导致了单模式和多模式模型的扩散。随着Genai生态系统的成熟，对可扩展和模型的风险识别框架的需求正在增长。为了满足这一需求，我们介绍了Python风险识别工具包（Pyrit），这是一个开源框架，旨在增强Genai Systems的红色团队工作。 pyrit是一种模型和平台不足的工具，使红色团队者能够在多模式生成AI模型中探​​究并确定新颖的危害，风险和越狱。其组合体系结构有助于重新使用核心构建块，并为未来的模型和模式提供了可扩展的性能。本文详细介绍了针对红色团队生成的AI系统，Pyrit的开发和功能及其在现实世界中的实际应用所带来的挑战。

### BlockFound: Customized blockchain foundation model for anomaly detection 
[[arxiv](https://arxiv.org/abs/2410.04039)] [[cool](https://papers.cool/arxiv/2410.04039)] [[pdf](https://arxiv.org/pdf/2410.04039)]
> **Authors**: Jiahao Yu,Xian Wu,Hao Liu,Wenbo Guo,Xinyu Xing
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 区块：定制区块链基础模型用于异常检测
- **领域**: 密码学和安全,人工智能
- **摘要**: 我们提出了Blockfound，这是一种用于异常区块链事务检测的定制基础模型。与现有的依赖基于规则的系统或直接应用现成的大语言模型的方法不同，Blockfound引入了一系列定制设计，以模拟区块链交易的唯一数据结构。首先，区块链事务是多模式的，其中包含区块链特异性令牌，文本和数字。我们设计了一个模块化的令牌仪来处理这些多模式输入，平衡了不同模式的信息。其次，我们设计了一种定制的掩码语言学习机制，用于预处理绳索嵌入和闪存，以处理更长的序列。在训练基础模型之后，我们进一步设计了一种用于异常检测的新型检测方法。对以太坊和索拉纳交易的广泛评估表明，在保持较低的假阳性速率的同时，在异常检测中的特殊能力。值得注意的是，块是唯一成功地检测出具有高度准确性的Solana异常交易的方法，而所有其他方法都达到了非常低或零检测的回忆分数。这项工作不仅为区块链提供了新的基础模型，还为在区块链数据中应用LLMS设定了新的基准。

### Gradient-based Jailbreak Images for Multimodal Fusion Models 
[[arxiv](https://arxiv.org/abs/2410.03489)] [[cool](https://papers.cool/arxiv/2410.03489)] [[pdf](https://arxiv.org/pdf/2410.03489)]
> **Authors**: Javier Rando,Hannah Korevaar,Erik Brinkman,Ivan Evtimov,Florian Tramèr
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 基于梯度的越狱图像多模式融合模型
- **领域**: 密码学和安全,人工智能
- **摘要**: 与需要离散优化的文本输入不同，使用图像输入的增强语言模型可以通过连续优化实现更有效的越狱攻击。但是，新的多模式融合模型使用非差异性函数将所有输入模态示为所有输入模式，从而阻碍了直接攻击。在这项工作中，我们介绍了以连续函数近似令牌化的令牌快捷方式的概念，并可以连续优化。我们使用Tokenizer快捷方式来创建针对多模式融合模型的第一个端到端梯度图像攻击。我们评估了对变色龙模型的攻击，并获得越狱图像，这些图像以72.5％的提示引起有害信息。越狱图像优于以相同目标优化的文本越狱，要求较低的计算预算以优化50倍的输入令牌。最后，我们发现表示仅在文本攻击方面受过训练的代表工程防御能力可以有效地转移到对抗图像输入中。

### Fortify Your Foundations: Practical Privacy and Security for Foundation Model Deployments In The Cloud 
[[arxiv](https://arxiv.org/abs/2410.05930)] [[cool](https://papers.cool/arxiv/2410.05930)] [[pdf](https://arxiv.org/pdf/2410.05930)]
> **Authors**: Marcin Chrapek,Anjo Vahldiek-Oberwagner,Marcin Spoczynski,Scott Constable,Mona Vij,Torsten Hoefler
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 强化您的基础：云中基础模型部署的实际隐私和安全性
- **领域**: 密码学和安全,人工智能
- **摘要**: 基础模型（FMS）在诸如自然语言处理之类的任务中显示出非凡的表现，并在越来越多的学科中应用。尽管通常在大型公共数据集中受过培训，但FMS通常经过微调或集成到依赖私人数据的检索型生成（RAG）系统中。这种通道以及它们的规模和昂贵的培训，增加了知识产权盗窃的风险。此外，多模式FMS可能会暴露敏感信息。在这项工作中，我们检查了FM威胁模型，并讨论了针对它们的各种方法的实用性和全面性，例如基于ML的方法和可信赖的执行环境（TEES）。我们证明T恤在强大的安全性，可用性和性能之间提供了有效的平衡。具体而言，我们提供了一个解决方案，该解决方案的范围少于10 \％的开销与裸机的裸机，用于整个LLAMA2 7B和13B推理管道，在\ Intel \ sgx和\ Intel \ tdx内部运行。我们还分享了我们实施的配置文件和见解。据我们所知，我们的工作是第一个展示T恤实用性以确保FMS的实用性。

### BlackDAN: A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking of Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.09804)] [[cool](https://papers.cool/arxiv/2410.09804)] [[pdf](https://arxiv.org/pdf/2410.09804)]
> **Authors**: Xinyuan Wang,Victor Shea-Jay Huang,Renmiao Chen,Hao Wang,Chengwei Pan,Lei Sha,Minlie Huang
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: Blackdan：一种黑盒多目标方法，用于有效和上下文的大型语言模型
- **领域**: 密码学和安全,人工智能,计算语言学,机器学习,神经和进化计算
- **摘要**: 尽管大型语言模型（LLMS）在各种任务中表现出显着的功能，但它们遇到了潜在的安全风险，例如越狱攻击，这些攻击攻击了漏洞以绕过安全措施并产生有害的产出。现有的越狱策略主要集中于最大化攻击成功率（ASR），经常忽略其他关键因素，包括越狱对查询的反应和隐形水平的相关性。对单一目标的这种狭窄关注可能会导致无效的攻击，这些攻击要么缺乏上下文相关性或易于识别。在这项工作中，我们介绍了Blackdan，这是一个具有多目标优化的创新的Black-Box攻击框架，旨在产生高质量的提示，从而有效地促进越狱，同时保持上下文相关性并最小化可检测性。 Blackdan利用多主体进化算法（MOEAS），特别是NSGA-II算法，以优化跨多个目标的越狱，包括ASR，隐身性和语义相关性。通过整合突变，跨界和帕累托占主导地位等机制，布莱克丹提供了一个透明且可解释的过程来产生越狱。此外，该框架允许根据用户偏好进行自定义，从而可以选择提示，以平衡有害性，相关性和其他因素。实验结果表明，Blackdan的表现优于传统的单瞄准方法，在各种LLM和多模式LLM中产生更高的成功率和提高的鲁棒性，同时确保越狱的响应既相关又较不可检测。

### A Formal Framework for Assessing and Mitigating Emergent Security Risks in Generative AI Models: Bridging Theory and Dynamic Risk Mitigation 
[[arxiv](https://arxiv.org/abs/2410.13897)] [[cool](https://papers.cool/arxiv/2410.13897)] [[pdf](https://arxiv.org/pdf/2410.13897)]
> **Authors**: Aviral Srivastava,Sourav Panda
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-18
> **comment**: This paper was accepted in NeurIPS 2024 workshop on Red Teaming GenAI: What can we learn with Adversaries?
- **标题**: 在生成AI模型中评估和减轻新兴安全风险的正式框架：桥接理论和动态风险降低
- **领域**: 密码学和安全,机器学习
- **摘要**: 作为生成的AI系统，包括大型语言模型（LLM）和扩散模型，迅速发展，它们的采用日益增长，导致了传统的AI风险评估框架中经常忽略的新的和复杂的安全风险。本文介绍了一个新颖的正式框架，用于通过整合适合生成模型独特漏洞的自适应，实时监控和动态风险缓解策略来对这些新兴的安全风险进行分类和减轻这些新兴的安全风险。我们确定先前探索的风险不足，包括潜在空间开发，多模式跨攻击媒介和反馈环诱导的模型降解。我们的框架采用了分层方法，结合了异常检测，连续的红色团队和实时对抗模拟来减轻这些风险。我们专注于正式验证方法，以确保面对不断发展的威胁时模型的鲁棒性和可扩展性。尽管理论上，这项工作通过建立详细的方法和指标来评估生成AI系统中的风险缓解策略的性能，为未来的经验验证奠定了基础。该框架解决了AI安全中的现有差距，为未来的研究和实施提供了全面的路线图。

### When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge? 
[[arxiv](https://arxiv.org/abs/2410.15267)] [[cool](https://papers.cool/arxiv/2410.15267)] [[pdf](https://arxiv.org/pdf/2410.15267)]
> **Authors**: Shang Wang,Tianqing Zhu,Dayong Ye,Wanlei Zhou
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: 15 pages, 9 figures, 9 tables
- **标题**: 当机器学习时会遇到检索演示的一代（RAG）：保持秘密还是忘记知识？
- **领域**: 密码学和安全,计算语言学
- **摘要**: 大型语言模型（LLM）等大型语言模型（如Chatgpt和Gemini）展示了其强大的自然语言生成能力。但是，这些模型可以在培训期间无意间学习并保留敏感信息和有害内容，从而引发了重大的道德和法律问题。为了解决这些问题，已经引入了机器学习作为潜在解决方案。尽管现有的未学习方法考虑了LLM的特定特征，但它们通常会遭受高计算需求，有限的适用性或灾难性遗忘的风险。为了解决这些局限性，我们提出了一个基于检索功能（RAG）技术的轻量级未学习框架。通过修改抹布的外部知识库，我们模拟了忘记的效果，而无需直接与未学习的LLM相互作用。我们将未经学习知识的构建作为一个约束的优化问题，从而得出了两个关键组成部分，这些组成部分是基于抹布的未学习有效性的基础。这种基于抹布的方法对于封闭源LLM特别有效，在现有的未学习方法中通常会失败。我们通过对开源和封闭源模型进行的广泛实验来评估我们的框架，包括Chatgpt，Gemini，Llama-2-7b-Chat-HF和Palm 2。结果表明，我们的方法符合五个关键的未学习标准：有效性，普遍性，无害性，无害性，简单性，简单性和鲁棒性。同时，这种方法可以扩展到多模式的大语言模型和基于LLM的代理。

### Jailbreaking and Mitigation of Vulnerabilities in Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.15236)] [[cool](https://papers.cool/arxiv/2410.15236)] [[pdf](https://arxiv.org/pdf/2410.15236)]
> **Authors**: Benji Peng,Ziqian Bi,Qian Niu,Ming Liu,Pohsun Feng,Tianyang Wang,Lawrence K. Q. Yan,Yizhu Wen,Yichao Zhang,Caitlyn Heqi Yin
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 大语言模型中越狱和缓解脆弱性
- **领域**: 密码学和安全,人工智能,机器学习
- **摘要**: 大型语言模型（LLM）通过推进自然语言理解和生成，使医疗保健，软件工程和对话系统以外的领域的应用程序通过提高自然语言的理解和生成来改变人工智能。尽管在过去几年中取得了这些进步，但LLMS表现出很大的脆弱性，尤其是在引起注射和越狱攻击方面。这篇综述分析了有关这些漏洞的研究状态，并提出了可用的防御策略。我们将攻击方法大致分为基于迅速的，基于模型的，多模式和多语言，涵盖了诸如对抗性提示，后门注射和跨模式利用等技术。我们还审查了各种防御机制，包括及时过滤，转换，对准技术，多代理防御和自我调节，评估其优势和缺点。我们还讨论了用于评估LLM安全性和鲁棒性的关键指标和基准，并指出了诸如量化交互式环境中攻击成功的挑战和现有数据集中的偏见。确定当前的研究差距，我们建议未来的弹性一致性策略，防止不断发展的攻击，自动化越狱检测以及对道德和社会影响的考虑。这篇评论强调了AI社区内继续进行研究与合作的必要性，以增强LLM安全性并确保其安全部署。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 327 篇论文

### Uncertainty-Guided Enhancement on Driving Perception System via Foundation Models 
[[arxiv](https://arxiv.org/abs/2410.01144)] [[cool](https://papers.cool/arxiv/2410.01144)] [[pdf](https://arxiv.org/pdf/2410.01144)]
> **Authors**: Yunhao Yang,Yuxin Hu,Mao Ye,Zaiwei Zhang,Zhichao Lu,Yi Xu,Ufuk Topcu,Ben Snyder
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: 通过基础模型对驾驶感知系统的不确定性引导增强
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式基础模型为增强驾驶感知系统提供了有希望的进步，但其高度计算和财务成本构成了挑战。我们开发了一种利用基础模型来完善现有驾驶感知模型的预测的方法（例如增强对象分类精度），同时最大程度地减少了使用这些资源密集型模型的频率。该方法定量地表征了感知模型的预测中的不确定性，并且仅当这些不确定性超过预先指定的阈值时才能参与基础模型。具体而言，它通过校准感知模型的置信度得分来表征不确定性，以使用保形预测的正确预测的概率为理论下限。然后，它将图像发送到基础模型和查询以完善预测的查询，仅当感知模型的结果的理论结合低于阈值时。此外，我们提出了一种时间推理机制，该机制通过整合历史预测来增强预测准确性，从而导致理论上的范围更紧密。该方法基于驾驶数据集的定量评估，该方法证明了预测准确性的10％至15％的提高，并将基础模型的查询数量减少了50％。

### FMBench: Benchmarking Fairness in Multimodal Large Language Models on Medical Tasks 
[[arxiv](https://arxiv.org/abs/2410.01089)] [[cool](https://papers.cool/arxiv/2410.01089)] [[pdf](https://arxiv.org/pdf/2410.01089)]
> **Authors**: Peiran Wu,Che Liu,Canyu Chen,Jun Li,Cosmin I. Bercea,Rossella Arcucci
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: FMBENCH：在医疗任务上的多模式大语言模型中的基准测试
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的进步具有显着改善的医疗任务绩效，例如视觉问题答案（VQA）和报告生成（RG）。但是，尽管在医疗保健中重要性，但这些模型在各种人群中的公平性仍然没有得到充实。这种监督部分是由于现有医疗多模式数据集缺乏人口统计学多样性，这使公平评估变得复杂。作为回应，我们提出了FMBench，这是第一个旨在评估各种人口属性的MLLM表现公平性的基准。 FMBENCH具有以下关键功能：1：它包括四个人群属性：在零摄影设置下，在VQA和RG上，跨两个任务，种族，种族，语言和性别。 2：我们的VQA任务是自由形式的，增强了现实世界的适用性，并减轻与预定义选择相关的偏见。 3：我们同时利用词汇指标和基于LLM的指标，与临床评估保持一致，不仅要评估语言准确性，而且从临床角度评估模型。此外，我们引入了一种新的指标，公平感知的性能（FAP），以评估MLLM在各种人口属性中的表现。我们彻底评估了八个最先进的开源MLLM的性能和公平性，包括一般和医疗MLLM，范围从7B到26B参数在拟议的基准上。我们的目标是FMBENCH协助研究社区提炼模型评估，并推动该领域的未来进步。所有数据和代码将在接受后发布。

### Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you! 
[[arxiv](https://arxiv.org/abs/2410.01023)] [[cool](https://papers.cool/arxiv/2410.01023)] [[pdf](https://arxiv.org/pdf/2410.01023)]
> **Authors**: Jiwan Chung,Seungwon Lim,Jaehyun Jeon,Seungbeen Lee,Youngjae Yu
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: Accepted as main paper in EMNLP 2024
- **标题**: 视觉语言模型可以通过视觉提示解决文本歧义吗？让视觉双关语告诉你！
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 人类具有多模式素养，使他们能够积极整合来自各种方式的信息以形成推理。面对诸如文本中的词汇歧义之类的挑战，我们将其补充其他方式，例如缩略图图像或教科书插图。机器是否有可能获得类似的多模式理解能力？作为回应，我们以图像解释（UNVIEN）的形式介绍了双关语，这是一种新颖的基准测试，旨在评估多模式输入在解决词汇歧义方面的影响。双关语是由于其内在的歧义而成为此评估的理想主题。我们的数据集包括1,000个双关语，每个双关语都伴随着一个解释这两种含义的图像。我们提出了三个多模式挑战，并通过注释来评估多模式素养的不同方面；双关基，歧义和重建。结果表明，当给定视觉上下文时，各种苏格拉底模型和视觉语言模型在纯文本模型上会改善，尤其是随着任务的复杂性的增加。

### ViDAS: Vision-based Danger Assessment and Scoring 
[[arxiv](https://arxiv.org/abs/2410.00477)] [[cool](https://papers.cool/arxiv/2410.00477)] [[pdf](https://arxiv.org/pdf/2410.00477)]
> **Authors**: Pranav Gupta,Advith Krishnan,Naman Nanda,Ananth Eswar,Deeksha Agarwal,Pratham Gohil,Pratyush Goel
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: Preprint
- **标题**: Vidas：基于视觉的危险评估和评分
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一个新颖的数据集，旨在通过解决量化视频内容中的危险的挑战并确定类似人类的大语言模型（LLM）评估者的挑战，旨在推进危险分析和评估。这是通过汇编100个YouTube视频的集合来实现的，其中包含各种活动。每个视频都由人类参与者注释，他们提供了从0（对人类无危险）到10（威胁生命）的危险评级，并具有精确的时间戳，表明危险越高。此外，我们利用LLM使用视频摘要独立评估这些视频中的危险水平。我们介绍了对人与LLM危险评估之间对齐的多模式元评估的平均平方误差（MSE）得分。我们的数据集不仅为视频内容中的危险评估提供了新的资源，而且还证明了LLM在实现类似人类评估方面的潜力。

### Deep Multimodal Fusion for Semantic Segmentation of Remote Sensing Earth Observation Data 
[[arxiv](https://arxiv.org/abs/2410.00469)] [[cool](https://papers.cool/arxiv/2410.00469)] [[pdf](https://arxiv.org/pdf/2410.00469)]
> **Authors**: Ivica Dimitrovski,Vlatko Spasev,Ivan Kitanovski
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: 深层多模式融合，用于遥感地球观测数据的语义分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 遥感图像的准确语义细分对于各种地球观察应用至关重要，例如土地覆盖地图，城市规划和环境监测。但是，各个数据源通常会列出此任务的局限性。非常高的分辨率（VHR）航空影像提供了丰富的空间细节，但无法捕获有关土地覆盖变化的时间信息。相反，卫星图像时间序列（坐着）捕获时间动态，例如植被的季节性变化，但空间分辨率有限，因此很难区分细尺度对象。本文提出了一个晚期的融合深度学习模型（LF-DLM）进行语义分割，以利用VHR空中图像和坐着的互补优势。提出的模型由两个独立的深度学习分支组成。一个分支集成了来自非显示器捕获的空中图像的详细纹理，并具有多轴视觉变压器（MAXVIT）主干。另一个分支使用带有时间注意编码器（U-TAE）的U-NET捕获了Sentinel-2卫星图像时间序列的复杂时空动力学。这种方法导致Flair数据集的最新结果，这是使用多源光学图像的大规模基准进行土地覆盖分割的基准。这些发现突出了多模式融合在提高遥感应用中语义分割的准确性和鲁棒性方面的重要性。

### GSPR: Multimodal Place Recognition Using 3D Gaussian Splatting for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2410.00299)] [[cool](https://papers.cool/arxiv/2410.00299)] [[pdf](https://arxiv.org/pdf/2410.00299)]
> **Authors**: Zhangshuo Qi,Junyi Ma,Jingyi Xu,Zijie Zhou,Luqi Cheng,Guangming Xiong
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: 8 pages, 6 figures
- **标题**: GSPR：使用3D高斯分裂进行自动驾驶的多模式位置识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 位置识别是一个至关重要的组成部分，使自动驾驶汽车能够在GPS污染环境中获得定位结果。近年来，多模式位置识别方法引起了人们越来越多的关注。他们通过利用来自不同模式的互补信息来克服单峰传感器系统的弱点。但是，大多数现有方法通过缺乏可解释性的特征级别或描述符级融合来探索跨模式相关性。相反，最近提出的3D高斯脱落通过将不同的模态调成明确的场景表示，从而为多模式融合提供了新的视角。在本文中，我们提出了一个以3D高斯分裂为基础的多模式识别网络，名为GSPR。它将多视图RGB图像和LiDar Point Clouds与提议的多模式高斯分裂结合到时空统一的场景表示中。由3D图卷积和变压器组成的网络旨在从高斯场景中提取时空特征和全局描述符，以进行位置识别。对三个数据集的广泛评估表明，我们的方法可以有效利用多视图摄像机和激光镜头的互补优势，在保持稳固的概括能力的同时，达到SOTA位置识别性能。我们的开源代码将在https://github.com/qizs-bit/gspr上发布。

### Delving Deep into Engagement Prediction of Short Videos 
[[arxiv](https://arxiv.org/abs/2410.00289)] [[cool](https://papers.cool/arxiv/2410.00289)] [[pdf](https://arxiv.org/pdf/2410.00289)]
> **Authors**: Dasong Li,Wenjie Li,Baili Lu,Hongsheng Li,Sizhuo Ma,Gurunandan Krishnan,Jian Wang
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: Accepted to ECCV 2024. Project page: https://github.com/dasongli1/SnapUGC_Engagement
- **标题**: 深入研究短视频的参与预测
- **领域**: 计算机视觉和模式识别,多媒体,社交和信息网络
- **摘要**: 了解和建模用户生成的内容的普及（UGC）在社交媒体平台上的简短视频提出了一个至关重要的挑战，对内容创建者和推荐系统的影响很大。这项研究深入研究了预测用户互动有限的新发布视频参与的复杂性。令人惊讶的是，我们的发现表明，以前视频质量评估数据集的平均意见分数与视频参与水平不密切相关。为了解决这个问题，我们引入了一个大量的数据集，其中包括Snapchat的90,000个现实世界中的UGC简短视频。我们提出了两个指标：标准化平均观察百分比（NAWP）和参与延续率（ECR）来描述短视频的参与度。研究了全面的多模式功能，包括视觉内容，背景音乐和文本数据，以增强参与预测。借助提出的数据集和两个关键指标，我们的方法展示了其仅从视频内容中预测短视频参与的能力。

### On Large Uni- and Multi-modal Models for Unsupervised Classification of Social Media Images: Nature's Contribution to People as a case study 
[[arxiv](https://arxiv.org/abs/2410.00275)] [[cool](https://papers.cool/arxiv/2410.00275)] [[pdf](https://arxiv.org/pdf/2410.00275)]
> **Authors**: Rohaifa Khaldi,Domingo Alcaraz-Segura,Ignacio Sánchez-Herrera,Javier Martinez-Lopez,Carlos Javier Navarro,Siham Tabik
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: 17 pages, 9 figures
- **标题**: 关于社交媒体图像的无监督分类的大型单型和多模式模型：自然对人们作为案例研究的贡献
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 事实证明，社交媒体图像是了解人类与文化遗产，生物多样性和自然等重要主题的互动的宝贵信息来源。由于视觉内容的高度多样性和大量的体积，将此类图像分为许多没有标签的语义上有意义的群集的任务是具有挑战性的。另一方面，大型视觉模型（LVM），大语言模型（LLM）和大型视觉语言模型（LVLM）的最新进展为探索新的生产和可扩展解决方案提供了重要的机会。这项工作建议，分析和比较基于一种或多种最先进的LVM，LLM和LVLM的各种方法，以将社交媒体图像映射到许多预定义的类中。作为一个案例研究，我们考虑了了解人类与自然之间相互作用的问题，也称为自然对人或文化生态系统服务（CES）的贡献。我们的实验表明，绩效最高的方法（精度高于95％）仍然需要创建一个小标记的数据集。其中包括微调的LVM Dinov2和LVLM Llava-1.5与微型LLM结合使用。 LVLMS是LVLM，特别是专有的GPT-4模型和公共LLAVA-1.5模型，最高的无监督方法是达到84％以上的精度。此外，LVM dinov2在10次学习设置中应用时，以83.99％的精度提供了竞争成果，与LVLM LLAVA-1.5的性能非常匹配。

### Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation 
[[arxiv](https://arxiv.org/abs/2410.00263)] [[cool](https://papers.cool/arxiv/2410.00263)] [[pdf](https://arxiv.org/pdf/2410.00263)]
> **Authors**: Kun Yuan,Vinkle Srivastav,Nassir Navab,Nicolas Padoy
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: Accepted at the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Main Track
- **标题**: 通过层次知识增强进行过程感知的手术视频语言预处理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 由于知识域间隙和多模式数据的稀缺，手术视频预读（VLP）面临着独特的挑战。这项研究旨在通过解决有关手术讲座视频中文本信息丢失以及外科VLP的时空挑战的问题来弥合差距。我们提出了一种分层知识增强方法，并提出了一种新型程序编码的外科手术知识提示视频语言预处理（PESKAVLP）框架，以解决这些问题。知识增强使用大型语言模型（LLM）来完善和丰富外科手术概念，从而提供全面的语言监督并降低过度拟合的风险。 Peskavlp将语言监督与视觉自学意义相结合，构建硬性否定样本并采用动态时间扭曲（DTW）损失函数，以有效理解跨模式的程序对齐。关于多个公共外科手术的理解和跨模式检索数据集的广泛实验表明，我们提出的方法显着改善了零拍传递的性能，并为手术现场理解的进一步发展提供了通才的视觉表现。

### MM-Conv: A Multi-modal Conversational Dataset for Virtual Humans 
[[arxiv](https://arxiv.org/abs/2410.00253)] [[cool](https://papers.cool/arxiv/2410.00253)] [[pdf](https://arxiv.org/pdf/2410.00253)]
> **Authors**: Anna Deichler,Jim O'Regan,Jonas Beskow
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: MM-CONV：虚拟人类的多模式对话数据集
- **领域**: 计算机视觉和模式识别,计算语言学,图形,人机交互
- **摘要**: 在本文中，我们提出了一个使用VR耳机捕获的新型数据集，以记录物理模拟器中参与者之间的对话（AI2-THOR）。我们的主要目的是通过将丰富的上下文信息纳入参考设置中扩展共同语音的手势生成的领域。参与者参与了各种对话方案，所有参与者都是基于参考通信任务。该数据集提供了丰富的多模式记录集，例如运动捕获，语音，凝视和场景图。这个全面的数据集旨在通过提供多样化和上下文丰富的数据来增强3D场景中手势生成模型的理解和发展。

### ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer 
[[arxiv](https://arxiv.org/abs/2410.00086)] [[cool](https://papers.cool/arxiv/2410.00086)] [[pdf](https://arxiv.org/pdf/2410.00086)]
> **Authors**: Zhen Han,Zeyinzi Jiang,Yulin Pan,Jingfeng Zhang,Chaojie Mao,Chenwei Xie,Yu Liu,Jingren Zhou
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: ACE：通过扩散变压器的全面创建者和编辑器按照说明
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 扩散模型已成为一种强大的生成技术，并被发现适用于各种情况。大多数现有的基础扩散模型主要是为文本引导的视觉生成而设计的，并且不支持多模式条件，这对于许多视觉编辑任务至关重要。该限制阻止了这些基本扩散模型在视觉生成领域的统一模型，例如自然语言处理领域中的GPT-4。在这项工作中，我们建议Ace，一个全方位的创作者和编辑器，与在广泛的视觉生成任务中的专家模型相比，它的性能可比性可比。为了实现这一目标，我们首先引入了称为长篇文化条件单元（LCU）的统一条件格式，并提出了一种新型的基于变压器的扩散模型，该模型使用LCU作为输入，旨在跨各种一代和编辑任务进行联合培训。此外，我们提出了一种有效的数据收集方法，以解决缺乏可用培训数据的问题。它涉及通过基于综合或基于聚类的管道来获取成对图像，并通过利用微调的多模式大语言模型来提供准确的文本指令。为了全面评估模型的性能，我们建立了在各种视觉生成任务中手动注释对数据的基准。广泛的实验结果证明了我们模型在视觉生成领域的优势。得益于我们模型的多合一功能，我们可以轻松地构建一个多模式聊天系统，该系统使用单个模型作为后端来响应图像创建的任何交互式请求，从而避免使用视觉代理中通常使用的繁琐管道。代码和模型将在项目页面上提供：https：//ali-vilab.github.io/ace-page/。

### Multimodal Power Outage Prediction for Rapid Disaster Response and Resource Allocation 
[[arxiv](https://arxiv.org/abs/2410.00017)] [[cool](https://papers.cool/arxiv/2410.00017)] [[pdf](https://arxiv.org/pdf/2410.00017)]
> **Authors**: Alejandro Aparcedo,Christian Lopez,Abhinav Kotta,Mengjie Li
> **First submission**: 2024-09-14
> **First announcement**: 2024-10-02
> **comment**: 7 pages, 4 figures, 1 table
- **标题**: 快速灾难响应和资源分配的多模式停电预测
- **领域**: 计算机视觉和模式识别,信号处理
- **摘要**: 由于气候变化，极端天气事件越来越普遍，带来了重大风险。为了减轻进一步的损害，必须向可再生能源转移。不幸的是，受影响最大的人数不足的社区经常获得基础设施的改善。我们提出了一个新颖的视觉时空框架，用于预测夜间灯（NTL），停电严重程度以及大飓风之前和之后的位置。我们解决方案的核心是视觉时空图神经网络（VST-GNN），从图像中学习空间和时间连贯性。我们的工作使人们意识到代表性不足的地区迫切需要增强的能源基础设施，例如未来的光伏（PV）部署。通过确定停电的严重性和定位，我们的倡议旨在提高政策制定者和社区利益相关者的意识并迅速采取行动。最终，这项努力旨在通过脆弱的能源基础设施来增强地区的权力，增强对高危社区的弹性和可靠性。

### EMMA: Efficient Visual Alignment in Multi-Modal LLMs 
[[arxiv](https://arxiv.org/abs/2410.02080)] [[cool](https://papers.cool/arxiv/2410.02080)] [[pdf](https://arxiv.org/pdf/2410.02080)]
> **Authors**: Sara Ghazanfari,Alexandre Araujo,Prashanth Krishnamurthy,Siddharth Garg,Farshad Khorrami
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: Emma：多模式LLM中有效的视觉对齐
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 多模式的大型语言模型（MLLM）最近通过利用Vision Foundation模型将图像的核心概念编码为表示形式，从而表现出令人印象深刻的通用能力。然后将它们与指令结合在一起，并由语言模型处理以产生高质量的响应。尽管在增强语言组成部分方面取得了重大进展，但挑战仍在最佳地融合了特定于任务适应性的语言模型中的视觉编码。最近的研究重点是通过模态适应模块改善这种融合，但付出了显着提高模型复杂性和培训数据需求的代价。在本文中，我们提出了Emma（有效的多模式适应），这是一个轻巧的跨模块模块，旨在有效地融合视觉和文本编码，为语言模型生成指令感知的视觉表示。我们的主要贡献包括：（1）一种有效的早期融合机制，将视觉和语言表示与最小的添加参数（型号增加了0.2％），（2）深入的解释性分析阐明了所提出的方法的内部机制； （3）全面的实验表明了MLLM的专业和一般基准的明显改进。经验结果表明，艾玛（Emma）将多个任务的绩效提高了9.3％，同时显着提高了针对幻觉的鲁棒性。我们的代码可从https://github.com/saraghazanfari/emma获得

### Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval 
[[arxiv](https://arxiv.org/abs/2410.02027)] [[cool](https://papers.cool/arxiv/2410.02027)] [[pdf](https://arxiv.org/pdf/2410.02027)]
> **Authors**: Kyle Buettner,Adriana Kovashka
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: EMNLP 2024 Main - Short
- **标题**: 在多模式，多语言检索的训练中量化翻译与本地感知之间的差距
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多语言视觉语言模型缺乏，可以正确地说明跨语言和文化的图像标题中反映的感知差异。在这项工作中，通过多模式的多语言检索案例研究，我们量化了现有的模型灵活性。我们从经验上表明了来自德国本地观念的标题的培训与已被机器翻译或从英语翻译成德语的字幕的标题之间的表现差距。为了解决这些差距，我们进一步提出和评估标题增强策略。尽管我们实现了召回改进（+1.3），但差距仍然存在，这表明社区未来工作的开放领域。

### UlcerGPT: A Multimodal Approach Leveraging Large Language and Vision Models for Diabetic Foot Ulcer Image Transcription 
[[arxiv](https://arxiv.org/abs/2410.01989)] [[cool](https://papers.cool/arxiv/2410.01989)] [[pdf](https://arxiv.org/pdf/2410.01989)]
> **Authors**: Reza Basiri,Ali Abedi,Chau Nguyen,Milos R. Popovic,Shehroz S. Khan
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: 13 pages, 3 figures, ICPR 2024 Conference (PRHA workshop)
- **标题**: 溃疡：一种多模式的方法利用大型语言和视力模型进行糖尿病足溃疡图像转录
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 糖尿病足溃疡（DFUS）是住院和下肢截肢的主要原因，给患者和医疗保健系统带来了重大负担。 DFU的早期发现和准确的分类对于预防严重的并发症至关重要，但是由于获得专业服务的机会有限，许多患者会延迟接受护理。远程医疗已成为一种有前途的解决方案，改善了获得护理的机会，并减少了对面对面访问的需求。通过从图像中实现自动检测，分类和监视，将人工智能和模式识别的整合到远程医疗中进一步增强了DFU管理。尽管在人工智能驱动的DFU图像分析方面取得了进步，但尚未探讨大型语言模型在DFU图像转录中的应用。为了解决这一差距，我们介绍了Ulcergpt，这是一种新型的多模式方法，利用大型语言和视觉模型进行DFU图像转录。该框架结合了先进的视觉和语言模型，例如大型语言和视觉助手以及聊天生成的预训练的变压器，通过共同检测，分类和本地化感兴趣的区域来转录DFU图像。通过专家临床医生评估的公共数据集上的详细实验，Ulcergpt在DFU转录的准确性和效率方面表现出了有希望的结果，从而为临床医生通过远程医疗提供及时的护理提供了潜在的支持。

### OCC-MLLM-Alpha:Empowering Multi-modal Large Language Model for the Understanding of Occluded Objects with Self-Supervised Test-Time Learning 
[[arxiv](https://arxiv.org/abs/2410.01861)] [[cool](https://papers.cool/arxiv/2410.01861)] [[pdf](https://arxiv.org/pdf/2410.01861)]
> **Authors**: Shuxin Yang,Xinhan Di
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: Accepted by ECCV 2024 Observing and Understanding Hands in Action Workshop (5 pages, 3 figures, 2 tables). arXiv admin note: substantial text overlap with arXiv:2410.01261
- **标题**: OCC-MLLM-Alpha：授权多模式的大语言模型，以理解具有自律的测试时间学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 在现有的大规模视觉语言多模式模型中，理解对遮挡对象存在差距。当前最新的多模式模型无法通过通用的视觉编码器和监督的学习策略来描述遮挡对象。因此，我们介绍了一个多模式的大型语言框架，并在3D代支持的情况下引入了相应的自我监督学习策略。在评估大规模数据集SOMVIDEO的评估中，我们开始进行实验[18]。最初的结果表明，与最先进的VLM模型相比，16.92％的改善。

### PixelBytes: Catching Unified Representation for Multimodal Generation 
[[arxiv](https://arxiv.org/abs/2410.01820)] [[cool](https://papers.cool/arxiv/2410.01820)] [[pdf](https://arxiv.org/pdf/2410.01820)]
> **Authors**: Fabien Furfaro
> **First submission**: 2024-09-16
> **First announcement**: 2024-10-03
> **comment**: 12 pages, 4 figures
- **标题**: PixelBytes：捕获多模式生成的统一表示
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 该报告介绍了PixelBytes，这是一种统一多模式表示学习的方法。从图像变形金刚，Pixelcnn和Mamba-bytes等序列模型中汲取灵感，我们探索将文本，音频，动作状态和像素化图像（Sprites）（Sprites）（Sprites）集成到凝聚力表示中。我们在PixelBytes Pokemon数据集和最佳控制数据集上进行了实验。我们的调查涵盖了各种模型架构，包括复发性神经网络（RNN），状态空间模型（SSM）和基于注意力的模型，重点是双向处理和我们的PXBY嵌入技术。我们根据数据减少策略和自回旋学习评估了模型，特别是研究了预测性和自回归模式中的长期短期记忆（LSTM）网络。我们的结果表明，在这种情况下，自回归模型的性能优于预测模型。此外，我们发现扩散模型可以应用于控制问题和并行化的生成。 PixelBytes旨在为开发多模式数据处理和生成的基础模型做出贡献。该项目的代码，模型和数据集可在线提供。

### From Experts to the Public: Governing Multimodal Language Models in Politically Sensitive Video Analysis 
[[arxiv](https://arxiv.org/abs/2410.01817)] [[cool](https://papers.cool/arxiv/2410.01817)] [[pdf](https://arxiv.org/pdf/2410.01817)]
> **Authors**: Tanusree Sharma,Yujin Potter,Zachary Kilhoffer,Yun Huang,Dawn Song,Yang Wang
> **First submission**: 2024-09-14
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 从专家到公众：政治敏感视频分析中的多模式模型
- **领域**: 计算机视觉和模式识别,人工智能,计算机与社会
- **摘要**: 本文通过个人和集体审议研究了多模式大语言模型（MM-LLM）的治理，重点是对政治敏感视频的分析。我们进行了两步研究：首先，对10位记者的访谈建立了对专家视频解释的基线理解；其次，来自公众的114个人使用包含性。EAI进行了审议，该平台通过分散的自治组织（DAO）机制促进了民主决策。我们的发现表明，尽管专家强调了情感和叙事，但公众将事实清晰度，局势的客观性和情感中立性优先考虑。此外，我们探讨了不同治理机制的影响：二次与加权排名投票，等于20-80的功率分布对用户对AI应如何行为的决策。具体而言，二次投票增强了人们对自由民主和政治平等的看法，并且对AI更乐观的参与者认为，投票过程具有更高的参与式民主。我们的结果表明，采用DAO机制来帮助使AI治理民主化的潜力。

### Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks 
[[arxiv](https://arxiv.org/abs/2410.01744)] [[cool](https://papers.cool/arxiv/2410.01744)] [[pdf](https://arxiv.org/pdf/2410.01744)]
> **Authors**: Mengzhao Jia,Wenhao Yu,Kaixin Ma,Tianqing Fang,Zhihan Zhang,Siru Ouyang,Hongming Zhang,Meng Jiang,Dong Yu
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: Our code is available at https://github.com/Jill0001/Leopard
- **标题**: 豹子：文本多图像任务的视觉语言模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 文本富含文本的图像作为指导整体理解的中心视觉元素，在现实世界中普遍存在，例如演示幻灯片，扫描文档和网页快照。涉及多个文本图像的任务尤其具有挑战性，因为它们不仅需要了解单个图像的内容，还需要有关跨多个视觉输入的关系间和逻辑流的推理。尽管这些方案非常重要，但由于两个主要挑战，当前的多模式大语言模型（MLLM）很难处理此类任务：（1）缺乏在文本富含文本的多图像方面的高质量指令调谐数据集，以及（2）在平衡图像分辨率与视觉功能序列之间的困难。为了应对这些挑战，我们提出了豹子，这是一种专门用于处理涉及多个文本丰富图像的视觉任务的MLLM。首先，我们策划了大约一百万个高质量的多模式指令调查数据，这些数据是针对文本丰富的多图像场景量身定制的。其次，我们开发了一个自适应高分辨率多图像编码模块，以基于输入图像的原始纵横比和分辨率动态优化视觉序列长度的分配。跨广泛基准的实验证明了我们的模型在文本丰富，多图像评估和一般领域评估中的竞争性能方面的出色能力。

### RADAR: Robust Two-stage Modality-incomplete Industrial Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2410.01737)] [[cool](https://papers.cool/arxiv/2410.01737)] [[pdf](https://arxiv.org/pdf/2410.01737)]
> **Authors**: Bingchen Miao,Wenqiao Zhang,Juncheng Li,Siliang Tang,Zhaocheng Li,Haochen Shi,Jun Xiao,Yueting Zhuang
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 雷达：强大的两阶段模态 - 均匀的工业异常检测
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 多模式工业异常检测（MIAD），利用3D点云和2D RGB图像来识别产品的异常区域，在工业质量检查中起着至关重要的作用。但是，常规的MIAD设置为所有2D和3D模式都配对的前提是，忽视了一个事实，即由于缺少模态，从现实世界中收集的多模式数据通常是不完美的。因此，在实践中非常需要证明对模态分配数据的鲁棒性的微型模型。为了应对这一实用挑战，我们介绍了一项首先研究，该研究全面研究了模态分配的工业异常检测（MIIAD），以考虑多模式信息可能不完整的不完美学习环境。毫不奇怪，我们发现大多数现有的MIAD方法无法应对MIIAD挑战，从而导致我们开发的MIIAD基准的绩效降低很大。在本文中，我们提出了一种新型的两阶段鲁棒模态 - 刻度融合和检测框架，缩写为雷达。我们的启动哲学是在MIIAD中增强两个阶段，改善了多模式变压器的鲁棒性：i）在功能融合中，我们首先探索学习模式成熟的讲座，从而指导预先训练的多模态变压器以鲁棒性地适应了基于适应性的各种适应性参数，并在适应性的方案上进行了适应性的超级研究； ii）在异常检测中，我们构建了一个实用假杂种模块，以突出模态组合的独特性，进一步增强了MIIAD模型的鲁棒性。我们的实验结果表明，提出的雷达在我们新创建的MIIAD数据集的有效性和鲁棒性方面显着超过了传统的MIAD方法，强调了其实际的应用值。

### LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.01620)] [[cool](https://papers.cool/arxiv/2410.01620)] [[pdf](https://arxiv.org/pdf/2410.01620)]
> **Authors**: Zhenyue Qin,Yu Yin,Dylan Campbell,Xuansheng Wu,Ke Zou,Yih-Chung Tham,Ninghao Liu,Xiuzhen Zhang,Qingyu Chen
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: 2025 NAACL: Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics Project Page: https://kfzyqin.github.io/lmod/
- **标题**: LMOD：大型视觉模型的大型多模式眼科数据集和基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 威胁性眼部疾病的普遍性是全球重大负担，许多病例仍未被诊断或诊断为有效治疗。大型视力语言模型（LVLM）有可能帮助理解解剖学信息，诊断眼病，起草解释和后续计划，从而减少临床医生的负担并改善眼部护理的机会。但是，有限的基准可用于评估LVLM在眼科特定应用中的性能。在这项研究中，我们介绍了LMOD，这是一种大规模的多模式眼科基准，包括（1）五种眼科成像模态的21,993个实例：光学相干断层扫描，颜色眼底照片，扫描激光镜，扫描激光眼镜，镜头照片以及曲格的场景； （2）自由文本，人口统计学和疾病生物标志物信息； （3）主要眼科特异性应用，例如解剖信息理解，疾病诊断和亚组分析。此外，我们为封闭式，开源和医疗领域的13位最先进的LVLM代表进行了基准测试。结果表明，与其他领域相比，LVLM在眼科中的性能下降。系统错误分析进一步确定了六种主要故障模式：错误分类，不弃权，推理不一致，幻觉，没有理由的断言以及缺乏特定领域的知识。相比之下，由于基准表现出很高的精度，因此对这些任务进行了专门培训的监督神经网络。这些发现强调了对眼科特定LVLM的开发和验证中对基准的紧迫需求。

### MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation 
[[arxiv](https://arxiv.org/abs/2410.01594)] [[cool](https://papers.cool/arxiv/2410.01594)] [[pdf](https://arxiv.org/pdf/2410.01594)]
> **Authors**: Mingzhen Sun,Weining Wang,Yanyuan Qiao,Jiahui Sun,Zihan Qin,Longteng Guo,Xinxin Zhu,Jing Liu
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: Accepted by ACM MM 2024
- **标题**: MM-LDM：多模式潜在扩散模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 声音生成（SVG）是一项音频视频联合一代任务，该任务受到高维信号空间，不同的数据格式和不同内容信息模式的挑战。为了解决这些问题，我们为SVG任务引入了一种新型的多模式潜在扩散模型（MM-LDM）。我们首先通过将音频和视频数据转换为单个图像或几个图像来统一音频和视频数据的表示。然后，我们引入了一个层次多模式自动编码器，该自动编码器为每种模态构建低级感知潜在空间和共享的高级语义特征空间。前一个空间在感知上等同于每种方式的原始信号空间，但大大降低了信号尺寸。后一个空间可以弥合方式之间的信息差距，并提供更有见地的跨模式指导。我们提出的方法以显着的质量和效率提高来实现新的最新结果。具体而言，我们的方法对所有评估指标以及对景观和AIST ++数据集的更快培训和抽样速度进行了全面改进。此外，我们探索了其在开放域的声音视频生成，长时间的视频生成，音频延续，视频延续和有条件的单模式生成任务以进行全面评估上的表现，我们的MM-LDM表现出令人兴奋的适应性和泛化能力。

### EUFCC-CIR: a Composed Image Retrieval Dataset for GLAM Collections 
[[arxiv](https://arxiv.org/abs/2410.01536)] [[cool](https://papers.cool/arxiv/2410.01536)] [[pdf](https://arxiv.org/pdf/2410.01536)]
> **Authors**: Francesc Net,Lluis Gomez
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: ECCV Workshop (AI4DH2024)
- **标题**: eufcc-cir：一个构成的图像检索数据集用于Glam收藏
- **领域**: 计算机视觉和模式识别
- **摘要**: 人工智能与数字人文科学的交集使研究人员能够以更大的深度和规模探索文化遗产收藏。在本文中，我们提出了EUFCC-CIR，这是一个专为画廊，图书馆，档案馆和博物馆（Glam）收藏的图像检索（CIR）而设计的数据集。我们的数据集建立在EUFCC-340K图像标签数据集的顶部，并包含超过180K注释的CIR三胞胎。每个三重态由多模式查询（输入图像以及描述所需属性操作的简短文本）和一组相关目标图像组成。 EUFCC-CIR数据集填补了数字人文特定资源的现有空白。我们通过与其他现有CIR数据集相比突出显示其独特质量并评估了几个零摄影CIR基线的性能，从而证明了EUFCC-CIR数据集的价值。

### Toward a Holistic Evaluation of Robustness in CLIP Models 
[[arxiv](https://arxiv.org/abs/2410.01534)] [[cool](https://papers.cool/arxiv/2410.01534)] [[pdf](https://arxiv.org/pdf/2410.01534)]
> **Authors**: Weijie Tu,Weijian Deng,Tom Gedeon
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: 17 pages, 10 figures, extension of NeurIPS'23 work: A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP). arXiv admin note: text overlap with arXiv:2402.07410
- **标题**: 在剪辑模型中对鲁棒性进行整体评估
- **领域**: 计算机视觉和模式识别
- **摘要**: 对比性语言图像预训练（剪辑）模型已显示出巨大的潜力，尤其是在各种分布变化的零击中分类中。这项工作以对整体分类鲁棒性的现有评估，旨在通过引入几种新观点来对剪辑进行更全面的评估。首先，我们研究了它们对特定视觉因素变化的鲁棒性。其次，我们评估了两个关键的安全目标 - 信心不确定性和分布外检测 - 仅分类精度。第三，我们评估了剪辑模型桥接图像和文本方式的技巧。第四，我们将考试扩展到剪辑模型中的3D意识，超越了传统的2D图像理解。最后，我们探讨了现代大型多模型（LMM）中的视觉和语言编码之间的相互作用，这些模型（LMM）将夹子用作视觉主链，重点是这种相互作用如何影响分类的鲁棒性。在每个方面，我们都会考虑六个因素对剪辑模型的影响：模型架构，训练分布，训练集大小，微调，对比度损失和测试时间提示。我们的研究发现了一些以前未知的见解。例如，剪辑中的视觉编码器的架构在对3D腐败的鲁棒性中起着重要作用。做出预测时，剪辑模型往往会对形状表现出偏见。此外，这种偏见往往会在成像网上进行微调后减少。诸如Llava之类的视觉模型，利用剪辑视觉编码器，可以单独使用夹具对具有挑战性的类别的分类性能表现出好处。我们的发现有望为增强剪辑模型的鲁棒性和可靠性提供宝贵的指导。

### Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion 
[[arxiv](https://arxiv.org/abs/2410.01506)] [[cool](https://papers.cool/arxiv/2410.01506)] [[pdf](https://arxiv.org/pdf/2410.01506)]
> **Authors**: Dexuan Ding,Lei Wang,Liyun Zhu,Tom Gedeon,Piotr Koniusz
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)
- **标题**: 多模式特征融合的图形运算符的可学习扩展
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 在计算机视觉任务中，功能通常来自不同的表示，域（例如室内和室外）以及模式（例如文本，图像和视频）。有效地融合这些功能对于稳健性能至关重要，尤其是在视觉模型（如视觉模型）等强大的预训练模型的可用性中。但是，常见的融合方法（例如串联，元素操作和非线性技术）通常无法捕获结构关系，深层特征相互作用，并且遇到了跨领域或模态特征的效率低下或损失。在本文中，我们通过构造在不同级别的特征关系的关系图，例如，剪辑，框架，斑点，贴片，令牌等，通过构造特征关系来捕获更深层的交互，我们通过迭代图形扩展图形来通过迭代图形的图形更新，并引入可学习的图形融合操作员来集成这些扩展的融合，从而扩展了图形。我们的方法是以关系为中心，在同质空间中运行，并且在数学上是通过多线性多项式的数学定义的，类似于元素的关系评分聚合。我们证明了基于图的融合方法在视频异常检测中的有效性，显示了多代表，多模式和多域特征融合任务的强劲性能。

### The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs 
[[arxiv](https://arxiv.org/abs/2410.01417)] [[cool](https://papers.cool/arxiv/2410.01417)] [[pdf](https://arxiv.org/pdf/2410.01417)]
> **Authors**: Hong Li,Nanxi Li,Yuanjie Chen,Jianbin Zhu,Qinlu Guo,Cewu Lu,Yong-Lu Li
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: Accepted by ICLR 2025. Project page: https://mvig-rhos.com/llm_inception
- **标题**: 链接迷宫：导航多模式LLMS的关联迷宫
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 多模式的大语言模型（MLLM）表现出令人印象深刻的能力。但是，与人类智能相比，已经发现了许多MLLM的缺陷，$ \ textit {e.g。} $，幻觉。为了推动MLLMS的研究，社区致力于通过复杂的任务来构建更大的基准。在本文中，我们提出了基准测试，以一种必不可少但通常被忽略的智能：$ \ textbf {cousection} $，它是人类链接观察和事先练习记忆的基本能力。为了全面研究MLLM在协会上的表现，我们制定了协会任务，并根据形容词和动词语义概念设计了标准基准。我们提出了一个方便的$ \ textbf {notation-free} $ construction方法，为我们的关联任务转换一般数据集，而不是昂贵的数据注释和策划。同时，我们设计了一个严格的数据完善过程，以消除RAW数据集中的混淆。在此数据库的基础上，我们建立了三个级别的关联任务：单步，同步和异步关联。此外，我们对MLLMS的零射击关联功能进行了全面调查，以解决多个维度，包括三种不同的内存策略，包括开源和封闭源MLLM，尖端的Experts（MOE）模型以及人类专家的参与。我们的系统调查表明，当前的开源MLLM在我们的关联任务中始终表现出较差的能力，即使目前最新的GPT-4V（视觉）也比人类也有很大的差距。我们相信我们的基准将为未来的MLLM研究铺平道路。 $ \ textit {我们的数据和代码可在以下网址提供：} $ https：//mvig-rhos.com/llm_inception。

### SHAP-CAT: A interpretable multi-modal framework enhancing WSI classification via virtual staining and shapley-value-based multimodal fusion 
[[arxiv](https://arxiv.org/abs/2410.01408)] [[cool](https://papers.cool/arxiv/2410.01408)] [[pdf](https://arxiv.org/pdf/2410.01408)]
> **Authors**: Jun Wang,Yu Mao,Nan Guan,Chun Jason Xue
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: Shap-cat：一种可解释的多模式框架，通过虚拟染色和基于Shapley的多模式融合来增强WSI分类
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模型模型在组织病理学方面表现出了希望。但是，大多数多模型模型基于H \＆E和基因组学，采用越来越复杂但黑色框的设计。在我们的论文中，我们提出了一种名为Shap-cat的新型可解释的多模式框架，该框架使用基于Shapley-value的降低技术来有效多模式融合。从两个配对的方式开始 -  H \＆E和IHC图像，我们采用虚拟染色技术来通过生成新的临床相关模式来增强有限的输入数据。从图像方式提取了轻巧的袋级表示，并将基于沙普利的基于莎普利的机制降低。对于袋级表示的每​​个维度，计算归因值以指示输入特定维度的变化如何影响模型输出。这样，我们选择了每种图像模态的袋级表示的一些顶级重要维度，以供晚融合。我们的实验结果表明，融合合成方式的提议的SHAP-CAT框架显着提高了模型性能，BCI的准确性增加了5 \％，IHC4BC-ER的增加8 \％，IHC4BC-PR-PR数据集增加了11 \％。

### Backdooring Vision-Language Models with Out-Of-Distribution Data 
[[arxiv](https://arxiv.org/abs/2410.01264)] [[cool](https://papers.cool/arxiv/2410.01264)] [[pdf](https://arxiv.org/pdf/2410.01264)]
> **Authors**: Weimin Lyu,Jiachen Yao,Saumya Gupta,Lu Pang,Tao Sun,Lingjie Yi,Lijie Hu,Haibin Ling,Chao Chen
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: ICLR 2025
- **标题**: 带有分布数据的背景视觉语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉模型（VLM）的出现代表了将计算机视觉与大语言模型（LLMS）整合在一起，以从视觉输入中生成详细的文本说明。尽管他们的重要性越来越大，但VLM的安全，尤其是针对后门攻击的安全性。此外，先前的工作通常会假设攻击者可以访问原始培训数据，这通常是不现实的。在本文中，我们解决了一个更实用，更具挑战性的情况，在该场景中，攻击者必须仅依靠分布（OOD）数据。我们介绍了VLOOD（带有分发数据的后门视觉语言模型），这是一种新颖的方法，具有两个关键的贡献：（1）在复杂的图像到文本任务中展示对VLM的后门攻击，同时最大程度地减少有毒输入中的原始语义的降级，而不需要访问原始技术。我们对图像字幕和视觉问题回答（VQA）任务的评估确认了VLOOD的有效性，揭示了VLMS中的关键安全性脆弱性，并为未来的研究奠定了基础，以确保确保多模型模型抵抗复杂威胁。

### OCC-MLLM:Empowering Multimodal Large Language Model For the Understanding of Occluded Objects 
[[arxiv](https://arxiv.org/abs/2410.01261)] [[cool](https://papers.cool/arxiv/2410.01261)] [[pdf](https://arxiv.org/pdf/2410.01261)]
> **Authors**: Wenmo Qiu,Xinhan Di
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: Accepted by CVPR 2024 T4V Workshop (5 pages, 3 figures, 2 tables)
- **标题**: OCC-MLLM：授权多模式的大型语言模型，以理解被封闭的对象
- **领域**: 计算机视觉和模式识别
- **摘要**: 在现有的大规模视觉语言多模式模型中，理解对遮挡对象存在差距。当前的最新多模式模型无法通过通用视觉编码器描述视觉语言多模型模型的遮挡对象令人满意的结果。另一个挑战是包含图像文本对的数据集有限，并具有大量的遮挡对象。因此，我们引入了一种新型的多模型模型，该模型应用了新设计的视觉编码器来了解RGB图像中的遮挡对象。我们还引入了一个大规模的视觉语言对数据集，用于训练大规模的视觉语言多模式并理解遮挡的对象。我们开始与最新模型进行比较的实验。

### AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark 
[[arxiv](https://arxiv.org/abs/2410.03051)] [[cool](https://papers.cool/arxiv/2410.03051)] [[pdf](https://arxiv.org/pdf/2410.03051)]
> **Authors**: Wenhao Chai,Enxin Song,Yilun Du,Chenlin Meng,Vashisht Madhavan,Omer Bar-Tal,Jenq-Neng Hwang,Saining Xie,Christopher D. Manning
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Accepted to ICLR 2025. Code, docs, weight, benchmark and training data are all avaliable at https://rese1f.github.io/aurora-web/
- **标题**: Auroracap：高效，性能的视频详细字幕和新的基准测试
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频详细字幕是一项关键任务，旨在生成视频内容的全面和连贯的文本描述，从而使视频理解和发电都受益。在本文中，我们提出了基于大型多模式的视频字幕仪Auroracap。我们遵循最简单的体系结构设计，而没有其他参数进行时间建模。为了解决冗长的视频序列引起的间接费用，我们实施了令牌合并策略，减少了输入视觉令牌的数量。令人惊讶的是，我们发现这种策略几乎不会导致绩效丧失。 Auroracap在各种视频和图像字幕上显示出卓越的性能，例如，在Flickr30k上获得88.9的苹果酒，击败GPT-4V（55.3）和Gemini-1.5 Pro（82.2）。但是，现有的视频字幕基准仅包括简单的描述，包括几十个单词，限制了该领域的研究。因此，我们开发了VDC，这是一个视频详细的字幕标题标题，并带有一千多个精心注释的结构化字幕。此外，我们提出了一种新的LLM辅助度量VDCSCORE，以改善评估，该指标采用了分裂和拼接策略，将长字幕评估转换为多个简短的提问 - 答案对。在人类ELO排名的帮助下，我们的实验表明，这种基准更好地与人类对视频详细字幕质量的判断相关。

### Leveraging Retrieval Augment Approach for Multimodal Emotion Recognition Under Missing Modalities 
[[arxiv](https://arxiv.org/abs/2410.02804)] [[cool](https://papers.cool/arxiv/2410.02804)] [[pdf](https://arxiv.org/pdf/2410.02804)]
> **Authors**: Qi Fan,Hongyu Yuan,Haolin Zuo,Rui Liu,Guanglai Gao
> **First submission**: 2024-09-18
> **First announcement**: 2024-10-04
> **comment**: Under reviewing
- **标题**: 利用检索增强方法在缺失方式下以多模式情绪识别
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式情绪识别利用完整的多模式信息和鲁棒的多模式表示形式来获得高性能。但是，完全模态完整性的理想条件通常不适用于现实，并且似乎似乎缺少某些方式。例如，由于传感器故障或网络带宽问题而缺少视频，音频或文本数据，这给MER研究带来了巨大的挑战。传统方法从完整的模式中提取有用的信息，并重建缺失的方式以学习强大的多模式表示。这些方法为该领域的研究奠定了坚实的基础，并在一定程度上减轻了多模式情绪识别的困难。但是，仅依靠内部重建和多模式联合学习具有其局限性，尤其是当丢失的信息对于情绪识别至关重要时。为了应对这一挑战，我们提出了一个新颖的检索框架，即缺失模态多模式识别（拉默），该框架引入了类似的多模式情感数据，以增强缺失方式下情绪识别的性能。通过利用包含相关多模式情绪数据的数据库，我们可以检索相似的多模式情感信息，以填补缺失模态留下的空白。各种实验结果表明，我们的框架优于缺失模式MER任务中现有的最新方法。我们的整个项目可在https://github.com/wooyoohl/retrieval_augment_mer上公开使用。

### BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering 
[[arxiv](https://arxiv.org/abs/2410.02768)] [[cool](https://papers.cool/arxiv/2410.02768)] [[pdf](https://arxiv.org/pdf/2410.02768)]
> **Authors**: Jin Chen,Kaijing Ma,Haojian Huang,Jiayu Shen,Han Fang,Xianghao Zang,Chao Ban,Zhongjiang He,Hao Sun,Yanmei Kang
> **First submission**: 2024-09-17
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: Bovila：通过基于LLM的自我问候和答案进行引导视频语言对齐
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式模型的发展一直在迅速发展，有些表现出了显着的功能。但是，注释视频对仍然昂贵且不足。以视频问题回答（VideoQA）任务为例，人类注释的问题和答案通常仅涵盖视频的一部分，并且类似的语义也可以通过不同的文本表格来表达，从而导致视频未充分利用。为了解决这个问题，我们提出了Bovila，Bovila是一个自我训练的框架，可以通过基于LLM的自我询问和答案在培训期间提出质疑样品，这有助于模型利用视频信息和LLM的内部知识，以更彻底地提高模态统一性。为了过滤不良的自我生成的问题，我们介绍了证据深度学习（EDL），以估计不确定性并通过评估上下文中的方式对齐方式来评估自我生成问题的质量。据我们所知，这项工作是第一个探索基于LLM的自我训练框架以进行模态对准的工作。我们在五个强大的VideoQA基准测试中评估了Bovila，它的表现优于几种最先进的方法，并证明了其有效性和通用性。此外，我们提供了自我训练框架和基于EDL的不确定性滤波机制的广泛分析。该代码将在https://github.com/dunknsabsw/bovila上提供。

### Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos 
[[arxiv](https://arxiv.org/abs/2410.02763)] [[cool](https://papers.cool/arxiv/2410.02763)] [[pdf](https://arxiv.org/pdf/2410.02763)]
> **Authors**: Jianrui Zhang,Mu Cai,Yong Jae Lee
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Project Page: https://vinoground.github.io
- **标题**: Vinoground：用简短视频仔细检查LMM在密集的时间推理上
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 最近，人们越来越多的情绪，现代大型多模型模型（LMM）解决了与简短视频理解有关的大多数关键挑战。结果，学术界和行业都在逐步将注意力转移到理解长期视频所带来的更复杂的挑战上。但是，真的是这样吗？我们的研究表明，即使处理简短的视频，LMMS仍然缺乏许多基本的推理功能。我们介绍了Vinoground，这是一种时间反事实LMM评估基准，其中包含1000个短而自然的视频包对。我们证明，现有的LMM严重难以区分不同动作和对象转换之间的时间差异。例如，最佳模型GPT-4O仅在我们的文本和视频分数上获得约50％，与人类基线约为90％相比，差距很大。所有开源的多模型模型和基于夹的模型的性能都更糟，主要产生随机的机会性能。通过这项工作，我们阐明了一个事实，即短视频中的时间推理是一个尚未得到充分解决的问题。数据集和评估代码可在https://vinoground.github.io上获得。

### FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.02761)] [[cool](https://papers.cool/arxiv/2410.02761)] [[pdf](https://arxiv.org/pdf/2410.02761)]
> **Authors**: Zhipei Xu,Xuanyu Zhang,Runyi Li,Zecheng Tang,Qing Huang,Jian Zhang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: Fakeshield：可解释的图像伪造检测和通过多模式大语模型的定位
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 生成AI的快速发展是一把双刃剑，它不仅有助于创建内容，而且还使图像操纵更加容易，更难检测到。尽管当前的图像伪造检测和本地化（IFDL）方法通常是有效的，但它们倾向于面临两个挑战：\ textbf {1）}黑盒性质，具有未知检测原理，\ textbf {2）}有限的概括限制了多样化的篡改方法（例如，photoshop，photoshop，deephop，deephop，aigc-c.-编辑）。为了解决这些问题，我们提出了可解释的IFDL任务和设计Fakeshield，这是一个能够评估图像真实性，生成篡改的区域口罩并基于像素级别和图像级别篡改线索的判断基础的多模式框架。此外，我们利用GPT-4O来增强现有的IFDL数据集，创建多模式篡改描述数据集（MMTD-Set）来培训Fakeshield的篡改分析功能。同时，我们结合了一个域名引导的可解释的伪造检测模块（DTE-FDM）和一个多模式伪造的定位模块（MFLM），以解决各种类型的篡改检测解释，并实现以详细的文本描述为指导的伪造定位。广泛的实验表明，Fakeshield有效地检测并定位了各种篡改技术，与以前的IFDL方法相比，提供了可解释和优越的解决方案。

### Contrastive Localized Language-Image Pre-Training 
[[arxiv](https://arxiv.org/abs/2410.02746)] [[cool](https://papers.cool/arxiv/2410.02746)] [[pdf](https://arxiv.org/pdf/2410.02746)]
> **Authors**: Hong-You Chen,Zhengfeng Lai,Haotian Zhang,Xinze Wang,Marcin Eichner,Keen You,Meng Cao,Bowen Zhang,Yinfei Yang,Zhe Gan
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Preprint
- **标题**: 对比性局部语言图像预训练
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 对比性语言图像预训练（剪辑）已成为培训视觉编码器生成图像/文本表示促进各种应用的著名方法。最近，剪辑已被广泛采用作为多模式大语模型（MLLM）的视觉骨干，以连接图像输入以进行语言交互。剪辑作为视觉基础模型的成功依赖于在图像级别对齐网络爬行的嘈杂文本注释。然而，此类标准可能不足以用于需要细粒度表示表示的下游任务，尤其是当地区级别的理解要求MLLM时。在本文中，我们提高了剪辑的本地化能力，并有多种进步。我们提出了一种通过与区域文本对比损失和模块补充剪辑来补充夹子，提出了一种称为对比的局部语言图像预训练（CLOC）的预训练方法。我们制定了一个新概念，即迅速的嵌入，编码器会产生易于转换为空间提示的区域表示形式的图像嵌入。为了支持大规模的预训练，我们设计了一个富含视觉的和空间定位的字幕框架，以便于以大规模生成区域文本伪​​标签。通过扩展多达数十亿个带注释的图像，CLOC可以为图像区域识别和检索任务提供高质量的区域嵌入，并且可以替换夹子以增强MLLM，尤其是在参考和接地任务上。

### AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity 
[[arxiv](https://arxiv.org/abs/2410.02745)] [[cool](https://papers.cool/arxiv/2410.02745)] [[pdf](https://arxiv.org/pdf/2410.02745)]
> **Authors**: Zhibin Lan,Liqiang Niu,Fandong Meng,Wenbo Li,Jie Zhou,Jinsong Su
> **First submission**: 2024-09-20
> **First announcement**: 2024-10-04
> **comment**: Preprint
- **标题**: AVG-LALAVA：具有自适应视觉粒度的大型多模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 最近，在处理高分辨率图像时，主要的LMM通常将它们分为多个局部图像和一个全局图像，这将导致大量的视觉令牌。在这项工作中，我们介绍了AVG-LALAVA，该LMM可以根据输入图像和指令自适应地选择适当的视觉粒度。这种方法不仅减少了视觉令牌的数量和加快推断的速度，而且还可以改善整体模型性能。具体而言，我们基于llava-next介绍了以下模块：（a）视觉粒度缩放器，其中包括多个池层，以获得具有不同粒度的视觉令牌； （b）一个视觉粒度路由器，其中包括一个变压器层，MLP层和选民层，用于根据图像和指令选择适当的视觉粒度。此外，我们提出了RGLF，这是一种新型的训练范式，旨在使路由器预测的粒度与LMM的偏好保持一致，而无需其他手动注释的数据。广泛的实验和分析表明，AVG-LALAVA在11个基准中实现了卓越的性能，并且显着减少了视觉令牌的数量和加快推理的速度（例如，视觉令牌的85.3％降低了85.3％，而AI2D Benchmark的推理速度增加了2.53 $ \倍。

### Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models 
[[arxiv](https://arxiv.org/abs/2410.02740)] [[cool](https://papers.cool/arxiv/2410.02740)] [[pdf](https://arxiv.org/pdf/2410.02740)]
> **Authors**: Zhengfeng Lai,Vasileios Saveris,Chen Chen,Hong-You Chen,Haotian Zhang,Bowen Zhang,Juan Lao Tebar,Wenze Hu,Zhe Gan,Peter Grasch,Meng Cao,Yinfei Yang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: CV/ML
- **标题**: 重新访问预训练多模式模型中的大规模图像捕获数据
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 多模式模型的最新进展突出了改写字幕对提高性能的价值，但仍然存在关键挑战。例如，虽然合成字幕通常提供较高的质量和图像文本对齐，但尚不清楚它们是否可以完全替换高alttexts：合成字幕的作用及其与原始网络爬行的AltTexts在预训练中的相互作用仍然不太了解。此外，不同的多模式基础模型可能对特定字幕格式具有独特的偏好，但是确定每个模型的最佳字幕的努力仍然有限。在这项工作中，我们提出了一种新颖，可控且可扩展的字幕管道，旨在生成针对各种多模型模型量身定制的各种字幕格式。通过检查简短的合成字幕（SSC）作为案例研究，我们系统地探索了它们的效果以及与剪辑，多峰LLMS和扩散模型等模型之间的效果以及与AltTexts的相互作用。我们的发现表明，保持合成字幕和alttexts的混合方法可以胜过单独使用合成字幕，改善对齐方式和性能，每个模型都表现出对特定字幕格式的偏好。这种全面的分析为优化字幕策略提供了宝贵的见解，从而进步了多模式基础模型的预培训。

### Video Instruction Tuning With Synthetic Data 
[[arxiv](https://arxiv.org/abs/2410.02713)] [[cool](https://papers.cool/arxiv/2410.02713)] [[pdf](https://arxiv.org/pdf/2410.02713)]
> **Authors**: Yuanhan Zhang,Jinming Wu,Wei Li,Bo Li,Zejun Ma,Ziwei Liu,Chunyuan Li
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/
- **标题**: 使用合成数据进行视频指令调整
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视频大型多模型模型（LMM）的开发受到了策划网络中大量高质量原始数据的困难。为了解决这个问题，我们通过创建专门针对视频指导遵循的高质量合成数据集（即llava-video-178K）提出了一种替代方法。该数据集包括关键任务，例如详细的字幕，开放式问题索问题（QA）和多项选择QA。通过对该数据集进行培训，结合了现有的视觉指令调整数据，我们介绍了新的视频LMM Llava-Video。我们的实验表明，LLAVA-VIDEO在各种视频基准中实现了强劲的性能，突出了我们数据集的有效性。我们计划发布数据集，其生成管道和模型检查点。

### LLaVA-Critic: Learning to Evaluate Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.02712)] [[cool](https://papers.cool/arxiv/2410.02712)] [[pdf](https://arxiv.org/pdf/2410.02712)]
> **Authors**: Tianyi Xiong,Xiyao Wang,Dong Guo,Qinghao Ye,Haoqi Fan,Quanquan Gu,Heng Huang,Chunyuan Li
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Accepted by CVPR 2025; Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic
- **标题**: llava-Critic：学习评估多峰模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 我们介绍了Llava-Critic，这是第一个开源大型多模型（LMM），设计为通才评估者，以评估各种多模式任务的性能。 Llava-Critic使用高质量的评论家指令遵循数据集进行了培训，该数据集包含各种评估标准和方案。我们的实验证明了该模型在两个关键领域的有效性：（1）LMM-AS-A-A-Gudge，其中Llava-Critic提供了可靠的评估得分，在多个评估基准上与GPT模型相当或超过GPT模型； （2）偏好学习，在偏好学习中生成奖励信号，从而增强模型对齐功能。这项工作强调了开源LMM在自我评估和评估中的潜力，为将来研究LMMS的可扩展的超人对齐反馈机制奠定了基础。

### NL-Eye: Abductive NLI for Images 
[[arxiv](https://arxiv.org/abs/2410.02613)] [[cool](https://papers.cool/arxiv/2410.02613)] [[pdf](https://arxiv.org/pdf/2410.02613)]
> **Authors**: Mor Ventura,Michael Toker,Nitay Calderon,Zorik Gekhman,Yonatan Bitton,Roi Reichart
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: nl-eye：图像的绑架NLI
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 视觉语言模型（VLM）的机器人是否会警告我们是否检测到湿地板？最近的VLM表现出了令人印象深刻的能力，但是它们推断成果和原因的能力仍然没有得到充实。为了解决这个问题，我们介绍了NL-EYE，这是一种旨在评估VLMS视觉绑架推理技能的基准。 NL-EYE将绑架性的自然语言推理（NLI）任务适应视觉域，要求模型根据前提图像评估假设图像的合理性并解释其决策。 NL-EYE由350个精心策划的三胞胎示例（1,050张图像）组成，涵盖了各种推理类别：物理，功能，逻辑，情感，文化和社会。数据策划过程涉及两个步骤 - 编写文本描述并使用文本形象模型生成图像，既需要大量的人类参与，以确保高质量和具有挑战性的场景。我们的实验表明，VLM在NL-EYE上经常以随机基线水平进行，而人类在合理性预测和解释质量方面都表现出色。这表明现代VLM的绑架推理能力缺乏。 NL-EYE代表了开发能够为现实世界应用（包括事故预防机器人和生成的视频验证）的鲁棒多模式推理的VLM的关键步骤。

### IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of Both Driver and Passengers 
[[arxiv](https://arxiv.org/abs/2410.02592)] [[cool](https://papers.cool/arxiv/2410.02592)] [[pdf](https://arxiv.org/pdf/2410.02592)]
> **Authors**: Zihan Fang,Zheng Lin,Senkang Hu,Hangcheng Cao,Yiqin Deng,Xianhao Chen,Yuguang Fang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: 16 pages, 17 figures
- **标题**: IC3M：驾驶员和乘客异常状态的车内多模式多模式监视
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,系统与控制
- **摘要**: 最近，在车内监测已成为一种有前途的技术，用于检测驾驶员的早期异常状态并提供及时的警报以防止交通事故。尽管具有多模式数据的训练模型增强了异常状态检测的可靠性，但标记的数据的稀缺性和类分布的不平衡会阻碍临界异常状态特征的提取，从而大大恶化训练性能。此外，由于环境和硬件限制而缺少模式，进一步加剧了异常状态识别的挑战。更重要的是，监测乘客的异常健康状况，尤其是在老年护理中，至关重要，但仍然没有被置换。为了应对这些挑战，我们介绍了IC3M，这是一个有效的基于摄像头的多模式框架，用于监视汽车中的驾驶员和乘客。我们的IC3M包括两个关键模块：自适应阈值伪标记策略和缺失的模态重建。前者根据类分布定制了针对不同类别的伪标记阈值，从而产生了平衡的伪标签，以有效地指导模型培训，而后者则利用了从有限标签中学到的跨模式关系，以通过从可用模态转移分布来准确地恢复缺失的模式。广泛的实验结果表明，在有限的标记数据和严重缺失的模态下，IC3M的精度，精度和召回率都优于准确性，精度和回忆的最先进的基准。

### Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment 
[[arxiv](https://arxiv.org/abs/2410.02505)] [[cool](https://papers.cool/arxiv/2410.02505)] [[pdf](https://arxiv.org/pdf/2410.02505)]
> **Authors**: Kai Liu,Ziqing Zhang,Wenbo Li,Renjing Pei,Fenglong Song,Xiaohong Liu,Linghe Kong,Yulun Zhang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: 10 pages, 5 figures. The code and models will be available at https://github.com/Kai-Liu001/Dog-IQA
- **标题**: Dog-iqa：用于混合元素图像质量评估的标准引导的零摄像机MLLM
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 图像质量评估（IQA）是几乎所有计算机视野中所有模型性能的黄金标准。但是，它仍然遭受分布外的概括能力和昂贵的培训成本的影响。为了解决这些问题，我们提出了Dog-iqa，这是一种标准的零摄像混合物IQA方法，该方法是无训练的，并利用了多模式大语言模型（MLLMS）的特殊先验知识。为了获得准确的IQA得分，即与人类一致的分数，我们设计了一个基于MLLM的推理管道，模仿了人类专家。详细说明，Dog-iqa应用了两种技术。首先，Dog-iqa用特定的标准进行了客观评分，该标准使用MLLM的行为模式并最大程度地减少主观因素的影响。其次，Dog-iqa全面地将本地语义对象和整个图像作为输入，并汇总其分数，利用本地和全球信息。与无训练方法相比，我们提议的Dog-IQA与跨数据库场景中的基于训练的方法相比，与无训练方法相比，达到了最先进的表现（SOTA）。我们的代码将在https://github.com/kai-liu001/dog-iqa上找到。

### DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM 
[[arxiv](https://arxiv.org/abs/2410.02492)] [[cool](https://papers.cool/arxiv/2410.02492)] [[pdf](https://arxiv.org/pdf/2410.02492)]
> **Authors**: Xuchen Li,Shiyu Hu,Xiaokun Feng,Dailing Zhang,Meiqi Wu,Jing Zhang,Kaiqi Huang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Preprint, Under Review
- **标题**: DTVLT：基于LLM的视觉语言跟踪的多模式多种文本基准
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉语言跟踪（VLT）已成为一个尖端的研究领域，利用语言数据来增强使用多模式输入的算法，并扩大传统的单个对象跟踪（SOT）的范围，以涵盖视频理解应用程序。尽管如此，大多数VLT基准仍取决于每个视频的简洁，人宣传的文本描述。这些描述通常在捕获视频内容动态的细微差别并且缺乏语气多样性的情况下缺乏，受其统一的细节水平和固定注释频率的限制。结果，算法倾向于默认“记住答案”策略，这与对视频内容有更深入了解的核心目标不同。幸运的是，大语言模型（LLM）的出现使文本的产生能够产生。这项工作利用LLM为代表性SOT基准生成各种语义注释（就文本长度和粒度而言），从而建立了一种新型的多模式基准。具体来说，我们（1）基于五个突出的VLT和SOT基准，提出了一种新的视觉语言跟踪基准，名为DTVLT，包括三个子任务：短期跟踪，长期跟踪和全球实例跟踪。 （2）考虑到语义信息的程度和密度，我们在基准中提供了四个粒度文本。我们希望这种多生成一代策略旨在促进VLT和视频理解研究的有利环境。 （3）我们对DTVLT进行了全面的实验分析，评估了各种文本对跟踪性能的影响，并希望现有算法的确定性能瓶颈可以支持在VLT和视频理解方面的进一步研究。拟议的基准，实验结果和工具包将在http://videocube.aitestunion.com/上逐渐发布。

### A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond 
[[arxiv](https://arxiv.org/abs/2410.02362)] [[cool](https://papers.cool/arxiv/2410.02362)] [[pdf](https://arxiv.org/pdf/2410.02362)]
> **Authors**: Shubhi Bansal,Sreeharish A,Madhava Prasath J,Manikandan S,Sreekanth Madisetty,Mohammad Zia Ur Rehman,Chandravardhan Singh Raghaw,Gaurav Duggal,Nagendra Kumar
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: 对医学图像分析的MAMBA体系结构进行的全面调查：分类，细分，修复及其他
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 曼巴（Mamba）是国家空间模型的一种特殊情况，它在医学图像分析中替代了基于模板的深度学习方法的替代方案。尽管变形金刚是强大的体系结构，但它们具有缺点，包括二次计算复杂性和无法有效地解决远程依赖关系。这种限制会影响医学成像中大型且复杂的数据集的分析，那里存在许多空间和时间关系。相比之下，Mamba提供的好处使其非常适合医学图像分析。它具有线性时间的复杂性，这是对变压器的重大改进。 Mamba处理没有注意机制的较长序列，可以更快地推断并需要更少的记忆。 Mamba在合并多模式数据，提高诊断准确性和患者预后方面还表现出强烈的表现。本文的组织使读者可以逐步欣赏Mamba在医学成像中的功能。我们首先定义SSM和模型的核心概念，包括S4，S5和S6，然后探索诸如纯Mamba，U-NET变体等MAMBA体系结构以及具有卷积神经网络，变形金刚和图形神经网络的混合模型。我们还涵盖了MAMBA的优化，技术和适应，扫描，数据集，应用程序，实验结果，并以其医学成像的挑战和未来方向结束。这篇评论旨在证明曼巴在克服医学成像中现有障碍的同时，为该领域的创新进步铺平道路。 Github可在此工作中审查的医学领域应用的MAMBA架构综合列表。

### SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial Attack 
[[arxiv](https://arxiv.org/abs/2410.02240)] [[cool](https://papers.cool/arxiv/2410.02240)] [[pdf](https://arxiv.org/pdf/2410.02240)]
> **Authors**: Zihao Pan,Weibin Wu,Yuhang Cao,Zibin Zheng
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: SCA：高效的语义一致的不受限制攻击
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在敏感环境中部署的基于神经网络的深层系统容易受到对抗性攻击的影响。不受限制的对抗性攻击通常会操纵图像的语义内容（例如颜色或纹理），以创建有效且呈现现实主义的对抗性示例。最近的工作利用扩散反演过程将图像映射到潜在空间中，在该空间中，通过引入扰动来操纵高级语义。但是，它们通常会导致deno的输出的大量语义扭曲，并且效率低。在这项研究中，我们提出了一个新颖的框架，称为语义一致的不受限制的对抗攻击（SCA），该框架采用反转方法来提取编辑友好的噪声图，并利用多模式大语言模型（MLLM）在整个过程中提供语义指导。在MLLM提供的丰富语义信息的条件下，我们使用一系列对编辑友好的噪声图执行每个步骤的DDPM DeNoising过程，并利用DPM求解器++加速此过程，从而具有具有语义一致性的有效抽样。与现有方法相比，我们的框架可以有效地产生对对抗性示例的有效产生，这些示例表现出最小的可识别语义变化。因此，我们第一次引入语义一致的对抗例子（SCAE）。广泛的实验和可视化已经证明了SCA的效率很高，尤其是比最新的攻击快12倍。我们的研究可以进一步提请人们注意多媒体信息的安全性。

### Estimating Body Volume and Height Using 3D Data 
[[arxiv](https://arxiv.org/abs/2410.02800)] [[cool](https://papers.cool/arxiv/2410.02800)] [[pdf](https://arxiv.org/pdf/2410.02800)]
> **Authors**: Vivek Ganesh Sonar,Muhammad Tanveer Jan,Mike Wells,Abhijit Pandya,Gabriela Engstrom,Richard Shih,Borko Furht
> **First submission**: 2024-09-18
> **First announcement**: 2024-10-04
> **comment**: 6 pages
- **标题**: 使用3D数据估算身体体积和高度
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 准确的体重估计对于适当剂量的基于体重的药物的急诊医学至关重要，但是在紧急情况下，直接测量通常是不切实际的。本文提出了一种非侵入性方法，可通过使用3D成像技术来计算全身体积和身高来估计体重。使用Realsense D415相机来捕获患者的高分辨率深度图，从中生成3D模型。然后，应用凸壳算法来计算总体体积，并通过将点云数据分为多个部分并求和它们的个体体积，从而提高了精度。高度是从3D模型得出的，通过识别身体上的钥匙点之间的距离。这种合并的方法提供了对体重的准确估计，从而提高了精确的体重数据无法获得的医疗干预措施的可靠性。提出的方法显示出在紧急情况下增强患者安全和治疗结果的巨大潜力。

### ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.04659)] [[cool](https://papers.cool/arxiv/2410.04659)] [[pdf](https://arxiv.org/pdf/2410.04659)]
> **Authors**: Ziyue Wang,Chi Chen,Fuwen Luo,Yurui Dong,Yuanchi Zhang,Yuzhuang Xu,Xiaolong Wang,Peng Li,Yang Liu
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: Actiview：评估多模式大语言模型的主动感知能力
- **领域**: 计算机视觉和模式识别
- **摘要**: 积极的感知是一种至关重要的人类能力，涉及基于当前对环境的理解和执行实现该目标的行动的目标。尽管在评估多模式大语言模型（MLLM）方面做出了重大努力，但积极的感知仍在很大程度上被忽略了。为了解决这一差距，我们提出了一种名为Actiview的新型基准测试，以评估MLLM中的主动感知。由于全面评估主动感知是具有挑战性的，因此我们着重于一种专门的视觉问题回答形式（VQA），从而简化了现有MLLM的评估但具有挑战性。给定图像，我们限制了模型的感知字段，要求其基于推理的推理来积极缩小或移动其感知字段，以成功回答该问题。我们对27个模型进行了广泛的评估，包括专有和开源模型，并观察到能够同时阅读和理解多个图像的能力在实现主动感知中起着重要作用。结果表明，MLLM的主动感知能力有显着差距，表明该领域值得更多关注。我们希望我们的基准可以帮助开发MLLM的方法，以更自然和整体的方式理解多模式输入。

### VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.04609)] [[cool](https://papers.cool/arxiv/2410.04609)] [[pdf](https://arxiv.org/pdf/2410.04609)]
> **Authors**: Harshit,Tolga Tasdizen
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: Vista：用于解释多模式模型的视觉和文本注意数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 深度学习的最新发展导致自然语言处理（NLP）与计算机视觉的整合，从而产生了强大的综合视觉和语言模型（VLMS）。尽管它们具有显着的功能，但这些模型在机器学习研究社区中通常被视为黑匣子。这提出了一个关键的问题：图像的哪些部分与文本的特定段相对应，我们如何破译这些关联？了解这些联系对于增强模型透明度，可解释性和可信赖性至关重要。为了回答这个问题，我们提出了一个图像文本对齐的人类视觉注意数据集，该数据集绘制图像区域和相应文本段之间的特定关联。然后，我们将VL模型生成的内部热图与此数据集进行了比较，从而使我们能够分析并更好地了解该模型的决策过程。这种方法旨在通过提供有关这些模型如何使视觉和语言信息如何相结合的见解来提高模型透明度，可解释性和可信度。我们在这些VL模型中就文本引导的视觉显着性检测进行了全面研究。这项研究旨在了解不同模型如何优先考虑并关注对相应文本细分的特定视觉元素，从而更深入地了解其内部机制，并提高我们解释其产出的能力。

### MC-CoT: A Modular Collaborative CoT Framework for Zero-shot Medical-VQA with LLM and MLLM Integration 
[[arxiv](https://arxiv.org/abs/2410.04521)] [[cool](https://papers.cool/arxiv/2410.04521)] [[pdf](https://arxiv.org/pdf/2410.04521)]
> **Authors**: Lai Wei,Wenkai Wang,Xiaoyu Shen,Yu Xie,Zhihao Fan,Xiaojin Zhang,Zhongyu Wei,Wei Chen
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: 21 pages, 14 figures, 6 tables
- **标题**: MC-COT：使用LLM和MLLM Integration的零摄入医学VQA的模块化协作COT框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 在最近的进步中，多模式大语言模型（MLLM）已在特定的医疗图像数据集上进行了微调，以解决医疗视觉问题答案（MED-VQA）任务。但是，这种特定于任务的微调方法的常见方法是昂贵的，需要为每个下游任务单独的模型，从而限制了零击功能的探索。在本文中，我们介绍了MC-COT，这是一种模块化的跨模式协作链（COT）框架，旨在通过利用大型语言模型（LLMS）来增强MED-VQA中MLLM的零弹性性能。 MC-COT通过整合医学知识和特定于任务的指导来改善推理和信息提取，其中LLM提供了各种复杂的医学推理链，MLLM根据LLM的说明提供了对医疗图像的各种观察结果。我们在诸如SLAKE，VQA-RAD和PATH-VQA等数据集上的实验表明，MC-COT在召回率和准确性方面超过了独立的MLLM和各种多模式COT框架。这些发现突出了将背景信息和详细指南纳入复杂零摄像的Med-VQA任务的重要性。

### Video Summarization Techniques: A Comprehensive Review 
[[arxiv](https://arxiv.org/abs/2410.04449)] [[cool](https://papers.cool/arxiv/2410.04449)] [[pdf](https://arxiv.org/pdf/2410.04449)]
> **Authors**: Toqa Alaa,Ahmad Mongy,Assem Bakr,Mariam Diab,Walid Gomaa
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 视频摘要技术：全面评论
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频内容在包括社交媒体，教育，娱乐和监视在内的各种行业之间的快速扩展使视频总结成为了必不可少的研究领域。当前的工作是一项调查，探讨了为视频总结创建的各种方法和方法，强调抽象性和提取性策略。提取性摘要的过程涉及识别源视频中的关键帧或片段，并利用诸如射击边界识别和聚类之类的方法。另一方面，抽象性摘要通过从视频中获取基本内容来创建新内容，使用机器学习模型，例如深度神经网络和自然语言处理，增强学习，注意力机制，生成的对抗性网络和多模式学习。我们还包括结合两种方法的方法，以及讨论现实世界实现中遇到的用途和困难。该论文还涵盖了用于基准这些技术的数据集。这篇综述试图提供有关视频摘要研究的当前状态和未来方向的最先进的知识。

### CoVLM: Leveraging Consensus from Vision-Language Models for Semi-supervised Multi-modal Fake News Detection 
[[arxiv](https://arxiv.org/abs/2410.04426)] [[cool](https://papers.cool/arxiv/2410.04426)] [[pdf](https://arxiv.org/pdf/2410.04426)]
> **Authors**: Devank,Jayateja Kalla,Soma Biswas
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: Accepted in ACCV 2024
- **标题**: COVLM：利用视觉语言模型的共识，用于半监督的多模式假新闻检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项工作中，我们解决了现实世界中挑战性的误解发现检测的挑战，其中一个真实的图像与不正确的标题相结合，用于创建假新闻。这项任务的现有方法假设可用性大量标记的数据，这在现实世界中通常是不切实际的，因为它需要大量的手动干预和域专业知识。相比之下，由于获得了大量未标记的图像文本对的语料库要容易得多，因此，我们提出了一个半监督协议，该协议可以访问有限数量的标记的图像文本对和大量的无标记对。此外，与真实新闻相比，假新闻的发生要小得多，数据集往往会高度失衡，从而使任务更具挑战性。为了实现这一目标，我们提出了一个新颖的框架，即视觉语言模型（COVLM）的共识，该框架使用从标记的数据中得出的阈值生成了无标记对的稳健伪标记。此方法可以自动确定模型的正确阈值参数，以选择自信的伪标签。在具有挑战性的条件下对基准数据集的实验结果，并与最先进的方法进行比较证明了我们框架的有效性。

### MVP-Bench: Can Large Vision--Language Models Conduct Multi-level Visual Perception Like Humans? 
[[arxiv](https://arxiv.org/abs/2410.04345)] [[cool](https://papers.cool/arxiv/2410.04345)] [[pdf](https://arxiv.org/pdf/2410.04345)]
> **Authors**: Guanzhen Li,Yuxi Xie,Min-Yen Kan
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: MVP基础：大型视力 - 语言模型可以像人类一样进行多层次的视觉感知吗？
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 人类在多个层面上进行视觉感知，包括低级对象识别和高级语义解释，例如行为理解。低级细节的细微差异可能会导致高级感知的实质性变化。例如，用枪支持有的购物袋代替枪支的购物袋表明暴力行为，暗示犯罪或暴力活动。尽管在各种多模式任务中取得了重大进步，但大型视觉语言模型（LVLM）仍未探索其进行此类多层次视觉感知的能力。为了研究LVLM和人类之间的感知差距，我们介绍了MVP基础，这是第一个视觉语言基准系统地评估LVLM的低水平和高级视觉感知。我们在自然图像和合成图像上构建MVP基座，以研究操纵内容如何影响模型感知。使用MVP板台，我们诊断了10个开源的视觉感知和2个闭合源LVLM，这表明高级感知任务显着挑战了现有的LVLM。最先进的GPT-4O仅在“是/否”问题上仅能达到56美元的准确性，而低级方案中的$ 74 \％$。此外，自然图像和受操纵图像之间的性能差距表明，当前的LVLM并未像人类那样理解合成图像的视觉语义。我们的数据和代码可在https://github.com/guanzhenli/mvp-bench上公开获取。

### Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model 
[[arxiv](https://arxiv.org/abs/2410.04161)] [[cool](https://papers.cool/arxiv/2410.04161)] [[pdf](https://arxiv.org/pdf/2410.04161)]
> **Authors**: Keda Tao,Jinjin Gu,Yulun Zhang,Xiucheng Wang,Nan Cheng
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: 23 Pages, 28 Figures
- **标题**: 通过多模式引导扩散模型克服现实世界中的虚假幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们介绍了一种新型的多模式指导现实世界恢复（MGFR）技术，旨在提高低质量输入的面部图像恢复质量。利用属性文本提示，高质量的参考图像和身份信息的混合物，MGFR可以减轻通常与生成面部修复方法相关的虚假面部属性和身份的产生。通过合并双控制适配器和两阶段的训练策略，我们的方法有效地利用了多模式的先验信息来实现目标恢复任务。我们还提供了Reface-HQ数据集，其中包括5,000个身份的23,000多个高分辨率面部图像，以满足参考面部训练图像的需求。我们的方法在严重降解下恢复面部细节并允许进行控制的恢复过程，从而提高了身份保存和属性校正的准确性。培训中包括负质量样本和属性提示，进一步完善了该模型生成详细和感知准确图像的能力。

### MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty 
[[arxiv](https://arxiv.org/abs/2410.03860)] [[cool](https://papers.cool/arxiv/2410.03860)] [[pdf](https://arxiv.org/pdf/2410.03860)]
> **Authors**: Leo Bringer,Joey Wilson,Kira Barton,Maani Ghaffari
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: MDMP：具有不确定性的监督运动预测的多模式扩散
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了运动预测（MDMP）的多模式扩散模型，该模型集成并同步骨骼数据和动作的文本描述，以生成精致的长期运动预测，并具有可量化的不确定性。现有的运动预测或运动产生方法仅依赖于先前的动作或文本提示，面临精确或控制的局限性，尤其是在延长持续时间内。我们方法的多模式性质增强了对人类运动的上下文理解，而我们的基于图形的变压器框架有效地捕获了空间和时间运动动力学。结果，我们的模型在准确预测长期动作方面始终优于现有的生成技术。此外，通过利用扩散模型捕获不同预测模式的能力，我们估计不确定性，通过将存在的置信度与每个身体关节的置信度变化，从而显着提高了人类机器人相互作用的空间意识。

### Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.03659)] [[cool](https://papers.cool/arxiv/2410.03659)] [[pdf](https://arxiv.org/pdf/2410.03659)]
> **Authors**: Tinghui Zhu,Qin Liu,Fei Wang,Zhengzhong Tu,Muhao Chen
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: Website: https://darthzhu.github.io/cross-modality-knowledge-conflict/
- **标题**: 大型视力语言模型中的跨模式知识冲突
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 大型视觉模型（LVLM）表现出了在多模式输入上捕获和推理的令人印象深刻的能力。但是，这些模型容易出现参数知识冲突，这是由于其视力和语言组成部分之间的知识的不一致引起的。在本文中，我们正式定义了$ \ textbf {交叉模式参数知识冲突} $的问题，并提出了一种系统的方法来检测，解释和减轻它们。我们介绍了一条管道，该管道可以确定视觉和文本答案之间的冲突，显示了最近LVLMS中跨模态的冲突率持续高，而不管模型大小如何。我们进一步研究了这些冲突如何干扰推理过程，并提出了对比度指标，以辨别其他样本与其他样本。在这些见解的基础上，我们开发了一种新颖的动态对比解码方法，该方法从基于答案信心的较不自信的模态组件中去除了不良的逻辑。对于不提供逻辑的模型，我们还引入了两种基于及时的策略来减轻冲突。我们的方法在Viquae和Infoseek数据集上的准确性都取得了令人鼓舞的提高。具体而言，使用LLAVA-34B，我们提出的动态对比解码提高了平均准确性2.24％。

### Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.03577)] [[cool](https://papers.cool/arxiv/2410.03577)] [[pdf](https://arxiv.org/pdf/2410.03577)]
> **Authors**: Xin Zou,Yizhou Wang,Yibo Yan,Sirui Huang,Kening Zheng,Junkai Chen,Chang Tang,Xuming Hu
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 回答之前两次查看：内存空间视觉回溯以多模式模型缓解幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管具有令人印象深刻的功能，但多模式大语言模型（MLLM）还是容易受到幻觉的影响，尤其是在视觉输入中不存在的内容肯定地制造内容。为了应对上述挑战，我们遵循一个共同的认知过程 - 当人们对关键的观点细节的最初记忆逐渐消失时，第二次查看它们以寻求事实和准确的答案是直观的。因此，我们介绍了记忆空间视觉回溯（MEMVR），这是一种新型的减轻幻觉缓解范式，不需要外部知识检索或其他微调。特别是，当模型不确定甚至是与问题相关的视觉记忆时，我们将视觉提示视为通过Feed Forward Network（FFN）将MLLM重新注入MLLM的补充证据。全面的实验评估表明，MEMVR可显着减轻各种MLLM的幻觉问题，并在一般基准中脱颖而出，而不会引起添加的时间开销，从而强调了其广泛适用性的潜力。

### A Multimodal Framework for Deepfake Detection 
[[arxiv](https://arxiv.org/abs/2410.03487)] [[cool](https://papers.cool/arxiv/2410.03487)] [[pdf](https://arxiv.org/pdf/2410.03487)]
> **Authors**: Kashish Gandhi,Prutha Kulkarni,Taran Shah,Piyush Chaudhari,Meera Narvekar,Kranti Ghag
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: 22 pages, 14 figures, Accepted in Journal of Electrical Systems
- **标题**: 多模式检测的多模式框架
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,计算机科学中的逻辑
- **摘要**: DeepFake技术的快速发展对数字媒体完整性构成了重大威胁。 DeepFakes是使用AI创建的合成媒体，可以令人信服地改变视频和音频以歪曲现实。这产生了错误信息，欺诈和对个人隐私和安全的严重影响的风险。我们的研究通过创新的多模式方法来解决深层蛋糕的关键问题，以视觉和听觉元素为目标。这种全面的策略认识到，人类的感知整合了多个感官输入，尤其是视觉和听觉信息，以完全理解媒体内容。为了进行视觉分析，开发了采用高级特征提取技术的模型，从而提取了九种不同的面部特征，然后应用各种机器学习和深度学习模型。为了获得听觉分析，我们的模型利用MEL光谱图分析进行特征提取，然后应用各种机器学习和深度学习模型。为了实现组合分析，将原始数据集中的真实和深击音频交换以进行测试目的并确保均衡样品。使用我们提出的用于视频和音频分类的模型，即人工神经网络和VGG19，如果将任何一个组件都标识为，则将整体样本归类为Deepfake。我们的多模式框架结合了视觉和听觉分析，精度为94％。

### An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation 
[[arxiv](https://arxiv.org/abs/2410.03334)] [[cool](https://papers.cool/arxiv/2410.03334)] [[pdf](https://arxiv.org/pdf/2410.03334)]
> **Authors**: Ahmed Abdulaal,Hugo Fry,Nina Montaña-Brown,Ayodeji Ijishakin,Jack Gao,Stephanie Hyland,Daniel C. Alexander,Daniel C. Castro
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: X射线价值15个功能：稀疏的自动编码器，用于可解释的放射报告生成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 放射线服务正在经历前所未有的需求，从而增加了对自动放射学报告生成的兴趣。现有的视觉模型（VLMS）遭受幻觉的影响，缺乏解释性，需要昂贵的微调。我们介绍了SAE-RAD，它使用稀疏的自动编码器（SAE）将潜在的表示从预先训练的视觉变压器分解为人解剖功能。我们的混合架构结合了最先进的SAE进步，在保持稀疏性的同时，实现了准确的潜在重建。使用现成的语言模型，我们将地面真实性报告为每个SAE功能的放射学描述，然后将其编译为每个图像的完整报告，消除了对此任务进行微型大型模型的需求。据我们所知，SAE-RAD代表了将机械性解释性技术明确用于下游多模式推理任务的第一个实例。在模拟CXR数据集上，SAE-RAD与最先进的模型相比，SAE-RAD可以实现竞争性放射学特异性指标，同时使用培训的计算资源明显较少。定性分析表明，SAE-RAD学习有意义的视觉概念，并生成与专家解释紧密保持一致的报告。我们的结果表明，SAE可以增强医疗保健中的多模式推理，为现有VLM提供了更可解释的替代方法。

### Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning 
[[arxiv](https://arxiv.org/abs/2410.03321)] [[cool](https://papers.cool/arxiv/2410.03321)] [[pdf](https://arxiv.org/pdf/2410.03321)]
> **Authors**: Minheng Ni,Yutao Fan,Lei Zhang,Wangmeng Zuo
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: Visual-O1：通过多模式多扭转链的推理理解模棱两可的说明
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着大规模模型的发展，语言指令越来越多地用于多模式任务。由于人类的语言习惯，这些说明通常包含在现实情况下的歧义，因此需要将视觉上下文或常识的整合以进行准确的解释。但是，即使是高度智能的大型模型也对模棱两可的说明显示出显着的性能限制，在这种指示中，弱势歧义的推理能力可能导致灾难性错误。为了解决这个问题，本文提出了Visual-O1，这是一个多模式多旋转的推理推理框架。它模拟了人类的多模式多弯曲推理，为高度智能的模型或经验经验提供了即时体验，以了解理解模棱两可的指示。与需要模型具有高智力来理解长文本或执行冗长的复杂推理的传统方法不同，我们的框架并不能显着增加计算开销，并且甚至对于通常智能的模型即使是更一般和有效的。实验表明，我们的方法不仅显着提高了模棱两可的指令模型的性能，而且还可以提高其在一般数据集上的性能。我们的作品突出了人工智能在现实情况下以不确定性和歧义的现实场景中的人类工作的潜力。我们将发布我们的数据和代码。

### Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models 
[[arxiv](https://arxiv.org/abs/2410.03311)] [[cool](https://papers.cool/arxiv/2410.03311)] [[pdf](https://arxiv.org/pdf/2410.03311)]
> **Authors**: Ye Wang,Sipeng Zheng,Bin Cao,Qianshan Wei,Qin Jin,Zongqing Lu
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 瓦迪斯（Quo Vadis），运动产生？从大型语言模型到大型运动模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 受LLM最近成功的启发，人类运动理解领域越来越多地转向大型运动模型的发展。尽管有一些进展，但目前的最新作品仍无法实现真正​​的通才模型，这主要是由于缺乏大型，高质量的运动数据。为了解决这个问题，我们介绍了MotionBase，这是第一个百万级运动生成基准，提供了15倍以前最大数据集的数据量，并具有带有层次详细文本描述的多模式数据。通过利用这个庞大的数据集，我们的大型运动模型在包括看不见的动作在内的广泛动作中表现出强劲的性能。通过系统的调查，我们强调了扩展数据和模型大小的重要性，合成数据和伪标签在减轻数据采集成本方面起着至关重要的作用。此外，我们的研究揭示了现有评估指标的局限性，尤其是在处理室外文本指令时 - 长期以来一直被忽略的问题。除此之外，我们还引入了一种新颖的2D查找方法进行运动令牌化，该方法可保留运动信息并扩大代码书的能力，从而进一步增强了大型运动模型的代表性能力。这项研究的释放和动态基础的释放和从这项研究中获得的见解将为开发更强大和多功能的运动生成模型铺平道路。

### Bridging the Gap between Text, Audio, Image, and Any Sequence: A Novel Approach using Gloss-based Annotation 
[[arxiv](https://arxiv.org/abs/2410.03146)] [[cool](https://papers.cool/arxiv/2410.03146)] [[pdf](https://arxiv.org/pdf/2410.03146)]
> **Authors**: Sen Fang,Sizhou Chen,Yalin Feng,Xiaofeng Zhang,Teik Toe Teoh
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 弥合文本，音频，图像和任何序列之间的差距：使用基于光泽的注释的新方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文提出了一种名为BGTAI的创新方法，以利用基于光泽的注释作为将文本和音频与图像对齐的中间步骤简化多模式的理解。虽然文本和音频输入中的动态时间因素包含各种影响整个句子含义的谓词形容词，但图像，另一方面，图像呈现静态场景。通过将文本和音频表示为忽略复杂语义细微差别的光泽符号，可以实现与图像更好的对齐方式。这项研究探讨了这一想法的可行性，具体来说，我们首先提出了第一个Langue2Gloss模型，然后将其集成到多模型Unibrivl中以进行关节训练。为了通过文本/音频增强光泽的适应性，并克服了多模式训练中的效率和不稳定性问题，我们提出了DS-NET（数据对选择网络），结果滤波器模块和新型的SP-als-loss功能。我们的方法在主要实验中的表现优于先前的多模式模型，这证明了其在增强多模式表示形式方面的功效，并提高了文本，音频，视觉和任何序列模态之间的兼容性。

### TeaserGen: Generating Teasers for Long Documentaries 
[[arxiv](https://arxiv.org/abs/2410.05586)] [[cool](https://papers.cool/arxiv/2410.05586)] [[pdf](https://arxiv.org/pdf/2410.05586)]
> **Authors**: Weihan Xu,Paul Pu Liang,Haven Kim,Julian McAuley,Taylor Berg-Kirkpatrick,Hao-Wen Dong
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: 曲刺：为长纪录片生成预告片
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 预告片是促进娱乐，商业和教育领域内容的有效工具。但是，为长视频创建有效的预告片是具有挑战性的，因为它需要在输入视频上进行远程多模式建模，同时需要维持视听的一致性，管理场景变化并保留输出预告片的事实准确性。由于缺乏公共可用的数据集，因此沿着这一研究方向的进步受到了阻碍。在这项工作中，我们介绍了DocormaryNet，该纪录片是1,269家纪录片与他们的预告片配对的集合，其中包含视频，语音，音乐，声音效果和叙述的多模式数据流。借助DocumentaryNet，我们提出了一个新的两阶段系统，用于从长纪录片中生成预告片。提出的曲植物系统首先使用预告片的大型语言模型从纪录片的转录叙述中生成预告片叙述，然后选择最相关的视觉内容，以通过语言视觉模型伴随生成的叙述。对于旁白视频匹配，我们探讨了两种方法：一种基于预训练的模型，使用验证的对比语言视觉模型和一个深层的顺序模型，该模型了解叙述和视觉效果之间的映射。我们的实验结果表明，基于训练的方法比直接训练的深度自回归模型更有效地识别相关的视觉内容。

### On Feature Decorrelation in Cloth-Changing Person Re-identification 
[[arxiv](https://arxiv.org/abs/2410.05536)] [[cool](https://papers.cool/arxiv/2410.05536)] [[pdf](https://arxiv.org/pdf/2410.05536)]
> **Authors**: Hongjun Wang,Jiyuan Chen,Renhe Jiang,Xuan Song,Yinqiang Zheng
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: 在换衣服的人重新识别的特征解相关上
- **领域**: 计算机视觉和模式识别,人工智能,信息检索
- **摘要**: 改变布的人重新识别（CC-REID）在计算机视觉中构成了重大挑战。一种流行的方法是促使模型专注于因果属性，例如面部特征和发型，而不是混淆诸如服装外观之类的元素。实现此目的的传统方法涉及整合多模式数据或采用手动注释的服装标签，这往往会使模型复杂化并需要大量的人类努力。在我们的研究中，我们证明，在训练过程中仅仅减少特征相关性可以显着提高基线模型的性能。从理论上讲，我们阐明了这种效果，并根据密度比估计引入了一种新颖的正则化技术。该技术旨在最大程度地减少换衣服REID基线的训练过程中的特征相关性。我们的方法是与模型无关的，可提供广泛的增强功能，而无需其他数据或标签。我们通过对普遍的CC-Reid数据集进行全面实验来验证我们的方法，显示其在改善基线模型的概括能力方面的有效性。

### R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions? 
[[arxiv](https://arxiv.org/abs/2410.05474)] [[cool](https://papers.cool/arxiv/2410.05474)] [[pdf](https://arxiv.org/pdf/2410.05474)]
> **Authors**: Chunyi Li,Jianbo Zhang,Zicheng Zhang,Haoning Wu,Yuan Tian,Wei Sun,Guo Lu,Xiaohong Liu,Xiongkuo Min,Weisi Lin,Guangtao Zhai
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: R BENCH：您的大型多模型模型对现实世界的腐败是否强大？
- **领域**: 计算机视觉和模式识别,多媒体,图像和视频处理
- **摘要**: 大型多模型模型（LMM）的出色性能使它们广泛应用于与视觉相关的任务中。但是，现实世界中的各种腐败意味着图像将不像模拟那样理想，这对LMM的实际应用带来了重大挑战。为了解决这个问题，我们介绍了R-Bench，这是一个针对LMMS **现实世界的鲁棒性的基准测试。具体来说，我们：（a）模拟从用户捕获到LMMS接收的完整链接，包括33个损坏维度，包括根据损坏序列的7个步骤，以及基于低级属性的7个组； （b）在腐败之前/之后收集参考/扭曲的图像数据集，包括2,970个问题解答与人类标签； （c）对绝对/相对鲁棒性和基准20主流LMM提出全面评估。结果表明，尽管LMM可以正确处理原始参考图像，但面对扭曲的图像时，它们的性能不稳定，与人类视觉系统相比，稳健性存在很大的差距。我们希望R BEN座位会激发LMM的鲁棒性，**将它们从实验模拟扩展到现实世界应用程序**。查看https://q-future.github.io/r-bench有关详细信息。

### DAAL: Density-Aware Adaptive Line Margin Loss for Multi-Modal Deep Metric Learning 
[[arxiv](https://arxiv.org/abs/2410.05438)] [[cool](https://papers.cool/arxiv/2410.05438)] [[pdf](https://arxiv.org/pdf/2410.05438)]
> **Authors**: Hadush Hailu Gebrerufael,Anil Kumar Tiwari,Gaurav Neupane,Goitom Ybrah Hailu
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: 13 pages, 4 fugues, 2 tables
- **标题**: DAAL：多模式深度学习
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 多模式深度度量学习对于有效捕获面部验证，细粒对象识别和产品搜索等任务中的各种表示至关重要。传统的度量学习方法，无论是基于距离或边缘指标，主要强调班级分离，通常忽略了多模式特征学习必不可少的阶级内部分布。在这种情况下，我们提出了一种称为密度感知的自适应边缘损失（DAAL）的新型损失函数，该功能保留了嵌入的密度分布，同时鼓励每个类别内适应性亚群体的形成。通过采用自适应线策略，DAAL不仅可以增强级别的差异，而且还确保了稳健的阶层间分离，从而促进了有效的多模式表示。基准细粒数据集的全面实验证明了DAAL的出色性能，强调了其在进行检索应用和多模式深度度量学习方面的潜力。

### TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models 
[[arxiv](https://arxiv.org/abs/2410.05239)] [[cool](https://papers.cool/arxiv/2410.05239)] [[pdf](https://arxiv.org/pdf/2410.05239)]
> **Authors**: Rabin Adhikari,Safal Thapaliya,Manish Dhakal,Bishesh Khanal
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Accepted at ACCV 2024 (oral presentation)
- **标题**: tunevlseg：及时调整视觉分割模型的基准
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉模型（VLM）在视觉任务中表现出了令人印象深刻的性能，但是将它们适应新领域通常需要昂贵的微调。迅速调整技术，包括文本，视觉和多模式提示，通过利用可学习的提示来提供有效的替代方案。但是，它们应用于视觉分割模型（VLSM）和重大域移位下的评估。这项工作介绍了一个开源基准测试框架TunevlSeg，以将各种单模式和多模式的提示调谐技术集成到VLSM中，从而使及时调整可用于下游细分数据集和任何数量的下游细分数据集。 Tunevlseg包括$ 6 $ $ 6 $迅速调整策略，以$ 2 $ VLSMS的各种及时的深度为$ 8 $不同的组合。我们测试了$ 8 $不同的医疗数据集的各种提示调整，包括$ 3 $放射学数据集（乳腺肿瘤，超声心动图，胸部X射线病理）和$ 5 $非红外学数据集（息肉，溃疡，溃疡，皮肤癌）和两个自然领域分段数据集。我们的研究发现，从天然域图像到医疗数据，文本及时的调整斗争在重大域的转移下进行。此外，视觉及时调谐，比多模式及时调整的超参数少，通常可以实现与多模式方法竞争的性能，从而使其成为有价值的首次尝试。我们的工作提高了不同及时调整技术对稳健域特异性细分的理解和适用性。源代码可在https://github.com/naamiinepal/tunevlseg上找到。

### Organizing Unstructured Image Collections using Natural Language 
[[arxiv](https://arxiv.org/abs/2410.05217)] [[cool](https://papers.cool/arxiv/2410.05217)] [[pdf](https://arxiv.org/pdf/2410.05217)]
> **Authors**: Mingxuan Liu,Zhun Zhong,Jun Li,Gianni Franchi,Subhankar Roy,Elisa Ricci
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Preprint. Project webpage: https://oatmealliu.github.io/opensmc.html
- **标题**: 使用自然语言组织非结构化图像收集
- **领域**: 计算机视觉和模式识别
- **摘要**: 将非结构化的视觉数据组织到语义簇中是计算机视觉中的关键挑战。传统的深度聚类方法集中于单个数据分区，而多个聚类（MC）方法通过发现不同的聚类解决方案来解决此限制。大型语言模型（LLM）和多模式LLM的兴起通过允许用户定义文本聚类标准来增强MC。但是，希望用户在理解数据之前先手动为大数据集定义此类标准是不切实际的。在这项工作中，我们介绍了开放式语义多重聚类的任务，该任务旨在自动从大型的，非结构化的图像集合中发现聚类标准，从而在不需要人类输入的情况下发现可解释的子结构。我们的框架X群集：探索性聚类，使用文本作为代理，同时推理大型图像集，发现分区标准，以自然语言表达并揭示语义下结构。为了评估X群集，我们介绍了可可-4C和Food-4C基准，每个基准都包含四个分组标准和地面真相注释。我们将X群集应用于各种真实世界的应用程序，例如发现偏见和分析社交媒体形象的受欢迎程度，展示了其效用是组织大型非结构化图像集合并揭示新见解的实用工具。我们将开放我们的代码和基准，以进行可重复性和未来的研究。

### Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality 
[[arxiv](https://arxiv.org/abs/2410.05210)] [[cool](https://papers.cool/arxiv/2410.05210)] [[pdf](https://arxiv.org/pdf/2410.05210)]
> **Authors**: Youngtaek Oh,Jae Won Cho,Dong-Jin Kim,In So Kweon,Junmo Kim
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: EMNLP 2024 (Long, Main). Project page: https://ytaek-oh.github.io/fsc-clip
- **标题**: 保留预先训练的VLM的多模式功能，以改善视觉语言构图
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 在本文中，我们提出了一种新方法，以增强预训练的视觉和语言模型（VLM）中的组成理解，而无需牺牲零拍的多模式任务的性能。传统的微调方法通常以降低多模式功能的成本来改善构图推理，这主要是由于使用全球硬负（HN）损失，这与图像和文本的全球表示形成对比。这种全球HN损失推动了与原始文本高度相似的HN文本，从而损害了模型的多模式表示。为了克服这一限制，我们提出了细粒的选择性校准夹（FSC-CLIP），该剪辑（FSC-CLIP）整合了局部硬损耗和选择性校准的正则化。这些创新提供了精细的负面监督，同时保留了模型的代表性完整性。我们对组合性和多模式任务的各种基准的广泛评估表明，FSC-CLIP不仅可以与最先进的模型相同，而且还保留了强大的多模式功能。代码可在以下网址提供：https：//github.com/ytaek-oh/fsc-clip。

### VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks 
[[arxiv](https://arxiv.org/abs/2410.05160)] [[cool](https://papers.cool/arxiv/2410.05160)] [[pdf](https://arxiv.org/pdf/2410.05160)]
> **Authors**: Ziyan Jiang,Rui Meng,Xinyi Yang,Semih Yavuz,Yingbo Zhou,Wenhu Chen
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Technical Report
- **标题**: VLM2VEC：大规模多模式嵌入任务的培训视觉语言模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 嵌入模型对于实现各种下游任务（例如语义相似性，信息检索和聚类）至关重要。最近，人们对开发可以跨任务概括的通用文本嵌入模型（例如MTEB）引起了人们的兴趣。但是，尽管其重要性和实用性，但学习通用多模式嵌入模型的进展仍然相对较慢。在这项工作中，我们旨在探索能够处理能够处理各种下游任务的通用嵌入的潜力。我们的贡献是双重的：（1）MMEB（大量的多模式嵌入基准），其中涵盖4个元任务（即分类，视觉询问答录，多模式检索和视觉接地）和36个数据集，包括20个培训和16个评估数据集，涵盖了分发和分发型和远程分配 - 和2），以及（2）VLMAINGE-2）。 Vector），一个对比训练框架，将任何最先进的视觉模型转换为通过MMEB培训的嵌入模型。与以前的模型（例如剪辑和BLIP）独立编码文本或图像而没有任何任务指令的模型不同，VLM2VEC可以根据任务说明来处理图像和文本的任何组合，以生成固定的维矢量。我们在phi-3.5-V，llava-1.6等SOTA VLM上构建了一系列VLM2VEC模型，并根据MMEB的评估分配进行评估。我们的结果表明，VLM2VEC在MMEB中的分布和分布数据集上的现有多模式嵌入模型的绝对平均提高10％至20％。我们表明，VLM秘密地嵌入模型。

### Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side Information 
[[arxiv](https://arxiv.org/abs/2410.05143)] [[cool](https://papers.cool/arxiv/2410.05143)] [[pdf](https://arxiv.org/pdf/2410.05143)]
> **Authors**: Timofey Efimov,Harry Dong,Megna Shah,Jeff Simmons,Sean Donegan,Yuejie Chi
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: 利用多模式扩散模型与侧面信息加速成像
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散模型发现了解决反问题的表达先验的惊人成功，但是它们的扩展超出了自然图像，对更结构化的科学领域仍然有限。通过材料科学的应用，我们旨在通过利用辅助方式的侧面信息来减少昂贵的成像方式所需的测量数量，从而便宜得多。为了处理远期模型的非差异和黑盒性质，我们提出了一个框架来训练多模式扩散模型在关节模式上训练多模式扩散模型，从而将黑框前向模型的逆问题变成了简单的线性介绍问题。从数值上讲，我们证明了训练扩散模型比材料图像数据的可行性，并证明我们的方法通过利用可用的侧面信息来实现卓越的图像重建，而昂贵的显微镜模式需要少量的数据。

### HE-Drive: Human-Like End-to-End Driving with Vision Language Models 
[[arxiv](https://arxiv.org/abs/2410.05051)] [[cool](https://papers.cool/arxiv/2410.05051)] [[pdf](https://arxiv.org/pdf/2410.05051)]
> **Authors**: Junming Wang,Xingyu Zhang,Zebin Xing,Songen Gu,Xiaoyang Guo,Yang Hu,Ziying Song,Qian Zhang,Xiaoxiao Long,Wei Yin
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: He-drive：视觉语言模型类似人类的端到端驾驶
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 在本文中，我们提出了He-Drive：第一个以人类为中心的端到端自动驾驶系统，以产生既一致又舒适的轨迹。最近的研究表明，基于模仿学习的计划者和基于学习的轨迹得分手可以有效地生成并选择密切模仿专家演示的准确性轨迹。但是，这种轨迹规划师和得分手面临着产生时间不一致和不舒服的轨迹的困境。为了解决上述问题，我们的He-Drive首先通过稀疏感知提取了关键的3D空间表示，然后作为条件降级扩散概率模型（DDPMS）基于基于的运动计划者的条件输入，以产生时间一致性多模式轨迹。视觉语言模型（VLMS）引导的轨迹得分手随后从这些候选者中选择最舒适的轨迹来控制车辆，从而确保了类似人类的端到端驾驶。实验表明，HE-DRIVE不仅可以在具有挑战性的Nuscenes和OpenScene数据集上实现比VAD的平均碰撞率降低71％）和效率（即比Sparsedrive快1.9倍），而且还提供了最舒适的驾驶信息。 https://jmwang0117.github.io/he-drive/。

### PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion 
[[arxiv](https://arxiv.org/abs/2410.04939)] [[cool](https://papers.cool/arxiv/2410.04939)] [[pdf](https://arxiv.org/pdf/2410.04939)]
> **Authors**: Sijie Wang,Qiyu Kang,Rui She,Kai Zhao,Yang Song,Wee Peng Tay
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: accepted by IEEE TITS 2024
- **标题**: 临界：使用图像和点云融合迈向有效且强大的多模式位置识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 位置识别在机器人技术和计算机视野的领域中起着至关重要的作用，在自动驾驶，映射和本地化等领域中找到了应用。位置识别使用查询传感器数据和已知数据库标识一个地方。主要的挑战之一是开发一个模型，该模型可以提供准确的结果，同时对环境变化有牢固的态度。我们提出了两个多模式的位置识别模型，即profusion和Profusion ++。 Prfusion利用全球融合，并具有多种指标，从而在不需要摄像机外部校准的情况下在功能之间有效相互作用。相比之下，Profusion ++假设外在校准的可用性，并利用像素点对应关系来增强本地窗口上的特征学习。此外，这两个模型都结合了神经扩散层，即使在具有挑战性的环境中，也可以可靠的操作。我们在三个大规模基准上验证了这两种模型的最先进性能。值得注意的是，在苛刻的Boreas数据集中，它们的表现优于现有模型+3.0 AR@1。此外，我们进行消融研究以验证我们提出的方法的有效性。这些代码可在以下网址找到：https：//github.com/sijieaaa/prfusion

### OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction 
[[arxiv](https://arxiv.org/abs/2410.04932)] [[cool](https://papers.cool/arxiv/2410.04932)] [[pdf](https://arxiv.org/pdf/2410.04932)]
> **Authors**: Leheng Li,Weichao Qiu,Xu Yan,Jing He,Kaiqiang Zhou,Yingjie Cai,Qing Lian,Bingbing Liu,Ying-Cong Chen
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: Omnibooth：通过多模式指令学习图像合成的潜在控制
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出Omnibooth，这是一个图像生成框架，可以通过实例级多模式自定义实现空间控制。对于所有情况，可以通过文本提示或图像引用来描述多模式指令。给定一组用户定义的掩码以及相关的文本或图像指导，我们的目标是生成一个图像，其中将多个对象放在指定的坐标处，并且它们的属性与相应的指南完全一致。这种方法大大扩大了文本对图像生成的范围，并将其提升到可控性的更广泛和实用的维度。在本文中，我们的核心贡献在于提出的潜在控制信号，这是一种高维空间特征，该特征提供了统一表示，以无缝地集成空间，文本和图像条件。文本条件扩展了ControlNet，以提供实例级的开放式唱机生成。图像条件进一步实现了具有个性化身份的细粒度控制。实际上，我们的方法使用户具有更大的可控生成，因为用户可以根据需要从文本或图像中选择多模式条件。此外，彻底的实验证明了我们在不同任务和数据集跨图像综合保真度和对齐方式的增强性能。项目页面：https：//len-li.github.io/omnibooth-web/

### Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models 
[[arxiv](https://arxiv.org/abs/2410.04884)] [[cool](https://papers.cool/arxiv/2410.04884)] [[pdf](https://arxiv.org/pdf/2410.04884)]
> **Authors**: Dehong Kong,Siyuan Liang,Xiaopeng Zhu,Yuansheng Zhong,Wenqi Ren
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: accepted by Visual Intelligence
- **标题**: 贴片就足够了：自然主义的对抗斑块针对视觉语言预训练模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉语言预训练（VLP）模型在各个领域都取得了巨大的成功，但它们仍然容易受到对抗攻击的影响。解决这些对抗性脆弱性对于增强多模式学习的安全性至关重要。传统上，针对VLP模型的对抗方法涉及同时扰动图像和文本。但是，这种方法面临着显着的挑战：首先，对抗性扰动通常无法有效地转化为现实世界情景。其次，对文本的直接修改是明显可见的。为了克服这些局限性，我们提出了一种新颖的策略，该策略专门采用图像贴片进行攻击，从而保留原始文本的完整性。我们的方法利用扩散模型的先验知识来增强扰动的真实性和自然性。此外，为了优化补丁放置并提高攻击的功效，我们利用了交叉注意机制，该机制通过产生注意力图来指导战略补丁位置来封装模式之间的相互作用。在白色盒子设置中为图像到文本方案进行的全面实验表明，我们提出的方法显着胜过现有技术，实现了100％的攻击成功率。此外，它在涉及文本对图像配置的转移任务中表现出了值得称赞的性能。

### Multimodal Fusion Strategies for Mapping Biophysical Landscape Features 
[[arxiv](https://arxiv.org/abs/2410.04833)] [[cool](https://papers.cool/arxiv/2410.04833)] [[pdf](https://arxiv.org/pdf/2410.04833)]
> **Authors**: Lucia Gordon,Nico Lang,Catherine Ressijac,Andrew Davies
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: 9 pages, 4 figures, ECCV 2024 Workshop in CV for Ecology
- **标题**: 绘制生物物理景观特征的多模式融合策略
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 多模式的航空数据用于监视天然系统，机器学习可以显着加速此类图像中景观特征的分类，以使生态学和保护受益。然而，它仍然不足以探索这些多种方式应如何融合到深度学习模型中。作为填补这一空白的一步，我们使用这三种方式中使用空间对准的正原瘤数据集研究了三种策略（早期融合，晚期融合和专家的混合），以融合热，RGB和激光雷达图像。特别是，我们旨在绘制非洲大草原生态系统中三个与生态相关的生物物理景观特征：犀牛中间，白蚁丘和水。三种融合策略在早期还是晚期融合的方式上有所不同，如果迟到，该模型是在每个类别中学习固定权重，还是根据输入来适应每个类别的权重。总体而言，这三种方法具有相似的宏观表现，而晚期融合的AUC为0.698，但它们的每一类性能差异很大，早期的融合可以实现最佳的中等水和水和专家混合的回忆，并获得了最佳的回忆。

### Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality 
[[arxiv](https://arxiv.org/abs/2410.04780)] [[cool](https://papers.cool/arxiv/2410.04780)] [[pdf](https://arxiv.org/pdf/2410.04780)]
> **Authors**: Guanyu Zhou,Yibo Yan,Xin Zou,Kun Wang,Aiwei Liu,Xuming Hu
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Accepted by The Thirteenth International Conference on Learning Representations (ICLR 2025)
- **标题**: 通过解密因果关系来减轻多模式大语言模型中幻觉的幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）已成为行业和学术界的核心重点，但经常遭受视觉和语言先验引入的偏见，这可能导致多模式幻觉。这些偏见来自视觉编码器和大型语言模型（LLM）主链，影响了负责对齐多模式输入的注意机制。现有的基于解码的缓解方法着重于统计相关性，并忽略了注意机制和模型输出之间的因果关系，从而限制了它们在解决这些偏见方面的有效性。为了解决这个问题，我们提出了一个因果关系的因果推理框架，该因果关系将结构性因果建模应用于MLLM，将模态先验视为注意机制和输出之间的混杂因素。具体来说，通过在视觉和语言注意水平上采用后门调整和反事实推理，我们的方法减轻了模态先验的负面影响，并增强了MLLM的输入和输出的对齐，与MME基础标记的6 Vlind Bench指标的最高分数提高了65.3％，与常规方法相比，65％的分数为65.3％。广泛的实验可以验证我们的方法的有效性，同时是插件解决方案。我们的代码可在以下网址找到：https：//github.com/the-martyr/causalmm

### MM-R$^3$: On (In-)Consistency of Multi-modal Large Language Models (MLLMs) 
[[arxiv](https://arxiv.org/abs/2410.04778)] [[cool](https://papers.cool/arxiv/2410.04778)] [[pdf](https://arxiv.org/pdf/2410.04778)]
> **Authors**: Shih-Han Chou,Shivam Chandhok,James J. Little,Leonid Sigal
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: mm-r $^3 $：在（内）多模式大语模型（MLLMS）的一致性上
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着大型语言模型（LLM）和多模式（Visio-lls）LLM的出现，一系列研究的出现，分析了这些模型在各种任务中的性能。尽管大多数研究都致力于通过任务准确性（例如，视觉质量答案，接地）评估最先进的MLLM模型的功能（SOTA）MLLM模型，但我们的工作探讨了一致性的相关但互补的方面 -  MLLM模型对在语义上类似或相似类似的查询产生相似或相同的响应的能力。我们注意到，一致性是对MLLM的鲁棒性和信任的基本先决条件（必要但不足的条件）。尤其是人类的响应中，人类的反应高度一致（即使并非总是准确），并且从AI系统中固有期望的一致性。根据这个观点，我们提出了MM-R $^3 $基准，该基准在SOTA MLLMS中分析了具有三个任务的一致性和准确性的性能：问题重新启动，图像重新安装和上下文推理。我们的分析表明，一致性并不总是与准确性保持一致，表明具有更高准确性的模型不一定更一致，反之亦然。此外，我们提出了一种简单而有效的缓解策略，其适配器模块的形式受过训练，以最大程度地减少跨提示的不一致。有了我们提出的策略，就其现有同行的一致性而言，我们能够平均实现5.7％和12.5％的绝对改善。

### Diffusion Models in 3D Vision: A Survey 
[[arxiv](https://arxiv.org/abs/2410.04738)] [[cool](https://papers.cool/arxiv/2410.04738)] [[pdf](https://arxiv.org/pdf/2410.04738)]
> **Authors**: Zhen Wang,Dongyuan Li,Renhe Jiang
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: 3D视觉中的扩散模型：调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，3D视觉已成为计算机视觉中的关键领域，为诸如自动驾驶，机器人技术，增强现实（AR）和医学成像等广泛应用提供动力。该字段依赖于来自图像和视频等2D数据源的3D场景的准确感知，理解和重建。最初为2D生成任务设计的扩散模型为更灵活的概率方法提供了潜力，可以更好地捕获现实世界中3D数据中存在的可变性和不确定性。但是，传统方法通常会因效率和可扩展性而困难。在本文中，我们回顾了为3D视觉任务提供扩散模型的最新方法，包括但不限于3D对象生成，形状完成，点云重建和场景理解。我们对扩散模型的基本数学原理进行了深入的讨论，概述了它们的前进和反向过程，以及各种架构进步，使这些模型能够与3D数据集一起使用。我们还讨论了将扩散模型应用于3D视觉的关键挑战，例如处理阻塞和不同点密度以及高维数据的计算需求。最后，我们讨论了潜在的解决方案，包括提高计算效率，增强多模式融合以及探索大规模预处理以更好地跨越3D任务的概括。本文为这个迅速发展的领域的未来探索和发展是基础。

### The Sampling-Gaussian for stereo matching 
[[arxiv](https://arxiv.org/abs/2410.06527)] [[cool](https://papers.cool/arxiv/2410.06527)] [[pdf](https://arxiv.org/pdf/2410.06527)]
> **Authors**: Baiyu Pan,jichao jiao,Bowen Yao,Jianxin Pang,Jun Cheng
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: TL;DR: A novel Gaussian distribution-based supervision method for stereo matching. Implemented with five baseline methods and achieves notable improvement. Main content, 10 pages. conference submission
- **标题**: 采样高斯立体声匹配
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 软弧操作在基于神经网络的立体声匹配方法中广泛采用，以实现可区分的差异回归。但是，由于对概率分布的形状没有明确的限制，因此使用软弧线训练的网络很容易成为多模式。先前的方法利用拉普拉斯分布和跨凝结疗法进行训练，但无法有效提高准确性，甚至损害了网络的效率。在本文中，我们对先前的基于分布的方法进行了详细的分析，并提出了一种新型的立体声匹配方法，采样高斯。我们从高斯分布中采样以进行监督。此外，我们将训练解释为最大程度地减少向量空间中的距离，并提出了L1损失和余弦相似性损失的综合损失。此外，我们利用双线性插值来升级成本量。我们的方法可以直接应用于任何基于软弧线的立体声匹配方法，而不会降低效率。我们进行了全面的实验，以证明我们采样高斯的卓越表现。实验结果证明，我们在五种基线方法和两个数据集上取得了更好的准确性。我们的方法易于实现，并且代码可在线提供。

### Adver-City: Open-Source Multi-Modal Dataset for Collaborative Perception Under Adverse Weather Conditions 
[[arxiv](https://arxiv.org/abs/2410.06380)] [[cool](https://papers.cool/arxiv/2410.06380)] [[pdf](https://arxiv.org/pdf/2410.06380)]
> **Authors**: Mateus Karvat,Sidney Givigi
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: 8 pages
- **标题**: ADVER-CITY：在不利天气条件下进行协作感知的开源多模式数据集
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 不利的天气条件通过影响LiDARS和相机等传感器来广泛采用自动驾驶汽车（AV）构成了重大挑战。尽管协作感知（CP）在困难条件下改善了AV感知，但现有的CP数据集缺乏不利的天气条件。为了解决这个问题，我们介绍了Adver-City，这是第一个开放源代码合成CP数据集，专注于不利天气条件。它在卡拉（Carla）中使用Opencda​​模拟，其中包含超过24,000帧，超过8.9万个注释，以及在六个不同天气条件下的110个独特场景：晴朗的天气，软雨量，大雨，雾气，雾气，大雨，以及合成CP数据集中首次在合成的CP数据集中。它有六个对象类别，包括行人和骑自行车的人，并使用了带有激光，RGB和语义分段摄像机，GNSS和IMU的车辆和路边单元的数据。根据实际崩溃报告，它的场景描述了不良天气和可见性条件差的最相关的道路配置，物体密度的变化以及稀疏场景都不同，从而允许CP模型的新型测试条件。数据集上运行的基准测试表明，天气条件为感知模型创造了具有挑战性的条件，将多模式对象检测性能降低了多达19％，而对象密度对基于激光雷达的检测产生了高达29％。数据集，代码和文档可从https://labs.cs.queensu.ca/quarrg/datasets/adver-city/获得。

### Temporal Image Caption Retrieval Competition -- Description and Results 
[[arxiv](https://arxiv.org/abs/2410.06314)] [[cool](https://papers.cool/arxiv/2410.06314)] [[pdf](https://arxiv.org/pdf/2410.06314)]
> **Authors**: Jakub Pokrywka,Piotr Wierzchoń,Kornel Weryszko,Krzysztof Jassem
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: ef:Proceedings of the 18th Conference on Computer Science and Intelligence Systems, M. Ganzha, L. Maciaszek, M. Paprzycki, D. Ślęzak (eds). ACSIS, Vol. 35, pages 1331-1336 (2023)
- **标题**: 时间图像标题检索竞赛 - 描述和结果
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 结合视觉和文本信息的多模型模型最近获得了重大认可。本文解决了文本图像检索的多模式挑战，并介绍了一项新的任务，该任务扩展了模式以包括时间数据。本文介绍的时间图像标题检索竞赛（TICRC）是基于《美国编年史和挑战美国项目》，该项目可访问大量数字化的历史悠久的美国报纸，涉及274年。除了竞争结果外，我们还提供了交付的数据集及其创建过程的分析。

### Monocular Visual Place Recognition in LiDAR Maps via Cross-Modal State Space Model and Multi-View Matching 
[[arxiv](https://arxiv.org/abs/2410.06285)] [[cool](https://papers.cool/arxiv/2410.06285)] [[pdf](https://arxiv.org/pdf/2410.06285)]
> **Authors**: Gongxin Yao,Xinyang Li,Luowei Fu,Yu Pan
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 通过跨模式状态空间模型和多视图匹配，在激光雷达地图中识别单眼视觉位置
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 实现预先构建的激光雷达图中的单眼摄像头定位可以绕过视觉大满贯系统的同时映射过程，从而有可能减少自动定位的计算开销。为此，关键挑战之一是跨模式的位置识别，该识别涉及根据在线RGB图像从激光雷达地图中检索3D场景（点云）。在本文中，我们引入了一个有效的框架，以了解RGB图像和点云的描述符。它以视觉状态空间模型（VMAMBA）为骨干，并采用像素视图 - 赛式联合训练策略进行跨模式对比度学习。为了解决视野差异，独立描述符是从多个均匀分布的点云的分布观点生成的。然后，设计了可见的3D点重叠策略，以量化点云视图和RGB图像之间的相似性，以进行多视图监督。此外，当使用NetVlad从像素级特征生成描述符时，我们会补偿几何信息的损失，并引入了多视图生成的有效方案。 Kitti和Kitti-360数据集的实验结果证明了我们方法的有效性和概括。该代码将在接受后发布。

### RelitLRM: Generative Relightable Radiance for Large Reconstruction Models 
[[arxiv](https://arxiv.org/abs/2410.06231)] [[cool](https://papers.cool/arxiv/2410.06231)] [[pdf](https://arxiv.org/pdf/2410.06231)]
> **Authors**: Tianyuan Zhang,Zhengfei Kuang,Haian Jin,Zexiang Xu,Sai Bi,Hao Tan,He Zhang,Yiwei Hu,Milos Hasan,William T. Freeman,Kai Zhang,Fujun Luan
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: webpage: https://relit-lrm.github.io/
- **标题**: RelitlRM：大型重建模型的生成可重新辐射
- **领域**: 计算机视觉和模式识别,图形,机器学习
- **摘要**: 我们提出了RelitlRM，这是一种大型重建模型（LRM），用于在不知名的静态照明下捕获的稀疏（4-8）构图的新型照明（4-8）中产生3D对象的高质量高斯分裂表示。与先前需要密集捕获和缓慢优化的逆渲染方法不同，通常会导致诸如不正确的高光或阴影烘焙之类的伪像，RelitlRM采用基于馈送的变压器模型，具有基于几何重建器的新型组合，并且基于扩散的新型外观生成器。该模型是在变化的已知照明下对物体的合成多视图渲染进行训练的端到端。这种体系结构设计使能够有效分解几何形状和外观，解决材料和照明之间的歧义，并捕获光影外观中阴影和镜面的多模式分布。我们显示，我们的稀疏视图馈送retitlrm为最先进的密集视图优化基线提供了有竞争力的重新效果，同时更快。我们的项目页面可在以下网址找到：https：//relit-lrm.github.io/。

### Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See 
[[arxiv](https://arxiv.org/abs/2410.06169)] [[cool](https://papers.cool/arxiv/2410.06169)] [[pdf](https://arxiv.org/pdf/2410.06169)]
> **Authors**: Zeliang Zhang,Phu Pham,Wentian Zhao,Kun Wan,Yu-Jhe Li,Jianing Zhou,Daniel Miranda,Ajinkya Kale,Chenliang Xu
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 将视觉令牌视为文本？但是您的MLLM只需要更少的努力即可看到
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过将视觉编码器的视觉令牌视为文本令牌，多模式大型语言模型（MLLM）在各种视觉理解任务中取得了显着的进步，利用了大语言模型（LLMS）的强大体系结构。但是，随着令牌计数的增长，LLMS中计算的二次缩放会引入显着的效率瓶颈，从而阻碍了进一步的可扩展性。尽管最近的方法探索了修剪视觉令牌或采用较轻的LLM体系结构，但来自越来越多的视觉令牌的计算开销仍然是一个重大挑战。在这项研究中，我们研究了代表性MLLM LLAVA内的参数和计算模式级别的视觉计算的冗余，并引入了一套简化的策略以提高效率。这些包括邻居感知的视觉令牌注意力，对无效的视觉注意力头进行修剪以及用于视觉计算的选择性层掉落。通过在LLAVA实施这些策略，我们可以将计算需求的减少降低为88％，同时保持跨关键基准的模型性能。此外，我们验证了其他MLLM中的视觉计算冗余的存在，例如QWEN2-VL-7B和InternVL-2.0-4B/8B/26B。这些结果为MLLM提供了一种以最低计算成本处理密集的视觉令牌的新途径。代码和模型检查站将发布以支持进一步的研究。

### Aria: An Open Multimodal Native Mixture-of-Experts Model 
[[arxiv](https://arxiv.org/abs/2410.05993)] [[cool](https://papers.cool/arxiv/2410.05993)] [[pdf](https://arxiv.org/pdf/2410.05993)]
> **Authors**: Dongxu Li,Yudong Liu,Haoning Wu,Yue Wang,Zhiqi Shen,Bowen Qu,Xinyao Niu,Fan Zhou,Chengen Huang,Yanpeng Li,Chongyan Zhu,Xiaoyi Ren,Chao Li,Yifan Ye,Peng Liu,Lihuan Zhang,Hanshu Yan,Guoyin Wang,Bei Chen,Junnan Li
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: ARIA：开放的多模式本机混合物模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 信息有多种方式。多模式本机AI模型对于整合现实世界信息并提供全面的理解至关重要。尽管存在专有的多模式模型，但他们缺乏开放性却施加了收养的障碍，更不用说适应了。为了填补这一空白，我们介绍了ARIA，这是一种开放的多模式模型，在各种多模式，语言和编码任务中具有一流的性能。 ARIA分别是每个视觉令牌和文本令牌的3.9b和3.5b激活参数的Expert模型。它的表现优于pixtral-12b和llama3.2-11b，并且与各种多模式任务的最佳专有模型具有竞争力。我们在四阶段管道后从头开始预先培训ARIA，该管道逐渐使模型在语言理解，多模式理解，长上下文窗口和随后的指令方面具有强大的功能。我们将模型权重和一个代码库开放，该代码库有助于在现实世界应用中易于采用和适应ARIA。

### DeMo: Decoupling Motion Forecasting into Directional Intentions and Dynamic States 
[[arxiv](https://arxiv.org/abs/2410.05982)] [[cool](https://papers.cool/arxiv/2410.05982)] [[pdf](https://arxiv.org/pdf/2410.05982)]
> **Authors**: Bozhou Zhang,Nan Song,Li Zhang
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: NeurIPS 2024
- **标题**: 演示：将运动预测到定向意图和动态状态
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 对交通代理的准确运动预测对于确保动态变化的环境中自动驾驶系统的安全性和效率至关重要。主流方法采用单个孔子范式范式，其中每个查询对应于预测多模式轨迹的唯一轨迹。虽然直接有效，但缺乏未来轨迹的详细表示可能会产生次优的结果，鉴于代理人的表现会随着时间的流逝而动态发展。为了解决此问题，我们介绍了Demo，该框架将多模式轨迹查询分解为两种类型：模式查询捕获不同的方向性意图和状态查询，并随着时间的推移跟踪代理的动态状态。通过利用这种格式，我们分别优化了轨迹的多模式和动态进化特性。随后，集成了模式和状态查询，以获得轨迹的全面和详细的表示。为了实现这些操作，我们还为全球信息聚合和状态序列建模引入了综合注意力和MAMBA技术，利用它们各自的优势。对Argoverse 2和Nuscenes基准的广泛实验表明，我们的演示在运动预测中实现了最新的性能。

### PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling 
[[arxiv](https://arxiv.org/abs/2410.05970)] [[cool](https://papers.cool/arxiv/2410.05970)] [[pdf](https://arxiv.org/pdf/2410.05970)]
> **Authors**: Xudong Xie,Hao Yan,Liang Yin,Yang Liu,Jing Ding,Minghui Liao,Yuliang Liu,Wei Chen,Xiang Bai
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: PDF-Wukong：一种大型多模型，用于有效的长pdf读取，端到端稀疏采样
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 多模式文档理解是处理和理解大量文本和视觉信息的一项具有挑战性的任务。大型语言模型（LLM）的最新进展已大大提高了此任务的性能。但是，现有方法通常集中在纯文本或有限数量的文档图像上，他们努力处理带有交错文本和图像的长PDF文档，尤其是对于学术论文。在本文中，我们介绍了PDF-Wukong，这是一种多模式的大语模型（MLLM），旨在增强长PDF文档的多模式提问（QA）。 PDF-Wukong结合了一个稀疏的采样器，该采样器同时在文本和图像表示上运行，从而显着提高了MLLM的效率和能力。稀疏采样器与MLLM的图像编码集成在一起，并选择与用户查询最相关的段落或图表，以通过语言模型处理。为了有效地训练和评估我们的模型，我们构建了PaperPDF，该数据集由大量英语和中国学术论文组成。提出了多种策略，以自动产生110万QA对以及相应的证据来源。实验结果表明，在长期多模式文档理解的任务上，我们的方法比其他模型的优势和高效率，在F1上平均超过8.6％的专有产品。我们的代码和数据集将在https://github.com/yh-hust/pdf-wukong上发布。

### EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment 
[[arxiv](https://arxiv.org/abs/2410.05938)] [[cool](https://papers.cool/arxiv/2410.05938)] [[pdf](https://arxiv.org/pdf/2410.05938)]
> **Authors**: Yifei Xing,Xiangyuan Lan,Ruiping Wang,Dongmei Jiang,Wenjun Huang,Qingfang Zheng,Yaowei Wang
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 艾玛（Emma）：通过结构和分层对齐，赋予多模式曼巴
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 基于Mamba的架构已证明是深度学习模型的有希望的新方向，因为它们的竞争性能和次级部署速度。但是，当前的MAMBA多模式大语言模型（MLLM）不足以提取视觉特征，从而导致视觉和纹理潜在和纹理潜在的跨模式对齐不平衡，从而对多模式任务的性能产生负面影响。在这项工作中，我们提出了使用结构和分层对齐（EMMA）授权多模式的MAMBA，这使MLLM能够提取细粒度的视觉信息。具体而言，我们提出了一个按像素对齐模块，以自动重新测试优化空间图像级特征的学习和处理以及文本令牌，从而在图像级别启用结构对齐。此外，为了防止在跨模型对齐过程中的视觉信息降解，我们提出了一个多尺度特征融合（MFF）模块，以结合中间层中的多尺度视觉特征，从而在特征级别启用层次结构。广泛的实验是在各种多模式基准的跨基准进行的。我们的模型显示出比其他基于MAMBA的MLLM的延迟较低，并且在推断过程中基于变形金刚的MLLM的速度几乎快四倍。由于更好的跨模式对齐，我们的模型表现出较低的幻觉程度和对视觉细节的敏感性增强，这表现出在各种多模式基准的卓越性能中。将提供代码。

### MedUniSeg: 2D and 3D Medical Image Segmentation via a Prompt-driven Universal Model 
[[arxiv](https://arxiv.org/abs/2410.05905)] [[cool](https://papers.cool/arxiv/2410.05905)] [[pdf](https://arxiv.org/pdf/2410.05905)]
> **Authors**: Yiwen Ye,Ziyang Chen,Jianpeng Zhang,Yutong Xie,Yong Xia
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: Meduniseg：通过及时驱动的通用模型进行2D和3D医疗图像分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 通用细分模型通过有效利用离散注释来解决广泛的任务，具有巨大的潜力。随着任务和方式的范围的扩展，在通用模型中生成和战略性地定位特定于任务和模态特定的先验变得越来越重要。但是，现有的通用模型通常会忽略不同先验之间的相关性，而这些先验的最佳位置和频率仍然没有被忽视。在本文中，我们介绍了Meduniseg，这是一种迅速驱动的通用分段模型，旨在跨不同方式和域，旨在2D和3D多任务分段。 Meduniseg使用多个模态特异性提示以及通用任务提示，以准确表征模式和任务。为了生成相关的先验，我们提出了模态映射（MMAP）以及融合和选择（保险丝）模块，该模块将模态和任务提示转换为相应的先验。这些模式和任务先验是在编码过程的开始和结尾系统地介绍的。我们在由17个子数据集组成的全面多模式上游数据集上评估Meduniseg。结果表明，Meduniseg达到了优越的多任务分割性能，与NNUNET基准相比，在17个上游任务中的平均骰子得分提高了1.2％，而使用少于1/10的参数。对于在最初的多任务联合培训中表现不佳的任务，我们冻结了Meduniseg并引入了新的模块以重新学习这些任务。这种方法产生了增强的版本Meduniseg*，该版本在所有任务中始终胜过Meduniseg。此外，Meduniseg超过了六个下游任务上的先进的自我监督和监督的预训练的模型，将自己确立为高质量的高质量，高度可推广的预训练模型。

### ModalPrompt:Dual-Modality Guided Prompt for Continual Learning of Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.05849)] [[cool](https://papers.cool/arxiv/2410.05849)] [[pdf](https://arxiv.org/pdf/2410.05849)]
> **Authors**: Fanhu Zeng,Fei Zhu,Haiyang Guo,Xu-Yao Zhang,Cheng-Lin Liu
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 模式下：双模式指导的提示，以持续学习大型多模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过共同学习混合数据集，大型多模型（LMM）具有显着的多任务能力。但是，新颖的任务将在动态世界中依次遇到，并且不断调整的LMM通常会导致性能降低。为了应对灾难性遗忘的挑战，现有方法利用数据重播或模型扩展，这两者都不是专门针对LMM的，并且具有固有的局限性。在本文中，我们提出了一种新颖的双模式指导性的及时学习框架（ModalPrompt），适合多模式持续学习，以有效地学习新任务，同时减轻忘记先前的知识。具体而言，我们学习每个任务的原型提示，并利用有效的及时选择任务标识符，并根据图像文本监督及时融合知识转移。广泛的实验证明了我们方法的优势，例如，ModalPrompt在LMMS持续学习基准中获得 +20％的绩效增长，$ \ times $ \ times $ 1.42推理速度避免以成比例的培训成本与任务数量成比例。该代码将公开可用。

### ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition 
[[arxiv](https://arxiv.org/abs/2410.05774)] [[cool](https://papers.cool/arxiv/2410.05774)] [[pdf](https://arxiv.org/pdf/2410.05774)]
> **Authors**: Mohammadreza Salehi,Jae Sung Park,Tanush Yadav,Aditya Kusupati,Ranjay Krishna,Yejin Choi,Hannaneh Hajishirzi,Ali Farhadi
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: ef:NeurIPS 2024 Track Datasets and Benchmarks
- **标题**: ActionAtlas：用于域特有动作识别的VideoQA基准测试
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们的世界充满了各种各样的行动，并跨越了我们作为人类努力识别和理解的专业领域。在任何单个域中，动作通常都可以看起来非常相似，这对于深层模型而言是具有挑战性的，以准确区分它们。为了评估多模式基础模型在帮助我们识别此类动作方面的有效性，我们提出了Actionatlas v1.0，这是一个多选择的视频问题，回答了基准测试，其中包含各种运动中的简短视频。数据集中的每个视频都与一个问题和四到五个选择配对。这个问题指出了特定的个人，询问哪种选择“最佳”描述了他们在特定时间上下文中的行动。总体而言，该数据集包括934个视频，展示了56项运动中580个独特动作，共有1896个动作。与大多数现有的视频问题回答仅涵盖简单动作的基准（通常可以从单个帧中可以识别）的基准不同，Actionatlas专注于复杂的运动，并严格测试模型的能力，可以识别每个域中看起来相似的动作之间微妙的差异。我们在此基准测试上评估了开放且专有的基础模型，发现最佳模型GPT-4O的最高准确度为45.52％。同时，非专家人群的非专家人群为每种选择提供了行动描述，其准确度为61.64％，随机机会约为21％。我们使用最先进的模型的发现表明，具有高框架采样率对于准确识别ActionAtlas中的动作很重要，该功能某些领先的专有视频模型（例如Gemini）并未在其默认配置中包括。

### STNet: Deep Audio-Visual Fusion Network for Robust Speaker Tracking 
[[arxiv](https://arxiv.org/abs/2410.05964)] [[cool](https://papers.cool/arxiv/2410.05964)] [[pdf](https://arxiv.org/pdf/2410.05964)]
> **Authors**: Yidi Li,Hong Liu,Bing Yang
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: STNET：可靠扬声器跟踪的深度音频融合网络
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视听扬声器跟踪旨在使用多传感器平台捕获的信号来确定人类目标在场景中的位置，该信号可以通过多模式融合方法来提高其准确性和鲁棒性。最近，已经提出了几种融合方法来模拟多种方式的相关性。但是，对于扬声器跟踪问题，音频和视觉信号之间的跨模式相互作用尚未得到充分利用。为此，我们在这项工作中提出了一个具有深层视听融合模型的新颖扬声器跟踪网络（STNET）。我们设计了一种视觉引导的声学测量方法，以在统一的本地化空间中融合异质线索，该方法通过摄像机模型采用视觉观测来构建增强的声学图。对于特征融合，采用了一个跨模式注意模块来共同模拟多模式的上下文和相互作用。音频和视觉特征之间的相关信息在融合模型中进一步相互作用。此外，基于STNET的跟踪器通过质量感知的模块应用于多演讲案例，该模块评估了多模式观测值的可靠性，以在复杂场景中实现可靠的跟踪。 AV16.3和CAV3D数据集的实验表明，所提出的基于STNET的跟踪器优于单模式方法和最先进的视听扬声器跟踪器。

### A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks 
[[arxiv](https://arxiv.org/abs/2410.07593)] [[cool](https://papers.cool/arxiv/2410.07593)] [[pdf](https://arxiv.org/pdf/2410.07593)]
> **Authors**: Hoin Jung,Taeuk Jang,Xiaoqian Wang
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: NeurIPS 2024 (Spotlight), the Thirty-Eighth Annual Conference on Neural Information Processing Systems
- **标题**: 跨越模式和任务的视觉模型的统一辩护方法
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉模型（VLM）的最新进展通过同时处理文本和图像数据来显着增强人工智能领域，从而实现了复杂的多模式任务。但是，这些模型经常表现出可能偏向社会刻板印象的偏见，因此需要采取辩解策略。现有的辩论方法狭义地集中在特定的方式或任务上，需要广泛的再培训。为了解决这些局限性，本文介绍了用于脱缩（SFID）的选择性特征插补，这是一种新颖的方法，该方法将特征修剪和置信度降低（LCI）有效地减少VLM中的偏见。 SFID具有多功能性，保持了输出的语义完整性，并通过消除了重新培训的需求来保持昂贵的效率。我们的实验结果证明了SFID在各种VLM任务中的有效性，包括零射击分类，文本对图像检索，图像字幕和文本形象生成，通过显着降低性别偏见而不会损害绩效。这种方法不仅可以增强VLMS应用程序的公平性，而且可以保留其在各种情况下的效率和实用性。

### 3D Vision-Language Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2410.07577)] [[cool](https://papers.cool/arxiv/2410.07577)] [[pdf](https://arxiv.org/pdf/2410.07577)]
> **Authors**: Qucheng Peng,Benjamin Planche,Zhongpai Gao,Meng Zheng,Anwesa Choudhuri,Terrence Chen,Chen Chen,Ziyan Wu
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: main paper + supplementary material
- **标题**: 3D视觉高斯裂口
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D重建方法和视觉语言模型的最新进展推动了多模式3D场景理解的发展，这在机器人技术，自动驾驶以及虚拟/增强现实中具有至关重要的应用。但是，当前的多模式场景理解方法将天真的语义表示形式嵌入了3D重建方法中，而无需在视觉和语言模态之间取得平衡，这会导致不满意的语义栅格对半透明或反射性对象的语义栅格化，以及对颜色模式的过度拟合。为了减轻这些局限性，我们提出了一种解决方案，该解决方案可以充分处理独特的视觉和语义方式，即，一个3D视觉语言的高斯分裂模型，用于场景理解，以强调对语言方式的表示。我们提出了一种新型的跨模式栅格器，并使用模态融合以及平滑的语义指示器来增强语义栅格化。我们还采用了一种摄像机视图混合技术来提高现有视图和合成视图之间的语义一致性，从而有效地减轻了过度拟合。广泛的实验表明，我们的方法在开放式语义分段中实现了最先进的性能，从而超过了现有的方法。

### CoPESD: A Multi-Level Surgical Motion Dataset for Training Large Vision-Language Models to Co-Pilot Endoscopic Submucosal Dissection 
[[arxiv](https://arxiv.org/abs/2410.07540)] [[cool](https://papers.cool/arxiv/2410.07540)] [[pdf](https://arxiv.org/pdf/2410.07540)]
> **Authors**: Guankun Wang,Han Xiao,Huxin Gao,Renrui Zhang,Long Bai,Xiaoxiao Yang,Zhen Li,Hongsheng Li,Hongliang Ren
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: COPESD：用于训练大型视觉模型的多层手术运动数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 粘膜下清除（ESD）可以快速切除大病变，最大程度地减少复发率并改善长期总生存率。尽管有这些优势，但ESD在技术上具有挑战性，并且具有很高的并发症风险，需要熟练的外科医生和精确的仪器。大型视觉语言模型（LVLM）的最新进展为机器人系统提供了有希望的决策支持和预测计划功能，这可以提高ESD的准确性并降低程序风险。但是，现有的用于多级细粒ESD手术运动理解的数据集是稀缺的，缺乏详细的注释。在本文中，我们设计了ESD运动粒度的层次分解，并引入了一个多级手术运动数据集（COPESD），用于训练LVLMS作为机器人\ TextBf {Co}  -  \ textbf {p} \ textbf {d}发行。 COPESD包括17,679张图像，其中包含32,699个边界盒和88,395个多级动作，从35个小时的ESD视频中，用于机器人辅助和常规手术。 COPESD实现了ESD运动的颗粒分析，重点是粘膜下剖分的复杂任务。对LVLM的广泛实验证明了COPESD在训练LVLM中的有效性预测手术机器人运动。作为第一个多模式ESD运动数据集，COPESD支持ESD指导跟随和手术自动化方面的高级研究。该数据集可在\ href {https://github.com/gkw0010/copesd} {https://github.com/gkw0010/copesd。}}}

### Progressive Multi-Modal Fusion for Robust 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2410.07475)] [[cool](https://papers.cool/arxiv/2410.07475)] [[pdf](https://arxiv.org/pdf/2410.07475)]
> **Authors**: Rohit Mohan,Daniele Cattaneo,Florian Drews,Abhinav Valada
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: ef:8th Annual Conference on Robot Learning, 2024
- **标题**: 稳健3D对象检测的进行性多模式融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 多传感器融合对于自动驾驶中的准确3D对象检测至关重要，相机和激光镜头是最常用的传感器。但是，现有方法通过在鸟类视图（BEV）或透视图（PV）中从模态上投影特征，从而在单个视​​图中执行传感器融合，从而牺牲了互补信息，例如高度或几何比例。为了解决此限制，我们提出了Profusion3D，这是一个渐进式融合框架，在中间和对象查询级别结合了BEV和PV中的特征。我们的体系结构层次融合了本地和全局特征，从而增强了3D对象检测的鲁棒性。此外，我们引入了一种自我监督的面具建模预训练策略，以通过三个新型目标来提高多模式表示学习和数据效率。对Nuscenes和Argoverse2数据集进行的广泛实验最终证明了Profusion3d的功效。此外，Profusion3D对传感器的故障是可靠的，在只有一种模式时表现出强烈的性能。

### Exploring Efficient Foundational Multi-modal Models for Video Summarization 
[[arxiv](https://arxiv.org/abs/2410.07405)] [[cool](https://papers.cool/arxiv/2410.07405)] [[pdf](https://arxiv.org/pdf/2410.07405)]
> **Authors**: Karan Samel,Apoorva Beedu,Nitish Sontakke,Irfan Essa
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: 11 pages, 4 figures
- **标题**: 探索有效的基础多模式用于视频摘要
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 基础模型能够在给定提示说明和文本，音频或图像输入的情况下生成文本输出。最近，这些模型已合并以在视频上执行任务，例如视频摘要。这样的视频基础模型通过将每个模式特异性模型的输出对准相同的嵌入空间进行预训练。然后在每个模型中使用的嵌入在语言模型中，该模型在所需的指令集上进行了微调。在预训练期间对齐每种方式在计算上是昂贵的，并且可以防止对不同基本模式模型的快速测试。在微调过程中，在内域视频中进行评估，在该视频中很难理解这些方法的普遍性和数据效率。为了减轻这些问题，我们建议使用插件的视频语言模型。它直接将每个输入模式生成的文本用于语言模型，避免了预训练的对齐开销。我们没有进行微调，而是利用一些弹药的适应策略。我们比较了插件样式方法和基线调整方法的性能与计算成本。最后，我们探讨了域移动过程中每种方法的普遍性，并在训练数据受到限制时就哪些数据有用。通过此分析，我们提供了有关如何利用多模式基础模型的实用见解，以取得有效的结果，并且鉴于现实的计算和数据限制。

### PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.07278)] [[cool](https://papers.cool/arxiv/2410.07278)] [[pdf](https://arxiv.org/pdf/2410.07278)]
> **Authors**: Yingen Liu,Fan Wu,Ruihui Li,Zhuo Tang,Kenli Li
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: 10 pages, 5 figures,3 tables
- **标题**: PAR：有效的大型多模型的及时感知令牌减少方法
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式大语言模型（MLLMS）在视觉任务中表现出强大的性能，但是它们的效率受到了重大计算和记忆需求的阻碍，并且在多模式输入中处理长上下文。为了解决这个问题，我们引入了PAR（及时了解令牌减少），这是一种新颖的插件方法，可有效地降低视觉令牌而不会损害模型性能。与以前依赖注意力机制和忽视跨模式相互作用的方法不同，我们使用及时感知的策略来识别和聚集基本的视觉令牌。 PAR将视觉上下文冗余分为两种类型：外部和内部。外部冗余是通过语义检索最小化的，而内部冗余则是使用令牌路由机制来解决的。这种方法大大减少了计算负载，而无需进行其他培训或复杂的体系结构修改。 \ textBf {实验结果表明，在各种视觉问题回答任务中，par降低了83 \％，压缩比为89 \％，同时保留了基线准确性的97％\％。}与先验的降低比例相比，与先前的降低比率相比，可以使PAR的自适应设计达到2倍的降低率。

### Learning Content-Aware Multi-Modal Joint Input Pruning via Bird's-Eye-View Representation 
[[arxiv](https://arxiv.org/abs/2410.07268)] [[cool](https://papers.cool/arxiv/2410.07268)] [[pdf](https://arxiv.org/pdf/2410.07268)]
> **Authors**: Yuxin Li,Yiheng Li,Xulei Yang,Mengying Yu,Zihang Huang,Xiaojun Wu,Chai Kiat Yeo
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 通过Bird's-eye-view表示，学习内容感知的多模式联合输入修剪
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在自动驾驶的景观中，Bird's-Eye-View（BEV）表示最近引起了大量的学术关注，是多模式传感器输入融合的变革框架。该BEV范式有效地将传感器融合挑战从基于规则的方法转移到以数据为中心的方法，从而促进了从一系列异质传感器中提取更多细微的特征。尽管具有明显的优点，但与基于BEV的技术相关的计算间接费用通常会要求高容量硬件基础架构，从而为实用的现实世界实现带来挑战。为了减轻这种限制，我们引入了一种新颖的内容感知的多模式关节输入修剪技术。我们的方法将BEV作为共享锚点，以在将感知模型的主链引入其之前识别和消除非必需的传感器区域。我们通过在Nuscenes数据集上进行的广泛实验来确认方法的功效，从而证明了实质性的计算效率而不牺牲感知准确性。据我们所知，这项工作代表了从输入修剪点减轻计算负担的首次尝试。

### MM-Ego: Towards Building Egocentric Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2410.07177)] [[cool](https://papers.cool/arxiv/2410.07177)] [[pdf](https://arxiv.org/pdf/2410.07177)]
> **Authors**: Hanrong Ye,Haotian Zhang,Erik Daxberger,Lin Chen,Zongyu Lin,Yanghao Li,Bowen Zhang,Haoxuan You,Dan Xu,Zhe Gan,Jiasen Lu,Yinfei Yang
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: Technical Report
- **标题**: MM-EGO：建立以自我为中心的多模式LLM
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 这项研究旨在全面探索建立以自我为中心视频理解的多模式基础模型。为了实现这一目标，我们在三个方面工作。首先，由于缺乏质量为中心视频理解的质量检查数据，我们开发了一个数据引擎，该数据引擎有效地生成了700万高质量的QA样本，用于基于人类注射的数据，以从30秒到一小时的egentric视频。这是当前最大的以egentric QA数据集。其次，我们通过629个视频和7,026个问题为具有挑战性的以Egentric QA为中心的QA基准，以评估模型在不同长度的视频中识别和记住视觉细节的能力。我们引入了一种新的偏见评估方法，以帮助减轻所评估模型中存在的不可避免的语言偏见。第三，我们提出了一种专门的多模式架构，该体系结构具有小说“记忆指针提示”机制。该设计包括一个全局的瞥见步骤，以获得对整个视频的总体理解并确定关键的视觉信息，然后进行后备步骤，该步骤利用关键的视觉信息来生成响应。这使该模型能够更有效地理解扩展的视频内容。借助数据，基准和模型，我们成功地构建了MM-EGO，这是一种以自我为中心的多模式LLM，在以自我为中心的视频理解上显示出强大的性能。

### Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate 
[[arxiv](https://arxiv.org/abs/2410.07167)] [[cool](https://papers.cool/arxiv/2410.07167)] [[pdf](https://arxiv.org/pdf/2410.07167)]
> **Authors**: Qidong Huang,Xiaoyi Dong,Pan Zhang,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin,Weiming Zhang,Nenghai Yu
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: Project page: https://github.com/shikiw/Modality-Integration-Rate
- **标题**: 在具有模态整合速率的大型视觉模型中解密的跨模式对齐
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 我们介绍了一种有效，健壮且广义的度量的模态积分率（MIR），以指示大型视觉语言模型（LVLM）的多模式预训练质量。大规模的预训练在构建能力的LVLM中起着至关重要的作用，同时评估其训练质量而没有昂贵的监督微调阶段的探索阶段不足。损失，困惑性和内部文化评估结果是大型语言模型（LLMS）通常使用的预训练指标，而我们观察到，当将训练有素的LLM与新的方式相结合时，这些指标的指示性较低。由于缺乏适当的指标，在​​关键的预训练阶段对LVLM的研究受到了极大的阻碍，包括训练数据选择，有效的模块设计等。在本文中，我们建议评估从模式间分配距离的角度观点和MIR的预训练质量，并且在MIR中进行MIR，呈现的MIR，与textbf相关的质量，有效地表示质量，以表现为1）\ textbf;监督微调。 2）\ textbf {robust}朝着不同的培训/评估数据。 3）\ textbf {概括}跨培训配置和体系结构选择。我们进行了一系列培训实验，以探索MIR的有效性，并观察到MIR在训练数据选择，培训策略计划和模型体系结构设计方面表示的令人满意的结果，以获得更好的培训结果。我们希望MIR可以成为建立能力的LVLM的有用指标，并激发以下有关不同领域模式一致性的研究。我们的代码位于：https：//github.com/shikiw/modality-integration-rate。

### Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis 
[[arxiv](https://arxiv.org/abs/2410.07155)] [[cool](https://papers.cool/arxiv/2410.07155)] [[pdf](https://arxiv.org/pdf/2410.07155)]
> **Authors**: Bohan Zeng,Ling Yang,Siyu Li,Jiaming Liu,Zixiang Zhang,Juanxi Tian,Kaixin Zhu,Yongzhen Guo,Fu-Yun Wang,Minkai Xu,Stefano Ermon,Wentao Zhang
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: Project: https://github.com/YangLing0818/Trans4D
- **标题**: Trans4D：构图文本到4D合成的现实几何感知过渡
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散模型的最新进展表明，图像和视频生成方面具有出色的功能，进一步提高了4D合成的有效性。现有的4D代方法可以根据用户友好的条件生成高质量的4D对象或场景，从而使游戏和视频行业受益。但是，这些方法难以合成复杂4D转换和场景中相互作用的显着对象变形。为了应对这一挑战，我们提出了Trans4D，这是一种新颖的文本到4D综合框架，可以实现现实的复杂场景过渡。具体而言，我们首先使用多模式大语言模型（MLLM）来生成4D场景初始化和有效过渡时机计划的物理感知场景描述。然后，我们提出了一个几何感知4D转换网络，以基于计划实现复杂的场景级别4D转换，该计划涉及表达的几何对象变形。广泛的实验表明，Trans4D在生成具有准确和高质量过渡的4D场景时始终优于现有的最新方法，从而验证了其有效性。代码：https：//github.com/yangling0818/trans4d

### Towards Interpreting Visual Information Processing in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.07149)] [[cool](https://papers.cool/arxiv/2410.07149)] [[pdf](https://arxiv.org/pdf/2410.07149)]
> **Authors**: Clement Neo,Luke Ong,Philip Torr,Mor Geva,David Krueger,Fazl Barez
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 要在视觉模型中解释视觉信息处理
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 视觉模型（VLM）是处理和理解文本和图像的强大工具。我们研究了突出的VLM Llava语言模型组成部分中视觉令牌的处理。我们的方法着重于分析对象信息的定位，跨层的视觉令牌表示的演变以及整合视觉信息进行预测的机制。通过消融研究，我们证明当删除对象特异性令牌时，对象识别精度下降了70 \％。我们观察到，视觉令牌表示在跨层的词汇空间中变得越来越可以解释，这表明与与图像内容相对应的文本令牌进行了对齐。最后，我们发现该模型从这些精制表示的预测位置上提取对象信息，从而反映了仅文本语言模型的事实关联任务中的过程。这些发现提供了有关VLM如何处理和整合视觉信息，弥合我们对语言和视觉模型之间的差距的关键见解，并为更加可解释和可控制的多模式系统铺平了道路。

### Personalized Visual Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2410.07113)] [[cool](https://papers.cool/arxiv/2410.07113)] [[pdf](https://arxiv.org/pdf/2410.07113)]
> **Authors**: Renjie Pi,Jianshu Zhang,Tianyang Han,Jipeng Zhang,Rui Pan,Tong Zhang
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 个性化的视觉教学调整
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的最新进展已显示出很大的进步。但是，这些模型表现出一个明显的限制，我们称之为“面部失明”。具体来说，他们可以参与一般对话，但未能针对特定个人进行个性化对话。这种缺陷阻碍了MLLM在个性化设置中的应用，例如在移动设备上量身定制的视觉助手或需要识别家庭成员的家庭机器人。在本文中，我们介绍了个性化的视觉教学调整（PVIT），这是一个新颖的数据策划和培训框架，旨在使MLLM能够在图像中识别目标个人并进行个性化和连贯的对话。我们的方法涉及开发一条复杂的管道，该管道自动生成包含个性化对话的培训数据。该管道利用各种视觉专家，图像生成模型和（多模式）大语言模型的功能。为了评估MLLM的个性化潜力，我们提出了一个名为P Bench的基准，该基准包括各种难度级别的问题类型。实验证明了与我们的策划数据集进行了微调后的实质性个性化绩效提高。

### Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology 
[[arxiv](https://arxiv.org/abs/2410.07087)] [[cool](https://papers.cool/arxiv/2410.07087)] [[pdf](https://arxiv.org/pdf/2410.07087)]
> **Authors**: Xiangyu Wang,Donglin Yang,Ziqin Wang,Hohin Kwan,Jinyu Chen,Wenjun Wu,Hongsheng Li,Yue Liao,Si Liu
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 迈向现实的无人机视觉导航：平台，基准和方法论
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 开发能够根据语言说明和视觉信息（称为视觉导航（VLN））导航到目标位置的代理人引起了广泛的兴趣。大多数研究都集中在基于地面的代理上，而基于无人机的VLN仍然相对不受欢迎。无人机视觉导航的最新努力主要采用基于地面的VLN设置，依靠预定义的离散动作空间，忽略了代理运动动态的固有差异以及地面和空中环境之间导航任务的复杂性。为了解决这些差异和挑战，我们从三个角度提出了解决方案：平台，基准和方法论。为了在VLN任务中启用现实的无人机轨迹模拟，我们提出了OpenUAV平台，该平台具有不同的环境，现实的飞行控制和广泛的算法支持。我们进一步构建了一个面向目标的VLN数据集，该数据集由该平台上的大约12K轨迹组成，它是专为现实无人机VLN任务设计的第一个数据集。为了应对复杂的空中环境带来的挑战，我们提出了一个助理引导的无人机对象搜索基准，称为UAV-Need-HELP，该基准提供了不同级别的指导信息，以帮助无人机更好地完成现实的VLN任务。我们还提出了一个无人机导航LLM，鉴于多视图图像，任务描述和助手说明，它利用了MLLM的多模式理解能力共同处理视觉和文本信息，并执行层次结构轨迹生成。我们方法的评估结果极大地超过了基线模型，而我们的结果与人类运营商实现的结果之间存在很大的差距，强调了无人机需要的赫尔普任务所带来的挑战。

### Pixtral 12B 
[[arxiv](https://arxiv.org/abs/2410.07073)] [[cool](https://papers.cool/arxiv/2410.07073)] [[pdf](https://arxiv.org/pdf/2410.07073)]
> **Authors**: Pravesh Agrawal,Szymon Antoniak,Emma Bou Hanna,Baptiste Bout,Devendra Chaplot,Jessica Chudnovsky,Diogo Costa,Baudouin De Monicault,Saurabh Garg,Theophile Gervet,Soham Ghosh,Amélie Héliou,Paul Jacob,Albert Q. Jiang,Kartik Khandelwal,Timothée Lacroix,Guillaume Lample,Diego Las Casas,Thibaut Lavril,Teven Le Scao,Andy Lo,William Marshall,Louis Martin,Arthur Mensch,Pavankumar Muddireddy, et al. (17 additional authors not shown)
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: pixtral 12b
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 我们介绍了Pixtral-12b，这是一种120亿参数的多模式模型。 PixTral-12b经过培训，可以了解自然图像和文档，从而在各种多模式基准上实现领先的性能，超过了许多较大的模型。与许多开源模型不同，PixTral也是其大小的尖端文本模型，并且不会妥协自然语言性能在多模式任务中脱颖而出。 Pixtral使用了从头开始训练的新型构想，它可以以自然分辨率和纵横比摄取图像。这为用户提供了用于处理图像的代币数量的灵活性。 PixTral还能够在其128K令牌的长上下文窗口中处理任何数量的图像。 PixTral 12b取代胜于其他大小的开放模型（Llama-3.2 11b \＆Qwen-2-Vl 7b）。它还胜过更大的开放型号，例如紫红色-3.2 90B，而较小7倍。我们进一步贡献了开源基准MM-MT Bench，用于评估实际场景中的视觉模型，并为多模式LLMS的标准化评估协议提供详细的分析和代码。 PixTral-12b在Apache 2.0许可下发布。

### TinyEmo: Scaling down Emotional Reasoning via Metric Projection 
[[arxiv](https://arxiv.org/abs/2410.07062)] [[cool](https://papers.cool/arxiv/2410.07062)] [[pdf](https://arxiv.org/pdf/2410.07062)]
> **Authors**: Cristian Gutierrez
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: I am withdrawing this work in favour of the confidentiality of research ideas that are still under development.
- **标题**: Tinyemo：通过公制投影缩减情感推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了Tinyemo，这是一个小型多模式模型的家庭，用于情感推理和分类。我们的方法特征：（1）用于预训练和微调阶段的合成情感指示数据集，（2）一个指标投影仪，将分类从语言模型中委托，允许进行更有效的培训和推理，（3）多模式大语言模型（MM-LLM），用于情绪推理，以及（4）（4）半autoction fromction frot ftercouttion frot fetection。 Tinyemo能够执行情感分类和情感推理，同时使用的参数少于可比的模型。这种效率使我们能够自由地合并更多样化的情绪数据集，从而在分类任务上实现强大的性能，而我们最小的模型（700m参数）优于基于通用MM-LLLS的更大最新模型，具有超过7B参数。此外，公制投影仪还可以在大型模型中进行可解释性和间接偏置检测，而无需其他培训，提供了一种理解和改善AI系统的方法。我们在https://github.com/ggcr/tinyemo上发布代码，模型和数据集

### HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric Understanding 
[[arxiv](https://arxiv.org/abs/2410.06777)] [[cool](https://papers.cool/arxiv/2410.06777)] [[pdf](https://arxiv.org/pdf/2410.06777)]
> **Authors**: Keliang Li,Zaifei Yang,Jiahe Zhao,Hongze Shen,Ruibing Hou,Hong Chang,Shiguang Shan,Xilin Chen
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: HERM：基准测试和增强多模式LLM，以人为中心的理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）之后的视觉理解和教学的重大进步为在各种以人为中心的方式中更广泛的应用开辟了更多可能性。但是，现有的图像文本数据可能不支持多元信息信息的精确模态对齐和集成，这对于以人为中心的视觉理解至关重要。在本文中，我们介绍了HERM-Bench，这是评估MLLM的以人为本理解能力的基准。我们的工作揭示了现有的MLLM在理解复杂的以人为中心的情况方面的局限性。为了应对这些挑战，我们介绍了HERM-100K，这是一个具有多级以人为中心的注释的综合数据集，旨在增强MLLM的培训。此外，我们开发了HERM-7B，这是一个利用HERM-100K增强培训数据的MLLM。对HERM板凳的评估表明，HERM-7B在各种以人为本的维度上显着胜过现有的MLLM，这反映了MLLM培训中用于以人为中心的视觉理解的数据注释的当前数据注释不足。这项研究强调了专业数据集和基准在推进MLLMs以人为本理解的能力方面的重要性。

### Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.06699)] [[cool](https://papers.cool/arxiv/2410.06699)] [[pdf](https://arxiv.org/pdf/2410.06699)]
> **Authors**: Yubo Wang,Chaohu Liu,Yanqiu Qu,Haoyu Cao,Deqiang Jiang,Linli Xu
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: Accepted to ACMMM 2024
- **标题**: 打破视觉感知：针对大型视觉模型编码的视觉令牌的对抗性攻击
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 大型视觉模型（LVLM）将视觉信息整合到大语言模型中，展示了显着的多模式对话能力。但是，视觉模块在LVLM的鲁棒性方面引入了新的挑战，因为攻击者可以制作视觉上清洁但可能误导模型以产生错误的答案的对抗图像。通常，LVLMS依靠视觉编码将图像转换为视觉令牌，这对于语言模型至关重要，以有效地感知图像内容。因此，我们对一个问题感到好奇：当攻击编码的视觉令牌并破坏视觉信息时，LVLM仍然会产生正确的响应吗？为此，我们提出了一种称为VT-攻击（Visual令牌攻击）的非目标攻击方法，该方法从多个角度构建了对抗性示例，其目的是全面破坏特征表示和固有的关系，以及图像编码器的Visual代币输出的语义属性。仅在拟议的攻击中使用对图像编码器的访问，生成的对抗示例利用相同的图像编码器和跨不同任务的一般性在不同的LVLM上具有可传递性。广泛的实验验证了VT攻击在基线方法上的出色攻击性能，证明了其在使用图像编码器攻击LVLMS方面的有效性，这反过来又可以为LVLMS的鲁棒性提供指导，尤其是在视觉特征空间的稳定性方面。

### Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization 
[[arxiv](https://arxiv.org/abs/2410.06682)] [[cool](https://papers.cool/arxiv/2410.06682)] [[pdf](https://arxiv.org/pdf/2410.06682)]
> **Authors**: Changli Tang,Yixuan Li,Yudong Yang,Jimin Zhuang,Guangzhi Sun,Wei Li,Zujun Ma,Chao Zhang
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 增强多模式LLM，以使用多轮偏好优化进行详细和准确的视频字幕
- **领域**: 计算机视觉和模式识别,计算语言学,图像和视频处理
- **摘要**: 视频包含大量信息，并在自然语言中生成详细而准确的描述是视频理解的关键方面。在本文中，我们介绍了Video-Salmonn 2，这是一种高级视听语言模型（LLM），其低级适应性（LORA）设计用于增强视频（带有配对音频）通过定向偏好优化（DPO）字幕。我们提出了新的指标来评估视频描述的完整性和准确性，这些描述是使用DPO进行了优化的。为了进一步改善培训，我们介绍了一种新型的多发DPO（MRDPO）方法，该方法涉及定期更新DPO参考模型，将LORA模块合并并重新定位，作为在每个培训回合（1,000步）（1,000步）之后的参数更新（1000步）（1000步）（1000步）的代理，并从地面视频视频中纳入了该过程。为了解决由于MRDPO引起的非捕获能力的潜在灾难性忘记，我们提出了重生调整，该调整通过使用由MRDPO训练的模型作为监督标签来填充DPO LLM的predpo llm。实验表明，MRDPO显着增强了Video-Salmonn 2的字幕准确性，将全球和局部错误率分别降低了40 \％和20 \％，同时将重复率降低了35 \％。最终的视频 - 萨尔蒙2模型只有70亿个参数，在视频字幕任务中超过了诸如GPT-4O和Gemini-1.5-Pro之类的领先模型，同时在相似大小的模型中，在广泛使用的视频提问的基准上维持了较为广泛使用的视频提问基准的竞争性能。接受后，我们将发布代码，模型检查点以及培训和测试数据。演示可在\ href {https://video-salmonn-2.github.io} {https://video-salmonn-2.github.io}中获得。

### ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time 
[[arxiv](https://arxiv.org/abs/2410.06625)] [[cool](https://papers.cool/arxiv/2410.06625)] [[pdf](https://arxiv.org/pdf/2410.06625)]
> **Authors**: Yi Ding,Bolian Li,Ruqi Zhang
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: 29pages
- **标题**: ETA：评估在推理时对视觉语言模型的安全性
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 视觉语言模型（VLM）已成为多模式智能的必不可少的骨干，但严重的安全挑战限制了其现实应用。尽管通常有效地保护文本输入，但对抗性视觉输入很容易绕过VLM防御机制。现有的防御方法要么是资源密集的，需要大量数据并计算，要么无法同时确保对响应的安全性和实用性。为了解决这些限制，我们提出了一个新颖的两相推理时间对齐框架，评估然后对齐（ETA）：1）评估输入视觉内容和输出响应，以在多模态设置中建立强大的安全意识，以及2）2）通过对vlms的生成范围进行验证，以使句子的最佳范围与vlms的范围进行验证 - 无害而有用的一代道路。广泛的实验表明，ETA在无害性，帮助性和效率方面优于基线方法，在跨模性攻击中将不安全率降低了87.5％，并在GPT-4帮助评估中获得了96.6％的胜利。该代码可在https://github.com/dripnowhy/eta上公开获取。

### Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N 1-to-1 Relationships for Text-Video Retrieval 
[[arxiv](https://arxiv.org/abs/2410.06618)] [[cool](https://papers.cool/arxiv/2410.06618)] [[pdf](https://arxiv.org/pdf/2410.06618)]
> **Authors**: Jian Xiao,Zhenzhen Hu,Jia Li,Richang Hong
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 文本代理：将1到N关系的检索分解为文本视频检索的N 1-1关系
- **领域**: 计算机视觉和模式识别,信息检索,多媒体
- **摘要**: 近年来，文本视频检索（TVR）在利用预训练模型和大型语言模型（LLMS）的推动下，已经取得了重大进步。尽管取得了这些进步，但由于视频和文本模式之间的固有差异以及数据表示中的不规则性之间的固有差异，在TVR中实现准确的匹配仍然具有挑战性。在本文中，我们提出了文本视频 - 螺纹网络（TV-Proxynet），这是一个新颖的框架，旨在将TVR传统的1对N关系分解为N独特的1-1关系。通过用一系列文本代理替换单个文本查询，TV-Proxynet不仅扩大了查询范围，而且还可以实现更精确的扩展。每个文本代理都是通过精致的迭代过程制成的，由我们称为导演和仪表板的机制控制，该机制调节代理相对于原始文本查询的方向和距离。这种设置不仅有助于更精确的语义对齐，而且有效地管理了多模式数据中固有的差异和噪声。我们对三个代表性视频检索基准MSRVTT，DIDEMO和ActivityNet字幕的实验证明了TV-Proxynet的有效性。结果表明，在基线比R@1的2.0％至3.3％的提高。 TV-Proxynet在MSRVTT和ActivityNet字幕上实现了最先进的性能，与现有方法相比，DIDEMO的提高了2.0％，从而验证了我们方法增强语义映射和减少错误倾向的能力。

### Deep Correlated Prompting for Visual Recognition with Missing Modalities 
[[arxiv](https://arxiv.org/abs/2410.06558)] [[cool](https://papers.cool/arxiv/2410.06558)] [[pdf](https://arxiv.org/pdf/2410.06558)]
> **Authors**: Lianyu Hu,Tongkai Shi,Wei Feng,Fanhua Shang,Liang Wan
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: NeurIPS 2024, add some results
- **标题**: 深度相关的提示，以视觉识别，缺失方式
- **领域**: 计算机视觉和模式识别
- **摘要**: 大规模的多模式模型在一系列任务上表现出了出色的性能，这些任务由配对的多模式训练数据提供支持。通常，始终假定它们接收模式完整输入。但是，由于隐私限制或收集难度，这种简单的假设可能并不总是在现实世界中，在这种情况下，在模式完整数据上鉴定的模型很容易证明在缺失模式案例上的性能下降。为了解决这个问题，我们指的是迅速学习，以适应大型的多模式模型来处理缺失的模式方案，通过将不同的丢失案例视为不同类型的输入。我们展示的不仅是将独立的提示准备到中间层，还可以利用提示和输入功能之间的相关性，并挖掘不同提示的不同层之间的关系以仔细设计说明。我们还结合了不同方式的互补语义，以指导每种方式的提示设计。在三个常用数据集上进行的广泛实验始终证明了我们方法的优越性，与以前的不同情况下的方法相比。进一步进行了大量消融，以显示我们方法对不同模态失误比率和类型的可靠性和可靠性。

### SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.08474)] [[cool](https://papers.cool/arxiv/2410.08474)] [[pdf](https://arxiv.org/pdf/2410.08474)]
> **Authors**: Haotian Xia,Zhengbang Yang,Junbo Zou,Rhys Tracy,Yuqing Wang,Chi Lu,Christopher Lai,Yanjun He,Xun Shao,Zhuoqing Xie,Yuan-fang Wang,Weining Shen,Hanjie Chen
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: Sportu：多式联运大语模型的全面运动理解基准
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式的大语言模型（MLLM）正在通过集成文本和视觉信息来推进对复杂运动场景进行推理的能力。为了全面评估其功能，我们介绍了Sportu，这是一种旨在评估多层运动推理任务的MLLM的基准。 Sportu包括两个关键组成部分：Sportu-Text，其中包含900个多项选择问题，并带有人类通知的解释，以了解规则理解和策略。该组件的重点是测试模型仅通过提问（QA）推理运动的能力，而无需视觉输入； Sportu-Video，由7个不同的运动和12,048个QA对组成的1,701个慢动作视频片段组成，旨在评估多层推理，从简单的运动识别到复杂的任务，例如犯规检测和规则应用。我们评估了四个普遍的LLM，主要利用少量的学习范式，这些学习范例是在Sportu-Text部分提示的，这些学习范式（COT）。我们使用少量的学习和链接链（COT）评估了四个LLM，从而在Sportu-Text上评估了四个LLM。 GPT-4O实现了71％的最高精度，但仍未达到人类水平的表现，突出了改善规则理解和推理的空间。对Sportu-Video部分的评估包括7个专有和6个开源MLLM。实验表明，模型缺乏需要深厚推理和基于规则的理解的艰巨任务。 Claude-3.5-Sonnet在艰巨的任务上只有52.6％的精度表现出色，显示了很大的改进空间。我们希望Sportu将是评估模型在体育理解和推理方面的能力的关键一步。

### AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning 
[[arxiv](https://arxiv.org/abs/2410.08405)] [[cool](https://papers.cool/arxiv/2410.08405)] [[pdf](https://arxiv.org/pdf/2410.08405)]
> **Authors**: Muhammad Awais,Ali Husain Salem Abdulla Alharthi,Amandeep Kumar,Hisham Cholakkal,Rao Muhammad Anwer
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: Accepted at WACV, 2025
- **标题**: Agrogpt：具有专家调整的高效农业视觉语言模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在推进大型多模式对话模型（LMM）方面取得了重大进展，利用在线可用的图像文本数据的大量存储库。尽管取得了这种进步，这些模型经常会遇到巨大的领域差距，从而阻碍了他们在新领域进行复杂对话的能力。最近的努力旨在减轻此问题，尽管依靠特定于域特定的图像文本数据来策划指令调整数据。但是，许多领域，例如农业，都缺乏这种视力语言数据。在这项工作中，我们提出了一种构建指导调整数据的方法，该数据可以利用农业领域的仅视觉数据。我们利用跨越多个领域的各种农业数据集，特定于类的班级信息，并采用大型语言模型（LLMS）来构建专家调整集，从而产生了一个名为Agroinstruction的70K专家调节数据集。随后，我们对Agrogpt进行了专家调整和创建，这是一种有效的LMM，可以进行复杂的农业相关对话并提供有用的见解。我们还开发了用于评估和比较{Agrogpt的}性能与大型开放和封闭式模型的agroevals。 {Agrogpt}擅长识别细粒度的农业概念，可以充当农业专家，并为多模式农业问题提供有用的信息。代码，数据集和模型可在https://github.com/awaisrauf/agrogpt上找到。

### LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts 
[[arxiv](https://arxiv.org/abs/2410.08211)] [[cool](https://papers.cool/arxiv/2410.08211)] [[pdf](https://arxiv.org/pdf/2410.08211)]
> **Authors**: Anh-Quan Cao,Maximilian Jaritz,Matthieu Guillaumin,Raoul de Charette,Loris Bazzani
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: Latteclip：通过LMM合成文本无监督的剪贴微调
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 大规模视觉语言预训练（VLP）模型（例如剪辑）以其多功能性而闻名，因为它们可以在零弹奏设置中应用于不同的应用。但是，当这些模型被用于特定领域时，由于域间隙或训练数据中这些域的代表性不足，它们的性能通常会降低。虽然具有人类通知标签的自定义数据集上的微调VLP模型可以解决此问题，但即使是小规模的数据集（例如，100K样本）的注释也可能是一项昂贵的努力，如果任务复杂，通常需要专家注释。为了应对这些挑战，我们提出了LatteClip，这是一种无监督的方法，用于在不依赖人类注释的情况下与已知的类名中的已知类名进行分类的微调剪辑模型。我们的方法利用大型多模型模型（LMM）为单个图像和图像组生成表达性文本描述。这些提供了其他上下文信息，以指导自定义域中的微调过程。由于LMM生成的描述容易出现幻觉或缺少细节，因此我们引入了一种新颖的策略来提炼有用的信息并稳定培训。具体而言，我们从嘈杂的文本和双伪标签中学习丰富的每级原型表示。我们在10个域特异性数据集上的实验表明，Latteclip的平均提高TOP-1准确性和其他最先进的无人监督方法的平均提高+4.74点，超过了预训练的零射击方法。

### Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision 
[[arxiv](https://arxiv.org/abs/2410.08209)] [[cool](https://papers.cool/arxiv/2410.08209)] [[pdf](https://arxiv.org/pdf/2410.08209)]
> **Authors**: Shengcao Cao,Liang-Yan Gui,Yu-Xiong Wang
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 在大型多模型中的新兴像素接地，而无需接地监督
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 当前的大型多模式模型（LMM）在接地中面临挑战，这要求该模型将语言组件与视觉实体联系起来。与经过额外接地监督的微型LMM的普遍做法相反，我们发现接地能力实际上可以在接受培训的LMM中出现，而无需明确的接地监督。为了揭示这种新兴的基础，我们引入了一种“参加段”方法，该方法利用标准LMM的注意力图来执行像素级分段。此外，为了增强接地能力，我们提出了Difflmm，一种利用基于扩散的视觉编码器的LMM，而不是标准的夹具视觉编码器，并接受了相同的弱监督训练。在不受基础特定监督数据的偏见和有限规模的限制的情况下，我们的方法更具概括性和可扩展性。与接地LMM和通才LMM相比，我们在基础方面和一般视觉问题上都取得了竞争性能。值得注意的是，我们在没有任何接地监督的情况下实现了44.2个接地的面具召回，超过了广泛监督的模型Glamm。项目页面：https：//groundlmm.github.io。

### SPA: 3D Spatial-Awareness Enables Effective Embodied Representation 
[[arxiv](https://arxiv.org/abs/2410.08208)] [[cool](https://papers.cool/arxiv/2410.08208)] [[pdf](https://arxiv.org/pdf/2410.08208)]
> **Authors**: Haoyi Zhu,Honghui Yang,Yating Wang,Jiange Yang,Limin Wang,Tong He
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: Project Page: https://haoyizhu.github.io/spa/
- **标题**: 水疗中心：3D空间意识可实现有效的体现表示
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术
- **摘要**: 在本文中，我们介绍了Spa，这是一个新颖的表示学习框架，该框架强调了3D空间意识在体现AI中的重要性。我们的方法利用多视图图像上的可区分神经渲染来赋予香草视觉变压器（VIT）具有内在的空间理解。我们介绍了对迄今为止的体现表示学习的最全面评估，涵盖了8个模拟器的268个任务，这些模拟器具有单个任务和语言条件的多任务场景。结果令人信服：水疗中心始终优于10种以上的最先进表示方法，包括专门为体现AI，以视觉为中心的任务和多模式应用程序设计的方法，同时使用较少的培训数据。此外，我们进行了一系列现实的实验，以确认其在实际情况下的有效性。这些结果突出了3D空间意识在体现表示学习中的关键作用。我们最强大的模型需要超过6000个GPU小时才能训练，我们致力于开放所有代码和模型权重，以促进体现表示学习的未来研究。项目页面：https：//haoyizhu.github.io/spa/。

### Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training 
[[arxiv](https://arxiv.org/abs/2410.08202)] [[cool](https://papers.cool/arxiv/2410.08202)] [[pdf](https://arxiv.org/pdf/2410.08202)]
> **Authors**: Gen Luo,Xue Yang,Wenhan Dou,Zhaokai Wang,Jiawen Liu,Jifeng Dai,Yu Qiao,Xizhou Zhu
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 单与内vl：通过内源性预训练的单片多模型模型的界限
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 在本文中，我们专注于整体多模式大型语言模型（MLLM），将视觉编码和语言解码整合到单个LLM中。特别是，我们确定了整体MLLM的现有预训练策略通常会遭受不稳定的优化或灾难性遗忘。为了解决这个问题，我们的核心思想是将新的视觉参数空间嵌入到预训练的LLM中，从而在冻结LLM时从嘈杂的数据中稳定地学习视觉知识。基于此原理，我们提出了一种新型的单片MLLM，它通过多模式的Experts结构无缝整合了一组视觉专家。此外，我们提出了一种创新的预训练策略，以最大程度地提高单互式内源的视觉能力，即内源性视觉预训练（EVIP）。特别是，EVIP被设计为视觉专家的渐进学习过程，该过程旨在将视觉知识从嘈杂的数据充分利用到高质量的数据。为了验证我们的方法，我们对16个基准进行了广泛的实验。实验结果证实了一单位数字比现有的单片MLLM在16个多模式基准中的13个，例如在Ocrbench上的EMU3上+80点。与模块化基线（即Internvl-1.5）相比，单式内vl仍然保留了可比的多模式性能，同时降低了高达67％的第一个令牌延迟。代码和模型在https://huggingface.co/opengvlab/mono-internvl-2b上发布。

### MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.08182)] [[cool](https://papers.cool/arxiv/2410.08182)] [[pdf](https://arxiv.org/pdf/2410.08182)]
> **Authors**: Wenbo Hu,Jia-Chen Gu,Zi-Yi Dou,Mohsen Fayyaz,Pan Lu,Kai-Wei Chang,Nanyun Peng
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: https://mragbench.github.io
- **标题**: MRAG基础：以视觉评估进行检索的多模型评估
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 现有的多模式检索基准主要集中于评估模型是否可以检索和利用外部文本知识来回答问题。但是，在某些情况下，检索视觉信息要么比文本数据更有益或更容易访问。在本文中，我们介绍了多模式检索的一代基准MRAG-BENCH，在该基准中，我们系统地识别和分类了场景，其中视觉增强知识比文本知识更好，例如，更多的图像来自不同的观点。 MRAG板凳由16,130张图像和1,353个在9种不同场景中进行的人类注销的多项选择问题。使用MRAG板，我们对10个开源和4个专有大型视力模型（LVLM）进行了评估。我们的结果表明，与文本知识相比，与图像相比，所有LVLM都表现出更大的改进，证实了Mrag Bench以视觉为中心。此外，我们对MRAG Bench进行了广泛的分析，MRAG BENCH提供了对检索功能的LVLM的宝贵见解。值得注意的是，在有效利用检索的知识的有效利用知识方面，表现最佳的模型GPT-4O面临着挑战，与地基真实信息仅取得了5.82％的提高，与人类参与者中观察到的33.16％的改善相反。这些发现凸显了MRAG基座在鼓励社区增强LVLMS更有效地利用视觉知识的能力方面的重要性。

### Q-VLM: Post-training Quantization for Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.08119)] [[cool](https://papers.cool/arxiv/2410.08119)] [[pdf](https://arxiv.org/pdf/2410.08119)]
> **Authors**: Changyuan Wang,Ziwei Wang,Xiuwei Xu,Yansong Tang,Jie Zhou,Jiwen Lu
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: Q-VLM：大视力语言模型的训练后量化
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们提出了一个大型视觉模型（LVLM）的训练后量化框架，以进行有效的多模式推断。常规量化方法通过最大程度地减少激活离散误差来依次搜索层的圆形函数，该误差未能获得最佳量化策略而无需考虑跨层依赖性。相反，我们挖掘了跨层的依赖性，该依赖性显着影响整个视觉模型的离散错误，并将这种依赖性嵌入到最佳量化策略中，以低搜索成本搜索。具体而言，我们观察到激活熵与有关输出离散误差的跨层依赖性之间的强相关性。因此，我们采用熵作为最佳分区的代理，旨在实现离散错误和搜索成本之间令人满意的权衡。此外，我们优化了视觉编码器，以将跨层依赖性解散以进行搜索空间的细粒度分解，以便进一步降低搜索成本而不会损害量化精度。实验结果表明，我们的方法将记忆压缩为2.78倍，并增加生成速度约1.44倍，约13B LLAVA模型，而无需在各种多模式推理任务上进行性能降解。代码可从https://github.com/changyuanwang17/qvlm获得。

### CrackSegDiff: Diffusion Probability Model-based Multi-modal Crack Segmentation 
[[arxiv](https://arxiv.org/abs/2410.08100)] [[cool](https://papers.cool/arxiv/2410.08100)] [[pdf](https://arxiv.org/pdf/2410.08100)]
> **Authors**: Xiaoyan Jiang,Licheng Jiang,Anjie Wang,Kaiying Zhu,Yongbin Gao
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: CrackSegdiff：基于扩散概率模型的多模式裂纹分段
- **领域**: 计算机视觉和模式识别
- **摘要**: 将灰度和深度数据整合在道路检查机器人中可以提高道路状况评估的准确性，可靠性和全面性，从而改善维护策略和更安全的基础设施。但是，这些数据源通常会因人行道上的背景噪声而损害。扩散概率模型（DPM）的最新进展在图像分割任务中取得了显着的成功，展示了有效的DeNoSIS能力，如Segdiff这样的研究所证明的。尽管取得了这些进步，但当前基于DPM的分段并未完全利用原始图像数据的潜力。在本文中，我们提出了一种基于DPM的新型裂纹分割方法，名为CrackSegdiff，该方法唯一融合了灰度​​和范围/深度图像。该方法通过通过DPM和全局特征提取之间的局部特征提取之间的相互作用来增强反向扩散过程。与使用变形金刚用于全球功能的传统方法不同，我们的方法采用VM-UNET有效地捕获原始数据的远程信息。通过两个创新的模块进一步完善了特征的集成：通道融合模块（CFM）和浅层特征补偿模块（SFCM）。我们对查找数据集中三级裂纹图像分割任务的实验评估表明，裂纹策略的表现优于最先进的方法，尤其是在检测浅层裂纹方面表现出色。代码可从https://github.com/sky-visionx/cracksegdiff获得。

### Generated Bias: Auditing Internal Bias Dynamics of Text-To-Image Generative Models 
[[arxiv](https://arxiv.org/abs/2410.07884)] [[cool](https://papers.cool/arxiv/2410.07884)] [[pdf](https://arxiv.org/pdf/2410.07884)]
> **Authors**: Abhishek Mandal,Susan Leavy,Suzanne Little
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 产生的偏见：审核文本到图像生成模型的内部偏见动力学
- **领域**: 计算机视觉和模式识别,计算机与社会
- **摘要**: 文本对图像（TTI）扩散模型（例如DALL-E和稳定扩散）能够从文本提示中生成图像。但是，它们已被证明可以使性别刻板印象永存。这些模型在多个阶段进行内部处理数据，并采用多种组成模型，通常是单独训练的。在本文中，我们提出了两个新颖的指标，以在这些多阶段多模型模型中内部测量偏差。开发了扩散偏置以检测和测量模型扩散阶段引入的偏差。偏置放大测量在文本到图像转换过程中偏差的放大。我们的实验表明，TTI模型扩大了性别偏差，扩散过程本身有助于偏见，并且稳定的扩散V2比DALL-E 2更容易出现性别偏见。

### HeGraphAdapter: Tuning Multi-Modal Vision-Language Models with Heterogeneous Graph Adapter 
[[arxiv](https://arxiv.org/abs/2410.07854)] [[cool](https://papers.cool/arxiv/2410.07854)] [[pdf](https://arxiv.org/pdf/2410.07854)]
> **Authors**: Yumiao Zhao,Bo Jiang,Xiao Wang,Qin Xu,Jin Tang
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: HegraphAdapter：使用异质图适配器调整多模式视觉语言模型
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 基于适配器的调整方法已显示出从将知识从预训练的视觉模型转移到下游任务的显着潜力。但是，在回顾了现有的适配器之后，我们发现它们通常无法完全探索构建特定于任务知识的不同方式之间的相互作用。同样，现有作品通常只关注正文提示之间的相似性匹配，从而使区分高视觉内容的类挑战。为了解决这些问题，在本文中，我们提出了一个新型的异质图适配器，以实现下游任务的VLM。要具体而言，我们首先构建了统一的异质图模式，其中包含i）视觉节点，正面文本节点和负文本节点以及ii）几种类型的边缘连接，以全面地建模模式内模式，模式间和类间结构知识。接下来，我们采用特定的异构图神经网络来挖掘多模式结构知识，以适应下游任务的视觉和文本特征。最后，在HegraphAdapter之后，我们同时构建基于文本的和基于视觉的分类器，以全面增强剪辑模型的性能。 11个基准数据集的实验结果证明了拟议的HegraphAdapter的有效性和好处。

### CLIP Multi-modal Hashing for Multimedia Retrieval 
[[arxiv](https://arxiv.org/abs/2410.07783)] [[cool](https://papers.cool/arxiv/2410.07783)] [[pdf](https://arxiv.org/pdf/2410.07783)]
> **Authors**: Jian Zhu,Mingkai Sheng,Zhangmin Huang,Jingfei Chang,Jinling Jiang,Jian Long,Cheng Luo,Lei Liu
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: Accepted by 31st International Conference on MultiMedia Modeling (MMM2025)
- **标题**: 用于多媒体检索的剪辑多模式散列
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式哈希方法被广泛用于多媒体检索中，可以将多源数据融合以生成二进制哈希代码。但是，单个骨干网络具有有限的特征表达能力，并且没有在大规模无监督的多模式数据上共同预训练，从而导致较低的检索精度。为了解决这个问题，我们提出了一种新型的剪辑多模式哈希（Clipmh）方法。我们的方法采用剪辑框架来提取文本和视觉功能，然后融合它们生成哈希代码。由于每个模态特征的增强，我们的方法在多模式哈希方法的检索性能方面有了很大的改善。与最新的无监督和监督的多模式散列方法相比，实验表明，所提出的夹子可以显着提高性能（MAP中最大增加了8.38％）。

### MMHead: Towards Fine-grained Multi-modal 3D Facial Animation 
[[arxiv](https://arxiv.org/abs/2410.07757)] [[cool](https://papers.cool/arxiv/2410.07757)] [[pdf](https://arxiv.org/pdf/2410.07757)]
> **Authors**: Sijing Wu,Yunhao Li,Yichao Yan,Huiyu Duan,Ziwei Liu,Guangtao Zhai
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: Accepted by ACMMM 2024. Project page: https://wsj-sjtu.github.io/MMHead/
- **标题**: mmhead：朝向细粒度多模式3D面部动画
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D面部动画由于其在多媒体领域的广泛应用而引起了广泛的关注。音频驱动的3D面部动画已被广泛探索，结果令人鼓舞。但是，由于缺乏多模式3D面部动画数据集，很少探索多模式3D面部动画，尤其是文本引导的3D面部动画。为了填补这一空白，我们首先构建了一个大规模的多模式3D面部动画数据集MMHead，该数据集由49小时的3D面部运动序列，语音音频和丰富的层次结构文本注释组成。每个文本注释都包含抽象动作和情感描述，细粒度的面部和头部运动（即表达和头部姿势）描述，以及可能引起这种情感的三种可能的情况。具体而言，我们整合了五个公共2D肖像视频数据集，并提出了自动管道至1）从单眼视频中重建3D面部运动序列； 2）借助AU检测和Chatgpt获得分层文本注释。基于MMHEAD数据集，我们为两个新任务建立基准：文本引起的3D Talking Head Animation和Text To Text-To-3D面部运动生成。此外，提出了一种名为MM2Face的简单但有效的基于VQ-VAE的方法，以统一多模式信息并产生多样化和合理的3D面部运动，从而在两个基准上都取得了竞争性的结果。广泛的实验和全面分析表明，我们的数据集和基准在促进多模式3D面部动画的开发方面具有重要的潜力。

### LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.09962)] [[cool](https://papers.cool/arxiv/2410.09962)] [[pdf](https://arxiv.org/pdf/2410.09962)]
> **Authors**: Han Qiu,Jiaxing Huang,Peng Gao,Qin Qi,Xiaoqin Zhang,Ling Shao,Shijian Lu
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: Longhalqa：多模式大语模型的长篇小说幻觉评估
- **领域**: 计算机视觉和模式识别
- **摘要**: 幻觉是一种现象，其中多模式大语言模型〜（MLLM）倾向于产生合理但与图像不符的文本响应，已成为各种与MLLM相关的应用程序中的一个主要障碍。已经创建了几个基准，以评估MLLM的幻觉水平，通过提出有关对象存在的歧视性问题，或者引入LLM评估者以从MLLM中获得生成的文本。但是，歧视性数据在很大程度上涉及与现实世界文本不符的简单问题，而生成数据涉及由于其固有的随机性而在计算上且不稳定的LLM评估器。我们提出了Longhalqa，这是一种无LLM的幻觉基准，包括6K长而复杂的幻觉文本。 Longhalqa以GPT4V生成的幻觉数据为特征，这些数据与现实世界中的方案很好，包括对象/图像描述和具有14/130个单词和189个单词的多轮对话。它介绍了两个新任务，分别是幻觉歧视和幻觉完成，以单个多项选项问题的形式统一了歧视性和生成性评估，并导致更可靠，更有效的评估，而无需LLM评估者。此外，我们提出了一条先进的管道，该管道极大地促进了未来幻觉基准的构建，并进行了漫长而复杂的问题和描述。在使用长而复杂的文本数据处理幻觉时，对最近的多个MLLM的广泛实验揭示了各种新挑战。数据集和评估代码可在https://github.com/hanqiu-hq/longhalqa上找到。

### The Roles of Contextual Semantic Relevance Metrics in Human Visual Processing 
[[arxiv](https://arxiv.org/abs/2410.09921)] [[cool](https://papers.cool/arxiv/2410.09921)] [[pdf](https://arxiv.org/pdf/2410.09921)]
> **Authors**: Kun Sun,Rong Wang
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 上下文语义相关指标在人类视觉处理中的作用
- **领域**: 计算机视觉和模式识别
- **摘要**: 语义相关性指标可以捕获单个对象的固有语义及其与视觉场景中其他元素的关系。先前的许多研究表明，这些指标可以影响人类的视觉处理。但是，这些研究通常并未充分说明上下文信息，也没有采用最近的深度学习模型来进行更准确的计算。这项研究通过引入上下文语义相关性的指标来研究人类的视觉感知和处理。我们从基于视觉的观点和基于语言的观点评估目标对象及其周围环境之间的语义关系。从视觉理解中测试大型眼动数据集，我们采用最先进的深度学习技术来计算这些指标，并通过高级统计模型分析其对人类视觉处理的固定措施的影响。这些指标还可以在视觉感知中模拟自上而下和自下而上的处理。这项研究进一步将基于视觉的指标和基于语言的指标整合到一个新型的合并指标中，以解决先前研究中通常分别处理视觉和语义相似性的危险差距。结果表明，所有指标都可以精确预测视觉感知和处理中的固定度量，但在预测中具有不同的作用。合并的度量胜过其他指标，支持在塑造视觉感知/处理时强调语义和视觉信息之间相互作用的理论。这一发现与对人类认知中多模式信息处理的重要性的认识越来越吻合。这些见解增强了我们对视觉处理基础的认知机制的理解，并对在认知科学和人类计算机相互作用等领域中开发更准确的计算模型具有影响。

### Improving Colorectal Cancer Screening and Risk Assessment through Predictive Modeling on Medical Images and Records 
[[arxiv](https://arxiv.org/abs/2410.09880)] [[cool](https://papers.cool/arxiv/2410.09880)] [[pdf](https://arxiv.org/pdf/2410.09880)]
> **Authors**: Shuai Jiang,Christina Robinson,Joseph Anderson,William Hisey,Lynn Butterly,Arief Suriawinata,Saeed Hassanpour
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 通过对医学图像和记录进行预测建模来改善结直肠癌筛查和风险评估
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 结肠镜检查筛查是一种有效的方法，可以在结肠癌（CRC）发展为结肠息肉（CRC）之前。如美国多社会工作组所概述的针对患有息肉的个体所概述的当前后续建议，主要依靠组织病理学特征，忽略了其他重要的CRC风险因素。此外，病理学家之间大肠息肉表征的显着差异在有效的结肠镜检查随访或监测中提出了挑战。数字病理学的演变和深度学习的最新进展为研究提供了一个独特的机会，以调查包括其他病历信息以及使用计算机视觉技术在计算未来CRC风险时使用计算机视觉技术对病理幻灯片进行自动处理。利用新罕布什尔州结肠镜检查注册表的广泛数据集，许多具有纵向结肠镜的随访信息，我们在5年CRC风险预测中调整了最近开发的基于变压器的组织病理学图像分析模型。此外，我们研究了各种多模式融合技术，将医疗记录信息与深度学习衍生的风险估计相结合。我们的发现表明，训练变压器模型预测中间临床变量有助于提高5年CRC风险预测性能，而AUC为0.630，与直接预测相比。此外，在不需要手动检查显微镜图像的同时，成像和非成像特征的融合表现出了5年CRC风险的预测能力，与从结肠镜检查程序和显微镜发现中提取的变量相比，相比之下。这项研究表示将各种数据源和先进计算技术整合到改变未来CRC风险评估的准确性和有效性方面的潜力。

### ViFi-ReID: A Two-Stream Vision-WiFi Multimodal Approach for Person Re-identification 
[[arxiv](https://arxiv.org/abs/2410.09875)] [[cool](https://papers.cool/arxiv/2410.09875)] [[pdf](https://arxiv.org/pdf/2410.09875)]
> **Authors**: Chen Mao,Chong Tan,Jingqi Hu,Min Zheng
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: VIFI-REID：一种对人重新识别的两流视觉Wifi多模式的方法
- **领域**: 计算机视觉和模式识别,信息检索
- **摘要**: 人重新识别（REID）作为安全领域的重要技术，在安全检查，人员计数等中起着至关重要的作用。当前大多数REID方法主要从图像中提取特征，这些特征很容易受到客观条件（例如衣服变化和遮挡）的影响。除相机外，我们还通过通过wifi信号中的通道状态信息（CSI）捕获行人信息来利用广泛可用的路由器作为传感设备，并贡献多模式数据集。我们采用两流网络来分别处理视频理解和信号分析任务，并对行人视频和WiFi数据进行多模式融合和对比度学习。在实际情况下进行的广泛实验表明，我们的方法有效地发现了异质数据之间的相关性，桥接视觉和信号方式之间的差距，显着扩大了传感范围，并提高了多个传感器的REID准确性。

### Text4Seg: Reimagining Image Segmentation as Text Generation 
[[arxiv](https://arxiv.org/abs/2410.09855)] [[cool](https://papers.cool/arxiv/2410.09855)] [[pdf](https://arxiv.org/pdf/2410.09855)]
> **Authors**: Mengcheng Lan,Chaofeng Chen,Yue Zhou,Jiaxing Xu,Yiping Ke,Xinjiang Wang,Litong Feng,Wayne Zhang
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: ICLR 2025. Project page: https://mc-lan.github.io/Text4Seg/
- **标题**: Text4Seg：将图像分割重新构想为文本生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在视觉任务中表现出非凡的功能；但是，有效地将图像分割整合到这些模型中仍然是一个重大挑战。在本文中，我们介绍了Text4Seg，这是一种新颖的文本掩盖范式，将图像分割作为文本生成问题，消除了对其他解码器的需求，并显着简化了分割过程。我们的关键创新是语义描述符，这是一种分割掩码的新文本表示，每个图像补丁都映射到其相应的文本标签。这种统一表示形式允许无缝集成到MLLM的自动回归训练管道，以便于优化。我们证明，代表$ 16 \ times16 $语义描述符的图像会产生竞争性分段性能。为了提高效率，我们介绍了行式运行长度编码（R-RLE），该编码（R-RLE）压缩冗余文本序列，将语义描述符的长度降低了74％，并将推论加速$ 3 \ times $，而不会损害性能。跨各种视觉任务（例如引用表达分割和理解力）进行的广泛实验表明，Text4Seg通过微调不同的MLLM骨架来实现多个数据集上的最新性能。我们的方法为MLLM框架内的以视觉为中心的任务提供了有效，可扩展的解决方案。

### DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2410.09821)] [[cool](https://papers.cool/arxiv/2410.09821)] [[pdf](https://arxiv.org/pdf/2410.09821)]
> **Authors**: Kecen Li,Bingquan Dai,Jingjing Fu,Xinwen Hou
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: DAS3D：3D异常检测的双模式异常合成
- **领域**: 计算机视觉和模式识别
- **摘要**: 事实证明，合成异常样品是自我监视的2D工业异常检测的有效策略。但是，这种方法在多模式异常检测中很少探索，尤其是涉及3D和RGB图像的方法。在本文中，我们提出了一种用于3D异常合成的新型双模式增强方法，该方法简单并且能够模仿3D缺陷的特征。与我们的异常合成方法结合在一起，我们引入了基于重建的判别性异常检测网络，其中使用双模式鉴别器融合了两种模态检测方式的原始和重建嵌入。此外，我们设计了一种增强辍学机制，以增强歧视者的普遍性。广泛的实验表明，我们的方法优于检测精度的最新方法，并在MVTEC 3D-AD和Eyescandies数据集上实现竞争性分割性能。

### ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos 
[[arxiv](https://arxiv.org/abs/2410.09776)] [[cool](https://papers.cool/arxiv/2410.09776)] [[pdf](https://arxiv.org/pdf/2410.09776)]
> **Authors**: Arpan Phukan,Manish Gupta,Asif Ekbal
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: Accepted in EMNLP 2024, https://openreview.net/forum?id=CriKOn01dI
- **标题**: ECIS-VQG：从视频中产生以实体为中心的信息寻求问题
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 以前关于从视频产生的问题的研究主要集中在产生有关常见对象和属性的问题，因此并非以实体为中心。在这项工作中，我们关注从视频中以实体为中心的信息寻求问题的产生。这样的系统对于基于视频的学习可能很有用，建议``人们还问''问题，基于视频的聊天机器人和事实检查。我们的工作解决了三个主要挑战：确定值得问题的信息，将其链接到实体，并有效地利用多模式信号。此外，据我们所知，对于此任务而言，没有一个大规模的数据集。大多数视频问题生成数据集都在电视节目，电影或人类活动上，或者缺乏以实体为中心的信息寻求问题。因此，我们为YouTube视频，视频标语提供了多种数据集，由411个视频和2265个手动注释的问题组成。我们进一步提出了一个结合变压器，丰富的上下文信号（书名，成绩单，字幕，嵌入）的模型架构，以及跨凝性和对比度损失函数的组合，以鼓励以实体为中心的问题生成。我们的最佳方法分别得出71.3、78.6、7.31和81.9的BLEU，胭脂，苹果酒和流星得分，证明了实际可用性。我们将代码和数据集公开可用。 https://github.com/thephukan/ecis-vqg

### Data Adaptive Few-shot Multi Label Segmentation with Foundation Model 
[[arxiv](https://arxiv.org/abs/2410.09759)] [[cool](https://papers.cool/arxiv/2410.09759)] [[pdf](https://arxiv.org/pdf/2410.09759)]
> **Authors**: Gurunath Reddy,Dattesh Shanbhag,Deepa Anand
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 使用基础模型的数据自适应少数弹药的多标签分段
- **领域**: 计算机视觉和模式识别
- **摘要**: 获得图像分割和本地化准确注释的高昂成本使得使用一种和几个射击算法有吸引力。已经出现了几种用于几次分段的最先进方法，包括基于文本的提示来完成该任务，但在医学图像中遭受了优越的性能。现有视觉变压器（VIT）的基础基础模型的利用子像素级特征识别基于单个模板图像的相似区域（ROI）的基础模型已显示出对跨模态的医学图像中的一个hot septementation和在医疗图像中的定位非常有效。但是，这种方法依赖于假设模板图像和测试图像是良好匹配的，并且简单相关足以获得对应关系。实际上，由于患者的姿势变化，即使在单个模态内或使用单个模板图像扩展到3D数据，因此这种方法可能无法概括临床数据。此外，对于多标签任务，必须依次执行ROI识别。在这项工作中，我们提出了基于基础模型（FM）的单个标签，多标签本地化和分割的适配器，以解决这些问题。我们证明了所提出的方法对2D和3D数据的多个分割和本地化任务的功效，因为我们和具有不同姿势的临床数据并根据最新的临床数据进行了评估。

### Surgical-LLaVA: Toward Surgical Scenario Understanding via Large Language and Vision Models 
[[arxiv](https://arxiv.org/abs/2410.09750)] [[cool](https://papers.cool/arxiv/2410.09750)] [[pdf](https://arxiv.org/pdf/2410.09750)]
> **Authors**: Juseong Jin,Chang Wook Jeong
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: NeurIPS 2024 AIM-FM Workshop
- **标题**: 外科手术：通过大语言和视觉模型迈向手术方案
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 由大语言模型提供动力的对话代理正在改变我们与视觉数据互动的方式。最近，已经对图像和视频进行了广泛研究大型视觉模型（LVLM）。但是，这些研究通常集中在常见方案上。在这项工作中，我们引入了专门为手术场景设计的LVLM。我们将手术图像和视频的视觉表示形式集成到语言特征空间中。因此，我们建立了一个LVLM模型，手术闭合，根据手术方案的数据进行了微调。我们的实验表明，外科手术室在手术环境中表现出令人印象深刻的多模式聊天能力，偶尔在看不见的说明中表现出多模式行为。我们对手术方案的视觉提问数据集进行了定量评估。与以前的工作相比，结果表明表现出色，这表明我们模型应对更复杂的手术方案的潜力。

### t-READi: Transformer-Powered Robust and Efficient Multimodal Inference for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2410.09747)] [[cool](https://papers.cool/arxiv/2410.09747)] [[pdf](https://arxiv.org/pdf/2410.09747)]
> **Authors**: Pengfei Hu,Yuhang Qian,Tianyue Zheng,Ang Li,Zhe Chen,Yue Gao,Xiuzhen Cheng,Jun Luo
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: 14 pages, 16 figures
- **标题**: T-Readi：变压器驱动的强大而有效的多模式推断用于自动驾驶
- **领域**: 计算机视觉和模式识别,人工智能,分布式、并行和集群计算,机器学习,机器人技术
- **摘要**: 鉴于自动驾驶汽车（AVS）广泛采用了多模式传感器（例如，相机，激光雷达，雷达，雷达，雷达，雷达），必须进行深层分析以融合其输出以获得强大的感知。但是，现有的融合方法通常在实践中很少有两个假设：i）所有输入的类似数据分布，ii）所有传感器的恒定可用性。例如，例如，激光雷达可能会发生各种分辨率和雷达的失败，因此这种可变性通常会导致融合的显着性能降解。为此，我们提出了Treadi，这是一种适应多模式感觉数据的可变性的自适应推理系统，因此可以实现强大而有效的感知。 T-Readi标识了变异敏感但特定于结构的模型参数；然后，它仅适应这些参数，同时保持其余部分完整。 T-Readi还利用了一种跨模式对比度学习方法来补偿缺失模式的损失。实现这两个功能以保持与现有多模式深融合方法的兼容性。广泛的实验显然表明，与现状方法相比，T-Readi不仅将平均推理准确性提高了6％以上，而且在现实数据和模态变化下，在最坏情况下，在最坏情况下，推理潜伏期几乎减少了15倍，而在最坏情况下仅额外的存储空间仅为5％。

### MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.09733)] [[cool](https://papers.cool/arxiv/2410.09733)] [[pdf](https://arxiv.org/pdf/2410.09733)]
> **Authors**: Hang Hua,Yunlong Tang,Ziyun Zeng,Liangliang Cao,Zhengyuan Yang,Hangfeng He,Chenliang Xu,Jiebo Luo
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: 21 pages, 15 figures
- **标题**: MMComposition：重新审视预训练的视觉模型的组成性
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉模型（VLMS）的出现具有显着高级的多模式理解，从而使能够在各种任务上更加复杂，更精确地整合视觉和文本信息，包括图像和视频字幕，视觉询问答案以及交叉模式检索。尽管VLMS具有出色的功能，但研究人员仍缺乏对其组成性的全面理解 - 理解和生成已知的视觉和文本组件的新型组合的能力。从对象，关系和属性的角度来看，以前的基准仅提供相对粗糙的组成性评估，同时忽略了对对象相互作用，计数和复杂组成的更深入的推理。但是，组成性是一种关键能力，可以促进VLM跨模态的连贯推理和理解。为了解决这一限制，我们提出了MMComposition，这是一种新型的人类注销基准，用于全面，准确地评估VLMS的组成性。我们提出的基准测试是对这些早期作品的补充。使用MMComposition，我们可以量化和探索主流VLM的组成性。令人惊讶的是，我们发现GPT-4O的组成性不如最佳开源模型，我们分析了根本原因。我们的实验分析揭示了VLM在细粒的组成感知和推理中的局限性，并指向了改善VLM设计和训练领域。可用的资源：https：//hanghuacs.github.io/mmcomposition/

### LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.09732)] [[cool](https://papers.cool/arxiv/2410.09732)] [[pdf](https://arxiv.org/pdf/2410.09732)]
> **Authors**: Junyan Ye,Baichuan Zhou,Zilong Huang,Junan Zhang,Tianyi Bai,Hengrui Kang,Jun He,Honglin Lin,Zihao Wang,Tong Wu,Zhizheng Wu,Yiping Chen,Dahua Lin,Conghui He,Weijia Li
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: 79 pages, 63 figures
- **标题**: Loki：使用大型多模型的全面合成数据检测基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着AI生成的内容的快速发展，未来的Internet可能会被合成数据所淹没，从而使对真实和可信的多峰数据的歧视越来越具有挑战性。因此，合成数据检测引起了广泛的关注，并且在此任务中大型多模型（LMM）的性能引起了重大兴趣。 LMM可以为其真实性判断提供自然语言解释，从而增强合成内容检测的解释性。同时，区分真实数据和合成数据的任务有效地测试了LMM的感知，知识和推理能力。作为响应，我们引入了Loki，这是一种新型的基准测试，旨在评估LMMS跨多种方式检测合成数据的能力。 Loki涵盖了视频，图像，3D，文本和音频模式，包括在26个子类别中精心策划的问题，并具有清晰的难度级别。基准包括粗粒度的判断和多项选择问题，以及细粒度的异常选择和解释任务，从而可以对LMM进行全面的分析。我们在LOKI上评估了22个开源LMM和6种封闭式模型，强调了它们作为合成数据检测器的潜力，并揭示了LMM功能发展的某些局限性。有关Loki的更多信息，请访问https://opendatalab.github.io/loki/

### MIRAGE: Multimodal Identification and Recognition of Annotations in Indian General Prescriptions 
[[arxiv](https://arxiv.org/abs/2410.09729)] [[cool](https://papers.cool/arxiv/2410.09729)] [[pdf](https://arxiv.org/pdf/2410.09729)]
> **Authors**: Tavish Mankash,V. S. Chaithanya Kota,Anish De,Praveen Prakash,Kshitij Jadhav
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: 5 pages, 9 figures, 3 tables, submitted to ISBI 2025
- **标题**: 幻影：印度一般处方中注释的多模式识别和认可
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 尽管有电子记录（EMR），但印度的医院仍然依靠手写病历，这使统计分析复杂化和记录检索。手写记录提出了一个独特的挑战，需要培训模型的专业数据才能识别药物及其建议模式。尽管传统的手写识别方法采用了2-D LSTM，但最近的研究使用多模式大语模型（MLLM）进行了OCR任务探索。在这种方法的基础上，我们专注于从模拟的医疗记录中提取药物名称和剂量。我们的方法论幻影（印度一般处方中的注释的多模式识别和识别）涉及对743,118高分辨率模拟的QWEN VL，LLAVA 1.6和IDEFICS2模型进行微调，并从印度1,133位医生进行了注释。我们的方法在提取药物名称和剂量方面达到了82％的准确性。

### EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation 
[[arxiv](https://arxiv.org/abs/2410.09704)] [[cool](https://papers.cool/arxiv/2410.09704)] [[pdf](https://arxiv.org/pdf/2410.09704)]
> **Authors**: Milos Vukadinovic,Xiu Tang,Neal Yuan,Paul Cheng,Debiao Li,Susan Cheng,Bryan He,David Ouyang
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: 30 pages, 3 tables, 3 figures
- **标题**: Echoprime：用于综合超声心动图解释的多视频观看视觉语言模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 超声心动图是最广泛使用的心脏成像方式，可捕获超声视频数据以评估心脏结构和功能。超声心动图中的人工智能（AI）具有简化手动任务并提高可重复性和精度的潜力。但是，大多数超声心动图AI模型是单视图，单任务系统，这些系统无法从完整考试期间捕获的多个视图中综合互补信息，从而导致应用程序的性能和范围有限。为了解决这个问题，我们介绍了Echoprime，这是一种多视图，基于视频的视觉语言基础模型，该模型在超过1200万个视频报告对上训练。 Echoprime使用对比度学习在全面的超声心动图研究中训练所有标准视图的统一嵌入模型，并代表了稀有疾病和常见疾病和诊断。然后，Echoprime利用视图分类和视频的解剖学注意模型来精确地绘制超声心动图视图与解剖结构之间的关系。通过检索解释，Echoprime在一项全面的研究中整合了所有超声心动图视频的信息，并进行了整体综合临床超声心动图解释。在来自两个独立医疗系统的数据集中，Echoprime在23种不同的心脏形式和功能的基准上实现了最先进的性能，超过了特定于任务的方法和先前的基础模型的性能。经过严格的临床评估，Echoprime可以帮助医生对全面的超声心动图进行自动初步评估。

### Reconstructive Visual Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2410.09575)] [[cool](https://papers.cool/arxiv/2410.09575)] [[pdf](https://arxiv.org/pdf/2410.09575)]
> **Authors**: Haochen Wang,Anlin Zheng,Yucheng Zhao,Tiancai Wang,Zheng Ge,Xiangyu Zhang,Zhaoxiang Zhang
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 重建视觉教学调整
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 本文介绍了一个大型多模型（LMMS）的重建视觉教学调整（ROSS），这些家族利用了以视觉为中心的监督信号。与独家监督文本输出的常规视觉教学调谐方法相反，Ross提示LMMS通过重建输入图像来监督视觉输出。通过这样做，它利用了输入图像本身中固有的丰富性和细节，这些固有的细节和细节通常在纯文本监督中丢失。但是，由于视觉信号的大量空间冗余，从自然图像中产生有意义的反馈是具有挑战性的。为了解决这个问题，罗斯采用了一个固定目标来重建输入图像的潜在表示，避免直接回归精确的原始RGB值。这种内在的激活设计固有地鼓励LMM维护图像细节，从而增强其细粒度的理解能力并减少幻觉。从经验上讲，罗斯始终在不同的视觉编码器和语言模型中带来重大改进。与汇总多个视觉专家的外部辅助替代方案相比，Ross通过单个siglip视觉编码器提供竞争性能，这证明了我们为视觉输出量身定制的以视觉监督的功效。

### DiffuTraj: A Stochastic Vessel Trajectory Prediction Approach via Guided Diffusion Process 
[[arxiv](https://arxiv.org/abs/2410.09550)] [[cool](https://papers.cool/arxiv/2410.09550)] [[pdf](https://arxiv.org/pdf/2410.09550)]
> **Authors**: Changlin Li,Yanglei Gan,Tian Lan,Yuxiang Cai,Xueyi Liu,Run Lin,Qiao Liu
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: containing 14pages, 9 figures and 3 tables; Submitted to IEEE Transactions on Intelligent Transportation Systems on 17-June-2024
- **标题**: Diffutraj：通过引导扩散过程的随机血管轨迹预测方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 海上船只的操纵以其固有的复杂性和不确定性为特征，需要船舶轨迹预测系统，能够对未来运动状态的多模式性质进行建模。但是，常规的随机轨迹预测方法利用潜在变量表示血管运动的多模式，但是，倾向于忽略海上行为固有的复杂性和动力学。相反，我们明确模拟了血管运动从不确定性转变为确定性状态的过渡，从而有效地处理了动态场景中未来的不确定性。在本文中，我们提出了一个新颖的框架（\ textit {diffutraj}），以将轨迹预测任务概念化为运动模式不确定性扩散的指导反向过程，在这种情况下，我们逐渐从海上区域消除了不确定性区域以描绘出意图的轨迹。具体而言，我们编码了目标血管的先前状态，血管 - 血管相互作用以及环境环境作为轨迹产生的指导因素。随后，我们设计了一个基于变压器的条件denoiser来捕获时空依赖性，从而使能够在特定海上环境中更好地对齐轨迹。关于血管轨迹预测基准的全面实验证明了我们方法的优越性。

### Skipping Computations in Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2410.09454)] [[cool](https://papers.cool/arxiv/2410.09454)] [[pdf](https://arxiv.org/pdf/2410.09454)]
> **Authors**: Mustafa Shukor,Matthieu Cord
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: Accepted at NeurIPS 2024 Workshop RBFM. Code: https://github.com/mshukor/ima-lmms
- **标题**: 在多模式LLMS中跳过计算
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 大型语言模型（LLMS）在文本和多模式领域都表现出了很大的成功。但是，这种成功通常会带有大量的计算成本，尤其是当处理多模式输入的冗长序列时。这引发了许多努力，重点是提高培训和推理期间的效率。在这项研究中，我们研究了推理过程中多模式大语言模型（MLLM）中的计算冗余。我们提出了不同的方法来跳过计算，例如跳过整个块，FFN或自我注意力（SA）层。此外，我们探索平行于某些层，例如FFN和SA层。我们的发现验证了（1）在推理时可以避免大量计算，尤其是对于诸如视觉问题回答（VQA）之类的任务。 （2）在训练期间的跳过计算可以恢复原始性能的97％，即使跳过一半的块或删除了70％的权重。或者，（3）使用较小的LLM适当训练可以产生与LLMS相当2或3倍的可比性能。总而言之，我们将调查扩展到最近的MLLM，例如LLAVA-1.5，显示了类似的观察结果。我们的工作表明，MLLM内部有冗余的计算，因此有可能在不牺牲绩效的情况下显着提高推理成本。该代码可在此处提供：https：//github.com/mshukor/ima-lmms。

### VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment 
[[arxiv](https://arxiv.org/abs/2410.09421)] [[cool](https://papers.cool/arxiv/2410.09421)] [[pdf](https://arxiv.org/pdf/2410.09421)]
> **Authors**: Lei Li,Zhihui Xie,Mukai Li,Shunian Chen,Peiyi Wang,Liang Chen,Yazheng Yang,Benyou Wang,Lingpeng Kong,Qi Liu
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: EMNLP 2024 Main Conference camera-ready version (fixed small typos). This article supersedes arXiv:2312.10665
- **标题**: VLFEEDBACK：大型AI反馈数据集用于大型视觉模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 随着大型视觉模型（LVLM）的发展迅速，对这些模型的高质量和多种数据的需求变得越来越重要。但是，通过人类监督创建此类数据是昂贵且耗时的。在本文中，我们研究了AI反馈对扩展LVLM的规模监督的功效。我们介绍了VLFEEDBACK，这是第一个大规模视觉反馈数据集，其中包括超过82K的多模式指令以及没有人类注释的现成模型产生的全面理由。为了评估AI反馈对视觉比对的有效性，我们通过直接偏好优化的VLFEEDBACK进行了微调训练Silkie。 Silkie展示了有关帮助，视觉忠诚和安全指标的出色表现。在感知和认知任务中，它的表现优于6.9 \％和9.5％的基本模型，减少了MMHAL板凳上的幻觉问题，并具有对红线攻击的弹性增强。此外，我们的分析强调了AI反馈的优势，尤其是在促进偏好多样性以提供更全面的改进方面。我们的数据集，培训代码和模型可在https://vlf-silkie.github.io上找到。

### Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering 
[[arxiv](https://arxiv.org/abs/2410.09380)] [[cool](https://papers.cool/arxiv/2410.09380)] [[pdf](https://arxiv.org/pdf/2410.09380)]
> **Authors**: Ting Yu,Kunhao Fu,Shuhui Wang,Qingming Huang,Jun Yu
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: IEEE Transactions on Circuits and Systems for Video Technology
- **标题**: 促使视频基础模型具有特定领域的细粒启发式方法，以回答视频问题
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视频问题回答（videoqa）代表视频理解和语言处理之间的关键交集，需要判别的单峰理解和复杂的跨模式互动才能准确推断。尽管多模式预训练的模型和视频语言基础模型取得了进步，但由于其广义的预训练目标，这些系统通常会与特定于领域的VideoQA斗争。解决这一差距需要弥合广泛的跨模式知识与视频QA任务的特定推理需求之间的鸿沟。为此，我们介绍了Heurvidqa，该框架利用特定领域的实体启发式方法来完善预训练的视频基础模型。我们的方法将这些模型视为隐式知识引擎，采用特定领域的实体启动提示者将模型的焦点引向增强推理的精确提示。通过提供细粒度的启发式方法，我们提高了该模型识别和解释关键实体和行动的能力，从而增强了其推理能力。多个VideoQA数据集的广泛评估表明，我们的方法显着胜过现有模型，强调将特定领域知识集成到视频语言模型中，以提供更准确和上下文感知的VideoQA。

### Multi-granularity Contrastive Cross-modal Collaborative Generation for End-to-End Long-term Video Question Answering 
[[arxiv](https://arxiv.org/abs/2410.09379)] [[cool](https://papers.cool/arxiv/2410.09379)] [[pdf](https://arxiv.org/pdf/2410.09379)]
> **Authors**: Ting Yu,Kunhao Fu,Jian Zhang,Qingming Huang,Jun Yu
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: Transactions on Image Processing
- **标题**: 端到端长期视频问题回答的多晶对比跨模式合作生成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 长期视频问答（VideoQA）是一项挑战性的视觉和语言桥接任务，重点是对未经修剪的长期视频和各种自由形式问题的语义理解，同时强调全面的跨模式推理，以产生精确的答案。规范方法通常依靠现成的特征提取器来绕开昂贵的计算开销，但通常会导致与域无关的模态无关表示。此外，单峰理解和跨模式相互作用之间的固有梯度阻碍了可靠的答案。相比之下，最近出现的成功视频语言预训练模型可实现具有成本效益的端到端建模，但在特定于域特异性的比率上不足，并且在任务配方中表现出差异。为此，我们为长期VideoQA提供了一个完全端到端的解决方案：多晶格对比跨模式协作生成（MCG）模型。为了得出具有高视觉概念的歧视性表示，我们在夹骨架构上引入了联合单峰建模（JUM），并利用多晶格对比学习（MCL）来利用本质或明确的语义对应关系。为了减轻任务配方差异问题，我们提出了一个跨模式协作生成（CCG）模块，以将视频QA重新定义为生成任务，而不是传统的分类方案，从而使模型具有跨模式高弹性融合的能力，以便于合理化和答案。在六个公开可用的VideoQA数据集上进行的广泛实验强调了我们提出的方法的优势。

### GEM-VPC: A dual Graph-Enhanced Multimodal integration for Video Paragraph Captioning 
[[arxiv](https://arxiv.org/abs/2410.09377)] [[cool](https://papers.cool/arxiv/2410.09377)] [[pdf](https://arxiv.org/pdf/2410.09377)]
> **Authors**: Eileen Wang,Caren Han,Josiah Poon
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: GEM-VPC：视频段落字幕的双图增强多模式集成
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频段落字幕（VPC）旨在生成段落字幕，总结视频中的关键事件。尽管最近的进步，挑战仍然存在，特别是有效地利用视频中固有的多模式信号并解决了单词的长尾分布。本文引入了一个新型的多式联运字幕生成框架，用于VPC，该框架利用各种模式和外部知识库的信息。我们的框架构造了两个图：“特定于视频”的时间图，可捕获多模式信息和常识知识之间的重大事件和交互，以及代表特定主题单词之间相关性的“主题图”。这些图作为具有共享编码器架构的变压器网络的输入。我们还引入了一个节点选择模块，以通过从图形中选择最相关的节点来提高解码效率。我们的结果表明，在基准数据集中性能出色。

### Debiasing Vison-Language Models with Text-Only Training 
[[arxiv](https://arxiv.org/abs/2410.09365)] [[cool](https://papers.cool/arxiv/2410.09365)] [[pdf](https://arxiv.org/pdf/2410.09365)]
> **Authors**: Yunfan Yang,Chaoquan Jiang,Zhiyu Lin,Jinlin Xiao,Jiaming Zhang,Jitao Sang
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 使用纯文本培训的Vison语言模型进行辩护
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 通过在统一的嵌入空间中对齐文本和图像，预训练的视觉模型（VLMS）（例如剪辑）在各种下游任务中表现出了不起的性能。但是，由于预先训练的数据集的分布不平衡，剪辑遇到了现实世界应用中的偏差问题。现有的陈述方法难以为少数群体获得足够的图像样本，并为小组标签带来了高昂的成本。为了解决这些限制，我们提出了一个名为TOD的仅文本辩论框架，利用文本图像培训范式来减轻视觉偏见。具体而言，此方法将文本编码器重新塑造为作为图像编码器的功能，从而消除了对图像数据的需求。同时，它利用大型语言模型（LLM）生成平衡的文本数据集，然后将其用于及时调整。但是，我们观察到该模型过度适合文本模式，因为标签名称作为监督信号，在文本中明确出现。为了解决这个问题，我们进一步引入了一个多目标预测（MTP）任务，该任务激励模型专注于复杂的环境并区分目标和偏见信息。在水鸟和Celeba数据集上进行的广泛实验表明，与图像监督方法相比，我们的方法显着提高了群体的鲁棒性，在无图像方法之间达到了最新的结果，甚至可以在无图像的方法之间获得最新的结果。此外，所提出的方法可以适应具有多个或未知偏置属性的具有挑战性的场景，表明其强烈的概括和稳健性。

### Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment 
[[arxiv](https://arxiv.org/abs/2410.09347)] [[cool](https://papers.cool/arxiv/2410.09347)] [[pdf](https://arxiv.org/pdf/2410.09347)]
> **Authors**: Huayu Chen,Hang Su,Peize Sun,Jun Zhu
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 通过条件对比度对准无引导AR的视觉生成
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: 无分类器引导（CFG）是提高视觉生成模型样品质量的关键技术。但是，在自回旋（AR）多模式生成中，CFG引入了语言和视觉内容之间的设计不一致，与设计统一视觉AR的不同方式的设计理念相矛盾。通过语言模型对准方法的启发，我们提出\ textit {条件对比对准}（CCA），以促进具有高性能的无指导AR视觉生成，并使用带导的采样方法分析其理论联系。与更改采样过程以实现理想采样分布的指导方法不同，CCA直接微调模型以适合相同的分布目标。实验结果表明，CCA可以在预处理数据集中显着提高所有测试模型的无指导性能（$ \ sim $ 1 \％\％），与指导采样方法相等。这在很大程度上消除了对AR视觉生成中的指导采样的需求，并将采样成本降低了一半。此外，通过调整培训参数，CCA可以在类似于CFG的样本多样性和忠诚度之间取舍。该实验在实验上证实了以语言为目标的对准和视觉目标指导方法之间的牢固理论联系，从而统一了两个以前独立的研究领域。代码和模型权重：https：//github.com/thu-ml/cca。

### Towards Multi-Modal Animal Pose Estimation: A Survey and In-Depth Analysis 
[[arxiv](https://arxiv.org/abs/2410.09312)] [[cool](https://papers.cool/arxiv/2410.09312)] [[pdf](https://arxiv.org/pdf/2410.09312)]
> **Authors**: Qianyi Deng,Oishi Deb,Amir Patel,Christian Rupprecht,Philip Torr,Niki Trigoni,Andrew Markham
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: 35 pages, 5 figures, 8 tables. Qianyi Deng and Oishi Deb are Joint Major Contributors to this work
- **标题**: 迈向多模式动物姿势估计：一项调查和深入分析
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 动物姿势估计（APE）旨在使用各种传感器和模态输入（例如RGB摄像机，Lidar，Infrared，IMU，声学和语言提示）定位动物体部位，这对于跨NeuRoscience，生物力学和兽医医学的研究至关重要。通过评估自2011年以来的176篇论文，APE方法由其输入传感器和模态类型，输出形式，学习范式，实验设置和应用领域进行分类，并在单一和多模式猿系统中详细介绍了当前趋势，挑战和未来方向的详细分析。该分析还强调了人与动物姿势估计之间的过渡，以及猿类的创新如何相互富集人的姿势估计和更广泛的机器学习范式。此外，还提供了基于不同传感器和方式的2D和3D APE数据集以及评估指标。这里提供了定期更新的项目页面：https：//github.com/chennydeng/mm-pape。

### The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024 
[[arxiv](https://arxiv.org/abs/2410.09088)] [[cool](https://papers.cool/arxiv/2410.09088)] [[pdf](https://arxiv.org/pdf/2410.09088)]
> **Authors**: Yinan Han,Qingyuan Jiang,Hongming Mei,Yang Yang,Jinhui Tang
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 时间动作定位任务的解决方案感知测试挑战2024
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 本报告介绍了我们的时间动作本地化方法（TAL），该方法着重于在视频序列中识别和分类操作。我们通过使用来自Soseing-SomethingV2数据集的重叠标签扩展培训数据集来采用数据增强技术，从而增强了模型在各种动作类别中概括的能力。为了提取功能，我们利用最先进的型号，包括UMT，视频功能的VideomaEv2，以及用于音频功能的Beats和Cav-Mae。我们的方法涉及培训多模式（视频和音频）和单峰（仅视频）模型，然后使用加权盒融合（WBF）方法组合预测。这种融合策略确保了强大的行动定位。我们的整体方法的得分为0.5498，在比赛中获得了第一名。

### MiRAGeNews: Multimodal Realistic AI-Generated News Detection 
[[arxiv](https://arxiv.org/abs/2410.09045)] [[cool](https://papers.cool/arxiv/2410.09045)] [[pdf](https://arxiv.org/pdf/2410.09045)]
> **Authors**: Runsheng Huang,Liam Dugan,Yue Yang,Chris Callison-Burch
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: EMNLP 2024 Findings
- **标题**: Miragenews：多模式现实的AI生成的新闻检测
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 近年来，炎症或误导性的“假”新闻内容的扩散变得越来越普遍。同时，使用AI工具来生成描绘任何可以想象的场景的影像图像变得比以往任何时候都更容易。将这两个（AI生成的假新闻内容）结合在一起特别有效和危险。为了打击AI生成的虚假新闻的传播，我们提出了Miragenews DataT，该数据集是来自先进的生成器的12,500个高质量的真实和AI生成的图像扣对。我们发现，我们的数据集对人（60％F-1）和最先进的多模式LLM（<24％F-1）构成了重大挑战。使用我们的数据集，我们训练一个多模式检测器（Mirage），该检测器（Mirage）比最先进的基线相比，在室外图像生成器和新闻发布者的图像合作对上的最先进基线相比。我们发布我们的代码和数据，以帮助未来检测AI生成的内容的工作。

### Can GPTs Evaluate Graphic Design Based on Design Principles? 
[[arxiv](https://arxiv.org/abs/2410.08885)] [[cool](https://papers.cool/arxiv/2410.08885)] [[pdf](https://arxiv.org/pdf/2410.08885)]
> **Authors**: Daichi Haraguchi,Naoto Inoue,Wataru Shimoda,Hayato Mitani,Seiichi Uchida,Kota Yamaguchi
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: Accepted to SIGGRAPH Asia 2024 (Technical Communications Track)
- **标题**: GPT可以根据设计原理评估图形设计吗？
- **领域**: 计算机视觉和模式识别,图形
- **摘要**: 基础模型的最新进展显示出图形设计生成的有希望的能力。假设LMM可以正确评估其质量，但已经开始采用大型多模型（LMM）来评估图形设计，但是目前尚不清楚评估是否可靠。评估图形设计质量的一种方法是评估设计是否遵守基本图形设计原理，这是设计师的共同实践。在本文中，我们使用60名受试者收集的人类注释基于设计原理比较了基于GPT的评估和启发式评估的行为。我们的实验表明，尽管GPT无法区分小细节，但它们与人类注释有着相当良好的相关性，并且基于设计原理表现出与启发式指标相似的趋势，这表明它们确实能够评估图形设计的质量。我们的数据集可从https://cyberagentailab.github.io/graphic-design-evaluation获得。

### Multi-modal Fusion based Q-distribution Prediction for Controlled Nuclear Fusion 
[[arxiv](https://arxiv.org/abs/2410.08879)] [[cool](https://papers.cool/arxiv/2410.08879)] [[pdf](https://arxiv.org/pdf/2410.08879)]
> **Authors**: Shiao Wang,Yifeng Wang,Qingchuan Ma,Xiao Wang,Ning Yan,Qingquan Yang,Guosheng Xu,Jin Tang
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 基于多模式融合的Q分布预测
- **领域**: 计算机视觉和模式识别
- **摘要**: Q分布预测是受控核融合的关键研究方向，深入学习是解决预测挑战的关键方法。在本文中，我们利用深度学习技术来解决Q分布预测的复杂性。具体来说，我们探索计算机视觉中的多模式融合方法，将2D线图像数据与原始1D数据集成在一起以形成双峰输入。此外，我们采用了变压器的注意机制来提取特征和双峰信息的互动融合。广泛的实验验证了我们方法的有效性，从而大大降低了Q分布中的预测错误。

### Hespi: A pipeline for automatically detecting information from hebarium specimen sheets 
[[arxiv](https://arxiv.org/abs/2410.08740)] [[cool](https://papers.cool/arxiv/2410.08740)] [[pdf](https://arxiv.org/pdf/2410.08740)]
> **Authors**: Robert Turnbull,Emily Fitzgerald,Karen Thompson,Joanne L. Birch
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: HESPI：用于自动检测出植物标本板信息的管道
- **领域**: 计算机视觉和模式识别,人工智能,信息检索
- **摘要**: 寻找相关的生物多样性数据，以寻求生物学，环境，气候和保护科学。从样品图像中提取数据需要速率转移，以消除对这些数据所代表的人介导的转录的依赖的瓶颈。我们应用了先进的计算机视觉技术来开发“ hespi”（植物标本室标本板管道），该技术从其数字图像中提取了在植物标本室标本上的机构标签上收集数据的库前子集。管道集成了两个对象检测模型；第一个检测到基于文本标签的边界框，第二个检测到主要机构标签上基于文本的数据字段的边界框。该管道将​​基于文本的机构标签分类为打印，键入，手写或组合，并应用光学字符识别（OCR）和手写文本识别（HTR）进行数据提取。然后，根据分类单元名称的权威数据库进行纠正。提取的文本还可以用多模式大语言模型（LLM）的助手进行校正。 Hespi准确地检测并提取文本用于测试数据集，包括来自国际草药的标本表图像。管道的组件是模块化的，用户可以用自己的数据训练自己的模型，并将其代替提供的模型。

### MMLF: Multi-modal Multi-class Late Fusion for Object Detection with Uncertainty Estimation 
[[arxiv](https://arxiv.org/abs/2410.08739)] [[cool](https://papers.cool/arxiv/2410.08739)] [[pdf](https://arxiv.org/pdf/2410.08739)]
> **Authors**: Qihang Yang,Yang Zhao,Hong Cheng
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: MMLF：多模式的多级晚期融合，用于对象检测，不确定性估计
- **领域**: 计算机视觉和模式识别,系统与控制
- **摘要**: 自动驾驶需要高级对象检测技术，该技术将信息整合到多种模式中以克服与单模式方法相关的限制。使早期融合和复杂性各种数据保持一致的挑战，以及深层融合引入的过度拟合问题，强调了决策水平后期融合的功效。晚期融合可确保无缝集成，而不会改变原始检测器的网络结构。本文介绍了一种开创性的多模式多级后期融合方法，该方法旨在延迟融合以实现多级检测。在KITTI验证和官方测试数据集上进行的融合实验说明了大量的性能改进，将我们的模型作为自动驾驶中多模式对象检测的多功能解决方案提出。此外，我们的方法将不确定性分析纳入分类融合过程中，使我们的模型更加透明和值得信赖，并为类别预测提供了更可靠的见解。

### Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping 
[[arxiv](https://arxiv.org/abs/2410.08695)] [[cool](https://papers.cool/arxiv/2410.08695)] [[pdf](https://arxiv.org/pdf/2410.08695)]
> **Authors**: Yue Yang,Shuibai Zhang,Wenqi Shao,Kaipeng Zhang,Yi Bin,Yu Wang,Ping Luo
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 动态多模式评估具有柔性复杂性，通过视觉启动启动
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉模型（LVLM）在多模式任务（例如视觉感知和推理）之间表现出了显着的功能，从而在各种多模式评估基准上都表现出色。但是，这些基准测试保持静态性质并与预训练数据重叠，从而导致固定的复杂性约束和数据污染问题。这引起了人们对评估有效性的关注。为了应对这两个挑战，我们引入了一种动态多模式评估协议，称为视觉引导（VLB）。 VLB对LVLM提供了强大而全面的评估，其数据污染和灵活的复杂性降低。为此，VLB通过多模式的引导模块动态生成新的视觉提问样本，该模块可修改图像和语言，同时确保新生成的样本与法官模块的原始样本保持一致。通过制定各种自举策略，VLB提供了具有多种复杂性的现有基准的动态变体，从而使评估能够与LVLMS不断发展的功能共同发展。多个基准（包括Seedbench，MMBENCH和MME）之间的广泛实验结果表明，VLB显着降低了数据污染并暴露了LVLMS的性能限制。

### Cross-Modal Bidirectional Interaction Model for Referring Remote Sensing Image Segmentation 
[[arxiv](https://arxiv.org/abs/2410.08613)] [[cool](https://papers.cool/arxiv/2410.08613)] [[pdf](https://arxiv.org/pdf/2410.08613)]
> **Authors**: Zhe Dong,Yuzhe Sun,Yanfeng Gu,Tianzhu Liu
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 用于引用遥感图像分割的跨模式双向交互模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 给定自然语言表达式和遥感图像，引用遥感图像分割（RRSIS）的目标是生成由引用表达式标识的目标对象的像素级掩码。与自然情景相反，RRSIS中的表达通常涉及复杂的地理空间关系，而感兴趣的目标对象在规模上差异很大，并且缺乏视觉显着性，从而增加了实现精确细分的困难。为了应对上述挑战，提出了一种新型的RRSIS框架，称为跨模式双向相互作用模型（Crobim）。具体而言，上下文感知的提示调制（CAPM）模块旨在将空间位置关系和特定于任务的知识整合到语言特征中，从而增强了捕获目标对象的能力。此外，引入了语言引导的特征聚合（LGFA）模块，以将语言信息整合到多尺度的视觉特征中，并结合了注意力不足补偿机制以增强特征聚合。最后，相互交流解码器（MID）旨在通过级联的双向交叉注意来增强跨模式特征对齐，从而实现了精确的分割掩码预测。为了进一步发展RRSIS的研究，我们还构建了Risbench，这是一种新的大规模基准数据集，其中包括52,472个图像标签标签。 Risbench和另外两个普遍数据集的基准测试表明，拟议的Crobim优于现有的最新方法（SOTA）方法。 Crobim和Risbench数据集的源代码将在https://github.com/hit-sirs/crobim上公开获得

### VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding 
[[arxiv](https://arxiv.org/abs/2410.08593)] [[cool](https://papers.cool/arxiv/2410.08593)] [[pdf](https://arxiv.org/pdf/2410.08593)]
> **Authors**: Houlun Chen,Xin Wang,Hong Chen,Zeyang Zhang,Wei Feng,Bin Huang,Jia Jia,Wenwu Zhu
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: Accepted by 38th NeurIPS Datasets & Benchmarks Track (NeurIPS 2024)
- **标题**: 已验证：视频语料库时刻检索基准测试，以了解细粒度的视频理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 现有的视频语料库时刻检索（VCMR）仅限于粗粒的理解，当给出细粒度的查询时，这会阻碍精确的视频力矩定位。在本文中，我们提出了一个更具挑战性的细粒VCMR基准测试，需要使用其他部分匹配的候选人来定位语料库中最佳匹配的时刻。为了提高数据集构造效率并确保高质量的数据注释，我们提出了验证，自动\下划线{v} id \ usewissline {e} o-text注释管道，以生成\ usewissline {r} el \ ellinline {el \ useverline {i} able {i} able {i} able pastinline {fi} nlline {fi} n \ plustline {fi} n \ plustline { \下划线{d} ynamics。具体来说，我们使用我们提出的静态和动态增强字幕模块来求助于大型语言模型（LLM）和大型多模型模型（LMM），以为每个视频生成多样的细粒字幕。为了滤除LLM幻觉引起的不准确注释，我们提出了一个细颗粒性的噪声评估器，在其中我们调整了带有干扰硬性阴性的视频基础模型增强了对比度和匹配损失。经过验证，我们构建了一个更具挑战性的细粒VCMR基准，其中包含charades-fig，didemo-fig和ActivityNet-Fig，表现出高度的注释质量。我们在拟议的数据集上评估了几种最先进的VCMR模型，表明在VCMR中，精细粒度的视频理解仍然存在很大的范围。代码和数据集在\ href {https://github.com/hlchen23/verified} {https://github.com/hlchen23/verified}中。

### Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting 
[[arxiv](https://arxiv.org/abs/2410.08612)] [[cool](https://papers.cool/arxiv/2410.08612)] [[pdf](https://arxiv.org/pdf/2410.08612)]
> **Authors**: Purushothaman Natarajan,Kamal Basha,Athira Nambiar
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: 12 pages, 5 tables and 9 figures
- **标题**: 合成器：通过双重扩散模型和GPT提示，声纳图像合成具有增强的多样性和现实主义
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 声纳图像合成对于推进水下探索，海洋生物学和防御的应用至关重要。传统方法通常依赖于使用声纳传感器的广泛且昂贵的数据收集，从而危害数据质量和多样性。为了克服这些局限性，本研究提出了一个新的声纳图像合成框架，合成器 - 利用扩散模型和GPT提示。合成器 - 索纳尔的主要新颖性是三倍：首先，通过整合基于AI的生成风格注入技术以及公开可用的真实/模拟数据，从而生成了声纳研究最大的声纳数据语料库之一。其次，双文本条件的声纳扩散模型层次结构综合了具有增强质量和多样性的粗粒和细粒声纳图像。第三，高级（粗）和低级（详细）基于文本的声纳生成方法利用了视觉语言模型（VLM）和GPT启动的高级语义信息。在推断期间，该方法从文本提示中产生了多样化且逼真的声纳图像，从而弥合了文本描述和声纳图像生成之间的差距。据我们所知，这标志着首次在声纳图像中使用GPT的应用。 Synth-Sonar实现最先进的结果，可以产生高质量的合成声纳数据集，从而显着增强其多样性和现实主义。

### Automatically Generating Visual Hallucination Test Cases for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.11242)] [[cool](https://papers.cool/arxiv/2410.11242)] [[pdf](https://arxiv.org/pdf/2410.11242)]
> **Authors**: Zhongye Liu,Hongbin Liu,Yuepeng Hu,Zedian Shao,Neil Zhenqiang Gong
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 自动生成多模式大语言模型的视觉幻觉测试用例
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 当多模式大语言模型（MLLM）生成带有不正确的视觉细节以提示的响应时，视觉幻觉（VH）发生。现有的生成VH测试案例的方法主要依赖于人类注释，通常以三元组的形式：（图像，问题，答案）。在本文中，我们引入了VH Expandion，这是第一种扩展MLLM的VH测试用例的自动化方法。给定初始的VH测试案例，VH Expansion通过否定和使用常见和对抗性扰动来修改图像来自动扩展问题和答案。此外，我们提出了一种新的评估度量，对称准确性，该度量可以衡量正确回答的VH测试案例对的比例。每对由测试案例及其被否定的对应组成。我们的理论分析表明，对称的准确性是一个公正的评估度量，当MLLM随机猜测答案时，VH测试案例的不平衡不平衡，而传统准确性则容易出现这种不平衡。我们应用VHExpansion扩展了手动注释的三个VH数据集，并将这些扩展的数据集使用来基准七个mllms。我们的评估表明，VHExpansion有效地识别了更多的VH测试案例。此外，与传统的准确度量相比，对称准确性（公正）导致了关于MLLMS对VH的脆弱性的不同结论。最后，我们表明，与原始的，手动注释的数据集对VHExpansion生成的扩展的VH数据集上的微调MLLM更有效地减轻VH。我们的代码可在以下网址提供：https：//github.com/lycheeefish/vhexpansion。

### TEOcc: Radar-camera Multi-modal Occupancy Prediction via Temporal Enhancement 
[[arxiv](https://arxiv.org/abs/2410.11228)] [[cool](https://papers.cool/arxiv/2410.11228)] [[pdf](https://arxiv.org/pdf/2410.11228)]
> **Authors**: Zhiwei Lin,Hongbo Jin,Yongtao Wang,Yufei Wei,Nan Dong
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Accepted by ECAI2024
- **标题**: TEOCC：雷达相机多模式占用率通过时间增强预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 作为一个新颖的3D场景表示，语义占用在自动驾驶中引起了很多关注。但是，现有的占用预测方法主要集中于设计更好的占用表示形式，例如三镜头视图或神经辐射率字段，同时忽略了使用长周期信息的优势。在本文中，我们提出了一个雷达相机多模式时间增强的占用预测网络，称为TeoCC。我们的方法的灵感来自于在3D对象检测中使用时间信息的成功。具体而言，我们引入了一个时间增强分支，以学习时间占用预测。在此分支中，我们随机丢弃多视图摄像机的T-K输入框架，并通过长期和短期时间解码器分别使用来自其他相邻帧和多模式输入的信息来预测其3D占用率。此外，为了降低计算成本并结合了多模式输入，我们专门设计了用于长期和短期时间解码器的3D卷积层。此外，由于轻巧的占用预测头是一个密集的分类头，因此我们建议使用共享的占用预测头进行时间增强和主要分支。值得注意的是，时间增强分支仅在训练期间进行，并在推理过程中被丢弃。实验结果表明，TEOCC实现了Nuscenes基准测试的最新占用预测。此外，拟议的时间增强分支是一个插件模块，可以轻松地集成到现有的占用预测方法中，以提高占用预测的性能。代码和模型将在https://github.com/vdigpku/teocc上发布。

### A CLIP-Powered Framework for Robust and Generalizable Data Selection 
[[arxiv](https://arxiv.org/abs/2410.11215)] [[cool](https://papers.cool/arxiv/2410.11215)] [[pdf](https://arxiv.org/pdf/2410.11215)]
> **Authors**: Suorong Yang,Peng Ye,Wanli Ouyang,Dongzhan Zhou,Furao Shen
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: 10 pages
- **标题**: 用于稳定和可推广数据选择的剪贴式框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，大型数据集对深度学习模型的进步至关重要，但是在如此大的数据集中进行的培训总是会产生大量的存储和计算开销。同时，现实世界中的数据集通常包含冗余和嘈杂的数据，对培训效率和模型性能构成负面影响。数据选择显示了确定整个数据集中最具代表性的样本的希望，该样本旨在通过降低的培训成本来最大程度地减少性能差距。现有作品通常依靠单模式信息来分配单个样本的重要性得分，这可能导致评估不准确，尤其是在处理嘈杂或损坏的样本时。为了解决这一限制，我们提出了一个新型的夹式数据选择框架，该框架利用多模式信息以进行更健壮和可推广的样本选择。具体而言，我们的框架包括三个关键模块数据的适应，样本评分和选择优化 - 它们可以通过多种预训练的多模式知识来全面评估样本影响并通过多目标优化来优化选择结果。广泛的实验表明，我们的方法始终优于各种基准数据集上现有的最新基线。值得注意的是，我们的方法有效地消除了数据集中的嘈杂或损坏的样本，从而使其能够通过更少的数据实现更高的性能。这表明这不仅是加速培训的一种方法，还可以提高整体数据质量。

### MANet: Fine-Tuning Segment Anything Model for Multimodal Remote Sensing Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2410.11160)] [[cool](https://papers.cool/arxiv/2410.11160)] [[pdf](https://arxiv.org/pdf/2410.11160)]
> **Authors**: Xianping Ma,Xiaokang Zhang,Man-On Pun,Bo Huang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: 12 pages, 9 figures
- **标题**: MANET：微调段的任何模型多模式遥感语义分段
- **领域**: 计算机视觉和模式识别
- **摘要**: 从多种传感器收集的多模式遥感数据为地球表面提供了全面而综合的视角。通过采用多模式融合技术，语义分割提供了与单模式方法相比，对地理场景的更多详细见解。基于视觉基础模型的最新进展，尤其是任何模型（SAM）的段，这项研究引入了一种新型的基于多模式适配器的网络（MANET），用于多模式遥感语义分割。这种方法的核心是开发多模式适配器（MMADAPTER），该适配器（MMADAPTER）微调SAM的图像编码器有效利用模型的通用知识来获取多模式数据。此外，还合并了基于金字塔的深层融合模块（DFM），以进一步在解码之前跨多个尺度整合高级地理特征。这项工作不仅引入了一个用于多模式融合的新型网络，而且还首次证明了SAM使用数字表面模型（DSM）数据的强大概括功能。在两个公认的精细分辨率多模式遥感数据集的实验结果，即ISPRS Vaihingen和ISPRS Potsdam，确认所提出的MANET在多模式语义分段的任务中显着超过当前模型。这项工作的源代码将在https://github.com/sstary/ssrs上访问。

### TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models 
[[arxiv](https://arxiv.org/abs/2410.10818)] [[cool](https://papers.cool/arxiv/2410.10818)] [[pdf](https://arxiv.org/pdf/2410.10818)]
> **Authors**: Mu Cai,Reuben Tan,Jianrui Zhang,Bocheng Zou,Kai Zhang,Feng Yao,Fangrui Zhu,Jing Gu,Yiwu Zhong,Yuzhang Shang,Yao Dou,Jaden Park,Jianfeng Gao,Yong Jae Lee,Jianwei Yang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Project Page: https://temporalbench.github.io/
- **标题**: 临时bench：为多模式录像模型的基准测试良好的时间理解
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 了解细粒度的时间动力学对于多模式的视频理解和产生至关重要。由于缺乏细粒度的时间注释，现有的视频基准主要类似于静态图像基准，并且在评估模型的时间理解方面是无能的。在本文中，我们介绍了TemalAlbench，这是一种新的基准测试，致力于评估视频中细粒度的时间理解。临时bench由〜10K视频提问对组成，源自〜2K高质量的人类注释，详细介绍了视频剪辑中的时间动态。结果，我们的基准提供了一个独特的测试床，用于评估各种时间的理解和推理能力，例如动作频率，运动幅度，事件顺序等。此外，它可以评估各种任务，例如视频问题回答和字幕，简短和长时间的视频理解，以及不同的模型，例如多模式视频嵌入式嵌入式模型和文本生成模型。结果表明，诸如GPT-4O之类的最新模型仅在临时bench上实现了38.5％的问题答案准确性，这表明人类和AI之间的显着差距（〜30％）在时间理解中。此外，我们注意到多选质量质量质量质量检查的严重陷阱，其中LLM可以检测负面字幕的细微变化，并找到集中式描述作为其预测的提示，我们提出了多个二进制准确性（MBA）来纠正此类偏见。我们希望临时bench可以促进改善模型的时间推理能力的研究。数据集和评估代码都将提供。

### MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling 
[[arxiv](https://arxiv.org/abs/2410.10798)] [[cool](https://papers.cool/arxiv/2410.10798)] [[pdf](https://arxiv.org/pdf/2410.10798)]
> **Authors**: Jian Yang,Dacheng Yin,Yizhou Zhou,Fengyun Rao,Wei Zhai,Yang Cao,Zheng-Jun Zha
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: MMAR：迈向无损多模式自动回归概率建模
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型的最新进展推动了能够理解和产生的联合概率模型的发展。但是，我们已经确定，由于图像离散化或扩散剥离步骤，最近的方法不可避免地会遭受理解任务过程中图像信息的丢失。为了解决这个问题，我们提出了一种新型的多模式自动回归（MMAR）概率建模框架。与方法的离散线不同，MMAR接收连续值的图像令牌，以避免信息丢失。与基于扩散的方法不同，我们通过在每个自动注册的图像贴片嵌入式上使用轻量重量扩散头来将扩散过程与自动回归骨干模型脱离。这样，当模型从图像生成到通过文本生成理解时，骨干模型对图像的隐藏表示不限于最后一个Denoising步骤。为了成功培训我们的方法，我们还提出了一种理论上验证的技术，该技术解决了数值稳定性问题和培训策略，以平衡一代和理解任务目标。通过对18张图像理解基准的广泛评估，MMAR的性能比其他联合多模式模型更出色，与采用验证的夹具视觉编码器的方法相匹配，同时能够同时生成高质量的图像。我们还表明，我们的方法具有较大的数据和模型大小可扩展。

### CAFuser: Condition-Aware Multimodal Fusion for Robust Semantic Perception of Driving Scenes 
[[arxiv](https://arxiv.org/abs/2410.10791)] [[cool](https://papers.cool/arxiv/2410.10791)] [[pdf](https://arxiv.org/pdf/2410.10791)]
> **Authors**: Tim Broedermann,Christos Sakaridis,Yuqian Fu,Luc Van Gool
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: IEEE Robotics and Automation Letters, The source code is publicly available at: https://github.com/timbroed/CAFuser
- **标题**: Cafuser：条件感知的多模式融合，用于驾驶场景的强大语义感知
- **领域**: 计算机视觉和模式识别
- **摘要**: 利用多个传感器对于自主驾驶中的鲁棒语义感知至关重要，因为每种传感器类型都具有互补的优势和劣势。但是，现有的传感器融合方法通常在所有条件下均匀处理传感器，从而导致次优性能。相比之下，我们提出了一种新颖的，有条件感知的多模式融合方法，用于驾驶场景的强大语义感知。我们的方法Cafuser使用RGB摄像头输入来对环境条件进行分类，并生成一个导致多种传感器方式融合的条件令牌。我们将进一步新地引入了特定于模式的特征适配器，以将各种传感器输入与共享潜在空间相结合，从而有效地与单个和共享的预训练的骨架进行了有效的集成。通过基于实际条件的动态调整传感器融合，我们的模型显着提高了鲁棒性和准确性，尤其是在不利条件方案中。 Cafuser在公众想法的基准中排名第一，用于多模式圆形的59.7 PQ和78.2 MIOU的语义细分，并为交付的新现状设定了新的最新状态。源代码可公开可用：https：//github.com/timbroed/cafuser。

### LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content 
[[arxiv](https://arxiv.org/abs/2410.10783)] [[cool](https://papers.cool/arxiv/2410.10783)] [[pdf](https://arxiv.org/pdf/2410.10783)]
> **Authors**: Nimrod Shabtay,Felipe Maia Polo,Sivan Doveh,Wei Lin,M. Jehanzeb Mirza,Leshem Chosen,Mikhail Yurochkin,Yuekai Sun,Assaf Arbelle,Leonid Karlinsky,Raja Giryes
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: LiveXiv-基于Arxiv论文内容的多模式实时基准测试
- **领域**: 计算机视觉和模式识别
- **摘要**: 从网络上刮除的数据上对多模式模型进行的大规模培训显示出了出色的实用性，可以将这些模型注入所需的世界知识，以在多个下游任务上有效地执行。但是，从网络上刮擦数据的一个缺点可能是经常评估这些模型能力的基准的潜在牺牲。为了保护测试数据污染并真正测试这些基础模型的能力，我们提出了LiveXiv：基于科学ARXIV论文的可扩展实时基准测试。 LiveXiv在任何给定的时间戳上都访问特定于域的手稿，并建议自动生成视觉质疑答案对（VQA）。这是在没有任何人类的任何人类中，使用手稿中的多模式内容（如图形，图表和表格）完成的。此外，我们介绍了一种有效的评估方法，该方法仅使用模型子集的评估来估计所有模型在不断发展的基准测试中的性能。这大大降低了整体评估成本。我们在基准的第一个版本上基准了多个开放和专有的大型多模型模型（LMM），显示了其挑战性的性质并揭示了模型的真实能力，避免了污染。最后，在我们对高质量的承诺中，我们收集并评估了一个手动验证的子集。通过将其整体结果与我们的自动注释进行比较，我们发现性能差异确实很小（<2.5％）。我们的数据集可在HuggingFace上在线提供，我们的代码将在此处提供。

### Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework 
[[arxiv](https://arxiv.org/abs/2410.10663)] [[cool](https://papers.cool/arxiv/2410.10663)] [[pdf](https://arxiv.org/pdf/2410.10663)]
> **Authors**: Zhengwei Yang,Yuke Li,Qiang Sun,Basura Fernando,Heng Huang,Zheng Wang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: 15 pages, 9 figures, 7 tables
- **标题**: 跨模式的几次学习：生成转移学习框架
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 大多数现有关于几次学习的研究都集中在单峰设置上，在该设置中，使用有限的单个模式的标记示例进行了训练以概括为看不见的数据。但是，现实世界的数据本质上是多模式的，这种单峰方法限制了几乎没有学习的实际应用。为了弥合这一差距，本文介绍了交叉模式的几杆学习（CFSL）任务，该任务旨在识别跨多种模式的实例，同时依靠稀缺标记的数据。这项任务与经典的几次学习相比，带来了独特的挑战，这些学习是由每种方式固有的独特视觉属性和结构差异引起的。为了应对这些挑战，我们通过模拟人类如何抽象和推广概念来提出生成转移学习（GTL）框架。具体而言，GTL共同估计了跨模式的潜在共享概念以及通过生成结构的内模扰。在丰富的单峰数据之间建立潜在概念与视觉内容之间的关系，使GTL能够像人类一样有效地将知识从单峰转移到新型的多模式数据。全面的实验表明，GTL在RGB-Sketch，RGB-Infrared和RGB-Depth的七个多模式数据集中实现了最先进的性能。

### BrainMVP: Multi-modal Vision Pre-training for Brain Image Analysis using Multi-parametric MRI 
[[arxiv](https://arxiv.org/abs/2410.10604)] [[cool](https://papers.cool/arxiv/2410.10604)] [[pdf](https://arxiv.org/pdf/2410.10604)]
> **Authors**: Shaohao Rui,Lingzhi Chen,Zhenyu Tang,Lilong Wang,Mianxin Liu,Shaoting Zhang,Xiaosong Wang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: BrainMVP：使用多参数MRI进行大脑图像分析的多模式视觉预训练
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 通过包含互补的多参数MRI成像数据，可以大大提高脑异常的准确诊断。开发通用训练模型具有巨大潜力，该模型可以快速适应图像方式和各种临床方案。但是，当前的模型通常依赖于单模式图像数据，忽略了不同图像模式之间的跨模式相关性，或者在缺少模态数据的情况下努力扩大预训练。在本文中，我们提出了BrainMVP，这是使用多参数MRI扫描进行大脑图像分析的多模式视觉预训练框架。首先，我们收集了16,022次大脑MRI扫描（超过240万张图像），其中包括来自各种中心和设备的八种MRI模式。然后，为多模式MRI数据提出了一种新颖的预训练范式，以解决缺失模式的问题并实现多模式信息融合。探索了跨模式重建，以学习独特的大脑图像嵌入和有效的模态融合功能。提出了模态的数据蒸馏模块，以提取每个MR图像模式的本质表示，以供前训练和下游应用目的。此外，我们引入了一个模态感方的对比学习模块，以增强研究中的跨模式关联。与最先进的医疗领域训练方法相比，在下游任务上进行了广泛的实验表明，在六个分段基准中，骰子得分提高了0.28％-14.47％，在四项单独分类任务中，骰子得分提高了0.28％-14.47％。

### MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks 
[[arxiv](https://arxiv.org/abs/2410.10563)] [[cool](https://papers.cool/arxiv/2410.10563)] [[pdf](https://arxiv.org/pdf/2410.10563)]
> **Authors**: Jiacheng Chen,Tianhao Liang,Sherman Siu,Zhengqing Wang,Kai Wang,Yubo Wang,Yuansheng Ni,Wang Zhu,Ziyan Jiang,Bohan Lyu,Dongfu Jiang,Xuan He,Yuan Liu,Hexiang Hu,Xiang Yue,Wenhu Chen
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Technical report. Project page: https://tiger-ai-lab.github.io/MEGA-Bench/. v2 includes more evaluated models and a single-image setting
- **标题**: 巨型台式：将多模式评估缩放到500多个现实世界任务
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了Mega-Bench，这是一个评估套件，该套件将多模式评估扩展到500多个现实世界任务，以解决最终用户的高度异构日常用例。我们的目标是针对涵盖高度多样且丰富的多模式任务集的一组高质量数据样本进行优化，同时实现具有成本效益和准确的模型评估。特别是，我们收集了505项现实任务，其中包含来自16个专家注释者的8,000多个样本，以广泛覆盖多模式的任务空间。我们没有将这些问题统一为标准的多项选择问题（例如MMMU，MMBENCH和MMT板凳），而是包含多种输出格式，例如数字，短语，代码，\乳胶，坐标，JSON，JOSON，JON，FRIEForm等。为了适应这些格式，我们开发了40个指标来评估这些任务。与现有的基准测试不同，Mega-Bench提供了跨多个维度（例如，应用程序，输入类型，输出格式，技能）的精细颗粒功能报告，使用户可以深入互动并可视化模型功能。我们在大型基础上评估了各种边界视觉模型，以了解它们在这些维度上的能力。

### Hybrid Transformer for Early Alzheimer's Detection: Integration of Handwriting-Based 2D Images and 1D Signal Features 
[[arxiv](https://arxiv.org/abs/2410.10547)] [[cool](https://papers.cool/arxiv/2410.10547)] [[pdf](https://arxiv.org/pdf/2410.10547)]
> **Authors**: Changqing Gong,Huafeng Qin,Mounîm A. El-Yacoubi
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 早期阿尔茨海默氏症检测的混合变压器：基于手写的2D图像和1D信号功能的集成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 阿尔茨海默氏病（AD）是早期检测至关重要的普遍神经退行性疾病。手写通常在AD早期受到影响，提供了一种无创且具有成本效益的方式来捕获微妙的运动变化。关于手写的最先进的研究，主要是基于在线的广告检测，主要取决于手动提取的功能，这是对浅机器学习模型的输入。最近的一些作品提出了深度学习（DL）的模型，即1D-CNN或2D-CNN体系结构，其性能与手工制作的方案有利。但是，这些方法忽略了笔触的2D空间模式与其1D动态特征之间的内在关系，从而限制了它们捕获手写数据的多模式性质的能力。此外，变压器模型的应用基本上仍未开发。为了解决这些局限性，我们提出了一种新颖的AD检测方法，该方法由可学习的多模式杂种注意模型组成，该模型将2D手写图像与一维动态手写信号集成在一起。我们的模型利用封闭的机制结合了相似性和差异的关注，通过在不同尺度上合并信息来融合两种方式和学习鲁棒特征。我们的模型在达尔文数据集上实现了最先进的性能，在任务8（'L'写作）中的F1得分为90.32 \％，精度为90.91 \％，分别超过了4.61％和6.06％的最佳成绩。

### Learning to Ground VLMs without Forgetting 
[[arxiv](https://arxiv.org/abs/2410.10491)] [[cool](https://papers.cool/arxiv/2410.10491)] [[pdf](https://arxiv.org/pdf/2410.10491)]
> **Authors**: Aritra Bhowmik,Mohammad Mahdi Derakhshani,Dennis Koelma,Martin R. Oswald,Yuki M. Asano,Cees G. M. Snoek
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 在不忘记的情况下学习接地VLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 空间意识是启用具体的多模式AI系统的关键。但是，如果没有大量的空间监督，当前的视觉语言模型（VLM）在此任务上挣扎。在本文中，我们介绍了Lynx，这是一个框架，该框架为VLM提供了视觉接地能力，而无需忘记其现有的图像和语言理解技能。为此，我们提出了专家模块的双重混合物，该模块仅修改语言模型的解码器层，其中使用了一种对图像和语言理解的专家（MOE）的冷冻混合物（MOE），并为新的接地功能提供了另一个可学习的MOE。这使VLM可以在获取缺失的东西的同时保留先前学习的知识和技能。为了有效地训练模型，我们生成了一个称为Scout的高质量合成数据集，该数据集在视觉接地中模仿人类的推理。该数据集提供了丰富的监督信号，描述了一个逐步的多模式推理过程，从而简化了视觉接地的任务。我们在几个对象检测和视觉接地数据集上评估lynx，在对象检测中表现出强烈的性能，零弹性定位和接地推理，同时保持其原始图像和语言理解能力在七个标准基准数据集中。

### Free Video-LLM: Prompt-guided Visual Perception for Efficient Training-free Video LLMs 
[[arxiv](https://arxiv.org/abs/2410.10441)] [[cool](https://papers.cool/arxiv/2410.10441)] [[pdf](https://arxiv.org/pdf/2410.10441)]
> **Authors**: Kai Han,Jianyuan Guo,Yehui Tang,Wei He,Enhua Wu,Yunhe Wang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Tech report
- **标题**: 免费视频llm：及时引导的视觉感知，用于有效的无培训视频LLMS
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉语言大型模型在各种多模式任务中取得了巨大的成功，但是由于视频数据的固有复杂性和计算需求，将它们应用于视频理解仍然具有挑战性。尽管基于培训的视频LLM提供高性能，但他们通常需要大量资源进行培训和推理。相反，无训练方法通过在没有额外培训的情况下调整预先训练的图像-LLMS模型来提供更有效的替代方法，但是由于视频框架产生了大量的视觉令牌，它们会面临推理效率瓶颈。在这项工作中，我们提出了一个新颖的迅速引入的视觉感知框架（作为免费视频-LLM的缩写），以有效推断无训练的视频LLM。提出的框架分离了时空维度，并根据特定于任务的提示分别执行时间框架采样和空间ROI裁剪。我们的方法有效地减少了视觉令牌的数量，同时在多个视频提问基准的基准中保持高性能。广泛的实验表明，我们的方法以较少的代币获得了竞争成果，与最先进的视频LLM相比，准确性和计算效率之间的最佳权衡。该代码将在https://github.com/contrastive/freevideollm上找到。

### Class Balancing Diversity Multimodal Ensemble for Alzheimer's Disease Diagnosis and Early Detection 
[[arxiv](https://arxiv.org/abs/2410.10374)] [[cool](https://papers.cool/arxiv/2410.10374)] [[pdf](https://arxiv.org/pdf/2410.10374)]
> **Authors**: Arianna Francesconi,Lazzaro di Biase,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Rosa Sicilia,Valerio Guarrasi
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 平衡多样性多模式合奏，用于阿尔茨海默氏病诊断和早期检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 阿尔茨海默氏病（AD）由于越来越多的患病率和相关的社会成本而构成了重大的全球健康挑战。 AD的早期检测和诊断对于延迟进展和改善患者预后至关重要。传统的诊断方法和单模式数据通常在识别早期AD并将其与轻度认知障碍（MCI）区分开时差不多。这项研究通过引入一种新颖的方法来解决这些挑战：通过阶级平衡多样性的多模式合奏（不平衡数据）。失调整合了来自阿尔茨海默氏病神经成像计划数据库中的多模式数据，包括临床评估，神经影像型表型，生物测定和主题特征数据。它采用了模型分类器的合奏，每个分类器都经过不同的类平衡技术训练，以克服阶级失衡并增强模型精度。我们对两个诊断任务（二进制和三元分类）和四个二进制早期检测任务（在12、24、36和48个月）进行了评估，将其与最新算法和不平衡数据集方法进行了比较。在二进制和三元分类任务中表现出较高的诊断准确性和预测性能，从而显着改善了48个月时间点MCI的早期检测。该方法显示出改善的分类性能和鲁棒性，为AD的早期检测和管理提供了有希望的解决方案。

### Spatial-Aware Efficient Projector for MLLMs via Multi-Layer Feature Aggregation 
[[arxiv](https://arxiv.org/abs/2410.10319)] [[cool](https://papers.cool/arxiv/2410.10319)] [[pdf](https://arxiv.org/pdf/2410.10319)]
> **Authors**: Shun Qian,Bingquan Liu,Chengjie Sun,Zhen Xu,Baoxun Wang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: 10 pages, 3 figures
- **标题**: 通过多层特征聚合，MLLM的空间感知高效投影仪
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 投影仪在多模式语言模型（MLLM）中起着至关重要的作用。 IT输出的视觉令牌的数量会影响MLLM的效率，而视觉令牌的质量会影响MLLM的视觉理解能力。当前对投影仪的探索着重于减少视觉令牌的数量以提高效率，通常忽略了串行的二维视觉令牌序列与自然语言令牌序列之间固有的空间差异。提出了一个空间感知的高效投影仪（SAEP）来解决此问题。详细说明，我们的SAEP方法在多层视觉特征上采用了可分开的深度卷积模块，以增强视觉令牌的空间信息。结果，我们的SAEP方法不仅可以很大程度上将视觉令牌的数量减少75 \％，而且还可以显着提高MLLM的多模式空间理解能力。此外，与现有投影仪相比，我们的SAEP在大规模的多模式评估基准上获得了最佳性能，这表示其在弥合模式差距上的有效性。

### Saliency Guided Optimization of Diffusion Latents 
[[arxiv](https://arxiv.org/abs/2410.10257)] [[cool](https://papers.cool/arxiv/2410.10257)] [[pdf](https://arxiv.org/pdf/2410.10257)]
> **Authors**: Xiwen Wang,Jizhe Zhou,Xuekang Zhu,Cheng Li,Mao Li
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 扩散潜伏潜望潜伏期的显着性优化
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着扩散模型的快速发展，从文本提示中产生体面的图像不再具有挑战性。文本到图像生成的关键是如何优化文本对图像生成模型的结果，以便它们可以更好地与人类意图或提示保持一致。现有的优化方法通常统一地对待整个图像并进行全局优化。这些方法忽略了一个事实，即在查看图像时，人类视觉系统自然会优先考虑向显着区域的关注，通常会忽略较少或非偏见的区域。也就是说，人类可能会忽略非偏好地区的优化。因此，尽管保留模型是在其他大型和多模型模型的指导下进行的，但现有的方法（进行均匀的优化）产生了亚最佳结果。为了有效，有效地应对这种一致性挑战，我们提出了扩散潜力（Sgool）的显着性优化。我们首先采用显着探测器来模仿人类的视觉关注系统并标出显着区域。为了避免再培训其他模型，我们的方法直接优化了扩散潜在的潜力潜在。此外，Sgool还利用了可逆扩散过程，并以恒定内存实现的优点赋予了它。因此，我们的方法成为一种参数效率和插件的微调方法。通过几个指标和人类评估进行了广泛的实验。实验结果证明了Sgool在图像质量和迅速比对方面的优越性。

### ForgeryGPT: Multimodal Large Language Model For Explainable Image Forgery Detection and Localization 
[[arxiv](https://arxiv.org/abs/2410.10238)] [[cool](https://papers.cool/arxiv/2410.10238)] [[pdf](https://arxiv.org/pdf/2410.10238)]
> **Authors**: Jiawei Liu,Fanrui Zhang,Jiaying Zhu,Esther Sun,Qiang Zhang,Zheng-Jun Zha
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: 16 pages, 14 figures
- **标题**: ForeryGPT：可解释图像伪造检测和本地化的多模式大语言模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式的大型语言模型（MLLM），例如GPT4O，在视觉推理和解释生成中表现出强大的功能。然而，尽管有这些优势，但它们在图像伪造检测和本地化的日益关键任务（IFDL）中面临重大挑战。此外，现有的IFDL方法通常仅限于学习低级语义不可能的线索，而仅提供单一的结果判断。为了解决这些问题，我们提出了ForgeryGPT，这是一个新颖的框架，通过捕获来自不同语言特征空间的伪造图像的高级取证知识相关性来推进IFDL任务，同时通过新定制的大型语言模型（LLM）建筑启用了可解释的生成和交互式对话。具体而言，伪造方法通过整合面罩感知的伪造物来增强传统的LLM，从而可以从输入图像中发掘精确的伪造蒙版信息，并促进对篡改文物的像素级别的了解。面具感知的伪造提取器由伪造的定位专家（FL-Expert）和面具编码器组成，其中FL-Expert通过对象敏捷的伪造及时和词汇增强的视觉编码器进行了增强，可有效捕获多刻度尺度的精细伪造案例的详细信息。为了提高其性能，我们实施了三阶段的培训策略，并得到了我们设计的掩模文本对齐和特定于任务的指令调谐数据集的支持，该数据集使视觉语言模式相结合并改善了伪造的检测和指导遵循的功能。广泛的实验证明了该方法的有效性。

### KNN Transformer with Pyramid Prompts for Few-Shot Learning 
[[arxiv](https://arxiv.org/abs/2410.10227)] [[cool](https://papers.cool/arxiv/2410.10227)] [[pdf](https://arxiv.org/pdf/2410.10227)]
> **Authors**: Wenhao Li,Qiangchang Wang,Peng Zhao,Yilong Yin
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: 10 pages, 5 figures, accepted by ACM Multimedia 2024
- **标题**: 带有金字塔的KNN变压器提示进行几次学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 很少有射击学习（FSL）旨在识别具有有限标记数据的新课程。最近的研究试图通过文本提示来调查稀有样本的挑战，以调节视觉特征。但是，他们通常很难捕获文本和视觉特征之间复杂的语义关系。此外，由于图像中的无用信息，香草自我注意力受到了很大的影响，严重限制了FSL语义先验的潜力，这是由于相互作用过程中许多无关的代币的混淆。为了解决这些上述问题，提出了带有金字塔提示（KTPP）的K-NN变压器，以选择具有K-NN上下文注意力（KCA）（KCA）的歧视性信息（KCA），并使用金字塔交叉模式提示（PCP）自适应调节视觉特征。首先，对于每个令牌，KCA仅选择k个最相关的令牌来计算自我发挥作用矩阵，并将所有令牌的平均值作为上下文提示，以在三个级联阶段提供全局上下文。结果，可以逐渐抑制无关的令牌。其次，在PCP中引入了金字塔提示，以通过基于文本的类吸引提示和多尺度视觉特征之间的交互来强调视觉特征。这使得VIT可以根据不同尺度上的丰富语义信息动态调整视觉特征的重要性权重，从而使模型可与空间变化进行健全。最后，通过KCA进行了增强的视觉功能和班级感知提示，以提取特定于类的功能。因此，我们的模型进一步通过深层交叉模式相互作用增强了无噪声的视觉表示，在很少有标记的样品的情况下提取了广义的视觉表示。四个基准数据集的广泛实验证明了我们方法的有效性。

### Eliminating the Language Bias for Visual Question Answering with fine-grained Causal Intervention 
[[arxiv](https://arxiv.org/abs/2410.10184)] [[cool](https://papers.cool/arxiv/2410.10184)] [[pdf](https://arxiv.org/pdf/2410.10184)]
> **Authors**: Ying Liu,Ge Bai,Chenji Lu,Shilong Li,Zhang Zhang,Ruifang Liu,Wenbin Guo
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: ef:2024 IEEE International Conference on Multimedia and Expo (ICME), Niagara Falls, ON, Canada, 2024, pp. 1-6
- **标题**: 消除通过细粒度因果干预回答视觉问题的语言偏见
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 尽管视觉问题回答（VQA）取得了显着进步，但减轻文本信息引入的语言偏见的挑战仍未解决。以前的方法从粗粒的角度捕获语言偏见。但是，句子中的细粒度信息（例如上下文和关键字）可能会导致不同的偏见。由于对细粒度信息的无知，大多数现有方法无法充分捕获语言偏见。在本文中，我们提出了一种新颖的因果干预培训计划，名为CIBI，以从更细粒度的角度消除语言偏见。具体而言，我们将语言偏见分为上下文偏见和关键字偏差。我们采用因果干预和对比度学习来消除上下文偏见并改善多模式表示。此外，我们设计了一个基于反事实生成的新的问题分支，以提炼和消除关键字偏差。实验结果表明，CIBI适用于各种VQA模型，产生了竞争性能。

### X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing 
[[arxiv](https://arxiv.org/abs/2410.10167)] [[cool](https://papers.cool/arxiv/2410.10167)] [[pdf](https://arxiv.org/pdf/2410.10167)]
> **Authors**: Xinyan Chen,Jianfei Yang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: X-FI：多模式人类感测的模态不变的基础模型
- **领域**: 计算机视觉和模式识别,信号处理
- **摘要**: 人类传感采用各种传感器和先进的深度学习技术来准确捕获和解释人体信息，它影响了公共安全和机器人技术等领域。然而，当前的人类感应主要取决于摄像机和激光雷达等模态，每种都有其自身的优势和局限性。此外，现有的多模式融合解决方案通常是针对固定模态组合设计的，需要在添加或删除各种情况时进行大量重新训练。在本文中，我们为所有模式，X-Fi提出了一个模式不变的基础模型，以解决此问题。 X-FI通过利用变压器结构可容纳变量输入尺寸，并在多模态集成过程中使用新颖的“ X融合”机制来容纳变量输入大小，从而实现传感器方式的独立或组合使用，而无需进行其他训练。这种方法不仅增强了适应性，而且还促进了跨模式的互补特征的学习。采用六种不同模式的MM-FI和XRF55数据集进行的广泛实验表明，X-Fi在人类姿势估计（HPE）和人类活动识别（HAR）任务中实现了最先进的表现。研究结果表明，我们提出的模型可以有效地支持广泛的人类传感应用，最终有助于可扩展的多峰传感技术的演变。

### MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.10139)] [[cool](https://papers.cool/arxiv/2410.10139)] [[pdf](https://arxiv.org/pdf/2410.10139)]
> **Authors**: Peng Xia,Siwei Han,Shi Qiu,Yiyang Zhou,Zhaoyang Wang,Wenhao Zheng,Zhaorun Chen,Chenhang Cui,Mingyu Ding,Linjie Li,Lijuan Wang,Huaxiu Yao
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: MMIE：大型视觉模型的大量多模式交织的理解基准
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 交错的多模式理解和生成，使模型能够以任意序列产生和解释图像和文本，已成为多模式学习的关键领域。尽管取得了重大进步，但对此能力的评估仍然不足。现有基准受到数据量表，范围和评估深度的局限性，而当前的评估指标通常是昂贵或有偏见的，因此缺乏对实际应用的可靠性。为了应对这些挑战，我们介绍了MMIE，这是一种大规模的知识密集型基准，用于评估大型视觉模型（LVLMS）中交织的多模式理解和产生。 MMIE包括20K精心策划的多模式查询，涵盖3个类别，12个字段和102个子场，包括数学，编码，物理，文学，健康，健康和艺术。它支持交错的输入和产出，提供多项选择和开放式问题格式的混合，以评估各种能力。此外，我们提出了一个可靠的自动化评估度量标准，该指标利用人类注销数据和系统评估标准进行了微调的评分模型，旨在降低偏见和提高评估准确性。广泛的实验证明了我们的基准和指标在对交织LVLM的全面评估方面的有效性。具体来说，我们评估了八个LVLM，表明即使是最佳模型也显示出很大的改进空间，大多数人只能取得适度的结果。我们认为，MMIE将推动交错LVLM的发展进步。我们在https://mmie-bench.github.io/中公开发布基准和代码。

### ROSAR: An Adversarial Re-Training Framework for Robust Side-Scan Sonar Object Detection 
[[arxiv](https://arxiv.org/abs/2410.10554)] [[cool](https://papers.cool/arxiv/2410.10554)] [[pdf](https://arxiv.org/pdf/2410.10554)]
> **Authors**: Martin Aubard,László Antal,Ana Madureira,Luis F. Teixeira,Erika Ábrahám
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 罗莎（Rosar）：可靠的侧扫声纳对象检测的对抗性重新训练框架
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术
- **摘要**: 本文介绍了Rosar，这是一个新颖的框架，可增强针对侧扫声纳（SSS）图像的深度学习对象检测模型的鲁棒性，该模型由使用声纳传感器由自主水下车辆生成。通过扩展我们先前的知识蒸馏（KD）工作，该框架将KD与对抗性重新培训集成在一起，以应对模型效率和对SSS噪声的鲁棒性的双重挑战。我们介绍了三个新颖的公开SSS数据集，以捕获不同的声纳设置和噪声条件。我们建议并形式化两个SSS安全性能，并利用它们生成对抗数据集进行重新训练。通过对预计梯度下降（PGD）和基于斑块的对抗攻击的比较分析，Rosar在SSS特异性条件下显示出模型鲁棒性和检测准确性的显着提高，从而提高了模型的鲁棒性高达1.85％。 Rosar可从https://github.com/remaro-network/rosar-framework获得。

### Sparse Prototype Network for Explainable Pedestrian Behavior Prediction 
[[arxiv](https://arxiv.org/abs/2410.12195)] [[cool](https://papers.cool/arxiv/2410.12195)] [[pdf](https://arxiv.org/pdf/2410.12195)]
> **Authors**: Yan Feng,Alexander Carballo,Kazuya Takeda
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 可解释的行人行为预测的稀疏原型网络
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 预测行人行为对于诸如自动驾驶和智能城市之类的应用至关重要，但至关重要。最近的深度学习模型在做出准确的预测方面取得了出色的性能，但他们无法提供内部运作的解释。此问题的原因之一是多模式输入。为了弥合这一差距，我们提出了稀疏的原型网络（SPN），这是一种可解释的方法，旨在同时预测行人的未来动作，轨迹和姿势。 SPN利用中间原型瓶颈层为其预测提供了基于样本的解释。原型是与模态无关的，这意味着它们可以与输入中的任何模态相对应。因此，SPN可以扩展到任意形式的组合。这些原型通过单声道和聚类约束，学习一致，人为理解的特征，并在动作，轨迹和姿势预测的泰坦和派对上实现最先进的表现。最后，我们提出了一个名为TOP-K单声道量表的度量标准，以定量评估解释性。定性结果表明稀疏性与解释性之间的正相关。可在https://github.com/equinoxxxxx/spn上找到代码。

### TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration 
[[arxiv](https://arxiv.org/abs/2410.12183)] [[cool](https://papers.cool/arxiv/2410.12183)] [[pdf](https://arxiv.org/pdf/2410.12183)]
> **Authors**: Yiwei Guo,Shaobin Zhuang,Kunchang Li,Yu Qiao,Yali Wang
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: NeurIPS 2024
- **标题**: Transagent：具有异质代理协作的转移视觉基础基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言基础模型（例如剪辑）最近显示了由于大规模的图像文本预训练而在传递学习方面的力量。但是，下游任务中的目标域数据可能与训练前阶段高度不同，这使得如此单一的模型很难很好地概括。另外，存在着广泛的专家模型，这些模型包含在不同的模式，任务，网络和数据集上预先训练的多元化视觉和/或语言知识。不幸的是，这些模型是具有异质结构的“孤立代理”，以及如何整合其知识以概括类似夹子的模型。为了弥合这一差距，我们提出了一个通用和简洁的跨性别框架，该框架以统一的方式传输了孤立的药物的知识，并有效地指导夹子以多源知识蒸馏进行概括。通过如此独特的框架，我们与11种异构代理一起灵活地合作，以增强视觉基础模型的能力，而在推理阶段没有进一步的成本。最后，我们的跨性别在11个视觉识别数据集上实现了最先进的性能。在相同的低射击设置下，它的表现平均超过了受欢迎的鸡舍，而欧洲股票的占20％，其中包含大型域名。

### Unveiling the Limits of Alignment: Multi-modal Dynamic Local Fusion Network and A Benchmark for Unaligned RGBT Video Object Detection 
[[arxiv](https://arxiv.org/abs/2410.12143)] [[cool](https://papers.cool/arxiv/2410.12143)] [[pdf](https://arxiv.org/pdf/2410.12143)]
> **Authors**: Qishun Wang,Zhengzheng Tu,Kunpeng Wang,Le Gu,Chuanwang Guo
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 揭示对齐方式的限制：多模式动态本地融合网络和不规学的RGBT视频对象检测的基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的RGB热视频对象检测（RGBT VOD）方法仍然取决于在图像级别手动对齐数据，这在实际世界情景中阻碍了其实际应用，因为多光谱传感器捕获的图像对在两个视野和分辨率上通常都有所不同。为了解决这一限制，我们提出了一个多模式动态局部融合网络（MDLNET），旨在处理未对准的RGBT图像对。具体而言，我们提出的多模式动态局部融合（MDLF）模块包括一组预定义的框，每个框都用随机的高斯噪声增强，以生成动态框。每个盒子从原始的高分辨率RGB图像中选择一个局部区域。然后将该区域与来自另一种模式的相应信息融合并重新插入RGB。该方法通过与不同范围的本地特征进行交互来适应各种数据对齐方案。同时，我们在端到端的体系结构中引入了一个级联的时间扰动器（CTS）。该模块利用连续帧的一致时空信息，以增强当前帧的表示能力，同时保持网络效率。我们已经为未对准的RGBT VOD策划了一个名为UVT-VOD2024的打开数据集。它由直接从多光谱摄像机捕获的30,494对未对齐的RGBT图像组成。我们与MDLNET和最先进的模型进行了全面的评估和比较，证明了MDLNET的出色有效性。我们将向公众发布代码和UVT-VOD2024，以进行进一步研究。

### CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning 
[[arxiv](https://arxiv.org/abs/2410.11963)] [[cool](https://papers.cool/arxiv/2410.11963)] [[pdf](https://arxiv.org/pdf/2410.11963)]
> **Authors**: Qingqing Cao,Mahyar Najibi,Sachin Mehta
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: CTRLSYNTH：可控的图像文本综合，用于数据有效的多模式学习
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 预处理稳健的视力或多模式基础模型（例如，剪辑）依赖于可能嘈杂，可能未对准并且具有长尾分布的大规模数据集。以前的工作通过生成合成样本来增强数据集的有希望的结果。但是，它们仅支持域特异性的临时用例（例如，仅图像或文本，但不两者），并且由于缺乏对合成过程的细粒度控制而受到数据多样性的限制。在本文中，我们设计了一个\ emph {可控}图像 - 文本合成管道，ctrlsynth，用于数据效率和稳健的多模式学习。关键想法是将图像的视觉语义分解为基本元素，应用用户指定的控制策略（例如，删除，添加或替换操作），然后重新组合它们以合成图像或文本。 CTRLSYNTH中的分解和重新组合功能使用户可以通过定义自定义的控制策略来操纵基本元素，以细粒度的方式控制数据综合。 CTRLSYTH利用了验证的基础模型（例如大语言模型或扩散模型）的能力来推理和重新组建基本元素，以便合成样本是自然的，并且以多种方式组成。 Ctrlsynth是一个闭环，无训练和模块化的框架，使得支持不同的预验证模型变得容易。通过在31个数据集上进行的大量实验，涵盖了不同的视觉和视觉语言任务，我们表明CTRLSYNTH可改善剪辑模型的零弹药分类，图像文本检索和组成推理性能。

### MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding 
[[arxiv](https://arxiv.org/abs/2410.11829)] [[cool](https://papers.cool/arxiv/2410.11829)] [[pdf](https://arxiv.org/pdf/2410.11829)]
> **Authors**: Yue Cao,Yangzhou Liu,Zhe Chen,Guangchen Shi,Wenhai Wang,Danhuai Zhao,Tong Lu
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: 11 pages, 6 figures, technical report
- **标题**: MMFUSER：多模式的多层特征定影器，可用于细粒度的视觉理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管多模式大语言模型（MLLM）在通过跨模式相互作用理解复杂的人类意图方面取得了重大进步，但捕获复杂的图像细节仍然具有挑战性。整合多个视觉编码器以增强视觉细节的先前方法引入了冗余和计算开销。我们观察到，大多数MLLM仅利用视觉编码器的最后层特征图来视觉表示，从而忽略了浅色特征图中丰富的细粒信息。为了解决这个问题，我们提出了\ modelName，这是一种简单而有效的多层功能固定器，可有效整合视觉变压器（VIT）的深层和浅色特征。具体而言，它利用语义对齐的深度特征作为查询来动态从浅层特征中提取缺失的细节，从而保留语义对齐，同时用细粒度的信息丰富表示表示。应用于LLAVA-1.5模型，\ modelName〜与多模型集合方法相比，可提供视觉表示和基准性能的显着改进，提供了更灵活，更轻巧的解决方案。代码和模型已在https://github.com/yuecao0119/mmfuser上发布。

### Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices 
[[arxiv](https://arxiv.org/abs/2410.11795)] [[cool](https://papers.cool/arxiv/2410.11795)] [[pdf](https://arxiv.org/pdf/2410.11795)]
> **Authors**: Zhiyuan Ma,Yuzhu Zhang,Guoli Jia,Liangliang Zhao,Yichao Ma,Mingjie Ma,Gaofeng Liu,Kaiyan Zhang,Jianjun Li,Bowen Zhou
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: :I.4.9
- **标题**: 有效的扩散模型：从原则到实践的全面调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 作为近年来最受欢迎和最受欢迎的生成模型之一，扩散模型激发了许多研究人员的兴趣，并稳步地显示出在各种生成任务中的优势，例如图像合成，视频产生，分子设计，3D场景渲染和多模态生成，依靠其密集的理论原理和可靠的应用程序实践。这些最近在扩散模型的努力取得了显着的成功，主要来自渐进的设计原理和有效的体系结构，培训，推理和部署方法。但是，没有进行全面和深入的审查来总结这些原则和实践，以帮助快速理解和应用扩散模型。在这项调查中，我们对这些现有努力提供了一种新的面向效率的观点，该观点主要集中在建筑设计，模型培训，快速推理和可靠的部署方面的深刻原则和有效实践，以指导进一步的理论研究，算法迁移和模型以一种读者友善的方式进行新的场景。 \ url {https://github.com/ponyzym/efficited-dms-survey}

### SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding 
[[arxiv](https://arxiv.org/abs/2410.11761)] [[cool](https://papers.cool/arxiv/2410.11761)] [[pdf](https://arxiv.org/pdf/2410.11761)]
> **Authors**: Ying Chen,Guoan Wang,Yuanfeng Ji,Yanjun Li,Jin Ye,Tianbin Li,Bin Zhang,Nana Pei,Rongshan Yu,Yu Qiao,Junjun He
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: SlideChat：全视觉助手全扫描病理学图像理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 尽管多模式大语言模型（MLLM）在计算病理学中取得了进展，但它们仍然受到对贴片级分析的主要关注，在整个级别层面上缺少基本的上下文信息。缺乏大规模的指导数据集和整个幻灯片图像（WSIS）的Gigapixel量表构成了重大的发展挑战。在本文中，我们提出了Slidechat，这是能够理解Gigapixel全斜面图像的第一个视觉语言助手，展现了各种病理方案的出色多模式对话能力和反应复杂的教学。为了支持其开发，我们创建了SlideSinstruction，这是由4.2K WSI字幕和176K VQA对组成的WSIS最大的指令数据集，具有多个类别。此外，我们提出了SlideBench，这是一种多模式基准，该基准包含字幕和VQA任务，以评估SlideChat在各种临床环境中的功能，例如显微镜，诊断。与一般和专业的MLLM相比，Slidechat在22个任务中的18个任务中表现出了出色的功能，可实现最先进的表现。例如，它在SlideBench-VQA（TCGA）上实现了81.17％的总准确度，而SlideBench-VQA（BCNB）的总准确度为54.15％。我们将完全释放SlideChat，SlideStruction和SlideBench作为开源资源，以促进计算病理学的研发。

### RS-MOCO: A deep learning-based topology-preserving image registration method for cardiac T1 mapping 
[[arxiv](https://arxiv.org/abs/2410.11651)] [[cool](https://papers.cool/arxiv/2410.11651)] [[pdf](https://arxiv.org/pdf/2410.11651)]
> **Authors**: Chiyi Huang,Longwei Sun,Dong Liang,Haifeng Liang,Hongwu Zeng,Yanjie Zhu
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: RS-MOCO：一种基于深度学习的拓扑图像注册方法，用于心脏T1映射
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 心脏T1映射可以评估心肌组织的各种临床症状。但是，目前缺乏在心脏T1映射中进行运动校正的有效，健壮和有效的方法。在本文中，我们提出了一个基于深度学习和拓扑的图像注册框架，用于心脏T1映射中的运动校正。值得注意的是，我们提出的隐式一致性约束在某种程度上通过双向一致性约束和局部反折叠约束来保存图像拓扑。为了解决对比变化问题，我们引入了一个加权图像相似性度量，以用于心脏T1加权图像的多模式注册。此外，将半监督的心肌分割网络和双域注意模块集成到框架中，以进一步提高注册的性能。许多比较实验以及消融研究都证明了我们方法的有效性和高鲁棒性。结果还表明，建议的加权图像相似性度量，专门为我们的网络制作，为增强运动校正疗效做出了很大的贡献，而双向一致性约束结合了局部反折叠约束，可确保更可取的拓扑拓扑预测注册映射。

### Efficient and Effective Universal Adversarial Attack against Vision-Language Pre-training Models 
[[arxiv](https://arxiv.org/abs/2410.11639)] [[cool](https://papers.cool/arxiv/2410.11639)] [[pdf](https://arxiv.org/pdf/2410.11639)]
> **Authors**: Fan Yang,Yihao Huang,Kailong Wang,Ling Shi,Geguang Pu,Yang Liu,Haoyu Wang
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: 11 pages
- **标题**: 有效有效的通用对抗性攻击对视力语言预训练模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过大规模图像文本对训练的视觉语言预训练（VLP）模型已被广泛用于各种下游视觉和语言（V+L）任务。这种广泛的采用引起了人们对它们对对抗攻击的脆弱性的担忧。非普遍的对抗攻击虽然有效，但由于每个数据实例的计算需求很高，因此对于实时在线应用程序而言，通常是不切实际的。最近，已引入了通用对抗扰动（UAP）作为解决方案，但是现有的基于发生器的UAP方法显着耗时。为了克服限制，我们提出了一种直接基于优化的UAP方法，称为DO-UAP，该方法可大大降低资源消耗，同时保持高攻击性能。具体而言，我们探讨了多模式损失设计的必要性，并引入了有用的数据增强策略。在三个基准VLP数据集，六个流行的VLP模型和三个经典下游任务上进行的广泛实验证明了DO-UAP的效率和有效性。具体而言，我们的方法大幅度地将时间消耗降低了23倍，同时实现了更好的攻击性能。

### VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI 
[[arxiv](https://arxiv.org/abs/2410.11623)] [[cool](https://papers.cool/arxiv/2410.11623)] [[pdf](https://arxiv.org/pdf/2410.11623)]
> **Authors**: Sijie Cheng,Kechen Fang,Yangyang Yu,Sicheng Zhou,Bohao Li,Ye Tian,Tingguang Li,Lei Han,Yang Liu
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: videgothink：评估以体现AI的以自我为中心的视频理解功能
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 多模式大语言模型（MLLM）的最新进展为体现AI的应用开辟了新的途径。我们介绍了以前的工作，我们介绍了Videgothink，这是一种评估以自我为中心的视频理解能力的综合基准。为了弥合MLLM与体现AI中低级控制之间的差距，我们设计了四个关键的相互关联任务：视频提问，层次结构计划，视觉接地和奖励建模。为了最大程度地减少手动注释成本，我们根据EGO4D数据集开发了自动数据生成管道，利用GPT-4O的先验知识和多模式功能。然后，三个人类注释者过滤了生成的数据，以确保多样性和质量，从而获得了Videgothink基准。我们使用三种类型的模型进行了广泛的实验：基于API的MLLM，开源基于图像的MLLM和基于开源视频的MLLM。实验结果表明，包括GPT-4O在内的所有MLLM在与以自我为中心的视频理解相关的所有任务中的表现较差。这些发现表明，基础模型仍然需要有效地应用于体现AI的第一人称场景。总之，Videgothink反映了采用MLLM的以自我为中心视力的研究趋势，类似于人类的能力，从而在复杂的现实世界环境中实现了积极的观察和相互作用。

### MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval 
[[arxiv](https://arxiv.org/abs/2410.11619)] [[cool](https://papers.cool/arxiv/2410.11619)] [[pdf](https://arxiv.org/pdf/2410.11619)]
> **Authors**: Reno Kriz,Kate Sanders,David Etter,Kenton Murray,Cameron Carpenter,Kelly Van Ochten,Hannah Recknor,Jimena Guallar-Blasco,Alexander Martin,Ronald Colaianni,Nolan King,Eugene Yang,Benjamin Van Durme
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 多人2.0：以事件为中心的视频检索的巨大多语言基准
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 从大规模多模式收集中有效检索和合成信息已成为一个关键挑战。但是，现有的视频检索数据集遭受了范围限制，主要集中于匹配描述性但模糊的查询，其中包含少量以专业编辑的英语视频收集。为了解决此差距，我们介绍了$ \ textbf {Multivent 2.0} $，这是一个大规模的，以事件为中心的大规模，以事件为中心的视频检索基准，其中包含218,000多个针对特定世界活动的新闻视频和3,906个查询。这些查询专门针对视觉内容，视听，嵌入式文本和视频的文本元数据中发现的信息，要求系统利用所有这些来源在任务中取得成功。初步结果表明，最先进的视觉语言模型在此任务上大为挣扎，尽管替代方法表现出希望，但它们仍然不足以充分解决这个问题。这些发现强调了需要更强大的多式模式检索系统的需求，因为有效的视频检索是迈向多模式内容理解和产生的关键一步。

### Breaking Modality Gap in RGBT Tracking: Coupled Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2410.11586)] [[cool](https://papers.cool/arxiv/2410.11586)] [[pdf](https://arxiv.org/pdf/2410.11586)]
> **Authors**: Andong Lu,Jiacong Zhao,Chenglong Li,Yun Xiao,Bin Luo
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: Accepted by ACM MM2024
- **标题**: RGBT跟踪中的破坏方式差距：耦合知识蒸馏
- **领域**: 计算机视觉和模式识别
- **摘要**: RGB和热红外（TIR）图像之间的模态差距是一个至关重要的问题，但在现有的RGBT跟踪方法中经常被忽略。可以观察到模态差距主要在于图像样式差异。在这项工作中，我们提出了一个名为CKD的新型耦合知识蒸馏框架，该框架追求不同方式的常见方式以打破模态差距，以进行高性能RGBT跟踪。特别是，我们介绍了两个学生网络并采用样式蒸馏损失，以使其风格功能尽可能一致。通过减轻两个学生网络的样式差异，我们可以很好地打破不同方式的模态差距。但是，风格功能的蒸馏可能会损害学生网络中两种方式的内容表示。为了解决这个问题，我们将原始的RGB和TIR网络作为教师，并通过样式 - 符合正交功能解耦方案分别将其内容知识提炼为两个学生网络。我们将以上两个蒸馏过程融入在线优化框架中，以形成RGB和热模式的新功能表示，而无需模态差距。此外，我们设计了一个蒙版的建模策略和多模式候选令牌消除策略，以分别提高跟踪鲁棒性和效率。在五个标准RGBT跟踪数据集上进行的广泛实验验证了所提出的方法针对最新方法的有效性，同时达到了最快的跟踪速度96.4 fps。代码可在https://github.com/multi-modality-tracking/ckd上找到。

### On-the-fly Modulation for Balanced Multimodal Learning 
[[arxiv](https://arxiv.org/abs/2410.11582)] [[cool](https://papers.cool/arxiv/2410.11582)] [[pdf](https://arxiv.org/pdf/2410.11582)]
> **Authors**: Yake Wei,Di Hu,Henghui Du,Ji-Rong Wen
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: Accepted by T-PAMI 2024
- **标题**: 平衡多模式学习的自然调制
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体
- **摘要**: 预计多模式学习将通过整合来自不同模式的信息来提高模型性能。但是，它的潜力并没有得到充分利用，因为广泛使用的联合训练策略（对所有方式都具有统一的目标）导致不平衡和优化的Uni-Modal表示形式。具体来说，我们指出，通常会有更多的歧视信息，例如，踢足球和吹风的声音的愿景。他们可以主导联合训练过程，从而导致其他方式显着不优化。为了减轻这个问题，我们首先分析了优化过程中馈送前及以后阶段的不高度现象。然后，提出了即时预测调制（OPM）和即时梯度调制（OGM）策略，以通过监视训练过程中模态之间的歧视性差异来调节每种模式的优化。具体而言，OPM通过在进料阶段以动力学概率降低其特征来削弱主体模态的影响，而OGM在后传播阶段减轻了其梯度。在实验中，我们的方法显示了各种多模式任务的大幅改进。这些简单而有效的策略不仅可以提高香草和以任务为导向的多峰模型的性能，还可以提高更复杂的多模式任务，从而展示了它们的有效性和灵活性。源代码可在\ url {https://github.com/gewu-lab/bml_tpami2024}中获得。

### MCTBench: Multimodal Cognition towards Text-Rich Visual Scenes Benchmark 
[[arxiv](https://arxiv.org/abs/2410.11538)] [[cool](https://papers.cool/arxiv/2410.11538)] [[pdf](https://arxiv.org/pdf/2410.11538)]
> **Authors**: Bin Shan,Xiang Fei,Wei Shi,An-Lan Wang,Guozhi Tang,Lei Liao,Jingqun Tang,Xiang Bai,Can Huang
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: 12 pages, 5 figures, project page: https://github.com/xfey/MCTBench?tab=readme-ov-file
- **标题**: MCTBENCH：对文本丰富的视觉场景的多模式认知基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于其广泛的应用程序，对文本丰富的视觉场景的理解已成为评估多模式大型语言模型（MLLM）的焦点。针对该方案的当前基准测试强调知觉能力，同时忽略了认知能力的评估。为了解决此限制，我们向文本富裕的视觉场景引入了多模式基准，以通过视觉推理和内容创造任务（MCTBENCH）评估MLLM的认知能力。为了减轻数据集不同分布的潜在评估偏差，MCTBench结合了几个感知任务（例如，场景文本识别），以确保对MLLM的认知和感知能力的一致比较。为了提高内容创造评估的效率和公平性，我们进行了自动评估管道。对MCTBench上各种MLLM的评估表明，尽管具有令人印象深刻的感知能力，但其认知能力仍需要增强。我们希望MCTBench将为社区提供有效的资源，以探索和增强对文本丰富的视觉场景的认知能力。

### VidCompress: Memory-Enhanced Temporal Compression for Video Understanding in Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.11417)] [[cool](https://papers.cool/arxiv/2410.11417)] [[pdf](https://arxiv.org/pdf/2410.11417)]
> **Authors**: Xiaohan Lan,Yitian Yuan,Zequn Jie,Lin Ma
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: 9 pages, 4 figures
- **标题**: VidCompress：在大语言模型中用于视频理解的内存增强的时间压缩
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 基于视频的多模式大型语言模型（视频LLMS）具有视频理解任务的重要潜力。但是，大多数视频LLM将视频视为一组顺序的单个帧，这会导致时间空间交互不足，从而阻碍了由于视觉令牌能力有限而导致的较长的理解和处理更长的视频。为了应对这些挑战，我们提出了VidCompress，这是一种具有内存增强的时间压缩的新型视频-LLM。 vidCompress采用双重压缩机方法：一种内存增强的压缩机在视频中捕获短期和长期的时间关系，并使用具有内存 - 气息机制的多尺度变压器来压缩视觉令牌，而文本感知的压缩机则通过将Q-Former和Crossect crotsed crotsed crotsed crotsed和grountife crotsed crotsed和Query goovers组成。几个VideoQA数据集和全面基准的实验表明，VidCompress有效地对复杂的时间空间关系进行了建模，并且显着胜过现有的视频符号。

### MoChat: Joints-Grouped Spatio-Temporal Grounding LLM for Multi-Turn Motion Comprehension and Description 
[[arxiv](https://arxiv.org/abs/2410.11404)] [[cool](https://papers.cool/arxiv/2410.11404)] [[pdf](https://arxiv.org/pdf/2410.11404)]
> **Authors**: Jiawei Mo,Yixuan Chen,Rifen Lin,Yongkang Ni,Min Zeng,Xiping Hu,Min Li
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 摩卡特：用于多转弯运动理解和描述的关节组时空接地LLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管深度学习以了解人类运动的持续发展，但现有模型通常很难准确地识别动作时机和特定的身体部位，通常仅支持单轮互动。捕获细颗粒运动细节的这种限制会降低其运动理解任务中的有效性。在本文中，我们提出了Mochat，Mochat是一种多式模式的大型语言模型，该模型能够对人类运动的时空接地和理解多转向对话的背景。为了实现这些能力，我们根据人解剖结构对每个骨架框架的空间信息进行分组，然后将它们与关节组骨骼编码器一起应用，它们的输出与LLM嵌入式组合在一起，以创建空间意识和时间意识的嵌入。此外，我们开发了一条管道，用于根据文本注释从骨骼序列中提取时间戳，并为空间接地构建多转对话。最后，为共同培训生成了各种任务说明。实验结果表明，Mochat在运动理解任务的多个指标中实现了最新的性能，使其成为第一个能够对人类运动的精细颗粒时空接地的模型。

### Preserve or Modify? Context-Aware Evaluation for Balancing Preservation and Modification in Text-Guided Image Editing 
[[arxiv](https://arxiv.org/abs/2410.11374)] [[cool](https://papers.cool/arxiv/2410.11374)] [[pdf](https://arxiv.org/pdf/2410.11374)]
> **Authors**: Yoonjeon Kim,Soohyun Ryu,Yeonsung Jung,Hyunkoo Lee,Joowon Kim,June Yong Yang,Jaeryong Hwang,Eunho Yang
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: Under review
- **标题**: 保存还是修改？在文本指导图像编辑中平衡保存和修改的上下文感知评估
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉语言和生成模型的开发具有显着高级的文本引导图像编辑，该图像编辑在\ textit {textit {修改}基于目标文本时，寻求源图像中核心元素的\ textit {保存}。但是，现有的指标存在\ textbf {Context-Blindness}问题，并在完全不同的源图像和目标文本对上不加区别地应用相同的评估标准，对修改或保存有偏见。方向夹相似性是唯一考虑源图像和目标文本的唯一指标，也偏向于修改方面，并参与图像的无关编辑区域。我们提出\ texttt {augclip}，\ textbf {context-ware}衡量标准，该指标可自适应地坐标保存和修改方面，具体取决于给定源图像和目标文本的特定上下文。这是通过得出理想编辑图像的剪辑表示来完成的，该图像保留了具有必要修改以与目标文本保持一致的源图像。更具体地说，使用多模式的大语言模型，\ texttt {augclip}增强了源和目标的文本描述，然后通过超平面计算一个修改向量，该超平面在剪辑空间中分开源和目标属性。在五个基准数据集上进行了广泛的实验，包括各种编辑方案，表明\ texttt {augclip}与人类评估标准非常相似，超过了现有的指标。该代码将被开源供社区使用。

### SeaDATE: Remedy Dual-Attention Transformer with Semantic Alignment via Contrast Learning for Multimodal Object Detection 
[[arxiv](https://arxiv.org/abs/2410.11358)] [[cool](https://papers.cool/arxiv/2410.11358)] [[pdf](https://arxiv.org/pdf/2410.11358)]
> **Authors**: Shuhan Dong,Yunsong Li,Weiying Xie,Jiaqing Zhang,Jiayuan Tian,Danian Yang,Jie Lei
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 秒：通过对比度学习进行多模式对象检测的补救双意见变压器，具有语义对齐方式
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式对象检测利用多种模态信息，以增强检测器的准确性和鲁棒性。通过学习长期依赖性，变压器可以有效地整合特征提取阶段的多模式特征，从而大大提高了多模式对象检测的性能。但是，当前的方法只是堆叠变压器引导的融合技术，而没有探索其在网络各个深度层提取特征的能力，从而限制了检测性能的改进。在本文中，我们引入了一种名为Seadate的准确有效的对象检测方法。最初，我们提出了一种新型的双重注意特征融合（DTF）模块，在变压器的指导下，通过双重注意机制整合了本地和全球信息，从而从正交角度使用空间和通道标记从正交角度增强了模态特征的融合。同时，我们的理论分析和经验验证表明，与深层语义信息相比，变压器引导的融合方法将图像视为融合的像素序列，在浅表特征的详细信息上表现更好。为了解决这个问题，我们设计了一个旨在学习多模式样本的对比度学习（CL）模块，从而在提取深层语义特征时纠正了变形金刚引导的融合的缺点，并有效地利用了跨模式信息。对FLIR，LLVIP和M3FD数据集进行了广泛的实验和消融研究证明了我们的有效方法，可以实现最新的检测性能。

### GCM-Net: Graph-enhanced Cross-Modal Infusion with a Metaheuristic-Driven Network for Video Sentiment and Emotion Analysis 
[[arxiv](https://arxiv.org/abs/2410.12828)] [[cool](https://papers.cool/arxiv/2410.12828)] [[pdf](https://arxiv.org/pdf/2410.12828)]
> **Authors**: Prasad Chaudhari,Aman Kumar,Chandravardhan Singh Raghaw,Mohammad Zia Ur Rehman,Nagendra Kumar
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: GCM-NET：具有元启发式驱动网络的图形增强跨模式输注，用于视频情感和情感分析
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 鉴于以不同方式传达的信息的多样性和复杂性，视频中的情感分析和情感识别是具有挑战性的任务。在该领域中，开发一个高度胜任的框架，有效地解决各种方式之间的不同特征是一个主要问题。先前关于组合多模式情感和情绪分析的研究通常忽略了模态整合，模式上下文一致性的有效融合，从而优化了串联的特征空间，从而导致了次优体系结构。本文介绍了一个新颖的框架，该框架利用了话语中的多型上下文信息，并应用元启发式算法来学习贡献的特征，以实现话语级别的情感和情感预测。我们使用元启发式驱动网络（GCM-NET）进行图形增强的跨模式输注，集成了图抽样和聚集，以重新校准视频情感和情感预测的模态特征。 GCM-NET包括一个跨模式的注意模块，以确定互相互作用和话语相关性。使用元启发式算法的谐波优化模块结合了所在的功能，允许处理单一和多量表输入。为了显示我们方法的有效性，我们对三个突出的多模式基准数据集（CMU MOSI，CMU MOSEI和IEMOCAP）进行了广泛的评估。实验结果证明了我们提出的方法的功效，显示了MOSI和MOSEI数据集的情感分析的91.56％和86.95％的精度。我们已经对Iemocap数据集进行了情感分析，该数据集获得了85.66％的精度，这表明对现有方法的性能提高了。

### Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.12790)] [[cool](https://papers.cool/arxiv/2410.12790)] [[pdf](https://arxiv.org/pdf/2410.12790)]
> **Authors**: Ce Zhang,Simon Stepputtis,Katia Sycara,Yaqi Xie
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: Accepted by NeurIPS 2024. Project page: https://zhangce01.github.io/DPE-CLIP
- **标题**: 双重原型演变为视觉模型的测试时间概括
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 测试时间适应性使模型能够通过未标记的测试样本推广到不同的数据，在现实世界中具有显着价值。最近，研究人员将此设置应用于先进的预训练视力语言模型（VLMS），开发了诸如测试时间及时调整之类的方法，以进一步扩展其实际适用性。但是，这些方法通常仅着眼于从单个模式中调整VLM，并且随着更多样本的处理，无法积累特定于任务的知识。为了解决这个问题，我们介绍了双重原型演变（DPE），这是一种针对VLM的新型测试时间适应方法，可有效地从多模式中积累特定于任务的知识。具体而言，我们创建和进化了两组原型 - 文本和视觉 - 在测试时间期间逐步捕获目标类的更准确的多模式表示。此外，为了促进一致的多模式表示，我们介绍并优化了每个测试样品的可学习残差，以使两种模式的原型对齐。 15个基准数据集的广泛实验结果表明，我们提出的DPE始终优于先前的最新方法，同时表现出竞争性的计算效率。代码可在https://github.com/zhangce01/dpe-clip上找到。

### The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio 
[[arxiv](https://arxiv.org/abs/2410.12787)] [[cool](https://papers.cool/arxiv/2410.12787)] [[pdf](https://arxiv.org/pdf/2410.12787)]
> **Authors**: Sicong Leng,Yun Xing,Zesen Cheng,Yang Zhou,Hang Zhang,Xin Li,Deli Zhao,Shijian Lu,Chunyan Miao,Lidong Bing
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: Project Page: cmm-damovl.site
- **标题**: 多模式的诅咒：评估跨语言，视觉和音频的大型多模型的幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）的最新进展已大大提高了各种任务的性能，并持续努力进一步整合其他模式，例如视频和音频。但是，大多数现有的LMM仍然容易受到幻觉的影响，事实多模式输入与生成的文本输出之间的差异限制了它们在各种现实世界中的适用性。本文介绍了涉及三种最常见方式的LMM中幻觉的首次系统研究：语言，视觉和音频。我们的研究揭示了幻觉的两个关键因素：对单峰先验和虚假模式间相关性的过度依赖。为了应对这些挑战，我们介绍了多模式（CMM）的基准，该基准全面评估了LMM中的幻觉，从而对其潜在问题进行了详细的分析。我们的发现突出了关键漏洞，包括模态整合的失衡和培训数据的偏见，强调了对平衡跨模式学习的需求和增强的减轻幻觉缓解策略。根据我们的观察和发现，我们建议潜在的研究方向，以提高LMM的可靠性。

### DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception 
[[arxiv](https://arxiv.org/abs/2410.12628)] [[cool](https://papers.cool/arxiv/2410.12628)] [[pdf](https://arxiv.org/pdf/2410.12628)]
> **Authors**: Zhiyuan Zhao,Hengrui Kang,Bin Wang,Conghui He
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: Github Repo: https://github.com/opendatalab/DocLayout-YOLO
- **标题**: doclayout-yolo：通过各种综合数据和全球自适应感知来增强文档布局分析
- **领域**: 计算机视觉和模式识别
- **摘要**: 文档布局分析对于现实世界中的文档理解系统至关重要，但是它遇到了速度和准确性之间的具有挑战性的权衡：多模式的方法利用文本和视觉特征实现了更高的准确性，但具有明显的延迟，而单峰方法则仅依赖于视觉特征，以更快的处理速度以准确的速度提供了更快的处理速度。为了解决这一难题，我们介绍了Doclayout-Yolo，这是一种新颖的方法，可以提高准确性，同时通过在预训练和模型设计中的文档特定优化来维持速度优势。对于强大的文档预培训，我们介绍了网状candidate BestFit算法，该算法将文档综合作为二维垃圾箱包装问题，从而产生大型，多样的Docsynth-300K数据集。对最终的Docsynth-300K数据集进行预培训可显着提高各种文档类型的微调性能。在模型优化方面，我们提出了一个全局到本地可控的接收模块，该模块能够更好地处理文档元素的多尺度变化。此外，为了验证不同文档类型的性能，我们引入了一个复杂且具有挑战性的基准，名为DocStructBench。在下游数据集上进行的大量实验表明，doclayout-yolo在速度和准确性方面均出色。代码，数据和模型可在https://github.com/opendatalab/doclayout-yolo上找到。

### Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor Fusion 
[[arxiv](https://arxiv.org/abs/2410.12592)] [[cool](https://papers.cool/arxiv/2410.12592)] [[pdf](https://arxiv.org/pdf/2410.12592)]
> **Authors**: Minkyoung Cho,Yulong Cao,Jiachen Sun,Qingzhao Zhang,Marco Pavone,Jeong Joon Park,Heng Yang,Z. Morley Mao
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: 23 pages
- **标题**: Cocoon：不确定性感知传感器融合的强大多模式感知
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 3D对象检测中的一个重要范式是使用多种模态在正常条件和挑战性条件下提高准确性，尤其是对于长尾方案。为了解决这一问题，最近的研究探讨了自适应方法的两个方向：基于MOE的适应性融合，这些融合在不同的对象构型引起的不确定性以及用于输出级别自适应融合的晚期融合而苦苦挣扎，这依赖于不同的检测管道和全面的理解。在这项工作中，我们介绍了Cocoon，这是一个对象和功能级别的不确定性融合框架。关键的创新在于异质表示的不确定性量化，通过引入功能对齐器和可学习的替代地面真理，可以在模态上进行公平比较，称为特征印象。我们还定义了一个培训目标，以确保他们的关系为不确定性量化提供有效的指标。在正常和挑战性的条件下，包括具有自然和人工腐败的茧在正常和具有挑战性的条件下始终优于现有的静态和适应性方法。此外，我们还显示了各种数据集中不确定性度量的有效性和功效。

### FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with Image Insertion 
[[arxiv](https://arxiv.org/abs/2410.12564)] [[cool](https://papers.cool/arxiv/2410.12564)] [[pdf](https://arxiv.org/pdf/2410.12564)]
> **Authors**: Jiacheng Ruan,Yebin Yang,Zehao Lin,Yuchen Feng,Feiyu Xiong,Zeyun Tang,Zhiyu Li
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: Work in progress. 9 pages, 3 figures
- **标题**: FTII板凳：带有图像插入流量文本的全面多模式基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 从大型语言模型（LLM）和基础视觉模型中的革命进步中受益，大型视觉模型（LVLM）也取得了重大进展。但是，当前的基准测试集中于仅评估LVLM功能的单个方面的任务（例如，识别，检测，理解）。这些任务无法完全证明LVLM在复杂的应用程序方案中的潜力。为了全面评估现有LVLM的性能，我们提出了一项更具挑战性的任务，称为“流程文本”，具有图像插入任务（FTII）。此任务要求LVLM在图像理解，教学理解和长篇文本解释方面同时具有出色的能力。具体而言，给定几段文本段落和一组候选图像，随着文本段落的积累，LVLMS需要从候选人中选择最合适的图像以在相应的段落之后插入。为此类任务构建基准是高度挑战的，尤其是在确定流动文本和图像的顺序时。为了应对这一挑战，我们转向专业新闻报告，该报告自然包含图像文本序列的黄金标准。基于此，我们介绍了图像插入基准（FTII板凳）的流文字，其中包括318个高质量的中文图像文本新闻文章和307个高质量的英语图像文本新闻文章，涵盖了10个不同的新闻领域。使用这些625篇高质量的文章，我们构建了两种不同类型的问题，这些问题具有多个级别的难度。此外，我们基于剪辑模型和现有LVLM建立了两个不同的评估管道。我们评估了9个开源源和2个封闭源LVLM以及2个基于夹的型号。结果表明，即使在解决FTII任务时，即使是最先进的模型（例如GPT-4O）也会面临重大挑战。

### HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks 
[[arxiv](https://arxiv.org/abs/2410.12381)] [[cool](https://papers.cool/arxiv/2410.12381)] [[pdf](https://arxiv.org/pdf/2410.12381)]
> **Authors**: Fengji Zhang,Linquan Wu,Huiyu Bai,Guancheng Lin,Xiao Li,Xiao Yu,Yue Wang,Bei Chen,Jacky Keung
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: homepage https://humaneval-v.github.io/
- **标题**: HumaneVal-V：在编码任务中使用复杂图的高级视觉推理进行基准测试
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 理解和推理在图表上是人类智力的基本方面。尽管大型多模型模型（LMM）在各种任务中都表现出了令人印象深刻的功能，但现有的基准缺乏对其图表解释和推理能力的全面评估，尤其是在编码环境中。我们介绍HumaneVal-V，这是人类注销的编码任务的严格基准，该任务涵盖了六种任务类型并评估了各种视觉推理功能。每个任务都采用精心制作的图表与功能签名和测试用例配对，采用新颖的代码生成任务来彻底评估模型的图表理解。通过对22个LMM的广泛实验，我们发现即使表现出色的模型也达到了适度的成功率，Claude 3.5十四行诗仅达到36.8％的通行证@1，突出了大量改进的空间。我们的分析表明，当前的LMM与人类认为直观的空间转换，拓扑关系和动态模式斗争。这些发现为推进LMM的视觉推理能力提供了宝贵的见解。我们已经在https://github.com/humaneval-v/humaneval-v-benchmark上开源代码和基准。

### ARIC: An Activity Recognition Dataset in Classroom Surveillance Images 
[[arxiv](https://arxiv.org/abs/2410.12337)] [[cool](https://papers.cool/arxiv/2410.12337)] [[pdf](https://arxiv.org/pdf/2410.12337)]
> **Authors**: Linfeng Xu,Fanman Meng,Qingbo Wu,Lili Pan,Heqian Qiu,Lanxiao Wang,Kailong Chen,Kanglei Geng,Yilei Qian,Haojie Wang,Shuchang Zhou,Shimou Ling,Zejia Liu,Nanlin Chen,Yingjie Xu,Shaoxu Cheng,Bowen Tan,Ziyong Xu,Hongliang Li
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: arXiv admin note: text overlap with arXiv:2409.03354. Updated the description for ARIC supplement
- **标题**: ARIC：课堂监视图像中的活动识别数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 活动识别在``AI +教育）领域中的应用正在越来越受到关注。但是，当前的工作主要集中在对手动捕获的视频和有限的活动类型中的活动的识别和有限数量的活动类型上的识别，几乎没有关注识别来自真实教室的监视图像中的活动。从真实的教室中的活动中的活动。在课堂上识别的活动范围相似，例如构建了类似的活动，例如构建了类似的活动，例如，构建了类似的范围，我们构建了众多的范围。集中在教室的图像活动识别（在课堂上的活动识别）具有多种观点，32个活动类别，三个方式，三个模式和现实的课堂场景，除了我们的一般活动识别方面。教学方案。您可以从https://ivipclab.github.io/publication_aric/aric下载初步数据。

### MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs 
[[arxiv](https://arxiv.org/abs/2410.12332)] [[cool](https://papers.cool/arxiv/2410.12332)] [[pdf](https://arxiv.org/pdf/2410.12332)]
> **Authors**: Yunqiu Xu,Linchao Zhu,Yi Yang
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: MC-Bench：MLLM时代的多上下文视觉接地的基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 虽然多模式的大语言模型（MLLM）表现出了非凡的视力语言理解能力，并显示出作为通用助手的潜力，但除了单个图像之外，它们可以解决实例级别的视觉语言问题，这是单一图像的进一步探索。为了评估MLLM的这些未经证实的能力，本文提出了一项新的视觉接地任务，称为Multi-Context Visual接地，该任务旨在基于开放式文本提示，跨多个图像进行跨多个图像的兴趣实例。为了促进这项研究，我们精心构建了一个新的数据集MC基础，用于基准MLLM的视觉接地能力。 MC基座具有2K高质量和手动注释的样本，由实例级标记的图像对和相应的文本提示组成，这些提示指示图像中的目标实例。总共有三种不同的文本提示样式，涵盖了20个实用技能。我们基准了20个最先进的MLLM和具有潜在多上下文视觉接地功能的基础模型。我们的评估揭示了所有指标现有MLLM和人类之间的非平凡性能差距。我们还观察到，现有的MLLM通常仅在图像级指标上超过没有LLM的基础模型，而接受单个图像的专业MLLM通常却经常难以推广到多图像场景。此外，简单的逐步基线整合高级MLLM和检测器可以显着超过先前的端到端MLLM。我们希望我们的MC基础和经验发现可以鼓励研究界进一步探索和增强实例级任务中MLLM的未开发潜力，尤其是在多图像环境中。项目页面：https：//xuyunqiu.github.io/mc-bench/。

### Decoding Emotions: Unveiling Facial Expressions through Acoustic Sensing with Contrastive Attention 
[[arxiv](https://arxiv.org/abs/2410.12811)] [[cool](https://papers.cool/arxiv/2410.12811)] [[pdf](https://arxiv.org/pdf/2410.12811)]
> **Authors**: Guangjing Wang,Juexing Wang,Ce Zhou,Weikang Ding,Huacheng Zeng,Tianxing Li,Qiben Yan
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-17
> **comment**: The extended version of the 2023 IEEE INFOCOM conference paper
- **标题**: 解码情绪：通过声感应和对比度注意通过声学传感揭示面部表情
- **领域**: 计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 通过准确检测用户的情绪状态，表达识别对诸如内容推荐和心理保健等应用的巨大希望。传统方法通常依赖相机或可穿戴传感器，这些传感器引起了隐私问题并增加了设备负担。此外，现有的基于声学的方法在训练数据集和推理数据集之间存在分配变化时，难以保持令人满意的性能。在本文中，我们介绍了一种活跃的声学面部表达识别系统Facer+，它消除了对外部麦克风阵列的需求。 Facer+提取面部表达功能，通过分析3D面部轮廓和智能手机上的听筒扬声器之间发出的近乎启动信号的回声。这种方法不仅降低了背景噪声，还可以通过最少的培训数据来识别来自各种用户的不同表达式。我们开发了一个基于外部注意力的对比模型，以始终如一地学习不同用户的表达功能，从而减少了分布差异。涉及有和没有口罩的20名志愿者的广泛实验表明，面部+可以准确地识别六种常见的面部表情，在多种多样的，独立的现实生活中，超过90％的精度超过90％，超过了领先的声学传感方法的性能。 Facer+为面部表达识别提供了强大而实用的解决方案。

### ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom 
[[arxiv](https://arxiv.org/abs/2410.14138)] [[cool](https://papers.cool/arxiv/2410.14138)] [[pdf](https://arxiv.org/pdf/2410.14138)]
> **Authors**: Jingqi Zhou,Sheng Wang,Jingwei Dong,Lei Li,Jiahui Gao,Lingpeng Kong,Chuan Wu
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 培训：多模式主动推理，视力和智慧是脱钩的
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉模型（LVLM）在视觉理解任务上见证了重大进展。但是，他们经常将语言知识优先于有关视觉推理任务的图像信息，从而导致性能降级。为了解决这个问题，我们首先确定现有解决方案的缺点（即，视觉描述不足和无关紧要的描述以及有限的多模式能力）。然后，我们将视觉推理过程分为两个阶段：视觉感知（即视力）和文本推理（即智慧），并介绍一个名为Proreason的新颖的视觉推理框架。该框架具有多运行的主动感知和脱钩的视觉功能。简而言之，考虑到一个多模式的问题，前季节会迭代主动的信息收集和推理，直到可以通过必要和充分的视觉描述结束答案为止。值得注意的是，功能的分离允许现有大型语言模型（LLMS）的无缝集成以弥补LVLM的推理缺陷。我们的广泛实验表明，对于开源和封闭源型号，前季前赛优于现有的多步推理框架和被动同行方法。此外，在LLMS的帮助下，Proreason在MMMU基准方面的性能提高了15％。我们对现有解决方案的见解以及LLM可行整合的脱钩观点阐明了对视觉推理技术的未来研究，尤其是LLM辅助技术的研究。

### MMAD-Purify: A Precision-Optimized Framework for Efficient and Scalable Multi-Modal Attacks 
[[arxiv](https://arxiv.org/abs/2410.14089)] [[cool](https://papers.cool/arxiv/2410.14089)] [[pdf](https://arxiv.org/pdf/2410.14089)]
> **Authors**: Xinxin Liu,Zhongliang Guo,Siyuan Huang,Chun Pong Lau
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: MMAD-PULIFY：一个精确优化的框架，用于高效且可扩展的多模式攻击
- **领域**: 计算机视觉和模式识别
- **摘要**: 神经网络在各种任务中都取得了出色的性能，但它们仍然容易受到对抗性扰动的影响，这在安全至关重要的应用中构成了重大风险。随着多模态的兴起，扩散模型不仅是生成任务的强大工具，而且还用于各种应用程序，例如图像编辑，介入和超级分辨率。但是，由于对攻击它们增强弹性的研究有限，这些模型仍然缺乏鲁棒性。传统的攻击技术，例如基于梯度的对抗攻击和基于扩散模型的方法，由于其迭代性质而受到计算效率低下和可伸缩性问题的阻碍。为了应对这些挑战，我们引入了一个创新的框架，该框架利用了扩散模型的蒸馏骨头，并结合了精确的优化噪声预测器，以增强我们的攻击框架的有效性。这种方法不仅提高了攻击的效力，而且还大大降低了计算成本。我们的框架为多模式对抗攻击提供了尖端的解决方案，以确保潜伏期减少和产生具有较高成功率的高保真对抗性示例。此外，我们证明我们的框架可以实现针对纯化防御的出色可转移性和鲁棒性，在有效性和效率方面都优于基于梯度的攻击模型。

### Transformers Utilization in Chart Understanding: A Review of Recent Advances & Future Trends 
[[arxiv](https://arxiv.org/abs/2410.13883)] [[cool](https://papers.cool/arxiv/2410.13883)] [[pdf](https://arxiv.org/pdf/2410.13883)]
> **Authors**: Mirna Al-Shetairy,Hanan Hindy,Dina Khattab,Mostafa M. Aref
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 图表理解中的变形金刚利用：对最新进展和未来趋势的回顾
- **领域**: 计算机视觉和模式识别,人工智能,人机交互,机器学习
- **摘要**: 近年来，对视觉任务的兴趣已经增长，尤其是涉及图表互动的兴趣。这些任务本质上是多模式的，需要模型来处理图表图像，随附的文本，基础数据表以及通常的用户查询。传统上，图表理解（CU）依赖于启发式和基于规则的系统。但是，整合变压器体系结构的最新进步显着提高了性能。本文回顾了CU的著名研究，重点是最先进的（SOTA）框架，这些框架在端到端（E2E）解决方案中采用变压器。分析了相关的基准数据集和评估技术。此外，本文确定了主要的挑战，并概述了推进CU解决方案的有希望的未来方向。遵循PRISMA指南，在Google Scholar中进行了全面的文献搜索，重点是从1月20日到6月24日。经过严格的筛选和质量评估后，选择了32项研究进行深入分析。根据所需的认知任务，将CU任务分为三层范式。还审查了解决各种CU任务的框架中的最新进展。根据E2E解决方案可解决的任务数量，将框架分为单任务或多任务。在多任务框架中，探索了预先训练和及时工程技术的技术。这篇审查概述了领先的体系结构，数据集和预训练任务。尽管取得了重大进展，但OCR依赖性仍存在挑战，处理低分辨率图像并增强视觉推理。未来的方向包括应对这些挑战，开发强大的基准测试以及优化模型效率。此外，整合可解释的AI技术并探索真实数据和合成数据之间的平衡对于推进CU研究至关重要。

### PUMA: Empowering Unified MLLM with Multi-granular Visual Generation 
[[arxiv](https://arxiv.org/abs/2410.13861)] [[cool](https://papers.cool/arxiv/2410.13861)] [[pdf](https://arxiv.org/pdf/2410.13861)]
> **Authors**: Rongyao Fang,Chengqi Duan,Kun Wang,Hao Li,Hao Tian,Xingyu Zeng,Rui Zhao,Jifeng Dai,Hongsheng Li,Xihui Liu
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: Project page: https://rongyaofang.github.io/puma/
- **标题**: PUMA：通过多粒子视觉产生授权统一的MLLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式基础模型的最新进展在视觉理解方面取得了重大进展。初始尝试还探索了多模式大语言模型（MLLM）对视觉内容生成的潜力。但是，现有作品还不足以解决统一的MLLM范式内不同图像生成任务的不同粒度需求 - 从文本到图像生成所需的多样性到图像操纵所需的确切可控性。在这项工作中，我们提出了PUMA，以多种视觉生成赋予统一的MLLM能力。 PUMA将多个粒度视觉特征统一为MLLM的输入和输出，优雅地满足了统一的MLLM框架中各种图像生成任务的不同粒度要求。按照多模式预处理和特定于任务的指令调整，PUMA表明了在各种多模式任务中的熟练程度。这项工作代表了迈出真正统一的MLLM迈出的重要一步，能够适应各种视觉任务的粒度要求。代码和模型将在https://github.com/rongyaofang/puma中发布。

### MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.13859)] [[cool](https://papers.cool/arxiv/2410.13859)] [[pdf](https://arxiv.org/pdf/2410.13859)]
> **Authors**: Yaxin Luo,Gen Luo,Jiayi Ji,Yiyi Zhou,Xiaoshuai Sun,Zhiqiang Shen,Rongrong Ji
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: mod：探索多模式大语言模型的深入改编的混合物
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管多模式大语言模型（MLLM）取得了重大进展，但它们的高计算成本仍然是现实部署的障碍。受到自然语言处理深度（mod）的混合的启发，我们旨在从``激活的令牌''的角度解决这一限制。我们的关键见解是，如果大多数令牌对于层计算是多余的，则可以直接通过mod层跳过。但是，将MLLM的密集层直接转换为模块层会导致大量性能降解。为了解决这个问题，我们为现有的称为$γ$ -mod的现有MLLM提出了创新的mod适应策略。在$γ$ -MOD中，提出了一个新型指标来指导MOD在MLLM中的部署，即注意地图的等级（Arank）。通过Arank，我们可以有效地确定哪个层是冗余的，应用Mod层代替。基于Arank，我们进一步提出了两种新型设计，以最大程度地提高MLLM的计算稀疏性，同时保持其性能，即共享视觉路由器和掩盖路由学习。借助这些设计，可以有效地转换为MOD的90％以上密度的MLLM层。为了验证我们的方法，我们将其应用于三个流行的MLLM，并在9个基准数据集上进行大量实验。实验结果不仅验证了现有MLLM的$γ$ mod的显着效率益处，而且还证实了其在各种MLLM上的概括能力。例如，由于较小的性能下降，即-1.5％，$γ$ -MOD可以将LLAVA-HR的训练和推理时间分别降低31.0％和53.2％。

### Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation 
[[arxiv](https://arxiv.org/abs/2410.13848)] [[cool](https://papers.cool/arxiv/2410.13848)] [[pdf](https://arxiv.org/pdf/2410.13848)]
> **Authors**: Chengyue Wu,Xiaokang Chen,Zhiyu Wu,Yiyang Ma,Xingchao Liu,Zizheng Pan,Wen Liu,Zhenda Xie,Xingkai Yu,Chong Ruan,Ping Luo
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: Technical Report
- **标题**: Janus：将视觉编码解耦，以进行统一的多模式理解和生成
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 在本文中，我们介绍了Janus，这是一个自动回归框架，该框架统一了多模式的理解和产生。先前的研究通常依赖于两个任务（例如变色龙）的单个视觉编码器。但是，由于多模式理解和产生所需的信息粒度的不同水平，这种方法可能导致次优性能，尤其是在多模式理解中。为了解决这个问题，我们将视觉编码解码为单独的途径，同时仍利用单个统一的变压器体系结构进行处理。脱钩不仅减轻了视觉编码器在理解和发电中的作用之间的冲突，而且还可以增强框架的灵活性。例如，多模式理解和生成组件都可以独立选择其最合适的编码方法。实验表明，Janus超过了以前的统一模型，并且匹配或超过了特定于任务模型的性能。 Janus的简单性，高灵活性和有效性使其成为下一代统一多模型模型的有力候选人。

### Harnessing Webpage UIs for Text-Rich Visual Understanding 
[[arxiv](https://arxiv.org/abs/2410.13824)] [[cool](https://papers.cool/arxiv/2410.13824)] [[pdf](https://arxiv.org/pdf/2410.13824)]
> **Authors**: Junpeng Liu,Tianyue Ou,Yifan Song,Yuxiao Qu,Wai Lam,Chenyan Xiong,Wenhu Chen,Graham Neubig,Xiang Yue
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 利用网页UIS以进行文本丰富的视觉理解
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 文本丰富的视觉理解 - 处理与视觉词的密集文本内容的能力 - 对于多模式大型语言模型（MLLMS）至关重要，可以与结构化环境有效交互。为了增强此功能，我们建议使用基于文本的大语言模型（LLMS）从网页UIS进行一般多模式指令的合成。尽管缺乏直接的视觉输入，但基于文本的LLM仍能够从网页可访问性树中处理结构化的文本表示。然后将这些说明与UI屏幕截图配对以训练多模式。我们介绍了Multiui，这是一个数据集，其中包含来自100万个网站的730万个样本，涵盖了多种模式任务和UI布局。在MultiUI培训的模型中，不仅在Web UI任务中表现出色，可实现48％的visualwebbench，并在Web代理数据集Mind2Web上提高了19.1％的元素准确性，但还可以使非WEB UI任务甚至是非ui域，甚至对非UII域的概述，例如对非UI IN的范围，例如文档理解和图表process，OCR和图表。这些结果突出了Web UI数据的广泛适用性，用于在各种情况下推进文本丰富的视觉理解。

### Improving Multi-modal Large Language Model through Boosting Vision Capabilities 
[[arxiv](https://arxiv.org/abs/2410.13733)] [[cool](https://papers.cool/arxiv/2410.13733)] [[pdf](https://arxiv.org/pdf/2410.13733)]
> **Authors**: Yanpeng Sun,Huaxin Zhang,Qiang Chen,Xinyu Zhang,Nong Sang,Gang Zhang,Jingdong Wang,Zechao Li
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 通过提高视觉功能来改善多模式大型语言模型
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 我们专注于提高视觉理解能力，以增强视觉语言模型。我们提出了一种多模式模型\ TextBf {Arcana}，它引入了两种至关重要的技术。首先，我们提出了多模式洛拉（MM-Lora），这是一个旨在增强解码器的模块。与传统语言驱动的解码器不同，MM-Lora由两个平行的Loras组成 - 一个用于视觉，一个用于语言 - 每个都有其自己的参数。这种分离的参数设计允许在每种模式中进行更专业的学习，并更好地整合多模式信息。其次，我们介绍查询梯子适配器（Qladder）以改进视觉编码器。 Qladder采用可学习的``\ textit {ladder}'结构来深入从冷冻预处理的视觉编码器（例如剪辑图像编码器）中深入汇总中间表示。这使该模型能够学习新的且内容丰富的视觉特征，并保留了预验证的视觉编码器的强大功能。这些技术共同增强了Arcana的视觉感知能力，使其能够利用改进的视觉信息，以在各种多模式方案中更准确且上下文相关的输出。广泛的实验和消融研究证明了我们的奥ac的有效性和概括能力。代码和重新注销的数据可在\ url {https://arcana-project-page.github.io}上获得。

### Exploring the Design Space of Visual Context Representation in Video MLLMs 
[[arxiv](https://arxiv.org/abs/2410.13694)] [[cool](https://papers.cool/arxiv/2410.13694)] [[pdf](https://arxiv.org/pdf/2410.13694)]
> **Authors**: Yifan Du,Yuqi Huo,Kun Zhou,Zijia Zhao,Haoyu Lu,Han Huang,Wayne Xin Zhao,Bingning Wang,Weipeng Chen,Ji-Rong Wen
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: Long Video MLLM; work in progress
- **标题**: 探索视频MLLM中视觉上下文表示的设计空间
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视频多模式大型语言模型（MLLMS）表现出了出色的能力，可以理解各种下游任务的视频语义。尽管取得了进步，但仍缺乏对视觉上下文表示的系统研究，该研究指的是从视频中选择帧并进一步从框架中选择令牌的方案。在本文中，我们探讨了视觉上下文表示的设计空间，并旨在通过寻找更有效的表示方案来提高视频MLLM的性能。首先，我们将视觉上下文表示的任务作为一个受约束的优化问题，并建模语言建模损失与帧数的函数以及每个框架的嵌入数量（或令牌）的函数，给定最大的视觉上下文窗口大小。然后，我们分别探索框架选择和令牌选择中的缩放效果，并通过进行广泛的经验实验来拟合相应的功能曲线。我们研究了典型选择策略的有效性，并提出了经验发现以确定这两个因素。此外，我们研究了框架选择和令牌选择的关节效应，并得出确定这两个因素的最佳公式。我们证明，派生的最佳设置与经验实验的最佳表现结果保持一致。我们的代码和模型可在以下网址获得：https：//github.com/rucaibox/opt-visor。

### VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic Reasoning Tasks 
[[arxiv](https://arxiv.org/abs/2410.13666)] [[cool](https://papers.cool/arxiv/2410.13666)] [[pdf](https://arxiv.org/pdf/2410.13666)]
> **Authors**: Shailaja Keyur Sampat,Mutsumi Nakamura,Shankar Kailas,Kartik Aggarwal,Mandy Zhou,Yezhou Yang,Chitta Baral
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 18 pages, 7 figures
- **标题**: VL-Glue：一套基本但具有挑战性的Visuo语言推理任务
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 从异质输入（例如图像，文本和音频）中得出推断是人类执行日常任务的重要技能。对于高级人工智能（AI）系统的开发，需要类似的能力。尽管最先进的模型正在迅速缩小差距，并在各种计算机视觉和NLP任务上以人类水平的性能分别缩小差距，但他们努力解决需要对视觉和文本方式进行联合推理的任务。受Glue的启发（Wang等，2018） - 我们在本文中提出了VL-Glue的自然语言理解的多任务基准。 VL-Glue由跨越七个不同任务的超过100K样品组成，这基于其核心需要Visuo-Lighuistic推理。此外，我们的基准包括各种图像类型（从合成的渲染图，以及日常场景到图表和复杂图），其中包括广泛的特定领域特定文本（从烹饪，政治和体育运动到高中课程），表明需要在真实世界中对多模式的理解进行多模式的了解。我们表明，这种基准对于现有的大规模视觉模型而言非常具有挑战性，并鼓励开发具有强大的Visuo语言推理能力的系统。

### ActionCOMET: A Zero-shot Approach to Learn Image-specific Commonsense Concepts about Actions 
[[arxiv](https://arxiv.org/abs/2410.13662)] [[cool](https://papers.cool/arxiv/2410.13662)] [[pdf](https://arxiv.org/pdf/2410.13662)]
> **Authors**: Shailaja Keyur Sampat,Yezhou Yang,Chitta Baral
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 15 pages, 3 figures. arXiv admin note: text overlap with arXiv:2004.10796 by other authors
- **标题**: ActionComet：一种学习针对动作的特定图像常识概念的零拍方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类观察其他人（身体或视频/图像中）正在执行的各种动作，并且可以对其进行广泛的推论，而不是他们可以在视觉上感知到的。这种推论包括确定使行动执行成为可能的世界各个方面（例如，液体物体可以进行倾泻），预测世界将会因行动而变化（例如土豆在煎炸后黄金和脆皮），高级目标，与动作相关的高级目标（例如，击败鸡蛋或在鸡蛋中脱颖而出），并在当前的行动中脱颖而出（以前的鸡蛋）沸腾）。在自主系统中，非常需要类似的推理能力，这将有助于我们执行日常任务。为此，我们提出了一项多模式任务，以了解有关图像中正在执行的动作的上述概念。我们开发了一个数据集，该数据集由8.5k图像和59.3K推断，这些数据集关于基于这些图像中的操作，从注释的烹饪视频数据集收集。我们提出了ActionComet，这是一个零拍的框架，以辨别特定于提供的视觉输入的语言模型中存在的知识。我们介绍了收集到的数据集对Action Comet的基线结果，并将其与最佳现有VQA方法的性能进行比较。

### RemoteDet-Mamba: A Hybrid Mamba-CNN Network for Multi-modal Object Detection in Remote Sensing Images 
[[arxiv](https://arxiv.org/abs/2410.13532)] [[cool](https://papers.cool/arxiv/2410.13532)] [[pdf](https://arxiv.org/pdf/2410.13532)]
> **Authors**: Kejun Ren,Xin Wu,Lianming Xu,Li Wang
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: semotedet-mamba：一种用于遥感图像中多模式对象检测的混合MAMBA-CNN网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 无人驾驶飞机（UAV）遥感被广泛应用于紧急响应等领域，这是由于其快速信息获取和低成本的优势。但是，由于射击距离和成像机制的影响，图像中的物体带来了挑战，例如小尺寸，密集分布和较低的阶层间分化。为此，我们提出了一个多模式遥感检测网络，该网络采用了四个方向的选择性扫描融合策略，称为sellotedet-mamba。远征etet-mamba同时促进了学习单模式本地特征的学习，并在模态上集成了补丁级的全局特征，从而增强了对小物体的区分性，并利用本地信息来改善不同类别之间的歧视。另外，使用Mamba的串行处理可显着提高检测速度。无人机数据集的实验结果证明了远程难体 - 曼巴（Mamba）的有效性，与最先进的方法相比，在保持计算效率和参数计数的同时，它可以达到卓越的检测准确性。

### Temporal-Enhanced Multimodal Transformer for Referring Multi-Object Tracking and Segmentation 
[[arxiv](https://arxiv.org/abs/2410.13437)] [[cool](https://papers.cool/arxiv/2410.13437)] [[pdf](https://arxiv.org/pdf/2410.13437)]
> **Authors**: Changcheng Xiao,Qiong Cao,Yujie Zhong,Xiang Zhang,Tao Wang,Canqun Yang,Long Lan
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 时间增强的多模式变压器，用于引用多对象跟踪和分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 参考多对象跟踪（RMOT）是一项新兴的跨模式任务，旨在定位任意数量的目标对象并维护视频中语言表达式所引用的身份。这项复杂的任务涉及语言和视觉方式以及目标对象的时间关联的推理。但是，开创性的工作仅采用松散的特征融合，并忽略了追踪对象的长期信息的利用。在这项研究中，我们引入了一种基于紧凑的变压器方法，称为TenrMot。我们在编码和解码阶段进行功能融合，以充分利用变压器体系结构的优势。具体而言，我们在编码阶段逐步执行逐层融合层。在解码阶段，我们利用语言引导的查询来探测内存特征，以准确预测所需的对象。此外，我们引入了一个查询更新模块，该模块明确利用了跟踪对象的时间信息以增强其轨迹的一致性。此外，我们介绍了一项名为“参考多对象跟踪和分割（RMOT）”的新任务，并构建了一个名为ref-kitti分段的新数据集。我们的数据集由18个带有818个表达式的视频组成，每个表达式的平均值为10.7个掩码，与大多数现有的引用视频分割数据集中的典型单个掩码相比，它提出了更大的挑战。 TenrMot在参考多对象跟踪和分割任务上都表现出卓越的性能。

### Performance of Gaussian Mixture Model Classifiers on Embedded Feature Spaces 
[[arxiv](https://arxiv.org/abs/2410.13421)] [[cool](https://papers.cool/arxiv/2410.13421)] [[pdf](https://arxiv.org/pdf/2410.13421)]
> **Authors**: Jeremy Chopin,Rozenn Dahyot
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 8 pages
- **标题**: 在嵌入式特征空间上的高斯混合模型分类器的性能
- **领域**: 计算机视觉和模式识别
- **摘要**: 带有夹子和图像键的数据嵌入为多媒体和/或多模式数据的分析提供了强大的功能。我们在此评估他们的性能，以使用基于高斯混合模型（GMM）层作为标准软磁层层的替代品进行分类。最近已显示基于GMM的分类器具有有趣的表演，作为经过深度学习管道的端到端训练的一部分。我们的第一个贡献是利用嵌入式空间夹和图像键来研究基于GMM的分类性能。我们的第二个贡献是提出自己的基于GMM的分类器，其参数数量比以前提出的较低。我们的发现是，在大多数情况下，在这些经过测试过的嵌入式空间上，GMM中的一个高斯组件通常足以捕获每个类别，我们假设这可能是由于用于训练这些嵌入式空间的对比损失所致，这些嵌入式空间自然浓缩了每个类别的特征。我们还观察到，即使使用PCA压缩了这些嵌入式空间，图像bind通常比剪辑提供比剪辑更好的图像数据集的性能。

### Retrieval-Augmented Personalization for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.13360)] [[cool](https://papers.cool/arxiv/2410.13360)] [[pdf](https://arxiv.org/pdf/2410.13360)]
> **Authors**: Haoran Hao,Jiaming Han,Changsheng Li,Yu-Feng Li,Xiangyu Yue
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 用于多模式大语言模型的检索增强个性化
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,多媒体
- **摘要**: 大型语言模型（LLM）的发展显着增强了多模式LLM（MLLM）作为普通助理的能力。但是，缺乏特定用户的知识仍然限制了他们在人类日常生活中的应用。在本文中，我们介绍了MLLM的个性化检索增强个性化（RAP）框架。从一般的MLLM开始，我们将其分为三个步骤将其变成个性化的助手。 （a）记住：我们设计一个键值数据库，以存储用户相关信息，例如用户名，头像和其他属性。 （b）检索：当用户启动对话时，RAP将使用多模式猎犬从数据库中检索相关信息。 （c）生成：输入查询和检索到的概念的信息被馈入MLLM，以生成个性化的知识调整响应。与以前的方法不同，RAP允许通过更新外部数据库进行实时概念编辑。为了进一步提高用户特定信息的发电质量和对齐，我们设计了一条用于数据收集的管道，并创建一个专门的数据集，用于对MLLM的个性化培训。根据数据集，我们将一系列MLLM培训为个性化的多模式助手。通过在大规模数据集上进行预处理，RAP-MLLM可以将其推广到无限的视觉概念而无需额外的填充。我们的模型表现出各种任务的出色灵活性和发电质量，例如个性化图像字幕，问题答案和视觉识别。代码，数据和模型可在https://github.com/hoar012/rap-mllm上找到。

### Fundus to Fluorescein Angiography Video Generation as a Retinal Generative Foundation Model 
[[arxiv](https://arxiv.org/abs/2410.13242)] [[cool](https://papers.cool/arxiv/2410.13242)] [[pdf](https://arxiv.org/pdf/2410.13242)]
> **Authors**: Weiyi Zhang,Jiancheng Yang,Ruoyu Chen,Siyu Huang,Pusheng Xu,Xiaolan Chen,Shanfu Lu,Hongyu Cao,Mingguang He,Danli Shi
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 荧光素血管造影视频生成的眼底，作为视网膜生成基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 眼底荧光素血管造影（FFA）对于诊断和监测视网膜血管问题至关重要，但与颜色眼底（CF）成像相比，其侵入性性质和限制可及性受到限制。将CF图像转换为FFA的现有方法仅限于静态图像生成，缺少动态病变的变化。我们介绍了fistus2Video，这是一种自回归的生成对抗网络（GAN）模型，该模型从单个CF图像中生成动态FFA视频。 Fandus2Video在视频生成方面表现出色，达到1497.12的FVD，PSNR为11.77。临床专家已经验证了生成视频的忠诚度。此外，该模型的发电机在十个外部公共数据集中表现出显着的下游可转移性，包括血管分割，视网膜疾病诊断，全身性疾病预测和多模式检索，展示了令人印象深刻的零拍和很少的功能。这些发现将FENDUS2VIDEO定位为FFA考试的强大，非侵入性替代方案，并且是一种多功能的视网膜生成基础模型，可捕获静态和时间视网膜特征，从而能够表示复杂的模式间关系。

### Multimodal Learning for Embryo Viability Prediction in Clinical IVF 
[[arxiv](https://arxiv.org/abs/2410.15581)] [[cool](https://papers.cool/arxiv/2410.15581)] [[pdf](https://arxiv.org/pdf/2410.15581)]
> **Authors**: Junsik Kim,Zhiyi Shi,Davin Jeong,Johannes Knittel,Helen Y. Yang,Yonghyun Song,Wanhua Li,Yicong Li,Dalit Ben-Yosef,Daniel Needleman,Hanspeter Pfister
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: Accepted to MICCAI 2024
- **标题**: 临床IVF中胚胎生存能力预测的多模式学习
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在临床体内受精（IVF）中，鉴定转移最可行的胚胎对于增加成功怀孕的可能性很重要。传统上，此过程涉及使用光学显微镜以特定时间间隔手动评估胚胎的静态形态特征。由于需要专家分析，而且本质上是主观的，这项手动评估不仅是时间密集型且昂贵的，因此导致选择过程的可变性。为了应对这些挑战，我们开发了一种多模型，该模型利用延时视频数据和电子健康记录（EHR）来预测胚胎生存能力。我们研究的首要挑战之一是由于它们在形态上的固有差异而有效地结合了延时视频和EHR数据。我们通过各种模态输入和集成方法全面分析了我们的多模式模型。我们的方法将在临床IVF的规模上快速，自动化的胚胎生存能力预测。

### Generalized Multimodal Fusion via Poisson-Nernst-Planck Equation 
[[arxiv](https://arxiv.org/abs/2410.15475)] [[cool](https://papers.cool/arxiv/2410.15475)] [[pdf](https://arxiv.org/pdf/2410.15475)]
> **Authors**: Jiayu Xiong,Jing Wang,Hengjing Xiang,Jun Xue,Chen Xu,Zhouqiang Jiang
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: NeurIPS 2024 Rejected paper, 28 pages
- **标题**: 通过Poisson-Nernst-Planck方程式通用多模式融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 先前的研究强调了多模式融合的显着进步。然而，这种方法经常在特征提取，数据完整性，特征维度的一致性以及各种下游任务的适应性方面遇到挑战。本文通过Poisson-Nernst-Planck（PNP）方程提出了一种广义的多模式融合方法（GMF），该方程熟练解决了上述问题。从理论上讲，传统多模式任务的优化目标是通过集成信息熵和梯度向后步的流程来制定和重新定义的。利用这些理论见解，PNP方程用于特征融合，通过物理中带电粒子的框架重新思考多模式特征，并通过解离，浓度和重建来控制其运动。在这些理论基础的基础上，GMF分离的特征由单峰特征提取器提取为模态特异性和模态不变子空间，从而减少了相互的信息，并随后降低了下游任务的熵。该功能来源的可识别性使我们可以独立地作为前端发挥作用，与简单的串联后端无缝集成，或者充当其他模块的先决条件。多个下游任务的实验结果表明，所提出的GMF在利用较少的参数和计算资源的同时，达到了接近最先进（SOTA）精度的性能。此外，通过将GMF与高级融合方法整合在一起，我们超过了SOTA结果。

### EVA: An Embodied World Model for Future Video Anticipation 
[[arxiv](https://arxiv.org/abs/2410.15461)] [[cool](https://papers.cool/arxiv/2410.15461)] [[pdf](https://arxiv.org/pdf/2410.15461)]
> **Authors**: Xiaowei Chi,Hengyuan Zhang,Chun-Kai Fan,Xingqun Qi,Rongyu Zhang,Anthony Chen,Chi-min Chan,Wei Xue,Wenhan Luo,Shanghang Zhang,Yike Guo
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: EVA：一种实现的世界模型，用于未来的视频预期
- **领域**: 计算机视觉和模式识别,多媒体,机器人技术
- **摘要**: 世界模型整合了来自各种模式的原始数据，例如图像和语言，以模拟世界上的全面互动，从而在混合现实和机器人技术等领域中显示出至关重要的作用。然而，由于实践中各种场景的复杂和动态意图，将世界模型应用于准确的视频预测非常具有挑战性。在本文中，受到人类重新思考过程的启发，我们将复杂的视频预测分解为四个元任务，使世界模型能够以更细粒度的方式处理这个问题。除这些任务外，我们还引入了一个名为“体现视频预期基准”基准（EVA Bench）的新基准测试，以提供全面的评估。 EVA BENCH的重点是评估人类和机器人动作的视频预测能力，对语言模型和发电模型提出了重大挑战。针对体现的视频预测，我们提出了一个旨在视频理解和发电的统一框架。 EVA将视频生成模型与视觉语言模型集成在一起，有效地将推理能力与高质量的生成结合在一起。此外，为了增强我们的框架的概括，我们定制了设计的多阶段预处理范式，该范式适应洛拉（Lora）以产生高保真性结果。在EVA基础上进行的广泛实验突出了EVA显着提高体现场景的性能的潜力，为实际预测任务中的大规模预训练模型铺平了道路。

### MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis and Knowledge-based Departmental Consultation 
[[arxiv](https://arxiv.org/abs/2410.15403)] [[cool](https://papers.cool/arxiv/2410.15403)] [[pdf](https://arxiv.org/pdf/2410.15403)]
> **Authors**: Yi Ren,HanZhi Zhang,Weibin Li,Jun Fu,Diandong Liu,Tianyi Zhang,Jie He,Licheng Jiao
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: MMDS：一个多模式医学诊断系统整合图像分析和基于知识的部门咨询
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们提出MMD，这是一种能够识别医学图像和患者面部细节并提供专业医学诊断的系统。该系统由两个核心组成部分组成：第一个组件是对医学图像和视频的分析。我们培训了一种专门的多模式医学模型，能够解释医学图像并准确分析患者的面部情绪和面部瘫痪条件。该模型在FER2013面部情感识别数据集上获得了72.59％的精度，识别“快乐”情绪的精度为91.1％。在面部瘫痪识别中，该模型的准确性为92％，比GPT-4O高30％。基于此模型，我们开发了一个解析器，用于分析面部麻痹患者的面部运动视频，从而达到了瘫痪严重程度的精确分级。在对30个面部瘫痪患者的30个视频测试中，该系统的评分精度为83.3％。第二部分是产生专业医疗反应。我们采用了与医学知识库集成的大型语言模型，根据医学图像或视频的分析来生成专业诊断。核心创新在于我们开发特定于部门的知识基础路由管理机制，在该机制中，大语言模型将数据按医疗部门对数据进行了分类，并且在检索过程中，确定了适当的知识基础。这显着提高了抹布（检索仪的生成）过程的检索准确性。

### Modality-Fair Preference Optimization for Trustworthy MLLM Alignment 
[[arxiv](https://arxiv.org/abs/2410.15334)] [[cool](https://papers.cool/arxiv/2410.15334)] [[pdf](https://arxiv.org/pdf/2410.15334)]
> **Authors**: Songtao Jiang,Yan Zhang,Ruizhe Chen,Yeying Jin,Zuozhu Liu
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 对可信赖的MLLM对齐方式的模态偏好优化
- **领域**: 计算机视觉和模式识别
- **摘要**: 直接偏好优化（DPO）对于对齐大语言模型（LLMS）是有效的，但是当应用于多模式模型（MLLMS）时，它通常偏爱文本而不是图像信息，从而导致不可靠的输出和视觉幻觉。为了解决这个问题，我们提出了模态偏好优化（MFPO），以平衡文本和图像偏好。首先，我们发现，在偏好数据中缺乏与图像相关的奖励会偏向文本，因此我们创建了自动化的，细粒度的图像偏好数据以纠正这一点。然后，我们设计了一个学习目标，以确保模型同时捕获文本和图像偏好，同时保持高质量的输出。最后，我们使用多阶段的一致性方法来稳定训练并改善两种方式的学习。广泛的实验表明，MFPO显着增强了MLLM的可信度。在Llava-V1.5（7b，13b）之类的模型上，我们的方法大大降低了幻觉。在7B模型上，MFPO的表现优于GPT-4V，并且在对象HALBENCH上的使用方法比以前的方法取得了近40 \％的改善，并且在与最新的Llava-V1.6结合使用时，在对象Halbench和Amber上都能实现最先进的性能。代码将发布。

### Can LVLMs Describe Videos like Humans? A Five-in-One Video Annotations Benchmark for Better Human-Machine Comparison 
[[arxiv](https://arxiv.org/abs/2410.15270)] [[cool](https://papers.cool/arxiv/2410.15270)] [[pdf](https://arxiv.org/pdf/2410.15270)]
> **Authors**: Shiyu Hu,Xuchen Li,Xuzhao Li,Jing Zhang,Yipei Wang,Xin Zhao,Kang Hao Cheong
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: LVLM可以像人类一样描述视频吗？一个五合一视频注释基准，用于更好的人机比较
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉模型（LVLM）在解决复杂的视频任务方面取得了长足的进步，激发了研究人员对他们类似人类的多模式理解能力的兴趣。视频说明是评估视频理解的一项基本任务，需要对空间和时间动态有深入的了解，这给人类和机器带来了挑战。因此，研究LVLM是否可以像人类一样全面地描述视频（通过使用视频字幕作为代理任务进行合理的人机比较）将增强我们对这些模型的理解和应用。但是，当前用于视频理解的基准有明显的局限性，包括简短的视频持续时间，简短的注释以及对单个注释者的观点的依赖。这些因素阻碍了LVLMS理解复杂，冗长视频的能力的全面评估，并阻止建立坚固的人类基线，以准确反映人类的视频理解能力。为了解决这些问题，我们提出了一个新颖的基准Fiova（五合一视频注释），旨在更全面地评估LVLMS和人类理解之间的差异。 Fiova包括3,002个长的视频序列（平均33.6秒），涵盖具有复杂时空关系的各种情况。每个视频都由五个不同的注释者注释，捕获了广泛的观点，并导致字幕比现有基准长4-15倍，从而建立了强大的基线，该基线在视频描述任务中首次全面代表了人类的理解。使用FIOVA基准测试，我们对六个最先进的LVLM进行了深入评估，并将其与人类的表现进行了比较。可以在https://huuuuuusy.github.io/fiova/上找到更多详细的信息。

### LLaVA-Ultra: Large Chinese Language and Vision Assistant for Ultrasound 
[[arxiv](https://arxiv.org/abs/2410.15074)] [[cool](https://papers.cool/arxiv/2410.15074)] [[pdf](https://arxiv.org/pdf/2410.15074)]
> **Authors**: Xuechen Guo,Wenhao Chai,Shi-Yan Li,Gaoang Wang
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: Llava-Ultra：超声波的大型中文和视觉助手
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式大语言模型（MLLM）最近引起了人们的关注，这是一个著名的研究重点。通过利用强大的LLM，它促进了从单峰文本到执行多模式任务的对话生成AI的过渡。这种繁荣开始显着影响医疗领域。但是，通用视觉语言模型（VLM）缺乏对医学视觉问题答案（MED-VQA）的复杂理解。甚至专门针对医疗领域量身定制的模型也倾向于产生模糊的视觉相关性答案。在本文中，我们通过参数有效的调整提出了一种用于中国医学视觉对话的细粒自适应VLM架构。具体而言，我们设计了一个具有细粒视力编码器的融合模块，以增强微妙的医学视觉语义。然后，我们注意到大多数先前的工作中忽略了医疗场景常见的数据冗余。如果单个文本与多个图形配对，我们使用知识蒸馏的加权评分来适应有效的有效图像镜像文本描述。为了执行执行，我们利用了从医院获得的大规模多模式中国超声数据集。我们根据专业医生的文本创建指导关注数据，以确保有效调整。借助增强的模型和质量数据，我们的大型中文和视觉助理超声波（LLAVA-ULTRA）表现出强大的能力和鲁棒性，对医疗方案。在三个MED-VQA数据集上，Llava-Ultra超过了各种指标的先前最新模型。

### A General-Purpose Multimodal Foundation Model for Dermatology 
[[arxiv](https://arxiv.org/abs/2410.15038)] [[cool](https://papers.cool/arxiv/2410.15038)] [[pdf](https://arxiv.org/pdf/2410.15038)]
> **Authors**: Siyuan Yan,Zhen Yu,Clare Primiero,Cristina Vico-Alonso,Zhonghua Wang,Litao Yang,Philipp Tschandl,Ming Hu,Gin Tan,Vincent Tang,Aik Beng Ng,David Powell,Paul Bonnington,Simon See,Monika Janda,Victoria Mar,Harald Kittler,H. Peter Soyer,Zongyuan Ge
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: 56 pages; Technical report
- **标题**: 皮肤科多模型的通用多模式基础模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 诊断和治疗皮肤疾病需要跨多个领域的高级视觉技能，并具有从各种成像方式中综合信息的能力。当前的深度学习模型虽然在特定任务（例如从皮肤镜图像诊断皮肤癌）上有效，但在满足临床实践的复杂，多模式需求方面缺乏。在这里，我们介绍了Panderm，这是一种多模式皮肤病学基础模型，通过在超过200万个皮肤疾病的数据集中自学学习预测的皮肤病图像，从4种影像学方式中来自11个临床机构。我们评估了Panderm在28个不同的数据集上，涵盖了一系列临床任务，包括皮肤癌筛查，表型评估和风险分层，诊断为肿瘤和炎症性皮肤病，皮肤病变细分，变化监测以及转移预测和预测。 Panderm在所有评估的任务中都达到了最新的性能，即使仅使用5-10％的标记数据，也通常会优于现有模型。 Panderm的临床实用性是通过在多种成像方式的现实世界临床环境中的读者研究来证明的。在早期黑色素瘤检测准确性中，它的表现优于10.2％，并在协作的人类AI环境中提高了临床医生的多类皮肤癌诊断准确性。此外，Panderm在各种人口统计学因素中表现出了稳健的性能，包括不同的身体位置，年龄组，性别和肤色。基准评估和现实世界临床方案的强大结果表明，Panderm可以增强皮肤疾病的管理，并作为在其他医学专业中开发多模式基础模型的模型，从而有可能加快AI支持在医疗保健中的整合。

### Group Diffusion Transformers are Unsupervised Multitask Learners 
[[arxiv](https://arxiv.org/abs/2410.15027)] [[cool](https://papers.cool/arxiv/2410.15027)] [[pdf](https://arxiv.org/pdf/2410.15027)]
> **Authors**: Lianghua Huang,Wei Wang,Zhi-Fan Wu,Huanzhang Dou,Yupeng Shi,Yutong Feng,Chen Liang,Yu Liu,Jingren Zhou
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 组扩散变压器是无监督的多任务学习者
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管大型语言模型（LLMS）通过其任务不合时宜的功能彻底改变了自然语言处理，但视觉生成任务（例如图像翻译，样式传输和角色自定义）仍然很大程度上依赖于受监督的特定于任务特定的数据集。在这项工作中，我们介绍了群体扩散变压器（GDTS），这是一个新颖的框架，通过将它们重新定义为小组生成问题来统一不同的视觉生成任务。在这种方法中，同时生成了一组相关图像，可选地在组的子集中生成。 GDT通过跨图像串联自我注意力令牌来建立以最小的架构修饰为基础的扩散变压器。这使模型可以通过基于字幕的相关性隐式捕获跨图像关系（例如身份，样式，布局，环境和配色方案）。我们的设计实现了可扩展，无监督和任务不合时宜的预处理，并使用来自多模式互联网文章，图像画廊和视频框架的大量图像组集合。我们在全面的基准测试中评估了GDT，其中包含30个不同的视觉生成任务的200多种说明，包括图画书创建，字体设计，样式传输，素描，着色，绘图序列生成和角色自定义。我们的模型实现竞争性的零击性能，而无需任何其他微调或梯度更新。此外，消融研究证实了关键组件（例如数据缩放，小组大小和模型设计）的有效性。这些结果证明了GDT作为可扩展的通用视觉生成系统的潜力。

### MambaSOD: Dual Mamba-Driven Cross-Modal Fusion Network for RGB-D Salient Object Detection 
[[arxiv](https://arxiv.org/abs/2410.15015)] [[cool](https://papers.cool/arxiv/2410.15015)] [[pdf](https://arxiv.org/pdf/2410.15015)]
> **Authors**: Yue Zhan,Zhihong Zeng,Haijun Liu,Xiaoheng Tan,Yinli Tian
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: MAMBASOD：用于RGB-D显着对象检测的双Mamba驱动的跨模式融合网络
- **领域**: 计算机视觉和模式识别
- **摘要**: RGB-D显着对象检测（SOD）的目的是准确地查明图像中最显眼的区域。尽管传统的深层模型在很大程度上依赖CNN提取器并忽略了远距离上下文依赖性，但随后的基于变压器的模型在某种程度上解决了问题，但引入了高计算复杂性。此外，从深度图中合并空间信息已被证明对这项任务有效。这个问题的主要挑战是如何有效地融合来自RGB和深度的互补信息。在本文中，我们提出了一个由RGB-D SOD的双重MAMBA驱动的跨模式融合网络，名为Mambasod。具体而言，我们首先对RGB和深度使用双MAMBA驱动的特征提取器来对具有线性复杂性的多种模态输入中的远程依赖性进行建模。然后，我们为捕获的多模式特征设计了一个跨模式融合Mamba，以完全利用RGB和深度特征之间的互补信息。据我们所知，这项工作是第一次尝试在RGB-D SOD任务中探索Mamba的潜力，并提供一种新颖的观点。在六个流行数据集上进行的许多实验证明了我们的方法比16个最先进的RGB-D SOD模型的优越性。源代码将在https://github.com/yuezhan721/mambasod上发布。

### Making Every Frame Matter: Continuous Video Understanding for Large Models via Adaptive State Modeling 
[[arxiv](https://arxiv.org/abs/2410.14993)] [[cool](https://papers.cool/arxiv/2410.14993)] [[pdf](https://arxiv.org/pdf/2410.14993)]
> **Authors**: Hao Wu,Donglin Bai,Shiqi Jiang,Qianxi Zhang,Yifan Yang,Ting Cao,Fengyuan Xu
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 使每个框架都重要：通过自适应状态建模对大型模型的连续视频理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着多模式应用的兴起，视频理解变得越来越重要。由于流媒体视频的快速扩展，理解连续的视频构成了巨大的挑战，该视频包含多尺度和未修剪事件。我们介绍了一种新型系统C-Vue，以通过自适应状态建模来克服这些问题。 C-Vue具有三个关键设计。第一个是一种远程历史建模技术，它使用视频感知方法保留历史视频信息。第二个是一种空间冗余技术，可提高基于时间关系的历史建模效率。第三个是平行的训练结构，结合了框架加权损失，以了解长视频中的多尺度事件。我们的C-Vue提供了高准确性和效率。它以典型的边缘设备上的速度> 30 fps运行，并且精确度优于所有基准。此外，在我们的案例研究中，将C-Vue应用于视频基础模型作为视频编码器，在分布数据集上实现了0.46点的增强（以5分制），在零照片数据集上的改善范围从1.19 \％到4 \％。

### ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla 
[[arxiv](https://arxiv.org/abs/2410.14991)] [[cool](https://papers.cool/arxiv/2410.14991)] [[pdf](https://arxiv.org/pdf/2410.14991)]
> **Authors**: Deeparghya Dutta Barua,Md Sakib Ul Rahman Sourove,Md Farhan Ishmam,Fabiha Haider,Fariha Tanjim Shifat,Md Fahim,Md Farhad Alam
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: Chitrojera：一个与区域相关的视觉问题答复孟加拉的数据集
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉问题答案（VQA）提出了回答有关视觉上下文的自然语言问题的问题。尽管缺乏适当的基准数据集，但孟加拉国尽管是一种说话的语言，但在VQA领域被认为是低资源。缺乏此类数据集挑战已知在其他语言中表现的模型。此外，现有的Bangla VQA数据集几乎没有文化相关性，并且在很大程度上是根据其外国对应物进行的。为了应对这些挑战，我们介绍了一个名为Chitrojera的大规模Bangla VQA数据集，总计超过15K样品，其中使用了各种各样和本地相关的数据源。我们评估文本编码器，图像编码器，多模式模型的性能以及我们的新颖双重编码模型。实验表明，预训练的双重编码器的表现优于其规模的其他模型。我们还使用基于及时的技术来评估大语言模型（LLMS）的性能，而LLMS可以实现最佳性能。鉴于现有数据集的不发达状态，我们设想Chitrojera扩大了孟加拉的视力语言任务范围。

### Reflexive Guidance: Improving OoDD in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation 
[[arxiv](https://arxiv.org/abs/2410.14975)] [[cool](https://papers.cool/arxiv/2410.14975)] [[pdf](https://arxiv.org/pdf/2410.14975)]
> **Authors**: Jihyo Kim,Seulbi Lee,Sangheum Hwang
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: Accepted at ICLR 2025. The first two authors contributed equally
- **标题**: 反思性指导：通过自我引导的图像自适应概念生成改善视觉模型中的OODD
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着基础模型最近在互联网规模数据中训练并证明了非凡的概括能力，这种基础模型已被更广泛地采用，从而导致了一系列应用程序域。尽管存在这种快速扩散，但基础模型的可信度仍然没有得到充实。具体而言，大型视力语言模型（LVLMS）（例如GPT-4O）的分布外检测（OODD）功能尚未得到充分解决。它们所表现出的潜力和实际可靠性之间的差异引起了人们对基础模型安全和值得信赖的部署的担忧。为了解决这一差距，我们评估和分析了各种专有和开源LVLM的OODD功能。我们的调查有助于更好地理解这些基础模型如何通过产生的自然语言反应来代表信心得分。此外，我们提出了一种自我引入的提示方法，称为反思性指导（Geyuide），旨在通过利用自我生成的图像自适应概念建议来增强LVLM的OODD能力。实验结果表明，我们的研究可以提高图像分类和OODD任务中当前LVLM的性能。采样图像的列表以及每个示例的提示和响应可在https://github.com/daintlab/reguide上找到。

### Part-Whole Relational Fusion Towards Multi-Modal Scene Understanding 
[[arxiv](https://arxiv.org/abs/2410.14944)] [[cool](https://papers.cool/arxiv/2410.14944)] [[pdf](https://arxiv.org/pdf/2410.14944)]
> **Authors**: Yi Liu,Chengxin Li,Shoukun Xu,Jungong Han
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 零件的关系融合到多模式场景的理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式融合在多模式场景的理解中起着至关重要的作用。大多数现有的方法都集中在涉及两种模式的跨模式融合上，通常忽略了更复杂的多模式融合，这对于使用自动驾驶（例如可见，深度，事件，事件，激光雷达等）等现实世界中必不可少。此外，很少有尝试多模式融合的尝试，\ emph {e.g。}，简单的串联，跨模式关注和令牌选择，无法很好地研究多种模态的内在共享和特定细节。为了应对挑战，在本文中，我们提出了一个整体关系融合（PWRF）框架。该框架首次将多模式融合视为部分关系融合。它使用胶囊网络（CAPSNETS）的零件关系路由能力将多个单个零件级模式路由到融合的全级模式。通过此零件整个路由，我们的PWRF分别从整个级模胶和路由系数中生成模态共享和模态特定的语义。最重要的是，可以使用模态共享和模态特定的细节来解决多模式场景理解的问题，包括合成的多模式分割和本文中可见的深度对象检测。几个数据集的实验证明了提出的PWRF框架对多模式场景的理解的优越性。源代码已在https://github.com/liuyi1989/pwrf上发布。

### Multi-modal Pose Diffuser: A Multimodal Generative Conditional Pose Prior 
[[arxiv](https://arxiv.org/abs/2410.14540)] [[cool](https://papers.cool/arxiv/2410.14540)] [[pdf](https://arxiv.org/pdf/2410.14540)]
> **Authors**: Calvin-Khang Ta,Arindam Dutta,Rohit Kundu,Rohit Lal,Hannah Dela Cruz,Dripta S. Raychaudhuri,Amit Roy-Chowdhury
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 多模式姿势扩散器：多模式生成条件姿势先验
- **领域**: 计算机视觉和模式识别
- **摘要**: 皮肤多人线性（SMPL）模型在3D人类姿势估计中起着至关重要的作用，提供了人体的简化而有效的表示。但是，确保在人类网格回归等任务期间SMPL配置的有效性仍然是一个重大挑战，这突出了有能力辨别现实人类姿势的强大人姿势的必要性。为了解决这个问题，我们介绍了拖把：\下划线{m} ulti-m \下划线{o} dal \ dal \ undesline {p} os \ undesline {e} \ lisepline {d lisepline {d} iffuser。 Moped是第一种利用新型多模式条件扩散模型作为SMPL姿势参数的先验方法。我们的方法提供了强大的无条件姿势生成，能够在多模式输入（例如图像和文本）上进行条件。这种能力通过纳入传统姿势先验中经常忽略的其他上下文来增强我们的方法的适用性。对三个不同的任务估计，姿势降解和姿势完成的广泛实验表明，基于多模式扩散模型的先验显着优于现有方法。这些结果表明，我们的模型捕获了更广泛的人类姿势。

### Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension 
[[arxiv](https://arxiv.org/abs/2410.14332)] [[cool](https://papers.cool/arxiv/2410.14332)] [[pdf](https://arxiv.org/pdf/2410.14332)]
> **Authors**: Yin Xie,Kaicheng Yang,Ninghua Yang,Weimo Deng,Xiangzi Dai,Tiancheng Gu,Yumeng Wang,Xiang An,Yongle Zhao,Ziyong Feng,Roy Miles,Ismail Elezi,Jiankang Deng
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: 14 pages, 12 figures
- **标题**: CROC：具有跨模式理解的大型多模型预处理
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型语言模型（LLM）的最新进展促进了大型多模型（LMM）的发展。但是，现有研究主要集中于调整语言和图像说明，而忽略了模型学习共同处理文本和视觉方式的关键训练阶段。在本文中，我们为LMM提出了一种新的预处理范式，以通过引入新型的跨模式理解阶段来增强LLMS的视觉理解能力。具体来说，我们设计了一个动态学习的提示令牌池，并使用匈牙利算法用最相关的提示令牌代替了原始视觉令牌的一部分。然后，我们将视觉令牌概念化为类似于LLM的“外语”，并提出了一种具有双向视觉注意力的混合注意机制和单向文本关注，以全面增强对视觉令牌的理解。同时，我们集成了一项详细的字幕生成任务，利用丰富的描述进一步促进LLMS了解视觉语义信息。在对150万个公开访问数据进行了预处理后，我们提出了一种名为Croc的新基础模型。实验结果表明，CROC在大型视觉语言基准上实现了新的最新性能。为了支持可重复性并促进进一步的研究，我们在https://github.com/deepglint/croc上发布了培训代码和预训练的模型权重。

### Advanced Underwater Image Quality Enhancement via Hybrid Super-Resolution Convolutional Neural Networks and Multi-Scale Retinex-Based Defogging Techniques 
[[arxiv](https://arxiv.org/abs/2410.14285)] [[cool](https://papers.cool/arxiv/2410.14285)] [[pdf](https://arxiv.org/pdf/2410.14285)]
> **Authors**: Yugandhar Reddy Gogireddy,Jithendra Reddy Gogireddy
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 高级水下图像质量增强通过混合超分辨率卷积神经网络和基于多尺度视网膜的脱水技术
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在本研究报告中讨论了水下图像降解，导致低分辨率和可见度差的雾状颗粒引起的困难。我们建议一种复杂的混合策略，该策略将多尺度视网膜（MSR）融化方法与超分辨率卷积神经网络（SRCNN）结合在一起，以解决这些问题。 Etinex算法模仿人类的视觉感知，以减少不均匀的照明和雾化，而SRCNN组件改善了水下照片的空间分辨率。通过这些方法的组合，我们能够增强清晰度，对比度，对比度和颜色恢复水下图像，从而为可靠的途径提供了可靠的图像质量，使图像质量不足。该研究对实际水下数据集进行了广泛的实验，以进一步说明建议方法的功效。在清晰度，可见性和特征保留方面，使用诸如结构相似性指数量度（SSIM）和峰值信噪比之类的定量评估（PSNR）在常规技术上表现出显着的进步。在实时的水下应用等实时的水下应用，例如海洋探索，海洋水平的机器人和自动级别的水下和自动级别的水平和高度验证，并在水下和自动范围内进行了验证，并验证了高水平的水平，并有效地验证了高水平的范围。深度学习和常规图像处理技术提供了一个具有卓越结果的计算高效框架。

### PlaneSAM: Multimodal Plane Instance Segmentation Using the Segment Anything Model 
[[arxiv](https://arxiv.org/abs/2410.16545)] [[cool](https://papers.cool/arxiv/2410.16545)] [[pdf](https://arxiv.org/pdf/2410.16545)]
> **Authors**: Zhongchen Deng,Zhechen Yang,Chi Chen,Cheng Zeng,Yan Meng,Bisheng Yang
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: submitted to Information Fusion
- **标题**: Planesam：使用该段的任何模型的多模式平面实例分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 来自RGB-D数据的平面实例分割是许多下游任务的关键研究主题。但是，大多数现有的基于深度学习的方法仅利用RGB频段内的信息，从而忽略了深度频段在平面实例分割中的重要作用。基于SAM的快速版本，我们提出了一个名为Planesam的平面实例分割网络，该网络可以完全整合RGB频段（光谱频段）和D频段（几何频段）的信息，从而提高了平面实例分割的有效性，以多模态方式。具体来说，我们使用双重复合主链，主要具有简单的分支学习D波段功能，主要是更复杂的分支学习RGB波段特征。因此，即使D-band训练数据的规模限制，保留功能强大的RGB波段特征表示，骨干也可以有效地学习D波段特征表示，并允许原始的骨干分支在当前任务中进行微调。为了增强飞机对RGB-D结构域的适应性，我们使用基于不完善的伪标签的自我监督预处理策略在大规模RGB-D数据上使用该段进行双重复杂性主链预算了任何任务。为了支持大平面的分割，我们优化了效能函数的损耗函数组合率。此外，更快的R-CNN用作平面检测器，其预测的边界框作为提示送入我们的双重复合网络中，从而实现了全自动平面实例分割。实验结果表明，所提出的飞机在扫描仪数据集上设置了新的SOTA性能，并且在2D-3D-S，MatterPort3D和ICL-NUIM RGB-D数据集上以先前的SOTA方法优于先前的SOTA方法，而与EfficiCention相比，计算上的间接费用增加了10％。

### Joker: Conditional 3D Head Synthesis with Extreme Facial Expressions 
[[arxiv](https://arxiv.org/abs/2410.16395)] [[cool](https://papers.cool/arxiv/2410.16395)] [[pdf](https://arxiv.org/pdf/2410.16395)]
> **Authors**: Malte Prinzler,Egor Zakharov,Vanessa Sklyarova,Berna Kabadayi,Justus Thies
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: Project Page: https://malteprinzler.github.io/projects/joker/
- **标题**: 小丑：有条件的3D头部合成，具有极端的面部表情
- **领域**: 计算机视觉和模式识别,图形
- **摘要**: 我们介绍了Joker，这是一种具有极端表达的3D人头有条件合成的新方法。给定一个人的单个参考图像，我们合成具有参考身份和新表达式的体积人头。我们通过3D形态模型（3DMM）和文本输入来控制表达式。这种多模式调节信号至关重要，因为仅3DMMS就无法定义细微的情绪变化和极端表达，包括涉及口腔和舌头表达的那些。我们的方法建立在基于2D扩散的先验基础上，该先验可以很好地推广到造型，例如雕塑，厚实的妆容和绘画，同时达到了高水平的表现力。为了提高视图一致性，我们提出了一种新的3D蒸馏技术，将我们的2D先验的预测转换为神经辐射场（NERF）。 2D先验和我们的蒸馏技术都产生了最先进的结果，这通过我们的广泛评估证实。同样，据我们所知，我们的方法是第一个实现一致的极端舌头表达的方法。

### xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs 
[[arxiv](https://arxiv.org/abs/2410.16267)] [[cool](https://papers.cool/arxiv/2410.16267)] [[pdf](https://arxiv.org/pdf/2410.16267)]
> **Authors**: Michael S. Ryoo,Honglu Zhou,Shrikant Kendre,Can Qin,Le Xue,Manli Shu,Silvio Savarese,Ran Xu,Caiming Xiong,Juan Carlos Niebles
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: XGEN-MM-VID（BLIP-3-VIDEO）：即使在VLMS中，您只需要32个令牌即可表示视频
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 我们提出XGEN-MM-VID（BLIP-3-VIDEO）：视频的多模式模型，尤其是旨在通过多个帧有效捕获时间信息。 Blip-3-Video除了传统的视觉令牌外，还利用了“颞编码器”，该图形将多个令牌上的一系列令牌映射到一组紧凑的视觉令牌集中。这使Blip3-Video的视觉令牌比其竞争模型使用少得多（例如32 vs. 4608令牌）。我们探索了不同类型的时间编码器，包括可学习的时空池以及诸如Token Turing Machines之类的顺序模型。我们通过实验证实，BLIP-3-VIDEO获得了与更大的最新模型（例如34b）相当的视频提问精确度，而使用较少的视觉令牌则较小（即4B），并且更有效。该项目网站位于https://www.salesforceairesearch.com/opensource/xgen-mm-mm-vid/index.html

### Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance 
[[arxiv](https://arxiv.org/abs/2410.16261)] [[cool](https://papers.cool/arxiv/2410.16261)] [[pdf](https://arxiv.org/pdf/2410.16261)]
> **Authors**: Zhangwei Gao,Zhe Chen,Erfei Cui,Yiming Ren,Weiyun Wang,Jinguo Zhu,Hao Tian,Shenglong Ye,Junjun He,Xizhou Zhu,Lewei Lu,Tong Lu,Yu Qiao,Jifeng Dai,Wenhai Wang
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: Technical report
- **标题**: 迷你 - 内列列：具有5％参数和90％性能的灵活转移袋多模式模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大型语言模型（MLLM）在各种领域的视觉任务中表现出了令人印象深刻的表现。但是，大型模型量表和相关的高计算成本在培训和部署消费级GPU或边缘设备上构成了重大挑战，从而阻碍了他们的广泛应用。在这项工作中，我们引入了Mini-InternVL，这是一系列参数范围从1B到4B的MLLM，仅5％的参数可实现90％的性能。效率和有效性的重大提高使我们的模型更容易访问和适用于各种现实世界中的情况。为了进一步促进我们的模型的采用，我们为迷你互动开发了一个统一的适应框架，这使我们的模型能够在下游任务中转移和胜过专业模型，包括自动驾驶，医疗图像和遥感。我们认为，我们的研究可以提供宝贵的见解和资源，以推动高效有效的MLLM的发展。代码可在https://github.com/opengvlab/internvl上找到。

### LLaVA-KD: A Framework of Distilling Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.16236)] [[cool](https://papers.cool/arxiv/2410.16236)] [[pdf](https://arxiv.org/pdf/2410.16236)]
> **Authors**: Yuxuan Cai,Jiangning Zhang,Haoyang He,Xinwei He,Ao Tong,Zhenye Gan,Chengjie Wang,Xiang Bai
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: Under review
- **标题**: llava-kd：蒸馏多模式模型的框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型语言模型（LLM）的成功使研究人员探索了多模式的大型语言模型（MLLM），以实现统一的视觉和语言理解。但是，MLLM的模型大小和计算复杂性的增加限制了它们在资源受限环境中的使用。小型MLLM（S-MLLM）旨在保留大型模型（L-MLLM）的能力，同时减少计算需求，但导致性能显着下降。为了解决上述问题，我们提出了一个新颖的LLAVA-KD框架，将知识从L-MLLM转移到S-MLLM。具体而言，我们引入了多模式蒸馏（MDIST），以最大程度地减少L-MLLM和S-MLLM的视觉文本输出分布与关系蒸馏（RDIST）之间的差异，以传递L-MLLM在视觉特征之间建模相关性的能力。此外，我们提出了一个三阶段训练方案，以充分利用S-MLLM的潜力：1）蒸馏预训练以使视觉文本表示形式对齐，2）监督微调以使该模型具有多模式理解，而3）蒸馏微调以进一步传递L-MLLM功能。我们的方法大大提高了性能，而无需改变小型模型的体系结构。广泛的实验和消融研究验证了每个提出的成分的有效性。代码将在https://github.com/fantasyele/llava-kd上找到。

### Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM Pretraining 
[[arxiv](https://arxiv.org/abs/2410.16166)] [[cool](https://papers.cool/arxiv/2410.16166)] [[pdf](https://arxiv.org/pdf/2410.16166)]
> **Authors**: Han Huang,Yuqi Huo,Zijia Zhao,Haoyu Lu,Shu Wu,Bingning Wang,Qiang Liu,Weipeng Chen,Liang Wang
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 除了过滤外：MLLM预处理的自适应图像文本质量增强
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式大型语言模型（MLLM）通过整合视觉和文本方式取得了重大步伐。训练MLLM的关键因素是多模式预处理数据集中图像文本对的质量。但是，由于图像和文本之间的语义一致性不足，导致数据利用率和可扩展性的效率低下，因此$ \ textit {de facto} $基于滤波器的数据质量增强范式通常会丢弃高质量图像数据的大部分。在本文中，我们提出了自适应图像文本质量增强剂（AITQE），该模型可以动态评估和增强图像文本对的质量。 AITQE采用文本重写机制来进行低质量对，并结合了负面样本学习策略，以通过在训练过程中整合故意选择的低质量样本来提高评估能力。与以前的方法显着改变文本分布不同，我们的方法最小化调整文本以保留数据量，同时提高质量。实验结果表明，AITQE超过了各种基准上的现有方法，有效利用原始数据并随着数据量增加而有效地扩展。我们希望我们的工作能够激发未来的工作。代码和模型可在以下网址提供：https：//github.com/hanhuang22/aitqe。

### Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.16163)] [[cool](https://papers.cool/arxiv/2410.16163)] [[pdf](https://arxiv.org/pdf/2410.16163)]
> **Authors**: Yufei Zhan,Hongyin Zhao,Yousong Zhu,Fan Yang,Ming Tang,Jinqiao Wang
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: This work has been submitted to the IEEE for possible publication. Codes and data will be later released at https://github.com/jefferyZhan/Griffon
- **标题**: Griffon-G：通过大型多模型的桥接视觉语言和以视觉为中心的任务
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）在基于自动回归建模的各种视觉语言和以视觉为中心的任务中取得了重大突破。但是，这些模型通常集中于以视觉为中心的任务，例如视觉接地和区域描述，或视觉语言任务，例如图像标题和多幕纳里奥VQA。在自然语言处理领域中的大语言模型中可以看到，LMM尚未全面统一两种类型的任务。此外，即使有了丰富的多任务指令跟随数据，直接将这些数据堆叠以进行通用功能扩展仍然具有挑战性。为了解决这些问题，我们介绍了一种新型的多维策划和合并的多模式数据集，名为CCMD-8M，该数据集通过多级数据策划和多任务巩固来克服统一以视觉和视觉语言任务的数据障碍。更重要的是，我们提出了Griffon-G，这是一个通用的大型多模式模型，该模型旨在解决单一端到端范式内的以视觉为中心和视觉语言任务。 Griffon-G解决了这些任务联合优化期间遇到的训练崩溃问题，从而提高了培训效率。跨多模式基准测试，一般视觉问题答案（VQA）任务，场景文本中心的VQA任务，与文档相关的VQA任务，参考表达理解和对象检测的评估表明，Griffon-G超过了高级LMM并在复杂的视力千分位任务中实现了专家级别的表现。

### LMHaze: Intensity-aware Image Dehazing with a Large-scale Multi-intensity Real Haze Dataset 
[[arxiv](https://arxiv.org/abs/2410.16095)] [[cool](https://papers.cool/arxiv/2410.16095)] [[pdf](https://arxiv.org/pdf/2410.16095)]
> **Authors**: Ruikun Zhang,Hao Yang,Yan Yang,Ying Fu,Liyuan Pan
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: LMHAZE：强度感知图像使用大型多强度真实雾化数据集进行除去
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，Dobing Dohazing引起了人们的重大关注。基于学习的方法通常需要配对的朦胧和相应的地面真理（无薄雾）图像进行训练。但是，很难收集现实世界的图像对，从而阻止了现有方法的发展。尽管有几项作品通过使用合成数据集或小规模的真实数据集可以部分缓解此问题。现有数据集中的雾化强度分布偏差和场景均匀性限制了这些方法的概括能力，尤其是在遇到具有先前看不见的雾化强度的图像时。在这项工作中，我们提出了Lmhaze，这是一个大型，高质量的现实数据集。 Lmhaze包括在室内和室外环境中捕获的朦胧和无雾图像，涵盖了多种情况和雾霾强度。它包含超过5k的高分辨率图像对，超过了现有的现实世界脱掩的数据集的大小超过25次。同时，为了更好地处理具有不同雾化强度的图像，我们提出了基于MAMBA（MOE-MAMBA）的杂物模型，以进行脱掩和，该模型根据雾化强度动态调节模型参数。此外，借助我们提出的数据集，我们进行了新的大型多模型（LMM）基准研究，以模拟人类的看法，以评估除壳图像。实验表明，LMHAZE数据集在实际场景中提高了飞行性能，与最新方法相比，我们的脱掩护方法提供了更好的结果。

### Robust Visual Representation Learning with Multi-modal Prior Knowledge for Image Classification Under Distribution Shift 
[[arxiv](https://arxiv.org/abs/2410.15981)] [[cool](https://papers.cool/arxiv/2410.15981)] [[pdf](https://arxiv.org/pdf/2410.15981)]
> **Authors**: Hongkuan Zhou,Lavdim Halilaj,Sebastian Monka,Stefan Schmid,Yuqicheng Zhu,Bo Xiong,Steffen Staab
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 强大的视觉表示学习，具有多模式的先验知识，用于分配转移下的图像分类
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 尽管深度神经网络（DNN）在计算机视觉中取得了显着的成功，但在面对训练和测试数据之间的分配变化时，它们仍未表现出色。在本文中，我们提出了知识引导的视觉表示学习（KGV） - 一种基于分布的学习方法利用多模式的先验知识 - 以改善分布变化下的概括。它从两种不同的方式中整合了知识：1）具有分层和关联关系的知识图（kg）； 2）在kg中以语义表示的视觉元素的合成图像。相应的嵌入是由常见的潜在空间中给定的模态生成的，即来自原始图像和合成图像以及知识图嵌入（KGE）的视觉嵌入。这些嵌入是通过基于翻译的KGE方法的新型变体对齐的，其中kg的节点和关系嵌入分别为高斯分布和翻译。我们声称，合并多模型的先验知识可以使图像表示的更正规化学习。因此，模型能够更好地概括不同的数据分布。我们在不同的图像分类任务上评估了KGV，这些任务具有重大或次要分配的变化，即来自德国，中国和俄罗斯的数据集，与Mini-Imagenet数据集及其变体以及DVM-CAR数据集进行了图像分类。结果表明，KGV始终在所有实验中表现出更高的准确性和数据效率。

### Mitigating Object Hallucination via Concentric Causal Attention 
[[arxiv](https://arxiv.org/abs/2410.15926)] [[cool](https://papers.cool/arxiv/2410.15926)] [[pdf](https://arxiv.org/pdf/2410.15926)]
> **Authors**: Yun Xing,Yiheng Li,Ivan Laptev,Shijian Lu
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: To appear at NeurIPS 2024. Code is available at https://github.com/xing0047/cca-llava
- **标题**: 通过同心因果注意来减轻物体幻觉
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 近期大型视觉语言模型（LVLM）提出了具有多模式查询的显着零拍对性和推理功能。然而，它们遭受了对象幻觉的困扰，这是一种现象，其中lvlms容易产生与图像输入不符的文本响应。我们的试点研究表明，物体幻觉与旋转位置编码（绳索）紧密相关，这是现有LVLM中广泛采用的位置依赖建模设计。由于绳索的长期衰减，当相关的视觉提示与多模式输入序列中的指导令牌相距甚远时，LVLM倾向于幻觉更多。此外，在逆转多模式比对期间的视觉令牌顺序时，我们会观察到类似的效果。我们的测试表明，绳索的长期衰减对LVLM构成了挑战，同时捕获长距离的视觉指导相互作用。我们提出了同心因果注意（CCA），这是一种简单而有效的位置对准策略，通过自然降低视觉和指导令牌之间的相对距离，从而减轻绳索长期衰减的影响。使用CCA，视觉令牌可以更好地与指令令牌进行交互，从而增强了模型的感知能力和减轻对象幻觉的能力。如果没有铃铛和哨声，我们的位置对准方法在多个物体幻觉基准上通过大幅度的大幅度超过了现有的减轻幻觉策略。

### LiOn-XA: Unsupervised Domain Adaptation via LiDAR-Only Cross-Modal Adversarial Training 
[[arxiv](https://arxiv.org/abs/2410.15833)] [[cool](https://papers.cool/arxiv/2410.15833)] [[pdf](https://arxiv.org/pdf/2410.15833)]
> **Authors**: Thomas Kreutz,Jens Lemke,Max Mühlhäuser,Alejandro Sanchez Guinea
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: Preprint, Paper has been accepted at IROS2024
- **标题**: Lion-XA：无监督的域通过仅激光镜像对抗训练的适应
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们提出了Lion-XA，这是一种无监督的域适应（UDA）方法，该方法将仅激光雷达的跨模式（X）学习与3D激光点云语义分段的对抗性训练相结合，以弥合由环境和传感器设置而引起的域间隙。与利用多种数据模式（例如点云和RGB图像数据）的现有作品不同，我们在可能无法使用RGB图像的情况下解决UDA，并表明两个不同的LIDAR数据表示可以为UDA互相学习。更具体地说，我们利用3D Voxelized点云将重要的几何结构与2D投影的范围图像结合使用，这些范围图像提供了诸如对象方向或表面之类的信息。为了进一步调整两个域之间的特征空间，我们使用2D和3D神经网络的特征和预测应用对抗训练。我们对3种真实适应情景的实验证明了我们方法的有效性，与以前的Uni-和Multi-Model UDA方法相比，实现了新的最新性能。我们的源代码可在https://github.com/jensle97/lion-xa上公开获得。

### WildOcc: A Benchmark for Off-Road 3D Semantic Occupancy Prediction 
[[arxiv](https://arxiv.org/abs/2410.15792)] [[cool](https://papers.cool/arxiv/2410.15792)] [[pdf](https://arxiv.org/pdf/2410.15792)]
> **Authors**: Heng Zhai,Jilin Mei,Chen Min,Liang Chen,Fangzhou Zhao,Yu Hu
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: WILDOCC：越野3D语义占用预测的基准
- **领域**: 计算机视觉和模式识别,人工智能,机器人技术
- **摘要**: 3D语义占用预测是自动驾驶的重要组成部分，重点是捕获场景的几何细节。越野环境富含几何信息，因此它适用于重建此类场景的3D语义占用预测任务。但是，大多数研究都集中在公路环境上，由于缺乏相关的数据集和基准，很少有用于越野3D语义占用预测的方法。据我们所知，我们介绍了WildoCC，这是第一个为越野3D语义占用预测任务提供密集占用注释的基准。本文提出了一条地面真相产生的管道，该管道采用粗到精细的重建来实现更现实的结果。此外，我们引入了一个多模式3D语义占用预测框架，该框架从多帧图像和Voxel级别的点云中融合了时空信息。另外，还引入了跨模式蒸馏函数，该功能将几何知识从点云传输到图像特征。

### Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment 
[[arxiv](https://arxiv.org/abs/2410.15744)] [[cool](https://papers.cool/arxiv/2410.15744)] [[pdf](https://arxiv.org/pdf/2410.15744)]
> **Authors**: Yankai Jiang,Wenhui Lei,Xiaofan Zhang,Shaoting Zhang
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: Accepted as ICLR 2025 conference paper
- **标题**: 通过mask-atribute对齐释放3D零射门病变分割的视觉预训练的潜力
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 医学视觉培训预训练模型的最新进展已促进了零照片识别的显着进步。但是，将图像级知识转移到像素级任务（例如3D CT扫描中的病变分割）仍然是一个至关重要的挑战。由于病理视觉特征的复杂性和可变性，现有方法难以将与疾病相关的文本表示培训期间未遇到细粒细胞病变特征。在本文中，我们提出了一种新型的多尺度病变级面膜 - 属性对准框架，该框架专为3D零摄像病变分割而设计。 Malenia改善了掩模表示及其相关的元素属性之间的兼容性，明确地将看不见病变的视觉特征与从先前见过的知识中学到的可扩展知识联系起来。此外，我们设计了一个跨模式知识注入模块，以增强具有互惠信息的视觉和文本特征，从而有效地指导分割结果的产生。跨三个数据集和12个病变类别的全面实验验证了麦伦尼亚的出色表现。

### LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding 
[[arxiv](https://arxiv.org/abs/2410.17434)] [[cool](https://papers.cool/arxiv/2410.17434)] [[pdf](https://arxiv.org/pdf/2410.17434)]
> **Authors**: Xiaoqian Shen,Yunyang Xiong,Changsheng Zhao,Lemeng Wu,Jun Chen,Chenchen Zhu,Zechun Liu,Fanyi Xiao,Balakrishnan Varadarajan,Florian Bordes,Zhuang Liu,Hu Xu,Hyunwoo J. Kim,Bilge Soran,Raghuraman Krishnamoorthi,Mohamed Elhoseiny,Vikas Chandra
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: Project page: https://vision-cair.github.io/LongVU
- **标题**: Longvu：时空自适应压缩
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在理解和分析视频内容方面表现出了有希望的进展。但是，在LLM的上下文规模限制的长期视频仍然是一个重大挑战。为了解决这一限制，我们提出了Longvu，这是一种时空的自适应压缩机制，这减少了视频令牌的数量，同时保留了长视频的视觉细节。我们的想法是基于利用跨模式查询和框架间依赖性，以适应视频中的时间和空间冗余。具体来说，我们利用Dinov2功能去除表现出很高相似性的冗余框架。然后，我们利用文本引导的跨模式查询来减少选择性框架功能。此外，我们根据其时间依赖性进行跨帧的空间令牌减小。我们的自适应压缩策略有效地处理了大量框架，而在给定上下文长度内则很少有视觉信息丢失。我们的Longvu始终超过各种视频理解基准测试的现有方法，尤其是在长达一个小时的视频理解任务上，例如视频和MLVU。鉴于轻巧的LLM，我们的Longvu还可以有效地缩放到较小的尺寸，并具有最先进的视频理解性能。

### Towards Real Zero-Shot Camouflaged Object Segmentation without Camouflaged Annotations 
[[arxiv](https://arxiv.org/abs/2410.16953)] [[cool](https://papers.cool/arxiv/2410.16953)] [[pdf](https://arxiv.org/pdf/2410.16953)]
> **Authors**: Cheng Lei,Jie Fan,Xinran Li,Tianzhu Xiang,Ao Li,Ce Zhu,Le Zhang
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: No comments
- **标题**: 迈向真正的零摄像的伪装对象进行分割，而无需伪装注释
- **领域**: 计算机视觉和模式识别
- **摘要**: 伪装的物体细分（COS）由于缺乏带注释的数据而面临重大挑战，其中细致的像素级注释既劳动密集型又昂贵，这主要是由于复杂的对象背景边界。解决核心问题：“可以在没有手动注释的任何伪装物体的情况下以零拍的方式有效地实现cos？”我们肯定地做出了响应并引入了强劲的零拍摄cos框架。该框架利用COS的固有局部模式偏置，并采用了从显着对象分割（SOS）得出的广泛的语义特征空间进行有效的零拍传输。我们合并了针对参数有效的微调（PEFT），多模式大语言模型（M-LLLM）和多规模的细粒度比对（MFA）机制优化的基于蒙版的图像建模（MIM）图像编码器。 MIM预先训练的图像编码器专注于捕获基本的低级特征，而M-LLM生成了与这些视觉提示一起处理的字幕嵌入。这些嵌入使用MFA精确对齐，使我们的框架能够准确地解释和浏览复杂的语义上下文。为了优化操作效率，我们引入了一本可学习的代码簿，该代码本在推理过程中代表M-LLM，从而大大降低了计算开销。我们的框架通过严格的实验证明了其多功能性和功效，在迷彩上以$f_β^w $得分为72.9％，在COD10K上达到71.7 \％，在零拍摄的cos中实现了最新性能。通过在推理过程中删除M-LLM，我们达到了与传统端到端模型相当的推理速度，达到18.1 fps。代码：https：//github.com/r-lei360725/zscos-camf

### Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks 
[[arxiv](https://arxiv.org/abs/2410.18387)] [[cool](https://papers.cool/arxiv/2410.18387)] [[pdf](https://arxiv.org/pdf/2410.18387)]
> **Authors**: Lehan Wang,Haonan Wang,Honglong Yang,Jiaji Mao,Zehong Yang,Jun Shen,Xiaomeng Li
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: Technical Report
- **标题**: 可解释的双语多模式大型语言模型，用于多种生物医学任务
- **领域**: 计算机视觉和模式识别
- **摘要**: 已经开发了几种医学多模式大型模型（MLLM），以解决涉及各种医学方式的文本说明的视觉图像的任务，从而取得了令人印象深刻的结果。当前的大多数医学通才模型是区域不可能的，将整个图像视为整体代表。但是，他们努力确定产生句子时要关注的特定区域。为了模仿医生的行为，他们通常是在专注于特定区域以进行彻底评估之前对整个图像进行审查，我们旨在增强医疗MLLM在理解整个医学扫描中的解剖区域方面的能力。为了实现这一目标，我们首先要制定以区域为中心的任务，并构建一个大规模数据集MedreginStruct，将区域信息纳入培训。将收集到的数据集与其他医学多模式语料库结合起来进行培训，我们提出了一个地区感知的医学MLLM Medrega，这是第一个双语通才医学AI系统，可以同时处理各种模式范围内的图像级和地区级医疗视觉 - 语言任务。我们的MEDREGA不仅可以实现三个以区域为中心的任务，而且还可以在8种模式上以视觉问题的回答，报告生成和医学图像分类来实现最佳性能，从而展示了明显的多功能性。实验表明，我们的模型不仅可以在双语环境中在各种医学视觉语言任务中实现强大的性能，而且还可以识别和检测多模式医学扫描中的结构，从而提高医疗MLLM的可解释性和用户交互性。我们的项目页面是https://medrega.github.io。

### AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.18325)] [[cool](https://papers.cool/arxiv/2410.18325)] [[pdf](https://arxiv.org/pdf/2410.18325)]
> **Authors**: Kim Sung-Bin,Oh Hyun-Bin,JungMok Lee,Arda Senocak,Joon Son Chung,Tae-Hyun Oh
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: URL: https://github.com/AVHBench/AVHBench
- **标题**: AVHBENCH：视听大语模型的跨模式幻觉基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 遵循大型语言模型（LLMS）的成功，将其边界扩展到新模式代表了多模式理解的范式转变。人类的看法本质上是多模式的，不仅依赖文本，还依赖于听觉和视觉提示，以完全理解世界。为了认识到这一事实，最近出现了视听LLM。尽管有希望的发展，但缺乏专用的基准测试对理解和评估模型构成了挑战。在这项工作中，我们表明，视听LLM努力辨别音频和视觉信号之间的微妙关系，导致幻觉，强调了对可靠基准的需求。为了解决这个问题，我们介绍了Avhbench，这是第一个专门旨在评估视听LLM的感知和理解能力的全面基准。我们的基准包括评估幻觉的测试，以及这些模型的跨模式匹配和推理能力。我们的结果表明，大多数现有的视听LLM与模式之间交叉互动引起的幻觉斗争，因为它们能够感知复杂的多模式信号及其关系的能力有限。此外，我们证明了简单的训练通过我们的Avhbench提高了视听LLM的稳健性。

### UnCLe: Unsupervised Continual Learning of Depth Completion 
[[arxiv](https://arxiv.org/abs/2410.18074)] [[cool](https://papers.cool/arxiv/2410.18074)] [[pdf](https://arxiv.org/pdf/2410.18074)]
> **Authors**: Suchisrit Gangopadhyay,Xien Chen,Michael Chu,Patrick Rim,Hyoungseob Park,Alex Wong
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: Preprint
- **标题**: 叔叔：无监督的深度完成学习
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 我们提出了叔叔，这是一个标准化的基准，用于无监督的多模式深度估计任务的持续学习：深度完成旨在从一对同步的RGB图像和稀疏深度图中推断出密集的深度图。在无监督学习的实际情况下，我们基准了深度完成模型。现有方法通常在静态或固定数据集上进行培训。但是，当适应新颖的非平稳分布时，他们“灾难性地忘记”以前学习的信息。叔叔通过将深度完成模型调整为包含使用不同的视觉和范围传感器从不同域捕获的不同场景的数据集序列来模拟这些非平稳分布。我们采用了来自持续学习范式的代表性方法，并将其翻译成使无监督的深度完成学习。我们基准这些模型用于室内和室外，并通过标准定量指标研究了灾难性遗忘的程度。此外，我们将模型反演质量引入了遗忘的额外衡量标准。我们发现，无监督的持续学习深度完成是一个开放的问题，我们邀请研究人员利用叔叔作为开发平台。

### TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts 
[[arxiv](https://arxiv.org/abs/2410.18071)] [[cool](https://papers.cool/arxiv/2410.18071)] [[pdf](https://arxiv.org/pdf/2410.18071)]
> **Authors**: Yuxuan Xie,Tianhua Li,Wenqi Shao,Kaipeng Zhang
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: TP-Eval：通过自定义提示来点击多模式LLM的评估潜力
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 最近，多模式的大语言模型（MLLM）因其令人印象深刻的能力而受到了很多关注。 MLLM的评估对于分析MLLM的属性并提供有价值的见解变得至关重要。但是，当前的基准测试忽略了迅速灵敏度的问题 - 较小的迅速变化可能会导致绩效的显着波动。因此，不适当的提示可能会掩盖模型的功能，从而低估了模型的性能。此外，不同模型对不同的提示具有不同的偏好，因此，使用相同的提示，所有模型都会导致评估偏差。本文分析了现有基准中的这种缺陷，并进一步介绍了一个名为TP-Eval的新评估框架，该框架引入了一种及时的自定义方法，以减少评估偏见和TAP模型的潜力。 TP-Eval将将原始提示重写为不同型号的不同自定义提示。特别是，我们提出了一些精心设计的模块，以迅速定制针对MLLM评估方案的定制。广泛的实验证明了我们揭示模型能力的方法的有效性，而TP-eval应该使社区受益于开发更全面和令人信服的MLLM评估基准。

### CLEAR: Character Unlearning in Textual and Visual Modalities 
[[arxiv](https://arxiv.org/abs/2410.18057)] [[cool](https://papers.cool/arxiv/2410.18057)] [[pdf](https://arxiv.org/pdf/2410.18057)]
> **Authors**: Alexey Dontsov,Dmitrii Korzh,Alexey Zhavoronkin,Boris Mikheev,Denis Bobkov,Aibek Alanov,Oleg Y. Rogov,Ivan Oseledets,Elena Tutubalina
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: 清晰：字符在文本和视觉方式上学习
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 机器解读（MU）对于从深度学习模型中删除私人或危险信息至关重要。尽管MU在单峰（文本或视觉）设置中已显着提高，但由于缺乏评估跨模式数据删除的开放基准，多模式的学习（MMU）仍然没有被忽视。为了解决这一差距，我们介绍了Clear，这是专门为MMU设计的第一个开源基准测试。 Clear包含200个虚构的个体和3,700张与相应的问题解答对相关的图像，从而实现了跨模态的彻底评估。我们对四个评估集进行了11种MU方法（例如磨砂，梯度上升，DPO）的全面分析，表明共同学习两种模式的表现都超过了单模式方法。该数据集可从https://huggingface.co/datasets/therem/clear获得

### Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation 
[[arxiv](https://arxiv.org/abs/2410.17918)] [[cool](https://papers.cool/arxiv/2410.17918)] [[pdf](https://arxiv.org/pdf/2410.17918)]
> **Authors**: Wenfang Yao,Chen Liu,Kejing Yin,William K. Cheung,Jing Qin
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: Accepted by NeurIPS-24
- **标题**: 通过个性化的胸部X射线产生来解决临床多模式融合中的异步性
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 整合多模式临床数据，例如电子健康记录（EHR）和胸部X射线图像（CXR），对临床预测任务特别有益。但是，在时间设置中，多模式数据通常是本质上异步的。 EHR可以连续收集，但由于其高成本和辐射剂量，CXR通常会以更长的间隔服用。当需要临床预测时，最后可用的CXR图像可能已经过时，导致了次优的预测。为了应对这一挑战，我们提出了DDL-CXR，该方法会动态生成个性化CXR图像的最新潜在表示。我们的方法利用了以前的CXR图像和EHR时间序列的策略性条件的患者特定生成的潜在扩散模型，分别提供了有关解剖结构和疾病进展的信息。这样，潜在的CXR生成过程可以更好地捕获跨模式的相互作用，最终改善了预测性能。使用模拟数据集的实验表明，所提出的模型可以有效地解决多模式融合中的异步性，并且始终超过现有方法。

### ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting 
[[arxiv](https://arxiv.org/abs/2410.17856)] [[cool](https://papers.cool/arxiv/2410.17856)] [[pdf](https://arxiv.org/pdf/2410.17856)]
> **Authors**: Shaofei Cai,Zihao Wang,Kewei Lian,Zhancun Mu,Xiaojian Ma,Anji Liu,Yitao Liang
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: Rocket-1：掌握与视觉上环境提示的开放世界互动
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉语言模型（VLM）在多模式任务中表现出色，但将它们适应在开放世界环境中体现的决策提出了挑战。一个关键的问题是弥合低级观察中离散实体之间的差距和有效计划所需的抽象概念。一个常见的解决方案是构建分层代理，其中VLM是高级推理器，将任务分解为可执行的子任务，通常使用语言指定。但是，语言无法传达详细的空间信息。我们提出了视觉周期上下文提示，这是VLM和策略模型之间的新型通信协议。该协议利用从过去的观测值到指导政策环境相互作用的对象细分。使用这种方法，我们训练Rocket-1，这是一种低级政策，可预测基于串联的视觉观察和分割掩码的动作，并由SAM-2的实时对象跟踪支持。我们的方法释放了VLM的潜力，使它们能够应对需要空间推理的复杂任务。 Minecraft中的实验表明，我们的方法使代理能够实现以前无法实现的任务，并具有$ \ mathbf {76} \％$绝对改进开放世界交互性能的绝对改进。现在可以在项目页面上提供代码和演示：https：//craftJarvis.github.io/Rocket-1。

### EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2410.17810)] [[cool](https://papers.cool/arxiv/2410.17810)] [[pdf](https://arxiv.org/pdf/2410.17810)]
> **Authors**: Yaxiong Wang,Yaxiong Wang,Lianwei Wu,Lechao Cheng,Zhun Zhong,Meng Wang
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: EntityClip：通过多模式的对比学习，以实体为中心的图像文本匹配
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像文本匹配的最新进展是显着的，但主要的模型主要迎合广泛的查询，并在适应细粒度的查询意图方面挣扎。在本文中，我们致力于\ textbf {e} ntity-centric \ textbf {i} mage- \ textbf {t} ext \ textbf {m} atching（eITM），文本和图像涉及特定与特定相关的特定相关信息。 The challenge of this task mainly lies in the larger semantic gap in entity association modeling, comparing with the general image-text matching problem.To narrow the huge semantic gap between the entity-centric text and the images, we take the fundamental CLIP as the backbone and devise a multimodal attentive contrastive learning framework to tam CLIP to adapt EITM problem, developing a model named EntityCLIP.我们多模式的专注对比学习的关键是使用大语言模型（LLM）作为桥梁线索生成解释性解释文本。具体而言，我们通过从现成的LLM中提取解释性文本进行。然后，此解释文本，加上图像和文本，然后将其输入到我们精心设计的多模式专家（MMAE）模块中，该模块有效地集成了解释文本，以缩小在共享语义空间中与实体相关文本和图像的间隙缩小差距。在源自MMAE的丰富功能的基础上，我们进一步设计了有效的封闭式集成图像文本匹配（GI-ITM）策略。 GI-ITM采用自适应门控机制来汇总MMAE的特征，随后应用图像文本匹配的约束来引导文本和图像之间的对齐。在包括N24News，VisualNews和GoodNews在内的三个社交媒体新闻基准上进行了广泛的实验，结果表明，我们的方法以明显的利润超过了竞争方法。

### ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning 
[[arxiv](https://arxiv.org/abs/2410.17779)] [[cool](https://papers.cool/arxiv/2410.17779)] [[pdf](https://arxiv.org/pdf/2410.17779)]
> **Authors**: Zhiwei Hao,Jianyuan Guo,Li Shen,Yong Luo,Han Hu,Yonggang Wen
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: ADEM-VL：自适应和嵌入式融合以进行有效的视觉调整
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式融合的最新进步见证了视觉语言（VL）模型的显着成功，这些模型在各种多模式应用中脱颖而出，例如图像字幕和视觉问题答案。但是，构建VL模型需要大量的硬件资源，其中效率受两个关键因素的限制：具有视觉功能的语言模型的扩展输入顺序需要更多的计算操作，并且大量其他可学习的参数会增加内存的复杂性。这些挑战极大地限制了此类模型的更广泛的适用性。为了弥合这一差距，我们提出了ADEM-VL，这是一种有效的视觉语言方法，该方法通过采用无参数的跨注意机制来调整基于预处理的大语言模型（LLMS）的VL模​​型，以在多模式融合中进行相似性测量。这种方法仅需要将视觉特征嵌入语言空间，大大减少了可训练的参数的数量并加速训练和推理速度。为了增强融合模块中的表示学习，我们引入了一种有效的多尺度特征生成方案，该方案只需要单个前向通过视觉编码器。此外，我们提出了一种自适应融合方案，该方案会根据其注意力评分动态丢弃每个文本令牌的相关视觉信息。这样可以确保融合过程优先考虑最相关的视觉特征。通过对各种任务的实验，包括视觉问题回答，图像字幕和跟随说明，我们证明我们的框架表现优于现有方法。具体而言，我们的方法在ScienceQA数据集上的平均准确度超过了现有方法，并且培训和推理潜伏期减少，这证明了我们的框架的优势。该代码可在https://github.com/hao840/adem-vl上找到。

### YOLO-Vehicle-Pro: A Cloud-Edge Collaborative Framework for Object Detection in Autonomous Driving under Adverse Weather Conditions 
[[arxiv](https://arxiv.org/abs/2410.17734)] [[cool](https://papers.cool/arxiv/2410.17734)] [[pdf](https://arxiv.org/pdf/2410.17734)]
> **Authors**: Xiguang Li,Jiafu Chen,Yunhe Sun,Na Lin,Ammar Hawbani,Liang Zhao
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: Yolo-Vehicle-Pro：在不利天气条件下自动驾驶中对象检测的云边缘协作框架
- **领域**: 计算机视觉和模式识别,信息检索
- **摘要**: 随着自动驾驶技术的快速发展，高效，准确的对象检测功能已成为确保自动驾驶系统安全性和可靠性的关键因素。但是，在诸如朦胧条件之类的低可见性环境中，传统对象检测算法的性能通常会大大降低，因此无法满足自动驾驶的需求。为了应对这一挑战，本文提出了两个创新的深度学习模型：Yolo-Vehicle和Yolo-Vehicle-Pro。 Yolo-Vehicle是一种针对自主驾驶场景量身定制的对象检测模型，采用多模式融合技术将图像和文本信息结合起来以进行对象检测。 Yolo-Vehicle-Pro通过引入改进的图像除算算法，从而在低可见性环境中提高检测性能，从而建立在该基础上。除了模型创新外，本文还设计和实现了云边缘协作对象检测系统，在边缘设备上部署模型，并在复杂的情况下将部分计算任务卸载到云中。实验结果表明，在KITTI数据集上，Yolo-Vehicle-V1S模型在保持226 fps的检测速度和12ms的推理时间时达到了92.1％的精度，满足自动驾驶的实时要求。在处理朦胧的图像时，Yolo-Vehicle-Pro模型在雾化CityScapes数据集上达到了82.3％的地图@50，同时保持43 fps的检测速度。

### Double Banking on Knowledge: Customized Modulation and Prototypes for Multi-Modality Semi-supervised Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2410.17565)] [[cool](https://papers.cool/arxiv/2410.17565)] [[pdf](https://arxiv.org/pdf/2410.17565)]
> **Authors**: Yingyu Chen,Ziyuan Yang,Ming Yan,Zhongzhou Zhang,Hui Yu,Yan Liu,Yi Zhang
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: 知识的双重依据：多模式的自定义调制和原型半监督医学图像细分
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于多模式（MM）半监督学习（SSL）的医疗图像分割最近已越来越多地注意其使用MM数据并减少对标记图像的依赖的能力。但是，当前的方法面临几个挑战：（1）复杂的网络设计阻碍了具有多种方式以上方式的方案。 （2）仅着眼于模式不变的表示，同时忽略了特定于模态特征，从而导致MM学习不完整。 （3）使用生成方法利用未标记的数据对于SSL可能不可靠。为了解决这些问题，我们提出了双银行双重一致性（DBDC），这是一种新型的MM-SSL方法进行医学图像细分。为了应对挑战（1），我们提出了一种多合一分割网络的方式，该网络可容纳任何数量的模式数据，从而消除了模态数的限制。为了应对挑战（2），我们设计了两个可学习的插件银行，模式级调制库（MLMB）和模式级原型（MLPB）银行，以捕获模式不变和特定于模式的知识。这些银行使用我们提出的模式原型对比度学习（MPCL）进行更新。此外，我们设计了模态自适应加权（MAW），以动态调整每种模态的学习权重，以确保平衡的MM学习，因为不同的方式以不同的速度学习。最后，为了应对挑战（3），我们引入了双重一致性（DC）策略，该策略在不依赖生成方法的情况下在图像和特征级别上执行一致性。我们使用三个开源数据集在2到4的模式分割任务上评估我们的方法，并且广泛的实验表明，我们的方法优于最先进的方法。

### Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant 
[[arxiv](https://arxiv.org/abs/2410.19144)] [[cool](https://papers.cool/arxiv/2410.19144)] [[pdf](https://arxiv.org/pdf/2410.19144)]
> **Authors**: Abhirama Subramanyam Penamakuri,Anand Mishra
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Accepted to EMNLP (Main) 2024
- **标题**: 视觉文本事项：通过视觉文本实体知识知识改善文本-KVQA大型多模式助手
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 根据大型多模型模型（LMM）的现代进步，我们重新访问了基于文本的视觉问题回答，也称为Text-KVQA，并做出以下贡献：（i）我们提出了Vistel，这是执行视觉文本实体链接的原则方法。提出的Vistel模块利用最先进的视觉文本识别引擎和大型多模型模型的力量，使用图像中的周围线索获得的文本和视觉上下文共同理性，将视觉文本实体链接到正确的知识库实体。 （ii）我们提出了Kalma-一种知识吸引的大型多模式助手，它通过图像中与视觉文本实体相关的知识增强了LMM，以获得准确的答案。此外，我们提供了全面的实验分析，并将我们的方法与传统的视觉问题答案，大型多模式模型和大型多模式模型以及先前表现最佳的方法进行了比较。我们所提出的方法平均三个划分的文本KVQA超过了以前的最佳方法，在绝对规模上超过了23.3％，并确立了新的最新状态。我们公开实施。

### VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks 
[[arxiv](https://arxiv.org/abs/2410.19100)] [[cool](https://papers.cool/arxiv/2410.19100)] [[pdf](https://arxiv.org/pdf/2410.19100)]
> **Authors**: Lawrence Jang,Yinheng Li,Dan Zhao,Charles Ding,Justin Lin,Paul Pu Liang,Rogerio Bonatti,Kazuhito Koishida
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: VideoWebarena：评估长上下文多模式的代理，并了解视频理解Web任务
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视频通常用于学习或提取必要的信息，以不同于文本和静态图像所能提供的方式来完成任务。但是，许多现有的代理基准忽略了长篇小说视频的理解，而是专注于文本或静态图像输入。为了弥合这一差距，我们介绍了VideoWebarena（Videowa），这是一种评估长篇小说多模式的能力以用于视频理解的基准。 Videowa由基于手动制作的视频教程的2,021个Web代理任务组成，总计将近四个小时的内容。对于我们的基准，我们定义了具有两个主要重点领域的基于长篇小说视频的代理商任务的分类学：技能保留和事实保留。尽管技能保留任务评估了代理是否可以使用给定的人类演示来有效地完成任务，但事实保留任务评估代理是否可以从视频中检索与指导相关的信息以完成任务。我们发现，最佳模型在事实保留任务上取得了13.3％的成功，而事实保留质量保留质量保留质量保留质量质量质量质量质量质量质量质量质量质量质量质量质量质量质量质量质量质量质量质量质量保留质量保留质量保留质量保留质量质量保留质量质量保留质量保留质量质量保留质量质量保留质量质量保留质量质量保留（QA）分别远低于人类绩效，分别为73.9％和79.3％。在技​​能保留任务上，长篇小说模型在教程中的表现要比没有教程差，在Webarena任务中表现出5％的性能下降，而VisualWebarena任务降低了10.3％。我们的工作强调了提高长篇文化多模型的代理能力的必要性，并为未来开发的测试床提供了长篇文化视频代理。

### CAMEL-Bench: A Comprehensive Arabic LMM Benchmark 
[[arxiv](https://arxiv.org/abs/2410.18976)] [[cool](https://papers.cool/arxiv/2410.18976)] [[pdf](https://arxiv.org/pdf/2410.18976)]
> **Authors**: Sara Ghaboura,Ahmed Heakl,Omkar Thawakar,Ali Alharthi,Ines Riahi,Abduljalil Saif,Jorma Laaksonen,Fahad S. Khan,Salman Khan,Rao M. Anwer
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: 10 pages, 5 figures, NAACL
- **标题**: 骆驼台：全面的阿拉伯LMM基准
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,计算机与社会,机器学习
- **摘要**: 近年来，人们对开发能够执行各种视觉推理和理解任务的大型多模型模型（LMM）产生了重大兴趣。这导致引入了多个LMM基准，以评估不同任务的LMM。但是，大多数现有的LMM评估基准主要以英语为中心。在这项工作中，我们为阿拉伯语提供了全面的LMM评估基准，以代表超过4亿扬声器。所提出的名为骆驼板的基准包括八个不同的领域和38个子域，包括多图像理解，复杂的视觉感知，手写文档理解，视频理解，医学成像，植物疾病和基于遥感的土地利用理解，以评估广泛的场景概括性。我们的骆驼板组成了大约29,036个问题，这些问题是从较大的样本中过滤的，其中母语者手动验证质量以确保可靠的模型评估。我们对包括GPT-4系列和开源LMM在内的封闭源进行评估。我们的分析表明，有必要改进的必要性，尤其是在最佳开源模型中，即使是封闭源的GPT-4O也达到了62％的总分。我们的基准和评估脚本是开源的。

### Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms 
[[arxiv](https://arxiv.org/abs/2410.18967)] [[cool](https://papers.cool/arxiv/2410.18967)] [[pdf](https://arxiv.org/pdf/2410.18967)]
> **Authors**: Zhangheng Li,Keen You,Haotian Zhang,Di Feng,Harsh Agrawal,Xiujun Li,Mohana Prasad Sathya Moorthy,Jeff Nichols,Yinfei Yang,Zhe Gan
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Accepted to ICLR 2025
- **标题**: Ferret-UI 2：掌握跨平台的通用用户界面理解
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 由于各种基础问题，例如平台多样性，分辨率变化和数据限制，建立用于用户界面（UI）理解的通才模型（UI）的理解都具有挑战性。在本文中，我们介绍了雪貂UI 2，这是一种多模式大语言模型（MLLM），旨在跨越包括iPhone，Android，Android，iPad，WebPage和AppleTV在内的广泛平台上的通用UI理解。 Ferret-UI 2以雪貂UI的基础为基础，介绍了三个关键创新：对多种平台类型的支持，通过自适应缩放来进行高分辨率感知以及由GPT-4O提供支持的高级任务培训数据生成，并带有标志性的视觉提示。这些进步使雪貂2能够执行复杂的，以用户为中心的交互作用，使其具有高度的用途和适应性的平台生态系统多样性。有关引用，接地，以用户为中心的高级任务的广泛实证实验（包括9个子任务$ \ times $ 5平台），指南下一步预​​测数据集和GUI-World多平台基准表明，雪貂2表明2巨大的越野斗式腌鱼架和强大的交叉平面cablats cablats cabbiorsials cablations cabbiorsials均显着胜过。

### SegLLM: Multi-round Reasoning Segmentation 
[[arxiv](https://arxiv.org/abs/2410.18923)] [[cool](https://papers.cool/arxiv/2410.18923)] [[pdf](https://arxiv.org/pdf/2410.18923)]
> **Authors**: XuDong Wang,Shaolun Zhang,Shufan Li,Konstantinos Kallidromitis,Kehan Li,Yusuke Kato,Kazuki Kozuka,Trevor Darrell
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: 22 pages, 10 figures, 11 tables
- **标题**: segllm：多轮推理细分
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们提出了Segllm，这是一种新型的多轮交互推理分割模型，通过利用视觉和文本输出的对话记忆来增强基于LLM的分割。通过利用掩模感知的多模式LLM，Segllm将先前的分割结果重新整合到其输入流中，使其能够与先前识别的实体有关，包括位置，交互性，交互性和等级关系，以了解复杂的用户意图和段对象。此功能使Segllm可以像聊天一样响应视觉和文本查询。在新策划的MRSEG基准测试中，Segllm在多轮交互推理细分中的现有方法胜过20％以上。此外，我们观察到，对多轮推理分段数据进行培训可以提高标准单一参考分段和本地化任务的性能，从而提高CIOU的5.5％，以提高表达分割的参考表达细分，并在ACC@0.5中提高了4.5％的提高，以推荐表达表达本地化。

### Towards Visual Text Design Transfer Across Languages 
[[arxiv](https://arxiv.org/abs/2410.18823)] [[cool](https://papers.cool/arxiv/2410.18823)] [[pdf](https://arxiv.org/pdf/2410.18823)]
> **Authors**: Yejin Choi,Jiwan Chung,Sumin Shim,Giyeong Oh,Youngjae Yu
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: 跨语言的视觉文本设计转移
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉文本设计在传达诸如电影海报和专辑封面等多模式格式的主题，情感和氛围中起着至关重要的作用。跨语言翻译这些视觉和文本元素，将翻译的概念扩展到了单纯的文本之外，需要适应美学和风格的特征。为了解决这个问题，我们介绍了一项新型的多模式样式翻译（必须卧）的任务，这是一种基准测试，旨在评估视觉文本生成模型跨不同写作系统执行翻译的能力，同时保持设计意图。我们在必备基础上进行的最初实验表明，由于文本描述不足，现有的视觉文本生成模型在传达视觉设计时不足。作为回应，我们介绍了Sigil，这是多模式样式翻译框架，消除了对样式描述的需求。 Sigil通过三个创新来增强图像生成模型：用于多语言设置的字形潜在，预审计的VAE用于稳定样式指导，以及具有增强学习反馈的OCR模型，以优化可读性的角色产生。 Sigil通过在保持视觉保真度的同时实现卓越的风格一致性和可读性来优于现有基线，从而使自己与传统的基于描述的方法区分开来。我们公开释放必备替补和探索https://huggingface.co/datasets/yejinc/must-bench。

### ChatSearch: a Dataset and a Generative Retrieval Model for General Conversational Image Retrieval 
[[arxiv](https://arxiv.org/abs/2410.18715)] [[cool](https://papers.cool/arxiv/2410.18715)] [[pdf](https://arxiv.org/pdf/2410.18715)]
> **Authors**: Zijia Zhao,Longteng Guo,Tongtian Yue,Erdong Hu,Shuai Shao,Zehuan Yuan,Hua Huang,Jing Liu
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: ChatSearch：一般对话图像检索的数据集和生成检索模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们研究了在开放域图像上检索一般对话图像的任务。目的是根据人与计算机之间的交互式对话搜索图像。为了促进此任务，我们策划了一个名为chatsearch的数据集。该数据集包括每个目标图像的多轮多模式对话上下文查询查询，从而需要检索系统从数据库中找到准确的图像。同时，我们提出了一个名为ChatSearcher的生成检索模型，该模型受过训练的端到端，可以接受/生产交错的图像文本输入/输出。 ChatSearcher在具有多模式上下文的推理方面表现出强大的能力，并可以利用世界知识来产生视觉检索结果。它在ChatSearch数据集上展示了出色的性能，并在其他图像检索任务和视觉对话任务上取得了竞争成果。我们预计这项工作将激发有关互动多模式检索系统的进一步研究。我们的数据集将在https://github.com/joez17/chatsearch上找到。

### DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation 
[[arxiv](https://arxiv.org/abs/2410.18666)] [[cool](https://papers.cool/arxiv/2410.18666)] [[pdf](https://arxiv.org/pdf/2410.18666)]
> **Authors**: Yuang Ai,Xiaoqiang Zhou,Huaibo Huang,Xiaotian Han,Zhengyu Chen,Quanzeng You,Hongxia Yang
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Accepted by NeurIPS 2024
- **标题**: DreamClear：使用隐私安全数据集策划的高容量现实世界图像恢复
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于缺乏高容量模型和全面的数据集，在现实世界情景中的图像恢复（IR）提出了重大挑战。为了解决这些问题，我们提出了双重策略：Genir，创新的数据策展管道和DreamClear，这是一个基于尖端的扩散变压器（DIT）基于图像恢复模型。 Genir是我们的开拓性贡献，是一条双重提示的学习管道，它克服了现有数据集的局限性，通常仅包含几千张图像，因此为大型模型提供了有限的可推广性。 Genir将该过程简化为三个阶段：图像文本对结构，基于双预播的微调以及数据生成和过滤。这种方法规避了艰苦的数据爬行过程，确保了版权合规，并为IR数据集构建提供了具有成本效益的隐私安全解决方案。结果是一百万个高质量图像的大规模数据集。我们的第二个贡献是DreamClear，是基于DIT的图像恢复模型。它利用文本对图像（T2I）扩散模型的生成先验以及多模式大语言模型（MLLM）的强大感知能力来实现情相恢复。为了提高模型对不同现实世界降低的适应性，我们介绍了自适应调制器（MOAM）的混合物。它采用令牌降级先验来动态整合各种恢复专家，从而扩大了模型可以解决的降级范围。我们的详尽实验证实了Dreamclear的出色表现，这突显了我们对现实世界图像修复的双重策略的功效。代码和预培训模型可在以下网址提供：https：//github.com/shallowdream204/dreamclear。

### Referring Human Pose and Mask Estimation in the Wild 
[[arxiv](https://arxiv.org/abs/2410.20508)] [[cool](https://papers.cool/arxiv/2410.20508)] [[pdf](https://arxiv.org/pdf/2410.20508)]
> **Authors**: Bo Miao,Mingtao Feng,Zijie Wu,Mohammed Bennamoun,Yongsheng Gao,Ajmal Mian
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-28
> **comment**: Accepted by NeurIPS 2024. https://github.com/bo-miao/RefHuman
- **标题**: 在野外参考人姿势和掩盖估计
- **领域**: 计算机视觉和模式识别,人机交互
- **摘要**: 我们在野外介绍了引用人姿势和面具估计（R-HPM），其中文本或位置提示指定图像中感兴趣的人。这项新任务具有以人为中心的应用（例如辅助机器人技术和体育分析）的巨大潜力。与以前的作品相反，R-HPM（i）确保了与推荐人相对应的高质量，身份意识的结果，并且（ii）同时预测人类的姿势和面具，以进行全面的代表。为此，我们引入了一个名为Refhan的大规模数据集，该数据集基本上扩展了MS Coco数据集，并使用其他文本和位置提示注释。 Refhan在野外包含超过50,000个注释的实例，每个实例都配备了键盘，口罩和及时注释。为了启用及时条件的估计，我们提出了第一种端到端的敏捷方法，称为R-HPM的UNIPHD。 UNIPHD提取多模式表示，并采用提出的以姿势为中心的分层解码器来处理（文本或位置）实例查询和关键点查询，从而产生特定于推荐人的结果。广泛的实验表明，UNIPHD基于用户友好的提示，并在Refhuman Val和MS Coco Val2017上实现顶级性能产生质量结果。数据和代码：https：//github.com/bo-miao/refhuman

### GiVE: Guiding Visual Encoder to Perceive Overlooked Information 
[[arxiv](https://arxiv.org/abs/2410.20109)] [[cool](https://papers.cool/arxiv/2410.20109)] [[pdf](https://arxiv.org/pdf/2410.20109)]
> **Authors**: Junjie Li,Jianghong Ma,Xiaofeng Zhang,Yuhang Li,Jianyang Shi
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 给出：引导视觉编码器以感知被忽略的信息
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **摘要**: 多模式大型语言模型在文本到视频生成和视觉问题等应用中具有AI级AI。这些模型依靠可视编码器将非文本数据转换为向量，但是当前的编码器要么缺乏语义对准或忽略非偏好对象。我们建议引导视觉编码器感知被忽略的信息（给予）方法。通过注意引导的适配器（AG-ADAPTER）模块和以对象为中心的视觉语义学习模块，给出增强的视觉表示。这些结合了三个新颖的损失术语：以对象为中心的图像对比（OITC）损失，以对象为中心的图像图像对比（OIIC）损失以及以对象为中心的图像区分（OID）损失，改善对象的考虑，检索准确性和理解力。我们的贡献包括动态视觉焦点调整，增强对象检索的新型损失功能以及多对象指令（MOINST）数据集。实验表明我们的方法达到了最先进的表现。

### Generative Adversarial Patches for Physical Attacks on Cross-Modal Pedestrian Re-Identification 
[[arxiv](https://arxiv.org/abs/2410.20097)] [[cool](https://papers.cool/arxiv/2410.20097)] [[pdf](https://arxiv.org/pdf/2410.20097)]
> **Authors**: Yue Su,Hao Li,Maoguo Gong
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 用于跨模式行人重新识别的物理攻击的生成对抗斑块
- **领域**: 计算机视觉和模式识别
- **摘要**: 可见的红外行人重新识别（VI-REID）旨在匹配由红外摄像机和可见摄像机捕获的行人图像。但是，Vi-Reid与其他传统的跨模式匹配任务一样，由于其以人为本的性质提出了重大挑战。现有方法的缺点证明了这一点，这些方法很难跨模态提取共同的特征，同时在隐含特征空间中弥合它们之间的差距时会失去宝贵的信息，这可能会损害安全性。为了解决此漏洞，本文引入了针对Vi-Reid模型的第一次物理对抗攻击。我们的方法称为边缘攻击，专门测试了模型通过专注于边缘信息来利用深层隐式特征的能力，最明显的明确特征是跨模态的个人。 Edge-Ittack采用了一种新颖的两步方法。首先，以自我监督的方式对多级边缘特征提取器进行训练，以捕获每个人的区分边缘表示。其次，采用基于视觉变压器生成对抗网络（Vitgan）的生成模型来生成以提取的边缘特征为条件的对抗斑块。通过将这些斑块应用于行人服装，我们可以创建现实的，可实现的对抗样品。这种黑框，自我监督的方法可确保我们对各种Vi-Reid模型的攻击的普遍性。在包括现实世界中的SYSU-MM01和REGDB数据集上进行了广泛的实验，证明了边缘攻击在显着降低最先进的VI-REID方法的性能方面的有效性。

### Turn-by-Turn Indoor Navigation for the Visually Impaired 
[[arxiv](https://arxiv.org/abs/2410.19954)] [[cool](https://papers.cool/arxiv/2410.19954)] [[pdf](https://arxiv.org/pdf/2410.19954)]
> **Authors**: Santosh Srinivasaiah,Sai Kumar Nekkanti,Rohith Reddy Nedhunuri
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 视力障碍的转弯室内导航
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于复杂的布局和缺乏GPS信号，导航室内环境对视觉受损的个体提出了重大挑战。本文介绍了一个新型系统，该系统仅使用配备了相机的智能手机，利用多模式模型，深度学习算法和大型语言模型（LLMS）提供建筑物内部的转弯导航。智能手机的相机捕获了周围环境的实时图像，然后将其发送到附近的Raspberry Pi，能够运行设备上的LLM型号，多模式模型和深度学习算法，以检测和识别建筑特征，标牌和障碍。然后，解释的视觉数据将通过在Raspberry Pi上运行的LLM转换为自然语言指令，Raspberry Pi将其发送给用户，并通过音频提示提供直观和上下文感知的指导。该解决方案需要在用户设备上的最小工作负载，以防止其过载并提供与所有类型的设备（包括无法运行AI型号的设备）的兼容性。这种方法使客户不仅可以运行高级模型，还可以确保培训数据和其他信息不会离开建筑物。初步评估证明了该系统在准确指导用户通过复杂的室内空间的有效性，从而突出了其广泛应用的潜力

### A Multimodal Approach For Endoscopic VCE Image Classification Using BiomedCLIP-PubMedBERT 
[[arxiv](https://arxiv.org/abs/2410.19944)] [[cool](https://papers.cool/arxiv/2410.19944)] [[pdf](https://arxiv.org/pdf/2410.19944)]
> **Authors**: Nagarajan Ganapathy,Podakanti Satyajith Chary,Teja Venkata Ramana Kumar Pithani,Pavan Kavati,Arun Kumar S
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: 11 Pages, 2 Figures, Capsule Vision 2024 Challenge
- **标题**: 使用BiomedClip-Pubmedbert进行内窥镜VCE图像分类的多模式方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了一种多模型的微型生物胶囊PubMedbert的先进方法，用于对视频胶囊内窥镜检查（VCE）帧的异常进行分类，旨在提高胃肠道医疗保健的诊断效率。通过将PubMedbert语言模型与视觉变压器（VIT）整合到处理内窥镜图像，我们的方法将图像分为十个特定类别：血管症，出血，侵蚀，红斑，异物，异物，淋巴结症，淋巴结症，念珠菌，息肉，溃疡，溃疡，蠕虫，蠕虫和正常。我们的工作流程包含图像预处理和微型生物胶囊模型，以生成视觉和文本输入的高质量嵌入，从而通过分类的相似性评分来对齐它们。绩效指标，包括分类，准确性，召回和F1得分，表明模型的强大能力可以准确识别内窥镜框架中的异常，从而显示出在临床诊断中实际使用的希望。

### FLAASH: Flow-Attention Adaptive Semantic Hierarchical Fusion for Multi-Modal Tobacco Content Analysis 
[[arxiv](https://arxiv.org/abs/2410.19896)] [[cool](https://papers.cool/arxiv/2410.19896)] [[pdf](https://arxiv.org/pdf/2410.19896)]
> **Authors**: Naga VS Raviteja Chappa,Page Daniel Dobbs,Bhiksha Raj,Khoa Luu
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: Under review at Image and Vision Computing Journal; 20 pages, 4 figures, 5 tables
- **标题**: Flaash：多模式烟草内容分析的流动意见自适应语义分层融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 社交媒体平台上与烟草相关的内容的扩散为公共卫生监测和干预带来了重大挑战。本文介绍了一个新型的多模式深度学习框架，名为流动性自适应语义分层融合（FLAASH），旨在全面分析与烟草相关的视频内容。 Flaash通过利用流动网络理论启发的分层融合机制来解决在短形式视频中整合视觉和文本信息的复杂性。我们的方法结合了三个关键的创新，包括一种流动发注意机制，该机制捕获了视觉和文本方式之间细微的相互作用，一种适应性加权方案，平衡了不同层次结构级别的贡献以及一种选择性强调相关特征的门控机制。这种多方面的方法使Flaash能够有效地处理和分析与烟草相关的各种内容，从产品展示到使用情况。我们在多模式烟草内容分析数据集（MTCAD）上评估了Flaash，这是来自流行社交媒体平台的大规模烟草相关视频集合。我们的结果表明，对现有方法的改进，在分类准确性，F1分数和时间一致性方面表现优于最先进的方法。当在标准视频提问数据集中测试时，提出的方法还显示出强大的概括功能，超过了当前模型。这项工作有助于公共卫生和人工智能的交汇处，为分析数字媒体中的烟草促销提供了有效的工具。

### Radar and Camera Fusion for Object Detection and Tracking: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2410.19872)] [[cool](https://papers.cool/arxiv/2410.19872)] [[pdf](https://arxiv.org/pdf/2410.19872)]
> **Authors**: Kun Shi,Shibo He,Zhenyu Shi,Anjun Chen,Zehui Xiong,Jiming Chen,Jun Luo
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 雷达和相机融合用于对象检测和跟踪：一项全面的调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式融合对于在复杂环境中实施可靠的对象检测和跟踪至关重要。利用异质模态信息的协同作用赋予感知系统实现更全面，健壮和准确的性能的能力。作为无线视觉协作的核心关注，雷达相机融合促使了前瞻性研究方向，这是由于其广泛的适用性，互补性和兼容性。尽管如此，仍然缺乏系统的调查，专门针对雷达和相机的深层融合，以进行对象检测和跟踪。为了填补这一空白，我们踏上了一项努力，以整体方式全面回顾雷达相机融合。首先，我们详细介绍了雷达相机融合感知的基本原理，方法和应用。接下来，我们深入研究有关传感器校准，模态表示，数据对齐和融合操作的关键技术。此外，我们提供了一个详细的分类法，其中涵盖了与对象检测和跟踪相关的研究主题。在雷达和相机技术的背景下。在本文中，我们讨论了雷达相机融合感知领域的新兴观点，并突出了未来研究的潜在领域。

### Benchmarking Large Language Models for Image Classification of Marine Mammals 
[[arxiv](https://arxiv.org/abs/2410.19848)] [[cool](https://papers.cool/arxiv/2410.19848)] [[pdf](https://arxiv.org/pdf/2410.19848)]
> **Authors**: Yijiashun Qi,Shuzhang Cai,Zunduo Zhao,Jiaming Li,Yanbin Lin,Zhiqiang Wang
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-28
> **comment**: ICKG 2024
- **标题**: 为海洋哺乳动物的图像分类的大型语言模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 随着过去几十年来人工智能（AI）的发展，新一代的AI，大型语言模型（LLMS）在大型数据集中培训，在许多应用程序中都取得了突破性的性能。在多模式LLM中取得了进一步的进展，创建了许多数据集来评估具有视力能力的LLM。但是，这些数据集都没有仅专注于海洋哺乳动物，这对于生态平衡是必不可少的。在这项工作中，我们构建了一个基准数据集，其中包含1,423张65种海洋哺乳动物的图像，其中每种动物都唯一地分为不同的类别，从物种级别到中等水平到组级别。此外，我们评估了几种对这些海洋哺乳动物进行分类的方法：（1）使用神经网络提供的嵌入式机器学习（ML）算法，（2）有影响力的预训练的神经网络，（3）零弹药模型：剪贴和LLMS，以及（4）一种新型的LLM基于LLM的多代理系统（MAS）。结果证明了传统模型和LLM在不同方面的优势，MAS可以进一步改善分类性能。该数据集可在GitHub上找到：https：//github.com/yeyimilk/llm-vision-marine-animals.git。

### Automating Video Thumbnails Selection and Generation with Multimodal and Multistage Analysis 
[[arxiv](https://arxiv.org/abs/2410.19825)] [[cool](https://papers.cool/arxiv/2410.19825)] [[pdf](https://arxiv.org/pdf/2410.19825)]
> **Authors**: Elia Fantini
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-28
> **comment**: 150 pages, 60 figures
- **标题**: 通过多模式和多阶段分析自动化视频缩略图选择和生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文提出了一种创新的方法，可以自动化视频缩略图选择传统广播内容。我们的方法论建立了针对各种，代表性和美学上令人愉悦的缩略图的严格标准，这些因素考虑了徽标位置空间，垂直纵横比的结合以及对面部身份和情感的准确认识。我们介绍了一条复杂的多阶段管道，该管道可以选择候选框架或通过混合视频元素或使用扩散模型来生成新颖图像。该管道结合了针对各种任务的最先进模型，包括减排，减少冗余，自动化裁剪，面部识别，闭眼和情感检测，射击标度和美学预测，分段，垫子和协调。它还利用大型语言模型和视觉变压器来达到语义一致性。 GUI工具促进了管道输出的快速导航。为了评估我们的方法，我们进行了全面的实验。在对69个视频的研究中，我们提出的套装中有53.6％包括专业设计师选择的缩略图，其中73.9％包含相似图像。对82名参与者的调查显示，我们的方法偏爱45.77％，而手动选择的缩略图为37.99％，另一种方法为16.36％。专业设计师报告说，与替代方法相比，有效候选人的有效候选者增加了3.57倍，证实我们的方法符合既定标准。总之，我们的发现确认，所提出的方法在保持高质量标准并促进更大的用户参与度的同时加速了缩略图的创建。

### Leveraging Multi-Temporal Sentinel 1 and 2 Satellite Data for Leaf Area Index Estimation With Deep Learning 
[[arxiv](https://arxiv.org/abs/2410.19787)] [[cool](https://papers.cool/arxiv/2410.19787)] [[pdf](https://arxiv.org/pdf/2410.19787)]
> **Authors**: Clement Wang,Antoine Debouchage,Valentin Goldité,Aurélien Wery,Jules Salzinger
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 利用多阶梯哨兵1和2卫星数据进行深度学习
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 叶面积指数（LAI）是了解生态系统健康和植被动态的关键参数。在本文中，我们通过利用Sentinel 1雷达数据的互补信息和Sentinel 2多光谱数据在多个时间戳上提出了一种用于像素LAI预测的新方法。我们的方法基于专门针对此任务量身定制的多个U-NET，使用了深层神经网络。为了处理不同输入方式的复杂性，它由几个单独训练的模块组成，以表示公共潜在空间中的所有输入数据。然后，我们用一个共同的解码器将它们端对端微调，该解码器还考虑了季节性，我们发现这起重要作用。我们的方法在公开可用的数据上获得了0.06 RMSE和0.93 R2分数。我们在https://github.com/valentingol/leafnothingbehind上提供了贡献，以进一步改善我们当前的进步。

### Copula-Linked Parallel ICA: A Method for Coupling Structural and Functional MRI brain Networks 
[[arxiv](https://arxiv.org/abs/2410.19774)] [[cool](https://papers.cool/arxiv/2410.19774)] [[pdf](https://arxiv.org/pdf/2410.19774)]
> **Authors**: Oktay Agcaoglu,Rogers F. Silva,Deniz Alacam,Sergey Plis,Tulay Adali,Vince Calhoun
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-28
> **comment**: 25 pages, 10 figures, journal article
- **标题**: 副链接的平行ICA：一种耦合结构和功能性MRI脑网络的方法
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,可能性,计算
- **摘要**: 不同的大脑成像方式为大脑功能和结构提供了独特的见解。将它们结合起来增强了我们对神经机制的理解。先前的多模式研究融合功能性MRI（fMRI）和结构MRI（SMRI）已显示出这种方法的好处。由于SMRI缺乏时间数据，因此现有的融合方法通常将fMRI的时间信息压缩为摘要措施，牺牲丰富的时间动态。通过在SMRI和静息状态fMRI中鉴定出协方差网络的观察，我们通过将深度学习框架，Copulas和独立组件分析（ICA）（命名为copula链接的平行ICA（clip-ica））结合来开发了一种新颖的融合方法。该方法使用基于Copula的模型来更灵活地集成了时间和空间数据的模型来估算每种模式的独立源，并将fMRI和SMRI的空间来源链接。我们使用阿尔茨海默氏病神经影像倡议（ADNI）的数据测试了夹子。我们的结果表明，夹子可以有效地捕获强烈和弱连接的SMRI和FMRI网络，包括小脑，感觉运动，视觉，认知控制和默认模式网络。它揭示了更有意义的组件和更少的工件，解决了ICA中最佳模型顺序的长期问题。与患有阿尔茨海默氏症患者相比，夹子还检测到认知下降阶段的复杂功能连通性模式，认知正常受试者通常显示出更高的感觉运动和视觉网络连通性，以及暗示潜在的补偿机制的模式。

### Reliable, Routable, and Reproducible: Collection of Pedestrian Pathways at Statewide Scale 
[[arxiv](https://arxiv.org/abs/2410.19762)] [[cool](https://papers.cool/arxiv/2410.19762)] [[pdf](https://arxiv.org/pdf/2410.19762)]
> **Authors**: Yuxiang Zhang,Bill Howe,Anat Caspi
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-28
> **comment**: arXiv admin note: text overlap with arXiv:2303.02323
- **标题**: 可靠，可路由和可再现：在全州范围内的行人途径收集
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 尽管包括自动驾驶汽车和多模式导航系统在内的移动技术进步可以改善残疾人的移动股权，但这些技术在准确，标准化和完整的行人路径网络上至关重要。临时收集工作导致数据记录稀疏，不可靠且不可行。本文提出了一种社会技术方法，用于以全州范围的规模收集，管理，服务和维护行人路径数据。将应用于航空影像和现有道路网络数据的计算机视觉方法提供的自动化与交互式工具提供的质量控制相结合，我们旨在在大约两年内为整个华盛顿州生产可路由的行人途径。我们从空中图像中提取路径，交叉和路缘坡道，将多输入分割方法与道路拓扑数据集成在一起，以确保连接的可路由网络。然后，我们将预测组织到选择的项目区域中，以将每个项目区域分为相交尺度任务。这些任务是通过管理并发，进度，反馈和数据管理的交互式工具来分配和跟踪的。我们证明，我们的自动化系统在产生可路径的路径网络方面的表现优于最先进的方法，从而大大减少了人类审查所需的时间。我们的结果表明，在整个状态的规模上产生准确，健壮的行人通路网络的可行性。本文打算通过提供行人股权，安全性和可及性来为国家规模的ADA合规性提供信息，并改善所有用户的城市环境。

### Movie Trailer Genre Classification Using Multimodal Pretrained Features 
[[arxiv](https://arxiv.org/abs/2410.19760)] [[cool](https://papers.cool/arxiv/2410.19760)] [[pdf](https://arxiv.org/pdf/2410.19760)]
> **Authors**: Serkan Sulun,Paula Viana,Matthew E. P. Davies
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-28
> **comment**: ef:Expert Systems with Applications 258 (2024) 125209
- **标题**: 电影预告片类型分类使用多模式预审前的功能
- **领域**: 计算机视觉和模式识别,人工智能,多媒体,图像和视频处理
- **摘要**: 我们介绍了一种新颖的电影类型分类方法，利用了一组易于访问的预审预周化模型。这些模型提取了与视觉风景，对象，字符，文本，语音，音乐和音频效果有关的高级功能。为了智能融合这些预处理的功能，我们训练具有低时间和内存要求的小分类器模型。我们的方法采用了变压器模型，使用了电影预告片的所有视频和音频帧，而无需进行任何时间池，有效利用所有元素之间的对应关系，而不是传统方法通常使用的固定和较低框架。我们的方法融合了源自不同任务和方式的特征，具有不同的维度，不同的时间长度和复杂的依赖性，而不是当前的方法。我们的方法在精确度，召回和平均精度（地图）方面优于最先进的电影类型分类模型。为了促进未来的研究，我们为整个Movienet数据集提供了预估计的功能，以及我们的流派分类代码和受过训练的模型，并公开使用。

### TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning 
[[arxiv](https://arxiv.org/abs/2410.19702)] [[cool](https://papers.cool/arxiv/2410.19702)] [[pdf](https://arxiv.org/pdf/2410.19702)]
> **Authors**: Xiangyu Zeng,Kunchang Li,Chenting Wang,Xinhao Li,Tianxiang Jiang,Ziang Yan,Songze Li,Yansong Shi,Zhengrong Yue,Yi Wang,Yali Wang,Yu Qiao,Limin Wang
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: Accepted by ICLR2025
- **标题**: 时间套件：通过接地调整改进MLLM，以进行长时间的视频理解
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **摘要**: 多模式的大型语言模型（MLLM）在简短的视频理解中表现出了令人印象深刻的表现。但是，了解长期视频对于MLLM仍然具有挑战性。本文提出了TimeSuite，这是一系列新设计的集合来调整现有的短形式视频MLLM，以进行长时间的视频理解，包括一个简单而有效的框架来处理长时间的视频序列，一个用于接地MLLM的高质量视频数据集，以及精心设计的指令调整任务，以明确的QA形式将接地监督纳入了接地。具体来说，根据视频聊天，我们提出了通过实施令牌弹装来压缩长的视频令牌并引入时间自适应位置编码（磁带）来提高视觉表示的时间意识，从而提出了作为VideoChat-T的长期Video mllm。同时，我们介绍了TimePro，这是一个全面的，以基础为中心的教学调谐数据集，该数据集由9个任务和349K高质量的接地注释组成。值得注意的是，我们设计了一种新的指令调整任务类型，称为“时间接地”字幕，以使用相应的时间戳预测来详细的视频描述。这种明确的时间位置预测将指导MLLM在生成描述时正确地了解视觉内容，从而降低LLMS引起的幻觉风险。实验结果表明，我们的时间套件提供了成功的解决方案，以增强短形式MLLM的长期理解能力，分别在Egoschema和Videomme的基准上获得5.6％和6.8％的提高。此外，VideoChat-T具有强劲的零击时间接地能力，极大地表现出了现有的最新MLLM。经过微调后，它与传统监督专家模型相当。

### Multi-modal Motion Prediction using Temporal Ensembling with Learning-based Aggregation 
[[arxiv](https://arxiv.org/abs/2410.19606)] [[cool](https://papers.cool/arxiv/2410.19606)] [[pdf](https://arxiv.org/pdf/2410.19606)]
> **Authors**: Kai-Yin Hong,Chieh-Chih Wang,Wen-Chieh Lin
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024), accepted by IROS2024
- **标题**: 使用基于学习的聚合使用时间结合的多模式运动预测
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 近年来，人们向基于学习的轨迹预测方法发生了转变，在解决不确定性和捕获多模式分布方面仍存在挑战。本文介绍了与基于学习的聚合的时间结合，这是一种元叠加，旨在减轻轨迹预测中缺失行为的问题，从而导致连续帧之间的预测不一致。与传统的模型交换不同，时间结合的效果从附近的框架进行预测，以增强空间覆盖范围和预测多样性。通过确认来自多个帧的预测，时间结合可以补偿各个框架预测中偶尔出现的错误。此外，由于缺乏对交通环境的考虑及其倾向于将具有不正确驾驶行为的候选轨迹分配给最终预测的候选轨迹，因此通常在模型结合中使用的轨迹级聚合不足以进行时间结合。我们进一步强调了基于学习的汇总的必要性，通过利用模式查询在类似detr的架构中进行我们的时间结合，利用附近框架的预测特征。我们的方法在Argoverse 2数据集上得到验证，显示出显着的改进：与最强的基线，QCNET相比，MINADE降低了4％，MINFDE降低了5％，降低了1.16％的失误率，强调了其效果和自主驱动的潜力。

### MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset 
[[arxiv](https://arxiv.org/abs/2410.19488)] [[cool](https://papers.cool/arxiv/2410.19488)] [[pdf](https://arxiv.org/pdf/2410.19488)]
> **Authors**: Xin Shen,Heming Du,Hongwei Sheng,Shuyun Wang,Hui Chen,Huiqiang Chen,Zhuojie Wu,Xiaobiao Du,Jiaying Ying,Ruihan Lu,Qingzheng Xu,Xin Yu
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: MM-Wlauslan：多视图多模式词级澳大利亚手语识别数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 孤立的手语识别（ISLR）的重点是识别单个手语的光泽。考虑到各个地理区域的符号语言多样性，开发特定区域的ISLR数据集对于支持沟通和研究至关重要。作为特定于澳大利亚的手语，奥斯兰仍然缺乏用于ISLR任务的专用大型单词级数据集。为了填补这一空白，我们策划\下划线{\ textbf {textbf {the the第一个}}大规模多视图多模式级文字级的澳大利亚手语识别数据集，称为MM-Wlauslan。与其他公开数据集相比，MM-Wlauslan具有三个重要优势：（1）数据量最大，（2）最广泛的词汇量，以及（3）最多样化的多模式相机视图。具体来说，我们记录了282K+标志视频，其中涵盖了3,215个通常在工作室环境中由73个签名者呈现的Auslan光泽。此外，我们的拍摄系统包括两种不同类型的摄像机，即三台Kinect-V2摄像机和一个Realsense相机。我们将相机半球形放置在模型的前半部分，并使用所有四个相机同时录制视频。此外，我们使用MM-Wlauslan上各种多模式ISLR设置的最新方法基准结果，包括多视图，跨相机和跨视图。实验结果表明，MM-Wlauslan是一个具有挑战性的ISLR数据集，我们希望该数据集将有助于Auslan的发展和全球范围内的标志语言的发展。所有数据集和基准都可以在MM-Wlauslan上找到。

### Fusion-then-Distillation: Toward Cross-modal Positive Distillation for Domain Adaptive 3D Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2410.19446)] [[cool](https://papers.cool/arxiv/2410.19446)] [[pdf](https://arxiv.org/pdf/2410.19446)]
> **Authors**: Yao Wu,Mingwei Xing,Yachao Zhang,Yuan Xie,Yanyun Qu
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 融合 - 然后进行：朝着域自适应3D语义分割的跨模式正蒸馏
- **领域**: 计算机视觉和模式识别
- **摘要**: 在跨模式的无监督域的适应性中，一种对源域数据（例如，合成）训练的模型适用于目标域数据（例如，现实世界），而无需访问目标注释。以前的方法试图在每个域中相互模拟的跨模式输出，该输出强制执行在不同域中可以达到的类概率分布。但是，他们忽略了跨模式学习中异质融合带来的互补性。鉴于此，我们提出了一种新型的融合 - 然后进行缩减（FTD ++）方法，以探索3D语义分割的源和目标域的跨模式正蒸馏。 FTD ++实现了输出之间的分布一致性，不仅对于2D图像和3D点云，而且对于源域和增强域。特别是，我们的方法包含三种关键成分。首先，我们提出一个模型 - 反应特征融合模块，以生成用于建立潜在空间的跨模式融合表示。在这个空间中，两种方式是实施最大相关性和互补性的。其次，提出的跨模式正蒸馏保留了多模式输入的完整信息，并将源域的语义内容与目标域的样式相结合，从而实现了域模式对齐。最后，跨模式的伪伪标记是通过自我训练方式设计为伪标签的不确定性的。广泛的实验报告了最先进的结果，结果是在无监督和半监督的设置下的几个领域自适应场景。代码可在https://github.com/barcaaaa/ftd-plusplus上找到。

### Unified Cross-Modal Image Synthesis with Hierarchical Mixture of Product-of-Experts 
[[arxiv](https://arxiv.org/abs/2410.19378)] [[cool](https://papers.cool/arxiv/2410.19378)] [[pdf](https://arxiv.org/pdf/2410.19378)]
> **Authors**: Reuben Dorent,Nazim Haouchine,Alexandra Golby,Sarah Frisken,Tina Kapur,William Wells
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: Manuscript under review
- **标题**: 统一的跨模式图像合成与专家产品的分层混合物
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: 我们提出了称为MMHVAE的多模式层次变异自动编码器的深层混合物，该自动编码器综合了以不同方式观察到的图像中缺少的图像。 MMHVAE的设计着重于应对四个挑战：（i）创建多模式数据的复杂潜在表示以生成高分辨率图像； （ii）鼓励变异分布来估计跨模式图像合成所需的缺失信息； （iii）学习在缺少数据的背景下融合多模式信息； （iv）利用数据集级信息在培训时处理不完整的数据集。在术前大脑多参数磁共振和术中超声成像的挑战性问题上进行了广泛的实验。

### Resolution Enhancement of Under-sampled Photoacoustic Microscopy Images using Implicit Neural Representations 
[[arxiv](https://arxiv.org/abs/2410.19786)] [[cool](https://papers.cool/arxiv/2410.19786)] [[pdf](https://arxiv.org/pdf/2410.19786)]
> **Authors**: Youshen Xiao,Sheng Liao,Xuanyang Tian,Fan Zhang,Xinlong Dong,Yunhui Jiang,Xiyu Chen,Ruixi Sun,Yuyao Zhang,Fei Gao
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 使用隐式神经表示的分辨率增强的光声显微镜图像的分辨率增强
- **领域**: 计算机视觉和模式识别
- **摘要**: 声学声学显微镜（AR-PAM）有望用于皮下血管成像，但其空间分辨率受到点扩散功能（PSF）的约束。 Richardson-Lucy和基于模型的Deonvolution等传统的反卷积方法使用PSF来改善分辨率。但是，准确测量PSF是困难的，从而依赖较少准确的盲卷曲技术。此外，AR-PAM遭受了较长的扫描时间，可以通过下采样来减少，但是这需要从采样不足的数据中有效恢复图像，这项任务在传统的插值方法中不足，尤其是在高采样率下。为了应对这些挑战，我们提出了一种基于隐式神经表示（INR）的方法。该方法学习了从空间坐标到初始声压的连续映射，克服了离散成像的局限性并增强了AR-PAM的分辨率。通过将PSF视为INR框架中的可学习参数，我们的技术会减轻与PSF估计相关的不准确性。我们评估了模拟血管数据的方法，显示了峰值信噪比（PSNR）和结构相似性指数（SSIM）的显着改善，而不是常规方法。在叶静脉和体内小鼠脑微脉管图中也观察到了定性增强。当应用于自定义AR-PAM系统时，使用铅笔铅的实验表明，我们的方法可提供更清晰的高分辨率结果，这表明其潜力可以推进光声显微镜。

### Unsupervised Modality Adaptation with Text-to-Image Diffusion Models for Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2410.21708)] [[cool](https://papers.cool/arxiv/2410.21708)] [[pdf](https://arxiv.org/pdf/2410.21708)]
> **Authors**: Ruihao Xia,Yu Liang,Peng-Tao Jiang,Hao Zhang,Bo Li,Yang Tang,Pan Zhou
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: NeurIPS 2024
- **标题**: 语义分割的文本对图像扩散模型的无监督模态改编
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管他们成功，但无监督的域适应方法用于语义分割，主要集中于图像域之间的适应性，并且不利用其他丰富的视觉模态（例如深度，红外和事件）。这种限制阻碍了他们的性能，并限制了他们在现实世界多模式方案中的应用。为了解决这个问题，我们提出了使用文本到图像扩散模型（MADM）进行语义分割任务的模态适应，该任务利用文本到图像扩散模型在广泛的图像文本对上进行了预训练，以增强模型的交叉模式功能。具体而言，MADM包括两个关键的互补组件，以应对主要挑战。首先，由于较大的模态差距，使用一个模态数据来生成伪标签，以使另一种模态的精度显着下降。为了解决这个问题，MADM设计基于扩散的伪标签生成，该生成增加了潜在噪声以稳定伪标记并提高标签精度。其次，为了克服扩散模型中潜在的低分辨率特征的局限性，MADM引入了标签调色板和潜在回归，通过调色板将一热的编码标签转换为RGB形式，并在潜在空间中对其进行回归，从而确保预先培训的解码器以提高采样以获得细化的功能。广泛的实验结果表明，MADM在各种模态任务中实现最新的适应性表现，包括图像到深度，红外和事件方式。我们通过https://github.com/xiarho/madm开放代码和模型。

### OFER: Occluded Face Expression Reconstruction 
[[arxiv](https://arxiv.org/abs/2410.21629)] [[cool](https://papers.cool/arxiv/2410.21629)] [[pdf](https://arxiv.org/pdf/2410.21629)]
> **Authors**: Pratheba Selvaraju,Victoria Fernandez Abrevaya,Timo Bolkart,Rick Akkerman,Tianyu Ding,Faezeh Amjadi,Ilya Zharkov
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: OFER：遮挡的面部表情重建
- **领域**: 计算机视觉和模式识别
- **摘要**: 从单个图像重建3D面模型是一个固有的问题，在遮挡的情况下，它变得更加具有挑战性。除了较少的可用观察结果外，闭塞还引入了额外的歧义来源，其中多个重建同样有效。尽管这个问题无处不在，但很少有方法可以解决其多种假设的性质。在本文中，我们介绍了Ofer，这是一种新型的单位图3D面部重建方法，即使在强烈的遮挡下，也可以产生合理，多样和表现力的3D面。具体而言，我们训练两个扩散模型，以生成以输入图像为条件的面部参数模型的形状和表达系数。这种方法捕获了问题的多模式性质，从而产生解决方案作为输出的分布。但是，为了保持各种表达式的一致性，挑战是选择最佳的匹配形状。为了实现这一目标，我们提出了一种新颖的排名机制，该机制基于预测的形状精度得分对形状扩散网络的输出进行分类。我们使用标准基准评估我们的方法，并引入Co-545，这是一种新协议和数据集，旨在评估闭塞下表达面的准确性。我们的结果表明，基于闭塞的方法的性能提高了，同时还可以为给定图像产生各种表达式。

### Multi-path Exploration and Feedback Adjustment for Text-to-Image Person Retrieval 
[[arxiv](https://arxiv.org/abs/2410.21318)] [[cool](https://papers.cool/arxiv/2410.21318)] [[pdf](https://arxiv.org/pdf/2410.21318)]
> **Authors**: Bin Kang,Bin Chen,Junjie Wang,Yong Xu
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 对文本对象人员检索的多路探索和反馈调整
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 基于文本的人检索旨在使用文本描述作为查询来识别特定人员。现有的AD VANGED方法通常取决于视觉训练的（VLP）模型，以促进有效的跨模式比对。但是，VLP Mod-els的固有约束（包括全局对齐偏见和不断的自我反馈调节）会阻碍每种效果的最佳检索。在本文中，我们提出了MEFA，MEFA是一种多条纹的探索，反馈和调整框架，该框架深入探讨了内部和模式间的内在反馈，以进行有针对性的调整，从而实现更精确的人角度关联。具体而言，我们首先设计了一种模态推理途径，该途径生成了用于跨模式数据的硬性阴性SAM PLES，利用这些样品的反馈来完善模式内推理，从而增强对微妙差异的敏感性。随后，我们介绍了一种跨模式的改进途径，该途径利用全球信息和模式反馈来完善局部形成，从而增强了其全球语义表示。最后，歧视性线索校正途径Incorpo率是二级相似性的细粒度特征作为歧视线索，以进一步减轻这些特征差异引起的检索失败。三个公共基准的实验结果表明，MEFA可以实现高级人的检索性能，而无需其他数据或复杂的结构。

### MMDocBench: Benchmarking Large Vision-Language Models for Fine-Grained Visual Document Understanding 
[[arxiv](https://arxiv.org/abs/2410.21311)] [[cool](https://papers.cool/arxiv/2410.21311)] [[pdf](https://arxiv.org/pdf/2410.21311)]
> **Authors**: Fengbin Zhu,Ziyang Liu,Xiang Yao Ng,Haohui Wu,Wenjie Wang,Fuli Feng,Chao Wang,Huanbo Luan,Tat Seng Chua
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-29
> **comment**: Under review
- **标题**: mmdocbench：基准测试大型视觉模型，以了解细粒度的视觉文档理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉语言模型（LVLM）在许多视觉任务中都取得了出色的性能，但是它们在细粒度的视觉理解中的能力仍未得到充分评估。现有的基准测试要么包含有限的细粒评估样本，这些样本与其他数据混合在一起，要么仅限于自然图像中的对象级评估。为了整体评估LVLM的细粒视觉理解能力，我们建议使用具有多范围和多模式信息的文档图像来补充自然图像。从这个角度来看，我们构建了MMDocbench，这是一种基准，具有各种无OCR文档的文档理解任务，以评估细粒度的视觉感知和推理能力。 MMDOCBENCH通过4,338对QA对定义了15项主要任务和11,353个支持区域，涵盖了各种文档图像，例如研究论文，收据，财务报告，Wikipedia表，图表，图表和信息图表。基于MMDocbench，我们使用13个开源和3个专有的高级LVLM进行了广泛的实验，评估了它们在不同任务和文档图像类型的优势和劣势。将公开提供基准，任务说明和评估代码。

### LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior 
[[arxiv](https://arxiv.org/abs/2410.21264)] [[cool](https://papers.cool/arxiv/2410.21264)] [[pdf](https://arxiv.org/pdf/2410.21264)]
> **Authors**: Hanyu Wang,Saksham Suri,Yixuan Ren,Hao Chen,Abhinav Shrivastava
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: Project page: https://hywang66.github.io/larp/
- **标题**: LARP：具有学识渊博的自学生成性先验的标记视频
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们提出了LARP，这是一种新型的视频令牌，旨在克服当前视频令牌化方法的局限性，用于自回归（AR）生成模型。与将本地视觉贴片直接编码为离散令牌的传统贴片引导者不同，LARP引入了整体令牌化方案，该方案使用一组学习的整体查询从视觉内容中收集信息。该设计使LARP可以捕获更多的全球和语义表示，而不是仅限于本地补丁级信息。此外，它通过支持任意数量的离散令牌来提供灵活性，从而根据任务的特定要求实现自适应和高效的令牌化。为了使离散的令牌空间与下游AR生成任务保持一致，LARP将轻量级的AR变压器集成为训练时间的先验模型，该模型可以预测其离散潜在空间的下一个令牌。通过在培训期间合并先前的模型，LARP学习了一个潜在空间，该空间不仅可以针对视频重建进行优化，而且还以更有利于自动回归产生的方式结构。此外，此过程定义了离散令牌的顺序顺序，在训练过程中逐渐将它们推向最佳配置，从而确保推理时更顺畅，更精确。全面的实验表明了LARP的出色表现，可以在UCF101类别的视频生成基准上实现最先进的FVD。 LARP增强了AR模型与视频的兼容性，并打开了建立统一的高保真多模式大型语言模型（MLLM）的潜力。

### Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines 
[[arxiv](https://arxiv.org/abs/2410.21220)] [[cool](https://papers.cool/arxiv/2410.21220)] [[pdf](https://arxiv.org/pdf/2410.21220)]
> **Authors**: Zhixin Zhang,Yiyuan Zhang,Xiaohan Ding,Xiangyu Yue
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: Code is available at https://github.com/cnzzx/VSA
- **标题**: 视觉搜索助理：将视觉语言模型授权作为多模式搜索引擎
- **领域**: 计算机视觉和模式识别,人工智能,信息检索,机器学习
- **摘要**: 搜索引擎可以通过文本检索未知信息。但是，在理解陌生的视觉内容时，传统方法缺乏，例如识别模型从未见过的对象。对于大型视觉语言模型（VLMS），这一挑战尤其明显：如果该模型尚未暴露于图像中描述的对象，则它很难为用户对该图像的问题生成可靠的答案。此外，随着新的对象和事件不断出现，由于沉重的计算负担，经常更新VLM是不切实际的。为了解决这一限制，我们提出了视觉搜索助理，这是一个新颖的框架，可促进VLMS和Web代理之间的合作。这种方法利用VLMS的视觉理解功能和Web代理的实时信息访问来通过Web执行开放世界检索的生成。通过通过此协作整合视觉和文本表示，即使图像对系统是新颖的，该模型也可以提供知情的响应。在开放式和封闭设置的QA基准上进行的广泛实验表明，视觉搜索助手显着胜过其他模型，并且可以广泛应用于现有的VLMS。

### LiGAR: LiDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity Recognition 
[[arxiv](https://arxiv.org/abs/2410.21108)] [[cool](https://papers.cool/arxiv/2410.21108)] [[pdf](https://arxiv.org/pdf/2410.21108)]
> **Authors**: Naga Venkata Sai Raviteja Chappa,Khoa Luu
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: Accepted at WACV 2025; 14 pages, 4 figures, 10 tables
- **标题**: LIGAR：用于多模式组活性识别的激光雷达引导层次变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于多代理相互作用的复杂性质，小组活动识别（GAR）在计算机视觉中仍然具有挑战性。本文介绍了Ligar，这是一种用于多模式组活性识别的激光雷达引导的分层变压器。 Ligar利用LiDAR数据作为结构性主链，以指导视觉和文本信息的处理，从而可以牢固地处理遮挡和复杂的空间排列。我们的框架结合了多尺度的激光变压器，跨模式引导的注意力和一个自适应融合模块，以有效地在不同语义级别集成多模式数据。 Ligar的分层体系结构捕获了各种粒度的小组活动，从个人动作到场景级的动态。关于JRDB-PAR，排球和NBA数据集的广泛实验证明了Ligar的出色表现，可实现最先进的结果，在JRDB-PAR上的F1得分高达10.6％，NBA数据集对JRDB-PAR的F1得分和5.9％。值得注意的是，即使在推断过程中不可用，Ligar仍保持高性能，展示其适应性。我们的消融研究突出了每个组件的重要贡献以及我们多模式多尺度方法在促进小组活动识别领域方面的有效性。

### Efficient Mixture-of-Expert for Video-based Driver State and Physiological Multi-task Estimation in Conditional Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2410.21086)] [[cool](https://papers.cool/arxiv/2410.21086)] [[pdf](https://arxiv.org/pdf/2410.21086)]
> **Authors**: Jiyao Wang,Xiao Yang,Zhenyu Wang,Ximeng Wei,Ange Wang,Dengbo He,Kaishun Wu
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 有条件自主驾驶中基于视频的驱动程序状态和生理多任务估计的高效混合物
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 道路安全仍然是全世界的一项关键挑战，每年约有135万死亡归因于交通事故，这通常是由于人为错误所致。随着我们朝着更高水平的车辆自动化发展，挑战仍然存在，因为自动化的驾驶如果从事与驾驶无关的任务（NDRT），或者如果驾驶是唯一的任务，则可以在认知驱动器上超越驱动器。这要求迫切需要有效的驱动器监控系统（DMS），该系统可以评估SAE级别2/3自主驾驶环境中的认知负荷和嗜睡。在这项研究中，我们提出了一种新型的多任务DM，称为VDMOE，该DME vDMOE，它利用RGB视频输入以非侵入性监视驱动程序状态。通过利用关键的面部特征来最大程度地减少计算负载并整合远程光摄影学（RPPG）以进行生理见解，我们的方法在保持效率的同时提高了检测准确性。此外，我们优化了专家（MOE）框架的混合物，以适应多模式输入并改善各个任务的性能。引入了一种新型的先前包含的正则化方法，用于与统计先验的对齐模型输出，从而加速收敛并减轻过度适合风险。我们通过创建一个新数据集（MCDD）来验证我们的方法，该数据集包含来自42位参与者的RGB视频和生理指标和两个公共数据集。我们的发现证明了VDMOE在监视驾驶员状态的有效性，从而有助于更安全的自动驾驶系统。代码和数据将发布。

### IndraEye: Infrared Electro-Optical UAV-based Perception Dataset for Robust Downstream Tasks 
[[arxiv](https://arxiv.org/abs/2410.20953)] [[cool](https://papers.cool/arxiv/2410.20953)] [[pdf](https://arxiv.org/pdf/2410.20953)]
> **Authors**: Manjunath D,Prajwal Gurunath,Sumanth Udupa,Aditya Gandhamal,Shrikar Madhu,Aniruddh Sikdar,Suresh Sundaram
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: 9 pages, 2 figures
- **标题**: Indraeye：用于强大下游任务的红外电光无人机感知数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 深度神经网络（DNNS）在受到电流（EO）摄像机捕获的鲜明的图像进行培训时，表现出了出色的性能，该图像提供了丰富的纹理细节。但是，在诸如空中感知之类的关键应用中，DNN必须在所有条件下保持一致的可靠性，包括弱光场景，其中EO摄像机经常难以捕获足够的细节。此外，由于不同的高度和倾斜角度的规模变化，基于无人机的空中对象检测面临重大挑战，增加了另一层复杂性。现有方法通常仅将照明变化或样式变化作为域的变化，但在空中感知中，相关转移也会影响DNN的性能。在本文中，我们介绍了Indraeye数据集，这是一种用于各种任务的多传感器（EO-IR）数据集。它包括5,612张图像，其中包含145,666个实例，包括多个观看角度，高度，七个背景以及整个印度次大陆的不同时间。数据集为多模式学习，对象检测和分割的域适应以及传感器特定的优势和劣势探索了一些研究机会。 Indraeye的目标是通过支持更健壮和准确的空中感知系统的发展，尤其是在具有挑战性的条件下。 Indraeye数据集以对象检测和语义分割任务为基准。数据集和源代码可在https://bit.ly/indraeye上找到。

### Improving Generalization in Visual Reasoning via Self-Ensemble 
[[arxiv](https://arxiv.org/abs/2410.20883)] [[cool](https://papers.cool/arxiv/2410.20883)] [[pdf](https://arxiv.org/pdf/2410.20883)]
> **Authors**: Tien-Huy Nguyen,Quang-Khai Tran,Anh-Tuan Quang-Hoang
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 通过自我汇总改善视觉推理的概括
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉推理的认知能力必须整合多模式感知处理，常识和世界的外部知识。近年来，已经提出了大量大型视觉模型（LVLM），表明了跨不同领域和任务的常识性推理的出色能力和出色的能力。但是，培训这样的LVLM需要大量昂贵的资源。最近的方法，而不是在各种大型数据集上从头开始训练LVLM，而是专注于探索利用许多不同LVLM的功能（例如集合方法）的功能。在这项工作中，我们提出了一种自我安装，这是一种新的方法，可改善模型的概括和视觉推理，而无需更新任何参数，一种无训练的方法。我们的关键见解是，我们意识到LVLM本身可以在无需任何其他LVLM的情况下进行合奏，这有助于解锁其内部功能。对各种基准测试的广泛实验证明了我们方法在在SketchyyVQA，外部知识VQA和分布外VQA任务上实现最先进的（SOTA）性能的有效性。

### Fidelity-Imposed Displacement Editing for the Learn2Reg 2024 SHG-BF Challenge 
[[arxiv](https://arxiv.org/abs/2410.20812)] [[cool](https://papers.cool/arxiv/2410.20812)] [[pdf](https://arxiv.org/pdf/2410.20812)]
> **Authors**: Jiacheng Wang,Xiang Chen,Renjiu Hu,Rongguang Wang,Min Liu,Yaonan Wang,Jiazheng Wang,Hao Li,Hang Zhang
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: Fidelignity强化的流离失所编辑2024 SHG-BF挑战
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: 第二次谐波产生（SHG）和明亮场（BF）显微镜的共审查可以使组织成分和胶原蛋白纤维的分化，有助于对人乳腺癌和胰腺癌组织的分析。但是，SHG和BF图像之间的巨大差异对将SHG对准BF的当前基于学习的注册模型构成了挑战。在本文中，我们提出了一个新型的多模式注册框架，该框架采用了富达性的流离失所编辑来应对这些挑战。该框架集成了批处理的对比度学习，基于功能的预订和实例级优化。 Learn2Reg Comulisglobe SHG-BF挑战的实验结果验证了我们方法的有效性，并确保了在线排行榜上的第一名。

### Face-MLLM: A Large Face Perception Model 
[[arxiv](https://arxiv.org/abs/2410.20717)] [[cool](https://papers.cool/arxiv/2410.20717)] [[pdf](https://arxiv.org/pdf/2410.20717)]
> **Authors**: Haomiao Sun,Mingjie He,Tianheng Lian,Hu Han,Shiguang Shan
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 脸部：一个大的面部感知模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管多模式的大语言模型（MLLM）在广泛的视力语言任务上取得了有希望的结果，但很少探索他们感知和理解人面孔的能力。在这项工作中，我们全面评估了面部感知任务上的现有MLLM。定量结果表明，现有的MLLM努力处理这些任务。主要原因是缺乏包含对人脸的细粒描述的图像文本数据集。为了解决这个问题，我们设计了一个用于构建数据集的实用管道，我们在此过程中进一步构建了一种新型的多模式大面感知模型，即面对面。具体而言，我们重新注释了Laion-Face数据集，其中包含更详细的面部字幕和面部属性标签。此外，我们使用询问答案样式重新构建了传统的面部数据集，该样式适合MLLM。与这些丰富的数据集一起，我们开发了一种新型的三阶段MLLM培训方法。在前两个阶段中，我们的模型分别学习视觉文本对齐和基本的视觉问题答案功能。在第三阶段，我们的模型学会了处理多个专业的面部感知任务。实验结果表明，我们的模型超过了五个著名的面部感知任务的先前MLLM。此外，在我们新引入的零摄像面部属性分析任务上，我们的面部效果也提出了卓越的性能。

### PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection and Natural Language Interpretation 
[[arxiv](https://arxiv.org/abs/2410.22623)] [[cool](https://papers.cool/arxiv/2410.22623)] [[pdf](https://arxiv.org/pdf/2410.22623)]
> **Authors**: Ryozo Masukawa,Sanggeon Yun,Yoshiki Yamaguchi,Mohsen Imani
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: Accepted to WACV 2025, Dataset Available Here : https://ryozomasukawa.github.io/PV-VTT.github.io/
- **标题**: PV-VTT：用于特定任务异常检测和自然语言解释的以隐私为中心的数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频犯罪检测是计算机视觉和人工智能的重要应用。但是，现有数据集主要集中于通过分析整个视频剪辑来检测严重犯罪，通常会忽略可能阻止这些犯罪的前体活动（即侵犯隐私行为）。为了解决此限制，我们将PV-VTT（违反隐私视频视频）介绍给文本，这是一个旨在识别侵犯隐私行为的独特多模式数据集。 PV-VTT在方案中为视频和文本提供了详细的注释。为了确保个人在视频中的隐私，我们仅提供视频功能向量，以避免发布任何原始视频数据。这种以隐私为重点的方法使研究人员可以在保护参与者机密的同时使用数据集。认识到侵犯隐私通常是模棱两可的，我们提出了一个基于图形神经网络（GNN）的视频描述模型。我们的模型生成了一个基于GNN的提示，该提示具有大型语言模型（LLM）的图像，该图像提供了具有成本效益和高质量的视频描述。通过利用单个视频框架以及相关文本，我们的方法减少了所需的输入令牌的数量，在优化LLM API-USAGE的同时保持描述性质量。广泛的实验验证了我们在视频描述任务和PV-VTT数据集的灵活性中的方法的有效性和解释性。

### Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2410.22489)] [[cool](https://papers.cool/arxiv/2410.22489)] [[pdf](https://arxiv.org/pdf/2410.22489)]
> **Authors**: Zhaochong An,Guolei Sun,Yun Liu,Runjia Li,Min Wu,Ming-Ming Cheng,Ender Konukoglu,Serge Belongie
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: Published at ICLR 2025 (Spotlight)
- **标题**: 多模式有助于几个射击3D点云语义分段
- **领域**: 计算机视觉和模式识别
- **摘要**: 很少有3D点云分割（FS-PC）旨在将模型概括为以最少的注释支持样本分割新类别。尽管现有的FS-PCS方法已显示出希望，但它们主要集中于单峰点云输入，忽略了利用多模式信息的潜在优势。在本文中，我们通过引入多模式FS-PCS设置来解决此差距，并利用文本标签和潜在的2D图像模式。在此易于实现的设置下，我们介绍了多模式的少数segnet（MM-FSS），该模型有效地利用了来自多种模式的互补信息。 MM-FSS采用一个带有两个头的共享主链来提取世间和单峰视觉特征，以及一个预验证的文本编码器来生成文本嵌入。为了充分利用多模式信息，我们提出了一个多模式相关融合（MCF）模块来生成多模式相关性，并使用多模式语义融合（MSF）模块，以使用文本知识的语义指南来完善相关性。此外，我们提出了一种简单而有效的测试时间自适应跨模式校准（TACC）技术，以减轻训练偏见，从而进一步改善概括。 S3DIS和SCANNET数据集的实验结果表明，我们的方法可以取得重大的性能提高。我们方法的功效表明了利用FS-PC的普遍自由方式的好处，为将来的研究提供了宝贵的见解。该代码可从https://github.com/zwaochongan/multimodality-3d-few-shot获得

### ContextIQ: A Multimodal Expert-Based Video Retrieval System for Contextual Advertising 
[[arxiv](https://arxiv.org/abs/2410.22233)] [[cool](https://papers.cool/arxiv/2410.22233)] [[pdf](https://arxiv.org/pdf/2410.22233)]
> **Authors**: Ashutosh Chaubey,Anoubhav Agarwaal,Sartaki Sinha Roy,Aayush Agrawal,Susmita Ghose
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: Accepted at WACV 2025
- **标题**: ContextiQ：一种基于多模式专家的视频检索系统，用于上下文广告
- **领域**: 计算机视觉和模式识别,人工智能,信息检索
- **摘要**: 上下文广告提供与用户正在查看的内容保持一致的广告。社交平台和流媒体服务上视频内容的快速增长以及隐私问题增加了对上下文广告的需求。将正确的广告放在正确的上下文中会创造出无缝而愉快的广告观看体验，从而获得更高的受众参与度，并最终获得更好的广告货币化。从技术的角度来看，有效的上下文广告需要一个视频检索系统，能够在非常细腻的层面上理解复杂的视频内容。基于联合多模式培训的当前文本对视频检索模型要求大量数据集和计算资源，从而限制了它们的实用性，并且缺乏AD生态系统集成所需的关键功能。我们介绍了Contextiq，这是一种基于多模式专家的视频检索系统，专门为上下文广告设计。 Contextiq使用特定于模态的专家-Video，音频，成绩单（字幕）和元数据，例如对象，动作，情感等。创建语义上丰富的视频表示。我们表明，在没有联合培训的情况下，我们的系统可以在多个文本到视频检索基准上获得更好或可比较的结果。我们的消融研究强调了利用多种方式以增强视频检索精度而不是仅使用视觉语言模型的好处。此外，我们还展示了如何将视频检索系统（例如ContextIQ）用于广告生态系统中的上下文广告，同时还解决了与品牌安全和过滤不适当的内容有关的问题。

### A Survey on RGB, 3D, and Multimodal Approaches for Unsupervised Industrial Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2410.21982)] [[cool](https://papers.cool/arxiv/2410.21982)] [[pdf](https://arxiv.org/pdf/2410.21982)]
> **Authors**: Yuxuan Lin,Yang Chang,Xuan Tong,Jiawen Yu,Antonio Liotta,Guofan Huang,Wei Song,Deyu Zeng,Zongze Wu,Yan Wang,Wenqiang Zhang
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: 28 pages, 18 figures
- **标题**: 无监督工业异常检测的RGB，3D和多模式方法的调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 为了提高工业信息化，无监督的工业异常检测（UIAD）技术有效地克服了异常样本的稀缺性，并显着增强了智能制造的自动化和可靠性。尽管RGB，3D和多模式异常检测表明在工业信息化领域中具有全面和强大的功能，但现有的有关工业异常检测的综述尚未充分分类和讨论3D和多模态环境中的方法。我们专注于3D UIAD和多模式UIAD，提供了三种模态环境中无监督工业异常检测的全面摘要。首先，我们将调查与最近的作品进行比较，引入了常用的数据集，评估指标以及对异常检测问题的定义。其次，我们总结了RGB，3D和多模式UIAD的五个研究范例以及RGB UIAD中的三个新兴工业制造优化方向，并在多模态设置中审查了三个多模式特征融合策略。最后，我们概述了UIAD在三种模态环境中目前面临的主要挑战，并提供了对未来发展方向的见解，旨在为研究人员提供详尽的参考，并为发展工业信息的发展提供新的观点。相应的资源可在https://github.com/sunny5250/awesome-multi-setting-uiad上获得。

### Spatio-temporal Transformers for Action Unit Classification with Event Cameras 
[[arxiv](https://arxiv.org/abs/2410.21958)] [[cool](https://papers.cool/arxiv/2410.21958)] [[pdf](https://arxiv.org/pdf/2410.21958)]
> **Authors**: Luca Cultrera,Federico Becattini,Lorenzo Berlincioni,Claudio Ferrari,Alberto Del Bimbo
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: Under review at CVIU. arXiv admin note: substantial text overlap with arXiv:2409.10213
- **标题**: 带有事件摄像机的动作单元分类的时空变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 已经从不同角度研究了面部分析，以推断情感，姿势，形状和地标。传统上，使用RGB摄像机，但是对于精细的任务，由于其延迟，标准传感器可能无法完成任务，因此无法记录和检测带有高度信息信号的微动作，这对于推断受试者的真实情感是必不可少的。事件摄像机越来越多地获得利息作为解决此问题和类似的高框架任务的解决方案。我们提出了一种新型时空视觉变压器模型，该模型使用移动的斑块令牌化（SPT）和局部自我注意力（LSA）来增强事件流的动作单位分类的准确性。我们还解决了文献中缺乏标记的事件数据，这可以被认为是RGB和神经形态视觉模型之间现有差距的主要原因之一。在事件域中，收集数据更难，因为它不能从网络上爬走，并且标记帧应考虑事件的聚合率，并且在某些帧中可能看不到静态零件。为此，我们提出了Facemorphic，这是一个由RGB视频和事件流组成的时间同步的多模式数据集。该数据集在带有面部动作单元的视频级别上注释，并包含带有各种可能应用的流，从3D形状估计到唇部阅读。然后，我们展示了时间同步如何允许无需手动注释视频的有效神经形态的面部分析：我们相反，我们通过在3D空间中代表面部形状来利用跨模式的跨模式监督。我们提出的模型通过有效捕获空间和时间信息来优于基线方法，这对于识别微妙的面部微表达至关重要。

### Enhanced Survival Prediction in Head and Neck Cancer Using Convolutional Block Attention and Multimodal Data Fusion 
[[arxiv](https://arxiv.org/abs/2410.21831)] [[cool](https://papers.cool/arxiv/2410.21831)] [[pdf](https://arxiv.org/pdf/2410.21831)]
> **Authors**: Aiman Farooq,Utkarsh Sharma,Deepak Mishra
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: Accepted to [ACCV 2024 Workshop]
- **标题**: 使用卷积阻止注意力和多模式数据融合，在头颈癌中增强了生存预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 头颈癌（HNC）的准确生存预测对于指导临床决策和优化治疗策略至关重要。传统模型（例如COX比例危害）已被广泛使用，但其处理复杂的多模式数据的能力受到限制。本文提出了一种基于深度学习的方法利用CT和PET成像方式来预测HNC患者的生存结果。我们的方法将特征提取与卷积块注意模块（CBAM）和多模式数据融合层相结合，该模块结合了成像数据以生成紧凑的特征表示。最终预测是通过完全参数离散的时间生存模型来实现的，从而允许克服传统生存模型的局限性的柔性危险函数。我们使用Hecktor和Head-Neck-Hadiomics-HN1数据集评估了我们的方法，这表明了与惯用统计和机器学习模型相比，其表现出色。结果表明，我们的深度学习模型显着提高了生存预测的准确性，为HNC提供了一个可靠的工具，用于个性化治疗计划

### Volumetric Conditioning Module to Control Pretrained Diffusion Models for 3D Medical Images 
[[arxiv](https://arxiv.org/abs/2410.21826)] [[cool](https://papers.cool/arxiv/2410.21826)] [[pdf](https://arxiv.org/pdf/2410.21826)]
> **Authors**: Suhyun Ahn,Wonjung Park,Jihoon Cho,Seunghyuck Park,Jinah Park
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: 17 pages, 18 figures, accepted @ WACV 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **摘要**: 使用预处理扩散模型上的其他模块的空间控制方法已引起人们对自然图像中有条件产生的关注。这些方法在利用大型模型的功能的同时，以新的条件指导生成过程。在3D医学成像的背景下，作为培训策略，它们可能是有益的，由于较高的计算成本和数据稀缺，从头开始训练扩散模型是具有挑战性的。但是，尚未探索具有其他模块在3D医学图像中的空间控制方法的潜在应用。在本文中，我们提出了一种针对3D医学图像的量身定制的空间控制方法，该方法具有新型轻型模块，体积调节模块（VCM）。我们的VCM采用不对称的U-NET体系结构来有效地编码来自各种3D条件的复杂信息，从而在图像合成中提供了详细的指导。为了检查空间控制方法的适用性以及VCM在3D医疗数据中的有效性，我们在跨各种数据集大小的单模式条件方案下进行实验，从具有10个样品的极小数据集到具有500个样品的大型数据集。实验结果表明，VCM对有条件的生成有效，并且在需要更少的培训数据和计算资源方面有效。我们通过医学图像的轴向超分辨率进一步研究了空间控制方法的潜在应用。我们的代码可在\ url {https://github.com/ahn-ssu/vcm}上找到

### MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding 
[[arxiv](https://arxiv.org/abs/2410.21747)] [[cool](https://papers.cool/arxiv/2410.21747)] [[pdf](https://arxiv.org/pdf/2410.21747)]
> **Authors**: Yuan Wang,Di Huang,Yaqi Zhang,Wanli Ouyang,Jile Jiao,Xuetao Feng,Yan Zhou,Pengfei Wan,Shixiang Tang,Dan Xu
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: MotionGPT-2：一种通用运动语言模型，用于运动和理解
- **领域**: 计算机视觉和模式识别
- **摘要**: Generating lifelike human motions from descriptive texts has experienced remarkable research focus in the recent years, propelled by the emerging requirements of digital humans.Despite impressive advances, existing approaches are often constrained by limited control modalities, task specificity, and focus solely on body motion representations.In this paper, we present MotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these limitations. MotionGPT-2通过预先训练的大语言模型（LLM）适应多个运动的任务，并支持多模式控制条件。它量化了多模式输入，例如文本和单帧构成离散的llm解释令牌，将它们无缝集成到LLM的词汇中。然后将这些令牌组织成统一的提示，从而引导LLM通过预训练的范式来产生运动输出。我们还表明，拟议的MotionGPT-2高度适应了具有挑战性的3D整体运动生成任务，该任务是由创新的运动离散框架，部分意识的VQVAE所实现的，该框架确保了身体和手动运动的细粒度表示。广泛的实验和可视化验证了我们方法的有效性，证明了在运动产生，运动字幕和广义运动完成任务的运动环-2的适应性。

### Multi-modal Speech Emotion Recognition via Feature Distribution Adaptation Network 
[[arxiv](https://arxiv.org/abs/2410.22023)] [[cool](https://papers.cool/arxiv/2410.22023)] [[pdf](https://arxiv.org/pdf/2410.22023)]
> **Authors**: Shaokai Li,Yixuan Ji,Peng Song,Haoqin Sun,Wenming Zheng
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: 通过功能分配适应网络识别多模式的语音情绪
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 在本文中，我们提出了一个新颖的深层归纳转移学习框架，称为特征分布适应网络，以解决具有挑战性的多模式语音情感识别问题。我们的方法旨在使用深层的转移学习策略来调整视觉和音频特征分布以获得一致的情感表示，从而提高语音情感识别的表现。在我们的模型中，预先训练的RESNET-34分别用于特征提取面部表达图像和声学MEL频谱图。然后，引入了交叉注意机制，以建模多模式特征的固有相似性关系。最后，通过馈送前向网络有效地进行多模式特征分布适应性，该网络使用局部最大平均差异损失进行扩展。实验是在两个基准数据集上进行的，结果表明，与现有的模型相比，我们的模型可以实现出色的性能。

### Using Multimodal Deep Neural Networks to Disentangle Language from Visual Aesthetics 
[[arxiv](https://arxiv.org/abs/2410.23603)] [[cool](https://papers.cool/arxiv/2410.23603)] [[pdf](https://arxiv.org/pdf/2410.23603)]
> **Authors**: Colin Conwell,Christopher Hamblin,Chelsea Boccagno,David Mayo,Jesse Cummings,Leyla Isik,Andrei Barbu
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: No comments
- **标题**: 使用多模式的深神经网络将语言与视觉美学解开
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 当我们经历视觉刺激为美丽的视觉刺激时，有多少经验源自感知计算，我们无法描述我们可以轻松地转化为自然语言的概念知识与概念知识？通过行为范式或神经影像学在视觉引起的情感和美学体验中，从语言中解脱出感知通常在经验上是棘手的。在这里，我们通过使用线性解码对单峰视觉，单峰语言和多模式（语言一致）深神经网络（DNN）模型的学习表示来预测自然主义图像的人类美容等级来绕过这一挑战。我们表明，单峰视觉模型（例如SIMCLR）在这些评级中说明了绝大多数可解释的差异。语言一致的视觉模型（例如滑移）相对于单峰视力而产生的较小收益。在视觉嵌入条件下以产生字幕（通过夹板）的非峰语模型（例如GPT2）没有进一步的收益。与图像组合（串联）相比，单独的字幕嵌入比图像和字幕嵌入的准确预测较少（串联）。综上所述，这些结果表明，无论我们最终可能会发现什么词来描述我们的美感，喂养感的看法无法言喻的计算可能为该经验提供足够的基础。

### Language-guided Hierarchical Fine-grained Image Forgery Detection and Localization 
[[arxiv](https://arxiv.org/abs/2410.23556)] [[cool](https://papers.cool/arxiv/2410.23556)] [[pdf](https://arxiv.org/pdf/2410.23556)]
> **Authors**: Xiao Guo,Xiaohong Liu,Iacopo Masi,Xiaoming Liu
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: Accepted by IJCV2024. arXiv admin note: substantial text overlap with arXiv:2303.17111
- **标题**: 语言指导的分层细粒图像伪造检测和本地化
- **领域**: 计算机视觉和模式识别
- **摘要**: CNN合成和图像编辑域中产生的图像的伪造属性差异很大，并且这种差异使统一的图像伪造检测和定位（IFDL）具有挑战性。为此，我们为IFDL表示学习提供了层次的细颗粒配方。具体而言，我们首先代表具有不同级别的多个标签的操纵图像的伪造属性。然后，我们使用它们之间的层次依赖性在这些级别上进行细粒度的分类。结果，鼓励算法学习综合特征和不同伪造属性的固有层次结构。在这项工作中，我们提出了一种语言引导的层次级细粒IFDL，称为HIFI-NET ++。具体而言，HIFI-NET ++包含四个组成部分：多分支特征提取器，一种语言引导的伪造定位增强子以及分类和定位模块。多分支特征提取器的每个分支都学会在一个级别上对伪造属性进行分类，而定位和分类模块分别分别检测图像级伪造区。此外，语言引导的伪造本地化增强子（LFLE），其中包含图像和文本编码器，以对比性语言图像预训练（剪辑）学习，用于进一步丰富IFDL表示。 LFLE将专门设计的文本和给定的图像作为多模式输入，然后生成视觉嵌入和操纵评分映射，该图用于进一步改善HIFI-NET ++操纵定位性能。最后，我们构建了一个分层细粒数据集，以促进我们的研究。我们通过在IFDL和伪造属性分类的任务中使用不同的基准测试来证明我们方法对$ 8 $的有效性。我们的源代码和数据集可用。

### CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP 
[[arxiv](https://arxiv.org/abs/2410.23330)] [[cool](https://papers.cool/arxiv/2410.23330)] [[pdf](https://arxiv.org/pdf/2410.23330)]
> **Authors**: Tianyu Yang,Lisen Dai,Zheyuan Liu,Xiangqi Wang,Meng Jiang,Yapeng Tian,Xiangliang Zhang
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: No comments
- **标题**: 剪贴剂酶：剪辑中视觉文本关联的有效学习
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 机器解读（MU）已引起了极大的关注，作为从训练有素的模型中删除特定数据的手段，而无需进行完整的重新训练过程。尽管在文本和图像分类等单峰域中取得了进展，但在多模型模型中进行的学习仍然相对不受影响。在这项工作中，我们解决了在剪辑中学习的独特挑战，这是一种与视觉和文本表示相符的突出的多模式模型。我们介绍了剪贴剂酶，这是一种新颖的方法，它可以分解并有选择地忘记视觉和文本关联，从而确保了学习不会损害模型性能。剪贴剂酶由三个关键模块组成：一个忘记模块，它破坏了忘记集合中的关联，保留性能的保留模块，可在保留集合上保留性能，以及与原始模型保持一致性的一致性模块。在CIFAR-100和FLICKR30K数据集上进行了四个夹子下游任务的大量实验表明，剪贴画酶有效地忘记了多模式样本的零摄像任务中指定的关联，同时保留了模型在完成后的固定设置上的性能。

### TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models 
[[arxiv](https://arxiv.org/abs/2410.23266)] [[cool](https://papers.cool/arxiv/2410.23266)] [[pdf](https://arxiv.org/pdf/2410.23266)]
> **Authors**: Ziyao Shangguan,Chuhan Li,Yuxuan Ding,Yanan Zheng,Yilun Zhao,Tesca Fitzgerald,Arman Cohan
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: No comments
- **标题**: 番茄：评估多模式基础模型中的视觉时间推理能力
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 现有的基准通常强调了最先进的多模式模型（MFMS）在利用时间上下文来了解视频理解中取得的显着性能。但是，这些模型真正执行视觉时间推理的能力如何？我们对现有基准测试的研究表明，MFM的这种能力可能被高估了，因为可以使用单个，几个或零工的框架来解决许多问题。为了系统地检查当前的视觉时间推理任务，我们提出了三个具有相应指标的原则：（1）多帧增益，（2）框架顺序灵敏度和（3）框架信息差异。遵循这些原则，我们介绍了番茄，时间推理多模式评估，这是一种新颖的基准，该基准精心旨在严格评估MFMS在视频理解中的时间推理能力。番茄构成了1,484个精心策划的人类通知的问题，涵盖了六个任务（即动作计数，方向，旋转，形状和趋势，速度和频率以及视觉提示），适用于1,417个视频，包括805个自我录制的自我录制和播放的视频，包括人类为人类且富有人为含有的人，现实，现实，现实的场景和模拟场景。我们的全面评估表明，表现最好的模型的人类模型性能差距为57.3％。此外，我们的深入分析发现了超出当前MFMS差距的更多基本局限性。尽管他们可以准确地识别孤立框架中的事件，但他们无法将这些帧解释为连续序列。我们认为，番茄将作为评估下一代MFM的关键测试台，并呼吁社区开发能够通过视频模式理解人类世界动态的AI系统。

### EMMA: End-to-End Multimodal Model for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2410.23262)] [[cool](https://papers.cool/arxiv/2410.23262)] [[pdf](https://arxiv.org/pdf/2410.23262)]
> **Authors**: Jyh-Jing Hwang,Runsheng Xu,Hubert Lin,Wei-Chih Hung,Jingwei Ji,Kristy Choi,Di Huang,Tong He,Paul Covington,Benjamin Sapp,Yin Zhou,James Guo,Dragomir Anguelov,Mingxing Tan
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: Blog post: https://waymo.com/blog/2024/10/introducing-emma/
- **标题**: 艾玛：自动驾驶的端到端多模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,机器人技术
- **摘要**: 我们介绍了艾玛（Emma），这是一种用于自动驾驶的端到端多模型。 Emma建立在多模式的大型语言模型基础上，将原始摄像机传感器数据直接映射到各种驾驶特定的输出中，包括规划轨迹轨迹，感知对象和道路图元素。 Emma通过表示所有非传播输入（例如导航指令和自我车辆状态）和输出（例如轨迹和3D位置）作为自然语言文本，从而最大化世界知识的实用性。这种方法允许Emma在统一的语言空间中共同处理各种驾驶任务，并使用特定于任务的提示为每个任务生成输出。从经验上讲，我们通过在Nuscenes的运动计划中实现最先进的表现以及Waymo Open Motion数据集（WOMD）的竞争成果来证明Emma的有效性。 Emma还为Waymo Open Datat（WOD）上的摄像头3D对象检测产生竞争结果。我们表明，EMMA与计划者轨迹，对象检测和路面任务共同训练，可以在所有三个领域中进行改进，这突显了Emma作为自主驾驶应用程序的通才模型的潜力。但是，Emma还表现出一定的局限性：它只能处理少量的图像帧，不包含精确的3D感应方式，例如Lidar或Radar，并且计算昂贵。我们希望我们的结果将激发进一步的研究以减轻这些问题，并进一步发展自动驾驶模型体系结构的最新技术。

### Aligning Audio-Visual Joint Representations with an Agentic Workflow 
[[arxiv](https://arxiv.org/abs/2410.23230)] [[cool](https://papers.cool/arxiv/2410.23230)] [[pdf](https://arxiv.org/pdf/2410.23230)]
> **Authors**: Shentong Mo,Yibing Song
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: No comments
- **标题**: 将视听联合表示与代理工作流程对齐
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体,声音,音频和语音处理
- **摘要**: 视觉内容和随附的音频信号自然会制定联合表示，以改善视听（AV）相关的应用。尽管研究开发了各种AV表示学习框架，但AV数据一致性的重要性通常会破坏以实现高质量表示。我们观察到音频信号可能包含背景噪声干扰。同样，音频和视频流之间可能会出现非同步。这些非图案数据一致性限制表示质量和降低应用程序性能。在本文中，我们建议通过将音频信号与视觉数据对齐，从以数据为中心的角度来改善AV联合表示。我们的对齐方式是在由LLM的助手Avagent控制的代理工作流程中进行的。对于每个输入AV数据对，我们的Avagent使用多模式LLM分别将音频和视觉数据转换为语言描述（即工具使用）。然后，Avagent的原因是该配对数据是否很好地对齐，并计划在需要时编辑音频信号（即计划）。音频编辑是通过过滤噪声或增强数据的预定义动作执行的。此外，我们使用VLM来评估修改后的音频信号如何匹配视觉内容并为Avagent（即反射）提供反馈。工具使用，计划和反射步骤循环运行，成为代理工作流程，在该工作流中，音频信号逐渐与视觉内容保持一致。为此，现有方法可以通过我们的代理工作流直接利用对齐的AV数据来改善AV联合表示。实验结果全面证明了在各种下游任务中针对先前基线的建议方法的最新性能。

### DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET 
[[arxiv](https://arxiv.org/abs/2410.23219)] [[cool](https://papers.cool/arxiv/2410.23219)] [[pdf](https://arxiv.org/pdf/2410.23219)]
> **Authors**: Yitong Li,Morteza Ghahremani,Youssef Wally,Christian Wachinger
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: Accepted by IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2025
- **标题**: 钻石：使用MRI和PET进行多模式视觉变压器的痴呆诊断
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 由于症状重叠，诊断痴呆症，特别是针对阿尔茨海默氏病（AD）和额颞痴呆（FTD）的诊断很复杂。尽管磁共振成像（MRI）和正电子发射断层扫描（PET）数据对诊断至关重要，而将这些方式整合在深度学习面对面的挑战中，通常会导致与使用单一模态相比的次优性能。此外，具有显着临床重要性的鉴别诊断中多模式方法的潜力仍然没有探索。我们提出了一个新颖的框架钻石，以解决视力变压器的这些问题，以有效地整合MRI和PET。 Diamond配备了自我注意事项和一种新型的双重注意机制，该机制协同结合了MRI和PET，并将其与多模式归一化相结合，以减少冗余依赖性，从而提高性能。钻石在各种数据集中显着胜过现有的多模式方法，在AD诊断中达到92.4％的平衡精度，AD-MCI-CN分类为65.2％，在AD和FTD的鉴别诊断中达到了76.5％。我们还在一项全面的消融研究中验证了钻石的鲁棒性。该代码可在https://github.com/ai-med/diamond上找到。

### PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via Existing MLLM Structures 
[[arxiv](https://arxiv.org/abs/2410.23089)] [[cool](https://papers.cool/arxiv/2410.23089)] [[pdf](https://arxiv.org/pdf/2410.23089)]
> **Authors**: Tianxiang Wu,Minxin Nie,Ziqiang Cao
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: No comments
- **标题**: PIP-MM：通过现有MLLM结构将及时的及时信息预成立为视觉编码
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大型语言模型（MLLM）激活了大型语言模型（LLMS）通过集成视觉信息来解决视觉语言任务的功能。现有MLLM中的流行方法涉及雇用图像编码器来提取视觉特征，通过适配器将这些Features转换为视觉令牌，然后将其与Prompts集成到LLM中。但是，由于编码ISPrompt-Agnostic的图像过程，因此提取的视觉特征仅提供图像的粗暴描述，不可能专注于提价的要求。一方面，图像功能很容易缺少有关及时指定对象的信息，从而导致响应不令人满意。在其他手上，视觉特征包含大量无关的信息，这不仅增加了记忆的负担，而且会加剧代代效率。为了解决上述问题，我们提出\ textbf {pip-mm}，一个\ textbf {p} re- \ \ \ textbf {i} ntegrates \ textbf {p}将信息rompt rompt rompt rompt rompt rompt rompt rompt rompt到视觉编码过程中。具体而言，我们在MLLM中使用冷冻的LLM来向量化输入提示，该提示总结了提示的要求。由于我们的模型仅需要添加可观的MLP，因此可以应用于任何MLLM。为了验证PIP-MM的有效性，我们对多个基准进行了实验。自动化评估对和手动评估表明，PIP-MM的强劲表现，特别值得注意的是，即使减少了一半的视觉令牌，我们的模型仍保持出色的生成率。

### VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2410.22995)] [[cool](https://papers.cool/arxiv/2410.22995)] [[pdf](https://arxiv.org/pdf/2410.22995)]
> **Authors**: Jingkun Ma,Runzhe Zhan,Derek F. Wong,Yang Li,Di Sun,Hou Pong Chan,Lidia S. Chao
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: 58 pages, 28 figures
- **标题**: VISAIDMATH：通过视觉辅助数学推理进行基准测试
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 尽管先前对大语言模型（LLM）和大型多模式模型（LMM）的研究已经系统地探索了数学解决问题（MPS），但对这些模型如何处理问题解决过程中的视觉信息的分析仍然不足。为了解决这一差距，我们提出了Visaidmath，这是评估与视觉信息相关的MPS过程的基准。我们遵循严格的数据策划管道，涉及自动化过程和手动注释，以确保数据质量和可靠性。因此，该基准包括来自各种数学分支，视觉AID配方和难度水平的1,200个具有挑战性的问题，这些问题是从教科书，考试论文和奥林匹克问题等各种来源收集的。根据提议的基准，我们对十个主流LLM和LMM进行全面评估，突出了视觉辅助推理过程中的缺陷。例如，在视觉辅助推理任务中，GPT-4V仅达到45.33％的精度，即使在提供金色的视觉辅助设备时，即使下降了2分。深入的分析表明，缺陷的主要原因在于关于隐式视觉推理过程的幻觉，从而在视觉辅助MPS过程中阐明了未来的研究方向。

## 计算机与社会(cs.CY:Computers and Society)

该领域共有 4 篇论文

### From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice 
[[arxiv](https://arxiv.org/abs/2410.01812)] [[cool](https://papers.cool/arxiv/2410.01812)] [[pdf](https://arxiv.org/pdf/2410.01812)]
> **Authors**: Qian Niu,Keyu Chen,Ming Li,Pohsun Feng,Ziqian Bi,Lawrence KQ Yan,Yichao Zhang,Caitlyn Heqi Yin,Cheng Fei,Junyu Liu,Benji Peng,Tianyang Wang,Yunze Wang,Silin Chen,Ming Liu
> **First submission**: 2024-09-13
> **First announcement**: 2024-10-03
> **comment**: 12 pages, 1 figure
- **标题**: 从文本到多模式：探索大语言模型在医学实践中的演变和影响
- **领域**: 计算机与社会,人工智能,计算语言学
- **摘要**: 大型语言模型（LLM）已从基于文本的系统转变为多模式平台，从而显着影响包括医疗保健在内的各个部门。这项全面的综述探讨了LLM到多模式大语言模型（MLLM）的发展及其在医疗实践中的影响力的增长。我们研究了MLLM在医疗保健中的当前格局，分析了其在临床决策支持，医学成像，患者参与和研究中的应用。该评论强调了MLLM在整合各种数据类型（例如文本，图像和音频）中的独特功能，以提供对患者健康的更全面的见解。我们还解决了MLLM实施面临的挑战，包括数据限制，技术障碍和道德考虑。通过确定关键的研究差距，本文旨在指导数据集开发，模式一致性方法以及建立道德准则等领域的未来研究。随着MLLM继续塑造医疗保健的未来，了解它们的潜力和局限性对于他们负责和有效地融入医学实践至关重要。

### A Social Context-aware Graph-based Multimodal Attentive Learning Framework for Disaster Content Classification during Emergencies 
[[arxiv](https://arxiv.org/abs/2410.08814)] [[cool](https://papers.cool/arxiv/2410.08814)] [[pdf](https://arxiv.org/pdf/2410.08814)]
> **Authors**: Shahid Shafi Dar,Mohammad Zia Ur Rehman,Karan Bais,Mohammed Abdul Haseeb,Nagendra Kumara
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: ef:journal={Expert Systems with Applications},pages={125337},year={2024},publisher={Elsevier}
- **标题**: 在紧急情况下，基于社会环境感知图的多模式专注学习框架
- **领域**: 计算机与社会,计算语言学
- **摘要**: 在危机时期，在社交媒体平台上共享的与灾难相关信息的迅速而精确的分类对于有效的灾难响应和公共安全至关重要。在此类关键事件中，个人使用社交媒体进行交流，共享多模式的文本和视觉内容。但是，由于未经过滤和多样化的数据大量涌入，人道主义组织在有效利用这一信息方面面临挑战。与灾害相关内容分类的现有方法通常无法模拟用户的信誉，情感上下文和社交互动信息，这对于准确的分类至关重要。为了解决这一差距，我们提出了CrisissPot，该方法利用基于图的神经网络来捕获文本和视觉方式之间的复杂关系，以及社交上下文特征，以结合以用户为中心和内容的以内容为中心的信息。我们还引入了倒双嵌入式的注意（想法），该注意捕获了数据中的和谐和对比模式，以增强多模式相互作用并提供更丰富的见解。此外，我们提出了TSEQD（TSEQD（土耳其 - 摩西地震数据集），这是一个大型注释数据集，用于一个灾难事件，其中包含10,352个样本。通过广泛的实验，与公开可用的Crisismmd数据集和TSEQD数据集的最新方法相比，CrisissPot的平均F1得分增益为9.45％和5.01％。

### Empowering Cognitive Digital Twins with Generative Foundation Models: Developing a Low-Carbon Integrated Freight Transportation System 
[[arxiv](https://arxiv.org/abs/2410.18089)] [[cool](https://papers.cool/arxiv/2410.18089)] [[pdf](https://arxiv.org/pdf/2410.18089)]
> **Authors**: Xueping Li,Haowen Xu,Jose Tupayachi,Olufemi Omitaomu,Xudong Wang
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: 通过生成基础模型赋予认知数字双胞胎能力：开发低碳综合运输系统
- **领域**: 计算机与社会,人工智能,机器学习,系统与控制
- **摘要**: 对货运运输的有效监控对于推进可持续的低碳经济至关重要。依靠单模式数据和离散模拟的传统方法在整体上优化联运系统方面缺乏。这些系统涉及影响运输时间，成本，排放和社会经济因素的互连过程。开发用于实时意识，预测分析和城市物流优化的数字双胞胎需要在知识发现，数据集成和多域模拟方面进行广泛的努力。生成AI的最新进展为通过自动化知识发现和数据集成，生成创新的仿真和优化解决方案而自动化了数字双胞胎开发的新机会。这些模型通过促进数据工程，分析和软件开发的自主工作流来扩展数字双胞胎的功能。本文提出了一种创新的范式，该范式利用生成的AI来增强数字双胞胎进行城市研究和运营。使用货运脱碳作为案例研究，我们提出了一个概念框架，该概念框架采用基于变压器的语言模型来通过基础模型增强城市数字双胞胎。我们共享初步结果以及对更智能，自主和通用数字双胞胎的愿景，以优化从多模式到同步范式的集成货运系统。

### Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs) 
[[arxiv](https://arxiv.org/abs/2410.19314)] [[cool](https://papers.cool/arxiv/2410.19314)] [[pdf](https://arxiv.org/pdf/2410.19314)]
> **Authors**: Leander Girrbach,Yiran Huang,Stephan Alaniz,Trevor Darrell,Zeynep Akata
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 揭示和减少视觉和语言助手的性别偏见（VLAS）
- **领域**: 计算机与社会,计算语言学
- **摘要**: 预先训练的大语言模型（LLM）已与多模式任务的视觉输入可靠地集成在一起。诸如llava和internvl之类的教学调整图像到文本视觉助手（VLAS）的广泛采用需要评估性别偏见。我们研究了22个受欢迎的开源VLA中的性别偏见，这些VLA在人格特质，技能和职业方面。我们的结果表明，VLA会复制数据中可能存在的人类偏见，例如现实世界中的职业失衡。同样，他们倾向于将更多的技能和积极的性格特征归因于女性，而不是男性，我们看到将负面人格特质与男人联系起来的一致倾向。为了消除这些模型中的性别偏见，我们发现基于芬太尼的偏见方法实现了在下游任务上的表现和保持绩效之间的最佳折衷。我们主张在VLAS中预先部署性别偏见评估，并激励进一步发展依据策略，以确保公平的社会成果。

## 新兴技术(cs.ET:Emerging Technologies)

该领域共有 1 篇论文

### OmniBuds: A Sensory Earable Platform for Advanced Bio-Sensing and On-Device Machine Learning 
[[arxiv](https://arxiv.org/abs/2410.04775)] [[cool](https://papers.cool/arxiv/2410.04775)] [[pdf](https://arxiv.org/pdf/2410.04775)]
> **Authors**: Alessandro Montanari,Ashok Thangarajan,Khaldoon Al-Naimi,Andrea Ferlini,Yang Liu,Ananta Narayanan Balaji,Fahim Kawsar
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: Omnibuds：一个可感知的可耳式平台，用于高级生物感应和设备机器学习
- **领域**: 新兴技术,机器学习
- **摘要**: 感官耳朵从基本音频增强设备演变为用于临床级健康监测和福祉管理的复杂平台。本文介绍了Omnibuds，这是一个高级感官耳朵平台，该平台集成了由机器学习加速器供电的多个生物传感器和机载计算，所有这些平台都在实时操作系统（RTOS）内。该平台的双耳对称设计，配备精确定位的动力学，声学，光学和热传感器，可实现高度准确和实时的生理评估。与依赖外部数据处理的常规耳套不同，综合综合利用实时计算来显着提高系统效率，通过本地处理数据来显着提高系统效率，并降低潜伏期和保护隐私。该功能包括直接在设备上执行复杂的机器学习模型。我们提供了综合设计，硬件和软件体系结构的全面分析，该设计展示了其用于多功能应用程序的能力，对生理参数的准确和健壮的跟踪以及先进的人类计算机交互。

## 人机交互(cs.HC:Human-Computer Interaction)

该领域共有 11 篇论文

### Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI 
[[arxiv](https://arxiv.org/abs/2410.04652)] [[cool](https://papers.cool/arxiv/2410.04652)] [[pdf](https://arxiv.org/pdf/2410.04652)]
> **Authors**: Chengyuan Xu,Radha Kumaran,Noah Stier,Kangyou Yu,Tobias Höllerer
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: 10 pages, 6 figures, accepted to IEEE ISMAR 2024
- **标题**: 多模式3D融合和空间意识AI的原位学习
- **领域**: 人机交互,人工智能,计算机视觉和模式识别
- **摘要**: 虚拟和物理世界在增强现实中的无缝整合从系统中“理解”物理环境中受益。 AR研究长期以来一直集中在上下文意识的潜力上，展示了在3D环境中利用语义来实现各种对象级相互作用的新型功能。同时，计算机视觉社区已经跳上了神经视觉语言的理解，以增强对自主任务的环境感知。在这项工作中，我们引入了一个多模式3D对象表示，该对象表示与几何表示，将语义和语言知识统一，从而使用户指导的机器学习涉及物理对象。我们首先提出了一个快速的多模式3D重建管道，该管道通过将剪辑视觉语言特征融合到环境和对象模型中，从而将语言理解带入AR。然后，我们提出“原位”机器学习，并结合多模式表示，可以使用户以空间和语言意义上有意义的方式与用户与物理空间和对象进行交互。我们通过魔术LEAP上的两个现实世界应用程序来证明拟议系统的有用性：a）具有自然语言的物理环境中的空间搜索，b）跟踪对象随时间变化的智能库存系统。我们还可以在（https://github.com/cy-xu/spataly_aware_ai）上提供完整的实施和演示数据，以鼓励对空间意识的AI进行进一步的探索和研究。

### AI-rays: Exploring Bias in the Gaze of AI Through a Multimodal Interactive Installation 
[[arxiv](https://arxiv.org/abs/2410.03786)] [[cool](https://papers.cool/arxiv/2410.03786)] [[pdf](https://arxiv.org/pdf/2410.03786)]
> **Authors**: Ziyao Gao,Yiwen Zhang,Ling Li,Theodoros Papatheodorou,Wei Zeng
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-07
> **comment**: Siggraph Asia 2024 Art Paper
- **标题**: AI射线：通过多模式互动安装探索AI注视中的偏见
- **领域**: 人机交互,人工智能,计算机与社会
- **摘要**: 数据监视已变得更加秘密和普遍存在，而AI算法可能会导致社会分类。外观提供了直观的身份信号，但是让AI观察并推测它们意味着什么？我们介绍了AI射线，这是一种交互式安装，AI从参与者的外观中产生投机性身份，这些身份通过放置在参与者袋中的合成个人物品来表达。它使用投机性X射线愿景将现实与AI生成的假设进行对比，从而隐喻地突出了AI的审查和偏见。 AI-Rays通过嬉戏，沉浸式的体验探索AI偏见，促进了现代监视和人机现实的未来的讨论。

### Multi-modal Atmospheric Sensing to Augment Wearable IMU-Based Hand Washing Detection 
[[arxiv](https://arxiv.org/abs/2410.03549)] [[cool](https://papers.cool/arxiv/2410.03549)] [[pdf](https://arxiv.org/pdf/2410.03549)]
> **Authors**: Robin Burchard,Kristof Van Laerhoven
> **First submission**: 2024-09-17
> **First announcement**: 2024-10-07
> **comment**: iWOAR2024
- **标题**: 多模式的大气传感以增强可穿戴IMU的手洗手检测
- **领域**: 人机交互,机器学习
- **摘要**: 洗手是个人卫生的关键部分。洗手液检测是在医疗和专业领域的应用中可穿戴感应的相关主题。洗手检测可用于帮助工人遵守卫生规则。使用基于身体IMU的传感器系统的手洗涤检测已被证明是一种可行的方法，尽管对于某些报道的结果，检测的特异性很低，导致误报率很高。在这项工作中，我们提出了一种新颖的开源原型设备，还包括湿度，温度和气压传感器。我们为10名参与者和43个洗手事件提供了基准数据集，并对传感器的好处进行评估。除此之外，我们概述了额外传感器在注释管道和机器学习模型中的有用性。通过视觉检查，我们表明尤其是湿度传感器在洗手活动过程中的相对湿度有很大的增加。对我们数据的机器学习分析表明，从这种相对湿度模式中受益的不同特征仍有待确定。

### DAT: Dialogue-Aware Transformer with Modality-Group Fusion for Human Engagement Estimation 
[[arxiv](https://arxiv.org/abs/2410.08470)] [[cool](https://papers.cool/arxiv/2410.08470)] [[pdf](https://arxiv.org/pdf/2410.08470)]
> **Authors**: Jia Li,Yangchen Yu,Yin Chen,Yu Zhang,Peng Jia,Yunbo Xu,Ziqiang Li,Meng Wang,Richang Hong
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: 1st Place on the NoXi Base dataset in theMulti-Domain Engagement Estimation Challenge held by MultiMediate 24, accepted by ACM Multimedia 2024. The source code is available at \url{https://github.com/MSA-LMC/DAT}
- **标题**: DAT：具有模态融合的对话感知变压器用于人类参与估算
- **领域**: 人机交互,计算机视觉和模式识别
- **摘要**: 参与估计在理解人类的社会行为方面起着至关重要的作用，吸引了情感计算和人类计算机相互作用等领域的研究兴趣。在本文中，我们提出了一个具有模态融合（MGF）的对话感知的变压器框架（DAT），该框架仅依赖于视听输入，而不是语言的输入，而依赖于语言，以估算人类参与对话的参与。具体而言，我们的方法采用了一种模态融合策略，该策略在推断整个视听内容之前，在每个人的每个人中都独立融合了每个人的音频和视觉特征。该策略大大提高了模型的性能和鲁棒性。此外，为了更好地估计目标参与者的参与水平，引入的对话感知变压器考虑了参与者的行为和对话伙伴的提示。我们的方法在多层型24的多域参与估算挑战中进行了严格的测试，这表明参与度级别的回归精度在基线模型上显着改善。值得注意的是，我们的方法在NOXI基础测试集上的CCC得分为0.76，NOXI基础，NOXI-ADD和MPIIGI测试集的平均CCC为0.64。

### Tracing Human Stress from Physiological Signals using UWB Radar 
[[arxiv](https://arxiv.org/abs/2410.10155)] [[cool](https://papers.cool/arxiv/2410.10155)] [[pdf](https://arxiv.org/pdf/2410.10155)]
> **Authors**: Jia Xu,Teng Xiao,Pin Lv,Zhe Chen,Chao Cai,Yang Zhang,Zehui Xiong
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: 19 pages, 11 figures
- **标题**: 使用UWB雷达从生理信号中追踪人体压力
- **领域**: 人机交互,硬件架构,机器学习,信号处理
- **摘要**: 压力追踪是一个重要的研究领域，支持许多应用，例如医疗保健和压力管理；其最接近的相关作品来自压力检测。但是，这些现有作品不能很好地解决压力检测面临的两个重要挑战。首先，这些研究大多数涉及要求用户佩戴生理传感器以检测其压力状态，这对用户体验产生了负面影响。其次，这些研究未能有效利用多模式生理信号，从而导致令人满意的检测结果。本文正式定义了应力追踪问题，该问题强调了人类压力状态的持续检测。提出了一种新颖的深层追踪方法，称为DST。请注意，DST提出了基于非接触式超级带雷达收集的生理信号来追踪人体压力，该信号在收集其生理信号时对用户更友好。在DST中，首先要精心设计信号提取模块，以从雷达的原始RF数据中鲁棒提取多模式的生理信号，即使在身体运动的情况下也是如此。之后，在DST中提出了多模式融合模块，以确保可以有效地融合和利用提取的多模式生理信号。广泛的实验是在三个现实世界数据集上进行的，包括一个自我收集的数据集和两个宣传数据集。实验结果表明，所提出的DST方法在追踪人压力状态方面显着优于所有基准。与最佳基线相比，平均而言，DST均衡的所有数据集的检测准确性提高了6.31％。

### ClickAgent: Enhancing UI Location Capabilities of Autonomous Agents 
[[arxiv](https://arxiv.org/abs/2410.11872)] [[cool](https://papers.cool/arxiv/2410.11872)] [[pdf](https://arxiv.org/pdf/2410.11872)]
> **Authors**: Jakub Hoscilowicz,Bartosz Maj,Bartosz Kozakiewicz,Oleksii Tymoshchuk,Artur Janicki
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-16
> **comment**: The code for ClickAgent is available at github.com/Samsung/ClickAgent
- **标题**: ClickAgent：增强自主代理的UI位置功能
- **领域**: 人机交互,人工智能,机器学习
- **摘要**: 随着对数字设备配备图形用户界面（GUI）（例如计算机和智能手机）的日益依赖，对有效的自动化工具的需求变得越来越重要。尽管在许多领域中GPT-4V之类的多模式大语言模型（MLLM）excel，但它们在GUI互动中挣扎，从而限制了它们在自动化日常任务中的有效性。在本文中，我们介绍了ClickAgent，这是一个用于构建自主代理的新颖框架。在ClickAgent中，MLLM处理推理和行动计划，而单独的UI位置模型（例如Seeclick）标识了屏幕上相关的UI元素。这种方法解决了当前生成MLLM的关键局限性：它们在准确定位UI元素方面的困难。 ClickAgent在AITW基准测试中胜过其他基于提示的自主代理（Cogagent，Appagent）。我们的评估是在Android智能手机仿真器和实际的Android智能手机上进行的，它使用任务成功率作为测量代理性能的关键指标。

### Vital Insight: Assisting Experts' Context-Driven Sensemaking of Multi-modal Personal Tracking Data Using Visualization and Human-In-The-Loop LLM Agents 
[[arxiv](https://arxiv.org/abs/2410.14879)] [[cool](https://papers.cool/arxiv/2410.14879)] [[pdf](https://arxiv.org/pdf/2410.14879)]
> **Authors**: Jiachen Li,Xiwen Li,Justin Steinberg,Akshat Choube,Bingsheng Yao,Xuhai Xu,Dakuo Wang,Elizabeth Mynatt,Varun Mishra
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 重要洞察力：通过可视化和人类在循环的LLM代理商中，协助专家对多模式的个人跟踪数据进行上下文驱动的感觉
- **领域**: 人机交互,人工智能
- **摘要**: 被动跟踪方法（例如电话和可穿戴感应）已在监测现代无处不在的计算研究中的人类行为方面占主导地位。尽管机器学习方法在将原始传感器数据的周期转换为建模瞬时行为（例如体育活动识别）方面取得了重大进展，但在这些传感流的翻译中仍然存在显着差距，为有意义的，高级的，高级的，上下文意识到的见解，这些洞察力是各种应用所必需的（例如，总结一个人的日常应用程序）。为了弥合这一差距，专家通常需要在实际研究中采用上下文驱动的感官过程来获得见解。由于人类行为的复杂性，此过程通常需要手动努力，即使对于经验丰富的研究人员来说也可能具有挑战性。我们与21名专家进行了三轮用户研究，以探索解决方案的解决方案，以解决感官的挑战。我们遵循以人为本的设计过程，以确定需求和设计，迭代，构建和评估重要的洞察力（VI），这是一种新颖的，由LLM辅助的原型系统，以实现智能手机和可穿戴设备的多模式无源传感数据的人类在环上的推理（sensemake）和可视化。我们将原型作为技术探测，我们观察到专家与它的互动，并开发了专家感官模型，该模型解释了专家如何在直接数据表示和AI支持的推论之间移动以探索，问题和验证见解。通过这个迭代过程，我们还综合并讨论了未来AI仪可视化系统设计的设计含义列表，以更好地帮助专家在多模式健康传感数据中的感觉过程。

### Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User Modeling 
[[arxiv](https://arxiv.org/abs/2410.16668)] [[cool](https://papers.cool/arxiv/2410.16668)] [[pdf](https://arxiv.org/pdf/2410.16668)]
> **Authors**: Chenyi Li,Guande Wu,Gromit Yeuk-Yin Chan,Dishita G Turakhia,Sonia Castelo Quispe,Dong Li,Leslie Welch,Claudio Silva,Jing Qian
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: Satori：迈向主动的AR助手，并具有信念 - 示威用户建模
- **领域**: 人机交互,人工智能
- **摘要**: 增强现实援助越来越受欢迎，可以为用户提供诸如组装和烹饪之类的任务。但是，当前的实践通常提供从用户请求初始化的反应响应，而缺乏丰富的上下文和用户特定信息。为了解决这一限制，我们提出了一种新型的AR援助系统Satori，该系统对用户状态和环境环境进行了建模以提供主动的指导。我们的系统将信念 - 戴术（BDI）模型与最先进的多模式大语言模型（LLM）结合在一起，以推断上下文适当的指导。这项设计由两项涉及十二位专家的形成性研究得知。一项16个受试者内的研究发现，Satori的性能与设计师创建的巫师（WOZ）系统相当，而无需依赖手动配置或启发式方法，从而增强了可用性，可重复性并为AR辅助提供了新的可能性。

### Estuary: A Framework For Building Multimodal Low-Latency Real-Time Socially Interactive Agents 
[[arxiv](https://arxiv.org/abs/2410.20116)] [[cool](https://papers.cool/arxiv/2410.20116)] [[pdf](https://arxiv.org/pdf/2410.20116)]
> **Authors**: Spencer Lin,Basem Rizk,Miru Jun,Andy Artze,Caitlin Sullivan,Sharon Mozgai,Scott Fisher
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: To be published in ACM Intelligent Virtual Agents (IVA) 2024 [DOI: 10.1145/3652988.3696198] [ACM ISBN: 979-8-4007-0625-7/24/09]
- **标题**: 河口：建立多模式低延迟实时社会互动代理的框架
- **领域**: 人机交互,人工智能
- **摘要**: 生成人工智能（AI）技术的能力和普遍性的上升使其能够应用于社会互动代理（SIA）的领域。尽管对用于实时SIA研究的现代AI驱动组件的兴趣不断增加，但由于缺乏标准化和通用的SIA框架，仍然存在大量摩擦。为了针对这种缺席，我们开发了河口：一个多模式（文本，音频和很快的视频）框架，可促进低延迟，实时SIA的发展。河口旨在减少研究之间的重复工作，并提供一个灵活的平台，该平台可以完全悬而未决，以最大程度地提高可配置性，可控性，研究的可重复性以及代理响应时间的速度。我们可以通过构建一个可靠的多模式框架来做到这一点，该框架将当前和未来的组件无缝地融合到模块化和可互操作的体系结构中。

### Large Language Model-assisted Speech and Pointing Benefits Multiple 3D Object Selection in Virtual Reality 
[[arxiv](https://arxiv.org/abs/2410.21091)] [[cool](https://papers.cool/arxiv/2410.21091)] [[pdf](https://arxiv.org/pdf/2410.21091)]
> **Authors**: Junlong Chen,Jens Grubert,Per Ola Kristensson
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: under review
- **标题**: 大型语言模型辅助语音和指向益处在虚拟现实中的多个3D对象选择
- **领域**: 人机交互,人工智能
- **摘要**: 在虚拟现实中选择遮挡对象是一个具有挑战性的问题，如果涉及多个对象，则更是如此。随着新的人工智能技术的出现，我们探索了利用大型语言模型通过多模式语音和射线播放交互技术在虚拟现实中协助多对象选择任务的可能性。我们验证了比较用户研究（n = 24）中的发现，其中参与者在具有不同级别的场景困惑的虚拟现实场景中选择了目标对象。将性能指标和用户体验指标与用作基线的基于迷你图的对象选择技术进行比较。结果表明，当有多个目标对象时，引入的技术，AssistVR优于基线技术。与语音界面的共同信念相反，即使目标对象难以口头参考，AssistVR也能够优于基线。这项工作证明了由大型Laguage模型提供支持的智能多模式交互系统的生存能力和相互作用潜力。基于结果，我们讨论了对沉浸式环境中未来智能多模式交互系统设计的含义。

### Analyzing Multimodal Interaction Strategies for LLM-Assisted Manipulation of 3D Scenes 
[[arxiv](https://arxiv.org/abs/2410.22177)] [[cool](https://papers.cool/arxiv/2410.22177)] [[pdf](https://arxiv.org/pdf/2410.22177)]
> **Authors**: Junlong Chen,Jens Grubert,Per Ola Kristensson
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: under review
- **标题**: 分析LLM辅助操纵3D场景的多模式相互作用策略
- **领域**: 人机交互,人工智能
- **摘要**: 随着对沉浸式环境的3D内容的大型语言模型（LLM）的越来越多的应用，研究用户行为以识别互动模式和潜在的障碍，以指导涉及LLMS的沉浸式内容创建和编辑系统的未来设计。在与12名参与者的经验用户研究中，我们将定量使用数据与经验后问卷的反馈相结合，以揭示LLM辅助3D场景编辑系统中常见的交互模式和关键障碍。我们确定了改善3D设计工具中自然语言界面的机会，并为将来的LLM集成3D内容创建系统提出设计建议。通过一项实证研究，我们证明了LLM辅助的交互式系统可以在沉浸式环境中有效地使用。

## 信息检索(cs.IR:Information Retrieval)

该领域共有 16 篇论文

### Integrating Visual and Textual Inputs for Searching Large-Scale Map Collections with CLIP 
[[arxiv](https://arxiv.org/abs/2410.01190)] [[cool](https://papers.cool/arxiv/2410.01190)] [[pdf](https://arxiv.org/pdf/2410.01190)]
> **Authors**: Jamie Mahowald,Benjamin Charles Germain Lee
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: 18 pages, 7 figures, accepted at the Computational Humanities Research Conference (CHR 2024)
- **标题**: 集成视觉和文本输入，以搜索大规模地图收集
- **领域**: 信息检索,数字图书馆
- **摘要**: 尽管地图在数字收集中的普遍性和历史重要性，但当前的导航和探索地图收集的方法在很大程度上仅限于目录记录和结构化元数据。在本文中，我们探讨了使用自然语言输入（“带有海怪的地图”），视觉输入（即反向映像搜索）和多模式输入（示例地图 +“更多求格拉斯尺度”）进行交互式搜索大规模地图集合的潜力。作为案例研究，我们采用了562,842张图像，可通过国会API图书馆公开访问。为此，我们使用MulitModal对比语言图像预训练（剪辑）机器学习模型来生成这些地图的嵌入，并开发了使用这些输入策略实现探索性搜索功能的代码。我们提出结果，例如，与国会图书馆地理和地图部门的员工协商进行搜索，并描述了这些搜索查询的优势，劣势和可能性。此外，我们介绍了10,504个地图对接对的微调数据集，以及用于在此数据集上微调剪辑模型的体系结构。为了促进重复使用，我们在记录的，交互式的jupyter笔记本中提供所有代码，并将所有代码放入公共领域。最后，我们讨论了在画廊，图书馆，档案馆和博物馆持有的数字化和诞生数字收藏中应用这些方法的机会和挑战。

### GraphRevisedIE: Multimodal Information Extraction with Graph-Revised Network 
[[arxiv](https://arxiv.org/abs/2410.01160)] [[cool](https://papers.cool/arxiv/2410.01160)] [[pdf](https://arxiv.org/pdf/2410.01160)]
> **Authors**: Panfeng Cao,Jian Wu
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: ef:Pattern Recognition Volume 140, August 2023, 109542
- **标题**: Graphreviseie：带有图形修复网络的多模式信息提取
- **领域**: 信息检索,计算机视觉和模式识别
- **摘要**: 来自视觉上丰富文档（VRD）的关键信息提取（KIE）在文档智能中一直是一项具有挑战性的任务，因为不仅使模型难以推广，而且缺乏利用VRD中多模式特征的方法。在本文中，我们提出了一个名为Graphreviseie的轻量级模型，该模型有效地嵌入了VRD和Leverages图形修订版和图形卷积的多模式，视觉，视觉和布局特征，以丰富具有全局上下文的多模态嵌入。在多个现实世界数据集上进行的广泛实验表明，与以前的KIE方法相比，Graphreviseie概括地将各种布局和可比较或更好的性能的文档概括。我们还发布了一个包含现实生活和综合文档的业务许可证数据集，以促进文档的研究。

### Multi-modal clothing recommendation model based on large model and VAE enhancement 
[[arxiv](https://arxiv.org/abs/2410.02219)] [[cool](https://papers.cool/arxiv/2410.02219)] [[pdf](https://arxiv.org/pdf/2410.02219)]
> **Authors**: Bingjie Huang,Qingyi Lu,Shuaishuai Huang,Xue-she Wang,Haowei Yang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: 基于大型模型和VAE增强的多模式服装推荐模型
- **领域**: 信息检索,人工智能
- **摘要**: 长期以来，准确推荐产品一直是需要深入研究的主题。这项研究提出了用于服装建议的多模式范式。具体而言，它设计了一种多模式分析方法，该方法集成了服装描述文本和图像，并利用预先训练的大语言模型来深入探索用户和产品的隐藏含义。此外，还采用了一个多变量编码器来学习用户信息和产品之间的关系，以解决推荐系统中的冷启动问题。这项研究还通过广泛的消融实验验证了该方法与各种建议系统方法的显着性能优势，从而为建议系统的全面优化提供了关键的实用指导。

### Dreaming User Multimodal Representation Guided by The Platonic Representation Hypothesis for Micro-Video Recommendation 
[[arxiv](https://arxiv.org/abs/2410.03538)] [[cool](https://papers.cool/arxiv/2410.03538)] [[pdf](https://arxiv.org/pdf/2410.03538)]
> **Authors**: Chengzhi Lin,Hezheng Lin,Shuchang Liu,Cangguang Ruan,LingJing Xu,Dezhao Yang,Chuyuan Wang,Yongqi Liu
> **First submission**: 2024-09-15
> **First announcement**: 2024-10-07
> **comment**: 4 Figure; 2 Table
- **标题**: 以柏拉图表示假设为指导的Micro-Video推荐的梦想用户多模式表示
- **领域**: 信息检索,人工智能,计算机视觉和模式识别
- **摘要**: 在线Micro-Video平台的扩散已经强调了高级推荐系统减轻信息过载并提供量身定制的内容的必要性。尽管有进步，但准确，迅速地捕获动态用户兴趣仍然是一个巨大的挑战。受柏拉图表示假设的启发，该假说认为不同的数据模式会趋向于共享的现实统计模型，我们介绍了Dreamumm（Dream Usher用户多模式表示），一种新颖的方法利用用户历史行为在多MimimaDA领域中创建实时用户表示。 Dreamumm采用封闭形式的解决方案将用户视频偏好与多模式相似性相关联，假设可以在统一的多模式空间中有效地表示用户兴趣。此外，我们为缺乏最近的用户行为数据的方案提出了候选人 - 德里姆姆，仅从候选视频中推断出兴趣。广泛的在线A/B测试表明，用户参与度指标（包括活跃的日子和游戏计数）有了显着改善。 Dreamumm成功地在两个微观电视平台上部署了数亿个日常活跃用户，这说明了其在个性化的Micro-Video内容交付中的实用效率和可扩展性。我们的工作通过提供经验证据来支持用户兴趣表示居住在多模式空间中的潜力，从而有助于对代表性融合的持续探索。

### Multimodal Point-of-Interest Recommendation 
[[arxiv](https://arxiv.org/abs/2410.03265)] [[cool](https://papers.cool/arxiv/2410.03265)] [[pdf](https://arxiv.org/pdf/2410.03265)]
> **Authors**: Yuta Kanzawa,Toyotaro Suzumura,Hiroki Kanezashi,Jiawei Yong,Shintaro Fukushima
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 多模式利益建议
- **领域**: 信息检索,社交和信息网络
- **摘要**: 大型语言模型应用于推荐任务，例如要购买的项目和要阅读的新闻文章。兴趣点是基于多模式数据集的语言表示的顺序推荐的一个新领域。作为证明我们概念的第一步，我们专注于基于每个用户过去访问历史的餐厅推荐。当选择下一家餐厅访问时，用户会考虑场地的流派和位置，如果有的话，可以在那里提供菜肴。我们通过将图片转换为文本描述，使用一个名为llava的多模式模型将图片转换为文本说明，并使用了一个名为LLAVA的多模型，并使用了一个基于语言的顺序推荐框架，名为2023培训了该模型的模型，该模型否则使用了另一个模型训练了另一个模型，我们使用了一个基于语言的顺序推荐框架来创建一个伪餐厅的入住历史数据集。这表明这种半偶像模型反映了实际人类行为，我们通往多模式推荐模型的道路朝着正确的方向。

### Personalized Item Representations in Federated Multimodal Recommendation 
[[arxiv](https://arxiv.org/abs/2410.08478)] [[cool](https://papers.cool/arxiv/2410.08478)] [[pdf](https://arxiv.org/pdf/2410.08478)]
> **Authors**: Zhiwei Li,Guodong Long,Jing Jiang,Chengqi Zhang
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: 12 pages, 4 figures, 5 tables, conference
- **标题**: 联合多式联运推荐中的个性化项目表示
- **领域**: 信息检索,人工智能,机器学习
- **摘要**: 联合推荐系统对于在保护用户隐私的同时提供个性化建议至关重要。但是，当前方法主要依赖于基于ID的项目嵌入，从而忽略了物品的丰富多模式信息。为了解决这个问题，我们提出了一个称为FEDMR的联合多模式推荐系统。 FEDMR使用服务器上的基础模型编码多模式项目数据，例如图像和文本。为了处理由用户偏好差异引起的数据异质性，FEDMR在每个客户端引入了一个混合功能融合模块，该模块根据用户互动历史记录调整融合策略权重，以生成捕获用户精细元素偏好的个性化项目表示。 FEDMR与现有的基于ID的联合推荐系统兼容，改善了性能，而无需修改原始框架。四个现实世界多模式数据集的实验证明了FEDMR的有效性。该代码可在https://anonymon.4open.science/r/fedmr上找到。

### Firzen: Firing Strict Cold-Start Items with Frozen Heterogeneous and Homogeneous Graphs for Recommendation 
[[arxiv](https://arxiv.org/abs/2410.07654)] [[cool](https://papers.cool/arxiv/2410.07654)] [[pdf](https://arxiv.org/pdf/2410.07654)]
> **Authors**: Hulingxiao He,Xiangteng He,Yuxin Peng,Zifei Shan,Xin Su
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: Accepted by ICDE 2024. The code is available at https://github.com/PKU-ICST-MIPL/Firzen_ICDE2024
- **标题**: Firzen：用冷冻的异质和同质图发射严格的冷启动物品以供推荐
- **领域**: 信息检索
- **摘要**: 使用唯一身份（ID）代表不同用户和项目的建议模型已在十多年内统治了推荐系统文献。由于项目（例如文本和图像）的多模式内容以及知识图（kgs）可能反映了与互动相关的用户的偏好和项目特征，因此它们已被用作有用的侧面信息来进一步提高建议质量。但是，这种方法的成功通常会限制温暖启动或严格的冷启动项目建议，其中某些项目既不出现在训练数据中，也没有在测试阶段进行任何互动：（1）有些人未能学习严格的冷启动项目的嵌入，因为仅利用侧面信息来增强温暖启动的ID表示； （2）其他人会恶化温暖启动建议的性能，因为无关的多模式含量或kgs中的实体可能会模糊最终表示。 In this paper, we propose a unified framework incorporating multi-modal content of items and KGs to effectively solve both strict cold-start and warm-start recommendation termed Firzen, which extracts the user-item collaborative information over frozen heterogeneous graph (collaborative knowledge graph), and exploits the item-item semantic structures and user-user behavioral association over frozen homogeneous graphs (item-item relation graph and用户用户共发生图）。此外，我们通过公开可用的亚马逊数据集建立了四个统一的严格启动评估基准，并通过重新安排交互数据和构建kgs来建立来自魏克因频道的现实世界工业数据集。广泛的经验结果表明，我们的模型在严格的冷启动建议方面产生了重大改进，并且在温暖启动方案中表现优于或与最先进的表现相匹配。

### SGUQ: Staged Graph Convolution Neural Network for Alzheimer's Disease Diagnosis using Multi-Omics Data 
[[arxiv](https://arxiv.org/abs/2410.11046)] [[cool](https://papers.cool/arxiv/2410.11046)] [[pdf](https://arxiv.org/pdf/2410.11046)]
> **Authors**: Liang Tao,Yixin Xie,Jeffrey D Deng,Hui Shen,Hong-Wen Deng,Weihua Zhou,Chen Zhao
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: 20 pages, 2 figures
- **标题**: SGUQ：使用多摩学数据的阿尔茨海默氏病诊断的分期图卷积神经网络
- **领域**: 信息检索,机器学习,定量方法
- **摘要**: 阿尔茨海默氏病（AD）是一种慢性神经退行性疾病，是痴呆症的主要原因，在全球范围内产生了显着影响的成本，死亡率和负担。高通量OMICS技术的出现，例如基因组学，转录组学，蛋白质组学和表观基因组学，彻底改变了对AD的分子理解。常规的AI方法通常需要一开始就完成所有OMIC数据，以实现最佳的AD诊断，这效率低下，可能是不必要的。为了降低临床成本并提高了使用Multi-Omics数据的AD诊断的准确性，我们提出了一个具有不确定性定量（SGUQ）的新型分期图卷积网络。 SGUQ从mRNA开始，仅在必要时才逐渐掺入DNA甲基化和miRNA数据，从而降低了整体成本并暴露于有害测试中。实验结果表明，仅使用单模式数数据（mRNA）可以可靠地预测46.23％的样品，而在组合两种OMICS数据类型（mRNA + DNA甲基化）时，额外的16.04％的样品可以实现可靠的预测。此外，提议的上演SGUQ在Rosmap数据集上达到了0.858的精度，该数据集的表现显着优于现有方法。所提出的SGUQ不仅可以使用多摩管数据应用于AD诊断，而且还具有使用多观看数据的临床决策进行临床决策的潜力。我们的实施可在https://github.com/chenzhao2023/multiomicsunclinety上公开获得。

### VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents 
[[arxiv](https://arxiv.org/abs/2410.10594)] [[cool](https://papers.cool/arxiv/2410.10594)] [[pdf](https://arxiv.org/pdf/2410.10594)]
> **Authors**: Shi Yu,Chaoyue Tang,Bokai Xu,Junbo Cui,Junhao Ran,Yukun Yan,Zhenghao Liu,Shuo Wang,Xu Han,Zhiyuan Liu,Maosong Sun
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: Visrag：基于视觉检索的多模式文档
- **领域**: 信息检索,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 检索增强的生成（RAG）是一种有效的技术，它使大型语言模型（LLMS）能够利用外部知识来源进行生成。但是，当前的抹布系统仅基于文本，使其不可能利用诸如布局和在现实世界多模式文档中扮演关键角色的视觉信息。在本文中，我们介绍了Visrag，该文章通过建立基于视觉语言模型（VLM）的RAG管道来解决此问题。在此管道中，该文档不是先解析文档以获取文本，而是使用VLM作为图像直接嵌入文档，然后检索以增强VLM的生成。与传统的基于文本的抹布相比，Visrag最大程度地提高了原始文档中数据信息的保留和利用，从而消除了解析过程中引入的信息损失。我们收集开源数据和合成数据，以训练回猎犬在Visrag中进行培训，并探索各种一代方法。实验表明，Visrag在检索和发电阶段都优于传统抹布，比传统的基于文本的RAG管道获得20--40％的端到端性能增长。进一步的分析表明，Visrag在使用训练数据方面有效地证明了强大的概括能力，将其定位为在多模式文档上的抹布的有前途的解决方案。我们的代码和数据可从https://github.com/openbmb/visrag获得。

### Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with Large Language Models for Multi-Behavior Recommendations 
[[arxiv](https://arxiv.org/abs/2410.12228)] [[cool](https://papers.cool/arxiv/2410.12228)] [[pdf](https://arxiv.org/pdf/2410.12228)]
> **Authors**: Luyi Ma,Xiaohan Li,Zezhong Fan,Kai Zhao,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sushant Kumar,Kannan Achan
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 三重方式融合：将视觉，文本和图形数据与多行为建议的大语言模型对齐
- **领域**: 信息检索,人工智能,计算语言学
- **摘要**: 整合各种数据模式对于增强个性化推荐系统的性能至关重要。通常依靠单数数据源的传统模型缺乏准确捕获项目功能和用户行为的多方面性质所需的深度。本文介绍了一个新的框架，用于多个行为建议，利用三模式的融合，该融合是通过与大语言模型（LLMS）对齐的视觉，文本和图形数据。通过合并视觉信息，我们捕获上下文和美学项目的特征；文本数据详细介绍了对用户兴趣和项目功能的见解；图形数据阐明了项目 - 行为异质图中的关系。我们提出的称为“三模式融合”（TMF）的模型利用LLM的力量来对齐和整合这三种方式，从而实现了用户行为的全面表示。 LLM对用户的交互作用进行建模，包括自然语言中的行为和项目功能。最初，LLM仅使用基于自然语言的提示进行热身。然后，我们基于交叉注意和自我注意解机制来设计模态融合模块，以将其他模型的不同模态整合到同一嵌入空间中，并将其纳入LLM。广泛的实验证明了我们方法在提高建议准确性方面的有效性。进一步的消融研究验证了我们的模型设计和TMF的好处的有效性。

### Deep Class-guided Hashing for Multi-label Cross-modal Retrieval 
[[arxiv](https://arxiv.org/abs/2410.15387)] [[cool](https://papers.cool/arxiv/2410.15387)] [[pdf](https://arxiv.org/pdf/2410.15387)]
> **Authors**: Hao Chen,Lei Zhu,Xinghui Zhu
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 多标签跨模式检索的深层班级引导散列
- **领域**: 信息检索
- **摘要**: 由于其低成本和有效的检索优势，深层散列在跨模式检索中被广泛重视。但是，现有的跨模式哈希方法要么探索数据点之间的关系，这不可避免地导致了类内部分散，要么探索数据点和类别之间的关系，同时忽略了阶层间结构关系，从而产生了次优哈希码。本文提出了一种DCGH方法，如何在响应这个问题上维持阶层内聚合和阶层间结构关系。具体而言，我们将代理损失用作维持数据内部集合的主要阶段，并结合成对损失以维持阶层间结构关系，并在此基础上进一步提出了一个方差约束，以解决由组合引起的语义偏见问题。三个基准数据集上的大量比较实验表明，与现有的跨模式检索方法相比，DCGH方法具有可比甚至更好的性能。实施我们的DCGH框架的代码可在https://github.com/donnotnormal/dcgh上获得。

### Personalized Image Generation with Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2410.14170)] [[cool](https://papers.cool/arxiv/2410.14170)] [[pdf](https://arxiv.org/pdf/2410.14170)]
> **Authors**: Yiyan Xu,Wenjie Wang,Yang Zhang,Biao Tang,Peng Yan,Fuli Feng,Xiangnan He
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: Accepted for publication in WWW'25
- **标题**: 具有大型多模型的个性化图像生成
- **领域**: 信息检索,人工智能,多媒体
- **摘要**: 个性化的内容过滤（例如推荐系统）已成为减轻信息过载的关键基础架构。但是，这些系统仅过滤现有内容，并受到其有限的多样性的限制，因此很难满足用户的各种内容需求。为了解决这一限制，个性化的内容产生已成为广泛应用的有希望的方向。然而，大多数现有的研究都集中在个性化的文本生成上，而对个性化图像生成的关注相对较少。个性化图像生成中的有限工作面临着准确捕获用户的视觉偏好和来自嘈杂的用户相互作用图像和复杂多模式说明的需求的挑战。更糟糕的是，缺乏用于培训个性化图像生成模型的监督数据。为了克服挑战，我们提出了一个名为Pigeon的个性化图像生成框架，该框架采用了具有三个专用模块的特殊大型多模式模型，以捕获用户的视觉偏好和嘈杂用户历史记录和多模式指令的需求。为了减轻数据稀缺性，我们引入了一个两阶段的偏好对齐方案，包括掩盖的偏好重建和成对的偏好对齐，以使鸽子与个性化的图像生成任务相结合。我们将鸽子应用于个性化的贴纸和电影海报的生成，其中广泛的定量结果和人类评估突出了其优于各种生成基线的优势。

### : A Multi-task Multi-topic Dataset for Multi-modal Entity Linking 
[[arxiv](https://arxiv.org/abs/2410.18096)] [[cool](https://papers.cool/arxiv/2410.18096)] [[pdf](https://arxiv.org/pdf/2410.18096)]
> **Authors**: Fang Wang,Shenglin Yin,Xiaoying Bai,Minghao Hu,Tianwei Yan,Yi Liang
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: ：用于多模式实体链接的多任务多任务数据集
- **领域**: 信息检索,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式实体链接（MEL）是各种下游任务的基本组件。但是，现有的MEL数据集遭受了小规模，主题类型的稀缺性和任务覆盖有限，这使得它们无法有效增强多模型模型的实体链接功能。为了解决这些障碍，我们提出了一个数据集施工管道，并发布了$ M^3el $，这是MEL的大规模数据集。 $ m^3el $包括79,625个实例，涵盖了9个不同的多模式任务和5个不同的主题。此外，为了进一步提高该模型对多模式任务的适应性，我们提出了一种模式增强的培训策略。利用$ m^3el $作为语料库，训练$ \ textIt {clip} _ {\ textit {nd}} $基于$ \ textit {clip}（\ textit {clipit {vextit {vit}  -  \ textit {bextit {bextit {b}  -  \ textit {b}  -  \ textit {32} {32}）$，并进行比较分析。实验结果表明，现有模型的性能远远低于预期（ACC为49.4％-75.8％），经过分析后，已经获得小型数据集大小，模式任务范围不足以及主题多样性的有限多样性导致多模式模型的概括不佳。我们的数据集有效地解决了这些问题，$ \ textIt {clip} _ {\ textit {nd}} $模型用$ m^3el $微调显示准确性的显着提高，平均改善了9.3％至25％。我们的数据集可从https://anonymon.4open.science/r/m3el获得。

### Learning ID-free Item Representation with Token Crossing for Multimodal Recommendation 
[[arxiv](https://arxiv.org/abs/2410.19276)] [[cool](https://papers.cool/arxiv/2410.19276)] [[pdf](https://arxiv.org/pdf/2410.19276)]
> **Authors**: Kangning Zhang,Jiarui Jin,Yingjie Qin,Ruilong Su,Jianghao Lin,Yong Yu,Weinan Zhang
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: 11 pages,6 figures
- **标题**: 通过象征性交叉的学习无ID项目表示形式多模式建议
- **领域**: 信息检索
- **摘要**: 当前的多模式推荐模型已广泛探索了多模式信息的有效利用。但是，他们对ID嵌入的依赖仍然是性能瓶颈。即使在多模式信息的帮助下，当相互作用数据稀疏时，优化ID嵌入对于基于ID的多模式推荐器仍然具有挑战性。此外，特定于项目ID嵌入的独特性质阻碍了相关项目之间的信息交换，并且ID嵌入的空间要求随着项目的规模而增加。基于这些限制，我们提出了一个名为“电动机”的无ID多模式代表方案，该方案使用可学习的多模式令牌来表示每个项目，并通过共享令牌将它们连接。具体而言，我们首先采用产品量化来将每个项目的多模式特征（例如，图像，文本）离散为离散的令牌ID。然后，我们将与这些令牌ID相对应的令牌嵌入方式解释为隐式项目功能，引入了新的令牌跨网络，以捕获这些令牌之间的隐式交互模式。结果表示形式可以替换原始的ID嵌入，并将基于ID的原始多模式推荐器转换为ID的无ID系统，而无需引入任何其他损失设计。电机减少了这些模型的整体空间要求，从而促进了相关项目之间的信息互动，同时也大大增强了模型的推荐功能。在九个主流模型上进行的广泛实验证明了电动机在增强多模式推荐系统方面的有效性。

### Personalized Recommendation Systems using Multimodal, Autonomous, Multi Agent Systems 
[[arxiv](https://arxiv.org/abs/2410.19855)] [[cool](https://papers.cool/arxiv/2410.19855)] [[pdf](https://arxiv.org/pdf/2410.19855)]
> **Authors**: Param Thakkar,Anushka Yadav
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 使用多模式，自主，多代理系统的个性化推荐系统
- **领域**: 信息检索,人工智能,机器学习,多代理系统
- **摘要**: 本文介绍了使用多模式，自主，多代理系统的高度发展的个性化推荐系统。该系统着重于将未来派AI技术和LLM（如Gemini-1.5- Pro和Llame-70B）纳入，以改善客户服务体验，尤其是在电子商务中。我们的方法使用多代理，多模式系统为其用户提供最佳建议。该系统由三个代理组成。第一代理建议使用适合回答给定问题的产品，而第二个代理商根据属于这些推荐产品的图像提出后续问题，然后由第三代理进行自动搜索。它还具有实时数据获取，基于用户偏好的建议，并且是自适应学习。在复杂的查询期间，使用交响乐的申请过程，并使用GROQ API快速回答响应时间较低。它使用多模式的方式全面利用文本和图像，以优化产品建议和客户互动。

### PerSRV: Personalized Sticker Retrieval with Vision-Language Model 
[[arxiv](https://arxiv.org/abs/2410.21801)] [[cool](https://papers.cool/arxiv/2410.21801)] [[pdf](https://arxiv.org/pdf/2410.21801)]
> **Authors**: Heng Er Metilda Chee,Jiayin Wang,Zhiqiang Guo,Weizhi Ma,Min Zhang
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: PERSRV：带视觉模型的个性化贴纸检索
- **领域**: 信息检索
- **摘要**: 即时消息传递是日常通信的流行手段，使用户可以发送文本和贴纸。俗话说，“图片值得一千个单词”，因此开发有效的贴纸检索技术对于增强用户体验至关重要。但是，现有的贴纸检索方法依靠标记的数据来解释贴纸，通用视觉语言模型（VLM）通常很难捕获贴纸的独特语义。此外，基于相关的贴纸检索方法缺乏个性化，从而在不同的用户期望和检索结果之间造成了差距。为了解决这些问题，我们提出了具有视觉语言模型框架的个性化贴纸检索，即PERSRV，构成了离线计算和在线处理模块。在线检索部分遵循相关召回和个性化排名的范式，并得到离线前计算零件的支持，这些零件是贴纸语义理解，公用事业评估和个性化模块。首先，对于贴纸级别的语义理解，我们监督了微调的Llava-1.5-7b来生成类似人类的贴纸语义，并补充了从图和历史互动查询中提取的文本内容。其次，我们研究了三个众包标准，以评估贴纸公用事业。第三，我们根据用户的历史互动来群体质心，以实现个人喜好建模。最后，我们在来自微信的公共贴纸检索数据集上评估了我们提出的PERSRV方法，其中包含543,098个候选人和12,568个交互。实验结果表明，PERSRV在多模式贴纸检索中明显胜过现有方法。此外，我们的微调VLM在贴纸语义理解方面有了显着改进。

## 机器学习(cs.LG:Machine Learning)

该领域共有 97 篇论文

### Explain Like I'm Five: Using LLMs to Improve PDE Surrogate Models with Text 
[[arxiv](https://arxiv.org/abs/2410.01137)] [[cool](https://papers.cool/arxiv/2410.01137)] [[pdf](https://arxiv.org/pdf/2410.01137)]
> **Authors**: Cooper Lorsung,Amir Barati Farimani
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: 22 pages, 15 figures, 7 tables
- **标题**: 像我五岁那样解释：使用LLMS用文本改善PDE代理模型
- **领域**: 机器学习,计算物理
- **摘要**: 求解部分微分方程（PDE）在科学和工程中无处不在。计算复杂性和编写数值求解器的困难促使数据驱动的机器学习技术的开发快速生成解决方案。大型语言模型（LLMS）的最新流行度的增长已使文本轻松地集成在多模式机器学习模型中，从而可以轻松整合其他系统信息，例如边界条件和通过文本进行管理方程。在这项工作中，我们使用验证的LLM探索将各种已知系统信息集成到PDE学习中。使用FactFormer作为测试主干，我们添加了一个多模式块来融合数值和文本信息。我们比较句子级的嵌入，单词级嵌入和跨2D热量，汉堡，纳维尔 - 斯托克斯和浅水数据集的标准令牌。这些具有挑战性的基准表明，经过验证的LLM能够利用系统信息的文本描述，并仅使用初始条件才能准确预测。

### Characterizing and Efficiently Accelerating Multimodal Generation Model Inference 
[[arxiv](https://arxiv.org/abs/2410.00215)] [[cool](https://papers.cool/arxiv/2410.00215)] [[pdf](https://arxiv.org/pdf/2410.00215)]
> **Authors**: Yejin Lee,Anna Sun,Basil Hosmer,Bilge Acun,Can Balioglu,Changhan Wang,Charles David Hernandez,Christian Puhrsch,Daniel Haziza,Driss Guessous,Francisco Massa,Jacob Kahn,Jeffrey Wan,Jeremy Reizenstein,Jiaqi Zhai,Joe Isaacson,Joel Schlosser,Juan Pino,Kaushik Ram Sadagopan,Leonid Shamis,Linjian Ma,Min-Jae Hwang,Mingda Chen,Mostafa Elhoushi,Pedro Rodriguez, et al. (5 additional authors not shown)
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: 13 pages including references. 8 Figures. Under review to HPCA 2025 Industry Track
- **标题**: 表征并有效加速多模式生成模型推断
- **领域**: 机器学习
- **摘要**: 生成人工智能（AI）技术正在彻底改变计算行业。不仅其应用程序扩大到各个领域，而且还带来了新的系统设计和优化机会。该技术能够以多种方式理解和反应。但是，高级功能当前伴随着重要的系统资源需求。为了可持续地扩展生成的AI功能，可以为世界上数十亿个用户提供推理，必须快速有效。本文通过表征真实系统上新兴的多模式生成模型的家族来查明关键系统设计和优化机会。自动回归令牌生成是一种关键的延迟性能瓶颈，通常由GPU闲置时间主导。除了在生成AI模型中进行记忆密集型注意力外，由于基于变压器的模型中的Feed Forward网络，线性操作还构成了明显的推断潜伏期。我们证明，从应用程序到系统软件和硬件的最新优化杆，设置了3.88倍更好的基线。

### M2Distill: Multi-Modal Distillation for Lifelong Imitation Learning 
[[arxiv](https://arxiv.org/abs/2410.00064)] [[cool](https://papers.cool/arxiv/2410.00064)] [[pdf](https://arxiv.org/pdf/2410.00064)]
> **Authors**: Kaushik Roy,Akila Dissanayake,Brendan Tidd,Peyman Moghadam
> **First submission**: 2024-09-29
> **First announcement**: 2024-10-02
> **comment**: IEEE ICRA 2025
- **标题**: M2Distill：终身模仿学习的多模式蒸馏
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器人技术
- **摘要**: 终身模仿学习任务的模仿学习构成了巨大的挑战，这是由于逐步学习步骤中发生的分配变化。现有的方法通常集中于无监督的技能发现，以构建不断增长的技能库或从多种政策中进行蒸馏，这可能导致可扩展性问题，因为不断引入各种操纵任务，并且可能无法确保在整个学习过程中保持一致的潜在空间，从而导致灾难性的遗忘先前学习的技能。在本文中，我们介绍了M2Distill，这是一种基于多模式蒸馏的方法，用于终身模仿学习，重点是在整个学习过程中保留跨视觉，语言和动作分布的一致的潜在空间。通过调节从前步骤到当前步骤的不同方式的潜在表示的变化，并减少连续学习步骤之间的高斯混合模型（GMM）策略的差异，我们确保学习的策略保留其执行先前学习的任务的能力，同时无缝整合新技能。对Libero终生模仿学习基准套件的广泛评估，包括Libero-Object，Libero-Gaal和Libero-Spatial，这表明我们的方法始终在所有评估的指标上均超过先前的最新方法。

### Transferable Unsupervised Outlier Detection Framework for Human Semantic Trajectories 
[[arxiv](https://arxiv.org/abs/2410.00054)] [[cool](https://papers.cool/arxiv/2410.00054)] [[pdf](https://arxiv.org/pdf/2410.00054)]
> **Authors**: Zheng Zhang,Hossein Amiri,Dazhou Yu,Yuntong Hu,Liang Zhao,Andreas Zufle
> **First submission**: 2024-09-28
> **First announcement**: 2024-10-02
> **comment**: This is an accepted paper on https://sigspatial2024.sigspatial.org/accepted-papers/
- **标题**: 人类语义轨迹的可转移无监督的离群检测框架
- **领域**: 机器学习
- **摘要**: 语义轨迹通过文本信息（例如旅行目的或位置活动）丰富了时空数据，是确定对医疗保健，社会保障和城市规划至关重要的异常行为的关键。传统的异常检测取决于启发式规则，这需要领域知识并限制其识别看不见的异常值的能力。此外，缺乏一种全面的方法，可以在空间，时间和文本维度上共同考虑多模式数据。为了满足对域 - 不可思议模型的需求，我们提出了针对人类语义轨迹（TOD4TRAJ）框架的可转移异常检测。对比度学习模块进一步实现，用于确定时间和跨种群的常规移动模式，从而可以基于个体一致性和群体多数模式对离群值进行联合检测。我们的实验结果表明，TOD4TRAJ优于现有模型，证明了其在检测各个数据集中人类轨迹异常值时的有效性和适应性。

### C-MELT: Contrastive Enhanced Masked Auto-Encoders for ECG-Language Pre-Training 
[[arxiv](https://arxiv.org/abs/2410.02131)] [[cool](https://papers.cool/arxiv/2410.02131)] [[pdf](https://arxiv.org/pdf/2410.02131)]
> **Authors**: Manh Pham,Aaqib Saeed,Dong Ma
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: C熔点：对比度增强的蒙版自动编码器，用于ECG语言预训练
- **领域**: 机器学习,计算语言学
- **摘要**: 心电图（ECG）信号的准确解释对于诊断心血管疾病是关键的。通过生理数据和定性见解的结合，将ECG信号及其随附的文本报告整合起来具有巨大的潜力，可以增强临床诊断。但是，由于固有的模态差异以及标记的数据缺乏可靠的跨模式学习，这种整合面临着重大挑战。为了解决这些障碍，我们提出了C融合，这是一个新颖的框架，该框架使用对比度的自动编码器体系结构预先培训和文本数据。 C融合独特地将生成剂的优势与增强的判别能力结合在一起，以实现强大的跨模式表示。这是通过掩盖的模态建模，专门的损失函数以及针对跨模式比对的改进的负抽样策略来完成的。在各种下游任务的五个公共数据集上进行了广泛的实验表明，C融合显着胜过现有的方法，在线性探测器上分别提高了15％和2％的线性探测和零拍摄的性能。这些结果突出了C融合的有效性，强调了其通过多模式表示的潜力来推动自动临床诊断。

### Anchors Aweigh! Sail for Optimal Unified Multi-Modal Representations 
[[arxiv](https://arxiv.org/abs/2410.02086)] [[cool](https://papers.cool/arxiv/2410.02086)] [[pdf](https://arxiv.org/pdf/2410.02086)]
> **Authors**: Minoh Jeong,Min Namgung,Zae Myung Kim,Dongyeop Kang,Yao-Yi Chiang,Alfred Hero
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 锚点aweigh！航行以获得最佳的统一多模式表示形式
- **领域**: 机器学习,计算机视觉和模式识别,机器学习
- **摘要**: 多模式学习在使机器学习模型能够融合和利用各种数据源（例如文本，图像和音频）来支持各种下游任务时起着至关重要的作用。各种方式的统一表示对于提高效率和性能尤为重要。最近的结合方法，例如ImageBind（Girdhar等，2023），通常使用固定锚模式将锚式模态嵌入空间中的多峰数据对齐。在本文中，我们数学分析了固定锚点结合方法并发现明显的局限性：（1）过度依赖锚定模式的选择，（2）未能捕获模式内信息，以及（3）未能说明非模态模式之间的模式间相关性。为了解决这些限制，我们提出了一种简单而强大的方法，它消除了对固定锚的需求。取而代之的是，它采用了从所有可用方式产生的动态可调节的基于质心的锚，从而带来了平衡且丰富的表示空间。从理论上讲，我们的方法捕获了多模式学习的三个关键特性：模式内学习，模式间学习和多模式对齐，同时还构建了所有模式中强大的统一表示。我们对合成和现实世界数据集的实验证明了所提出的方法的优越性，表明动态锚方法的表现优于所有固定锚点结合方法，因为前者捕获了更细微的多模式相互作用。

### MARPLE: A Benchmark for Long-Horizon Inference 
[[arxiv](https://arxiv.org/abs/2410.01926)] [[cool](https://papers.cool/arxiv/2410.01926)] [[pdf](https://arxiv.org/pdf/2410.01926)]
> **Authors**: Emily Jin,Zhuoyi Huang,Jan-Philipp Fränken,Weiyu Liu,Hannah Cha,Erik Brockbank,Sarah Wu,Ruohan Zhang,Jiajun Wu,Tobias Gerstenberg
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: NeurIPS 2024. First two authors contributed equally. Project page: https://marple-benchmark.github.io/
- **标题**: MARPLE：长胜推论的基准
- **领域**: 机器学习
- **摘要**: 重建过去的事件需要在长期范围内进行推理。为了弄清楚发生了什么，我们需要利用有关世界和人类行为的先验知识，并从包括视觉，语言和听觉提示在内的各种证据来源得出推论。我们介绍了Marple，这是一种使用多模式证据评估长马推断能力的基准。我们的基准为代理人与模拟家庭相互作用，支持视觉，语言和听觉刺激以及程序生成的环境和代理行为。受经典的``Whodunit''故事的启发，我们要求AI模型和人类参与者推断哪种代理会根据实际发生的事情的分步重播引起环境的变化。目标是尽早正确识别罪魁祸首。我们的发现表明，人类参与者在这项任务上的表现都优于传统的蒙特卡洛模拟方法和LLM基线（GPT-4）。与人类相比，传统推理模型较不健壮和表现，而GPT-4很难理解环境变化。我们分析哪些因素影响推理绩效和消融不同的证据模式，发现所有模式对于绩效都是有价值的。总体而言，我们的实验表明，我们的基准中的长马，多模式推理任务对当前模型提出了挑战。

### Foldable SuperNets: Scalable Merging of Transformers with Different Initializations and Tasks 
[[arxiv](https://arxiv.org/abs/2410.01483)] [[cool](https://papers.cool/arxiv/2410.01483)] [[pdf](https://arxiv.org/pdf/2410.01483)]
> **Authors**: Edan Kinderman,Itay Hubara,Haggai Maron,Daniel Soudry
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 可折叠的超级网：具有不同初始化和任务的变压器的可扩展合并
- **领域**: 机器学习
- **摘要**: 许多最近的方法旨在将神经网络（NNS）与经过不同任务训练的相同体系结构合并，以获得单个多任务模型。大多数现有的作品都可以解决更简单的合并NNS从共同训练的网络初始化的NN的设置，在该网络中，简单的启发式方法（例如平均工作）很好。这项工作的目标是一个更具挑战性的目标：合并大型变压器，经过不同初始化的不同任务训练。首先，我们证明了在此设置中灾难性的传统合并方法。为了克服这一挑战，我们提出了可折叠的超级网络合并（FS-MERGE），该方法可以使用特征重建损失优化超级网络以融合原始模型。 FS-Merge是简单，数据效率的，并且能够合并不同宽度的模型。我们针对现有方法，包括知识蒸馏，MLP和变形金刚在各种设置，大小，任务和方式上测试FS-Merge。 FS-Merge始终胜过它们，从而取得了SOTA结果，尤其是在有限的数据方案中。

### Sampling from Energy-based Policies using Diffusion 
[[arxiv](https://arxiv.org/abs/2410.01312)] [[cool](https://papers.cool/arxiv/2410.01312)] [[pdf](https://arxiv.org/pdf/2410.01312)]
> **Authors**: Vineet Jain,Tara Akhound-Sadegh,Siamak Ravanbakhsh
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 使用扩散从基于能量的策略采样
- **领域**: 机器学习
- **摘要**: 基于能量的策略为建模复杂的增强学习中的多模式行为（RL）提供了灵活的框架。在最大熵RL中，最佳策略是源自软Q功能的玻尔兹曼分布，但是在连续动作空间中，从该分布中进行直接采样在计算上是棘手的。结果，现有方法通常使用更简单的参数分布（例如高斯人）来进行策略表示 - 限制了它们捕获多模式动作分布的全部复杂性的能力。在本文中，我们引入了一种基于扩散的方法，用于从基于能量的策略中取样，其中负Q功能定义了能量函数。基于这种方法，我们提出了一种称为“扩散Q抽样（DQ）的参与者批评方法，该方法可以实现更具表现力的策略表示形式，从而可以在不同的环境中进行稳定的学习。我们表明，我们的方法可以增强探索并捕获连续控制任务中的多模式行为，从而解决了现有方法的关键局限性。

### FedMAC: Tackling Partial-Modality Missing in Federated Learning with Cross-Modal Aggregation and Contrastive Regularization 
[[arxiv](https://arxiv.org/abs/2410.03070)] [[cool](https://papers.cool/arxiv/2410.03070)] [[pdf](https://arxiv.org/pdf/2410.03070)]
> **Authors**: Manh Duong Nguyen,Trung Thanh Nguyen,Huy Hieu Pham,Trong Nghia Hoang,Phi Le Nguyen,Thanh Trung Huynh
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: The 22nd International Symposium on Network Computing and Applications (NCA 2024)
- **标题**: FEDMAC：通过跨模式聚合和对比正则化解决联合学习中缺少部分模式的问题
- **领域**: 机器学习,多媒体
- **摘要**: 联合学习（FL）是一种使用分布式数据源训练机器学习模型的方法。它通过允许客户在本地存储数据时协作学习共享的全局模型来确保隐私。但是，在处理客户数据集中缺失的模式时，出现了一个重大挑战，在某些功能或模式不可用或不完整的情况下，导致了异质数据分布。尽管以前的研究已经解决了完全模式缺失的问题，但由于客户在一个实例级别上的严重异质性，他们无法应对局部模式，在这种情况下，丢失的数据模式在一个样本之间可能会有很大差异。为了应对这一挑战，这项研究提出了一个名为FEDMAC的新型框架，旨在解决FL中缺少部分模式的条件下缺少多模式的。此外，为了避免多模式特征的微不足道聚合，我们引入了基于对比的正则化，以对潜在表示空间施加其他约束。实验结果证明了FEDMAC在各种客户端构型中具有统计异质性的有效性，在严重缺失的情况下，最多超过了26％的基线方法，强调了其潜力，以解决联合系统中部分缺失模态的挑战。

### CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification 
[[arxiv](https://arxiv.org/abs/2410.03038)] [[cool](https://papers.cool/arxiv/2410.03038)] [[pdf](https://arxiv.org/pdf/2410.03038)]
> **Authors**: Jinghao Shi,Xiang Shen,Kaili Zhao,Xuedong Wang,Vera Wen,Zixuan Wang,Yifan Wu,Zhixin Zhang
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Camera ready for CIKM 2024
- **标题**: CPFD：短视频分类的信心感知特权蒸馏
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 密集的功能是针对不同业务方案定制的，对于简短的视频分类至关重要。但是，它们的复杂性，特定的适应要求和高计算成本使它们在在线推断期间的资源密集型且易于访问范围。因此，这些密集的功能被归类为“特权密集功能”。同时，端到端的多模式模型在众多计算机视觉任务中显示出令人鼓舞的结果。在工业应用中，优先考虑端到端的多模式功能可以提高效率，但通常会导致从历史特权密集特征中丢失有价值的信息。为了整合这两个功能，同时保持效率和可管理的资源成本，我们提出了具有信心的特权功能蒸馏（CPFD），该功能蒸馏（CPFD）通过在训练过程中适应特权特征来增强端到端多模型模型的特征。与现有的特权功能蒸馏（PFD）方法不同，它们在蒸馏过程中的所有实例中都施加均匀的权重，可能会在不同的业务场景中引起不稳定的性能，并且在教师模型之间存在明显的性能差距（密集特征增强的多模式模型DF-X-VLM）和仅启用了MydiModal模型的X-VLM，CPF的启发，我们的CPF，我们的CPF均可播放，我们通过学生模型自适应减轻性能差异。我们对五项不同任务进行了广泛的离线实验，表明CPFD将视频分类F1得分提高了6.76％，而端到端的多模式模型（X-VLM）和Vanilla PFD On-Apervery进行了2.31％。它将绩效差距降低了84.6％，并达到与教师DF-X-VLM相当的结果。在线实验进一步证实了CPFD的有效性，我们的框架已在生产系统中用于多种模型。

### MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection 
[[arxiv](https://arxiv.org/abs/2410.03010)] [[cool](https://papers.cool/arxiv/2410.03010)] [[pdf](https://arxiv.org/pdf/2410.03010)]
> **Authors**: Niki Nezakati,Md Kaykobad Reza,Ameya Patil,Mashhour Solh,M. Salman Asif
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: MMP：使用蒙版模态投影朝着强大的多模式学习
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 多模式学习试图结合来自多个输入源的数据，以增强不同下游任务的性能。在实际情况下，如果缺少某些输入方式，则性能会大大降低。可以处理缺失模式的现有方法涉及每个输入方式组合的自定义培训或适应步骤。这些方法要么与特定方式相关，要么随着输入方式的数量增加而变得昂贵。在本文中，我们提出了蒙版模态投影（MMP），该方法旨在训练一个对任何缺失模态场景都有坚固型模型的训练。我们通过在训练和学习过程中随机掩盖一部分方式来实现这一目标，以估算掩盖方式的代币。这种方法使该模型能够有效地学会利用可用模式的信息来补偿缺失的信息，从而增强缺失的模态鲁棒性。我们使用各种基线模型和数据集进行了一系列实验，以评估该策略的有效性。实验表明，我们的方法提高了对不同缺失模态情景的鲁棒性，优于设计用于缺失模态或特定模态组合的现有方法。

### LoGra-Med: Long Context Multi-Graph Alignment for Medical Vision-Language Model 
[[arxiv](https://arxiv.org/abs/2410.02615)] [[cool](https://papers.cool/arxiv/2410.02615)] [[pdf](https://arxiv.org/pdf/2410.02615)]
> **Authors**: Duy M. H. Nguyen,Nghiem T. Diep,Trung Q. Nguyen,Hoang-Bao Le,Tai Nguyen,Tien Nguyen,TrungTin Nguyen,Nhat Ho,Pengtao Xie,Roger Wattenhofer,James Zhou,Daniel Sonntag,Mathias Niepert
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: First version, fixed typo
- **标题**: LOGRA-MED：医学视觉语言模型的长上下文多绘图
- **领域**: 机器学习
- **摘要**: 最先进的医学多模式大型语言模型（MED-MLLM），例如LLAVA-MED或BIOMEDGPT，在预训练中利用遵循的指导数据。但是，这些模型主要集中于扩展模型大小和数据量以提高性能，同时主要依靠自回归学习目标。令人惊讶的是，我们透露，这样的学习方案可能会导致视觉和语言方式之间的一致性较弱，从而使这些模型高度依赖广泛的预训练数据集 - 由于策划高质量指导以遵循范围的实例的昂贵且耗时的性质，在医疗领域中的一个重大挑战。我们使用Logra-Med解决此问题，这是一种新的多仪表对齐算法，可在图像模式，基于对话的描述和扩展字幕上强制执行三重态相关。这有助于模型捕获上下文含义，处理语言可变性，并在视觉效果和文本之间建立跨模式关联。为了扩展我们的方法，我们使用黑盒梯度估计设计了一种有效的端到端学习方案，从而实现了更快的Llama 7b培训。我们的结果表明，Logra-MED匹配了600K图像文本对医疗VQA的LLAVA-MED性能，并在10％的数据进行培训时大大优于它。例如，在VQA-RAD上，我们超过了Llava-MED 20.13％，几乎与100％的训练得分相匹配（72.52％比72.64％）。我们还超越了SOTA方法，例如在视觉聊天机器人上使用BioMedgpt和使用VQA进行零拍图像分类的RADFM，突出了多绘图对齐的有效性。

### ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction 
[[arxiv](https://arxiv.org/abs/2410.04721)] [[cool](https://papers.cool/arxiv/2410.04721)] [[pdf](https://arxiv.org/pdf/2410.04721)]
> **Authors**: Hyungjin Chung,Dohun Lee,Jong Chul Ye
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: 25 pages, 10 figures. Project page: https://acdc2025.github.io/
- **标题**: ACDC：使用扩散校正的自回旋相干多模式生成
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 自回旋模型（ARM）和扩散模型（DMS）代表生成建模中的两个领先范式，每个范式在不同的领域中都出色：全球环境建模和长期序列生成中的武器以及DMS在产生高质量的局部环境方面，尤其是对于持续的数据，尤其是图像和短视频等连续数据。但是，手臂通常会在长序列上累积指数误差，从而导致物理上难以置信的结果，而DMS受其局部上下文生成能力的限制。在这项工作中，我们通过扩散校正（ACDC）引入自回归相干的多模式产生，这是一种零射击方法，在推理阶段结合了臂和DMS的优势，而无需进行额外的微调。 ACDC利用武器为全球上下文生成和内存条件DMS进行局部校正，从而通过校正生成的多模式令牌中的工件来确保高质量的输出。特别是，我们提出了一个基于大语言模型（LLM）的内存模块，该模块会动态调整DMS的条件文本，从而保留至关重要的全局上下文信息。我们对多模式任务的实验，包括连贯的多帧故事产生和自回归视频生成，表明ACDC有效地减轻了错误的积累，并显着提高了生成的输出的质量，同时使得对特定的ARM和DM架构保持不可侵害。项目页面：https：//acdc2025.github.io/

### Enhancing Carbon Emission Reduction Strategies using OCO and ICOS data 
[[arxiv](https://arxiv.org/abs/2410.04288)] [[cool](https://papers.cool/arxiv/2410.04288)] [[pdf](https://arxiv.org/pdf/2410.04288)]
> **Authors**: Oskar Åström,Carina Geldhauser,Markus Grillitsch,Ola Hall,Alexandros Sopasakis
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: 18 pages, 7 figures, 1 table, 1 algorithm
- **标题**: 使用OCO和ICOS数据增强减少碳排放策略
- **领域**: 机器学习
- **摘要**: 我们提出了一种方法，通过整合来自轨道碳天文台（OCO-2和OCO-3）的卫星数据，并从集成碳观察系统（ICOS）和ECMWF重新分析V5（ERA5）中进行地面观测来增强局部二氧化碳监测。与传统方法不同的方法是国家数据，我们的方法使用多模式数据融合来进行高分辨率二氧化碳估计。我们使用机器学习模型使用加权K-Nearest邻居（KNN）插值来预测卫星测量值地面CO2，从而达到3.92 ppm的根平方误差。我们的结果表明，将各种数据源整合在捕获局部发射模式中的有效性，突出了高分辨率大气传输模型的价值。开发的模型改善了二氧化碳监测的粒度，为靶向碳缓解策略提供了精确的见解，并代表了神经网络和KNN在环境监测中的新应用，可适应各种区域和时间尺度。

### Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning 
[[arxiv](https://arxiv.org/abs/2410.04223)] [[cool](https://papers.cool/arxiv/2410.04223)] [[pdf](https://arxiv.org/pdf/2410.04223)]
> **Authors**: Gang Liu,Michael Sun,Wojciech Matusik,Meng Jiang,Jie Chen
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: 27 pages, 11 figures, 4 tables
- **标题**: 逆向合成计划的逆分子设计的多模式大语言模型
- **领域**: 机器学习,化学物理,生物分子
- **摘要**: 尽管大型语言模型（LLM）具有集成图像，但将它们适应图表仍然具有挑战性，从而限制了它们在材料和药物设计中的应用。这种困难源于在文本和图表之间进行连贯的自回归产生。为了解决这个问题，我们介绍了Llamole，这是第一个能够交织的文本和图生成的多模式LLM，从而实现了带有递归合成计划的分子逆设计。 Llamole将基本LLM与图扩散变压器和图神经网络相结合，以用于多条件分子的产生和文本中的反应推断，而LLM则具有增强的分子理解，灵活地控制了不同图形模块之间的激活。此外，Llamole将*搜索与基于LLM的成本功能相结合，以进行有效的递归术计划。我们创建基准测试数据集并进行广泛的实验，以评估紫红色，以防止在文本学习和监督微调。 llamole在12个指标上明显胜过14个改编的LLM，用于可控的分子设计和循环术计划。

### An Electrocardiogram Foundation Model Built on over 10 Million Recordings with External Evaluation across Multiple Domains 
[[arxiv](https://arxiv.org/abs/2410.04133)] [[cool](https://papers.cool/arxiv/2410.04133)] [[pdf](https://arxiv.org/pdf/2410.04133)]
> **Authors**: Jun Li,Aaron Aguirre,Junior Moura,Che Liu,Lanhai Zhong,Chenxi Sun,Gari Clifford,Brandon Westover,Shenda Hong
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: working in progress
- **标题**: 心电图基础模型建立在超过1000万张录音的基础上，并在多个领域进行外部评估
- **领域**: 机器学习,人工智能,信号处理
- **摘要**: 人工智能（AI）在心电图分析和心血管疾病评估中表现出巨大的潜力。最近，基础模型在进步医学AI方面发挥了出色的作用。 ECG基金会模型的发展具有将AI-ECG研究提升到新高度的希望。但是，建立这样的模型面临着几个挑战，包括数据库样本量不足和跨多个域的概括不足。此外，单铅和多铅ECG分析之间存在明显的性能差距。我们引入了ECG基础模型（ECGFUNGER），这是一种通用模型，该模型利用心脏病学专家的现实世界ECG注释来扩大ECG分析的诊断能力。对超过1000万个ECG进行了ECGFUNDER，该ECG的培训来自哈佛大学ECG数据库的150个标签类别，从而通过ECG分析实现了全面的心血管疾病诊断。该模型既设计为有效的开箱即用解决方案，又是可用于下游任务的微调，从而最大程度地提高可用性。重要的是，我们将其应用扩展到较低的ECG，尤其是任意的单铅ECG。 Ecgfounder适用于在移动监控方案中支持各种下游任务。实验结果表明，在内部验证集上，持续的人在内部验证集上实现了专家级别的性能，而AUROC超过0.95诊断。它还显示了在外部验证集的各种诊断中的强大分类性能和概括。经过微调时，在人口统计学分析，临床事件检测和跨模式心律诊断中，cofunder的表现优于基线模型。训练有素的模型和数据将在出版物中通过BDSP.IO公开发布。我们的代码可从https://github.com/bdsp-core/ecgfounder获得

### Metadata Matters for Time Series: Informative Forecasting with Transformers 
[[arxiv](https://arxiv.org/abs/2410.03806)] [[cool](https://papers.cool/arxiv/2410.03806)] [[pdf](https://arxiv.org/pdf/2410.03806)]
> **Authors**: Jiaxiang Dong,Haixu Wu,Yuxuan Wang,Li Zhang,Jianmin Wang,Mingsheng Long
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 元数据对于时间序列很重要：与变压器的信息预测
- **领域**: 机器学习,计算语言学
- **摘要**: 时间序列预测在广泛的现实应用程序（例如财务分析和能源计划）中很普遍。先前的研究主要集中在时间序列方式上，努力捕获时间序列固有的复杂变化和依赖性。除了数字时间序列数据之外，我们还会注意到元数据（例如〜数据集和variate描述）还具有预测必不可少的宝贵信息，这些信息可用于识别应用程序方案并提供比数字序列更明显的知识。受到这一观察的启发，我们提出了一个信息元素的时间序列变压器（metatst），该变压器（metAtst）将多个级别的特定于上下文特定元数据纳入变压器预测模型中，以实现信息性的时间序列预测。为了解决元数据的非结构化性质，Metatst通过预设设计的模板和利用大语言模型（LLMS）将它们形式化为自然语言，将这些文本编码为元数据代币，以补充经典系列代币，从而导致信息嵌入。此外，使用变压器编码器来传达串联和元数据代币，该代币可以通过元数据信息扩展串联表示形式，以更准确地预测。该设计还使模型可以自适应地学习各种情况的上下文特定模式，这在处理大规模，多样化的预测任务方面特别有效。在实验上，与高级时间序列模型和基于LLM的方法相比，MetAtST实现了最先进的方法，该方法涵盖了公认的短期和长期预测基准，涵盖了单数据库个人和多数据集合联合培训设置。

### MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents 
[[arxiv](https://arxiv.org/abs/2410.03450)] [[cool](https://papers.cool/arxiv/2410.03450)] [[pdf](https://arxiv.org/pdf/2410.03450)]
> **Authors**: Junpeng Yue,Xinru Xu,Börje F. Karlsson,Zongqing Lu
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: MLLM作为检索器：体现剂的互动学习多模式检索
- **领域**: 机器学习
- **摘要**: MLLM代理通过检索多模式与任务相关的轨迹数据来展示复杂的体现任务的潜力。但是，当前的检索方法主要集中于轨迹中文本或视觉提示的表面级相似性，从而忽略了它们对当前特定任务的有效性。为了解决这个问题，我们提出了一种新颖的方法，即MLLM作为检索器（MART），该方法通过利用相互作用数据来根据偏好学习来微调MLLM检索器，从而增强了体现剂的性能，从而使得检索器充分考虑了轨迹的有效性，并确定了它们的优先级别。我们还引入了轨迹抽象，该机制利用MLLM的摘要功能来表示具有较少令牌的轨迹，同时保留关键信息，从而使代理能够更好地理解轨迹中的里程碑。与基线方法相比，各种环境之间的实验结果表明，我们的方法显着提高了看不见的场景的任务成功率。这项工作通过微调通用MLLM作为评估轨迹有效性的检索器，提出了用于体现药物中多模式检索的新范式。将发布所有基准任务集和模拟器代码修改，以进行操作和观察空间。

### SELU: Self-Learning Embodied MLLMs in Unknown Environments 
[[arxiv](https://arxiv.org/abs/2410.03303)] [[cool](https://papers.cool/arxiv/2410.03303)] [[pdf](https://arxiv.org/pdf/2410.03303)]
> **Authors**: Boyu Li,Haobin Jiang,Ziluo Ding,Xinrun Xu,Haoran Li,Dongbin Zhao,Zongqing Lu
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: SELU：在未知环境中的自学体现的MLLM
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 最近，多模式的大语言模型（MLLM）表现出强烈的视觉理解和决策能力，从而探索了在未知环境中自主改善MLLM的探索。但是，外部反馈（例如人类或环境反馈）并不总是可用。为了应对这一挑战，现有的方法主要集中于通过投票和评分机制增强MLLM的决策能力，而在改善未知环境中MLLM的环境理解方面几乎没有付出努力。为了完全释放MLLM的自学潜力，我们提出了一种新型演员评论的自学范式，被称为Selu，灵感来自于演员批判性范式增强学习中的启发。评论家采用自我掩护和事后重新标记来从演员收集的互动轨迹中提取知识，从而增强其环境理解。同时，演员通过评论家提供的自我反馈改善，增强了其决策。我们在AI2和虚拟机环境中评估了我们的方法，Selu实现了大约28％和30％的评论家的改善，而演员通过自学来提高约20％和24％。

### RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction 
[[arxiv](https://arxiv.org/abs/2410.05361)] [[cool](https://papers.cool/arxiv/2410.05361)] [[pdf](https://arxiv.org/pdf/2410.05361)]
> **Authors**: Yuwei Zhang,Tong Xia,Aaqib Saeed,Cecilia Mascolo
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: RESPLLM：用多模式LLM统一音频和文本用于广义呼吸健康预测
- **领域**: 机器学习,人工智能,声音,音频和语音处理
- **摘要**: 与呼吸道疾病有关的高发病率和死亡率强调了早期筛查的重要性。机器学习模型可以自动化临床咨询和听诊，并在该领域提供重要的支持。但是，涉及的数据，涵盖人口统计学，病史，症状和呼吸音频，是异质且复杂的。现有的方法不足，缺乏普遍性，因为它们通常依赖于有限的培训数据，基本融合技术和特定于任务的模型。在本文中，我们提出了Respllm，这是一种新型的多模式大型语言模型（LLM）框架，该框架将文本和音频表示统一以进行呼吸健康预测。 Respllm利用了经过验证的LLM的广泛先验知识，并通过跨模式的注意力可以有效地进行音频融合。指令调整用于整合来自多个来源的多种数据，以确保模型的普遍性和多功能性。五个现实世界数据集的实验表明，Respllm在训练有素的任务上平均胜过4.6％，在看不见的数据集中7.9％，并促进对新任务的零射击预测。我们的工作为可以感知，倾听和理解异质数据的多模型模型奠定了基础，为可扩展的呼吸健康诊断铺平了道路。

### Interactive Event Sifting using Bayesian Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2410.05359)] [[cool](https://papers.cool/arxiv/2410.05359)] [[pdf](https://arxiv.org/pdf/2410.05359)]
> **Authors**: José Nascimento,Nathan Jacobs,Anderson Rocha
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Accepted in IEEE International Workshop on Information Forensics and Security - WIFS 2024, Rome, Italy
- **标题**: 使用贝叶斯图神经网络进行互动事件筛选
- **领域**: 机器学习,社交和信息网络
- **摘要**: 法医分析师经常使用社交媒体图像和文本来了解重要事件。主要的挑战是最初的筛选帖子。这项工作介绍了一个互动过程，用于培训一种以事件为中心的，基于学习的多模式分类模型，该模型可以自动消毒。我们提出了一种基于贝叶斯图神经网络（BGNN）的方法，并评估主动学习和伪标记公式，以减少分析师必须手动注释的职位数量。我们的结果表明，BGNN可用于社交媒体数据筛选感兴趣的事件的取证研究，主动学习和伪标记的价值根据设置而有所不同，并结合其他事件的未标记数据可改善性能。

### Recent Advances of Multimodal Continual Learning: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2410.05352)] [[cool](https://papers.cool/arxiv/2410.05352)] [[pdf](https://arxiv.org/pdf/2410.05352)]
> **Authors**: Dianzhi Yu,Xinni Zhang,Yankai Chen,Aiwei Liu,Yifei Zhang,Philip S. Yu,Irwin King
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: 多模式持续学习的最新进展：一项综合调查
- **领域**: 机器学习,人工智能
- **摘要**: 持续学习（CL）旨在使机器学习模型能够从新数据中不断学习，同时在先前获得的知识的基础上而无需忘记。随着机器学习模型从小型到大型预训练的架构发展，从支持单峰到多模式数据，最近出现了多模式持续学习（MMCL）方法。 MMCL的主要挑战是它超出了单峰CL方法的简单堆叠，因为这种直接的方法通常会产生不令人满意的性能。在这项工作中，我们介绍了有关MMCL的首次全面调查。我们提供基本的背景知识和MMCL设置，以及MMCL方法的结构化分类法。我们将现有的MMCL方法分为四个类别，即基于正则化的，基于架构的，基于重播和基于及时的方法，解释其方法并突出其关键创新。此外，为了促使该领域的进一步研究，我们总结了开放的MMCL数据集和基准，并讨论了一些有希望的未来调查和开发方向。我们还创建了一个GitHub存储库，用于索引相关的MMCL论文和开放资源，请访问https://github.com/lucydyu/awesome-multimodal-continual-learning。

### AnyAttack: Targeted Adversarial Attacks on Vision-Language Models toward Any Images 
[[arxiv](https://arxiv.org/abs/2410.05346)] [[cool](https://papers.cool/arxiv/2410.05346)] [[pdf](https://arxiv.org/pdf/2410.05346)]
> **Authors**: Jiaming Zhang,Junhong Ye,Xingjun Ma,Yige Li,Yunfan Yang,Jitao Sang,Dit-Yan Yeung
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: AnyAttack：针对视觉模型的针对对抗性攻击对任何图像
- **领域**: 机器学习,人工智能
- **摘要**: 由于它们的多模式功能，视觉语言模型（VLM）在现实世界中发现了许多有影响力的应用。但是，最近的研究表明，VLM容易受到基于图像的对抗攻击的影响，尤其是针对性的对抗图像，这些图像操纵模型以产生对手指定的有害内容。当前的攻击方法依靠预定义的目标标签来创建目标对抗攻击，这限制了其可扩展性和用于大规模鲁棒性评估的适用性。在本文中，我们提出了AnyAttack，这是一个自制的框架，在没有标签监督的情况下为VLM生成目标的对抗图像，从而允许任何图像作为攻击的目标。我们的框架采用了预训练和微调范式，对对抗性噪声发生器进行了预先训练的大规模LAION-400M数据集。这种大规模的预训练使我们的方法具有在广泛的VLM上的强大可传递性。在三个多模式任务（图像文本检索，多模式分类和图像字幕）上，对五个主流开源VLM（夹，Blip，Blip2，Mineigpt-4）进行了五个主流实验。此外，我们成功地将AnyAttack转移到了多个商业VLM上，包括Google Gemini，Claude Sonnet，Microsoft Copilot和Openai GPT。这些结果揭示了VLM的前所未有的风险，强调了对有效对策的需求。

### Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data 
[[arxiv](https://arxiv.org/abs/2410.05078)] [[cool](https://papers.cool/arxiv/2410.05078)] [[pdf](https://arxiv.org/pdf/2410.05078)]
> **Authors**: David Heurtel-Depeiges,Anian Ruoss,Joel Veness,Tim Genewein
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: No comments
- **标题**: 通过预训练的变压器进行压缩：关于字节级多模式数据的研究
- **领域**: 机器学习,人工智能,信息论
- **摘要**: 基础模型最近已被证明是强大的数据压缩机。但是，当考虑其过度参数计数时，其压缩比实际上不如标准压缩算法。此外，天真地减少参数的数量可能不一定会有所帮助，因为它会导致较差的预测，从而较弱。在本文中，我们进行了一项大规模的经验研究，以研究是否有一个最佳的竞争性压缩比具有预训练的香草变压器。为此，我们在165GB的文本，图像或音频数据的原始字节序列（以及三个可能的所有可能组合）上训练模型家族，然后从每种模态压缩1GB的1GB（OOD）数据。我们发现，相对较小的模型（即数百万参数）可以胜过标准通用 - 用途压缩算法（GZIP，LZMA2），甚至是域特异性压缩机（PNG，JPEG 2000，FLAC），即使在参数计数中分解时，也是如此。我们实现了OOD音频数据的最低压缩比（FLAC 0.54）。为了研究模型和数据集量表的影响，我们进行了广泛的消融和超参数扫描，并研究了单峰与多模态训练的效果。我们发现，即使是小型模型也可以训练以在多种方式上表现良好，但是与先前报道的大规模基础模型的结果相反，转移到看不见的方式通常很弱。

### TLDR: Token-Level Detective Reward Model for Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2410.04734)] [[cool](https://papers.cool/arxiv/2410.04734)] [[pdf](https://arxiv.org/pdf/2410.04734)]
> **Authors**: Deqing Fu,Tong Xiao,Rui Wang,Wang Zhu,Pengchuan Zhang,Guan Pang,Robin Jia,Lawrence Chen
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Published as a conference paper at ICLR 2025
- **标题**: TLDR：大型视觉语言模型的令牌级侦探奖励模型
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 尽管奖励模型已经成功地改善了多模式的大语言模型，但奖励模型本身仍然残酷，并且包含最少的信息。值得注意的是，现有的奖励模型仅通过向任何文本分配一个二进制反馈来模仿人类注释，无论文本多长时间。在需要模型来处理图像和文本的多模式模型的领域中，幼稚的奖励模型可能会学习对文本的隐性偏见，并且在图像中占据较低的基础。在本文中，我们提出了一个$ \ textbf {t} $ oken-$ \ textbf {l} $ evel $ \ textbf {d} $ etctive $ \ textbf {r textbf {r} $ eward Model（$ \ textbf {tldr} $），以向每个文本提供精细的nototation text text text text togken token。我们首先引入了一种基于扰动的方法，以生成合成硬质量质量及其令牌级别的标签来训练TLDR模型。然后，我们展示了TLDR模型在协助现成模型以自我纠正以及充当幻觉评估工具方面的丰富实用性。我们表明，TLDR会自动训练令牌级的可能性优化，并可以显着提高基本模型的性能。最后，我们表明TLDR模型可以大大加快人类注释3次，以获取更广泛的高质量视力语言数据。

### Multimodal Representation Learning using Adaptive Graph Construction 
[[arxiv](https://arxiv.org/abs/2410.06395)] [[cool](https://papers.cool/arxiv/2410.06395)] [[pdf](https://arxiv.org/pdf/2410.06395)]
> **Authors**: Weichen Huang
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 使用自适应图构建多模式表示学习
- **领域**: 机器学习,人工智能
- **摘要**: 多模式对比度学习训练神经网络通过勒沃加数据从图像和文本等异质来源的数据。但是，许多当前的多模式学习体系结构无法概括为任意数量的模式，需要手工构造。我们提出了AutoBind，这是一个新颖的对比学习框架，可以通过图形优化从任意数量的模态学习表示表示。我们评估了Autobind，因为它具有现实世界中的医疗适用性，并且包含广泛的数据方式。我们表明，自动索在此任务上的先前方法优于以前的方法，从而突出了该方法的普遍性。

### FLOPS: Forward Learning with OPtimal Sampling 
[[arxiv](https://arxiv.org/abs/2410.05966)] [[cool](https://papers.cool/arxiv/2410.05966)] [[pdf](https://arxiv.org/pdf/2410.05966)]
> **Authors**: Tao Ren,Zishi Zhang,Jinyang Jiang,Guanghao Li,Zeliang Zhang,Mingqian Feng,Yijie Peng
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: Published in the Thirteenth International Conference on Learning Representations(ICLR 2025)
- **标题**: 拖鞋：最佳抽样的前进学习
- **领域**: 机器学习,人工智能
- **摘要**: 鉴于反向传播的局限性，基于扰动的梯度计算方法最近仅使用正向通行证（也称为查询）获得了学习的重点。传统的前向学习会在每个数据点上使用巨大的查询，以通过蒙特卡洛采样来准确梯度估计，从而阻碍这些算法的可扩展性。但是，并非所有数据点都应对梯度估计值相同的查询。在本文中，我们研究了从新颖的角度提高前瞻性学习效率的问题：如何减少最低成本的梯度估计差异？为此，我们建议在培训期间一批分配每个数据的最佳查询数量，以在估计准确性和计算效率之间取得良好的平衡。具体而言，采用简化的代理目标和重新聚集技术，我们得出了具有最小参数的新型插件查询分配器。进行理论结果以验证其最优性。我们对各种数据集上的微调视觉变压器进行了广泛的实验，并将分配器进一步部署到两个黑框应用程序：及时调整和基础模型的多模式对齐。所有发现表明，我们提出的分配器显着提高了前进算法的可扩展性，为现实世界应用铺平了道路。

### Exploring the Meaningfulness of Nearest Neighbor Search in High-Dimensional Space 
[[arxiv](https://arxiv.org/abs/2410.05752)] [[cool](https://papers.cool/arxiv/2410.05752)] [[pdf](https://arxiv.org/pdf/2410.05752)]
> **Authors**: Zhonghan Chen,Ruiyuan Zhang,Xi Zhao,Xiaojun Cheng,Xiaofang Zhou
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 探索在高维空间中最近邻居搜索的有意义
- **领域**: 机器学习,数据库,信息检索
- **摘要**: 在计算机视觉，机器学习和大型语言模型（LLMS）等领域，密集的高维矢量越来越重要，它是多模式数据的标准表示。现在，这些向量的维度可以轻松超过数千个。尽管最近对这些密集的高维矢量进行了最近的邻居搜索（NNS）已被广泛用于检索增强发电（RAG）和许多其他应用，但由于“维度诅咒”造成的挑战，NNS在这样的高维空间中的有效性仍然不确定。为了解决上述问题，在本文中，我们在不同的距离，$ L_1 $距离，$ L_2 $距离和角距离等不同距离功能的研究中进行了广泛的NNS研究，跨越各种类型的嵌入数据集，维度和方式。我们的目的是研究影响NNS意义的因素。我们的实验表明，与随机矢量相比，高维文本嵌入的弹性表现出更高的弹性，随着尺寸上升到更高的水平。这种韧性表明，文本嵌入对“维度的诅咒”的影响较小，从而导致更有意义的NNS结果可用于实际使用。此外，距离函数的选择对NNS的相关性有很小的影响。我们的研究显示了基于嵌入的数据表示方法的有效性，并可以为进一步优化密集的矢量相关应用提供机会。

### Generating Origin-Destination Matrices in Neural Spatial Interaction Models 
[[arxiv](https://arxiv.org/abs/2410.07352)] [[cool](https://papers.cool/arxiv/2410.07352)] [[pdf](https://arxiv.org/pdf/2410.07352)]
> **Authors**: Ioannis Zachos,Mark Girolami,Theodoros Damoulas
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 在神经空间相互作用模型中生成起源用途矩阵
- **领域**: 机器学习,机器学习
- **摘要**: 基于代理的模型（ABM）正在作为运输，经济学和流行病学政策领域的决策工具增殖。在这些模型中，感兴趣的中心对象是离散的原点用途矩阵，该矩阵捕获了位置之间的空间相互作用和代理跳闸计数。现有方法诉诸该矩阵的连续近似以及随后的临时离散，以执行ABM模拟和校准。这阻碍了部分观察到的摘要统计数据，无法在离散组合支持上探索多模式矩阵分布，并造成离散误差。为了应对这些挑战，我们引入了一个计算高效的框架，该框架与原点用途对数量线性扩展，直接在离散组合空间上运行，并通过嵌入空间相互作用的神经微分方程来学习代理的跳闸强度。我们的方法在重建错误和地面真相矩阵覆盖范围方面优于先前的艺术，而计算成本的一小部分。我们在英国剑桥和美国华盛顿特区的大规模空间移动性ABM中证明了这些好处。

### ElasticTok: Adaptive Tokenization for Image and Video 
[[arxiv](https://arxiv.org/abs/2410.08368)] [[cool](https://papers.cool/arxiv/2410.08368)] [[pdf](https://arxiv.org/pdf/2410.08368)]
> **Authors**: Wilson Yan,Volodymyr Mnih,Aleksandra Faust,Matei Zaharia,Pieter Abbeel,Hao Liu
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: Elastictok：图像和视频的自适应令牌化
- **领域**: 机器学习
- **摘要**: 在学习能够处理长时间视频序列的通用视觉模型中，有效的视频令牌仍然是一个关键的瓶颈。流行的方法仅限于将视频编码为固定数量的令牌，在这种情况下，代币太少会导致过度有损的编码，而太多的令牌将导致序列长度长。在这项工作中，我们介绍了Elastictok，这种方法可以在先前的框架上适应框架将框架编码为可变数量的令牌。为了以计算可扩展的方式启用此功能，我们提出了一种掩盖技术，该技术在每个帧的令牌编码的末尾删除随机数的令牌。在推断期间，弹性蛋白可以在需要时动态分配令牌 - 更复杂的数据可以利用更多的令牌，而更简单的数据只需要一些令牌。我们对图像和视频的经验评估证明了我们方法在有效的代币使用方面的有效性，为未来更强大的多峰模型，世界模型和代理商的发展铺平了道路。

### Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts 
[[arxiv](https://arxiv.org/abs/2410.08245)] [[cool](https://papers.cool/arxiv/2410.08245)] [[pdf](https://arxiv.org/pdf/2410.08245)]
> **Authors**: Sukwon Yun,Inyoung Choi,Jie Peng,Yangfan Wu,Jingxuan Bao,Qiyiwen Zhang,Jiayi Xin,Qi Long,Tianlong Chen
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: NeurIPS 2024 Spotlight
- **标题**: Flex-Moe：通过柔性专家混合物对任意模态组合进行建模
- **领域**: 机器学习,人工智能
- **摘要**: 多模式学习在各个领域的重要性越来越重要，提供了整合来自图像，文本和个性化记录等不同来源的数据的能力，这些数据经常在医疗领域中观察到。但是，在缺少某些模式的情况下，许多现有的框架都难以适应任意模态组合，通常很大程度上依赖于单一的模式或完整的数据。对潜在方式组合的这种监督限制了它们在现实情况下的适用性。为了应对这一挑战，我们提出了Flex-Moe（灵活的Experts），这是一个新框架，旨在灵活地合并任意模态组合，同时保持较健壮的数据。 Flex-MoE的核心思想是首先使用新的缺失模态库来解决缺失模态，该模态库将观察到的模态组合与相应的缺失的模态组合在一起。其次是设计独特的稀疏MOE框架。具体来说，Flex-MoE首先使用具有各种方式的样本来训练专家，通过广义路由器（$ \ Mathcal {G} $  - 路由器）注入广义知识。然后，$ \ Mathcal {S} $  - 路由器通过将TOP-1门分配给对应于观察到的模态组合的专家，专门处理更少的模态组合。我们在ADNI数据集上评估了Flex-MoE，该数据集涵盖了阿尔茨海默氏病领域的四个模式，以及模拟物IV数据集。结果表明，Flex-MoE强调了其在各种缺失的模态方案中建模任意模态组合的能力。代码可从https://github.com/unites-lab/flex-moe获得。

### Self-Attention Mechanism in Multimodal Context for Banking Transaction Flow 
[[arxiv](https://arxiv.org/abs/2410.08243)] [[cool](https://papers.cool/arxiv/2410.08243)] [[pdf](https://arxiv.org/pdf/2410.08243)]
> **Authors**: Cyrile Delestre,Yoann Sola
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 银行交易流的多模式环境中的自我注意机制
- **领域**: 机器学习,人工智能
- **摘要**: 银行交易流（BTF）是在许多银行活动，例如营销，信用风险或银行欺诈等银行活动中发现的顺序数据。它是由三种模式组成的多模式数据：日期，数值和措辞。我们在这项工作中提出了自我注意机制在BTF处理中的应用。我们以一种自我监督的方式训练了大量BTF的两个通用模型：一种基于RNN的模型和一个基于变压器的模型。我们提出了一个特定的令牌化，以便能够处理BTF。在两个银行下游任务上评估了这两个模型的性能：交易分类任务和信用风险任务。结果表明，对这两个预训练的模型进行微调可以比这两个任务的最先进方法更好。

### MolMix: A Simple Yet Effective Baseline for Multimodal Molecular Representation Learning 
[[arxiv](https://arxiv.org/abs/2410.07981)] [[cool](https://papers.cool/arxiv/2410.07981)] [[pdf](https://arxiv.org/pdf/2410.07981)]
> **Authors**: Andrei Manolache,Dragos Tantaru,Mathias Niepert
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: Machine Learning for Structural Biology Workshop, NeurIPS 2024 v2: Added optimizer references
- **标题**: Molmix：多模式分子表示学习的简单而有效的基线学习
- **领域**: 机器学习,人工智能
- **摘要**: 在这项工作中，我们提出了一个简单的基于变压器的基线，用于多模式分子表示学习，整合了三种不同的模态：微笑字符串，2D图表示和分子的3D构象异构体。我们方法的一个关键方面是3D构象异构体的聚集，使该模型可以解释分子可以采用多种构象的事实 - 准确的分子表示的重要因素。使用特定于模态的编码器提取每种模式的令牌：微笑字符串的变压器，2D图的消息通讯神经网络以及用于3D构象异构体的e象神经网络。该框架的灵活性和模块化使这些编码器易于适应和更换，使该模型用于不同的分子任务。然后将提取的令牌组合成一个统一的多模式序列，该序列由下游变压器处理以进行预测任务。为了有效地扩展大型多模式数据集的模型，我们利用了Flash Pastion 2和Bfloat16精度。尽管它很简单，但我们的方法还是在多个数据集中取得了最先进的结果，证明了其有效性是多模式分子表示学习的强大基线。

### Scalable Representation Learning for Multimodal Tabular Transactions 
[[arxiv](https://arxiv.org/abs/2410.07851)] [[cool](https://papers.cool/arxiv/2410.07851)] [[pdf](https://arxiv.org/pdf/2410.07851)]
> **Authors**: Natraj Raman,Sumitra Ganesh,Manuela Veloso
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 多模式表交易的可扩展表示学习
- **领域**: 机器学习
- **摘要**: 大型语言模型（LLMS）的主要目的是理解非结构化的文本。当直接应用于结构化格式（例如表格数据）时，它们可能难以辨别固有的关系并忽略关键模式。尽管表格表示学习方法可以解决其中一些局限性，但现有的努力仍然面临着稀疏的高心态领域，精确的数值推理和圆柱重的表。此外，通过基于语言的界面来利用这些学习的表示为下游任务。在本文中，我们为这些挑战提供了一种创新且可扩展的解决方案。具体而言，我们的方法引入了一种多层分区机制，该机制利用幂律动态来处理大型词汇，一种自适应量化机制将先验施加在数值连续性上，并对核心柱和元信息形式的核心柱具有明显的处理。为了促进在LLM上进行指导调整，我们提出了一个参数有效的解码器，该解码器使用一系列适配器层交织在一起，并使用一系列适配器层，从而利用丰富的交叉任务知识。我们验证解决方案在合成支付交易的大规模数据集中的功效。

### Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference 
[[arxiv](https://arxiv.org/abs/2410.07673)] [[cool](https://papers.cool/arxiv/2410.07673)] [[pdf](https://arxiv.org/pdf/2410.07673)]
> **Authors**: Jianxing Yu,Shiqi Wang,Han Yin,Zhenlong Sun,Ruobing Xie,Bo Zhang,Yanghui Rao
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 使用因果表示推断，通过脱离并联偏差来检测多模式点击诱饵检测
- **领域**: 机器学习,人工智能
- **摘要**: 本文着重于检测网络上的点击诱饵帖子。这些帖子经常在混合方式中使用引人注目的虚假信息来误导用户以单击以获利。这会影响用户体验，因此将被内容提供商阻止。为了逃避探测，恶意创造者使用技巧将一些无关的非诱饵内容添加到诱饵柱中，使其为愚弄探测器的合法打扮。该内容通常与非诱饵标签有偏见的关系，但是传统探测器倾向于基于简单的同时发生而做出预测，而不是抓住导致恶意行为的固有因素​​。这种虚假的偏见很容易引起错误的判断。为了解决这个问题，我们提出了一种基于因果推断的新词汇方法。我们首先采用多种模式的一组功能来表征帖子。考虑到这些特征通常与未知的偏见混合在一起，然后我们将三种潜在因素从中解散，包括表明固有诱饵意图的不变因素；在某种情况下反映欺骗性模式的因果因素和非因果噪声。通过消除引起偏见的噪声，我们可以使用不变和因果因素来构建具有良好概括能力的强大模型。三个流行数据集的实验显示了我们方法的有效性。

### CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features 
[[arxiv](https://arxiv.org/abs/2410.07610)] [[cool](https://papers.cool/arxiv/2410.07610)] [[pdf](https://arxiv.org/pdf/2410.07610)]
> **Authors**: Po-han Li,Sandeep P. Chinchali,Ufuk Topcu
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: ef:Published at ICLR 2025
- **标题**: CSA：单峰特征到多模式特征的数据有效映射
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,信息检索
- **摘要**: 多模式编码器之类的诸如零摄像分类和跨模式检索之类的任务中的clip excel。但是，它们需要过多的培训数据。我们提出了规范相似性分析（CSA），该分析使用两个单峰编码器使用有限的数据来复制多模式编码器。 CSA使用新的相似性分数将单峰特征映射到多模式空间中，仅保留多模式信息。 CSA仅涉及单峰编码器和立方复合矩阵分解的推论，从而消除了对广泛基于GPU的模型训练的需求。实验表明，CSA的表现优于剪辑，同时需要$ 50,000 \ times $ $更少的多模式数据对，以弥合给定对Imagenet分类和错误信息新闻字幕检测的预先训练的单峰编码器的模态。 CSA超过了最先进的方法，可以将单峰特征映射到多模式特征。我们还展示了CSA具有图像和文本超出图像和文本的模式的能力，为将来的模态配对铺平了道路，并配对了配对的多模式数据，但丰富的未配对单峰数据（例如LiDAR和文本）。

### Physics and Deep Learning in Computational Wave Imaging 
[[arxiv](https://arxiv.org/abs/2410.08329)] [[cool](https://papers.cool/arxiv/2410.08329)] [[pdf](https://arxiv.org/pdf/2410.08329)]
> **Authors**: Youzuo Lin,Shihang Feng,James Theiler,Yinpeng Chen,Umberto Villa,Jing Rao,John Greenhall,Cristian Pantea,Mark A. Anastasio,Brendt Wohlberg
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: 29 pages, 11 figures
- **标题**: 计算波成像中的物理学和深度学习
- **领域**: 机器学习,信号处理
- **摘要**: 计算波成像（CWI）通过分析横穿该体积的波信号来提取一系列材料的隐藏结构和物理性能。应用包括对地球地下的地震探索，材料科学中的声学成像和非破坏性测试以及医学中超声计算机断层扫描。当前解决CWI问题的方法可以分为两类：植根于传统物理学的方法，以及基于深度学习的方法。基于物理学的方法是其提供高分辨率和定量准确估计培养基中声学特性的能力。但是，它们可以在计算上是密集型的，并且容易受到CWI问题的典型性和非洞穴性的影响。基于机器学习的计算方法最近出现了，提供了不同的观点来应对这些挑战。多元化的科学社区已独立地追求CWI深度学习的融合。这篇评论研究了当代科学的机器学习（ML）技术以及尤其是深层神经网络如何被利用以解决CWI问题。我们提出了一个结构化框架，该框架巩固了跨越多个领域的现有研究，包括计算成像，波浪物理和数据科学。这项研究以从现有的基于ML的方法中学到的重要经验教训结束，并通过对有关该主题的广泛文献进行系统分析来确定技术障碍和新兴趋势。

### Online Multi-modal Root Cause Analysis 
[[arxiv](https://arxiv.org/abs/2410.10021)] [[cool](https://papers.cool/arxiv/2410.10021)] [[pdf](https://arxiv.org/pdf/2410.10021)]
> **Authors**: Lecheng Zheng,Zhengzhang Chen,Haifeng Chen,Jingrui He
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 在线多模式根本原因分析
- **领域**: 机器学习,人工智能
- **摘要**: 根本原因分析（RCA）对于查明微服务系统中故障的根本原因至关重要。传统数据驱动的RCA方法通常仅限于离线应用程序，这是由于高计算需求，现有的在线RCA方法仅处理单模式数据，从而忽略了多模式系统中的复杂交互。在本文中，我们引入了海洋，这是一种新型的在线多模式因果结构学习方法，用于根源原因。海洋采用扩张的卷积神经网络来捕获长期的时间依赖性和图形神经网络，以学习系统实体之间的因果关系和关键绩效指标。我们进一步设计了一种多因素注意机制，以分析和重新评估不同指标和日志指标/属性之间的关系，以增强在线因果图学习。此外，开发了基于对比度信息最大化的图形融合模块，以有效地模拟各种模式之间的关系。在三个现实世界数据集上进行的广泛实验证明了我们提出的方法的有效性和效率。

### Multimodal Physical Activity Forecasting in Free-Living Clinical Settings: Hunting Opportunities for Just-in-Time Interventions 
[[arxiv](https://arxiv.org/abs/2410.09643)] [[cool](https://papers.cool/arxiv/2410.09643)] [[pdf](https://arxiv.org/pdf/2410.09643)]
> **Authors**: Abdullah Mamun,Krista S. Leonard,Megan E. Petrov,Matthew P. Buman,Hassan Ghasemzadeh
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: 9 pages, 5 figures
- **标题**: 自由生活临床环境中预测的多模式体育锻炼：即时干预的狩猎机会
- **领域**: 机器学习,人工智能,信号处理
- **摘要**: 目的：这项研究旨在开发一种称为Movesense的生活方式干预系统，该系统预测患者的活动行为，以允许在现实世界中的临床环境中进行早期和个性化的干预措施。方法：我们进行了两项临床研究，涉及58名前糖尿病前退伍军人和60例阻塞性睡眠呼吸暂停患者，以使用可穿戴设备收集多模式行为数据。我们开发了多模式长的短期内存（LSTM）网络模型，该模型能够通过检查活动和参与方式的数据提前24小时预测患者的步骤数量。此外，我们设计了基于目标的预测模型，以预测一个人的第二天步骤是否会超过一定的阈值。结果：具有早期融合的多模式LSTM比线性回归和ARIMA在糖尿病前数据集中分别达到33％和37％的绝对误差。 LSTM在睡眠数据集上还优于线性回归和Arima，边距为13％和32％。多模式的预测模型在糖尿病前数据集和睡眠数据集上的精度还具有72％和79％的精度，分别在基于目标的预测上。结论：我们的实验得出的结论是，具有早期融合的多模式LSTM模型比具有晚期融合和单峰LSTM模型的多模式LSTM更好，也比Arima和Arima和线性回归模型。意义：我们解决了在不受控制的环境中预测的时间序列的一项重要而具有挑战性的任务。对一个人的体育锻炼的有效预测可以帮助设计适应性行为干预措施，以保持用户的参与并遵守规定的常规。

### On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning 
[[arxiv](https://arxiv.org/abs/2410.09156)] [[cool](https://papers.cool/arxiv/2410.09156)] [[pdf](https://arxiv.org/pdf/2410.09156)]
> **Authors**: Bokun Wang,Yunwen Lei,Yiming Ying,Tianbao Yang
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: To appear in ICLR 2025
- **标题**: 关于自我监督表示学习的歧视性概率建模
- **领域**: 机器学习,机器学习
- **摘要**: 我们研究了（多模式）自我监督表示学习的数据预测任务的连续域上的区分概率建模。为了应对每个锚点数据计算分区函数的积分的挑战，我们利用可靠的蒙特卡洛集成的多重重要性采样（MIS）技术，该技术可以恢复基于Infonce的对比损失作为特殊情况。在这个概率的建模框架内，我们进行了概括误差分析，以揭示当前基于Infonce的对比损失的限制，用于自我监督的表示学习，并通过减少蒙特卡洛整合的误差来开发更好的方法。为此，我们提出了一种新型的非参数方法，用于近似通过凸优化MIS所需的条件概率密度的总和，从而产生了一个新的对比目标，以进行自我监督的表示。此外，我们设计了一种有效的算法来解决提出的目标。我们从经验上将我们的算法与代表性基准进行了对比度图像训练预处理任务的比较。 CC3M和CC12M数据集的实验结果证明了我们算法的总体性能。我们的代码可在https://github.com/bokun-wang/nuclr上找到。

### When Graph meets Multimodal: Benchmarking and Meditating on Multimodal Attributed Graphs Learning 
[[arxiv](https://arxiv.org/abs/2410.09132)] [[cool](https://papers.cool/arxiv/2410.09132)] [[pdf](https://arxiv.org/pdf/2410.09132)]
> **Authors**: Hao Yan,Chaozhuo Li,Jun Yin,Zhigang Yu,Weihao Han,Mingzheng Li,Zhengxin Zeng,Hao Sun,Senzhang Wang
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 当图形符合多模式时：在多模式归因图上进行基准测试和冥想
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 多模式属性图（MAGS）在现实世界应用中无处不在，包括通过附加到节点（例如，文本和图像）的多模式属性（例如，文本和图像）和代表节点交互的拓扑结构来涵盖广泛的知识。尽管由于缺乏标准化的数据集和评估框架，MAG代表学习（MAGRL）有潜力推进社交网络和电子商务等各种研究领域，但仍未得到充实。在本文中，我们首先提出了MAGB，这是一个综合的MAG基准数据集，其中包含来自各个域中具有文本和视觉属性的策划图。 Based on MAGB dataset, we further systematically evaluate two mainstream MAGRL paradigms: $\textit{GNN-as-Predictor}$, which integrates multimodal attributes via Graph Neural Networks (GNNs), and $\textit{VLM-as-Predictor}$, which harnesses Vision Language Models (VLMs) for zero-shot reasoning. MAGB上的广泛实验揭示了以下关键见解：$ \ textit {（i）} $模态意义随着特定领域特征的巨大波动。 $ \ textit {（ii）} $多模式嵌入可以提升GNN的性能上限。但是，模式之间的内在偏见可能会阻碍有效的训练，尤其是在低数据局情况下。 $ \ textIt {（iii）} $ vlms在生成多模式嵌入方面非常有效，以减轻文本属性和视觉属性之间的不平衡。这些发现阐明了多模式属性和图形拓扑之间的协同作用，促成了可靠的基准，为将来的MAG研究铺平了道路。 MAGB数据集和评估管道可在https://github.com/sktsherlock/magb上公开获得。

### Federated Learning in Practice: Reflections and Projections 
[[arxiv](https://arxiv.org/abs/2410.08892)] [[cool](https://papers.cool/arxiv/2410.08892)] [[pdf](https://arxiv.org/pdf/2410.08892)]
> **Authors**: Katharine Daly,Hubert Eichner,Peter Kairouz,H. Brendan McMahan,Daniel Ramage,Zheng Xu
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: Published at 2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)
- **标题**: 实践中的联合学习：反思和预测
- **领域**: 机器学习,人工智能,密码学和安全
- **摘要**: 联合学习（FL）是一种机器学习技术，使多个实体能够在不交换本地数据的情况下协作学习共享模型。在过去的十年中，FL系统取得了长足的进步，扩展到各个学习领域的数百万个设备，同时提供有意义的差异隐私（DP）保证。来自Google，Apple和Meta等组织的生产系统展示了FL的现实适用性。但是，仍然存在关键挑战，包括验证服务器端DP保证和跨异构设备的培训协调培训，从而限制了更广泛的采用。此外，新兴趋势，例如大型（多模式）模型和训练，推理和个性化之间的模糊界限挑战传统的FL框架。作为回应，我们提出了一个重新定义的FL框架，该框架优先考虑隐私原则而不是严格的定义。我们还利用可信赖的执行环境和开源生态系统来应对这些挑战并促进FL中的未来进步，从而绘制了前进道路。

### Data-Aware Training Quality Monitoring and Certification for Reliable Deep Learning 
[[arxiv](https://arxiv.org/abs/2410.10984)] [[cool](https://papers.cool/arxiv/2410.10984)] [[pdf](https://arxiv.org/pdf/2410.10984)]
> **Authors**: Farhang Yeganegi,Arian Eamaz,Mojtaba Soltanalian
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 数据感知培训质量监控和可靠深度学习认证
- **领域**: 机器学习
- **摘要**: 深度学习模型在通过线性和非线性转换的连续层捕获复杂表示方面表现出色，但是它们固有的黑盒本质和多模式训练景观引起了人们对可靠性，稳健性和安全性的关键关注，尤其是在高赌注应用中。为了应对这些挑战，我们介绍了是训练界，这是一个新颖的框架，用于实时，数据感知认证和对神经网络培训的监视。是的界限评估了数据利用率和优化动力学的效率，提供了一种有效的工具来评估进度并检测训练期间的次优行为。我们的实验表明，是的界限还提供了传统的本地优化观点以外的见解，例如识别何时在次优的地区训练损失高原。在合成和真实数据（包括图像降解任务）上验证，该界限被证明有效地证明了培训质量和指导调整以增强模型性能。通过将这些界限集成到基于颜色的云的监视系统中，我们为实时评估提供了强大的工具，为深度学习的培训质量保证设定了新的标准。

### ATLAS: Adapter-Based Multi-Modal Continual Learning with a Two-Stage Learning Strategy 
[[arxiv](https://arxiv.org/abs/2410.10923)] [[cool](https://papers.cool/arxiv/2410.10923)] [[pdf](https://arxiv.org/pdf/2410.10923)]
> **Authors**: Hong Li,Zhiquan Tan,Xingyu Li,Weiran Huang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: Atlas：基于适配器的多模式持续学习，具有两阶段的学习策略
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 尽管视觉和语言模型在许多领域都显着发展，但持续学习的挑战尚未解决。诸如适配器和提示之类的参数效率模块提出了一种减轻灾难性遗忘的有希望的方法。但是，现有作品通常会为每个任务学习单个适配器，这可能会导致适配器之间的多余知识。此外，他们继续使用原始的预训练模型来初始化下游模型，从而导致与原始模型相比，该模型的概括可以忽略不计。此外，仍然缺乏研究研究单模式和多模式任务的多模式模型及其对下游任务的后续影响的后果。在本文中，我们提出了一个基于适配器的两阶段学习范式，这是一种由基于经验的学习和新知识扩展组成的多模式持续学习方案，该方案有助于该模型充分利用经验知识并补偿新知识。广泛的实验表明，我们的方法精通持续学习。它扩大了上游表示的分布，同时还最大程度地减少了忘记先前任务的负面影响。此外，它增强了下游任务的概括能力。此外，我们将多模式和单模式任务纳入上游持续学习。我们观察到，从上游任务中学习可以帮助完成下游任务。我们的代码将提供：https：//github.com/lihong2303/atlas。

### Adapt-$\infty$: Scalable Lifelong Multimodal Instruction Tuning via Dynamic Data Selection 
[[arxiv](https://arxiv.org/abs/2410.10636)] [[cool](https://papers.cool/arxiv/2410.10636)] [[pdf](https://arxiv.org/pdf/2410.10636)]
> **Authors**: Adyasha Maharana,Jaehong Yoon,Tianlong Chen,Mohit Bansal
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: First two authors contributed equally. Code: https://github.com/adymaharana/adapt-inf
- **标题**: 适应 -  $ \ infty $：可扩展的终身多模式指令通过动态数据选择调谐
- **领域**: 机器学习,人工智能
- **摘要**: 来自各个分销商的视觉指令数据集在不同的时间发布，并且通常包含大量语义上的冗余文本图像对，具体取决于其任务组成（即技能）或参考来源。这种冗余极大地限制了终身适应性多模式模型的有效部署，阻碍了他们提高现有技能并随着时间的推移获得新能力的能力。为了解决这个问题，我们通过数据选择重新构架了终身指令调整（LIIT）的问题，该模型会根据模型中获得的知识的当前状态选择有益的样本以从较早的和新数据集中学习。基于经验分析表明，对于具有不断发展的分布的多任务数据集选择最佳数据子集通常是无效的，我们提出了Adapt-$ \ infty $，一种新的多路和自适应数据选择方法，一种动态平衡了LIET期间样本效率和有效性。我们通过对基于梯度的样品向量进行分组来构建伪技能群。接下来，我们从选择器专家池中为每个技能群集选择表现最佳的数据选择器，包括我们新提出的评分功能，图像接地得分。这些数据选择器从每个技能集群中为训练中采样了最重要的样本的子集。为了防止在LIIT期间的数据集池的尺寸持续增加，这将导致过度计算，我们进一步引入了群集的永久性数据修剪策略，以从每个群集中删除最多的语义冗余样本，从而使计算需求可管理。用Adapt-$ \ infty $减轻灾难性遗忘的样品进行培训，尤其是对于稀有任务，并仅使用原始数据集的一小部分来促进在连续体中的远期转移。

### Hard-Constrained Neural Networks with Universal Approximation Guarantees 
[[arxiv](https://arxiv.org/abs/2410.10807)] [[cool](https://papers.cool/arxiv/2410.10807)] [[pdf](https://arxiv.org/pdf/2410.10807)]
> **Authors**: Youngjae Min,Anoopkumar Sonar,Navid Azizan
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 具有通用近似保证的严格受限的神经网络
- **领域**: 机器学习,人工智能,机器学习
- **摘要**: 将投入输出关系的先验知识或规格纳入机器学习模型已引起了很大的关注，因为它从有限的数据中增强了概括，并导致输出符合输出。但是，大多数现有方法通过通过正规化来惩罚违规行为来使用软限制，这不能保证限制满意度，这是对安全至关重要的应用的基本要求。另一方面，对神经网络施加严格的限制可能会阻碍其代表权，从而对性能产生不利影响。为了解决这个问题，我们提出了HardNet，这是一个实用的框架，用于构建固有满足硬约束的神经网络而无需牺牲模型能力。具体而言，我们通过将可区分的投影层附加到网络的输出中来编码基于输入和输出的仿射和凸硬约束。该体系结构允许使用标准算法对网络参数进行不受约束的优化，同时确保构造的约束满意度。此外，我们表明硬核保留神经网络的通用近似功能。我们证明了硬核在各种应用程序中的多功能性和有效性：在约束下的拟合功能，学习优化求解器，优化安全至关重要系统中的控制策略以及对飞机系统的安全决策逻辑。

### Enhancing Unimodal Latent Representations in Multimodal VAEs through Iterative Amortized Inference 
[[arxiv](https://arxiv.org/abs/2410.11403)] [[cool](https://papers.cool/arxiv/2410.11403)] [[pdf](https://arxiv.org/pdf/2410.11403)]
> **Authors**: Yuta Oshima,Masahiro Suzuki,Yutaka Matsuo
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: 22 pages, 12 figures
- **标题**: 通过迭代摊销推断，增强多模式VAE中的单峰潜在表示
- **领域**: 机器学习,人工智能
- **摘要**: 多模式变异自动编码器（VAE）旨在通过整合来自不同数据模式的信息来捕获共享的潜在表示。一个重大挑战是从任何模式的任何子集准确地推断出代表性，而无需训练所有可能的模态组合的推理网络的不切实际数（2^m）。基于混合物的模型仅需要与模式一样多的推理模型来简化这一点，从而汇总了单峰推断。但是，当缺失方式时，它们会遭受信息损失。基于对齐的VAE通过将单峰推理模型与多模型模型对准通过最小化Kullback-Leibler（KL）差异来解决这一问题，但由于摊销空白差异而面临问题，这会损害推理准确性。为了解决这些问题，我们引入了多模式迭代摊销推断，这是多模式VAE框架内的迭代精炼机制。该方法克服了缺少模态的信息丢失，并通过使用所有可用方式迭代精炼多模式推断来最大程度地减少摊销差距。通过将单峰推断与这种精致的多模式后部保持一致，我们实现了单峰的推论，这些推论有效地包含了多模式信息，而在推理过程中仅需要单峰输入。基准数据集的实验表明，我们的方法可以提高推理性能，这是通过较高的线性分类精度和竞争性余弦相似性证明的，并增强了较低的FID分数指示的交叉模式产生。这表明我们的方法增强了单型输入的推断表示。

### TCP-Diffusion: A Multi-modal Diffusion Model for Global Tropical Cyclone Precipitation Forecasting with Change Awareness 
[[arxiv](https://arxiv.org/abs/2410.13175)] [[cool](https://papers.cool/arxiv/2410.13175)] [[pdf](https://arxiv.org/pdf/2410.13175)]
> **Authors**: Cheng Huang,Pan Mu,Cong Bai,Peter AG Watson
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: TCP扩散：全球热带气旋降水预测随着变化意识的预测的多模式扩散模型
- **领域**: 机器学习,人工智能,大气和海洋物理
- **摘要**: 来自热带气旋（TC）的降水会导致洪水，泥石流和滑坡等灾难。提前预测这种降水至关重要，使人们有时间准备和防御这些降水引起的灾难。开发深度学习（DL）降雨预测方法提供了一种预测潜在灾难的新方法。但是，一个问题是，大多数现有方法都遇到了累积错误并且缺乏身体一致性。其次，这些方法忽略了气象因素在TC降雨中的重要性及其与数值天气预测（NWP）模型的整合。因此，我们提出了热带气旋降水扩散（TCP扩散），这是一种用于全局热带气旋降水预测的多模式模型。它根据过去的降雨观测和多模式环境变量，预测接下来的12小时TC中心周围的TC降雨。相邻的残留预测（ARP）将训练目标从绝对降雨量的价值变为降雨趋势，并使我们的模型具有降雨变化意识的能力，降低了累积错误并确保身体一致性。考虑到与TC相关的气象因素的影响以及NWP模型预测的有用信息，我们提出了一个具有专门编码器的多模型框架，以从环境变量和NWP模型提供的结果中提取更丰富的信息。广泛的实验结果表明，我们的方法优于其他DL方法和来自欧洲中范围天气预测中心（ECMWF）的NWP方法。

### MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models 
[[arxiv](https://arxiv.org/abs/2410.13085)] [[cool](https://papers.cool/arxiv/2410.13085)] [[pdf](https://arxiv.org/pdf/2410.13085)]
> **Authors**: Peng Xia,Kangyu Zhu,Haoran Li,Tianze Wang,Weijia Shi,Sheng Wang,Linjun Zhang,James Zou,Huaxiu Yao
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: ICLR 2025
- **标题**: MMED-rag：用于医学视觉语言模型的多功能多模式抹布系统
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 人工智能（AI）在医疗保健中表现出了巨大的潜力，尤其是在疾病诊断和治疗计划方面。医学大型视觉模型（MED-LVLM）的最新进展为交互式诊断工具开辟了新的可能性。但是，这些模型通常会遭受事实幻觉的困扰，这可能会导致诊断不正确。微调和检索型发电（RAG）已成为解决这些问题的方法。但是，培训数据和部署数据之间的高质量数据和分布的数量限制了微调方法的应用。尽管抹布轻巧有效，但现有的基于抹布的方法不足以通用不同的医疗领域，并且可能在模式之间以及模型和地面真相之间引起不一致的问题。在本文中，我们提出了一个多功能的多模式抹布系统MMED-rag，旨在增强MED-LVLM的事实。我们的方法介绍了一种域感知的检索机制，一种自适应检索上下文选择方法以及可证明的基于抹布的偏好微调策略。这些创新使抹布过程充分通用和可靠，在引入检索到的环境时会大大改善对齐。五个医学数据集（涉及放射学，眼科，病理学）和报告生成的实验结果表明，MMED-rag可以在MED-LVLMS的事实准确性方面平均提高43.8％。我们的数据和代码可在https://github.com/richard-peng-xia/mmed-rag中找到。

### Hiding-in-Plain-Sight (HiPS) Attack on CLIP for Targetted Object Removal from Images 
[[arxiv](https://arxiv.org/abs/2410.13010)] [[cool](https://papers.cool/arxiv/2410.13010)] [[pdf](https://arxiv.org/pdf/2410.13010)]
> **Authors**: Arka Daw,Megan Hong-Thanh Chung,Maria Mahbub,Amir Sadovnik
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: Published in the 3rd Workshop on New Frontiers in Adversarial Machine Learning at NeurIPS 2024. 10 pages, 7 figures, 3 tables
- **标题**: 隐藏在剪辑上的隐藏式（臀部）攻击，以从图像中删除目标对象
- **领域**: 机器学习,人工智能,密码学和安全,计算机视觉和模式识别
- **摘要**: 已知机器学习模型容易受到对抗性攻击的影响，但是传统攻击主要集中在单模式上。随着大型多模型模型（LMM）的兴起，例如剪辑，结合了视觉和语言功能，出现了新的漏洞。但是，多模式目标攻击中的先前工作旨在将模型的输出完全更改为对手想要的。在许多现实的情况下，对手可能会寻求对输出进行微妙的修改，以使变化不被下游模型甚至人类注意到。我们介绍了隐藏式 - 视线（HIPS）攻击，这是一种新颖的对抗性攻击，通过选择性隐藏目标对象来巧妙地修改模型预测，就好像场景中没有目标对象一样。我们提出了两个臀部攻击变体，臀部-CL和臀部帽，并证明了它们在转移到下游图像字幕模型（例如夹子）中的有效性，例如夹具，以从图像字幕中删除目标对象。

### Multi-modal graph neural networks for localized off-grid weather forecasting 
[[arxiv](https://arxiv.org/abs/2410.12938)] [[cool](https://papers.cool/arxiv/2410.12938)] [[pdf](https://arxiv.org/pdf/2410.12938)]
> **Authors**: Qidong Yang,Jonathan Giezendanner,Daniel Salles Civitarese,Johannes Jakubik,Eric Schmitt,Anirban Chandra,Jeremy Vila,Detlef Hohl,Chris Hill,Campbell Watson,Sherrie Wang
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 用于局部离网天气预测的多模式图神经网络
- **领域**: 机器学习,大气和海洋物理
- **摘要**: 诸如野火管理和可再生能源产生之类的紧急应用需要在地球表面附近进行精确的局部天气预报。但是，目前在全球常规网格上生成了机器学习或数值天气模型的天气预报产品，幼稚的插值无法准确反映靠近地面的细颗粒天气模式。在这项工作中，我们端到端训练异类图神经网络（GNN），以降低了感兴趣的离网站的偏低刻板预测。这种多模式的GNN利用了当地的历史天气观测（例如，风，温度）来纠正在不同的交货时期朝着局部准确预测的不同的天气预报。每个数据模式都被建模为图中不同类型的节点。使用消息传递，预测位置的节点从其异质邻居节点汇总了信息。使用整个美国东北部的气象站的实验表明，我们的模型的表现优于一系列数据驱动和非数据驱动的外部预测方法。我们的方法表明，如何桥接全球大型天气模型和本地准确预测之间的差距如何为局部决策提供信息。

### Context Matters: Leveraging Contextual Features for Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2410.12672)] [[cool](https://papers.cool/arxiv/2410.12672)] [[pdf](https://arxiv.org/pdf/2410.12672)]
> **Authors**: Sameep Chattopadhyay,Pulkit Paliwal,Sai Shankar Narasimhan,Shubhankar Agarwal,Sandeep P. Chinchali
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 上下文问题：利用上下文特征的时间序列预测
- **领域**: 机器学习,人工智能
- **摘要**: 时间序列预测除了相应的历史外，通常还受外源上下文特征的影响。例如，在财务环境中，很难在不考虑新闻文章，推文等形式的公众情绪和政策决策的情况下准确预测股票价格。尽管这是常识，但由于其异质性和多态性，这是常识，但目前的最新预测模型未能包含此类上下文信息。为了解决这个问题，我们介绍了一种新颖的插件方法上下文Former，以将多模式上下文信息整合到现有的预训练的预测模型中。上下文形式有效地提取了从丰富的多模式上下文（包括分类，连续，时间变化甚至文本信息）的预测信息，以显着提高现有基础预测者的性能。在跨越能源，交通，环境和金融领域的一系列现实数据集上，上下文形式的预测模型高达30％。

### Training Neural Samplers with Reverse Diffusive KL Divergence 
[[arxiv](https://arxiv.org/abs/2410.12456)] [[cool](https://papers.cool/arxiv/2410.12456)] [[pdf](https://arxiv.org/pdf/2410.12456)]
> **Authors**: Jiajun He,Wenlin Chen,Mingtian Zhang,David Barber,José Miguel Hernández-Lobato
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: Accepted for publication at AISTATS 2025
- **标题**: 训练具有反向扩散KL差异的神经抽样器
- **领域**: 机器学习,机器学习
- **摘要**: 训练生成模型以从非构型密度功能中采样是机器学习中的重要且具有挑战性的任务。传统的训练方法通常依赖于反向Kullback-Leibler（KL）差异，这是由于其易干性。但是，反向KL的模式寻求行为阻碍了多模式目标分布的有效近似。为了解决这个问题，我们建议将沿模型和靶密度的扩散轨迹的反向KL最小化。我们将此目标称为反向扩散的KL差异，该差异允许模型捕获多个模式。利用这一目标，我们训练可以在一个步骤中从目标分布中有效生成样品的神经采样器。我们证明我们的方法可以增强各种玻尔兹曼分布的采样性能，包括合成多模式密度和N体粒子系统。

### Devil in the Tail: A Multi-Modal Framework for Drug-Drug Interaction Prediction in Long Tail Distinction 
[[arxiv](https://arxiv.org/abs/2410.12249)] [[cool](https://papers.cool/arxiv/2410.12249)] [[pdf](https://arxiv.org/pdf/2410.12249)]
> **Authors**: Liangwei Nathan Zheng,Chang George Dong,Wei Emma Zhang,Xin Chen,Lin Yue,Weitong Chen
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 尾巴中的魔鬼：用于长尾部区分的药物相互作用预测的多模式框架
- **领域**: 机器学习
- **摘要**: 药物相互作用（DDI）识别是药理学研究的关键方面。有许多DDI类型（数百种），并且没有均匀的机会均匀分布。一些很少发生的DDI类型通常是高风险，如果被忽视，可能是至关重要的，体现了长尾分配问题。现有模型反对这种分配挑战，并忽略了DDI预测中药物的多方面性质。在本文中，引入了一种新型的多模式深度学习框架，即TFDM，以利用药物的多种特性来实现DDI分类。提出的框架融合了药物的多模式特征，包括基于图的，分子结构，靶和酶，用于DDI鉴定。为了应对分布偏度跨类别带来的挑战，引入了一种新颖的损失功能，称为尾部局灶性损失，旨在进一步增强模型性能并解决极度长尾数据集中局部损失的消失问题。超过4个挑战性长尾数据集的密集实验表明，TFMD在长尾DDI分类任务中的最新SOTA方法优于最新的SOTA方法。发布源代码是为了复制我们的实验结果：https：//github.com/icuraslw/tfmd_longtailed_ddi.git

### Syn2Real Domain Generalization for Underwater Mine-like Object Detection Using Side-Scan Sonar 
[[arxiv](https://arxiv.org/abs/2410.12953)] [[cool](https://papers.cool/arxiv/2410.12953)] [[pdf](https://arxiv.org/pdf/2410.12953)]
> **Authors**: Aayush Agrawal,Aniruddh Sikdar,Rajini Makam,Suresh Sundaram,Suresh Kumar Besai,Mahesh Gopi
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: 7 pages, 4 figures and 3 tables
- **标题**: 使用侧扫声纳
- **领域**: 机器学习,计算机视觉和模式识别,图像和视频处理
- **摘要**: 由于现实世界中的数据缺乏，深入学习的水下矿山检测受到了局限性。这种稀缺性导致过度拟合，模型在训练数据上表现良好，但在看不见的数据上表现不佳。本文提出了使用扩散模型来应对这一挑战的Syn2real（合成至真实）域的概括方法。我们证明，DDPM和DDIM模型生成的合成数据，即使不是完全现实，也可以有效地增加现实世界中的样本进行训练。最终采样图像中的残留噪声提高了模型具有固有噪声和高变化的现实数据的能力。与仅根据原始培训数据进行培训相比，接受合成和原始培训数据集的组合进行培训时，基线面膜RCNN模型的平均精度（AP）大约增加了60％。这种重大的改进凸显了Syn2Real域概括在水下矿山检测任务中的潜力。

### Discovering Leitmotifs in Multidimensional Time Series 
[[arxiv](https://arxiv.org/abs/2410.12293)] [[cool](https://papers.cool/arxiv/2410.12293)] [[pdf](https://arxiv.org/pdf/2410.12293)]
> **Authors**: Patrick Schäfer,Ulf Leser
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 在多维时间序列中发现leitmotifs
- **领域**: 机器学习
- **摘要**: leitmotif是文学，电影或音乐中的一个反复出现的主题，它对它所包含的作品具有象征意义。当该作品可以表示为多维时间序列（MDT）时，例如声学或视觉观察，例如，LeitMotif与模式发现问题相当，这是一个不合格的问题，该序列是一个不合时宜的问题。与单变量情况相比，它具有额外的复杂性，因为模式通常并非在所有维度中发生，而仅在少数几个维度中发生 - 但是，这是未知的，必须通过该方法本身检测到。在本文中，我们介绍了用于MDTS的新颖，高效且高效的LeitMotif Discovery Lama。喇嘛基于两个核心原理：（a）leitmotif仅显示了尚未确定数量尚不清楚的子维度 - 既不太少，也不太多，而且（b）sub-dimensions的集合并不独立于在其中找到的最佳模式，因此必须以一种关节方式解决这两个问题。与大多数以前的方法相反，Lama共同解决这两个问题 - 而不是独立选择尺寸（或leitmotifs）并找到最佳的leitmotifs（或尺寸）。我们对14个不同现实生活数据集的新型基地注释的基准测试的实验评估表明，与四个最先进的基准相比，喇嘛在检测有意义的模式而没有计算复杂性的情况下显示出卓越的性能。

### Latent Weight Diffusion: Generating Policies from Trajectories 
[[arxiv](https://arxiv.org/abs/2410.14040)] [[cool](https://papers.cool/arxiv/2410.14040)] [[pdf](https://arxiv.org/pdf/2410.14040)]
> **Authors**: Shashank Hegde,Gautam Salhotra,Gaurav S. Sukhatme
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 潜在体重扩散：从轨迹产生政策
- **领域**: 机器学习,人工智能,机器人技术
- **摘要**: 随着开源机器人数据的可用性的增加，模仿学习已成为机器人操纵和运动的可行方法。当前，对大型广义策略进行了培训，可以使用扩散模型来预测控制或轨迹，这些模型具有学习多模式作用分布的理想特性。但是，可推广性带有成本 - 即较大的模型尺寸和较慢的推理。此外，在扩散策略（即扩散轨迹）的性能和动作范围之间存在已知的权衡：更少的扩散查询会累积更大的轨迹跟踪误差。因此，在机器人计算约束的前提下，以高推理频率运行这些模型是普遍的实践。为了解决这些局限性，我们提出了潜在的重量扩散（LWD），该方法使用扩散来学习有关机器人任务的策略的分布，而不是轨迹。我们的方法将演示轨迹编码为潜在空间，然后使用超网络将其解码为策略。我们在这个潜在空间中采用扩散的剥离模型来学习其分布。我们证明LWD可以重建生成轨迹数据集的原始策略的行为。 LWD在推理过程中提供了相当小的策略网络的好处，并且需要更少的扩散模型查询。当对Metaworld MT10基准测试时，LWD与香草多任务策略相比获得了更高的成功率，同时在推断过程中使用高达约18倍的模型。此外，由于LWD生成了闭环策略，因此我们表明它在长期动作范围设置中的扩散策略优于扩散策略，并且在推出过程中降低了扩散查询。

### DPLM-2: A Multimodal Diffusion Protein Language Model 
[[arxiv](https://arxiv.org/abs/2410.13782)] [[cool](https://papers.cool/arxiv/2410.13782)] [[pdf](https://arxiv.org/pdf/2410.13782)]
> **Authors**: Xinyou Wang,Zaixiang Zheng,Fei Ye,Dongyu Xue,Shujian Huang,Quanquan Gu
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: DPLM-2：多模式扩散蛋白语言模型
- **领域**: 机器学习,定量方法
- **摘要**: 蛋白质是由其氨基酸序列定义的必不可少的大分子，它们决定了其三维结构，因此它们在所有生物体中的功能。因此，生成蛋白建模需要一种多模式的方法来同时建模，理解和生成序列和结构。但是，现有方法通常对每种模式使用单独的模型，从而限制了它们捕获序列和结构之间复杂关系的能力。这导致在需要共同理解和生成这两种方式的任务中表现出色。在本文中，我们介绍了DPLM-2，这是一种多模式蛋白基础模型，该模型扩展了离散扩散蛋白语言模型（DPLM）以适应序列和结构。为了通过语言模型启用结构学习，使用基于无查找的基于量化的令牌仪将3D坐标转换为离散令牌。通过对实验和高质量合成结构进行训练，DPLM-2了解序列和结构的联合分布及其边际和条件。我们还实施了有效的热身策略，以利用基于预训练的基于序列的蛋白质语言模型的大规模进化数据与结构归纳偏见之间的联系。经验评估表明，DPLM-2可以同时生成高度兼容的氨基酸序列及其相应的3D结构，从而消除了对两阶段生成方法的需求。此外，DPLM-2在各种有条件的生成任务中展示了竞争性能，包括折叠，倒数折叠和使用多模式基序输入以及为预测任务提供结构感知的表示形式。

### Scaling Wearable Foundation Models 
[[arxiv](https://arxiv.org/abs/2410.13638)] [[cool](https://papers.cool/arxiv/2410.13638)] [[pdf](https://arxiv.org/pdf/2410.13638)]
> **Authors**: Girish Narayanswamy,Xin Liu,Kumar Ayush,Yuzhe Yang,Xuhai Xu,Shun Liao,Jake Garrison,Shyam Tailor,Jake Sunshine,Yun Liu,Tim Althoff,Shrikanth Narayanan,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Samy Abdel-Ghaffar,Daniel McDuff
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 缩放可穿戴的粉底型
- **领域**: 机器学习,人工智能,人机交互
- **摘要**: 由于各种健康跟踪功能，可穿戴传感器已变得无处不在。日常生活的连续和纵向测量结果产生了大量数据。但是，了解这些观察的科学和可行见解是不平凡的。受生成建模的经验成功的启发，大型神经网络从大量文本，图像，视频或音频数据中学习强大的表示，我们研究了跨计算，数据和模型大小的传感器基础模型的缩放属性。我们使用16.5万人超过165,000人的每分钟数据的数据集，心率可变性，心率变异性，高速度计，皮肤温度和高度计数据，我们创建了LSM，这是一种基于最大的耐磨性数据集的多模式基础模型，该模型具有最广泛的传感器模态范围。我们的结果为跨时间和传感器方式建立了LSM的缩放定律。此外，我们重点介绍了LSM如何使诸如锻炼和活动识别之类的任务启用样本高效的下游学习。

### PORTAL: Scalable Tabular Foundation Models via Content-Specific Tokenization 
[[arxiv](https://arxiv.org/abs/2410.13516)] [[cool](https://papers.cool/arxiv/2410.13516)] [[pdf](https://arxiv.org/pdf/2410.13516)]
> **Authors**: Marco Spinaci,Marek Polewczyk,Johannes Hoffart,Markus C. Kohler,Sam Thelin,Tassilo Klein
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: Accepted at Table Representation Learning Workshop at NeurIPS 2024
- **标题**: 门户：可扩展的表格基础模型通过特定于内容的令牌化
- **领域**: 机器学习
- **摘要**: 对表格数据的自我监督学习旨在将自然语言和图像领域的进步应用于表的各个领域。但是，当前的技术通常在整合多域数据并需要数据清洁或特定结构要求方面遇到困难，从而限制了培训数据集的可扩展性。我们介绍门户网站（所有表的一行预处理），该框架可以处理各种数据模式，而无需清洁或预处理。这种简单而强大的方法可以有效地在在线收集的数据集上进行预训练，并进行微调，以匹配有关复杂分类和回归任务的最新方法。这项工作为大规模表格数据提供了自我监督学习的实际进步。

### scFusionTTT: Single-cell transcriptomics and proteomics fusion with Test-Time Training layers 
[[arxiv](https://arxiv.org/abs/2410.13257)] [[cool](https://papers.cool/arxiv/2410.13257)] [[pdf](https://arxiv.org/pdf/2410.13257)]
> **Authors**: Dian Meng,Bohao Xing,Xinlei Huang,Yanran Liu,Yijun Zhou,Yongjun xiao,Zitong Yu,Xubin Zheng
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: ScfusionTTT：单细胞转录组学和蛋白质组学与测试时间训练层的融合
- **领域**: 机器学习,人工智能
- **摘要**: 单细胞多矩（SCMULTI-OMIC）是指通过测序（Cite-Seq）的成对多模式数据，例如转录组和表位的细胞索引，其中每个细胞的调节是从不同模态（即基因和蛋白质）中测量的。 Scmulti-omics可以揭示肿瘤内部的异质性，并了解各种细胞类型的不同遗传特性，这对于靶向治疗至关重要。当前，基于生物信息学领域的注意力结构的深度学习方法面临两个挑战。第一个挑战是单个细胞中的大量基因。传统的基于注意力的模块由于其长期文本学习和高复杂计算的能力有限而努力利用所有基因信息。第二个挑战是，人类基因组中的基因被排序并影响彼此的表达。大多数方法忽略了此顺序信息。最近引入的测试时间训练（TTT）层是一种新型的序列建模方法，尤其适合处理基因组学数据等长上下文，因为TTT层是线性复杂性序列建模结构，并且更适合具有顺序关系的数据。在本文中，我们提出了ScfusionTTT，这是一种与基于TTT的蒙版自动编码器单细胞多模式融合的新方法。值得注意的是，我们将人类基因组中基因和蛋白质的顺序信息与TTT层，融合多模式的OMICS结合在一起，并增强单峰法分析。最后，该模型采用了三阶段的训练策略，在四个多模式OMICS数据集和四个单像OMICS数据集中，大多数指标中的性能最佳，证明了我们的模型的出色性能。数据集和代码将在https://github.com/dm0815/scfusionttt上提供。

### MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records through Hierarchical Guided-Topic Modeling 
[[arxiv](https://arxiv.org/abs/2410.13217)] [[cool](https://papers.cool/arxiv/2410.13217)] [[pdf](https://arxiv.org/pdf/2410.13217)]
> **Authors**: Ruohan Wang,Zilong Wang,Ziyang Song,David Buckeridge,Yue Li
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: :J.3
- **标题**: Mixehr-Nest：通过分层引导主题建模在电子健康记录中识别亚表格
- **领域**: 机器学习,人工智能,信息检索,定量方法
- **摘要**: 电子健康记录（EHRS）的自动亚型亚型为具有独特亚组的疾病提供了许多机会，并为患者增强了个性化医学。但是，现有的机器学习算法要么关注特定疾病以更好地解释性，要么产生粗粒度的表型主题而不考虑细微的疾病模式。在这项研究中，我们建议使用多模式EHR数据从数千种疾病中推断出一个引导的主题模型Mixehr-nest，以推断出亚体型的主题。具体而言，Mixehr-Nest检测到每个表型主题的多个子主题，其先前是由专家策划的表型概念（例如表型代码（PHECODES）或临床分类软件（CCS）代码的指导。我们在两个EHR数据集上评估了Mixehr-Nest：（1）来自美国波士顿的Beth Israel Deaconess Medical Center（BIDMC）的38,000例患者（ICU）的模拟数据集； （2）医疗保健行政数据库POPHR，包括来自加拿大蒙特利尔的130万患者。实验结果表明，Mixehr-Nest可以识别每个表型中具有不同模式的亚表格，这可以预测疾病进展和严重程度。因此，Mixehr-Nest使用CCS代码来推断亚表格型来区分1型和2型糖尿病，这不会区分这两个亚型概念。此外，Mixehr-Nest不仅提高了ICU患者短期死亡率和糖尿病患者初始胰岛素治疗的预测准确性，而且还揭示了亚表现型的贡献。为了进行纵向分析，MixeHR-Nest在相同的表型下（例如哮喘，白血病，癫痫和抑郁症）鉴定了不同年龄患病率的亚表征。 Mixehr-nest软件可在GitHub上获得：https：//github.com/li-lab-mcgill/mixehr-nest。

### Bayesian Concept Bottleneck Models with LLM Priors 
[[arxiv](https://arxiv.org/abs/2410.15555)] [[cool](https://papers.cool/arxiv/2410.15555)] [[pdf](https://arxiv.org/pdf/2410.15555)]
> **Authors**: Jean Feng,Avni Kothari,Luke Zier,Chandan Singh,Yan Shuo Tan
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 带有LLM先验的贝叶斯概念瓶颈模型
- **领域**: 机器学习,人工智能,机器学习
- **摘要**: 概念瓶颈模型（CBM）已被认为是白框和黑盒模型之间的折衷方案，旨在实现可解释性而无需牺牲准确性。 CBMS的标准培训程序是预先定义一组人解剖概念，从训练数据中提取其值，并将稀疏子集确定为透明预测模型的输入。但是，这种方法通常会因列举足够大的概念以包含真正相关的概念与控制获得概念提取成本的概念之间的权衡受到阻碍。这项工作调查了一种避开这些挑战的新方法：BC-LLM迭代地搜索了贝叶斯框架内潜在的无限概念集，其中大语言模型（LLMS）既是概念提取机制又是先验。 BC-LLM广泛适用和多模式。尽管LLMS中的不完美，我们证明BC-LLM可以提供严格的统计推断和不确定性定量。在实验中，它胜过包括黑盒模型在内的比较方法，更快地收敛于相关概念，并远离伪造的相关概念，并且对分布外样品更强大。

### Exploring Curriculum Learning for Vision-Language Tasks: A Study on Small-Scale Multimodal Training 
[[arxiv](https://arxiv.org/abs/2410.15509)] [[cool](https://papers.cool/arxiv/2410.15509)] [[pdf](https://arxiv.org/pdf/2410.15509)]
> **Authors**: Rohan Saha,Abrar Fahim,Alona Fyshe,Alex Murphy
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: CoNLL BabyLM Challenge 2024 camera ready
- **标题**: 探索视力语言任务的课程学习：小规模多模式培训的研究
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 对于专业领域，通常没有大量的数据来训练大型机器学习模型。在这样有限的数据 /计算设置中，存在各种方法，目的是$ \ textit {以较少的} $进行更多的方法，例如从预告片的模型中进行填充，随着数据呈现到模型（课程学习）的调整，并考虑模型类型 /大小的作用。通过考虑使用机器学习系统可以访问13岁孩子（100m单词）的大约相同数量的单词，从$ \ textit {human} $学习中，从$ \ textit {human} $学习也从$ \ textit {human} $学习中汲取灵感的方法。我们研究了3个主要变量在有限的数据制度中的作用，这是Babylm挑战的多模式轨道的一部分。我们对比：（i）课程学习，（ii），预处理（仅文本数据），（iii）模型类型。我们调整这些变量并在两种类型的任务上进行评估：（a）多模式（文本+image）和（b）单模式（仅文本）任务。我们发现，课程学习有益于多模式的评估，而不是非巡边学习模型，尤其是在结合文本预处理时。在仅文本任务上，课程学习似乎有助于具有较小的可训练参数计数的模型。我们建议基于建筑差异和培训设计的可能原因，以了解为什么人们会观察到这种结果。

### IPO: Interpretable Prompt Optimization for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2410.15397)] [[cool](https://papers.cool/arxiv/2410.15397)] [[pdf](https://arxiv.org/pdf/2410.15397)]
> **Authors**: Yingjun Du,Wenfang Sun,Cees G. M. Snoek
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: Accepted by NeurIPS 2024
- **标题**: IPO：视觉模型可解释的及时优化
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 像剪辑这样的预训练的视觉模型非常适合各种下游任务。但是，它们的性能在很大程度上取决于输入文本提示的特异性，这需要熟练的及时模板工程。相反，当前方法以提示优化通过梯度下降来学习提示，其中提示被视为可调参数。但是，这些方法往往会导致在训练期间看到的基本类别过度拟合，并产生不再是人类可以理解的提示。本文介绍了一个简单但可解释的提示优化器（IPO），该优化器（IPO）利用大型语言模型（LLMS）动态生成文本提示。我们介绍了一个迅速的优化提示，不仅指导LLMS创建有效的提示，还可以将其绩效指标存储过去的提示，从而提供丰富的文本信息。此外，我们通过生成图像描述来将大型的多模型模型（LMM）融合到视觉内容的条件，从而增强了文本和视觉方式之间的相互作用。这允许创建特定于数据集的提示，从而在保持人类理解的同时提高概括性能。在11个数据集中进行的广泛测试表明，IPO不仅提高了现有基于梯度的及早学习方​​法的准确性，而且还可以大大提高生成的提示的可解释性。通过利用LLM的优势，我们的方法确保提示仍然可以理解为人类的理解，从而促进了对视觉模型的更好透明度和监督。

### Multimodal Policies with Physics-informed Representations 
[[arxiv](https://arxiv.org/abs/2410.15250)] [[cool](https://papers.cool/arxiv/2410.15250)] [[pdf](https://arxiv.org/pdf/2410.15250)]
> **Authors**: Haodong Feng,Peiyan Hu,Yue Wang,Dixia Fan
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 具有物理信息表示的多模式政策
- **领域**: 机器学习
- **摘要**: 在PDE系统的控制问题中，观察对于做出决定很重要。但是，由于传感器的局限性和故障，该观察结果通常在实践中稀疏而缺失。上述挑战会导致数量和方式不确定的观察。因此，如何利用不确定的观察结果，因为PDE系统控制问题的州已成为一个科学问题。 PDE系统的动力学依赖于初始条件，边界条件和PDE公式。给定以上三个元素，可以使用PINN来解决PDE系统。在这项工作中，我们发现神经网络还可用于使用PDE丢失和稀疏数据丢失来识别和表示PDE系统。受上述发现的启发，我们提出了一种物理信息表示表示（PIR）算法，以用于PDE系统控制中的多模式策略。它利用PDE损失符合神经网络和根据随机数量和方式计算的观测值计算的数据丢失，以将初始条件和边界条件的信息传播到输入中。输入可以是可学习的参数或编码器的输出。然后，在PDE系统的环境下，此类输入是当前状态的表示。在我们的实验中，PIR说明了与基线相比，即使缺失的方式，与基线相比，与地面真相的特征相比。此外，PIR已成功地应用于下游控制任务中，在该任务中，机器人通过PIR更快，更准确地利用了学习状态，从随机的起始位置通过复杂的涡旋街，以达到随机目标。

### DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents 
[[arxiv](https://arxiv.org/abs/2410.14803)] [[cool](https://papers.cool/arxiv/2410.14803)] [[pdf](https://arxiv.org/pdf/2410.14803)]
> **Authors**: Taiyi Wang,Zhihao Wu,Jianheng Liu,Jianye Hao,Jun Wang,Kun Shao
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: Paper and Appendix, 26 pages
- **标题**: 分散：一个异步的分布式加固学习框架
- **领域**: 机器学习,人工智能,分布式、并行和集群计算,系统与控制
- **摘要**: 设备控制代理，尤其是在移动设备上，负责操作移动设备以满足用户的请求，从而实现无缝和直观的交互。将多模式的大语言模型（MLLM）集成到这些代理中增强了其理解和执行复杂命令的能力，从而改善了用户体验。但是，由于数据可用性和效率低下的在线培训流程，对设备控制的微调MLLM提出了重大挑战。本文介绍了一个新颖的框架，旨在提高在线RL微调对移动设备控制剂的效率。 Distrl采用集中培训和分散数据获取，以确保在动态在线互动的背景下进行有效的微调。此外，该框架由我们量身定制的RL算法支持，该算法有效地平衡了探索与优先利用收集的数据，以确保稳定且稳定的培训。我们的实验表明，平均而言，ESTREL可以提高训练效率的3倍，并使培训数据收集比领先的同步多机器方法快2.4倍。值得注意的是，经过训练后，与从开放基准的一般Android任务上的最新方法相比，Exterl的成功率相对提​​高了20％，在维持相同的培训时间的同时，大大优于现有方法。这些结果证明了分散作为一种可扩展和高效的解决方案，从而为现实世界中的野外设备控制任务提供了训练效率和代理性能的实质性提高。

### Knowledge Graph Embeddings: A Comprehensive Survey on Capturing Relation Properties 
[[arxiv](https://arxiv.org/abs/2410.14733)] [[cool](https://papers.cool/arxiv/2410.14733)] [[pdf](https://arxiv.org/pdf/2410.14733)]
> **Authors**: Guanglin Niu
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-21
> **comment**: 22 pages, 8 figures, 3 tables, this paper is a modified English version of our article already published in Computer Science journal (in Chinese), released to facilitate communication among international researchers in the relevant fields
- **标题**: 知识图嵌入：一项有关捕获关系属性的综合调查
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 知识图嵌入（KGE）技术在将符号知识图（KGS）转换为数值表示中起着关键作用，从而增强了各种深度学习模型的知识增强应用程序。与实体不同，kg中的关系是语义含义的载体，其准确的建模对于KGE模型的性能至关重要。首先，我们解决了关系中固有的复杂映射属性，例如一对一，一对多，多一对一对一的映射。我们提供了基于关系映射的模型，利用特定表示空间的模型，基于张量分解的模型和基于神经网络的模型的全面摘要。接下来，着重于捕获对称模式，例如对称，不对称，反转和组成，我们回顾了采用修改的张量分解的模型，这些模型基于修改后的关系映射以及利用旋转操作的模型。随后，考虑到实体之间的隐式层次关系，我们介绍了结合辅助信息，基于双曲线空间的模型以及利用层坐标系的模型。最后，在响应更复杂的场景（例如稀疏和动态kg）时，本文讨论了潜在的未来研究方向。我们探讨了创新的思想，例如将多模式信息整合到KGE中，将关系模型与规则增强，并开发模型以捕获动态KGE设置中的关系特征。

### Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning 
[[arxiv](https://arxiv.org/abs/2410.14464)] [[cool](https://papers.cool/arxiv/2410.14464)] [[pdf](https://arxiv.org/pdf/2410.14464)]
> **Authors**: Jialu Tang,Tong Xia,Yuan Lu,Cecilia Mascolo,Aaqib Saeed
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 元学习回答的几片问题的心电图模型
- **领域**: 机器学习
- **摘要**: 心电图（ECG）的解释需要专门的专业知识，通常涉及来自具有自然语言的复杂临床查询的ECG信号的综合见解。标记为ECG数据的稀缺性以及临床查询的多样性性质为开发健壮和适应能力的ECG诊断系统带来了重大挑战。这项工作介绍了一种新型的多模式元学习方法，用于几乎没有ECG的问题回答，以解决有限标记数据的挑战，同时利用大型语言模型（LLMS）中编码的丰富知识。我们的LLM-Agnostic方法通过可训练的融合模块将预训练的ECG编码器与冷冻LLM（例如Llama和Gemma）相结合，使语言模型能够对ECG数据进行推理并产生临床意义的答案。与受监督的基线相比，广泛的实验表明，与监督的基线相比，具有较高的诊断任务的概括，即使ECG铅的铅也有明显的性能。例如，在5速5-shot设置中，我们使用Llama-3.1-8B的方法分别达到84.6％，77.3％和69.6％的精确度，分别在单个验证，选择和查询问题类型上。这些结果突出了我们方法通过将信号处理与LLMS的细微差异理解能力相结合，尤其是在数据约束的情况下，通过将信号处理与细微的语言理解能力相结合来增强临床心电图解释的潜力。

### ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding 
[[arxiv](https://arxiv.org/abs/2410.16592)] [[cool](https://papers.cool/arxiv/2410.16592)] [[pdf](https://arxiv.org/pdf/2410.16592)]
> **Authors**: Andrew Kan,Christopher Kan,Zaid Nabulsi
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: 7 pages, 2 figures
- **标题**: vimguard：一种新颖的视频误导性多模式系统
- **领域**: 机器学习,计算语言学,计算机与社会
- **摘要**: 社交媒体和简短视频（SFV）的兴起促进了错误信息的繁殖场面。随着大型语言模型的出现，通过自动虚假的索赔检测来遏制这个错误信息问题。不幸的是，SFV中错误信息的自动检测是一个更复杂的问题，在很大程度上尚未研究。虽然文本样本是单差（仅包含单词），但SFV构成了三种不同的方式：单词，视觉效果和非语言音频。在这项工作中，我们介绍了视频蒙面的自动编码器，以进行错误信息守护（Vimguard），这是第一个能够通过分析其所有三种组成方式来检查SFV的第一个深入学习架构。 Vimguard利用双组件系统。首先，视频和音频掩盖了自动编码器分析视频的视觉和非语言音频元素，以辨别其意图。特别是它是否打算提出丰富的主张。如果认为SFV具有丰富的意图，它将通过我们的第二个组件：检索增强生成系统，以验证口语的事实准确性。在评估中，Vimguard的表现优于三个尖端的事实检查器，从而为SFV事实检查和迈向社交平台上值得信赖的新闻的重大迈进。为了促进进一步的测试和迭代，将Vimguard部署到Chrome扩展中，所有代码都在Github上开源。

### Promoting cross-modal representations to improve multimodal foundation models for physiological signals 
[[arxiv](https://arxiv.org/abs/2410.16424)] [[cool](https://papers.cool/arxiv/2410.16424)] [[pdf](https://arxiv.org/pdf/2410.16424)]
> **Authors**: Ching Fang,Christopher Sandino,Behrooz Mahasseni,Juri Minxha,Hadi Pouransari,Erdrin Azemi,Ali Moin,Ellen Zippi
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: NeurIPS 2024 AIM-FM Workshop
- **标题**: 促进跨模式表示以改善生理信号的多模式基础模型
- **领域**: 机器学习
- **摘要**: 许多医疗保健应用本质上是多模式的，涉及多个生理信号。随着这些信号的传感器变得越来越普遍，改进的多模式医疗保健数据的机器学习方法至关重要。预训练的基础模型是成功的有前途的途径。但是，在医疗保健中开发基础模型的方法仍在早期探索中，鉴于生理信号的多样性，哪种预处理策略最有效。这部分是由于多模式健康数据中的挑战：在许多患者中获得数据很困难且昂贵，受试者间的可变性很多，并且在下游任务中，模态通常是异体的信息。在这里，我们在Physionet 2018数据集中探讨了这些挑战。我们使用蒙版的自动编码目标来预处理多模型。我们表明，该模型学习可以线性探索的代表，以针对各种下游任务进行线性探讨。我们假设跨模式重建目标对于成功的多模式训练非常重要，因为它们鼓励模型跨模态整合信息。我们证明，输入空间中的模态辍学会改善下游任务的性能。我们还发现，在多个任务中，对对比的学习目标预测的后期融合模型的有效性较小。最后，我们分析了模型的表示，表明注意力的权重变得更加跨模式，并且在时间上与我们的训练策略保持一致。学习的嵌入也从每个单元编码的方式方面变得更加分布。总体而言，我们的工作展示了具有健康数据的多模式基础模型的实用性，即使在各种生理数据来源之间也是如此。我们进一步认为，诱导跨模式的明确方法可能会增强多模式预处理策略。

### LiMTR: Time Series Motion Prediction for Diverse Road Users through Multimodal Feature Integration 
[[arxiv](https://arxiv.org/abs/2410.15819)] [[cool](https://papers.cool/arxiv/2410.15819)] [[pdf](https://arxiv.org/pdf/2410.15819)]
> **Authors**: Camiel Oerlemans,Bram Grooten,Michiel Braat,Alaa Alassi,Emilia Silvas,Decebal Constantin Mocanu
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: Accepted at the NeurIPS 2024 workshop Time Series in the Age of Large Models. Code available at https://github.com/Cing2/LiMTR
- **标题**: LIMTR：通过多模式特征集成为不同道路使用者的时间序列运动预测
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 准确地预测道路使用者的行为对于使城市或人口稠密地区的自动驾驶汽车的安全操作至关重要。因此，人们对时间序列运动预测研究的兴趣越来越大，近年来导致最新技术的显着进步。但是，使用LiDar数据捕获更详细的本地特征（例如人的凝视或姿势）的潜力仍然在很大程度上没有探索。为了解决这个问题，我们基于PointNet Foundation Model Architecture开发了一种新型的多模式预测方法，并结合了本地LIDAR功能。 Waymo Open数据集的评估显示，在整合并与以前的最新MTR进行了比较时，绩效提高了6.20％和1.58％。我们开源LIMTR模型的代码。

### Beyond the Kolmogorov Barrier: A Learnable Weighted Hybrid Autoencoder for Model Order Reduction 
[[arxiv](https://arxiv.org/abs/2410.18148)] [[cool](https://papers.cool/arxiv/2410.18148)] [[pdf](https://arxiv.org/pdf/2410.18148)]
> **Authors**: Nithin Somasekharan,Shaowu Pan
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-24
> **comment**: 31 pages
- **标题**: 超越Kolmogorov屏障：可学习的加权混合自动编码器，用于降低模型订单
- **领域**: 机器学习,人工智能,计算物理,机器学习
- **摘要**: 高维，复杂物理系统的表示学习旨在确定低维内在的潜在空间，这对于减少阶建模和模态分析至关重要。为了克服著名的科尔莫格罗夫障碍，近年来已经引入了深层自动编码器（AES），但是随着潜在空间的排名增加，它们常常遭受融合行为不佳。为了解决这个问题，我们提出了可学习的加权混合自动编码器，这是一种混合方法，通过可学习的加权框架将单数值分解（SVD）的优势与深度自动编码器结合在一起。我们发现，引入可学习的加权参数至关重要 - 没有它们，所得模型将崩溃成标准的POD，或者未能表现出所需的收敛行为。有趣的是，我们从经验上发现，与其他型号相比，我们训练的模型的清晰度数千倍。我们在经典混沌PDE系统上进行的实验，包括1D库拉莫托 - 西瓦辛斯基和强迫各向同性湍流数据集，表明我们的方法显着提高了与几种竞争方法相比的概括性能。此外，当与时间序列建模技术（例如Koopman操作员，LSTM）结合使用时，该技术为高维多规模PDE系统的替代建模提供了重大改进。

### Multimodal Information Bottleneck for Deep Reinforcement Learning with Multiple Sensors 
[[arxiv](https://arxiv.org/abs/2410.17551)] [[cool](https://papers.cool/arxiv/2410.17551)] [[pdf](https://arxiv.org/pdf/2410.17551)]
> **Authors**: Bang You,Huaping Liu
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: 31 pages
- **标题**: 多模式信息瓶颈，用于深入增强学习的多种传感器
- **领域**: 机器学习,机器人技术
- **摘要**: 强化学习在机器人控制任务上取得了令人鼓舞的结果，但努力从多种特征上有不同的感觉方式中有效地利用信息。最近的工作基于重建或共同信息构建辅助损失，以从多个感觉输入中提取联合表示，以提高样本效率和增强学习算法的性能。但是，这些方法学到的表示形式可以捕获与学习政策无关的信息并可能降低表现。我们认为，有关原始多模式观察的学习联合表示中的压缩信息很有帮助，并提出了一个多模式信息瓶颈模型，以从以上为中心的图像和前提受感受中学习与任务相关的联合表示。我们的模型在多模式观察中压缩并保留了学习压缩联合表示的预测信息，该观察融合了视觉和本体感受反馈中的互补信息，同时滤除了原始多模式观察中的任务 - 毫无疑问的信息。我们建议最大程度地减少多模式信息瓶颈目标的上限，以进行计算触觉优化。对以自我为中心的图像和本体感受的几项具有挑战性的运动任务进行的实验评估表明，我们的方法可实现更好的样品效率，而零拍的稳健性则比领先的基线相比，对看不见的白噪声。我们还从经验上证明，从以自我为中心的图像和本体感受中利用信息比仅使用一种单一模式更有用。

### Deep Insights into Cognitive Decline: A Survey of Leveraging Non-Intrusive Modalities with Deep Learning Techniques 
[[arxiv](https://arxiv.org/abs/2410.18972)] [[cool](https://papers.cool/arxiv/2410.18972)] [[pdf](https://arxiv.org/pdf/2410.18972)]
> **Authors**: David Ortiz-Perez,Manuel Benavent-Lledo,Jose Garcia-Rodriguez,David Tomás,M. Flores Vizcaya-Moreno
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: 对认知下降的深刻见解：通过深度学习技术利用非侵入性方式的调查
- **领域**: 机器学习,人工智能
- **摘要**: 认知能力下降是衰老的自然部分，通常导致认知能力降低。但是，在某些情况下，这种下降更明显，通常是由于诸如阿尔茨海默氏病这样的疾病。早期发现异常认知能力下降至关重要，因为它可以促进及时的专业干预。尽管医疗数据可以在此检测中有所帮助，但通常涉及侵入性程序。另一种方法是采用非侵入性技术，例如语音或手写分析，这不一定会影响日常活动。这项调查回顾了使用深度学习技术来自动化认知下降估计任务的最相关方法，包括音频，文本和视觉处理。我们讨论了每种方式和方法的关键特征和优势，包括变压器体系结构和基础模型等最先进的方法。此外，我们提出了整合不同模式以开发多模式模型的作品。我们还强调了最重要的数据集以及使用这些资源的研究的定量结果。从这篇评论中，得出了几个结论。在大多数情况下，文本方式取得了最佳的结果，并且与检测认知能力下降最相关。此外，将各种方法从各个模式结合到多模型模型都一致地增强了几乎所有情况的性能。

### Context is Key: A Benchmark for Forecasting with Essential Textual Information 
[[arxiv](https://arxiv.org/abs/2410.18959)] [[cool](https://papers.cool/arxiv/2410.18959)] [[pdf](https://arxiv.org/pdf/2410.18959)]
> **Authors**: Andrew Robert Williams,Arjun Ashok,Étienne Marcotte,Valentina Zantedeschi,Jithendaraa Subramanian,Roland Riachi,James Requeima,Alexandre Lacoste,Irina Rish,Nicolas Chapados,Alexandre Drouin
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Preprint; under review. First two authors contributed equally
- **标题**: 上下文是关键：使用基本文本信息进行预测的基准
- **领域**: 机器学习,人工智能,机器学习
- **摘要**: 预测是跨众多领域决策的关键任务。尽管历史数值数据提供了一个开始，但他们无法传达完整的上下文以获得可靠和准确的预测。人类预报员经常依靠其他信息，例如背景知识和约束，可以通过自然语言有效地传达这些信息。但是，尽管基于LLM的预测最近取得了进展，但他们有效整合此文本信息的能力仍然是一个悬而未决的问题。为了解决这个问题，我们介绍了“上下文是关键”（CIK），这是一个时间序列预测的基准，将数值数据与各种精心设计的文本上下文配对，需要模型来集成这两种模态；至关重要的是，CIK中的每个任务都需要成功理解文本上下文。我们评估了一系列方法，包括统计模型，时间序列基础模型和基于LLM的预测者，并提出了一种简单而有效的LLM提示方法，该方法的表现优于我们的基准测试方法。我们的实验强调了合并上下文信息的重要性，在使用基于LLM的预测模型时表现出令人惊讶的性能，并揭示了它们的一些关键缺点。该基准旨在通过促进具有多种技术专长的决策者既准确又可以访问的模型来推进多模式预测。基准可以在https://servicenow.github.io/context-is-key-forecasting/v0/上可视化。

### Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification 
[[arxiv](https://arxiv.org/abs/2410.18686)] [[cool](https://papers.cool/arxiv/2410.18686)] [[pdf](https://arxiv.org/pdf/2410.18686)]
> **Authors**: Xiaoyu Tao,Tingyue Pan,Mingyue Cheng,Yucong Luo
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: 具有语义空间对齐的分层多模式LLM，用于增强时间序列分类
- **领域**: 机器学习
- **摘要**: 利用大型语言模型（LLM）引起了人们的关注，并在时间序列分类中引入了新的观点。但是，现有方法通常忽略了时间序列数据中固有的关键动态时间信息，并且在将这些数据与文本语义对齐时面临挑战。为了解决这些局限性，我们提出了命中仪，这是一种层次多模式模型，将时间信息无缝整合到LLMS中以进行多元时间序列分类（MTSC）。我们的模型采用层次功能编码器，通过数据特定和特定于任务的嵌入来捕获时间序列数据的各个方面。为了促进时间序列和文本之间的语义空间对齐，我们引入了一个双视图对比对准模块，该模块弥合了模态之间的差距。此外，我们采用混合促使策略以参数有效的方式微调预训练的LLM。通过有效合并动态的时间特征并确保语义对齐，命中仪使LLMS可以处理连续的时间序列数据并通过文本生成来实现最新的分类性能。基准数据集的广泛实验表明，与大多数竞争性基线方法相比，命中仪可显着提高时间序列分类精度。我们的发现突出了将时间特征集成到LLM的潜力，为高级时间序列分析铺平了道路。该代码可公开用于进一步的研究和验证。我们的代码公开可用1。

### IMAN: An Adaptive Network for Robust NPC Mortality Prediction with Missing Modalities 
[[arxiv](https://arxiv.org/abs/2410.18551)] [[cool](https://papers.cool/arxiv/2410.18551)] [[pdf](https://arxiv.org/pdf/2410.18551)]
> **Authors**: Yejing Huo,Guoheng Huang,Lianglun Cheng,Jianbin He,Xuhang Chen,Xiaochen Yuan,Guo Zhong,Chi-Man Pun
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: The paper has been accepted by BIBM 2024
- **标题**: Iman：用于强大NPC死亡率预测的自适应网络，缺失了方式
- **领域**: 机器学习,人工智能
- **摘要**: 准确预测鼻咽癌（NPC）的死亡率是一种复杂的恶性肿瘤，在晚期阶段尤其具有挑战性，对于优化治疗策略和改善患者预后至关重要。但是，这种预测过程通常会因NPC相关数据的高维和异质性质而损害，再加上不完整的多模式数据的普遍问题，表现为缺失的放射学图像或不完整的诊断报告。当面对这种不完整的数据时，传统的机器学习方法遭受了重大的性能下降，因为它们无法有效处理跨模态的高维度和复杂的相关性。即使是变形金刚等先进的多模式学习技术，也很难在缺失方式的情况下保持稳健的性能，因为它们缺乏专业的机制来适应和调整各种数据类型，同时还可以在复杂的NPC数据中捕获细微的模式和上下文关系。为了解决这些问题，我们介绍了IMAN：一个自适应网络，用于稳健的NPC死亡率预测，缺失了模式。

### Generator Matching: Generative modeling with arbitrary Markov processes 
[[arxiv](https://arxiv.org/abs/2410.20587)] [[cool](https://papers.cool/arxiv/2410.20587)] [[pdf](https://arxiv.org/pdf/2410.20587)]
> **Authors**: Peter Holderrieth,Marton Havasi,Jason Yim,Neta Shaul,Itai Gat,Tommi Jaakkola,Brian Karrer,Ricky T. Q. Chen,Yaron Lipman
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 发电机匹配：使用任意马尔可夫进程的生成建模
- **领域**: 机器学习,人工智能
- **摘要**: 我们介绍了生成器匹配，这是一种使用任意Markov进程的生成建模的模态性不足的框架。发电机表征了Markov过程的无限演变，我们利用该过程将其用于类似静脉的生成建模到流匹配：我们构造了生成单个数据点的条件发生器，然后学会近似于生成完整数据分布的边缘发电机。我们表明，匹配生成器统一了各种生成建模方法，包括扩散模型，流匹配和离散扩散模型。此外，它将设计空间扩展到新的和未开发的马尔可夫进程，例如跳跃过程。最后，发电机匹配能够构建马尔可夫生成模型的叠加，并可以严格地构建多模式模型。我们从经验上验证了图像和多模式生成的方法，例如通过跳跃过程表明叠加可以提高性能。

### PaPaGei: Open Foundation Models for Optical Physiological Signals 
[[arxiv](https://arxiv.org/abs/2410.20542)] [[cool](https://papers.cool/arxiv/2410.20542)] [[pdf](https://arxiv.org/pdf/2410.20542)]
> **Authors**: Arvind Pillai,Dimitris Spathis,Fahim Kawsar,Mohammad Malekzadeh
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-28
> **comment**: Accepted at ICLR 2025. Improved version with new experiments and results. Code and models: https://github.com/nokia-bell-labs/papagei-foundation-model
- **标题**: Papagei：光学生理信号的开放基础模型
- **领域**: 机器学习,信号处理
- **摘要**: Photoplethysmography（PPG）是用于监测生物信号和心血管健康的领先的非侵入性技术，在临床环境和消费者可穿戴设备中都广泛采用。尽管在PPG信号上训练的机器学习模型已经显示出希望，但它们往往是特定于任务的，并且在泛化方面挣扎。当前的研究受到单个设备数据集的使用，对域外概括的探索不足以及缺乏公开可用模型的限制，这会阻碍可重复性。为了解决这些限制，我们提出Papagei，这是PPG信号的第一个开放基础模型。该模型已在超过57,000个小时的数据上进行了预培训，其中包括2000万个未标记的PPG段，可公开可用的数据集。我们介绍了一种新颖的表示学习方法，该方法利用跨个体的PPG信号形态的领域知识，与传统的对比学习方法相比，可以捕获更丰富的表示。我们评估了Papagei，可针对最新的时间序列基础模型以及从10个不同数据集的20个任务中进行自我监管的学习基准，跨越心血管健康，睡眠障碍，妊娠监测和良好评估。我们的模型表明，至少14个任务中，分类和回归指标分别提高了6.3％和2.9％。值得注意的是，Papagei在更大的数据和参数效率高70倍的模型的同时获得了这些结果。除了准确性之外，我们还检查了不同肤色之间的模型鲁棒性，从而在未来模型中建立了偏见评估的基准。 Papagei既可以作为功能提取器，也可以作为多模式模型的编码器，为多模式健康监测提供了新的机会。

### Deep Learning-Driven Microstructure Characterization and Vickers Hardness Prediction of Mg-Gd Alloys 
[[arxiv](https://arxiv.org/abs/2410.20402)] [[cool](https://papers.cool/arxiv/2410.20402)] [[pdf](https://arxiv.org/pdf/2410.20402)]
> **Authors**: Lu Wang,Hongchan Chen,Bing Wang,Qian Li,Qun Luo,Yuexing Han
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 深度学习驱动的微观结构表征和Vickers MG-GD合金硬度预测
- **领域**: 机器学习,材料科学,计算机视觉和模式识别
- **摘要**: 在材料科学领域，探索组成，微观结构和财产之间的关系一直是重要的研究重点。固定溶液MG-GD合金的机械性能受到GD含量，树突结构和次级存在的显着影响。为了更好地分析和预测这些因素的影响，本研究提出了基于图像处理和深度学习技术的多模式融合学习框架。该框架同时集成了元素组成和微观结构特征，以准确预测固定溶液MG-GD合金的Vickers硬度。最初，采用深度学习方法来从从文献和实验获得的各种固定型MG-GD合金图像中提取微结构信息。这为性能预测任务提供了精确的晶粒尺寸和二级微观结构特征。随后，将这些定量分析结果与GD内容信息结合使用，以构建性能预测数据集。最后，使用基于变压器结构的回归模型来预测MG-GD合金的Vickers硬度。实验结果表明，变压器模型在预测准确性方面表现最好，其R^2值为0.9。此外，Shap Analysis确定了影响MG-GD合金的Vickers硬度的四个关键特征的关键值，为合金设计提供了宝贵的指导。这些发现不仅增强了对合金性能的理解，而且还为未来的材料设计和优化提供了理论支持。

### Domain Specific Data Distillation and Multi-modal Embedding Generation 
[[arxiv](https://arxiv.org/abs/2410.20325)] [[cool](https://papers.cool/arxiv/2410.20325)] [[pdf](https://arxiv.org/pdf/2410.20325)]
> **Authors**: Sharadind Peddiraju,Srini Rajagopal
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: 7 pages, 3 figures
- **标题**: 域特异性数据蒸馏和多模式嵌入生成
- **领域**: 机器学习,社交和信息网络
- **摘要**: 创建以域为中心的嵌入的挑战是源于丰富的非结构化数据和域特异性结构化数据的稀缺性。传统的嵌入技术通常依赖于两种方式，从而限制其适用性和功效。本文介绍了一种新型的建模方法，该方法利用结构化数据从非结构化数据中过滤噪声，从而导致嵌入具有高精度和回忆的域特异性属性预测预测。所提出的模型在混合协作过滤（HCF）框架中运行，其中通用实体表示通过相关项目预测任务进行微调。我们的实验专注于云计算域，表明，基于HCF的嵌入量优于基于自动编码器的嵌入（使用纯粹的非结构数据），精确地提升了28％的升力，召回了域特异性属性预测的召回率11％。

### Chemical Language Model Linker: blending text and molecules with modular adapters 
[[arxiv](https://arxiv.org/abs/2410.20182)] [[cool](https://papers.cool/arxiv/2410.20182)] [[pdf](https://arxiv.org/pdf/2410.20182)]
> **Authors**: Yifan Deng,Spencer S. Ericksen,Anthony Gitter
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: 25 pages, 3 figures
- **标题**: 化学语言模型接头：将文本和分子与模块化适配器混合
- **领域**: 机器学习,人工智能,定量方法
- **摘要**: 大型语言模型和多模式模型的开发使得从文本描述中产生新分子的吸引力。生成建模将使范式从依赖大规模化学筛选以找到具有所需特性的分子以直接产生这些分子。但是，结合文本和分子的多模式模型通常是从​​头开始训练的，而不利用现有的高质量预验证的模型。该方法消耗了更多的计算资源，并禁止模型缩放。相比之下，我们提出了一种名为Chemical Language Model Linker（ChemLML）的基于轻型适配器的策略。 ChemLML将两个单个域模型混合在一起，并从文本描述中获得有条件的分子产生，同时仍在分子域的专门嵌入空间中运行。 ChemLML可以通过训练相对较少的适配器参数来量身定制分子生成的多样化文本模型。我们发现，在Chemlml中使用的分子表示的选择，微笑与自拍照对条件分子产生的性能有很大影响。尽管不能保证有效的分子，但笑通常还是可取的。我们提出了使用大型分子数据集及其相关描述来评估分子生成的问题，并提供数据集的过滤版本作为一代测试集。为了证明如何在实践中使用ChemLML，我们会产生候选蛋白抑制剂并使用对接来评估其质量。

### Prompt Diffusion Robustifies Any-Modality Prompt Learning 
[[arxiv](https://arxiv.org/abs/2410.20164)] [[cool](https://papers.cool/arxiv/2410.20164)] [[pdf](https://arxiv.org/pdf/2410.20164)]
> **Authors**: Yingjun Du,Gaowen Liu,Yuzhang Shang,Yuguang Yao,Ramana Kompella,Cees G. M. Snoek
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: Under review
- **标题**: 及时扩散可鲁棒化任何模式及时学习
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 基础模型启用了迅速的分类器，用于零拍和几乎没有学习。但是，采用固定提示的常规方法遭受了分布转移的影响，对未见样本的普遍性产生了负面影响。本文介绍了提示扩散，该扩散模型逐渐完善了提示，以获取每个样本的自定义提示。具体来说，我们首先优化了提示集的集合，以获取每个样本的过度提示。然后，我们在及时空间内提出了一个及时的扩散模型，从而使生成过渡过程从随机提示到其过度拟合的提示。由于我们无法在推断过程中访问测试图像的标签，因此我们的模型逐渐使用经过训练的，及时的扩散从随机提示中生成自定义提示。我们的及时扩散是通用，灵活和模态的，使其成为一个简单的插件模块，将无缝嵌入到现有的提示学习方法中，以进行文本，视觉或多模式提示。我们的扩散模型使用基于快速的采样策略，仅在五个步骤中优化测试样品提示，从而在绩效提高和计算效率之间进行了良好的权衡。对于所有及时测试的及时学习方法，添加及时扩散为基础到新的概括，跨数据集泛化和域的概括在分类任务中测试的15个不同数据集测试的分类任务中都会产生更强大的结果。

### Evaluating Cost-Accuracy Trade-offs in Multimodal Search Relevance Judgements 
[[arxiv](https://arxiv.org/abs/2410.19974)] [[cool](https://papers.cool/arxiv/2410.19974)] [[pdf](https://arxiv.org/pdf/2410.19974)]
> **Authors**: Silvia Terragni,Hoang Cuong,Joachim Daiber,Pallavi Gudipati,Pablo N. Mendes
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: ef:CIKM MMSR 2024
- **标题**: 评估多模式搜索相关性判断中的成本准确性权衡
- **领域**: 机器学习,计算语言学,信息检索
- **摘要**: 大型语言模型（LLM）表现出有效的搜索相关性评估者的潜力。但是，缺乏全面的指导，即模型在各种情况下或在特定用例中始终如一地发挥最佳性能。在本文中，我们根据多种多模式搜索方案的人类判断，评估了几种LLM和多模式模型（MLLM）。我们的分析调查了成本和准确性之间的权衡，强调该模型性能取决于上下文。有趣的是，在较小的模型中，包含视觉组件可能会阻碍性能而不是增强其性能。这些发现突出了为实际应用选择最合适的模型所涉及的复杂性。

### Simultaneous Dimensionality Reduction for Extracting Useful Representations of Large Empirical Multimodal Datasets 
[[arxiv](https://arxiv.org/abs/2410.19867)] [[cool](https://papers.cool/arxiv/2410.19867)] [[pdf](https://arxiv.org/pdf/2410.19867)]
> **Authors**: Eslam Abdelaleem
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-28
> **comment**: PhD Dissertation, available at Emory EDT @ https://etd.library.emory.edu/concern/etds/hd76s156x?locale=en
- **标题**: 同时减少维度，用于提取大型经验多模式数据集的有用表示
- **领域**: 机器学习,生物物理学,数据分析、统计和概率
- **摘要**: 在物理学中的简化寻求推动了对复杂系统简明的数学表示的探索。本文的重点是减少维度的概念，作为从高维数据中获得低维描述的一种手段，从而促进了理解和分析。我们解决了反对常规假设的现实数据所带来的挑战，例如神经系统中的复杂相互作用或高维动力学系统。从理论物理学和机器学习中利用见解，这项工作统一了综合框架下的各种还原方法，即深层的多元信息瓶颈。该框架可以根据特定的研究问题设计量身定制的还原算法。我们探索和断言同时还原方法对独立还原对应物的功效，这表明了它们在捕获多种方式之间的协方差方面的优势，同时需要更少的数据。我们还介绍了新型技术，例如深层变异对称信息瓶颈，以同时减少一般的非线性减少。我们表明，同时还原的相同原则是对共同信息有效估计的关键。我们表明，我们的新方法能够发现动态系统的高维观测值的坐标。通过分析研究和经验验证，我们阐明了降低方法的复杂性，为增强各个领域的数据分析铺平了道路。我们强调了这些方法的潜力，可以从复杂数据集中提取有意义的见解，推动基础研究和应用科学方面的进步。随着这些方法的发展，他们承诺将加深我们对复杂系统的理解，并为更有效的数据分析策略提供信息。

### Conformal Prediction for Multimodal Regression 
[[arxiv](https://arxiv.org/abs/2410.19653)] [[cool](https://papers.cool/arxiv/2410.19653)] [[pdf](https://arxiv.org/pdf/2410.19653)]
> **Authors**: Alexis Bose,Jonathan Ethier,Paul Guinand
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: 20 pages, 34 figures
- **标题**: 多模式回归的保形预测
- **领域**: 机器学习
- **摘要**: 本文介绍了多模式保形回归。传统上，仅限于具有数值输入特征的方案，现在通过我们的方法将共形预测扩展到多模式上下文，该方法可以利用复杂的神经网络体系结构处理图像和非结构化文本的内部特征。我们的发现突出了内部神经网络特征的潜力，这些特征是从合并多模式信息的收敛点中提取的，可以通过共形预测来构建预测间隔（PIS）。这种能力铺平了新的路径，用于在丰富的多模式数据的领域中部署保形预测，从而使更广泛的问题从保证的无分配不确定性量化中受益。

### A Review of Deep Learning Approaches for Non-Invasive Cognitive Impairment Detection 
[[arxiv](https://arxiv.org/abs/2410.19898)] [[cool](https://papers.cool/arxiv/2410.19898)] [[pdf](https://arxiv.org/pdf/2410.19898)]
> **Authors**: Muath Alsuhaibani,Ali Pourramezan Fard,Jian Sun,Farida Far Poor,Peter S. Pressman,Mohammad H. Mahoor
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 对无创认知障碍检测的深度学习方法的回顾
- **领域**: 机器学习,人工智能
- **摘要**: 这篇评论论文探讨了无创认知障碍检测的深度学习方法的最新进展。我们研究了认知能力下降的各种非侵入性指标，包括言语和语言，面部和摩托车。该论文概述了相关数据集，功能提取技术以及应用于该域的深度学习体系结构。我们已经分析了跨模式的不同方法的性能，并观察到基于语音和语言的方法通常达到了最高的检测性能。结合声学和语言特征的研究往往胜过使用单一模态的研究。面部分析方法表明了视觉方式的希望，但对研究的研究较少。大多数论文侧重于二进制分类（受损与未受损），较少解决多类或回归任务。转移学习和预训练的语言模型是流行和有效的技术，尤其是在语言分析中。尽管取得了重大进展，但仍有一些挑战，包括数据标准化和可访问性，模型解释性，纵向分析局限性和临床适应性。最后，我们提出了未来的研究方向，例如研究语言敏锐的语音分析方法，开发多模式诊断系统以及解决AI辅助医疗保健中的道德考虑。通过综合当前趋势并确定关键的障碍，该综述旨在指导基于深度学习的认知障碍检测系统的进一步发展，以改善早期诊断并最终导致患者结果。

### A Systematic Review of Machine Learning in Sports Betting: Techniques, Challenges, and Future Directions 
[[arxiv](https://arxiv.org/abs/2410.21484)] [[cool](https://papers.cool/arxiv/2410.21484)] [[pdf](https://arxiv.org/pdf/2410.21484)]
> **Authors**: René Manassé Galekwa,Jean Marie Tshimula,Etienne Gael Tajeuna,Kyamakya Kyandoghere
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 体育博彩中机器学习的系统评价：技术，挑战和未来的方向
- **领域**: 机器学习,计算工程、金融和科学,新兴技术,信息检索,社交和信息网络
- **摘要**: 体育博彩行业经历了快速增长，这在很大程度上取决于技术进步和在线平台的扩散。机器学习（ML）通过实现更准确的预测，动态赔率设定和增强风险管理的方式在该领域的转型中发挥了关键作用。这项系统评价探讨了各种ML技术，包括支持向量机，随机森林和神经网络，这些技术应用于足球，篮球，网球和板球等不同运动中。这些模型利用历史数据，游戏中的统计数据和实时信息来优化投注策略并确定价值投注，最终提高盈利能力。对于博彩公司，ML促进了动态的赔率调整和有效的风险管理，而Bettors则利用数据驱动的见解来利用市场效率低下。这篇综述还强调了ML在欺诈检测中的作用，在欺诈检测中，使用异常检测模型来识别可疑的投注模式。尽管取得了这些进步，但仍存在诸如数据质量，实时决策以及运动成果固有的不可预测性之类的挑战。与透明和公平性有关的道德问题也很重要。未来的研究应着重于开发自适应模型，以类似于金融投资组合的方式整合多模式数据并管理风险。这篇综述提供了对ML在体育博彩中当前的应用的全面检查，并强调了这些技术的潜力和局限性。

### AiSciVision: A Framework for Specializing Large Multimodal Models in Scientific Image Classification 
[[arxiv](https://arxiv.org/abs/2410.21480)] [[cool](https://papers.cool/arxiv/2410.21480)] [[pdf](https://arxiv.org/pdf/2410.21480)]
> **Authors**: Brendan Hogan,Anmol Kabra,Felipe Siqueira Pacheco,Laura Greenstreet,Joshua Fan,Aaron Ferber,Marta Ummus,Alecsander Brito,Olivia Graham,Lillian Aoki,Drew Harvell,Alex Flecker,Carla Gomes
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: Aiscivision：专门针对科学图像分类中的大型多模式模型的框架
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 信任和解释性对于在科学研究中使用人工智能（AI）至关重要，但是当前的模型通常作为黑匣子起作用，具有有限的透明度和对其产出的理由。我们介绍了Aiscivision，该框架将大型多模型（LMM）专门用于交互式研究伙伴和分类模型，用于利基科学领域中的图像分类任务。我们的框架使用了两个关键组件：（1）在代理工作流中使用的视觉检索 - 调格生成（Visrag）和（2）特定于域特异性工具。为了对目标图像进行分类，Aiscivision首先检索最相似的正和负标记图像作为LMM的上下文。然后，LMM代理会积极选择并应用工具来操纵和检查目标图像，然后在做出最终预测之前完善其分析。这些Visrag和工具组件旨在反映域专家的过程，因为人类经常将新数据与类似示例进行比较，并使用专门的工具在得出结论之前操纵和检查图像。每种推论都会产生预测和自然语言成绩单，详细介绍导致预测的推理和工具用法。我们在三个现实世界的科学图像分类数据集上评估了AISCIVISION：检测水产养殖池，患病的鳗草和太阳能电池板的存在。在这些数据集中，我们的方法在低标签数据设置中优于完全监督的模型。 Aiscivision通过专门的Web应用程序来积极地部署在现实世界中，特别是用于水产养殖研究，该应用程序显示并允许专家用户与成绩单交谈。这项工作代表了朝着既可以解释又有效的AI系统迈出的至关重要的一步，可以推进其在科学研究和科学发现中的使用。

### Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse 
[[arxiv](https://arxiv.org/abs/2410.21333)] [[cool](https://papers.cool/arxiv/2410.21333)] [[pdf](https://arxiv.org/pdf/2410.21333)]
> **Authors**: Ryan Liu,Jiayi Geng,Addison J. Wu,Ilia Sucholutsky,Tania Lombrozo,Thomas L. Griffiths
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 注意您的步骤（步骤）：经过思考可以降低在思维使人类更糟的任务上的绩效
- **领域**: 机器学习,人工智能,计算语言学,计算机与社会
- **摘要**: 经过思考链（COT）提示已成为使用大型语言和多模型模型的广泛使用的策略。尽管已证明COT可以改善许多任务的性能，但确定其有效的设置仍然是一项持续的努力。特别是，在哪些设置中，这仍然是一个空旷的问题，可以系统地降低模型性能。在本文中，我们试图确定任务的特征，即COT通过从认知心理学中汲取灵感来降低绩效，查看（i）（i）口头思维或审议会损害人类表现的案例，以及（ii）管理人类表现的约束对语言模型的推广。三种情况是隐式统计学习，视觉识别以及包含异常模式的分类。在所有三种设置的广泛实验中，我们发现，与零照片相比，使用推力时间推理时，多样化的最先进模型的性能表现出明显的性能下降（例如，与GPT-4O相比，OpenAI O1-Preview的绝对准确性高达36.3％）。我们还确定了满足条件（i）但不能（ii）的三个任务，并发现尽管口头思维降低了这些任务中的人类绩效，但COT保留或提高了模型性能。总体而言，我们的结果表明，虽然模型的认知过程与人类的认知过程之间没有完全的相似之处，但考虑到思维对人类绩效产生负面影响的情况，可以帮助我们确定其对模型产生负面影响的设置。通过将有关人类审议的文献与COT的评估联系起来，我们提供了一种新工具，可以用于理解迅速选择和推理时间推理的影响。

### Unsupervised Multimodal Fusion of In-process Sensor Data for Advanced Manufacturing Process Monitoring 
[[arxiv](https://arxiv.org/abs/2410.22558)] [[cool](https://papers.cool/arxiv/2410.22558)] [[pdf](https://arxiv.org/pdf/2410.22558)]
> **Authors**: Matthew McKinney,Anthony Garland,Dale Cillessen,Jesse Adamczyk,Dan Bolintineanu,Michael Heiden,Elliott Fowler,Brad L. Boyce
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: 无监督的多模式融合，用于高级制造过程监视的过程内传感器数据
- **领域**: 机器学习,信号处理
- **摘要**: 有效监测制造过程对于维持产品质量和运营效率至关重要。现代制造环境会产生大量的多模式数据，包括从各种角度和分辨率，高光谱数据以及机器健康监测信息（例如执行器位置，加速度计读数和温度测量）的信息。但是，解释这个复杂的高维数据提出了重大挑战，尤其是当标记的数据集不可用时。本文提出了一种新的在制造过程中的多模式传感器数据融合的方法，灵感来自对比性语言图像预训练（剪辑）模型。我们利用对比度学习技术将不同的数据模式相关联，而无需标记数据，开发了五种不同模式的编码器：视觉图像，音频信号，激光位置（X和Y坐标）和激光功率测量。通过将这些高维数据集压缩为低维代表空间，我们的方法促进了下游任务，例如过程控制，异常检测和质量保证。我们通过实验评估了方法的有效性，证明了它可以增强高级制造系统中的过程监测功能的潜力。这项研究通过为多模式数据融合提供灵活，可扩展的框架来促进智能制造，该框架可以适应各种制造环境和传感器配置。

### Multimodal Structure Preservation Learning 
[[arxiv](https://arxiv.org/abs/2410.22520)] [[cool](https://papers.cool/arxiv/2410.22520)] [[pdf](https://arxiv.org/pdf/2410.22520)]
> **Authors**: Chang Liu,Jieshi Chen,Lee H. Harrison,Artur Dubrawski
> **First submission**: 2024-10-29
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: 多模式结构保存学习
- **领域**: 机器学习
- **摘要**: 当选择数据以在实际应用中构建机器学习模型时，诸如可用性，获取成本和歧视性能力之类的因素是至关重要的考虑因素。不同的数据模式通常会捕获基本现象的独特方面，从而使其公用事业互补。另一方面，一些数据主机结构信息的来源是其价值的关键。因此，有时可以通过匹配另一种数据的结构来增强一种数据类型的实用性。我们将多模式结构保存学习（MSPL）作为一种新型学习数据表示的新方法，利用一种数据模式提供的聚类结构，以增强来自另一种模式的数据的效用。我们证明了MSPL在合成时间序列数据中发现潜在结构的有效性，并使用质谱数据支持整个基因组测序和抗菌抗性数据中恢复簇，以支持流行病学应用。结果表明，MSPL可以将学习的特征与外部结构浸入，并有助于获得不同数据方式的有益协同作用。

### Analytic Continual Test-Time Adaptation for Multi-Modality Corruption 
[[arxiv](https://arxiv.org/abs/2410.22373)] [[cool](https://papers.cool/arxiv/2410.22373)] [[pdf](https://arxiv.org/pdf/2410.22373)]
> **Authors**: Yufei Zhang,Yicheng Xu,Hongxin Wei,Zhiping Lin,Huiping Zhuang
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: 多模式腐败的分析持续测试时间适应
- **领域**: 机器学习,人工智能
- **摘要**: 测试时间适应（TTA）旨在仅使用预先训练的模型和未标记的测试数据来帮助预训练的模型桥接源和目标数据集之间的差距。 TTA的一个关键目的是解决由腐败引起的测试数据中的域变化，例如天气变化，噪声或传感器故障。多模式连续测试时间适应（MM-CTTA）是具有更好现实世界应用的TTA扩展，进一步允许预训练的模型处理多模式输入并适应不断变化的目标域。 MM-CTTA通常面临挑战，包括误差积累，灾难性遗忘和可靠性偏见，而现有的方法很少有效地解决多模式腐败方案中的这些问题。在本文中，我们提出了一种新的方法，即用于MM-CTTA任务的多模式动力学适配器（MDAA）。我们使用分析分类器（ACS）创新地将分析学习引入TTA，以防止模型遗忘。此外，我们开发动态选择机制（DSM）和软伪标签策略（SPS），这使MDAA能够动态过滤可靠的样本并整合来自不同模态的信息。广泛的实验表明，MDAA在MM-CTTA任务上实现最先进的性能，同时确保可靠的模型适应。

### Prosody as a Teaching Signal for Agent Learning: Exploratory Studies and Algorithmic Implications 
[[arxiv](https://arxiv.org/abs/2410.23554)] [[cool](https://papers.cool/arxiv/2410.23554)] [[pdf](https://arxiv.org/pdf/2410.23554)]
> **Authors**: Matilda Knierim,Sahil Jain,Murat Han Aydoğan,Kenneth Mitra,Kush Desai,Akanksha Saran,Kim Baraka
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: Published at the 26th ACM International Conference onMultimodalInteraction (ICMI) 2024
- **标题**: 韵律作为代理学习的教学信号：探索性研究和算法含义
- **领域**: 机器学习,人机交互
- **摘要**: 从人类互动中学习的特工通常依赖于明确的信号，但是内隐的社会提示，例如语音中的韵律，可以为更有效的学习提供有价值的信息。本文主张将韵律作为教学信号的整合，以增强对人类教师的学习者的学习。通过两项探索性研究 - 一项在交互式增强学习设置中检查语音反馈，以及其他分析三个Atari游戏中人类示范的限制音频的研究 - 我们证明韵律具有有关任务动态的重要信息。我们的发现表明，当韵律特征与明确的反馈结合在一起时，可以增强增强学习成果。此外，我们提出了韵律敏感算法设计指南，并讨论有关教学行为的见解。我们的工作强调了利用韵律作为更有效的代理学习的隐式信号的潜力，从而推进了人类代理人的相互作用范例。

### Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning 
[[arxiv](https://arxiv.org/abs/2410.22788)] [[cool](https://papers.cool/arxiv/2410.22788)] [[pdf](https://arxiv.org/pdf/2410.22788)]
> **Authors**: Yiqin Lv,Qi Wang,Dong Liang,Zheng Xie
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: No comments
- **标题**: 元学习中尾部任务风险最小化的理论调查和实践增强
- **领域**: 机器学习
- **摘要**: 在大型模型时代，元学习是一个有希望的范式，在现实世界中，任务分配鲁棒性已成为必不可少的考虑因素。最近的进步研究了尾巴任务风险最小化在快速适应性改善中的有效性\ citep {wang2023simple}。这项工作有助于该领域的更多理论研究和实践增强。具体而言，我们将分布鲁棒的策略降低到最大值的优化问题，构成Stackelberg均衡作为解决方案概念，并估计收敛速率。在存在尾巴风险的情况下，我们进一步得出了概括性结合，与估计的分位数建立连接，并实际上改善了研究的策略。因此，广泛的评估证明了我们的提案的重要性及其对多模式大型模型的可伸缩性的重要性。

## 多代理系统(cs.MA:Multiagent Systems)

该领域共有 1 篇论文

### MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation 
[[arxiv](https://arxiv.org/abs/2410.13757)] [[cool](https://papers.cool/arxiv/2410.13757)] [[pdf](https://arxiv.org/pdf/2410.13757)]
> **Authors**: Zichen Zhu,Hao Tang,Yansi Li,Dingye Liu,Hongshen Xu,Kunyao Lan,Danyang Zhang,Yixuan Jiang,Hao Zhou,Chenrun Wang,Situo Zhang,Liangtai Sun,Yixiao Wang,Yuheng Sun,Lu Chen,Kai Yu
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: NAACL 2025 Demo Track
- **标题**: MOBA：多方体内存增强的自适应计划，以进行有效的移动任务自动化
- **领域**: 多代理系统,人工智能,计算语言学,人机交互
- **摘要**: 现有的基于多模式的大型语言模型（MLLM）代理在处理设备上处理复杂的GUI（图形用户界面）交互方面面临重大挑战。这些挑战是由GUI环境的动态和结构化性质引起的，GUI环境会整合文本，图像和空间关系，以及不同页面和任务的动作空间的可变性。为了解决这些局限性，我们提出了一种新型的基于MLLM的移动助理系统MOBA。 MOBA引入了一个自适应计划模块，该模块结合了错误恢复的反射机制，并动态调整计划以与真实的环境环境和动作模块的执行能力保持一致。此外，多面内存模块提供了全面的内存支持，以提高适应性和效率。我们还提供Mobbench，这是一个专为复杂移动交互的数据集。关于Mobbench和Androidarena的实验结果表明，MOBA能够处理动态的GUI环境并执行复杂的移动任务。

## 多媒体(cs.MM:Multimedia)

该领域共有 2 篇论文

### RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training 
[[arxiv](https://arxiv.org/abs/2410.14154)] [[cool](https://papers.cool/arxiv/2410.14154)] [[pdf](https://arxiv.org/pdf/2410.14154)]
> **Authors**: Muhe Ding,Yang Ma,Pengda Qin,Jianlong Wu,Yuhong Li,Liqiang Nie
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 10 pages, 6 figures, Journal
- **标题**: RA-Blip：多模式自适应检索 - 启动引导语言图像预训练
- **领域**: 多媒体,人工智能
- **摘要**: 多模式的大语言模型（MLLM）最近引起了极大的兴趣，这表明了它们作为各种视觉任务的通用模型的新兴潜力。 MLLM在其参数中涉及重要的外部知识；但是，不断使用最新知识不断更新这些模型是一项挑战，涉及巨大的计算成本和不良的解释性。事实证明，检索增强技术对于LLM和MLLM都是有效的插件。在这项研究中，我们提出了多模式自适应检索式的引导语言图像预训练（RA-Blip），这是一种新型的各种MLLM的检索式框架。考虑到视觉方式中的冗余信息，我们首先利用该问题来指导视觉信息通过与一组可学习的查询进行交互，从而最大程度地减少检索和生成过程中的无关干扰。此外，我们引入了预先训练的多模式自适应融合模块，以实现问题到文本到媒体的检索，并通过将视觉和语言模态投影到统一的语义空间中，从而实现多模式知识的整合。此外，我们提出了一种自适应选择知识生成（ASKG）策略，以训练发电机以自主识别检索知识的相关性，从而实现了出色的DeNoSIS绩效。在开放的多模式提问数据集上进行的广泛实验表明，RA-Blip可以实现重要的性能并超过最新的检索模型。

### Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction 
[[arxiv](https://arxiv.org/abs/2410.21169)] [[cool](https://papers.cool/arxiv/2410.21169)] [[pdf](https://arxiv.org/pdf/2410.21169)]
> **Authors**: Qintong Zhang,Victor Shea-Jay Huang,Bin Wang,Junyuan Zhang,Zhengren Wang,Hao Liang,Shawn Wang,Matthieu Lin,Conghui He,Wentao Zhang
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 文档解析揭幕：结构化信息提取的技术，挑战和前景
- **领域**: 多媒体,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 文档解析对于转换非结构化和半结构化文件至关重要，例如合同，学术论文和发票 -  ininto结构化的机器可读数据。文档解析从非结构化输入中提取可靠的结构化数据，为众多应用程序提供了巨大的便利。尤其是在大型语言模型中取得的最新成就，文档解析在知识基础构建和培训数据生成中都起着必不可少的作用。这项调查对文档解析的当前状态进行了全面综述，涵盖了关键方法，从模块化管道系统到由大型视觉模型驱动的端到端模型。详细检查了诸如布局检测，内容提取（包括文本，表和数学表达式）以及多模式数据集成之类的核心组件。此外，本文讨论了模块化文档解析系统和视觉模型在处理复杂布局，集成多个模块以及识别高密度文本时面临的挑战。它强调了开发更大，更多样化的数据集并概述未来研究方向的重要性。

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

该领域共有 3 篇论文

### MC-QDSNN: Quantized Deep evolutionary SNN with Multi-Dendritic Compartment Neurons for Stress Detection using Physiological Signals 
[[arxiv](https://arxiv.org/abs/2410.04992)] [[cool](https://papers.cool/arxiv/2410.04992)] [[pdf](https://arxiv.org/pdf/2410.04992)]
> **Authors**: Ajay B S,Phani Pavan K,Madhav Rao
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: 13 pages, 15 figures. Published to IEEE Transactions on Computer Aided Design
- **标题**: MC-QDSNN：使用生理信号进行多个树枝室神经元进行量化的深度进化SNN，以进行压力检测
- **领域**: 神经和进化计算,机器学习
- **摘要**: 长期的短期内存（LSTM）已成为用于分析和推断时间序列数据的确定网络。 LSTM具有提取光谱特征和时间特征的混合物的能力。由于这种好处，针对针对时间序列数据的尖峰对应物探索了类似的特征提取方法。尽管LSTM的尖峰形式表现良好，但它们倾向于计算和力量密集型。在解决此问题时，这项工作提出了多室泄漏（Mcleaky）神经元作为有效处理时间序列数据的可行替代方案。源自泄漏的集成和火（LIF）神经元模型的mcleaky神经元包含多个相互连接以形成记忆成分的复发突触，从而模拟了人脑的海马区域。提出的基于McLeaky神经元的尖峰神经网络模型及其量化的变体是针对最先进的（SOTA）SPIKING LSTMS基准的，以通过比较计算要求，延迟需求，延迟和现实世界中的表现与通过神经结构搜索（NAS）得出的模型（NAS）进行比较，以执行人体压力检测。结果表明，具有mcleaky活化神经元的网络的高度精度为98.8％，可根据电动活动（EDA）信号检测应力，比任何其他研究的模型都要好，而平均参数少20％。还测试了McLeaky Neuron的各种信号，包括EDA腕和胸部，温度，ECG以及它们的组合。量化的McLeaky模型也已得出并验证，以预测其在硬件体系结构上的性能，从而导致91.84％的精度。对神经元进行了多种数据方式，以朝压力检测进行多种数据模式，这导致节省25.12倍至39.20倍，而EDP的增长率为ANN的52.37倍至81.9倍，而与SOTA实施的其余实施相比，最佳准确度为98.8％。

### Learning Graph Quantized Tokenizers for Transformers 
[[arxiv](https://arxiv.org/abs/2410.13798)] [[cool](https://papers.cool/arxiv/2410.13798)] [[pdf](https://arxiv.org/pdf/2410.13798)]
> **Authors**: Limei Wang,Kaveh Hassani,Si Zhang,Dongqi Fu,Baichuan Yuan,Weilin Cong,Zhigang Hua,Hao Wu,Ning Yao,Bo Long
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 学习图量化了变压器的引物
- **领域**: 神经和进化计算,人工智能,机器学习
- **摘要**: 变压器是基础模型的骨干架构，在该模型中，特定于域特异性的令牌器可以帮助它们适应各种域。 Graph Transformers（GTS）最近成为几何深度学习的领先模型，在各种图形学习任务中都优于图形神经网络（GNNS）。但是，用于图形的引物的发展落后于其他方式，现有方法依赖于启发式方法或与变压器共同训练的GNN。为了解决这个问题，我们介绍了GQT（\ textbf {g} Raph \ textbf {q} uantized \ textbf {t} okenizer），通过利用多任务训练，将令牌培训从变压器培训中解除了多任务培训，从而使多任务图形自我求助于自助图，从而产生了鲁棒和可靠的图形图形tokens tokens。此外，GQT利用残留矢量量化（RVQ）学习层次离散令牌，从而显着降低了内存需求并提高了概括能力。通过将GQT与令牌调制相结合，变压器编码器可以在18个基准测试中的16个（包括大规模同质量和异性数据集）中实现最先进的性能。该代码可在以下网址找到：https：//github.com/limei0307/graph-tokenizer

### Multiple Global Peaks Big Bang-Big Crunch Algorithm for Multimodal Optimization 
[[arxiv](https://arxiv.org/abs/2410.18102)] [[cool](https://papers.cool/arxiv/2410.18102)] [[pdf](https://arxiv.org/pdf/2410.18102)]
> **Authors**: Fabio Stroppa,Ahmet Astar
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-24
> **comment**: 16 pages
- **标题**: 多模式优化的多个全球峰值大爆炸型紧​​缩算法
- **领域**: 神经和进化计算,人工智能
- **摘要**: 多模式优化问题的主要挑战是在具有不规则景观的多维搜索空间中识别具有高精度的多个峰。这项工作提出了多个全球峰值Big Bang-Big Crunch（MGP-BBC）算法，该算法通过为每个操作员引入专业机制来解决多模式优化问题的挑战。该算法扩展了Big Bang-Big Crunch算法，这是一种受宇宙进化启发的最先进的元疗法。具体而言，MGP-BBBC将人口中最好的个体分为基于集群的质量中心，然后以逐渐降低干扰以确保收敛性扩大。在此过程中，它（i）应用了基于距离的过滤来消除不必要的精英，以使较小峰上的精英不会丢失，（ii）在聚类后根据其利基计数促进孤立的个体，（iii）在OffSpring生成期间生成目标特定准确性水平的估算和剥削。二十个多模式基准测试功能的实验结果表明，与其他最先进的多模式优化器相对于其他最先进的多模式优化器，MGP-BBBC通常表现更好或竞争性。

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

该领域共有 2 篇论文

### ENWAR: A RAG-empowered Multi-Modal LLM Framework for Wireless Environment Perception 
[[arxiv](https://arxiv.org/abs/2410.18104)] [[cool](https://papers.cool/arxiv/2410.18104)] [[pdf](https://arxiv.org/pdf/2410.18104)]
> **Authors**: Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: ENWAR：无线环境感知的RAG授权多模式LLM框架
- **领域**: 网络和互联网架构,人工智能
- **摘要**: 大型语言模型（LLM）在促进6G和超越网络的网络管理和编排方面具有巨大的希望。但是，现有的LLM在特定领域的知识中受到限制及其处理多模式感觉数据的能力，这对于动态无线环境中的实时情境意识至关重要。本文通过引入ENWAR来解决这一差距，Enwar是一种环境意识的检索增强生成赋权的多模式LLM框架。 Enwar无缝地集成了多模式的感觉输入，以感知，解释和认知上处理复杂的无线环境，以提供人性化的情境意识。在GPS，LIDAR和摄像机模态组合中对ENWAR进行了评估，以及最先进的LLM，例如Mistral-7b/8x7b和Llama3.1-8/70/405B。与这些香草LLM的一般且经常表面的环境描述相比，Enwar提供了更丰富的空间分析，准确地识别位置，分析障碍并评估车辆之间的视线。结果表明，Enwar达到了高达70％相关性，55％的上下文回忆，80％的正确性和86％的忠诚的关键绩效指标，这表明了其在多模式感知和解释中的功效。

### Wireless-Friendly Window Position Optimization for RIS-Aided Outdoor-to-Indoor Networks based on Multi-Modal Large Language Model 
[[arxiv](https://arxiv.org/abs/2410.20691)] [[cool](https://papers.cool/arxiv/2410.20691)] [[pdf](https://arxiv.org/pdf/2410.20691)]
> **Authors**: Jinbo Hou,Kehai Qiu,Zitian Zhang,Yong Yu,Kezhi Wang,Stefano Capolongo,Jiliang Zhang,Zeyang Li,Jie Zhang
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 基于多模式大语言模型的RIS辅助室外室内网络的无线窗口位置优化
- **领域**: 网络和互联网架构,机器学习,信号处理
- **摘要**: 本文旨在通过调整窗户的位置以及利用大型语言模型（LLM）作为优化者作为RIS辅助的室外室外（O2i）网络来调整窗户的位置以及窗户式可重构智能表面（RISS），同时优化室内无线和日光性能。首先，我们说明了RIS AID的O2I网络的无线和日光系统模型，并制定了一个联合优化问题，以增强无线交通和日光照明性能。然后，我们提出了一个基于多模式LLM的窗口优化（LMWO）框架，并伴随着一个及时的施工模板，以零拍摄方式优化整体性能，既可以充当建筑师和无线网络计划者。最后，我们分析了LMWO框架的优化性能以及窗户数量，房间大小，RIS单位数量和日光因素的影响。数值结果表明，与经典优化方法相比，我们提出的LMWO框架可以在初始性能，收敛速度，最终结果和时间复杂性方面实现出色的优化性能。该建筑物的无线性能可以显着提高，同时确保室内日光的性能。

## 机器人技术(cs.RO:Robotics)

该领域共有 31 篇论文

### Multimodal Coherent Explanation Generation of Robot Failures 
[[arxiv](https://arxiv.org/abs/2410.00659)] [[cool](https://papers.cool/arxiv/2410.00659)] [[pdf](https://arxiv.org/pdf/2410.00659)]
> **Authors**: Pradip Pramanick,Silvia Rossi
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: 机器人故障的多模式相干解释产生
- **领域**: 机器人技术,人工智能
- **摘要**: 机器人行动的解释性对于在社会空间中接受至关重要。解释为什么机器人无法完成给定任务，对于非专家用户来说，要了解机器人的功能和限制特别重要。到目前为止，关于解释机器人故障的研究仅考虑了产生文本解释，即使一些研究表明了多模式的解释。但是，多种模态的简单组合可能会导致各种方式之间的信息之间的语义不一致 - 这个问题尚未得到充分研究。一个不连贯的多模式解释可能很难理解，甚至可能与机器人和人类观察到的内容以及他们如何与观察进行推理变得不一致。这样的不一致可能会导致关于机器人能力的错误结论。在本文中，我们介绍了一种方法，通过检查不同方式的解释的逻辑相干性，然后根据需要进行改进，从而生成相干的多模式解释。我们提出了一种相干评估的分类方法，在该方法中，我们在逻辑上评估了解释是否遵循另一种解释。我们的实验表明，预先训练以识别文本需要进行的微调神经网络，可以很好地评估多模式解释。代码和数据：https：//pradippramanick.github.io/coherent-explain/。

### Task Success Prediction for Open-Vocabulary Manipulation Based on Multi-Level Aligned Representations 
[[arxiv](https://arxiv.org/abs/2410.00436)] [[cool](https://papers.cool/arxiv/2410.00436)] [[pdf](https://arxiv.org/pdf/2410.00436)]
> **Authors**: Miyu Goko,Motonari Kambara,Daichi Saito,Seitaro Otsuki,Komei Sugiura
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: Accepted for presentation at CoRL2024
- **标题**: 基于多层对齐表示
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 在这项研究中，我们考虑了基于指导句子和操纵前和以上的图像来预测操纵器开放式摄影作用的任务成功的问题。传统方法，包括多模式大语言模型（MLLM），通常无法适当理解对象和/或对象位置的细微变化的详细特征。我们提出了对比度$λ$  - 更换器，它通过将图像与指令句子对齐来预测桌面操纵任务的任务成功。我们的方法将以下三种关键功能类型集成到多级对齐表示形式中：保留本地图像信息的功能；与自然语言保持一致的功能；以及通过自然语言结构的特征。这使模型可以通过查看两个图像之间表示的差异来关注重要的更改。我们根据大规模标准数据集，RT-1数据集和物理机器人平台上的数据集上评估对比度$λ$  - 重复程序。结果表明，我们的方法优于包括MLLM在内的现有方法。与代表性的基于MLLM的模型相比，我们最佳模型的准确性提高了8.66点。

### Robo-MUTUAL: Robotic Multimodal Task Specification via Unimodal Learning 
[[arxiv](https://arxiv.org/abs/2410.01529)] [[cool](https://papers.cool/arxiv/2410.01529)] [[pdf](https://arxiv.org/pdf/2410.01529)]
> **Authors**: Jianxiong Li,Zhihao Wang,Jinliang Zheng,Xiaoai Zhou,Guanming Wang,Guanglu Song,Yu Liu,Jingjing Liu,Ya-Qin Zhang,Junzhi Yu,Xianyuan Zhan
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: preprint
- **标题**: 机器人 - 穆特（Robo-Mutual）：通过单峰学习的机器人多模式任务规范
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 多模式任务规范对于增强的机器人性能至关重要，其中\ textIt {交叉模式对齐}使机器人能够整体理解复杂的任务指令。直接注释模型训练的多模式指令由于配对多模式数据的稀疏性而被证明是不切实际的。在这项研究中，我们证明，通过利用真实数据中丰富的单形式指示，我们可以有效地教机器人学习多模式任务规格。首先，我们通过使用大量的室外数据来验证机器人多模式编码器，以强\ textIt {交叉模式对齐}功能赋予机器人。然后，我们采用两次崩溃和腐败的操作，进一步弥合了学识渊博的多模式表示的剩余模式差距。这种方法将相同的任务目标的不同模式与可互换表示形式相同，从而在良好的多模式潜在空间内实现了准确的机器人操作。对模拟的Libero基准和真实机器人平台进行了130多个任务和4000个评估的评估，展示了我们提出的框架的优势能力，在克服机器人学习中的数据约束方面表现出了重要优势。网站：zh1hao.wang/robo_mutual

### Multi-Robot Motion Planning with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2410.03072)] [[cool](https://papers.cool/arxiv/2410.03072)] [[pdf](https://arxiv.org/pdf/2410.03072)]
> **Authors**: Yorai Shaoul,Itamar Mishani,Shivam Vats,Jiaoyang Li,Maxim Likhachev
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: The first three authors contributed equally to this work. Under review for ICLR 2025
- **标题**: 通过扩散模型的多机器人运动计划
- **领域**: 机器人技术,人工智能,多代理系统
- **摘要**: 扩散模型最近已成功应用于从数据中学习复杂的多模式行为的广泛机器人应用。但是，由于学习多机器人扩散模型的样本复杂性很高，因此先前的工作主要仅限于单机器人和小规模环境。在本文中，我们提出了一种生成无碰撞的多机器人轨迹的方法，该轨迹仅使用单机器人数据而符合基础数据分布。我们的算法，多机器人多模型计划扩散（MMD），通过将学习的扩散模型与经典的基于基于搜索的技术相结合 - 在碰撞约束下生成数据驱动的动作。进一步扩展，我们展示了如何在单个扩散模型无法很好地概括的大环境中构成多个扩散模型。我们证明了在由物流环境动机的各种模拟场景中计划数十个机器人的有效性。在我们的补充材料中查看视频演示，以及我们的代码：https：//github.com/yoraish/mmd。

### Unpacking Failure Modes of Generative Policies: Runtime Monitoring of Consistency and Progress 
[[arxiv](https://arxiv.org/abs/2410.04640)] [[cool](https://papers.cool/arxiv/2410.04640)] [[pdf](https://arxiv.org/pdf/2410.04640)]
> **Authors**: Christopher Agia,Rohan Sinha,Jingyun Yang,Zi-ang Cao,Rika Antonova,Marco Pavone,Jeannette Bohg
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: Project page: https://sites.google.com/stanford.edu/sentinel. 35 pages, 9 figures. Accepted to the Conference on Robot Learning (CoRL) 2024
- **标题**: 解开生成策略的故障模式：一致性和进度的运行时监视
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 通过模仿学习训练的机器人行为政策在偏离训练数据的条件下容易失败。因此，必须在测试时监视学习政策并提供故障的早期警告的算法对于促进可扩展部署是必要的。我们提出了Sentinel，这是一个运行时监视框架，将失败的检测分为两个互补类别：1）不稳定的失败，我们使用时间动作一致性的统计量度来检测到，并且2）任务进程失败，在那里我们使用视觉语言模型（VLMS）在策略何时稳定地且一致地采取行动不解决任务。我们的方法有两个关键优势。首先，由于学到的政策表现出多种失败模式，因此将互补探测器组合起来会导致失败检测时明显更高的精度。其次，使用统计的时间动作一致性度量确保我们快速检测到以可忽略的计算成本表现出多模式的生成策略何时表现出不稳定的行为。相比之下，我们仅使用VLM来检测时间敏感的失败模式。我们在模拟和现实世界中对机器人移动操纵领域训练的扩散政策的背景下展示了我们的方法。通过统一时间一致性检测和VLM运行时监控，Sentinel检测到比单独使用两个检测器中的任何一个都多18％，并且显着超过了基线，从而突出了将专业检测器分配给互补类别失败的重要性。定性结果可从https://sites.google.com/stanford.edu/sentinel提供。

### Compositional Diffusion Models for Powered Descent Trajectory Generation with Flexible Constraints 
[[arxiv](https://arxiv.org/abs/2410.04261)] [[cool](https://papers.cool/arxiv/2410.04261)] [[pdf](https://arxiv.org/pdf/2410.04261)]
> **Authors**: Julia Briden,Yilun Du,Enrico M. Zucchelli,Richard Linares
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: Full manuscript submitted to IEEE Aerospace 2025 on 4-Oct-2024
- **标题**: 具有灵活约束的动力下降轨迹的组成扩散模型
- **领域**: 机器人技术,机器学习,系统与控制,优化与控制
- **摘要**: 这项工作介绍了Trajdiffuser，这是一种基于组成扩散的柔性和并发轨迹生成器，可用于6个自由度的下降下降指导。 trajdiffuser是一个统计模型，它可以学习模拟最佳轨迹数据集的多模式分布，每个分布只有一个或几个可能因不同轨迹而变化的约束。在推断期间，随着时间的推移，该轨迹是同时生成的，提供稳定的长马计划，并且可以将约束构成，从而提高模型的推广性并减少所需的训练数据。然后，生成的轨迹用于初始化优化器，从而提高其稳健性和速度。

### GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs 
[[arxiv](https://arxiv.org/abs/2410.03645)] [[cool](https://papers.cool/arxiv/2410.03645)] [[pdf](https://arxiv.org/pdf/2410.03645)]
> **Authors**: Pu Hua,Minghuan Liu,Annabella Macaluso,Yunfeng Lin,Weinan Zhang,Huazhe Xu,Lirui Wang
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: CoRL 2024. Project website: https://gensim2.github.io/
- **标题**: Gensim2：使用多模式和推理LLMS缩放机器人数据生成
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 由于人为创建各种模拟任务和场景所需的人类努力，如今的机器人模拟仍然具有挑战性。通过仿真训练的策略也面临可扩展性问题，因为许多SIM到现实方法都集中在单个任务上。为了应对这些挑战，这项工作提出了Gensim2，这是一个可扩展的框架，它利用具有多模式和推理功能的LLM来实现复杂和逼真的仿真任务创建，包括带有明确对象的长马操作。为了自动生成这些任务的演示数据，我们建议在对象类别中推广的计划和RL求解器。该管道可以生成最多100个具有200个对象的铰接任务的数据，并减少所需的人类努力。为了利用此类数据，我们提出了一种有效的多任务语言条件结构架构，称为本体感受点云变压器（PPT），该策略从生成的演示中学习，并展示了强大的SIM到现实零摄像转移。结合了提议的管道和政策体系结构，我们显示了Gensim2的有希望的用法，即生成的数据可用于零拍传输或与现实世界中收集的数据共同培训，这将策略绩效提高了20％，而专门针对有限的实际数据进行了培训。

### Enabling Novel Mission Operations and Interactions with ROSA: The Robot Operating System Agent 
[[arxiv](https://arxiv.org/abs/2410.06472)] [[cool](https://papers.cool/arxiv/2410.06472)] [[pdf](https://arxiv.org/pdf/2410.06472)]
> **Authors**: Rob Royce,Marcel Kaufmann,Jonathan Becktor,Sangwoo Moon,Kalind Carpenter,Kai Pak,Amanda Towler,Rohan Thakker,Shehryar Khattak
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: Preprint. Accepted at IEEE Aerospace Conference 2025, 16 pages, 12 figures
- **标题**: 实现新颖的任务操作以及与Rosa的互动：机器人操作系统代理
- **领域**: 机器人技术,人工智能,人机交互
- **摘要**: 机器人系统的进步彻底改变了许多行业，但他们的运营经常需要专业的技术知识，从而限制了非专家用户的可访问性。本文介绍了Rosa（机器人操作系统），这是一种由AI驱动的代理，它弥合了机器人操作系统（ROS）和自然语言接口之间的差距。通过利用最先进的语言模型并集成开源框架，Rosa使操作员能够使用自然语言与机器人进行交互，将命令转化为动作并通过定义明确的工具与ROS进行交互。 Rosa的设计是模块化的，可扩展的，可与Ros1和Ros2一起无缝集成，以及参数验证和约束执行等安全机制，以确保安全，可靠的操作。罗莎（Rosa）最初是为ROS设计的，但可以扩展到与其他机器人中型机器人合作，以最大程度地提高跨任务的兼容性。罗莎（Rosa）通过使对复杂的机器人系统的访问使所有专业水平的用户具有多模式能力，例如语音集成和视觉感知来增强人类机器人的互动。道德考虑得到了彻底解决，以诸如Asimov的三个机器人法则等基本原则为指导，确保AI整合促进了安全，透明度，隐私和问责制。通过使机器人技术更加用户友好和访问，Rosa不仅提高了运营效率，而且还为在机器人技术和潜在的未来任务运营中使用负责人的AI使用设定了新的标准。本文介绍了Rosa的架构，并在JPL的火星院子（实验室，以及使用三种不同机器人的模拟）中展示了初始模拟操作。核心Rosa库可作为开源。

### Context-Aware Command Understanding for Tabletop Scenarios 
[[arxiv](https://arxiv.org/abs/2410.06355)] [[cool](https://papers.cool/arxiv/2410.06355)] [[pdf](https://arxiv.org/pdf/2410.06355)]
> **Authors**: Paul Gajewski,Antonio Galiza Cerdeira Gonzalez,Bipin Indurkhya
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 桌面场景的上下文感知命令理解
- **领域**: 机器人技术,人工智能
- **摘要**: 本文介绍了一种新型的混合算法，旨在在桌面场景中解释自然人类命令。通过整合多个信息来源，包括语音，手势和场景上下文，系统可以为机器人提取可操作的说明，从而识别相关的对象和动作。该系统以零拍摄的方式运行，而无需依赖预定义的对象模型，从而在各种环境中具有灵活和适应性的使用。我们评估了多个深度学习模型的整合，评估了它们在现实世界机器人设置中的适用性。我们的算法在不同的任务中执行强大的性能，将语言处理与视觉接地相结合。此外，我们发布了用于评估系统的视频记录的小数据集。该数据集捕获了现实世界中的相互作用，其中人类向机器人提供自然语言的说明，这是对人类机器人互动的未来研究的贡献。我们讨论了系统的优势和局限性，尤其关注它如何处理多模式命令解释，并将其集成到符号机器人框架中以进行安全可解释的决策。

### Imitation Learning with Limited Actions via Diffusion Planners and Deep Koopman Controllers 
[[arxiv](https://arxiv.org/abs/2410.07584)] [[cool](https://papers.cool/arxiv/2410.07584)] [[pdf](https://arxiv.org/pdf/2410.07584)]
> **Authors**: Jianxin Bi,Kelvin Lim,Kaiqi Chen,Yifei Huang,Harold Soh
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 通过扩散计划者和Deep Koopman控制器进行有限动作的模仿学习
- **领域**: 机器人技术,机器学习
- **摘要**: 基于扩散的机器人策略的最新进展表明，在模仿多模式行为方面具有巨大的潜力。但是，这些方法通常需要大量的演示数据与相应的机器人动作标签配对，从而产生了实质性的数据收集负担。在这项工作中，我们提出了一个计划 - 对照框架，旨在通过利用观察性演示数据来提高反向动态控制器的动作数据效率。具体来说，我们采用深厚的Koopman操作员框架来对动态系统进行建模，并利用仅观察轨迹来学习潜在的动作表示。然后，可以使用线性动作解码器有效地将这种潜在表示形式有效地映射到真正的高维连续作用，需要最小的动作标记数据。通过对模拟机器人操纵任务的实验以及对多模式专家演示的真实机器人实验，我们证明我们的方法可以显着提高动作数据效率，并通过有限的动作数据实现高工作效率。

### Multimodal Perception System for Real Open Environment 
[[arxiv](https://arxiv.org/abs/2410.07926)] [[cool](https://papers.cool/arxiv/2410.07926)] [[pdf](https://arxiv.org/pdf/2410.07926)]
> **Authors**: Yuyang Sha
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: 真实开放环境的多模式感知系统
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 本文介绍了一种新型的多模式感知系统，以实现真正的开放环境。提出的系统包括一个嵌入式计算平台，相机，超声波传感器，GPS和IMU设备。与传统框架不同，我们的系统将多个传感器与先进的计算机视觉算法集成在一起，以帮助用户可靠地走出外部。该系统可以有效地完成各种任务，包括导航到特定位置，穿过障碍区域以及越过交叉点。具体而言，我们还使用超声波传感器和深度摄像机来增强避免障碍性能。路径规划模块旨在根据各种反馈和用户的当前状态找到本地最佳路由。为了评估拟议系统的性能，我们在不同情况下设计了几个实验。结果表明，该系统可以帮助用户在复杂的情况下有效，独立地行走。

### RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation 
[[arxiv](https://arxiv.org/abs/2410.07864)] [[cool](https://papers.cool/arxiv/2410.07864)] [[pdf](https://arxiv.org/pdf/2410.07864)]
> **Authors**: Songming Liu,Lingxuan Wu,Bangguo Li,Hengkai Tan,Huayu Chen,Zhengyi Wang,Ke Xu,Hang Su,Jun Zhu
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: 10 pages, conference
- **标题**: RDT-1B：双层操作的扩散基础模型
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 双人操作在机器人技术中至关重要，但是由于协调两个机器人臂（导致多模式的动作分布）和培训数据的稀缺性，开发基础模型非常具有挑战性。在本文中，我们介绍了机器人扩散变压器（RDT），这是一种双峰操纵的开创性扩散基础模型。 RDT建立在扩散模型的基础上，以有效地表示多模式，具有可扩展变压器的创新设计，以处理多模式输入的异质性，并捕获机器人数据的非线性和高频。为了解决数据稀缺性，我们进一步引入了一个可解释的统一动作空间，该空间可以统一各种机器人的动作表示，同时保留原始动作的物理含义，从而促进学习可转移的物理知识。借助这些设计，我们设法将train RDT预先介绍到迄今为止最大的多机器人数据集集合，并将其缩放到1.2B参数，这是最大的基于扩散的机器人操作基础模型。最终，我们在一个自创建的多任务双层数据集上微调了RDT，具有超过6K+的情节，以完善其操纵功能。对真实机器人的实验表明，RDT明显优于现有方法。它表现出零拍的概括，可以看不见的对象和场景，理解和遵循语言说明，仅1〜5演示学习新技能，并有效地处理复杂的，灵巧的任务。我们参考有关代码和视频的https://rdt-robotics.github.io/rdt-robotics/。

### Neural Semantic Map-Learning for Autonomous Vehicles 
[[arxiv](https://arxiv.org/abs/2410.07780)] [[cool](https://papers.cool/arxiv/2410.07780)] [[pdf](https://arxiv.org/pdf/2410.07780)]
> **Authors**: Markus Herb,Nassir Navab,Federico Tombari
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: Accepted at 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)
- **标题**: 自动驾驶汽车的神经语义图学习
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 自动驾驶汽车要求详细的地图通过交通可靠地操纵，这些交通需要保持最新状态以确保安全操作。将地图调整到不断变化的道路网络的一种有希望的方法是使用来自车队的人群数据。在这项工作中，我们提出了一个地图系统，该系统将在中央实例的车队中收集的本地订阅融合，以制作一张连贯的道路环境地图，包括可驱动区域，车道标记，电线杆，障碍物等3D网格。每辆车作为轻量级网格供应本地重建的子包，使我们的方法适用于广泛的重建方法和传感器方式。我们的方法共同使用特定于场景的神经签名距离字段来对齐并合并嘈杂和不完整的本地子膜，该距离距离距离距离距离，该距离距离距离距离距离距离距离距离距离，该场均使用subsap网格进行监督，以预测融合的环境表示。我们利用记忆有效的稀疏特征网格来扩展到大面积，并在场景重建中引入置信度评分以建模不确定性。我们的方法在两个具有不同局部映射方法的数据集上进行了评估，显示了对现有方法的改进的姿势比对和重建。此外，我们证明了多课程映射的好处，并检查了所需的数据数量，以实现自动驾驶汽车的高保真地图学习。

### PokeFlex: A Real-World Dataset of Volumetric Deformable Objects for Robotics 
[[arxiv](https://arxiv.org/abs/2410.07688)] [[cool](https://papers.cool/arxiv/2410.07688)] [[pdf](https://arxiv.org/pdf/2410.07688)]
> **Authors**: Jan Obrist,Miguel Zamora,Hehui Zheng,Ronan Hinchet,Firat Ozdemir,Juan Zarate,Robert K. Katzschmann,Stelian Coros
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: pokeflex：机器人技术的体积变形对象的现实世界数据集
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 数据驱动的方法在解决具有挑战性的操作任务方面表现出了巨大的潜力。但是，它们在可变形对象域中的应用部分受到了缺乏数据的一部分限制。为了解决这种缺乏，我们提出了PokeFlex，Pokeflex是一个具有成对和注释的现实世界多模式数据的数据集。模式包括3D纹理网格，点云，RGB图像和深度图。可以将这些数据用于多个下游任务，例如在线3D网格重建，并且可以潜在地启用未充满震惊的应用程序，例如基于网格模拟的传统控制方法的真实部署。为了应对现实世界3D网状重建所带来的挑战，我们利用了一种专业的体积捕获系统，该系统允许完整的360°重建。 Pokeflex由18个具有不同刚度和形状的可变形物体组成。通过将物体滴到平坦的表面或用机器人臂戳对象来产生变形。在后一种情况下，还报告了交互扳手和接触位置。使用不同的数据模式，我们为数据集培训模型展示了一个用例，鉴于Pokeflex的多模式性质的新颖性构成了从多模式数据的基于多对象在线模板的网格重建中的最新技术，构成了我们的最佳知识。我们将读者转到我们的网站（https://pokeflex-dataset.github.io/），以获取更多演示和示例。

### The Ingredients for Robotic Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2410.10088)] [[cool](https://papers.cool/arxiv/2410.10088)] [[pdf](https://arxiv.org/pdf/2410.10088)]
> **Authors**: Sudeep Dasari,Oier Mees,Sebastian Zhao,Mohan Kumar Srirama,Sergey Levine
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 机器人扩散变压器的成分
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 近年来，机器人主义者通过利用高容量变压器网络架构和生成扩散模型来解决灵巧机器人硬件的越来越一般任务取得了显着的进步。不幸的是，将这两个正交改进结合起来已被证明是令人惊讶的，因为没有明确且妥善理解的过程来做出重要的设计选择。在本文中，我们确定，研究和改善了高容量扩散变压器政策的关键建筑设计决策。最终的模型可以在多个机器人实施方案上有效地解决各种任务，而无需每设定的超参数调整的痛苦。通过将调查的结果与改进的模型组件相结合，我们能够提出一种名为\ Method的新型体系结构，该架构在解决双重人类Aloha Aloha机器人上的解决方案（1500美元+$ $ time Steps）在解决长期（$ 1500+$ $ time Steps）方面极大地表现出色。此外，我们发现，在10小时的高度多模式，注释的Aloha演示数据中接受了10个小时的培训时，我们的政策表现出改善的缩放性能。我们希望这项工作将为未来的机器人学习技术打开大门，以利用大型变压器体系结构的可扩展性来利用生成扩散建模的效率。代码，机器人数据集和视频可在以下网址找到：https：//dit-policy.github.io

### VQ-CNMP: Neuro-Symbolic Skill Learning for Bi-Level Planning 
[[arxiv](https://arxiv.org/abs/2410.10045)] [[cool](https://papers.cool/arxiv/2410.10045)] [[pdf](https://arxiv.org/pdf/2410.10045)]
> **Authors**: Hakan Aktas,Emre Ugur
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: 12 pages, 6 figures, Submitted to Conference on Robot Learning LEAP Workshop 2024
- **标题**: VQ-CNMP：双层计划的神经符号技能学习
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 本文提出了一个新型的神经网络模型，能够从未标记的演示数据中发现高级技能表示。我们还建议使用基于梯度的计划方法利用我们的模型的双层规划管道。在提取高级表示形式的同时，我们的模型还保留了低级信息，可用于低级动作计划。在实验中，我们在不同条件下测试了模型的技能发现性能，测试了是否可以利用多模式LLM来标记学习的高级技能表示，并最终测试了我们管道的高级和低级计划性能。

### Generalizable Spacecraft Trajectory Generation via Multimodal Learning with Transformers 
[[arxiv](https://arxiv.org/abs/2410.11723)] [[cool](https://papers.cool/arxiv/2410.11723)] [[pdf](https://arxiv.org/pdf/2410.11723)]
> **Authors**: Davide Celestini,Amirhossein Afsharrad,Daniele Gammelli,Tommaso Guffanti,Gioele Zardini,Sanjay Lall,Elisa Capello,Simone D'Amico,Marco Pavone
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: 8 pages, 6 figures, submitted to 2025 American Control Conference (ACC)
- **标题**: 通过多模式学习的变压器，可概括的航天器轨迹生成
- **领域**: 机器人技术,人工智能,优化与控制
- **摘要**: 有效的轨迹产生对于可靠的车载航天器自主权至关重要。除其他方法外，基于学习的温暖启动代表了解决轨迹生成问题的有吸引力的范式，有效地结合了优化和数据驱动方法的好处。当前的基于学习的轨迹生成的方法通常集中在固定的单幕环境上，其中关键场景特征（例如障碍位置或最终时间要求）在问题实例中保持不变。但是，实用的轨迹产生需要经常重新配置该方案，这使得单幕方法成为潜在不切实际的解决方案。为了应对这一挑战，我们提出了一个新型的轨迹生成框架，该框架通过利用能够从多模式数据源学习的高容量变压器神经网络跨越各种问题配置。具体而言，我们的方法将基于变压器的神经网络模型集成到轨迹优化过程中，并通过多模态表示编码场景级信息（例如障碍位置，初始和目标状态）和轨迹级约束（例如，时间范围，燃油消耗目标）。然后，变压器网络为非凸优化问题生成了近乎最佳的初始猜测，从而显着提高了收敛速度和性能。该框架通过在自由飞行平台上进行的广泛模拟和现实世界实验进行验证，相对于传统方法，可观案例的成本提高了30％，并降低了80％，并在不同方案变化方面证明了强大的概括。

### DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment 
[[arxiv](https://arxiv.org/abs/2410.11584)] [[cool](https://papers.cool/arxiv/2410.11584)] [[pdf](https://arxiv.org/pdf/2410.11584)]
> **Authors**: Wendi Chen,Han Xue,Fangyuan Zhou,Yuan Fang,Cewu Lu
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: forormpam：通过基于偏好的动作对齐的长马可变形对象操纵的数据效率学习
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **摘要**: 近年来，模仿学习在机器人操作领域取得了进步。但是，在处理复杂的长马可变形对象任务时，它仍然面临挑战，例如高维状态空间，复杂的动力学和多模式的动作分布。传统的模仿学习方法通​​常需要大量数据，并在这些任务中遇到分配变化和累积错误。为了解决这些问题，我们根据偏好学习和奖励指导的行动选择提出了一个数据有效的一般学习框架（DeformPAM）。 DeformPAM将长马操作任务分解为多个动作基底物，利用3D点云输入和扩散模型来建模动作分布，并使用人类偏好数据训练隐式奖励模型。在推理阶段，奖励模型得分多个候选动作，选择了执行的最佳动作，从而减少了异常动作的发生并提高了任务完成质量。对三个具有挑战性的现实世界长胜利对象操纵任务进行的实验证明了该方法的有效性。结果表明，与基线方法相比，DeformPAM可以提高任务完成质量和效率，即使数据有限。代码和数据将在https://deform-pam.robotflow.ai上找到。

### PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model 
[[arxiv](https://arxiv.org/abs/2410.11564)] [[cool](https://papers.cool/arxiv/2410.11564)] [[pdf](https://arxiv.org/pdf/2410.11564)]
> **Authors**: Shang-Ching Liu,Van Nhiem Tran,Wenkai Chen,Wei-Lun Cheng,Yen-Lin Huang,I-Bin Liao,Yung-Hui Li,Jianwei Zhang
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: PAVLM：通过视觉语言模型推进基于点云的负担能力理解
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 负担得起的理解，确定3D对象上可行区域的任务，在允许机器人系统在物理世界中互动和运营方面起着至关重要的作用。尽管视觉语言模型（VLMS）在用于机器人操作的高级推理和长途计划方面表现出色，但它们仍然缺乏掌握有效的人类机器人相互作用所需的细微物理特性。在本文中，我们介绍了PAVLM（Point Cloud Profise Vision-Language模型），这是一个创新的框架，它利用了预训练的语言模型中嵌入的广泛的多模式知识来增强对点云的3D负担能力理解。 PAVLM与大型语言模型（LLMS）的隐藏嵌入方式集成了几何引导的传播模块，以丰富视觉语义。在语言方面，我们提示Llama-3.1模型生成精致的上下文感知文本，从而通过更深入的语义提示来增强教学输入。 3D-Affordancenet基准的实验结果表明，PAVLM的表现优于全部和部分点云的基线方法，尤其是它对3D对象的新型开放世界负担能力任务的概括。有关更多信息，请访问我们的项目网站：pavlm-source.github.io。

### Flex: End-to-End Text-Instructed Visual Navigation with Foundation Models 
[[arxiv](https://arxiv.org/abs/2410.13002)] [[cool](https://papers.cool/arxiv/2410.13002)] [[pdf](https://arxiv.org/pdf/2410.13002)]
> **Authors**: Makram Chahine,Alex Quach,Alaa Maalouf,Tsun-Hsuan Wang,Daniela Rus
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: :68T40; 68T05; 68T50ACM Class:I.2.6; I.2.9; I.2.10; I.4.8
- **标题**: flex：带基础模型的端到端文本实施视觉导航
- **领域**: 机器人技术,人工智能
- **摘要**: 端到端的学习将感官输入直接映射到动作，为复杂的机器人任务创建高度集成和有效的策略。但是，这样的模型很难有效地训练，并且经常难以推广其训练场景，限制对新环境，任务和概念的适应性。在这项工作中，我们研究了在看不见的文本说明和视觉分布变化下，通过基于视觉的控制策略实现强大的闭环性能所需的最小数据需求和体系结构适应。为此，我们通过利用多模式基础模型编码器并评估不同策略网络负责人的适用性来设计具有各种数据表示丰富度的数据集，完善功能提取协议。我们的发现是在Flex（Fly-fly-fly）中合成的，该框架使用预训练的视觉语言模型（VLM）作为冷冻贴片特征提取器，从而生成具有空间意识的嵌入，从而集成了语义和视觉信息。这些丰富的功能构成了训练高度强大的下游政策，能够跨平台，环境和文本指定任务概括。我们证明了这种方法对四型飞行目标任务的有效性，在该任务中，通过行为克隆在一个小的模拟数据集上训练的代理成功地将其推广到现实世界场景，从而处理各种新颖的目标和命令公式。

### Coherence-Driven Multimodal Safety Dialogue with Active Learning for Embodied Agents 
[[arxiv](https://arxiv.org/abs/2410.14141)] [[cool](https://papers.cool/arxiv/2410.14141)] [[pdf](https://arxiv.org/pdf/2410.14141)]
> **Authors**: Sabit Hassan,Hye-Young Chung,Xiang Zhi Tan,Malihe Alikhani
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: To appear at AAMAS, 2025
- **标题**: 相干驱动的多模式安全对话与体现代理的主动学习
- **领域**: 机器人技术,计算语言学
- **摘要**: 在协助人们进行日常任务时，机器人需要准确地解释视觉线索并在不同的关键安全情况下（例如地板上尖锐的物体）有效做出响应。在这种情况下，我们提出了M-Codal，这是一种专门为体现的代理而设计的多模式数字系统，以在安全至关重要的情况下更好地理解和交流。该系统利用话语连贯的关系来增强其上下文理解和沟通能力。为了训练该系统，我们介绍了一种基于聚类的活跃学习机制，该机制利用外部大语言模型（LLM）来确定信息的实例。使用新创建的多模式数据集评估我们的方法，该数据集包括从2K Reddit图像中提取的1K安全性违规行为。这些违规使用大型多模型（LMM）注释，并通过人体注释验证。该数据集的结果表明，我们的方法改善了安全情况，用户情绪以及对话的安全性的解决。接下来，我们将对话系统部署在Hello机器人弹力机器人上，并与现实世界参与者进行受试者内部用户研究。在这项研究中，参与者的角色扮演两个安全方案与机器人的严重程度不同，并从我们的模型中接受了干预措施，以及由OpenAI的Chatgpt提供支持的基线系统。该研究结果证实并扩展了自动化评估的发现，表明我们所提出的系统在现实世界中体现的代理设置中更具说服力。

### Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand 
[[arxiv](https://arxiv.org/abs/2410.14022)] [[cool](https://papers.cool/arxiv/2410.14022)] [[pdf](https://arxiv.org/pdf/2410.14022)]
> **Authors**: Cheng Pan,Kai Junge,Josie Hughes
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 视觉语言动作模型和扩散策略切换可以对拟人化手的灵巧控制
- **领域**: 机器人技术,人工智能
- **摘要**: 为了推进自动灵活的灵活性操纵，我们提出了一种混合控制方法，该方法结合了微调视力语言操作（VLA）模型和扩散模型的相对优势。 VLA模型提供了高度可概括的语言高级计划，而扩散模型则处理低级交互，这提供了特定对象和环境所需的精确性和鲁棒性。通过将开关信号合并到训练数据中，我们启用了这两个模型之间的基于事件的过渡，以通过语言命令目标对象和位置位置。这种方法部署在我们的拟人化适应性手2上，这是一种13DOF机器人手，通过串联弹性驱动结合了合规性，从而使任何相互作用具有韧性：显示首次使用由VLA模型控制的多指手动控制。我们证明了这种模型切换方法导致超过80 \％的成功率，而仅当使用VLA模型时，通过VLA模型的准确近观察手臂运动和多模式的握把运动，并具有从扩散模型中的误差恢复能力。

### Diffusion Transformer Policy: Scaling Diffusion Transformer for Generalist Vision-Language-Action Learning 
[[arxiv](https://arxiv.org/abs/2410.15959)] [[cool](https://papers.cool/arxiv/2410.15959)] [[pdf](https://arxiv.org/pdf/2410.15959)]
> **Authors**: Zhi Hou,Tianyi Zhang,Yuwen Xiong,Hengjun Pu,Chengyang Zhao,Ronglei Tong,Yu Qiao,Jifeng Dai,Yuntao Chen
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: Preprint
- **标题**: 扩散变压器策略：通才视觉语言学习的扩展扩散变压器
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 在不同的机器人数据集上预测的最新视力语言动作模型已经证明了通过少数域中数据将新环境推广到新环境的潜力。但是，这些方法通常通过小动作头预测个人离散或连续行动，这限制了处理多种动作空间的能力。相比之下，我们用大型多模式扩散变压器（称为扩散变压器策略）对连续动作序列进行建模，其中我们直接通过大型变压器模型来直接将动作块变为动作块，而不是用于动作嵌入的小动作头。通过利用变形金刚的缩放能力，提出的方法可以有效地模拟大型机器人数据集中的连续最终影响动作，并实现更好的概括性能。广泛的实验证明了扩散变压器对Maniskill2，Libero，Calvin和Simpleerenv的有效性和概括，以及现实世界中的Franka Arm，与OpenVla和OpenVla和Opto相比，在真实的基准SimplerErenv，现实世界中的Franka Arm和Libero上实现了一致性的更好性能。具体而言，提议的方法没有铃铛和哨子，可以实现最先进的性能，而加尔文任务ABC-> d中只有一个第三视图的相机流，改善了5至3.6的平均任务数量，预处理阶段可显着促进Calvin上的成功序列超过1.2。项目页面：https：//zhihou7.github.io/dit_policy_vla/

### Multimodal LLM Guided Exploration and Active Mapping using Fisher Information 
[[arxiv](https://arxiv.org/abs/2410.17422)] [[cool](https://papers.cool/arxiv/2410.17422)] [[pdf](https://arxiv.org/pdf/2410.17422)]
> **Authors**: Wen Jiang,Boshu Lei,Katrina Ashton,Kostas Daniilidis
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: No comments
- **标题**: 使用Fisher信息的多模式LLM指导探索和主动映射
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 我们提出了一个主动的映射系统，该系统可以通过3D高斯（3DGS）表示为长期探索目标和短期行动计划。现有方法要么没有利用多模式大语言模型（LLM）的最新发展，要么没有考虑本地化不确定性中的挑战，这在体现的代理中至关重要。我们建议使用基于信息的算法使用多模式LLM与详细的运动计划一起进行长匹马计划。通过利用我们的3DG表示的高质量视图综合，我们的方法从语义角度利用多模式LLM作为零摄像机探索目标的零击计划者。我们还引入了一种不确定性感知的路径建议和选择算法，该算法平衡了最大化环境信息增益的双重目标，同时最大程度地减少了本地化错误的成本。在Gibson和Habitat-Matterport 3D数据集上进行的实验证明了该方法的最新结果。

### Real-time Vehicle-to-Vehicle Communication Based Network Cooperative Control System through Distributed Database and Multimodal Perception: Demonstrated in Crossroads 
[[arxiv](https://arxiv.org/abs/2410.17576)] [[cool](https://papers.cool/arxiv/2410.17576)] [[pdf](https://arxiv.org/pdf/2410.17576)]
> **Authors**: Xinwen Zhu,Zihao Li,Yuxuan Jiang,Jiazhen Xu,Jie Wang,Xuyang Bai
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: ICICT 2024, 18 pages
- **标题**: 通过分布式数据库和多模式感知，基于车辆通信的实时车辆到车辆通信的网络合作控制系统：在十字路口中证明
- **领域**: 机器人技术,人工智能,系统与控制
- **摘要**: 自动驾驶行业正在迅速发展，车辆到车辆（V2V）通信系统将重点介绍是提高道路安全和交通效率的关键组成部分。本文介绍了一种新型的基于车辆通信的实时车辆通信网络合作控制系统（VVCC），该系统旨在彻底改变自动驾驶中的宏观交通计划和避免碰撞。在Quanser Car（QCAR）硬件平台上实施，我们的系统将分布式数据库集成到单个自动驾驶汽车和可选的中央服务器中。我们还开发了一种具有多目标跟踪和雷达感测的全面多模式感知系统。通过在物理十字路口环境中的演示，我们的系统展示了其在拥挤且复杂的城市环境中应用的潜力。

### Non-rigid Relative Placement through 3D Dense Diffusion 
[[arxiv](https://arxiv.org/abs/2410.19247)] [[cool](https://papers.cool/arxiv/2410.19247)] [[pdf](https://arxiv.org/pdf/2410.19247)]
> **Authors**: Eric Cai,Octavian Donca,Ben Eisner,David Held
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Conference on Robot Learning (CoRL), 2024
- **标题**: 通过3D密集扩散的非刚性相对位置
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: “相对放置”的任务是预测一个对象与另一个对象相关的位置，例如将杯子放在杯子架上。通过明确的以对象为中心的几何推理，最新的相对位置方法已取得了巨大的进步，朝着对机器人操纵的数据有效学习，同时推广到看不见的任务变化。但是，尽管在现实世界中非刚性的身体无处不在，但它们尚未代表可变形的转换。作为弥合这一差距的第一步，我们提出``跨分解'' - 相对位置与可变形物体之间几何关系的原理的扩展 - 并提出了一种基于新颖的视觉方法，可以通过密集扩散来学习交叉分解的方法。为此，我们证明了该方法在多个观众的范围内，远距离范围的远程范围，并远距离范围，并构成了远距离的范围，并且远距离范围均可进行范围，并构成远距离的范围。可以在https://sites.google.com/view/tax3d-corl-2024上找到超出先前作品范围的任务（在模拟和现实世界中）。

### Learning Diffusion Policies from Demonstrations For Compliant Contact-rich Manipulation 
[[arxiv](https://arxiv.org/abs/2410.19235)] [[cool](https://papers.cool/arxiv/2410.19235)] [[pdf](https://arxiv.org/pdf/2410.19235)]
> **Authors**: Malek Aburub,Cristian C. Beltran-Hernandez,Tatsuya Kamijo,Masashi Hamaya
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: 从示范中学习扩散政策，以进行合规性的操纵
- **领域**: 机器人技术,人工智能
- **摘要**: 机器人对执行重复性或危险任务具有巨大的希望，但是实现类似人类的灵活性，尤其是在接触良好和动态的环境中，仍然具有挑战性。依靠位置或速度控制的刚性机器人通常在保持稳定的接触并在力密集型任务中运用一致的力量而努力。从示范中学习已成为解决方案，但是需要复杂的操作（例如粉末研磨）的任务带来了独特的困难。本文介绍了合规操作（DIPCOM）的扩散策略，这是一种新型基于扩散的框架，专为合规的控制任务而设计。通过利用生成扩散模型，我们制定了一种预测笛卡尔最终效果的策略，并调整了手臂刚度以保持必要的力。我们的方法通过多模式分布建模增强了力控制，改善了在合规性控制中扩散策略的整合，并通过证明其在现实世界任务中的有效性来扩展我们的先前工作。我们提供了框架和现有方法之间的详细比较，突出了部署基于扩散的合规性控制的优点和最佳实践。

### Neural Fields in Robotics: A Survey 
[[arxiv](https://arxiv.org/abs/2410.20220)] [[cool](https://papers.cool/arxiv/2410.20220)] [[pdf](https://arxiv.org/pdf/2410.20220)]
> **Authors**: Muhammad Zubair Irshad,Mauro Comi,Yen-Chen Lin,Nick Heppert,Abhinav Valada,Rares Ambrus,Zsolt Kira,Jonathan Tremblay
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: 20 pages, 20 figures. Project Page: https://robonerf.github.io
- **标题**: 机器人技术中的神经领域：一项调查
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 神经领域已成为计算机视觉和机器人技术中3D场景表示形式的一种变革性方法，从而可以准确推断几何学，3D语义和来自POSED 2D数据的动力学。利用可区分的渲染，神经领域涵盖了连续隐式和显式神经表示，实现了高保真3D重建，多模式传感器数据的整合以及新观点的产生。这项调查探讨了他们在机器人技术中的应用，强调了它们增强感知，计划和控制的潜力。它们的紧凑性，记忆效率和不同性，以及与基础和生成模型无缝集成，使其非常适合实时应用，改善机器人适应性和决策。本文对机器人技术的神经领域进行了详尽的审查，对各个领域的应用进行了分类，并根据200多篇论文评估了其优势和局限性。首先，我们提出四个关键的神经场框架：占用网络，签名距离场，神经辐射场和高斯裂口。其次，我们详细介绍了神经领域在五个主要机器人域中的应用：姿势估计，操纵，导航，物理和自主驾驶，突出关键作品并讨论外卖和开放挑战。最后，我们概述了机器人技术中神经领域的当前局限性，并为未来的研究提出了有希望的方向。项目页面：https：//robonerf.github.io

### PMM-Net: Single-stage Multi-agent Trajectory Prediction with Patching-based Embedding and Explicit Modal Modulation 
[[arxiv](https://arxiv.org/abs/2410.19544)] [[cool](https://papers.cool/arxiv/2410.19544)] [[pdf](https://arxiv.org/pdf/2410.19544)]
> **Authors**: Huajian Liu,Wei Dong,Kunpeng Fan,Chao Wang,Yongzhuo Gao
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: PMM-NET：具有基于修补的嵌入和显式模态调制的单阶段多代理轨迹预测
- **领域**: 机器人技术,人工智能
- **摘要**: 分析和预测行人（例如行人）的轨迹对具体的智能应用起着关键作用。人类行为的固有不确定性和丰富的代理人之间的复杂社会互动使这项任务比普通时间序列预测更具挑战性。在这封信中，我们旨在探索多代理轨迹预测框架的独特配方。具体而言，我们提出了一个基于修补的时间特征提取模块和一个基于图的社交特征提取模块，从而实现了有效的特征提取和跨刻录概括。此外，我们重新评估了社会互动的作用，并提出了一种基于明确模式调制以整合时间和社会特征的新方法，从而构建了有效的单级推理管道。公共基准数据集上的结果证明了与最先进的方法相比，我们的模型的出色性能。该代码可在以下网址提供：github.com/tib-k330/pmm-net。

### BEVPose: Unveiling Scene Semantics through Pose-Guided Multi-Modal BEV Alignment 
[[arxiv](https://arxiv.org/abs/2410.20969)] [[cool](https://papers.cool/arxiv/2410.20969)] [[pdf](https://arxiv.org/pdf/2410.20969)]
> **Authors**: Mehdi Hosseinzadeh,Ian Reid
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: Accepted for presentation at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024. Project page: https://m80hz.github.io/bevpose/
- **标题**: Bevpose：通过姿势指导的多模式BEV对齐揭示场景语义
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 在自主驾驶和移动机器人技术领域，用于创建鸟类视图（BEV）表示的方法发生了重大变化。这种转变的特征是使用变压器和学习将不同视力传感器（主要是LiDAR和摄像机）融合到2D平面地面表示形式中。但是，这些创建此类地图的基于学习的方法通常很大程度上取决于广泛的注释数据，这带来了显着的挑战，尤其是在稀缺大规模数据集的各种或非城市环境中。在这项工作中，我们提出了BevPose，这是一个框架，该框架使用传感器姿势作为指导监督信号集成了相机和LIDAR数据的BEV表示。该方法特别减少了对昂贵注释数据的依赖性。通过利用姿势信息，我们对齐和融合了多模式的感觉输入，促进了捕获环境几何和语义方面的潜在BEV嵌入的学习。我们的预处理方法表明，在BEV地图细分任务中表现出了有希望的性能，超过了完全监督的最新方法，同时仅需要最少的带注释的数据。这种发展不仅面临BEV表示学习中数据效率的挑战，而且还扩大了在包括越野环境和室内环境在内的各种领域中此类技术的潜力。

### PDSR: Efficient UAV Deployment for Swift and Accurate Post-Disaster Search and Rescue 
[[arxiv](https://arxiv.org/abs/2410.22982)] [[cool](https://papers.cool/arxiv/2410.22982)] [[pdf](https://arxiv.org/pdf/2410.22982)]
> **Authors**: Alaa Awad Abdellatif,Ali Elmancy,Amr Mohamed,Ahmed Massoud,Wadha Lebda,Khalid K. Naji
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: This paper is currently under review at IEEE IoT Magazine
- **标题**: PDSR：有效的无人机部署，用于迅速而准确的污点后搜索和救援
- **领域**: 机器人技术,人工智能,系统与控制
- **摘要**: 本文介绍了灾后搜救（PDSR）的综合框架，旨在优化利用无人机（UAV）（UAVS）的搜救操作。主要目标是提高感应能力的精度和可用性，尤其是在各种灾难性的情况下。这个概念的核心是迅速部署具有多种感应，沟通和智能功能的无人机群，它充当了一种集成系统，该系统结合了多种技术和方法，以有效地检测灾难后埋在瓦砾或碎屑下的个人。在此框架内，我们提出了架构解决方案并应对相关的挑战，以确保在现实世界中的灾难场景中的最佳性能。拟议的框架旨在使用多层群架构的传统方法更快地实现损坏区域的覆盖范围。此外，将多模式传感数据与机器学习进行数据融合可以提高检测准确性，从而确保对幸存者的精确识别。

## 声音(cs.SD:Sound)

该领域共有 31 篇论文

### VHASR: A Multimodal Speech Recognition System With Vision Hotwords 
[[arxiv](https://arxiv.org/abs/2410.00822)] [[cool](https://papers.cool/arxiv/2410.00822)] [[pdf](https://arxiv.org/pdf/2410.00822)]
> **Authors**: Jiliang Hu,Zuchao Li,Ping Wang,Haojun Ai,Lefei Zhang,Hai Zhao
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: 14 pages, 6 figures, accepted by EMNLP 2024
- **标题**: VHASR：带有视觉热词的多模式语音识别系统
- **领域**: 声音,计算语言学,音频和语音处理
- **摘要**: 基于图像的多模式自动语音识别（ASR）模型通过合并与音频相关的图像来增强语音识别性能。但是，一些作品表明，将图像信息引入模型无助于改善ASR性能。在本文中，我们提出了一种新颖的方法，可以有效地利用音频相关的图像信息，并设置VHASR，VHASR是一种多模式的语音识别系统，它使用视觉作为热门来增强模型的语音识别能力。我们的系统利用双流式体系结构，该体系结构首先分别在两个流上转录文本，然后结合输出。我们在四个数据集上评估了所提出的模型：FlickR8K，ADE20K，可可和开放型。实验结果表明，VHASR可以有效利用图像中的关键信息来增强模型的语音识别能力。它的性能不仅超过了单峰ASR，而且还可以在现有的基于图像的多模式ASR中实现SOTA。

### Heterogeneous sound classification with the Broad Sound Taxonomy and Dataset 
[[arxiv](https://arxiv.org/abs/2410.00980)] [[cool](https://papers.cool/arxiv/2410.00980)] [[pdf](https://arxiv.org/pdf/2410.00980)]
> **Authors**: Panagiota Anastasopoulou,Jessica Torrey,Xavier Serra,Frederic Font
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: DCASE2024, post-print, 5 pages, 2 figures
- **标题**: 具有广泛的声音分类法和数据集的异质声音分类
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 自动声音分类在机器聆听中具有广泛的应用，从而使上下文感知的声音处理和理解。本文探讨了自动分类以高级内变异性为特征的异质声音的方法。我们的研究使用广泛的声音分类法评估了分类任务，这是一项两级分类法，其中包括28个类，旨在涵盖具有针对实际用户应用程序的语义差异的异质声音。我们通过手动注释构建数据集，以确保每个类别中的准确性，各种表示的代表以及在现实世界中的相关性。我们比较了各种传统和现代机器学习方法，以建立一个基线，以实现异质声音分类的任务。我们研究了输入特征的作用，专门研究了声音得出的声音表示与用预先训练的深神经网络提取的嵌入方式相比，捕获声音和语义信息的声音信息。实验结果表明，编码声学和语义信息的音频嵌入在分类任务中的精度更高。经过仔细分析分类错误后，我们确定了一些失败的根本原因，并提出了减轻它们的措施。本文强调了需要更深入地探索分类的所有阶段，了解数据并采用能够有效处理数据复杂性并在现实世界声音环境中概括的方法。

### CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical Temporal Structure Augmentation 
[[arxiv](https://arxiv.org/abs/2410.02271)] [[cool](https://papers.cool/arxiv/2410.02271)] [[pdf](https://arxiv.org/pdf/2410.02271)]
> **Authors**: Junda Wu,Warren Li,Zachary Novack,Amit Namburi,Carol Chen,Julian McAuley
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: 4 pages
- **标题**: Collap：与音乐时间结构增强进行对比的长形语言审计
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 建模时间特征在音频波形的表示学习中起着重要作用。我们提出对比度的长形语言审计（\ textbf {collap}），以显着扩展输入音频（最多5分钟）和语言描述（超过250个单词）的感知窗口，同时促进跨模态和时间动力学的对比学习。我们利用最近的音乐乐队为全长歌曲生成长格式的音乐字幕，并随着音乐时间结构增强，我们收集了51.3k音频文本对，源自大型音频训练数据集，平均音频长度达到288秒。我们提出了一种新颖的对比学习体系结构，该体系结构将语言表示与结构化音频表示融合在一起，通过将每首歌曲分割成剪辑并提取其嵌入方式。通过注意机制，我们捕获了多模式的时间相关性，从而使模型可以自动称重并增强最终融合得分，以改善对比度比对。最后，我们开发了具有不同类型的骨干语言模型的Collap模型的两个变体。通过对多个长形式音乐检索数据集进行的全面实验，与基准相比，我们证明了检索准确性的稳定性提高。我们还表明，据异构的长形多模式上下文，预处理的崩溃模型可以转移到各种音乐信息检索任务中。

### A Pilot Study of Applying Sequence-to-Sequence Voice Conversion to Evaluate the Intelligibility of L2 Speech Using a Native Speaker's Shadowings 
[[arxiv](https://arxiv.org/abs/2410.02239)] [[cool](https://papers.cool/arxiv/2410.02239)] [[pdf](https://arxiv.org/pdf/2410.02239)]
> **Authors**: Haopeng Geng,Daisuke Saito,Nobuaki Minematsu
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Accepted by APSIPA ASC 2024. arXiv admin note: text overlap with arXiv:2409.11742
- **标题**: 一项用于应用序列到序列语音转换以使用母语者的影子评估L2语音的清晰度的试点研究
- **领域**: 声音,计算语言学,音频和语音处理
- **摘要**: 由于发音错误和韵律不当，L2扬声器的话语可能难以理解。在计算机辅助语言学习系统中，通常使用语音识别引擎提供文本反馈。但是，对于L2扬声器来说，一种理想的反馈形式应该是如此细粒度，以至于使他们能够检测和诊断L2扬声器的话语中无法理解的部分。受语言老师的启发，这些语言教师通过语音到声音的过程纠正了他的发音，这项试点研究利用了一个独特的半平行数据集，该数据集由非本地人说的人组成”（L2）大声朗读，本人说话者（L1）的阴影（L1）及其脚本遮盖的话语。我们探索使用语音转换技术复制L1扬声器阴影L2语音的过程的技术可能性，以创建虚拟的阴影系统。实验结果证明了VC系统在模拟L1的阴影行为方面的可行性。虚拟阴影系统的输出与语言和声学方面的真实L1阴影话语显示出合理的相似性。

### UniMuMo: Unified Text, Music and Motion Generation 
[[arxiv](https://arxiv.org/abs/2410.04534)] [[cool](https://papers.cool/arxiv/2410.04534)] [[pdf](https://arxiv.org/pdf/2410.04534)]
> **Authors**: Han Yang,Kun Su,Yutong Zhang,Jiaben Chen,Kaizhi Qian,Gaowen Liu,Chuang Gan
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: Unimero：统一的文本，音乐和动作生成
- **领域**: 声音,计算机视觉和模式识别,图形,机器学习,多媒体,音频和语音处理
- **摘要**: 我们介绍了Unimumo，这是一个统一的多模式模型，能够将任意文本，音乐和运动数据作为输入条件，以在所有三种模式中生成输出。为了解决缺乏时间同步数据，我们根据有节奏模式将未配对的音乐和运动数据对齐，以利用现有的仅大规模音乐和仅动作的数据集。通过将音乐，动作和文本转换为基于令牌的表示形式，我们的模型通过统一的编码器变压器体系结构桥接了这些模式。为了支持单个框架中的多个生成任务，我们介绍了几种架构改进。我们建议使用音乐代码手册编码运动，将运动映射到与音乐相同的功能空间中。我们介绍了一种音乐动作平行生成方案，该方案将所有音乐和运动生成任务统一为单个变压器解码器架构，并具有音乐动感联合一代的单个培训任务。此外，该模型是通过微调现有预训练的单模式模型而设计的，从而大大减少了计算需求。广泛的实验表明，Unimero在音乐，运动和文本方式的所有单向生成基准上都取得了竞争成果。 \ href {https://hanyangclarence.github.io/unimumo_demo/} {project page}中可获得定量结果。

### Audio-Agent: Leveraging LLMs For Audio Generation, Editing and Composition 
[[arxiv](https://arxiv.org/abs/2410.03335)] [[cool](https://papers.cool/arxiv/2410.03335)] [[pdf](https://arxiv.org/pdf/2410.03335)]
> **Authors**: Zixuan Wang,Chi-Keung Tang,Yu-Wing Tai
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 音频代理：利用LLM进行音频生成，编辑和组成
- **领域**: 声音,计算机视觉和模式识别,机器学习,音频和语音处理
- **摘要**: 我们介绍了基于文本或视频输入的Audio Agent，这是一个多模式的框架，用于音频生成，编辑和组成。文本到原告（TTA）任务的常规方法通常从文本描述中提出单通道。虽然直接，但在给定复杂的文本条件时，这种设计很难产生高质量的音频。在我们的方法中，我们利用预先训练的TTA扩散网络作为音频生成代理，与GPT-4同时起作用，该网络将文本条件分解为原子，特定的说明，并呼叫代理进行音频生成。这样一来，音频代理可以生成高质量的音频，该音频与提供的文本或视频表现为复杂和多个事件，同时支持可变长度和可变体积的生成。对于视频对审计（VTA）任务，大多数现有方法都需要培训时间戳检测器将视频事件与生成的音频同步，这一过程可能乏味且耗时。取而代之的是，我们通过微调预训练的大型语言模型（LLM）（例如Gemma2-2b-it）提出了一种更简单的方法，以获得桥接视频和音频方式的语义和时间条件。因此，我们的框架为TTA和VTA任务提供了全面的解决方案，而无需培训中实质性的计算开销。

### SONAR: A Synthetic AI-Audio Detection Framework and Benchmark 
[[arxiv](https://arxiv.org/abs/2410.04324)] [[cool](https://papers.cool/arxiv/2410.04324)] [[pdf](https://arxiv.org/pdf/2410.04324)]
> **Authors**: Xiang Li,Pin-Yu Chen,Wenqi Wei
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 声纳：合成AI-Audio检测框架和基准测试
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 使用生成人工智能（AI）技术的文本到语音转换（TTS）和语音转换（VC）的最新进展已使产生高质量和现实的人类音频成为可能。这引入了重大挑战，即将AI合成的演讲与真实的人类声音区分开来，并可能引发滥用恶意目的的潜在问题，例如模仿和欺诈，传播错误信息，深魔和骗局。但是，AI合成音频的现有检测技术尚未保持步伐，并且经常在不同数据集中表现出较差的概括。在本文中，我们介绍了Sonar，这是一个合成的AI Audio检测框架和基准，旨在提供全面的评估，以区分尖端的AI合成的听觉内容。 Sonar包括来自9种不同音频合成平台的新型评估数据集，包括领先的TTS提供商和最先进的TTS模型。这是在传统和基础模型的DeepFake检测系统中均匀基准AI-Audio检测基准AI-Audio检测的第一个框架。通过广泛的实验，我们揭示了现有检测方法的概括限制，并证明基础模型表现出更强的概括能力，这可以归因于其模型大小以及预处理数据的规模和质量。此外，我们探讨了很少的微调在改善概括方面的有效性和效率，从而强调了其量身定制应用的潜力，例如针对特定实体或个人的个性化检测系统。代码和数据集可从https://github.com/jessegator/sonar获得。

### The OCON model: an old but green solution for distributable supervised classification for acoustic monitoring in smart cities 
[[arxiv](https://arxiv.org/abs/2410.04098)] [[cool](https://papers.cool/arxiv/2410.04098)] [[pdf](https://arxiv.org/pdf/2410.04098)]
> **Authors**: Stefano Giacomelli,Marco Giordano,Claudia Rinaldi
> **First submission**: 2024-10-05
> **First announcement**: 2024-10-07
> **comment**: Accepted at "IEEE 5th International Symposium on the Internet of Sounds, 30 Sep / 2 Oct 2024, Erlangen, Germany"
- **标题**: OCON模型：一种旧的但绿色的解决方案，用于智能城市的声学监测的可分发监督分类
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 本文探讨了单级方法的结构化应用和监督分类任务的单级网络模型，重点关注元音音素分类以及说话者对自动语音识别（ASR）域的识别。对于我们的案例研究，ASR模型在专有的感应和闪电系统上运行，用于监测城市街道上的声学和空气污染。我们使用知情的网格搜索方法将伪神经架构搜索和超参数调整实验的组合形式化，以实现与当今最复杂的体系结构相当的分类精度，从而深入研究扬声器的识别和能源效率方面。尽管它很简单，但我们的模型提案还是很有机会概括语言和说话者的上下文，以在计算约束上下文中广泛适用，并由相关的统计和性能指标证明。我们的实验代码在我们的GitHub上公开访问。

### A quest through interconnected datasets: lessons from highly-cited ICASSP papers 
[[arxiv](https://arxiv.org/abs/2410.03676)] [[cool](https://papers.cool/arxiv/2410.03676)] [[pdf](https://arxiv.org/pdf/2410.03676)]
> **Authors**: Cynthia C. S. Liem,Doğa Taşcılar,Andrew M. Demetriou
> **First submission**: 2024-09-19
> **First announcement**: 2024-10-07
> **comment**: in Proceedings of the 21st International Conference on Content-based Multimedia Indexing, September 18-20 2024, Reykjavik, Iceland
- **标题**: 通过互连数据集的任务：来自高度引用的ICASSP论文的课程
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 随着音频机器的学习成果部署在具有社会影响力的应用中，重要的是要了解所使用的数据的质量和起源很重要。注意到，在应用机器学习域中的学术出版中，对这种感觉的明确性并不能力奖励，并且在典型的应用机器学习课程中都不包含在典型的应用机器学习课程中，我们介绍了一项研究，其中一项研究与与国际声学，语音，语音，信号处理（ICASSP）的国际前5篇论文相关的数据集使用。在这方面，我们对二手数据集的起源进行了彻底的深度优先分析，通常会导致搜索不得不超越官方论文中的报道，并以不清楚或纠缠起来的起源。尤其是在当前朝着更大甚至可能生成的AI模型方向发展时，对数据出处责任的需求的认识正在增加。有了这一点，我们呼吁社区不仅专注于工程更大的模型，而且为阐明应建立此类模型的基础创造更多的空间和奖励。

### SoundSignature: What Type of Music Do You Like? 
[[arxiv](https://arxiv.org/abs/2410.03375)] [[cool](https://papers.cool/arxiv/2410.03375)] [[pdf](https://arxiv.org/pdf/2410.03375)]
> **Authors**: Brandon James Carone,Pablo Ripollés
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: 10 pages, 1 figure, to be published in the 2024 International Symposium on the IEEE Internet of Sounds Proceedings
- **标题**: 声音签名：您喜欢哪种类型的音乐？
- **领域**: 声音,人工智能,信息检索,音频和语音处理
- **摘要**: Soundignature是一个音乐应用程序，它集成了自定义OpenAI助手，以分析用户喜欢的歌曲。该系统结合了最先进的音乐信息检索（MIR）Python包装，以将提取的声学/音乐功能与助手对艺术家和乐队的广泛了解相结合。利用新兴的声音互联网（ios）生态系统的语义音频和原理，将声音签名利用，将MIR与AI集成在一起，为用户提供对音乐的声学特性的个性化见解，类似于音乐偏好人格报告。然后，用户可以与聊天机器人进行互动，以探讨有关所执行声学分析以及与音乐品味如何相关的更深入的询问。这种互动性改变了应用程序，不仅充当有关熟悉和/或喜欢的歌曲的内容丰富的资源，而且还作为一个教育平台，使用户能够加深对音乐特征，音乐理论，信号处理中常用的声学特性以及音乐背后的艺术家的理解。除了一般可用性之外，该应用程序还包含了几种良好的开源音乐家特定的工具，例如和弦识别算法（CREMA），源分离算法（DEMUCS）和一个音频到米迪转换器（Basic-Pitch）。这些功能允许没有编码技能的用户仅通过与聊天机器人进行交互来访问高级的开源音乐处理算法（例如，您能给我这首歌的词干吗？）。在本文中，我们强调了该应用程序的创新功能和教育潜力，并从一项试验用户研究中介绍了评估其功效和可用性的发现。

### Incorporating Talker Identity Aids With Improving Speech Recognition in Adversarial Environments 
[[arxiv](https://arxiv.org/abs/2410.05423)] [[cool](https://papers.cool/arxiv/2410.05423)] [[pdf](https://arxiv.org/pdf/2410.05423)]
> **Authors**: Sagarika Alavilli,Annesya Banerjee,Gasser Elbanna,Annika Magaro
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: Submitted to ICASSP 2025
- **标题**: 将谈话者身份辅助辅助与对抗环境中的语音识别相结合
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 当前的最新语音识别模型经过训练，可以将声学信号映射到亚属单元中。尽管这些模型表现出卓越的性能，但它们仍然容易受到分发条件的影响，例如背景噪音和语音增强。在这项工作中，我们假设在语音识别过程中合并说话者表示可以增强对噪声的鲁棒性。我们开发了一个基于变压器的模型，该模型共同执行语音识别和说话者的识别。我们的模型利用ecapa-tdnn的耳语和扬声器嵌入的语音嵌入，它们经过共同处理以执行这两个任务。我们表明，在清洁条件下，联合模型的性能与耳语相对。值得注意的是，联合模型在高噪声环境中的表现优于窃窃私语，例如8扬声器Babble背景噪音。此外，我们的联合模型在处理高度增强的语音方面表现出色，包括正弦波和噪音录音的语音。总体而言，这些结果表明，将语音表示与语音识别的集成可以导致在对抗条件下更健壮的模型。

### Spectral and Rhythm Features for Audio Classification with Deep Convolutional Neural Networks 
[[arxiv](https://arxiv.org/abs/2410.06927)] [[cool](https://papers.cool/arxiv/2410.06927)] [[pdf](https://arxiv.org/pdf/2410.06927)]
> **Authors**: Friedrich Wolf-Monheim
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 具有深卷积神经网络的音频分类的光谱和节奏特征
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 卷积神经网络（CNN）广泛用于计算机视觉中。它们不仅可以用于传统的数字图像材料来识别模式，还可以用于从代表光谱和节奏特征的数字图像中提取特征，从时间域数字音频信号中提取的声音进行声学分类。不同的光谱和节奏具有诸如MEL尺度光谱图，Mel频率的心形系数（MFCC），环状频道图，短期傅立叶变换（STFT）Chromagram，Constant-Q Transform（CQT）和Chromoma Energy Chromagrams（CQT）的NE NENTORCTION（CENS）collomitiation Chrose clastiation Chromatiation Cartrution cartruction contruction contruction contruction contruction，可以清楚地表明，MEL量表的频谱图和Mel频率CEPSTRAL系数（MFCC）的性能明显优于这项研究的其他光谱和节奏特征，该研究对使用深CNN的音频分类任务进行了研究。实验是在ESC-50数据集的帮助下进行的，并具有2,000个标记的环境音频录制。

### IntrinsicVoice: Empowering LLMs with Intrinsic Real-time Voice Interaction Abilities 
[[arxiv](https://arxiv.org/abs/2410.08035)] [[cool](https://papers.cool/arxiv/2410.08035)] [[pdf](https://arxiv.org/pdf/2410.08035)]
> **Authors**: Xin Zhang,Xiang Lyu,Zhihao Du,Qian Chen,Dong Zhang,Hangrui Hu,Chaohong Tan,Tianyu Zhao,Yuxuan Wang,Bin Zhang,Heng Lu,Yaqian Zhou,Xipeng Qiu
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-11
> **comment**: No comments
- **标题**: InterNInsicVoice：赋予LLMS具有内在的实时语音互动能力
- **领域**: 声音,人工智能
- **摘要**: 当前使用语音互动功能构建LLM的方法在很大程度上取决于语音响应生成之前或期间的显式文本自回归产生，以维持内容质量，不幸的是，这带来了计算开销并增加了多转交流的延迟。为了解决这个问题，我们介绍了具有内在的实时语音交互功能设计的InternicVoic，e LLM。 IntrinsicVoice旨在通过减轻文本和语音之间的模态差距来促进预训练的LLM的文本能力转移到语音模式中。我们的新颖性架构，群体形式，可以将语音序列减少到与文本序列相当的长度，同时产生高质量的音频，从而大大降低了语音和文本之间的长度差异，加快推断并减轻长篇文本建模问题。此外，我们构建了一个名为\ Method-500K的多转音到语音对话数据集，其中包括近500K的语音到语音对话，以及一个跨模式训练策略，以增强语音和文本之间的语义一致性。实验结果表明，InterInsicVoice可以在多转向对话方案中产生高质量的语音响应，潜伏期低于100ms。演示可从https://instrinsicvoice.github.io/获得。

### M2M-Gen: A Multimodal Framework for Automated Background Music Generation in Japanese Manga Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.09928)] [[cool](https://papers.cool/arxiv/2410.09928)] [[pdf](https://arxiv.org/pdf/2410.09928)]
> **Authors**: Megha Sharma,Muhammad Taimoor Haseeb,Gus Xia,Yoshimasa Tsuruoka
> **First submission**: 2024-10-13
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: M2M-GEN：使用大语言模型的日本漫画中自动背景音乐发电的多模式框架
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 本文介绍了M2M Gen，这是一个多模式框架，用于生成针对日本漫画量身定制的背景音乐。此任务的主要挑战是缺乏可用的数据集或基线。为了应对这些挑战，我们提出了一条自动化的音乐生成管道，该管道为输入漫画书制作背景音乐。最初，我们使用漫画中的对话来检测场景边界，并使用场景中的角色面进行情感分类。然后，我们使用GPT4O将这种低级场景信息转化为高级音乐指令。 GPT 4O的另一个实例在场景信息和音乐指令上进行了调节，生成了页面级音乐字幕，以指导文本到音乐模型。这会产生与芒果不断发展的叙述相符的音乐。 M2M Gen的有效性通过广泛的主观评估得到了证实，展示了其产生更高质量，更相关和一致的音乐的能力，与我们的基线相比，这些音乐可以补充特定场景。

### DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for Zero-shot Audio Captioning 
[[arxiv](https://arxiv.org/abs/2410.09472)] [[cool](https://papers.cool/arxiv/2410.09472)] [[pdf](https://arxiv.org/pdf/2410.09472)]
> **Authors**: Xiquan Li,Wenxi Chen,Ziyang Ma,Xuenan Xu,Yuzhe Liang,Zhisheng Zheng,Qiuqiang Kong,Xie Chen
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: DRCAP：用零声音字幕的检索效果来解码拍手潜伏期
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 尽管自动音频字幕（AAC）取得了显着进展，但传统的全面监督AAC模型仍然面临两个关键挑战：在跨域转移时，需要昂贵的音频文本对数据来进行培训和性能退化。为了克服这些限制，我们提出DRCAP，这是一种数据效率且灵活的零击音频字幕系统，需要仅文本数据进行培训，并且可以迅速适应新域而无需进行其他微调。 DRCAP集成了对比的语言原告预训练（CLAP）模型和大型语言模型（LLM）作为骨干。在训练过程中，该模型通过拍手的固定文本编码器预测地面真相标题，而在推断期间，将文本编码器替换为音频编码器，以以零发的方式为音频剪辑生成字幕。为了减轻拍手模型的模态差距，我们同时使用编码器侧的投影策略，也使用解码器侧的检索增强生成策略。具体而言，首先将音频嵌入投影到文本嵌入支持上，以吸收拍手的联合多模式空间内的广泛语义信息。同时，从数据存储中检索到的类似字幕作为指导LLM的提示，并结合了外部知识以充分利用其强大的生成能力。该模型在投影的拍手嵌入和检索类似的标题上都可以产生更准确和语义上丰富的文本描述。通过将文本嵌入支持和标题数据存储量身定制到目标域，DRCAP可以获得强大的能力，可以以无训练的方式适应新域。实验结果表明，在内域场景中，DRCAP的表现优于所有其他零射模型，并在跨域情景中实现最先进的性能。

### Multimodal Audio-based Disease Prediction with Transformer-based Hierarchical Fusion Network 
[[arxiv](https://arxiv.org/abs/2410.09289)] [[cool](https://papers.cool/arxiv/2410.09289)] [[pdf](https://arxiv.org/pdf/2410.09289)]
> **Authors**: Jinjin Cai,Ruiqi Wang,Dezhong Zhao,Ziqin Yuan,Victoria McKenna,Aaron Friedman,Rachel Foot,Susan Storey,Ryan Boente,Sudip Vhaduri,Byung-Cheol Min
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 基于多模式音频的疾病预测，基于变压器的分层融合网络
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **摘要**: 基于音频的疾病预测正在成为传统医学诊断方法的有希望的补充，促进了早期，方便和非侵入性疾病检测和预防。多模式融合，该融合整合了来自生物声学方式内或跨多个领域的特征，已证明有效地增强了诊断性能。但是，该领域中的大多数现有方法采用单方面融合策略，仅着眼于模式内或模式间融合。这种方法限制了各种声学特征领域和生物声学方式的互补性的全面开发。此外，在模式特异性和模态共享空间内对潜在依赖性的不足和孤立的探索降低了其管理多模式数据中固有异质性的能力。为了填补这些空白，我们提出了一个基于变压器的分层融合网络，旨在用于一般多模式的疾病预测。具体而言，我们以层次的方式无缝地集成了模式内和模式间融合，并熟练地编码了必要的模式内和模式间互补相关性。全面的实验表明，我们的模型在预测三种疾病时实现了最先进的表现：Covid-19，帕金森氏病和病理构造障碍，在基于音频的疾病预测任务的广泛背景下展示了其有希望的潜力。此外，广泛的消融研究和定性分析突出了我们模型中每个主要组成部分的显着好处。

### Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation 
[[arxiv](https://arxiv.org/abs/2410.10676)] [[cool](https://papers.cool/arxiv/2410.10676)] [[pdf](https://arxiv.org/pdf/2410.10676)]
> **Authors**: Peiwen Sun,Sitong Cheng,Xiangtai Li,Zhen Ye,Huadai Liu,Honggang Zhang,Wei Xue,Yike Guo
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Accepted by ICLR 2025
- **标题**: 两种耳朵都敞开：朝着语言驱动的空间音频产生
- **领域**: 声音,计算机视觉和模式识别,音频和语音处理
- **摘要**: 最近，扩散模型在单渠道音频产生中取得了巨大的成功。但是，在立体声音频生成方面，音景通常具有多个对象和方向的复杂场景。由于高数据成本和不稳定的生成模型，用空间上下文控制立体声音频仍然具有挑战性。据我们所知，这项工作代表了解决这些问题的首次尝试。我们首先构建了一个大规模，基于仿真和GPT辅助数据集的Bewo-1m，即使包括移动和多个来源，也具有丰富的音景和描述。除文本模式外，我们还通过检索获得了一组图像和合理配对的立体声音频，以推动多模式生成。现有的音频生成模型往往会产生相当随机和模糊的空间音频。为了为潜在扩散模型提供准确的指导，我们介绍了利用空间感知编码器和方位角状态矩阵来揭示合理的空间指导的空间模型。通过利用空间指导，我们的模型不仅实现了从文本中产生沉浸式和可控制的空间音频的目的，而且还扩展到其他方式作为先驱尝试。最后，在公平的环境下，我们对模拟和实际数据进行主观和客观评估，以将我们的方法与现行方法进行比较。结果证明了我们方法的有效性，强调了其生成遵守物理规则的空间音频的能力。

### Reproducible Machine Learning-based Voice Pathology Detection: Introducing the Pitch Difference Feature 
[[arxiv](https://arxiv.org/abs/2410.10537)] [[cool](https://papers.cool/arxiv/2410.10537)] [[pdf](https://arxiv.org/pdf/2410.10537)]
> **Authors**: Jan Vrba,Jakub Steinbach,Tomáš Jirsa,Laura Verde,Roberta De Fazio,Yuwen Zeng,Kei Ichiji,Lukáš Hájek,Zuzana Sedláková,Zuzana Urbániová,Martin Chovanec,Jan Mareš,Noriyasu Homma
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Code repository: https://github.com/aailab-uct/Automated-Robust-and-Reproducible-Voice-Pathology-Detection, Supplementary materials: https://doi.org/10.5281/zenodo.14793017
- **标题**: 可重现的基于机器学习的语音病理检测：引入音调差异功能
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **摘要**: 这项研究介绍了一种使用公开可用的Saarbrücken语音数据库（SVD）数据库的新方法来检测语音病理学的方法，并结合了常用的声学手工制作的功能与两个新颖的功能（基本频率相对差异）和NAN功能（失败的基础频率估计）。我们评估了六个机器学习（ML）分类器 - 支持向量机，K -Nearest邻居，天真的贝叶斯，决策树，随机森林和Adaboost-使用网格搜索选定分类器的可行超参数以及20480个不同特征子集。每种分类器类型的前1000个分类器 - 特征子集组合都通过重复的分层交叉验证验证。为了解决班级失衡，我们应用K-均值SMOTE来增强培训数据。我们的方法达到了出色的表现，对女性，男性和综合成果相关的成绩达到了85.61％，84.69％和85.22％的未加权平均召回（UAR）。我们故意忽略准确性，因为它是一个不平衡数据的高度偏见的度量。这一进步表明，ML方法的临床部署具有巨大的潜力，为客观检查语音病理提供了有价值的支持工具。为了更轻松地利用我们的方法论并支持我们的主张，我们提供了一个公开可用的GitHub存储库，其中DOI 10.5281/Zenodo.13771573。最后，我们提供改革清单，以增强我们方法的可读性，可重复性和理由。

### EmotionCaps: Enhancing Audio Captioning Through Emotion-Augmented Data Generation 
[[arxiv](https://arxiv.org/abs/2410.12028)] [[cool](https://papers.cool/arxiv/2410.12028)] [[pdf](https://arxiv.org/pdf/2410.12028)]
> **Authors**: Mithun Manivannan,Vignesh Nethrapalli,Mark Cartwright
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: 情感贴图：通过情绪增强的数据生成增强音频字幕
- **领域**: 声音,机器学习,音频和语音处理,信号处理
- **摘要**: 音频建模（例如自动音频字幕）的最新进展受益于借助大型语言模型生成的合成数据的培训。但是，这种环境声音字幕的方法主要集中在音频事件标签上，并且没有探索利用录音中可能存在的情感信息。在这项工作中，我们通过以估计的音景情感的形式指导Chatgpt来探讨产生情感增强的合成音频字幕数据的好处。为此，我们介绍了Emotioncaps，这是一个音频字幕的数据集，该数据集由大约120,000个音频剪辑组成，其中包含配对的合成描述，富含音景情感识别（SER）信息。我们假设这些附加信息将导致更高质量的字幕，以符合音频录制的情感音调，这反过来又可以提高使用此数据训练的字幕模型的性能。我们通过客观和主观评估来检验这一假设，将与情感贴图数据集训练的模型与多个基线模型进行了比较。我们的发现挑战了当前的字幕方法，并提出了开发和评估字幕模型的新方向。

### EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning 
[[arxiv](https://arxiv.org/abs/2410.13179)] [[cool](https://papers.cool/arxiv/2410.13179)] [[pdf](https://arxiv.org/pdf/2410.13179)]
> **Authors**: Ashish Seth,Ramaneswaran Selvakumar,S Sakshi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: EH-MAM：用于自我监督的语音表示学习易于戴的掩盖声学建模
- **领域**: 声音,人工智能,计算语言学,机器学习,音频和语音处理
- **摘要**: 在本文中，我们介绍了EH-MAM（易于硬化的自适应掩盖声学建模），这是一种新型的自我监督学习方法，用于语音表示学习。与使用随机掩盖方案进行掩盖声学建模（MAM）的先前方法相反，我们引入了一种新颖的选择性和适应性掩盖策略。具体来说，在SSL培训期间，我们逐步将更艰难的区域引入重建模型。我们的方法会自动选择硬区域，并建立在观察的基础上，即MAM中单个框架的重建损失可以提供自然信号，以判断解决该框架的MAM预文本任务的困难。为了确定这些硬区域，我们采用了一种教师模型，该模型首先预测框架损失，然后决定掩盖哪些框架。通过学习创建具有挑战性的问题，例如确定更艰难的框架并同时解决它们，该模型能够学习更有效的表示形式，从而获得对演讲的更全面的理解。在数量上，EH-MAM的表现优于各种低资源语音识别的几个最先进的基线，并且出色的基准测试为5％-10％。此外，我们进行了彻底的分析，以表明用EH-MAM掩盖的区域有效地捕获了跨语音框架的有用背景。

### Multi-Source Spatial Knowledge Understanding for Immersive Visual Text-to-Speech 
[[arxiv](https://arxiv.org/abs/2410.14101)] [[cool](https://papers.cool/arxiv/2410.14101)] [[pdf](https://arxiv.org/pdf/2410.14101)]
> **Authors**: Shuwei He,Rui Liu
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 5 pages, 1 figure, Accepted by ICASSP'2025
- **标题**: 沉浸式视觉文本到语音的多源空间知识理解
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 视觉文本到语音（VTTS）的目的是将环境图像作为提示，以综合口语内容的混响语音。以前的作品着重于全球环境建模的RGB模式，忽略了多源空间知识（例如深度，说话者位置和环境语义）的潜力。为了解决这些问题，我们为沉浸式VTT提出了一种新型的多源空间知识理解方案，称为MS2KU-VTTS。具体而言，我们首先将RGB图像作为主要来源的优先级优先考虑，并考虑深度图像，对象检测中的说话者位置知识以及Gemini生成的语义字幕作为补充来源。之后，我们提出了一种串行相互作用机制，以有效地整合主导和补充来源。所得的多源知识是根据每个来源的各自贡献动态整合的。这种丰富的相互作用和多源空间知识的集成指导了语音生成模型，从而增强了沉浸式的语音体验。实验结果表明，MS $^2 $ KU-VTTS在产生沉浸式语音时超过了现有的基准。演示和代码可在以下网址提供：https：//github.com/ai-s2-lab/ms2ku-vtts。

### CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2410.13267)] [[cool](https://papers.cool/arxiv/2410.13267)] [[pdf](https://arxiv.org/pdf/2410.13267)]
> **Authors**: Shangda Wu,Yashan Wang,Ruibin Yuan,Zhancheng Guo,Xu Tan,Ge Zhang,Monan Zhou,Jing Chen,Xuefeng Mu,Yuejie Gao,Yuanliang Dong,Jiafeng Liu,Xiaobing Li,Feng Yu,Maosong Sun
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: 17 pages, 10 figures, 4 tables, accepted by NAACL 2025
- **标题**: 夹具2：使用大语言模型从101种语言中检索多模式音乐信息
- **领域**: 声音,计算语言学,音频和语音处理
- **摘要**: 当前的音乐信息检索系统面临着管理语言多样性和整合各种音乐方式的挑战。这些局限性在全球多模式的音乐环境中降低了它们的有效性。为了解决这些问题，我们介绍了Clamp 2，该系统与101种支持ABC符号（基于文本的音乐符号格式）和MIDI（乐器数字界面）的系统兼容，以进行音乐信息检索。在150万个ABC-Midi-Text三重轴上进行了预先训练的夹具2包括多语言文本编码器和通过对比度学习对齐的多模式编码器。通过利用大型语言模型，我们可以大规模获得精致且一致的多语言描述，从而大大降低了文本噪声和平衡语言分布。我们的实验表明，Clamp 2实现了最新的实验，从而导致多种语义搜索和跨模式的音乐分类，从而建立了一个新的标准，用于包容性和全球音乐信息检索。

### Towards Robust Transcription: Exploring Noise Injection Strategies for Training Data Augmentation 
[[arxiv](https://arxiv.org/abs/2410.14122)] [[cool](https://papers.cool/arxiv/2410.14122)] [[pdf](https://arxiv.org/pdf/2410.14122)]
> **Authors**: Yonghyun Kim,Alexander Lerch
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: Accepted to the Late-Breaking Demo Session of the 25th International Society for Music Information Retrieval (ISMIR) Conference, 2024
- **标题**: 朝着稳健的转录：探索训练数据增强的噪声注射策略
- **领域**: 声音,人工智能,信息检索,机器学习,音频和语音处理
- **摘要**: 自动钢琴转录（APT）的最新进展已显着改善了系统性能，但是嘈杂环境对系统性能的影响仍然在很大程度上没有探索。这项研究调查了白噪声在各种信噪比（SNR）水平（SNR）水平对最新APT模型的影响，并在接受噪声提示数据培训时评估了ONET和FRAMES模型的性能。我们希望这项研究能够为开发在一系列声学条件下保持一致性的转录模型的初步工作提供宝贵的见解。

### OpenMU: Your Swiss Army Knife for Music Understanding 
[[arxiv](https://arxiv.org/abs/2410.15573)] [[cool](https://papers.cool/arxiv/2410.15573)] [[pdf](https://arxiv.org/pdf/2410.15573)]
> **Authors**: Mengjie Zhao,Zhi Zhong,Zhuoyuan Mao,Shiqi Yang,Wei-Hsiang Liao,Shusuke Takahashi,Hiromi Wakaki,Yuki Mitsufuji
> **First submission**: 2024-10-20
> **First announcement**: 2024-10-21
> **comment**: Resources: https://github.com/sony/openmu
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,多媒体,音频和语音处理
- **摘要**: 我们提出了OpenMU Bench，这是一套大规模的基准套件，可在培训多模式模型中解决数据稀缺问题以了解音乐。为了构建OpenMU基础，我们利用了现有的数据集并进行了自举注释。 OpenMU基础也通过包括歌词理解和音乐工具的使用来扩大音乐理解的范围。使用OpenMU基础，我们通过大量消融培训了音乐理解模型OpenMU，这表明OpenMU的表现优于Mu-Lalla等基线模型。 OpenMU和OpenMU板凳都是开源的，以促进音乐理解的未来研究并提高创意音乐的生产效率。

### ImmerseDiffusion: A Generative Spatial Audio Latent Diffusion Model 
[[arxiv](https://arxiv.org/abs/2410.14945)] [[cool](https://papers.cool/arxiv/2410.14945)] [[pdf](https://arxiv.org/pdf/2410.14945)]
> **Authors**: Mojtaba Heydari,Mehrez Souden,Bruno Conejo,Joshua Atkins
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: ICASSP 2025 - IEEE International Conference onAcoustics, Speech, andSignalProcessing, 2025
- **标题**: 沉浸式生成：一种生成的空间音频潜扩散模型
- **领域**: 声音,新兴技术,机器学习,音频和语音处理
- **摘要**: 我们介绍了沉浸式iveusion，这是一种端到端的生成音频模型，该模型在声音对象的空间，时间和环境条件下产生3D沉浸式音景。培训了浸入式的训练可以生成一阶Ambisonics（FOA）音频，这是一种传统的空间音频格式，其中包括四个可以呈现为多镀金空间输出的通道。所提出的生成系统由一个空间音频编解码器组成，该空间音频编解码器将FOA音频映射到潜在的组件，这是一种基于各种用户输入类型的潜在扩散模型，即文本提示，空间，时间和环境声学参数，并选择地培训了与空间和文本编码的相反语言（clap）（clap）的培训（clap）。我们建议指标来评估生成的空间音频的质量和空间粘附。最后，我们从发电质量和空间符合度中评估了模型性能，比较了提出的两个模式：``描述性使用空间文本提示''和``参数''，使用非空间文本提示和空间参数。我们的评估证明了与用户条件一致的有希望的结果并反映了可靠的空间保真度。

### Challenge on Sound Scene Synthesis: Evaluating Text-to-Audio Generation 
[[arxiv](https://arxiv.org/abs/2410.17589)] [[cool](https://papers.cool/arxiv/2410.17589)] [[pdf](https://arxiv.org/pdf/2410.17589)]
> **Authors**: Junwon Lee,Modan Tailleur,Laurie M. Heller,Keunwoo Choi,Mathieu Lagrange,Brian McFee,Keisuke Imoto,Yuki Okamoto
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: accepted to NeurIPS 2024 Workshop: Audio Imagination
- **标题**: 在声音场景综合上挑战：评估文本到原告的生成
- **领域**: 声音,人工智能,机器学习,多媒体,音频和语音处理
- **摘要**: 尽管神经文本到审计的产生取得了重大进步，但挑战仍在可控性和评估中。本文通过声音场景综合挑战来解决这些问题，作为对声学场景和事件的检测和分类2024的一部分。我们提出了一种评估协议，结合了客观指标，即Fréchet音频距离，并使用感知评估，并利用结构化的及时格式来实现各种及时的及时格式，以启用多元化的字幕和有效的评估。我们的分析揭示了跨声音类别和模型体系结构的各种性能，更大的模型通常具有出色，但创新的轻量级方法也显示出希望。客观指标与人类评级之间的强烈相关性验证了我们的评估方法。我们讨论了文本与原告合成器的音频质量，可控性和架构考虑因素方面的结果，为将来的研究提供了指导。

### Conditional GAN for Enhancing Diffusion Models in Efficient and Authentic Global Gesture Generation from Audios 
[[arxiv](https://arxiv.org/abs/2410.20359)] [[cool](https://papers.cool/arxiv/2410.20359)] [[pdf](https://arxiv.org/pdf/2410.20359)]
> **Authors**: Yongkang Cheng,Mingjiang Liang,Shaoli Huang,Gaoge Han,Jifeng Ning,Wei Liu
> **First submission**: 2024-10-27
> **First announcement**: 2024-10-28
> **comment**: Accepted by WACV 2025 (Round 1)
- **标题**: 有条件的gan，以增强有效且真实的全球手势产生的扩散模型
- **领域**: 声音,人工智能,计算机视觉和模式识别,图形,音频和语音处理
- **摘要**: 音频驱动的同时姿态产生对于人为计算机的交流，AI游戏和电影制作至关重要。虽然先前的研究表现出了希望，但仍然存在局限性。基于VAE的方法伴随着局部抖动和全球不稳定的问题，而基于扩散模型的方法受到低发电效率的阻碍。这是因为后者中ddpm的去核过程取决于以下假设：每个步骤中添加的噪声都是从单峰分布中采样的，并且噪声值很小。 DDIM从Euler方法借用了求解微分方程，破坏Markov链过程并增加噪声步长以减少去索步骤的数量的想法，从而加速了生成。但是，仅在逐步降级过程中增加步骤大小就会导致结果逐渐偏离原始数据分布，从而导致生成的动作质量显着下降和不自然的人工制品的出现。在本文中，我们打破了DDPM的假设，并取得了速度和忠诚度的突破进步。具体而言，我们引入了一个条件gan来捕获音频控制信号，并隐式匹配在同一采样步骤中扩散和降解步骤之间的多模式deno的分布，旨在采样较大的噪声值，并为高速生成应用更少的deNoise步骤。

### Temporal Convolution-based Hybrid Model Approach with Representation Learning for Real-Time Acoustic Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2410.19722)] [[cool](https://papers.cool/arxiv/2410.19722)] [[pdf](https://arxiv.org/pdf/2410.19722)]
> **Authors**: Sahan Dissanayaka,Manjusri Wickramasinghe,Pasindu Marasinghe
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: 10 pages, 10 figures, ICMLC2024
- **标题**: 基于时间卷积的混合模型方法，具有代表性学习实时声学异常检测
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 工业机械组件中潜在故障的早期检测对于确保操作的可靠性和安全性至关重要，从而保存机器状况监测（MCM）。这项研究通过引入实时声学异常检测的创新方法来解决这一势在必行。我们的方法将半监督的时间卷积与代表学习和与时间卷积网络（TCN）的混合模型策略相结合，以有效地处理声学数据中发现的各种复杂的异常模式。提出的模型表明，与该领域的既定研究相比，表现出卓越的性能，强调了这种方法的有效性。我们不仅提供了其优越性的定量证据，而且还采用了视觉表示，例如T-SNE图，以进一步证实该模型的功效。

### OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup 
[[arxiv](https://arxiv.org/abs/2410.21269)] [[cool](https://papers.cool/arxiv/2410.21269)] [[pdf](https://arxiv.org/pdf/2410.21269)]
> **Authors**: Xize Cheng,Siqi Zheng,Zehan Wang,Minghui Fang,Ziang Zhang,Rongjie Huang,Ziyang Ma,Shengpeng Ji,Jialong Zuo,Tao Jin,Zhou Zhao
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: Working in progress
- **标题**: OmniSep：统一的Omni-Mododity声音分离与查询混合
- **领域**: 声音,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 近年来，扩大规模在视觉和语言领域取得了巨大的成功。但是，在音频方面，研究人员在扩展训练数据方面遇到了一个重大挑战，因为大多数自然音频都包含各种干扰信号。为了解决此限制，我们引入了Omni-Modal声音分离（Omnisep），这是一个新型框架，该框架能够根据Omni-Modal-Modal查询隔离清洁配乐，包括单模式和多模式组成的查询。具体来说，我们介绍了查询混合策略，该策略将培训过程中不同方式的查询功能融合在一起。这使Omnisep能够同时优化多种模式，有效地将所有模式带到统一的框架下以进行声音分离。我们通过允许查询积极或负面影响声音分离来进一步增强这种灵活性，从而促进根据需要的保留或去除特定声音。最后，OmniSep采用了一种被检索的演示方法，称为Query-aug，该方法可实现开放式声音分离。对音乐，VGGSOUND-CLEAN+和MUSIC-CLEAN+数据集的实验评估证明了Omnisep的有效性，在文本，图像和声音征服的声音分离任务中实现了最先进的性能。有关示例和更多信息，请访问\ url {https://omnisep.github.io/}的演示页面。

### A Novel Score-CAM based Denoiser for Spectrographic Signature Extraction without Ground Truth 
[[arxiv](https://arxiv.org/abs/2410.21557)] [[cool](https://papers.cool/arxiv/2410.21557)] [[pdf](https://arxiv.org/pdf/2410.21557)]
> **Authors**: Noel Elias
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: ef:2023 International Joint Conference on Neural Networks (IJCNN), Gold Coast, Australia, 2023, pp. 1-8
- **标题**: 基于分数的新型DeNoiser用于光谱学签名提取而没有地面真相
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 基于声纳的音频分类技术是水下声学领域的研究领域。通常，被动声纳传感器拾取的水下噪声包含各种各样的信号，这些信号穿过海洋，并转化为光谱图像。结果，旨在显示某个对象的时间频率数据的相应频谱图通常包括大量外部噪声的音调区域，这些噪声可以有效地干扰“接触”。因此，从水下音频信号中提取的大多数光谱样品由于杂物而无法使用，并且缺乏不同物体之间所需的不可区分性。通过有限的清洁真实数据进行监督培训，为这些音频信号创建分类模型被严重瓶颈。本文通过开发一种基于分数的DeNoiser来从嘈杂的光谱数据数据中提取物体的签名，而无需给出任何地面真相数据，从而得出了几种新技术来解决这个问题。特别是，本文提出了一种新颖的生成对抗网络架构，用于学习和生成与低功能频谱输入相似的分布中的光谱学训练数据。此外，本文还基于可推广的类激活映射映射器，用于不同的声学数据分布，甚至是现实世界数据分布。这些实验利用了这些新颖的结构并提出了剥离技术，证明了与当前音频分类标准相比，最先进的降噪精度和提高的分类精度。因此，这种方法不仅包含音频数据的应用程序，而且还针对全球用于机器学习的无数数据分布。

### A Transformer Model for Segmentation, Classification, and Caller Identification of Marmoset Vocalization 
[[arxiv](https://arxiv.org/abs/2410.23279)] [[cool](https://papers.cool/arxiv/2410.23279)] [[pdf](https://arxiv.org/pdf/2410.23279)]
> **Authors**: Bin Wu,Shinnosuke Takamichi,Sakriani Sakti,Satoshi Nakamura
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: No comments
- **标题**: 用于分割，分类和呼叫者识别的变压器模型
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: Marmoset是一种高度发声的灵长类动物，已成为研究社会交流行为及其与人类婴儿语言发展相比的基本机制的流行动物模型。在声音交流的研究中，了解呼叫者的身份，呼叫内容和声音交流至关重要。 CNN的先前工作已经实现了用于拨打摩尔马塞啤酒发声的呼叫细分，分类和呼叫者识别的联合模型。但是，CNN在建模远程声学模式时具有局限性。已显示出胜过CNN的变压器架构利用自我发挥的机制有效地将信息在长距离上平行并捕获了Marmoset发声的全局结构。我们建议使用变压器联合分段和分类Marmoset调用，并确定每种发声的呼叫者。

## 软件工程(cs.SE:Software Engineering)

该领域共有 3 篇论文

### Showing LLM-Generated Code Selectively Based on Confidence of LLMs 
[[arxiv](https://arxiv.org/abs/2410.03234)] [[cool](https://papers.cool/arxiv/2410.03234)] [[pdf](https://arxiv.org/pdf/2410.03234)]
> **Authors**: Jia Li,Yuqi Zhu,Yongmin Li,Ge Li,Zhi Jin
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: 根据LLMS的信心选择性地显示LLM生成的代码
- **领域**: 软件工程,计算语言学
- **摘要**: 大型语言模型（LLMS）在代码生成中显示出令人印象深刻的能力，但它们可能会产生错误的程序。阅读程序比编写时间长十倍。向开发人员展示这些错误的程序将浪费开发人员的能量，并将安全风险引入软件。为了解决上述限制，我们提出了诚实的库德，这是一种基于LLM的新型代码生成方法。诚实的编码器选择性地将生成的程序根据LLM的信心向开发人员展示。信心为生成程序的正确性提供了宝贵的见解。为了实现这一目标，我们提出了一种新颖的方法来估计LLM对代码生成的信心。它通过测量LLMS生成的程序之间的多模式相似性来估计置信度。我们收集并发布了一个名为TruthCodebench的多语言基准，该基准由2,265个样本组成，并涵盖了两种流行的编程语言（即Python和Java）。我们将诚实的编码器应用于四个流行的LLM（例如DeepSeek-Coder和Code Llama），并在TruthCodebench上对其进行评估。根据实验，我们获得以下见解。 （1）诚实雕像可以有效地估计LLM的信心，并准确确定生成程序的正确性。例如，诚实的雕像在AUROC中优于最先进的基线，而在AUCPR中，最先进的基线。 （2）诚实雕像可以减少向开发人员显示的错误程序的数量。与八个基线相比，它可以显示出更正确的程序和更少的错误程序向开发人员显示。 （3）与毫不差三角的代码相比，Hextcoder仅增加了少量时间开销（大约每要求0.4秒）。 （4）我们讨论未来的方向，以促进LLM在软件开发中的应用。我们希望这项工作可以激发有关测量LLM在执行与代码相关的任务时的可靠性的广泛讨论。

### Collaborative AI in Sentiment Analysis: System Architecture, Data Prediction and Deployment Strategies 
[[arxiv](https://arxiv.org/abs/2410.13247)] [[cool](https://papers.cool/arxiv/2410.13247)] [[pdf](https://arxiv.org/pdf/2410.13247)]
> **Authors**: Chaofeng Zhang,Jia Hou,Xueting Tan,Gaolei Li,Caijuan Chen
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 情感分析中的协作AI：系统体系结构，数据预测和部署策略
- **领域**: 软件工程,人工智能,人机交互
- **摘要**: 大型语言模型（LLM）的人工智能技术的进步一直是一种改变游戏规则的人，尤其是在情感分析中。这一进展使从高度专业化的研究环境转变为行业内实用，广泛的应用程序。但是，整合多种AI模型来处理复杂的多模式数据和相关的高成本提取带来了重大挑战。在以营销为导向的软件开发 +需求的推动下，我们的研究介绍了一个协作AI框架，旨在有效地分配和解决各种AI系统的任务以解决这些问题。最初，我们阐明了从我们的开发过程中得出的关键解决方案，突出了\ emph {chatgpt}，\ emph {google gemini}等生成ai模型的作用，在简化复杂的情感分析任务中为可管理的，分阶段的目标。此外，我们提出了一项详细的案例研究，该案例研究利用我们的协作AI系统在边缘和云中展示了其在分析各种在线媒体渠道的情感方面的有效性。

### WAFFLE: Multi-Modal Model for Automated Front-End Development 
[[arxiv](https://arxiv.org/abs/2410.18362)] [[cool](https://papers.cool/arxiv/2410.18362)] [[pdf](https://arxiv.org/pdf/2410.18362)]
> **Authors**: Shanchao Liang,Nan Jiang,Shangshu Qian,Lin Tan
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: No comments
- **标题**: 华夫饼：自动前端开发的多模式模型
- **领域**: 软件工程,计算语言学,计算机视觉和模式识别
- **摘要**: Web开发涉及将UI设计变成功能性网页，由于HTML的层次结构和样式的复杂性，对于初学者和经验丰富的开发人员来说，这对于两种开发人员来说都是困难的。尽管大型语言模型（LLM）在生成源代码方面表现出了希望，但在UI-TO-HTML代码生成中仍然存在两个主要挑战：（1）有效地表示HTML的LLMS层次结构，（（2）弥合UI设计的视觉性质与HTML代码的文本格式之间的差距。为了应对这些挑战，我们介绍了Waffle，这是一种新的微调策略，该策略使用结构意识到的注意机制来提高LLMS对HTML的结构的理解，并采用对比的微调方法来使LLMS对UI图像和HTML代码的理解。用华夫饼进行微调的模型显示高达9.00 pp（百分点）较高的HTML匹配，0.0982较高的CW-SSIM，32.99个较高的夹子和27.12 pp较高的LLLEM在我们的新基准测试Websight测试中，现有的基准测试和现有基准测试的设计2 code 2 code，超过了当前的微调方法。

## 社交和信息网络(cs.SI:Social and Information Networks)

该领域共有 1 篇论文

### More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram 
[[arxiv](https://arxiv.org/abs/2410.08642)] [[cool](https://papers.cool/arxiv/2410.08642)] [[pdf](https://arxiv.org/pdf/2410.08642)]
> **Authors**: Elisabeth Steffen
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: 12 pages, 10 figures
- **标题**: 不仅仅是模因：一种多模式的主题建模方法，用于阴谋理论
- **领域**: 社交和信息网络,计算语言学,计算机视觉和模式识别,多媒体
- **摘要**: 为了解决社交媒体上（音频）视觉数据的越来越多的流行，并捕获了这种交流的不断发展和动态性质，研究人员已经开始探索无监督方法的潜力，以分析多模式在线内容。但是，现有的研究通常会忽略模因之外的视觉内容，此外缺乏比较跨模式的主题模型的方法。我们的研究通过应用多模式主题建模来分析德语电报频道中的阴谋论来解决这些差距。我们将Bertopic与剪辑使用，以分析2023年10月在571年10月的50，000次电报消息中的文本和视觉数据分析，571德国语言电报频道以传播阴谋论而闻名。通过此数据集，我们通过分析跨模态的对称性和主题的交叉点来提供对单模式和多模式主题模型的见解。我们演示了通过主题建模发现的渠道中共享的文本和视觉内容的多样性，并提出了一个概念框架，以分析阴谋论的交流中的文本和视觉话语策略。我们在主题集团以色列加沙的案例研究中应用框架。

## 音频和语音处理(eess.AS:Audio and Speech Processing)

该领域共有 11 篇论文

### Moshi: a speech-text foundation model for real-time dialogue 
[[arxiv](https://arxiv.org/abs/2410.00037)] [[cool](https://papers.cool/arxiv/2410.00037)] [[pdf](https://arxiv.org/pdf/2410.00037)]
> **Authors**: Alexandre Défossez,Laurent Mazaré,Manu Orsini,Amélie Royer,Patrick Pérez,Hervé Jégou,Edouard Grave,Neil Zeghidour
> **First submission**: 2024-09-17
> **First announcement**: 2024-10-01
> **comment**: No comments
- **标题**: Moshi：实时对话的语音文本基础模型
- **领域**: 音频和语音处理,人工智能,计算语言学,机器学习,声音
- **摘要**: 我们介绍了莫西（Moshi），这是语音文本基础模型和全双工对话框架。当前的口语对话系统依赖于独立组件的管道，即语音活动检测，语音识别，文本对话和文本到语音。这样的框架不能模仿真实对话的经验。首先，它们的复杂性导致相互作用之间几秒钟的延迟。其次，文本是对话的中间方式，在互动中丢失了修改含义的非语言信息（例如情感或非语音声音）。最后，他们依靠对说话者转弯的细分，这没有考虑到重叠的语音，中断和插入。 Moshi通过将口语对话作为语音到语音的一代来完全解决这些独立问题。从文本语言模型骨干开始，Moshi从神经音频编解码器的残差量化器中产生语音，同时单独建模其自己的语音以及用户的语音为平行流。这允许删除明确的说话者转弯和任意对话动力学的建模。此外，我们将层次结构的语义到声音令牌的生成延长了先前的工作，以首先预测与时代令牌的前缀。不仅这种“内在独白”方法显着提高了所产生的语音的语言质量，而且我们还说明了它如何提供流语言识别和文本到语音。我们由此产生的模型是第一个实时全双工大型语言模型，理论延迟为160ms，实践中为200ms，可在https://github.com/kyutai-labs/moshi上找到。

### Predictive Speech Recognition and End-of-Utterance Detection Towards Spoken Dialog Systems 
[[arxiv](https://arxiv.org/abs/2409.19990)] [[cool](https://papers.cool/arxiv/2409.19990)] [[pdf](https://arxiv.org/pdf/2409.19990)]
> **Authors**: Oswald Zink,Yosuke Higuchi,Carlos Mullov,Alexander Waibel,Tetsunori Kobayashi
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-01
> **comment**: Submitted to ICASSP2025
- **标题**: 对口语对话系统的预测性语音识别和终结性检测
- **领域**: 音频和语音处理,计算语言学,声音
- **摘要**: 有效的口语对话系统应通过快速和节奏的时机促进自然互动，从而反映人类的沟通方式。为了减少响应时间，以前的努力集中在最大程度地减少自动语音识别（ASR）的延迟以优化系统效率。但是，这种方法需要等待ASR完成处理，直到说话者完成讲话为止，这限制了自然语言处理的时间（NLP）以制定准确的响应。作为人类，即使在另一方仍在讲话时，我们也不断期待和准备回应。这使我们能够做出适当的回应，而不会错过最佳发言时间。在这项工作中，作为模拟这种人类预期行为的对话系统的开创性研究，我们旨在实现一个可以预测即将到来的单词的功能，并估算出使用话语的中部，直到话语结束（EOU）。为了实现这一目标，我们为基于编码器的ASR系统提出了培训策略，其中涉及掩盖话语的未来细分，并促使解码器预测掩盖的音频中的单词。此外，我们开发了一种基于跨注意的算法，该算法同时结合了声学和语言信息以准确检测EOU。实验结果表明，该模型预测即将到来的单词并在实际EOU之前估计最高300毫秒的未来EOU事件的能力。此外，拟议的培训策略在ASR绩效方面表现出一般改进。

### Augmentation through Laundering Attacks for Audio Spoof Detection 
[[arxiv](https://arxiv.org/abs/2410.01108)] [[cool](https://papers.cool/arxiv/2410.01108)] [[pdf](https://arxiv.org/pdf/2410.01108)]
> **Authors**: Hashim Ali,Surya Subramani,Hafiz Malik
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: 通过洗涤攻击进行音频欺骗检测来增强
- **领域**: 音频和语音处理,人工智能,声音
- **摘要**: 最近的文本到语音（TTS）的开发使语音克隆（VC）更加现实，负担得起且易于访问。这引起了这项技术的许多潜在滥用，包括乔·拜登（Joe Biden）的新罕布什尔州Deepfake Robocall。已经提出了几种方法来检测此类克隆。但是，这些方法已经在相对干净的数据库上进行了培训和评估。最近，ASVSPOOF 5 Challenge推出了一个新的人群，包括各种声学条件，包括各种欺骗攻击和编解码条件。本文是我们对ASVSPOOF 5挑战的提交，旨在调查在ASVSPOOF 5数据库中使用数据扩展训练的音频欺骗检测的性能。结果表明，我们的系统在A18，A19，A20，A26和A30欺骗攻击以及CO08，C09和C10的压缩条件下执行最差。

### Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data 
[[arxiv](https://arxiv.org/abs/2410.02056)] [[cool](https://papers.cool/arxiv/2410.02056)] [[pdf](https://arxiv.org/pdf/2410.02056)]
> **Authors**: Sreyan Ghosh,Sonal Kumar,Zhifeng Kong,Rafael Valle,Bryan Catanzaro,Dinesh Manocha
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: Accepted at ICLR 2025. Code and Checkpoints available here: https://github.com/Sreyan88/Synthio
- **标题**: Synthio：使用合成数据增强小规模的音频分类数据集
- **领域**: 音频和语音处理,人工智能,计算语言学
- **摘要**: 我们提出了Synthio，这是一种使用合成数据来增强小规模音频分类数据集的新方法。我们的目标是通过有限的标记数据提高音频分类精度。传统的数据增强技术应用人工转换（例如，添加随机噪声或掩盖段），难以创建捕获现实世界中存在的真实多样性的数据。为了解决这一缺点，我们建议使用从文本到ADIO（T2A）扩散模型产生的合成音频来增强数据集。但是，合成有效的增强是具有挑战性的，因为不仅生成的数据与基本的小规模数据集在听觉上是一致的，而且还应具有足够的组成多样性。为了克服第一个挑战，我们使用首选项优化将T2A模型的世代与小规模数据集保持一致。这样可以确保生成数据的声学特性与小规模数据集保持一致。为了应对第二项挑战，我们提出了一种新颖的字幕生成技术，该技术利用大语言模型的推理能力来（1）产生多样的有意义的音频字幕，（2）迭代地完善其质量。然后，生成的字幕用于提示对齐的T2A模型。我们在十个数据集和四个模拟有限数据设置上广泛评估Synthio。结果表明，我们的方法始终使用仅在虚弱的音频集训练的T2A模型中均优于所有基准的0.1％-39％。

### HRTF Estimation using a Score-based Prior 
[[arxiv](https://arxiv.org/abs/2410.01562)] [[cool](https://papers.cool/arxiv/2410.01562)] [[pdf](https://arxiv.org/pdf/2410.01562)]
> **Authors**: Etienne Thuillier,Jean-Marie Lemercier,Eloi Moliner,Timo Gerkmann,Vesa Välimäki
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 使用基于分数的先验估算HRTF
- **领域**: 音频和语音处理,机器学习,声音
- **摘要**: 我们提出了与头部相关的传输函数（HRTF）估计方法，该方法依赖于基于分数的扩散模型给出的数据驱动的先验。使用自然激发信号，例如人言。通过基于房间声学的统计行为优化回响的参数模型，可以估算房间的冲动响应与HRTF一起估计。给定回响测量和激发信号的HRTF的后验分布是使用基于得分的HRTF先验和对数似然近似值建模的。我们表明，所得的方法优于几个基线，包括一个Oracle推荐系统，该系统基于在给定的到达方向的最小距离在我们的训练集中分配了最佳HRTF。特别是，我们表明，扩散先验可以说明HRTF中高频含量的巨大变化。

### An Eye for an Ear: Zero-shot Audio Description Leveraging an Image Captioner using Audiovisual Distribution Alignment 
[[arxiv](https://arxiv.org/abs/2410.05997)] [[cool](https://papers.cool/arxiv/2410.05997)] [[pdf](https://arxiv.org/pdf/2410.05997)]
> **Authors**: Hugo Malard,Michel Olvera,Stéphane Lathuiliere,Slim Essid
> **First submission**: 2024-10-08
> **First announcement**: 2024-10-09
> **comment**: No comments
- **标题**: 耳朵的眼睛：零拍音频描述使用视听分布对齐来利用图像标题
- **领域**: 音频和语音处理,计算机视觉和模式识别,机器学习,声音
- **摘要**: 多模式的大型语言模型在图像字幕上加剧了进度。这些模型在庞大的图像数据集上进行了微调，对语义概念具有深刻的理解。在这项工作中，我们表明可以将此功能重新使用以进行音频字幕，可以利用联合图像语言解码器来描述与视频中图像序列相关的听觉内容。这可以通过多模式对准来实现。然而，由于现实世界视频中的可见元素和可见元素之间的固有差异，因此这项多模式对齐任务是不平凡的。此外，多模式表示学习通常依赖于对比学习，面对所谓的模态差距的挑战，这阻碍了模态之间的平稳整合。在这项工作中，我们介绍了一种新颖的方法，用于弥合视听模式差距，通过与音频主链和图像字幕仪的代币的分布相匹配。我们的方法将音频令牌分布与图像令牌的分布保持一致，从而使模型能够以无监督的方式执行零击音频字幕，同时保持初始图像字幕字幕组件未经更换。这种对齐允许通过将图像编码与对齐的音频编码相结合或替换图像来使用音频或视听输入。与现有方法相比，我们的方法在零击音频字幕中的性能显着提高。

### Swin-BERT: A Feature Fusion System designed for Speech-based Alzheimer's Dementia Detection 
[[arxiv](https://arxiv.org/abs/2410.07277)] [[cool](https://papers.cool/arxiv/2410.07277)] [[pdf](https://arxiv.org/pdf/2410.07277)]
> **Authors**: Yilin Pan,Yanpei Shi,Yijia Zhang,Mingyu Lu
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: SWIN-BERT：专为基于语音的阿尔茨海默氏症痴呆检测而设计的功能融合系统
- **领域**: 音频和语音处理,人工智能,计算语言学,声音
- **摘要**: 语音通常用于构建自动的阿尔茨海默氏症痴呆症（AD）检测系统，因为声学和语言能力显示出早期阶段患有AD的人的下降。但是，语音不仅包括与广告相关的本地和全球信息，还包括与认知状况无关的其他信息，例如年龄和性别。在本文中，我们提出了一个基于语音的系统，名为Swin-Bert，用于自动痴呆检测。对于声学部分，转移的Windows多头注意力是从图像中提取本地和全局信息的，用于设计基于声学的系统。为了使年龄和性别对声学特征提取的影响，它们被用作设计的声学系统的额外输入。对于语言部分，在与AD和没有AD的人之间有很大变化的节奏相关信息在将音频记录转录为成绩单时被删除。为了补偿删除的节奏相关信息，提议将字符级的成绩单用作单词级bert式系统的额外输入。最后，Swin-Bert结合了从我们提出的基于声学的系统中学到的声学特征与我们的基于语言的系统。这些实验基于国际痴呆探测挑战提供的两个数据集：Adress和Adresso。结果表明，所提出的声学系统和语言系统都可以更好或与以前在两个数据集上的研究相媲美。拟议的Swin-bert系统在地址和地址数据集上分别为85.58 \％f-评分和87.32 \％F-评分，可以实现出色的结果。

### Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities 
[[arxiv](https://arxiv.org/abs/2410.11190)] [[cool](https://papers.cool/arxiv/2410.11190)] [[pdf](https://arxiv.org/pdf/2410.11190)]
> **Authors**: Zhifei Xie,Changqiao Wu
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: Technical report, work in progress. Demo and code: https://github.com/gpt-omni/mini-omni2
- **标题**: Mini-OMNI2：朝开源GPT-4O带有视觉，语音和双工功能
- **领域**: 音频和语音处理,人工智能,计算机视觉和模式识别,机器学习,声音
- **摘要**: GPT-4O是一种无所不包的模型，代表了大型多模式模型开发的一个里程碑。它可以理解视觉，听觉和文本方式，直接输出音频，并支持灵活的双工交互。来自开源社区的模型通常可以实现GPT-4O的某些功能，例如视觉理解和语音聊天。然而，由于多模式数据，复杂的模型体系结构和培训过程的复杂性，培训结合了所有模式的统一模型都具有挑战性。在本文中，我们介绍了Mini-Omni2，这是一位视觉Audio助手，能够对Visoin和Audio查询提供实时，端到端的语音响应。通过整合预处理的视觉和听觉编码器，Mini-OMNI2保持单个方式的性能。我们提出了一个三阶段的训练过程以使模式保持一致，从而使语言模型在有限的数据集上训练后可以处理多模式输入和输出。为了进行互动，我们引入了基于命令的中断机制，从而使与用户更加灵活地交互。据我们所知，Mini-OMNI2是GPT-4O的最接近的复制品之一，它们具有相似的功能形式，我们希望它可以为后续研究提供宝贵的见解。

### Exploiting Longitudinal Speech Sessions via Voice Assistant Systems for Early Detection of Cognitive Decline 
[[arxiv](https://arxiv.org/abs/2410.12885)] [[cool](https://papers.cool/arxiv/2410.12885)] [[pdf](https://arxiv.org/pdf/2410.12885)]
> **Authors**: Kristin Qi,Jiatong Shi,Caroline Summerour,John A. Batsis,Xiaohui Liang
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-17
> **comment**: IEEE International Conference on E-health Networking, Application & Services
- **标题**: 通过语音助理系统利用纵向语音会话，以早期检测认知能力下降
- **领域**: 音频和语音处理,计算语言学,定量方法
- **摘要**: 轻度认知障碍（MCI）是阿尔茨海默氏病（AD）的早期阶段，这是一种神经退行性疾病的形式。 MCI的早期鉴定对于通过及时的干预措施延迟其进展至关重要。现有的研究表明，使用临床访谈或数字设备收集的语音检测MCI的可行性。但是，这些方法通常分析在有限时间点收集的数据，从而限制了它们随着时间的推移识别认知变化的能力。本文提出了一项使用语音助理系统（VAS）的纵向研究，以在18个月的时间内以三个月的时间间隔远程收集七次演讲数据。我们提出了两种改善MCI检测和认知变化预测的方法。第一种方法包含历史数据，而第二种方法可以预测两个时间点的认知变化。我们的结果表明，在纳入历史数据时的改善：在声学特征的情况下，MCI检测的平均F1得分从58.6％提高到71.2％（增长12.6％），在语言特征的情况下从62.1％提高到75.1％（提高13.0％）。此外，在声学特征的情况下，认知变化的预测达到了73.7％的F1评分。这些结果证实了基于VAS的语音会话的潜力，即早期发现认知能力下降。

### Can a Machine Distinguish High and Low Amount of Social Creak in Speech? 
[[arxiv](https://arxiv.org/abs/2410.17028)] [[cool](https://papers.cool/arxiv/2410.17028)] [[pdf](https://arxiv.org/pdf/2410.17028)]
> **Authors**: Anne-Maria Laukkanen,Sudarsana Reddy Kadiri,Shrikanth Narayanan,Paavo Alku
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: Accepted in Journal of Voice
- **标题**: None
- **领域**: 音频和语音处理,人工智能,计算语言学,机器学习,声音
- **摘要**: 目的：在几项研究中，尤其是在女说话者中，社会吱吱作响的患病率不足。以前，通过将语音的感知评估与传统的声学参数（例如谐波与噪声比率和Cepstral Peak的突出性）相结合，对社会吱吱作响的研究进行了研究。在当前的研究中，机器学习（ML）被用来自动区分低数量的社会吱吱作响的语音和高度社会吱吱作响的语音。方法：首先，由两位配音专家评估了90位女说话者在芬兰语中产生的连续语音样本中的吱吱作响量。根据他们的评估，将语音样本分为两类（低$ vs $。吱吱作响的高度）。使用语音信号及其吱吱作响的标签，训练了七个不同的ML模型。三个光谱表示用作每个模型的特征。结果：结果表明，以下两个系统获得了最佳性能（精度为71.1 \％）：使用MEL-SPECTROGRAM特征和使用MEL频率Cepstral系数功能的Adaboost分类器和决策树分类器。结论：社会吱吱作响的研究在社会语言和职业研究中变得越来越流行。对吱吱作响量的传统人类感知评估很费力，因此可以使用ML技术来帮助研究社会吱吱作响的研究人员。在这项研究中报道的分类系统可以被视为基于ML的社会吱吱作用研究中的基准。

### MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark 
[[arxiv](https://arxiv.org/abs/2410.19168)] [[cool](https://papers.cool/arxiv/2410.19168)] [[pdf](https://arxiv.org/pdf/2410.19168)]
> **Authors**: S Sakshi,Utkarsh Tyagi,Sonal Kumar,Ashish Seth,Ramaneswaran Selvakumar,Oriol Nieto,Ramani Duraiswami,Sreyan Ghosh,Dinesh Manocha
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: Project Website: https://sakshi113.github.io/mmau_homepage/
- **标题**: MMAU：大量的多任务音频理解和推理基准
- **领域**: 音频和语音处理,人工智能,计算语言学,声音
- **摘要**: 理解音频（包括语音，非语音声音和音乐）的能力对于AI代理人与世界有效互动至关重要。我们提出了MMAU，这是一种新颖的基准测试，旨在评估需要专家级知识和复杂推理的任务的多模式音频理解模型。 MMAU包括10K精心策划的音频剪辑，以及涵盖语音，环境声音和音乐的人类宣传的自然语言问题和答案。它包括信息提取和推理问题，要求模型在独特而具有挑战性的任务中展示27个不同的技能。与现有的基准不同，MMAU强调了特定于领域的知识的高级感知和推理，这具有挑战性的模型，以解决类似于专家面临的任务。我们评估了18种开源和专有（大）音频语言模型，证明了MMAU面临的重大挑战。值得注意的是，即使是最先进的Gemini Pro V1.5也只能达到52.97％的精度，而最先进的开源Qwen2-Audio只能达到52.50％，突出了相当大的改进空间。我们认为，MMAU将推动音频和多模式研究社区，以开发能够解决复杂音频任务的更先进的音频理解模型。

## 图像和视频处理(eess.IV:Image and Video Processing)

该领域共有 28 篇论文

### Multimodal Alignment of Histopathological Images Using Cell Segmentation and Point Set Matching for Integrative Cancer Analysis 
[[arxiv](https://arxiv.org/abs/2410.00152)] [[cool](https://papers.cool/arxiv/2410.00152)] [[pdf](https://arxiv.org/pdf/2410.00152)]
> **Authors**: Jun Jiang,Raymond Moore,Brenna Novotny,Leo Liu,Zachary Fogarty,Ray Guo,Markovic Svetomir,Chen Wang
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: initial version
- **标题**: 使用细胞分割和点集匹配的组织病理学图像的多模式比对，以进行整合性癌症分析
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习,定量方法
- **摘要**: 组织病理学成像对于癌症研究和临床实践至关重要，具有多重免疫荧光（MXIF），苏木精和曙红（H＆E）提供互补见解。但是，由于形态差异，在细胞水平上对齐不同的污渍仍然是一个挑战。在本文中，我们提出了一个新的使用细胞分割结果的多模式图像对齐的框架。通过将单元格视为点集，我们将连贯的点漂移（CPD）应用于初始比对，并使用图形匹配（GM）对其进行完善。在评估卵巢癌组织微阵列（TMA）上，我们的方法达到了高对准的准确性，从而使模态跨模式的细胞水平特征整合，并从MXIF数据中产生虚拟的H＆E图像，以增强临床解释。

### Looking through the mind's eye via multimodal encoder-decoder networks 
[[arxiv](https://arxiv.org/abs/2410.00047)] [[cool](https://papers.cool/arxiv/2410.00047)] [[pdf](https://arxiv.org/pdf/2410.00047)]
> **Authors**: Arman Afrasiyabi,Erica Busch,Rahul Singh,Dhananjay Bhaskar,Laurent Caplette,Nicholas Turk-Browne,Smita Krishnaswamy
> **First submission**: 2024-09-27
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: 通过多模式编码器网络浏览思维的眼睛
- **领域**: 图像和视频处理,机器学习,神经元和认知
- **摘要**: 在这项工作中，我们使用fMRI测量值探索了对受试者的心理图像的解码。为了实现这种解码，我们首先在受试者观看的视频引起的受试者fMRI信号之间创建了映射。该映射将高维fMRI激活状态与视觉图像相关联。接下来，我们在文本上提示主题，主要是带有没有直接引用视觉对象的情感标签。然后，为了解码可能在一个人的脑海中的视觉图像，我们将这些fMRI测量值的潜在表示与基于对视频本身给出的文本标签的相应视频FMRI保持一致。这种对齐的效果是将视频fMRI与文本启动的fMRI嵌入重叠，从而使我们能够使用fMRI-to-video映射来解码。此外，我们通过包括来自我们团队收集的另外三个主题的录音来增强现有的fMRI数据集，该数据集最初由五个主题的数据组成。我们证明了我们的模型在该增强数据集上的功效，既可以准确地创建映射以及合理地解码心理图像。

### Mixture of Multicenter Experts in Multimodal Generative AI for Advanced Radiotherapy Target Delineation 
[[arxiv](https://arxiv.org/abs/2410.00046)] [[cool](https://papers.cool/arxiv/2410.00046)] [[pdf](https://arxiv.org/pdf/2410.00046)]
> **Authors**: Yujin Oh,Sangjoon Park,Xiang Li,Wang Yi,Jonathan Paly,Jason Efstathiou,Annie Chan,Jun Won Kim,Hwa Kyung Byun,Ik Jae Lee,Jaeho Cho,Chan Woo Wee,Peng Shu,Peilong Wang,Nathan Yu,Jason Holmes,Jong Chul Ye,Quanzheng Li,Wei Liu,Woong Sub Koom,Jin Sung Kim,Kyungsang Kim
> **First submission**: 2024-09-27
> **First announcement**: 2024-10-02
> **comment**: 39 pages
- **标题**: 多模式生成AI的多中心专家的混合物，用于先进放射疗法目标描述
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 临床专家在患者护理中采用各种哲学和策略，受到区域患者人群的影响。但是，现有的医疗人工智能（AI）模型通常经过对数据分布的培训，这些数据分布不成比例地反映了高度普遍的模式，增强了偏见并忽略了临床医生的多样化专业知识。为了克服这一限制，我们介绍了多中心专家（MOME）方法的混合。该方法从各种临床策略中策略性地整合了专业知识，从而增强了AI模型在多个医疗中心概括和适应的能力。基于摩尔的多模式目标量描绘模型，训练了几个射击样品，包括每个医疗中心的图像和临床笔记，优于前列腺癌放射疗法目标描述中的基线方法。当数据特性在整个中心变化或数据可用性限制时，莫米的优势最为明显，这表明其可能是更广泛的临床应用的潜力。因此，MOME框架可以通过仅使用少数样本数据来适应每个医疗中心的特定偏好，而无需在机构之间进行数据共享，从而使基于AI的目标量描述模型在资源受限的医疗设施中部署。扩大摩尔框架中多中心专家的数量将显着提高普遍性，同时还提高了精确放射肿瘤学领域的临床AI应用的可用性和适应性。

### COSMIC: Compress Satellite Images Efficiently via Diffusion Compensation 
[[arxiv](https://arxiv.org/abs/2410.01698)] [[cool](https://papers.cool/arxiv/2410.01698)] [[pdf](https://arxiv.org/pdf/2410.01698)]
> **Authors**: Ziyuan Zhang,Han Qiu,Maosen Zhang,Jun Liu,Bin Chen,Tianwei Zhang,Hewu Li
> **First submission**: 2024-10-02
> **First announcement**: 2024-10-03
> **comment**: No comments
- **标题**: 宇宙：通过扩散补偿有效地压缩卫星图像
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 随着空间中卫星数量的迅速增加及其增强功能，卫星收集的地球观测图像量超过了卫星到地面链路的传输极限。尽管现有的学习图像压缩解决方案通过使用复杂的编码器来提取富有成果的特征作为压缩并使用解码器重建来实现出色的性能，但仍然很难将这些复杂的编码器直接在当前卫星的嵌入式GPU上部署具有有限的计算能力和有限的计算能力和电源来压缩轨道中的图像。在本文中，我们提出了一种简单而有效的学习压缩解决方案，以传输卫星图像。我们首先在卫星上设计一个轻巧的编码器（即减少拖鞋2.6〜5x），以达到高图像压缩比以节省卫星向地面链接。然后，对于地面上的重建，要处理由于简化编码器而导致的特征提取能力退化，我们提出了一个基于扩散的模型，以补偿解码时的图像细节。我们的见解是，卫星的地球观察照片不仅是图像，而且是具有文本对图配对的性质的多模式数据，因为它们与丰富的传感器数据（例如坐标，时间戳等）一起收集，可以用作扩散产生的条件。广泛的实验表明，宇宙在感知和失真指标上都优于最先进的基线。

### NestedMorph: Enhancing Deformable Medical Image Registration with Nested Attention Mechanisms 
[[arxiv](https://arxiv.org/abs/2410.02550)] [[cool](https://papers.cool/arxiv/2410.02550)] [[pdf](https://arxiv.org/pdf/2410.02550)]
> **Authors**: Gurucharan Marthi Krishna Kumar,Janine Mendola,Amir Shmuel
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Accepted to IEEE/CVF Winter Conference on Applications of Computer Vision 2025
- **标题**: NestedMorph：使用嵌套注意机制增强可变形的医疗图像注册
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 可变形的图像登记对于以不同方式以非线性方式对齐医学图像至关重要，从而使变化的解剖结构之间具有精确的空间对应关系。本文介绍了嵌套网络，这是一个利用嵌套注意力融合方法的新型网络，以改善T1加权（T1W）MRI和扩散MRI（DMRI）数据之间的受试者内可变形的注册。 NestedMorph使用多尺度框架从编码器中将高分辨率的空间详细信息与解码器的语义信息集成在一起，从而增强了本地和全局特征提取。我们的模型特别优于现有方法，包括基于CNN的方法，例如VoxelMorph，Midir和Cyclemorph，以及基于变压器的模型，例如Transmorph和VIT-V-NET，以及NiftyReg和Syn等传统技术。使用HCP数据集进行的评估表明，NestedMorph在包括SSIM，HD95和SDLOGJ在内的关键指标上达到了卓越的性能，最高的SSIM为0.89，最低的HD95，2.5和SDLOGJ的最低HD95为0.22。这些结果突出了NestedMorph有效捕获本地图像特征的能力，从而导致了出色的注册性能。这项研究的有希望的结果强调了NestedMorph显着提高可变形的医疗图像注册的潜力，为将来的研究和临床应用提供了强大的框架。源代码和我们的实现可在以下网址获得：https：//github.com/as-lab/marthi-et-al-2024-nestedmorph-demorph-deformable-medical-image-image-registration

### MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2410.02458)] [[cool](https://papers.cool/arxiv/2410.02458)] [[pdf](https://arxiv.org/pdf/2410.02458)]
> **Authors**: Gurucharan Marthi Krishna Kumar,Aman Chadha,Janine Mendola,Amir Shmuel
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: No comments
- **标题**: Medvisionllama：利用预先培训的大型语言模型层来增强医学图像细分
- **领域**: 图像和视频处理,计算语言学,计算机视觉和模式识别
- **摘要**: 大型语言模型（LLMS）以文本数据的多功能性而闻名，越来越多地探索其增强医学图像分割的潜力，这是准确诊断成像的至关重要的任务。这项研究通过整合预先训练的LLM变压器块来探索医学图像分割的增强视觉变压器（VIT）。我们的方法将冷冻的LLM变压器块纳入基于VIT模型的编码器，从而导致各种医学成像方式的分割性能进行了重大改进。我们提出了一种混合注意机制，将全球和局部特征学习与多尺度融合块结合在一起，以跨不同尺度汇总特征。增强的模型显示出显着的性能增长，包括平均骰子得分从0.74增加到0.79，以及精度，精度和JACCARD指数的提高。这些结果证明了基于LLM的变压器在完善医学图像分割方面的有效性，突出了它们的潜力，以显着提高模型的准确性和鲁棒性。源代码和我们的实施可用：https：//bit.ly/3zf2cvs

### SpecSAR-Former: A Lightweight Transformer-based Network for Global LULC Mapping Using Integrated Sentinel-1 and Sentinel-2 
[[arxiv](https://arxiv.org/abs/2410.03962)] [[cool](https://papers.cool/arxiv/2410.03962)] [[pdf](https://arxiv.org/pdf/2410.03962)]
> **Authors**: Hao Yu,Gen Li,Haoyu Liu,Songyan Zhu,Wenquan Dong,Changjian Li
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: No comments
- **标题**: Specsar-Former：基于轻量变压器的网络，用于使用集成Sentinel-1和Sentinel-2的全局LULC映射
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 遥感的最新方法越来越集中于多模式数据，这是由于各种地球观测数据集的可用性的驱动。从不同方式整合互补信息已经显示出在增强语义理解的巨大潜力。但是，现有的全局多模式数据集通常缺乏包含合成孔径雷达（SAR）数据，该数据擅长捕获纹理和结构细节。 SAR作为其他方式的互补观点，促进了用于全球土地使用和土地覆盖（LULC）的空间信息的利用。为了解决这一差距，我们介绍了动态世界+数据集，并使用对齐的SAR数据扩展了当前权威的多光谱数据集，动态世界。此外，为了促进多光谱和SAR数据的组合，我们提出了一种称为Specsar-Former的轻质变压器架构。它结合了两个创新的模块，即双模态增强模块（DMEM）和相互模态聚集模块（MMAM），旨在以分裂融合方式利用两种模态之间的跨信息。这些模块增强了模型整合光谱和空间信息的能力，从而提高了全球LULC语义细分的整体性能。此外，我们采用不平衡的参数分配策略，该策略根据其重要性和信息密度将参数分配给不同的模态。广泛的实验表明，我们的网络的表现优于现有的变压器和基于CNN的模型，其平均相交相交的联合（MIOU）为59.58％，总体准确性（OA）为79.48％，F1得分为71.68％，仅为26.70m。该代码将在https://github.com/reagan1311/lulc_segentation上找到。

### NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping 
[[arxiv](https://arxiv.org/abs/2410.05341)] [[cool](https://papers.cool/arxiv/2410.05341)] [[pdf](https://arxiv.org/pdf/2410.05341)]
> **Authors**: Yamin Li,Ange Lou,Ziyuan Xu,Shengchao Zhang,Shiyu Wang,Dario J. Englot,Soheil Kolouri,Daniel Moyer,Roza G. Bayrak,Catie Chang
> **First submission**: 2024-10-06
> **First announcement**: 2024-10-08
> **comment**: This preprint has been accepted to NeurIPS 2024
- **标题**: Neurobolt：具有多维特征映射的静止状态EEG-TO-FMRI合成
- **领域**: 图像和视频处理,人工智能,机器学习
- **摘要**: 功能磁共振成像（fMRI）是现代神经科学中必不可少的工具，在毫米级空间分辨率下为全脑动力学提供了无创窗口。但是，fMRI受到高运营成本和固定性等问题的限制。随着跨模式合成和大脑解码方面的快速发展，深层神经网络的使用已成为一种有希望的解决方案，用于直接从电脑术（EEG）（EEG）（EEG）（一种更广泛的可访问且可移植的神经图像模式）推断全脑，高分辨率的fMRI特征。尽管如此，从神经活动到fMRI血液动力学反应的复杂投影和脑电图的空间歧义在建模和可解释性方面都构成了重大挑战。迄今为止，相对较少的研究开发了用于EEG-FMRI翻译的方法，尽管它们取得了显着的进展，但在给定研究中的fMRI信号的推论仅限于一小部分大脑区域和单个状况（即静止状态或特定任务）。预测其他大脑区域的功能磁共振成像信号的能力，以及跨越条件的概括，在现场仍然是关键的差距。为了应对这些挑战，我们引入了一个新颖且可推广的框架：Neurobolt，即Neuro-to-bold Transformer，它利用了从时间，空间和频谱域学习的多维表示学习，将RAW EEG数据转化为整个大脑中相应的FMRI活性信号。我们的实验表明，神经托有效地重建了来自原发性感觉，高级认知区域和深层皮层大脑区域的看不见的静息状态FMRI信号，从而实现了先进的准确性，从而有可能在各种条件和地点进行推广，从而在这两种模态上促进了这两种模态的整合。

### Utility of Multimodal Large Language Models in Analyzing Chest X-ray with Incomplete Contextual Information 
[[arxiv](https://arxiv.org/abs/2410.07111)] [[cool](https://papers.cool/arxiv/2410.07111)] [[pdf](https://arxiv.org/pdf/2410.07111)]
> **Authors**: Choonghan Kim,Seonhee Cho,Joo Heung Yoon
> **First submission**: 2024-09-19
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 多模式大语言模型的效用在分析胸部X射线上使用不完整的上下文信息
- **领域**: 图像和视频处理,计算语言学,计算机视觉和模式识别
- **摘要**: 背景：大型语言模型（LLM）正在临床环境中使用，但它们的性能可能会受到不完整的放射学报告。我们测试了多模式LLM（使用文本和图像）是否可以提高胸部射线照相报告中的准确性和理解，从而使它们在临床决策支持方面更有效。目的：评估LLM在使用不完整的数据和多模式数据中从胸部射线照相报告中产生准确印象的鲁棒性。材料和方法：我们使用了MIMIC-CXR数据库中的300个放射学图像报告对。三个LLM（OpenFlamingo，Medflamingo，IdeFics）以仅文本和多模式格式进行了测试。印象首先是从全文中产生的，然后通过删除20％，50％和80％的文本进行测试。使用胸部X射线评估添加图像的影响，并使用三个具有统计分析的指标比较模型性能。 Results: The text-only models (OpenFlamingo, MedFlamingo, IDEFICS) had similar performance (ROUGE-L: 0.39 vs. 0.21 vs. 0.21; F1RadGraph: 0.34 vs. 0.17 vs. 0.17; F1CheXbert: 0.53 vs. 0.40 vs. 0.40), with OpenFlamingo performing best on complete text (p<0.001).在所有模型中，绩效均未完整的数据下降。但是，添加图像显着提高了Medflamingo和Idefics的性能（P <0.001），即使文本不完整，也相等或超过OpenFlamingo。结论：LLM可以使用不完整的放射学数据产生低质量的输出，但是多模式LLM可以提高可靠性并支持临床决策。关键字：大语言模型；多模式；语义分析；胸部射线照相；临床决策支持；

### VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis 
[[arxiv](https://arxiv.org/abs/2410.08397)] [[cool](https://papers.cool/arxiv/2410.08397)] [[pdf](https://arxiv.org/pdf/2410.08397)]
> **Authors**: Andrew Hoopes,Victor Ion Butoi,John V. Guttag,Adrian V. Dalca
> **First submission**: 2024-10-10
> **First announcement**: 2024-10-11
> **comment**: 21 pages, 5 figures, vision-language agent, medical image analysis, neuroimage foundation model
- **标题**: VoxelPrompt：用于接地医学图像分析的视觉语言代理
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 我们提出了VoxelPrompt，这是一个由代理驱动的视觉语言框架，该框架通过自然语言，图像量和分析指标的联合建模来应对不同的放射学任务。 VoxelPrompt是多模式和多功能的，在提供定量接地的图像分析的同时，利用语言交互的灵活性。给定可变数量的3D医疗量，例如MRI和CT扫描，VoxelPrompt采用了一种语言代理，该语言代理可以迭代地预测可执行指令，以求解输入提示符指定的任务。这些指令与视觉网络通信以编码图像特征并生成体积输出（例如，分段）。 VoxelPrompt解释了中间说明的结果，并计划进一步的措施来计算离散措施（例如，一系列扫描中的肿瘤生长），并向用户呈现相关的输出。我们在不同的神经影像任务的沙盒中评估了该框架，我们表明单个voxelprompt模型可以描绘数百个解剖学和病理特征，测量许多复杂的形态特性，并对病变特征进行开放语言分析。 VoxelPrompt的准确性与微调的单件任务模型相似，以进行分割和视觉询问效率，同时促进了更大范围的任务。因此，通过支持语言互动的准确图像处理，VoxelPrompt为许多传统上需要专门模型来解决的众多成像任务提供了全面的实用程序。

### EG-SpikeFormer: Eye-Gaze Guided Transformer on Spiking Neural Networks for Medical Image Analysis 
[[arxiv](https://arxiv.org/abs/2410.09674)] [[cool](https://papers.cool/arxiv/2410.09674)] [[pdf](https://arxiv.org/pdf/2410.09674)]
> **Authors**: Yi Pan,Hanqi Jiang,Junhao Chen,Yiwei Li,Huaqin Zhao,Yifan Zhou,Peng Shu,Zihao Wu,Zhengliang Liu,Dajiang Zhu,Xiang Li,Yohannes Abate,Tianming Liu
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: 例如，尖峰形式：尖峰神经网络上的眼睛凝视引导的变压器进行医学图像分析
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习,神经和进化计算
- **摘要**: 神经形态计算已成为传统人工智能的一种有希望的节能替代品，主要利用在神经态硬件上实现的尖峰神经网络（SNN）。在基于SNN的卷积神经网络（CNN）和变压器体系结构中取得了重大进步。但是，医学成像域的神经形态计算仍然没有被逐渐倍增。在这项研究中，我们介绍了EG-SpikeFormer，这是一种针对临床任务的SNN体系结构，其中包含眼睛凝视数据，以指导模型对医学图像中诊断相关区域的关注。我们开发的方法有效地解决了常规模型中通常观察到的快捷学习问题，尤其是在临床数据有限且对模型可靠性，可推广性和透明度的需求较高的情况下。我们的EG SpikeFormer不仅在医学图像预测任务中表现出卓越的能源效率和性能，而且还通过多模式信息对准增强了临床相关性。通过结合眼睛的数据，该模型改善了可解释性和泛化，开辟了在医疗保健中应用神经形态计算的新方向。

### ViT3D Alignment of LLaMA3: 3D Medical Image Report Generation 
[[arxiv](https://arxiv.org/abs/2410.08588)] [[cool](https://papers.cool/arxiv/2410.08588)] [[pdf](https://arxiv.org/pdf/2410.08588)]
> **Authors**: Siyou Li,Beining Xu,Yihao Luo,Dong Nie,Le Zhang
> **First submission**: 2024-10-11
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: Llama的VIT3D对齐3：3D医学图像报告生成
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 旨在从医学图像中产生详细的文本报告的自动医疗报告生成（MRG）已成为该领域的关键任务。 MRG系统可以通过减少报告写作所需的时间和精力来增强放射学工作流，从而提高诊断效率。在这项工作中，我们提出了一种使用多模式模型自动MRG的新颖方法。具体而言，我们采用了从M3D-CLIP引入的3D Vision Transformer（VIT3D）图像编码器来处理3D扫描，并使用Asclepius-llama3-8B作为语言模型来通过自动回归解码来生成文本报告。该实验表明，我们的模型在MRG任务验证集上达到了0.3的平均绿色评分，并且在视觉问题答案（VQA）任务验证集的平均准确度中为0.61，表现优于基线模型。我们的方法通过在小数据集上调整模型来证明Llama3对Llama3对自动MRG和VQA任务的有效性。

### Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart Segmentation 
[[arxiv](https://arxiv.org/abs/2410.10551)] [[cool](https://papers.cool/arxiv/2410.10551)] [[pdf](https://arxiv.org/pdf/2410.10551)]
> **Authors**: Chenyu Zhang,Wenxue Guan,Xiaodan Xing,Guang Yang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: No comments
- **标题**: 保持心脏完整性：一种注入拓扑的方法
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 全心脏细分（WHS）支持心血管疾病（CVD）诊断，疾病监测，治疗计划和预后。近年来，深度学习已成为WHS应用程序最广泛使用的方法。但是，全心脏结构的分割面临许多挑战，包括心脏形状的变异性，临床伪像，运动和对比度与噪声比率较差，多中心数据的域变化以及CT和MRI的独特方式。为了解决这些局限性并提高细分质量，本文介绍了一个新的拓扑保存模块，该模块已集成到深神经网络中。该实现通过使用学习的拓扑保存字段来完全基于3D卷积，因此可以实现解剖学上的分割，因此对于3D Voxel数据非常有效。我们将结构之间的自然约束纳入端到端训练，并丰富神经网络的特征表示。该方法的有效性在开源医学心脏数据集上进行了验证，专门使用WHS ++数据。结果表明，该体系结构的表现异常出色，在测试过程中达到了骰子系数为0.939。这表明了单个结构的完整拓扑保存，并且在保留整体场景拓扑方面明显优于其他基线。

### Performance Evaluation of Deep Learning and Transformer Models Using Multimodal Data for Breast Cancer Classification 
[[arxiv](https://arxiv.org/abs/2410.10146)] [[cool](https://papers.cool/arxiv/2410.10146)] [[pdf](https://arxiv.org/pdf/2410.10146)]
> **Authors**: Sadam Hussain,Mansoor Ali,Usman Naseem,Beatriz Alejandra Bosques Palomo,Mario Alexis Monsivais Molina,Jorge Alberto Garza Abdala,Daly Betzabeth Avendano Avalos,Servando Cardona-Huerta,T. Aaron Gulliver,Jose Gerardo Tamez Pena
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-15
> **comment**: The paper was accepted and presented in 3rd Workshop on Cancer Prevention, detection, and intervenTion (CaPTion @ MICCAI 2024)
- **标题**: 使用多模式数据进行乳腺癌分类的深度学习和变压器模型的绩效评估
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 乳腺癌（BC）的增加和死亡率是女性的主要全球关注点。与人类专家读者相比，深度学习（DL）在BC分类中表现出了出色的诊断性能。但是，单峰（数字乳房X线摄影）的主要使用可能会限制诊断模型的当前性能。为了解决这个问题，我们收集了一个包括成像和文本数据的新型多模式数据集。这项研究提出了用于BC分类的多模式DL体系结构，利用图像（乳房X线照片；四个视图）和我们的新内部数据集中的文本数据（放射学报告）。应用了各种增强技术来增强成像和文本数据的训练数据大小。我们探索了11个SOTA DL架构（VGG16，VGG19，Resnet34，Resnet50，Mobilenet-V3，Effnet-B0，Effnet-B0，Effnet-B1，Effnet-B1，Effnet-B2，Effnet-B2，Effnet-B3，Effnet-B7，Effnet-B7和Vision Transformer（VET）的性能。对于文本特征提取，我们使用了人工神经网络（ANN）或长期记忆（LSTM）网络。然后，使用晚融合技术将组合成像和文本特征输入以进行BC分类的ANN分类器。我们评估了不同的特征提取器和分类器布置。 VGG19和ANN组合的最高精度为0.951。精确，VGG19和ANN组合再次超过了其他CNN和LSTM，基于ANN的体系结构的得分为0.95。 VGG16+LSTM实现了0.903的最佳灵敏度评分。 VGG19+LSTM实现了最高的F1得分为0.931。只有VGG16+LSTM在曲线（AUC）下达到了0.937的最佳区域，而VGG16+LSTM紧随其后的AUC分数为0.929。

### Scalable Drift Monitoring in Medical Imaging AI 
[[arxiv](https://arxiv.org/abs/2410.13174)] [[cool](https://papers.cool/arxiv/2410.13174)] [[pdf](https://arxiv.org/pdf/2410.13174)]
> **Authors**: Jameson Merkow,Felix J. Dorfner,Xiyu Yang,Alexander Ersoy,Giridhar Dasegowda,Mannudeep Kalra,Matthew P. Lungren,Christopher P. Bridge,Ivan Tarapov
> **First submission**: 2024-10-16
> **First announcement**: 2024-10-17
> **comment**: No comments
- **标题**: 医学成像中的可扩展漂移监测AI
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 人工智能（AI）在医学成像中的整合具有先进的临床诊断，但在管理模型漂移和确保长期可靠性方面构成了挑战。为了应对这些挑战，我们开发了MMC+，这是一个可扩展漂移监视的增强框架，建立在ChexStray框架的基础上，该框架使用多模式数据一致性引入了医学成像AI模型的实时漂移检测。这项工作扩展了原始框架的方法，为现实世界中的医疗设置提供了一种更可扩展和可自适应的解决方案，并为连续性能监控的连续性能监控提供了可靠且具有成本效益的替代方案，以解决连续和周期性监视方法的限制。 MMC+引入了对原始框架的关键改进，包括对各种数据流的更强大的处理，通过集成基础模型（如MedimimageInsight，无需特定地点训练的高维图像嵌入）的基础模型的整合，以及在动态临床环境中提出不确定性范围的高维图像嵌入。 MMC+在Covid-19大流行期间，通过马萨诸塞州综合医院的现实数据验证，有效地检测到了重大的数据变化并将其与模型性能变化相关联。 MMC+虽然无法直接预测性能降解，但它是预警系统，表明AI系统何时可能会偏离可接受的性能界限并实现及时的干预措施。通过强调监视多样化数据流的重要性并与模型性能一起评估数据转移，这项工作有助于在临床环境中更广泛地采用和集成AI解决方案。

### From Real Artifacts to Virtual Reference: A Robust Framework for Translating Endoscopic Images 
[[arxiv](https://arxiv.org/abs/2410.13896)] [[cool](https://papers.cool/arxiv/2410.13896)] [[pdf](https://arxiv.org/pdf/2410.13896)]
> **Authors**: Junyang Wu,Fangfang Xie,Jiayuan Sun,Yun Gu,Guang-Zhong Yang
> **First submission**: 2024-10-14
> **First announcement**: 2024-10-18
> **comment**: No comments
- **标题**: 从真实工件到虚拟参考：用于翻译内窥镜图像的强大框架
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 域适应性桥接不同方式的分布，在多模式医学图像分析中起着至关重要的作用。在内窥镜成像中，将术前数据与术中成像相结合对于手术计划和导航很重要。然而，现有的域适应方法受到体内工件引起的分配变化的阻碍，因此必须采用强大的技术来使嘈杂和艺术品丰富的患者内窥镜内镜视频与从术前术前数据进行重建的干净的虚拟图像进行整理，以便在内部内内术指导过程中进行姿势估计。本文提出了一种伪影图像翻译方法和为此目的的相关基准。该方法结合了一种新颖的``本地全球''翻译框架和噪声弹性提取策略。对于前者，它将图像翻译过程分解为特征denoising的本地步骤，也是全球样式转移的全球步骤。为了提取特征，提出了一种新的对比学习策略，该策略可以提取噪声功能，以在跨域中建立强大的对应关系。已经对公共和内部临床数据集进行了详细的验证，与当前的最新技术相比，表明性能明显提高。

### Automated Segmentation and Analysis of Cone Photoreceptors in Multimodal Adaptive Optics Imaging 
[[arxiv](https://arxiv.org/abs/2410.15158)] [[cool](https://papers.cool/arxiv/2410.15158)] [[pdf](https://arxiv.org/pdf/2410.15158)]
> **Authors**: Prajol Shrestha,Mikhail Kulyabin,Aline Sindel,Hilde R. Pedersen,Stuart Gilson,Rigmor Baraas,Andreas Maier
> **First submission**: 2024-10-19
> **First announcement**: 2024-10-21
> **comment**: No comments
- **标题**: 多模式自适应光学成像中锥形感受器的自动分割和分析
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 视网膜中锥细胞的准确检测和分割对于诊断和管理视网膜疾病至关重要。在这项研究中，我们使用了先进的成像技术，包括来自自适应光学扫描光眼镜（AOSLO）的共焦和非共焦型探测器图像，以分析光感受器以提高准确性。精确的分割对于理解每个锥细胞的形状，区域和分布至关重要。它有助于估计杆占据的周围区域，从而可以计算感兴趣区域中锥形感光体密度。反过来，密度对于评估整体视网膜健康和功能至关重要。我们探索了两个基于U-NET的分割模型：用于计算方式的共焦和细胞的Stardist。分析来自两种方式的图像中的锥细胞并实现一致的结果，这表明了该研究的可靠性和临床应用的潜力。

### Non-Invasive to Invasive: Enhancing FFA Synthesis from CFP with a Benchmark Dataset and a Novel Network 
[[arxiv](https://arxiv.org/abs/2410.14965)] [[cool](https://papers.cool/arxiv/2410.14965)] [[pdf](https://arxiv.org/pdf/2410.14965)]
> **Authors**: Hongqiu Wang,Zhaohu Xing,Weitong Wu,Yijun Yang,Qingqing Tang,Meixia Zhang,Yanwu Xu,Lei Zhu
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: ACMMM 24 MCHM
- **标题**: 无创至侵入性：使用基准数据集增强CFP的FFA合成和新型网络
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 眼底成像是眼科中的关键工具，不同的成像方式的特征是它们的特定优势。例如，眼底荧光素血管造影（FFA）独特地提供了对视网膜血管动力学和病理学的详细见解，在检测微血管异常和灌注状态方面超过了颜色眼底照片（CFP）。但是，传统的入侵FFA涉及由于注射荧光素染料的不适和风险，并且可以从非侵入性CFP中合成FFA图像是有意义的但具有挑战性的。先前的研究主要集中于单个疾病类别中的FFA合成。在这项工作中，我们通过设计扩散引导的生成对抗网络来探索多种疾病中的FFA合成，该网络将自适应和动态扩散过程引入歧视器并添加类别意识到的表示表示器。此外，为了促进这项研究，我们收集了第一个多疾病CFP和FFA配对数据集，该数据集将多种疾病配对眼合成（MPOS）数据集，其中有四种不同的眼底疾病。实验结果表明，与最新方法相比，我们的FFA合成网络可以生成更好的FFA图像。此外，我们引入了一个配对模式诊断网络，以验证合成FFA图像在多种眼疾病的诊断中的有效性，结果表明，我们具有实际CFP图像的合成FFA图像的诊断精度比FFA综合方法的合成精度更高。我们的研究弥合了非侵入性成像与FFA之间的差距，从而提供了有前途的前景来增强眼科诊断和患者护理，重点是通过非侵入性手术来减少对患者的伤害。我们的数据集和代码将发布以支持该领域的进一步研究（https://github.com/whq-xxh/ffa-synthesis）。

### Integrating Deep Learning with Fundus and Optical Coherence Tomography for Cardiovascular Disease Prediction 
[[arxiv](https://arxiv.org/abs/2410.14423)] [[cool](https://papers.cool/arxiv/2410.14423)] [[pdf](https://arxiv.org/pdf/2410.14423)]
> **Authors**: Cynthia Maldonado-Garcia,Arezoo Zakeri,Alejandro F Frangi,Nishant Ravikumar
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: Part of the book series: Lecture Notes in Computer Science ((LNCS,volume 15155))
- **标题**: 将深度学习与眼底疾病预测进行深度学习和光学相干断层扫描
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 早期鉴定出患有心血管疾病风险的患者（CVD）对于有效的预防性护理，减轻医疗保健负担和改善患者的生活质量至关重要。这项研究证明了视网膜光学相干断层扫描（OCT）成像的潜力，并结合了眼底照片，以识别未来的不良心脏事件。我们使用了来自977名患者的数据，这些患者在图像后的5年间隔内经历了CVD，以及1,877名没有CVD的对照参与者，总共2,854名受试者。我们提出了一个基于多渠道变异自动编码器（MCVAE）的新型二元分类网络，该网络将学习患者的眼底和OCT图像的潜在嵌入，以将个体分为两组：未来有可能发展CVD的人和那些不发展的CVD。我们对两种成像方式训练的模型都取得了令人鼓舞的结果（AUROC 0.78 +/- 0.02，准确性0.68 +/- 0.002，精度0.74 +/- 0.02，灵敏度0.73 +/- 0.02和特异性0.68 +/- 0.01），表明其在未来cvd vist ot cvd vist的疗效，以确定其在未来cvd的风险中的疗效。这项研究强调了视网膜OCT成像和眼底照片的潜力，作为预测心血管疾病风险的经济高效的无创替代方法。这些成像技术在验光实践和医院中的广泛可用性进一步增强了其大规模CVD风险筛查的潜力。我们的发现有助于开发标准化的，可访问的方法，用于早期CVD风险识别，有可能改善预防性护理策略和患者预后。

### 2D-3D Deformable Image Registration of Histology Slide and Micro-CT with ML-based Initialization 
[[arxiv](https://arxiv.org/abs/2410.14343)] [[cool](https://papers.cool/arxiv/2410.14343)] [[pdf](https://arxiv.org/pdf/2410.14343)]
> **Authors**: Junan Chen,Matteo Ronchetti,Verena Stehl,Van Nguyen,Muhannad Al Kallaa,Mahesh Thalwaththe Gedara,Claudia Lölkes,Stefan Moser,Maximilian Seidl,Matthias Wieczorek
> **First submission**: 2024-10-18
> **First announcement**: 2024-10-21
> **comment**: 12 pages, 4 figures
- **标题**: 基于ML初始化的组织学载玻片和Micro-CT的2d-3d可变形图像登记
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 组织学和微型层析成像（μCT）的注册的最新发展扩大了病理应用的观点，例如基于μCT的虚拟组织学。由于软组织CT的图像质量较低，该主题仍然具有挑战性。此外，在组织学幻灯片制备过程中，软组织样品通常会变形，因此难以将组织学幻灯片和μCT之间的结构相关联。在这项工作中，我们提出了一种新型的2d-3d多模式变形图像登记方法。该方法使用基于机器学习（ML）的初始化，然后使用注册。该注册通过平面外变形的细化完成了最终确定。该方法在从扁桃体和肿瘤组织中获取的数据集上进行评估。研究了μCT的相比和常规吸收方式。将所提出方法的注册结果与基于强度和基于关键点的方法的注册结果进行了比较。使用基于视觉和基于基准的评估进行比较。与其他两种方法相比，提出的方法证明了卓越的性能。

### Visual Question Answering in Ophthalmology: A Progressive and Practical Perspective 
[[arxiv](https://arxiv.org/abs/2410.16662)] [[cool](https://papers.cool/arxiv/2410.16662)] [[pdf](https://arxiv.org/pdf/2410.16662)]
> **Authors**: Xiaolan Chen,Ruoyu Chen,Pusheng Xu,Weiyi Zhang,Xianwen Shang,Mingguang He,Danli Shi
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-22
> **comment**: No comments
- **标题**: 视觉问题回答眼科：一种进步和实用的观点
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 眼科疾病的准确诊断在很大程度上依赖于多模式眼科图像的解释，这是一个通常耗时且依赖于专业知识的过程。视觉问题回答（VQA）通过合并计算机视觉和自然语言处理来理解和回应有关医学图像的查询，从而提出了潜在的跨学科解决方案。这篇评论文章从理论和实践观点探讨了VQA在眼科中的最新进步和未来前景，旨在为眼部护理专业人员提供更深入的理解和工具，以利用基础模型。此外，我们在增强VQA框架的各种组成部分以适应多模式眼科任务时，讨论了大语言模型（LLM）的有希望的趋势。尽管有前途的前景，但眼科VQA仍然面临几个挑战，包括缺乏带注释的多模式图像数据集，全面和统一的评估方法的必要性以及实现有效现实世界应用的障碍。本文强调了这些挑战，并阐明了未来用LLMS推进眼科VQA的指示。基于LLM的眼科VQA系统的开发要求医疗专业人员与AI专家之间的协作努力，以克服现有的障碍并提高眼部疾病的诊断和护理。

### Enhancing Multimodal Medical Image Classification using Cross-Graph Modal Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2410.17494)] [[cool](https://papers.cool/arxiv/2410.17494)] [[pdf](https://arxiv.org/pdf/2410.17494)]
> **Authors**: Jun-En Ding,Chien-Chin Hsu,Chi-Hsiang Chu,Shuqiang Wang,Feng Liu
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: No comments
- **标题**: 使用跨画模式对比学习增强多模式医学图像分类
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 医学图像的分类是疾病诊断的关键方面，通常通过深度学习技术增强。但是，传统方法通常集中在单峰医学图像数据上，忽略了各种非图像患者数据的整合。本文提出了一个新型的跨界模态对比度学习（CGMCL）框架，用于来自不同数据域的多模式结构数据，以改善医疗图像分类。该模型通过构建跨模式图并利用对比度学习来有效地集成了图像和非图像数据，以使共享潜在空间中的多模式特征对齐。模式间特征缩放模块通过减少异质方式之间的差距进一步优化表示的学习过程。在两个数据集上评估了拟议的方法：帕金森氏病（PD）数据集和一个公共黑色素瘤数据集。结果表明，在准确性，可解释性和早期疾病预测方面，CGMCL的表现优于常规的单峰方法。此外，该方法在多类黑色素瘤分类中显示出卓越的性能。 CGMCL框架为医学图像分类提供了宝贵的见解，同时提供了改善的疾病可解释性和预测能力。

### Frontiers in Intelligent Colonoscopy 
[[arxiv](https://arxiv.org/abs/2410.17241)] [[cool](https://papers.cool/arxiv/2410.17241)] [[pdf](https://arxiv.org/pdf/2410.17241)]
> **Authors**: Ge-Peng Ji,Jingyi Liu,Peng Xu,Nick Barnes,Fahad Shahbaz Khan,Salman Khan,Deng-Ping Fan
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-23
> **comment**: [Work in progress] A comprehensive survey of intelligent colonoscopy in themultimodalera. [Updated Version V2] New training strategy for colonoscopy-specificmultimodallanguage model
- **标题**: 智能结肠镜检查的前沿
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 结肠镜检查目前是结直肠癌最敏感的筛查方法之一。这项研究研究了智能结肠镜检查技术的前沿及其对多模式医学应用的前瞻性影响。有了这个目标，我们首先通过针对结肠镜面的四个任务评估当前以数据为中心和以模型为中心的景观，包括分类，检测，细分和视觉语言理解。这项评估使我们能够确定特定领域的挑战，并揭示了结肠镜检查中的多模式研究仍然开放，以进一步探索。为了拥抱即将到来的多模式时代，我们建立了三个基础举措：大规模的多模式指令调整数据集Coloninst，结肠镜检查设计的多模式语言模型Colongpt和多模式基准。为了促进对这个快速发展的领域的持续监控，我们为最新更新提供了一个公共网站：https：//github.com/ai4colonoscopy/intelliscope。

### Teach Multimodal LLMs to Comprehend Electrocardiographic Images 
[[arxiv](https://arxiv.org/abs/2410.19008)] [[cool](https://papers.cool/arxiv/2410.19008)] [[pdf](https://arxiv.org/pdf/2410.19008)]
> **Authors**: Ruoqi Liu,Yuelin Bai,Xiang Yue,Ping Zhang
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: 教多模式LLMS理解心电图图像
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 心电图（ECG）是用于评估心脏疾病的必不可分性诊断工具。现有的自动解释方法的可推广性有限，重点是狭窄的心脏条件，并且通常取决于原始的生理信号，在资源有限的设置中，只能访问只能打印或数字ECG图像的原始生理信号。多模式大语言模型（MLLM）的最新进展为解决这些挑战提供了有希望的机会。但是，由于缺乏指令调整数据集和良好的ECG图像基准进行定量评估，因此将MLLM的应用在ECG图像解释中仍然具有挑战性。为了应对这些挑战，我们介绍了Ecginstruct，这是一个全面的ECG图像指令调谐数据集，该数据集超过一百万个样本，涵盖了来自不同数据源的各种与ECG相关的任务。使用Ecginstruct，我们开发了脉冲，这是一种针对ECG图像理解的MLLM。此外，我们策划了ECGBENCH，这是一个新的评估基准，涵盖了九个不同数据集的四个关键ECG图像解释任务。我们的实验表明，脉冲设定了新的最先进的，优于总体MLLM，平均准确性提高了15％至30％。这项工作突出了脉搏在临床实践中增强心电图解释的潜力。

### Evaluating the Posterior Sampling Ability of Plug&Play Diffusion Methods in Sparse-View CT 
[[arxiv](https://arxiv.org/abs/2410.21301)] [[cool](https://papers.cool/arxiv/2410.21301)] [[pdf](https://arxiv.org/pdf/2410.21301)]
> **Authors**: Liam Moroy,Guillaume Bourmaud,Frédéric Champagnat,Jean-François Giovannelli
> **First submission**: 2024-10-21
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 评估稀疏视图CT中插头扩散方法的后验采样能力
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 插头播放（PNP）扩散模型是计算机断层扫描（CT）重建的最新方法。这种方法通常会考虑官方图包含足够数量的后验分布的应用，因此使用图像到图像指标（例如PSNR/SSIM）进行评估。取而代之的是，我们有兴趣从具有少量投影的辛克图中重建可压缩流图像，这导致后验分布不再峰值甚至多模式。因此，在本文中，我们旨在评估PNP扩散模型的近似后验，并引入两个后验评估标准。我们对三个不同数据集上的三种PNP扩散方法进行定量评估，以进行几个投影。我们出乎意料地发现，对于每种方法，当投影数减少时，近似后部偏离了真实的后部。

### Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2410.21000)] [[cool](https://papers.cool/arxiv/2410.21000)] [[pdf](https://arxiv.org/pdf/2410.21000)]
> **Authors**: Zhilin Zhang,Jie Wang,Ruiqi Zhu,Xiaoliang Gong
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-29
> **comment**: No comments
- **标题**: 有效的双线性注意融合用于医学视觉问题答案
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 医学视觉问题回答（MEDVQA）在计算机视觉和自然语言处理的交集中引起了人们的兴趣。通过解释医学图像并为相关的临床查询提供精确的答案，MEDVQA有可能支持诊断决策并减少各个领域（尤其是放射学）的工作量。尽管最近的方法在很大程度上依赖于统一的大型预训练的视觉语言模型，但在该领域中，对更有效的融合机制的研究仍然相对有限。在本文中，我们介绍了一种新颖的融合模型Omniban，该模型整合了正交性丢失，多头注意力和双线性注意网络，以实现高计算效率以及稳定的性能。我们进行全面的实验，并提供有关双线性注意融合如何近似较大融合模型（如跨模式变压器）的性能的见解。我们的结果表明，综合剂在保持较低的计算成本的同时，在关键的MEDVQA基准上胜过传统方法。效率和准确性之间的这种平衡表明，综合剂可能是现实世界中医学图像问题回答的可行选择，通常会限制计算资源。

### MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2410.22362)] [[cool](https://papers.cool/arxiv/2410.22362)] [[pdf](https://arxiv.org/pdf/2410.22362)]
> **Authors**: Jialin Luo,Yuanzhi Wang,Ziqi Gu,Yide Qiu,Shuaizhen Yao,Fuyun Wang,Chunyan Xu,Wenhua Zhang,Dan Wang,Zhen Cui
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-30
> **comment**: Accepted by NeurIPS 2024
- **标题**: MMM-RS：多式联运，多GSD，多场景遥感数据集和基准，用于文本到图像生成
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 最近，由于其准确的分布建模和稳定的训练过程，基于扩散的生成范式通过文本提示获得了令人印象深刻的一般图像生成能力。但是，由于缺乏具有各种方式，地面样品距离（GSD）和场景的全面遥感图像生成数据集，因此从规模和透视方面产生了与一般图像截然不同的多种遥感（RS）图像仍然是一个巨大的挑战。在本文中，我们提出了一个多模式，多GSD，多场景的遥感（MMM-RS）数据集和基准，用于在不同的遥感方案中进行文本到图像生成。具体来说，我们首先收集了9个公开可用的RS数据集并为所有样品进行标准化。为了将RS图像桥接到文本语义信息中，我们利用大规模的视觉语言模型自动输出文本提示并执行手工制作的纠正，从而导致信息丰富的文本图像对（包括多模式图像）。特别是，我们设计了一些方法，以在单个样本中获得不同的GSD和各种环境（例如，低光，有雾）的图像。通过大量的手动筛选和精炼注释，我们最终获得了一个MMM-RS数据集，该数据集约为210万个文本图像对。广泛的实验结果证明，我们提出的MMM-RS数据集允许现成的扩散模型在各种方式，场景，天气条件和GSD上生成各种RS图像。该数据集可从https://github.com/ljl5261/mmm-rs获得。

### Compositional Segmentation of Cardiac Images Leveraging Metadata 
[[arxiv](https://arxiv.org/abs/2410.23130)] [[cool](https://papers.cool/arxiv/2410.23130)] [[pdf](https://arxiv.org/pdf/2410.23130)]
> **Authors**: Abbas Khan,Muhammad Asad,Martin Benning,Caroline Roney,Gregory Slabaugh
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2025
- **标题**: 利用元数据的心脏图像的组成分割
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 心脏图像分割对于随着时间的推移而自动化心脏功能评估和监测心脏结构变化至关重要。受图像分析中的粗到精细方法的启发，我们提出了一种新型的多任务组成分割方法，可以同时将心脏定位在心脏图像中，并对不同感兴趣的区域进行基于部分的分割。我们证明，这种组成方法比对解剖学的直接分割取得更好的结果。此外，我们提出了一种新型的跨模式特征整合（CMFI）模块，以利用与图像采集期间收集的心脏成像相关的元数据。我们使用公共数据集，多疾病，多视图和多中心（M＆MS-2）以及多结构超声分段（CAMUS）数据对两种不同的模态进行实验，以展示所提出的组成方法和交叉模块组合模块的效率。源代码可用：https：//github.com/kabbas570/compseg-metadata。

## 信号处理(eess.SP:Signal Processing)

该领域共有 11 篇论文

### Loneliness Forecasting Using Multi-modal Wearable and Mobile Sensing in Everyday Settings 
[[arxiv](https://arxiv.org/abs/2410.00020)] [[cool](https://papers.cool/arxiv/2410.00020)] [[pdf](https://arxiv.org/pdf/2410.00020)]
> **Authors**: Zhongqi Yang,Iman Azimi,Salar Jafarlou,Sina Labbaf,Brenda Nguyen,Hana Qureshi,Christopher Marcotullio,Jessica L. Borelli,Nikil Dutt,Amir M. Rahmani
> **First submission**: 2024-09-15
> **First announcement**: 2024-10-02
> **comment**: ef:2023 IEEE 19th International Conference on Body Sensor Networks (BSN), 1-4
- **标题**: 在日常设置中使用多模式可穿戴和移动感测的孤独预测
- **领域**: 信号处理,机器学习
- **摘要**: 孤独对身体和心理健康的不利影响是深远的。尽管以前的研究利用移动传感技术来检测心理健康问题，但很少有研究利用最先进的可穿戴设备来预测孤独感并估算孤独感及其预测性的生理表现。这项研究的主要目的是通过使用可穿戴设备（例如智能环和手表）来监测孤独感早期生理指标，从而检查预测孤独感的可行性。此外，智能手机还用于捕获孤独的初始行为迹象。为此，我们采用了个性化的机器学习技术，利用了一个综合数据集，其中包括包括对大学生监测的研究期间获得的生理和行为信息。通过开发个性化模型，我们提前七天在预测孤独感中获得了明显的准确性0.82，F-1得分为0.82。此外，Shapley值的应用促进了模型的解释性。这项研究提供的大量数据，再加上所采用的预测方法，具有增加干预措施的潜力，并促进了在风险上的人群中孤独的早期鉴定。

### PHemoNet: A Multimodal Network for Physiological Signals 
[[arxiv](https://arxiv.org/abs/2410.00010)] [[cool](https://papers.cool/arxiv/2410.00010)] [[pdf](https://arxiv.org/pdf/2410.00010)]
> **Authors**: Eleonora Lopez,Aurelio Uncini,Danilo Comminiello
> **First submission**: 2024-09-13
> **First announcement**: 2024-10-02
> **comment**: The paper has been accepted at RTSI 2024
- **标题**: Phemonet：生理信号的多模式网络
- **领域**: 信号处理,机器学习
- **摘要**: 情绪识别至关重要，包括医疗应用和脑部计算机界面（BCI）。情绪反应包括行为反应，例如语调和身体运动，以及生理信号的变化，例如脑电图（EEG）。后者是非自愿的，因此它们为识别情绪提供了可靠的输入，与个人可以自觉地控制的前者相比。这些信号揭示了真正的情绪状态而没有故意改变，从而提高了情绪识别模型的准确性。但是，尚未对生理信号的多模式深度学习方法进行显着研究。在本文中，我们介绍了Phemonet，这是一个完全超复杂的网络，用于生理信号的多模式情绪识别。详细说明，该体系结构包括特定于模态的编码器和融合模块。编码器和融合模块均通过参数化的超复杂乘数（PHM）在超复杂域中定义，这些乘数（PHM）可以捕获每种模态不同维度和模态本身之间的不同维度之间的潜在关系。该方法在使用脑电图（EEG）和外围生理信号对MAHNOB-HCI数据集上的MAHNOB-HCI数据集上的最新模型优于当前的最新模型。这项工作的代码可在https://github.com/ispamm/mhyeeg上找到。

### Authentication by Location Tracking in Underwater Acoustic Networks 
[[arxiv](https://arxiv.org/abs/2410.03511)] [[cool](https://papers.cool/arxiv/2410.03511)] [[pdf](https://arxiv.org/pdf/2410.03511)]
> **Authors**: Gianmaria Ventura,Francesco Ardizzon,Stefano Tomasin
> **First submission**: 2024-10-04
> **First announcement**: 2024-10-07
> **comment**: Article submitted to IEEE Transaction on Wireless Communications
- **标题**: 按位置跟踪在水下声学网络中进行身份验证
- **领域**: 信号处理,机器学习
- **摘要**: 水下声学网络（UWANS）中的物理层消息身份验证利用水下声通道（UWAC）的特征作为传输设备的指纹。但是，随着设备移动其UWAC的变化，身份验证机制必须跟踪此类变化。在本文中，我们提出了一种基于上下文的身份验证机制，分为两个步骤：首先，我们估算了水下设备的位置，然后根据先前估计的设备预测其未来位置。为了检查传输的真实性，我们比较了估计和预测位置。使用卷积神经网络作为输入估计的UWAC的样品协方差矩阵来估计该位置。该预测使用卡尔曼过滤器或复发性神经网络（RNN）。对预测位置和估计位置之间的平方误差进行身份验证检查。基于卡尔曼过滤器的解决方案优于在设备根据相关的高斯 - 马尔科夫移动性模型移动时构建的RNN，该模型会重现典型的水下运动。

### Active inference and deep generative modeling for cognitive ultrasound 
[[arxiv](https://arxiv.org/abs/2410.13310)] [[cool](https://papers.cool/arxiv/2410.13310)] [[pdf](https://arxiv.org/pdf/2410.13310)]
> **Authors**: Ruud JG van Sloun
> **First submission**: 2024-10-17
> **First announcement**: 2024-10-18
> **comment**: ef:R. J. Van Sloun, "Active inference and deep generative modeling for cognitive ultrasound," in IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control, 2024
- **标题**: 积极推断和认知超声的深层生成建模
- **领域**: 信号处理,人工智能,机器学习
- **摘要**: 超声波（美国）具有独特的潜力，可以为任何人提供医学成像。设备已变得非常可观且具有成本效益，类似于听诊器。然而，我们的图像质量和诊断功效仍然高度依赖于患者和患者。在难以形象的患者中，图像质量通常不足以用于可靠的诊断。在本文中，我们提出，我们的成像系统可以作为寻求信息的代理人进行重新铸造，以与他们的解剖环境进行相互互动。这样的试剂自主可调整其传输序列的序列，以完全个性化成像并积极地最大化信息获得地位。为此，我们将证明，美国系统执行的脉搏回声实验的顺序可以解释为一种感知循环：动作是数据采集，具有声波的组织探测组织，并在检测阵列中记录反射，感知是对诊断的解剖学和功能状态的推理，包括诊断量，包括诊断量。然后，我们为系统设备了一种机制，可以积极降低不确定性并最大程度地提高一系列实验的诊断价值，并使用贝叶斯推断联合处理动作和感知，并在给定环境的生成模型和动作条件脉冲 - 回声观察中。由于生成模型的表示能力既决定了推断的解剖状态的质量，又决定了推断的未来成像动作序列的有效性，因此我们将极大地利用目前正在破坏许多领域和整个社会的深层生成建模的巨大进步。最后，我们基于跟踪解剖学信念状态的深层生成模型，展示了一些认知，闭环的美国系统的示例，这些示例是执行主动梁变形和自适应扫描线的选择。

### Spatio-Temporal 3D Point Clouds from WiFi-CSI Data via Transformer Networks 
[[arxiv](https://arxiv.org/abs/2410.16303)] [[cool](https://papers.cool/arxiv/2410.16303)] [[pdf](https://arxiv.org/pdf/2410.16303)]
> **Authors**: Tuomas Määttä,Sasan Sharifipour,Miguel Bordallo López,Constantino Álvarez Casado
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-22
> **comment**: 7 pages, 5 figures, 1 table
- **标题**: WiFi-CSI数据通过变压器网络的时空3D点云
- **领域**: 信号处理,机器学习
- **摘要**: 联合通信和传感（JC \＆S）正在成为5G和6G网络中的关键组成部分，从而使环境变化的动态适应能力，并增强上下文意识以进行优化的通信。通过利用实时环境数据，JC \＆S可以改善资源分配，降低潜伏期并提高功率效率，同时支持模拟和预测建模。这使其成为反应性系统和数字双胞胎的关键技术。这些系统可以实时响应环境事件，从而在智能城市，医疗保健和行业5.0等领域提供变革潜力，在这种行业中，自适应和多模式互动对于增强实时决策至关重要。在这项工作中，我们提出了一个基于变压器的体系结构，该体系结构处理时间通道状态信息（CSI）数据，特别是振幅和相位，以生成室内环境的3D点云。该模型利用多头注意力来捕获CSI数据中复杂的时空关系，并且适合不同的CSI配置。我们使用两种不同的协议在室内环境中捕获人类的存在，评估MM-FI数据集上的体系结构。该系统具有准确的3D重建的强大潜力，并有效地区分了近距离和遥远的对象，从而在未来的无线网络中推进了JC \＆S应用程序的空间传感。

### rECGnition_v1.0: Arrhythmia detection using cardiologist-inspired multi-modal architecture incorporating demographic attributes in ECG 
[[arxiv](https://arxiv.org/abs/2410.18985)] [[cool](https://papers.cool/arxiv/2410.18985)] [[pdf](https://arxiv.org/pdf/2410.18985)]
> **Authors**: Shreya Srivastava,Durgesh Kumar,Jatin Bedi,Sandeep Seth,Deepak Sharma
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: recgention_v1.0：使用心脏病专家启发的多模式结构的心律失常检测，将人口统计学属性纳入ECG
- **领域**: 信号处理,人工智能,机器学习
- **摘要**: 由于患者特征，ECG的大量可变性妨碍了临床实践中自动分析算法的采用。迄今为止，迄今为止，尚未开发过的心电图注释者考虑到多模式结构中患者的特征。我们采用XGBoost模型来分析UCI心律失常数据集，将患者特征与ECG形态变化联系起来。该模型使用具有87.75％置信度的判别性心电图特征准确地分类的患者性别。我们提出了一种用于心电图分析和心律不齐分类的新型多模式方法，可以有助于与患者特异性疾病相关的ECG的变异性。这种深度学习算法，名为Recgention_V1.0（强大的ECG异常检测版本1），融合了形态与患者特征相比，以创建一个歧视性特征图，以了解两种模态之间的内部相关性。考虑到患者的人口统计，已经引入了基于挤压和激发的患者特征编码网络（Sepcenet）。受过训练的模型通过在MITDB中达到十个心律失常类别分类的总体F1分数的总体F1分数优于各种现有算法，并且在LBBB，RBBB，RBBB，过早的心脏相互作用节拍，Artrial Forge Beat beat和pat ace fece beat beat和beat beat bbbb中达到〜0.99的完美预测得分。随后，使用转移学习对INCARTDB，EDB和不同类别的MITDB类别组进行了验证。通用性测试分别为INCARTDB，EDB，MITDB AAMI和MITDB正常分类和异常分类提供了0.980、0.946、0.977和0.980的F1得分。因此，拟议的RECGENTION_V1.0算法对正在检查的患者及其心电图的患者有了更高和全面的了解，为其在诊所中的部署铺平了道路。

### Multi-modal Data based Semi-Supervised Learning for Vehicle Positioning 
[[arxiv](https://arxiv.org/abs/2410.20680)] [[cool](https://papers.cool/arxiv/2410.20680)] [[pdf](https://arxiv.org/pdf/2410.20680)]
> **Authors**: Ouwen Huan,Yang Yang,Tao Luo,Mingzhe Chen
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 用于车辆定位的多模式数据的半监督学习
- **领域**: 信号处理,机器学习
- **摘要**: 在本文中，设计了基于多模式数据的半监督学习（SSL）框架，该框架设计了通道状态信息（CSI）数据和RGB图像进行车辆定位。特别是，考虑了由基站（BS）确定车辆位置的室外定位系统。配备多个摄像机的BS可以收集大量未标记的CSI数据和少数标记的车辆CSI数据，以及摄像头拍摄的图像。尽管收集的图像包含车辆的部分信息（即车辆方位角），但未标记的CSI数据及其方位角之间的关系以及BS与图像捕获的车辆之间的距离都是未知的。因此，这些图像不能直接用作未标记的CSI数据的标签来训练定位模型。为了利用未标记的CSI数据和图像，提出了一个由预处理阶段和下游训练阶段组成的SSL框架。在预处理阶段，从图像获得的方位角被认为是未标记的CSI数据的标签，以预识定位模型。在下游训练阶段，一个小型标记的数据集，其中将准确的车辆位置视为标签用于重新训练模型。仿真结果表明，与未鉴定模型的基线相比，所提出的方法可以将定位误差最多减少30％。

### A Multi-Modal Non-Invasive Deep Learning Framework for Progressive Prediction of Seizures 
[[arxiv](https://arxiv.org/abs/2410.20066)] [[cool](https://papers.cool/arxiv/2410.20066)] [[pdf](https://arxiv.org/pdf/2410.20066)]
> **Authors**: Ali Saeizadeh,Douglas Schonholtz,Joseph S. Neimat,Pedram Johari,Tommaso Melodia
> **First submission**: 2024-10-26
> **First announcement**: 2024-10-28
> **comment**: 4 pages, 5 figures, Proceedings of the IEEE 20th International Conference on Body Sensor Networks (BSN), October 2024
- **标题**: 一个多模式的非侵入性深度学习框架，用于进行癫痫发作的进行性预测
- **领域**: 信号处理,人工智能
- **摘要**: 本文介绍了一个创新的框架，该框架是为了通过基于非侵入性多模式传感器网络的深度学习（DL）方法的利用而设计的，该框架旨在进行渐进式（时代的颗粒状）预测癫痫发作。癫痫病是一种使人衰弱的神经系统疾病，在全球范围内影响约6500万人，尽管有药理干预措施，但面临耐药性癫痫的比例很大。为了应对这一挑战，我们倡导预测系统，这些系统为处于危险中的个人提供及时警报，从而使他们能够采取预防措施。我们的框架采用高级DL技术，并使用来自非侵入性脑电图（EEG）和心电图（ECG）传感器网络的个性化数据，从而提高了预测准确性。这些算法是针对边缘设备上实时处理的优化，减轻了隐私问题，并最大程度地减少了基于云解决方案固有的数据传输开销，最终保留了电池能量。此外，我们的系统可以预测癫痫发作的倒计时时间（在发病前一个小时内长达15分钟），为预防措施提供了关键的提前时间。我们的多模式模型在29名患者中平均达到95％的敏感性，98％的特异性和97％的精度。

### Multi-Modal Transformer and Reinforcement Learning-based Beam Management 
[[arxiv](https://arxiv.org/abs/2410.19859)] [[cool](https://papers.cool/arxiv/2410.19859)] [[pdf](https://arxiv.org/pdf/2410.19859)]
> **Authors**: Mohammad Ghassemi,Han Zhang,Ali Afana,Akram Bin Sediq,Melike Erol-Kantarci
> **First submission**: 2024-10-22
> **First announcement**: 2024-10-28
> **comment**: 5 pages, 5 figures, IEEE Networking Letters
- **标题**: 基于多模式变压器和强化学习的梁管理
- **领域**: 信号处理,人工智能
- **摘要**: 光束管理是提高信号强度并减少无线通信系统干扰的重要技术。最近，人们对使用各种感测模式进行光束管理一直引起了人们的兴趣。但是，有效地处理多模式数据并提取有用的信息仍然是一个巨大的挑战。另一方面，最近新兴的多模式变压器（MMT）是一种有前途的技术，可以通过捕获长期依赖性来处理多模式数据。尽管MMT在处理多模式数据并提供稳健的光束管理方面非常有效，但整合增强学习（RL）进一步增强了其在动态环境中的适应性。在这项工作中，我们通过将MMT与RL结合用于动态光束指数预测来提出一种两步梁的管理方法。在第一步中，我们将可用的光束指数分为几个组，并利用MMT来处理各种数据模式以预测最佳光束群。在第二步中，我们在每个组中采用RL进行快速束决策，以最大化吞吐量。我们提出的框架在6G数据集上进行了测试。在这种测试方案中，与仅基于MMT的方法和仅基于RL的方法相比，它可以达到更高的光束预测精度和系统吞吐量。

### Multi-modal Image and Radio Frequency Fusion for Optimizing Vehicle Positioning 
[[arxiv](https://arxiv.org/abs/2410.19788)] [[cool](https://papers.cool/arxiv/2410.19788)] [[pdf](https://arxiv.org/pdf/2410.19788)]
> **Authors**: Ouwen Huan,Tao Luo,Mingzhe Chen
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-28
> **comment**: No comments
- **标题**: 用于优化车辆定位的多模式图像和射频融合
- **领域**: 信号处理,计算机视觉和模式识别,机器学习
- **摘要**: 在本文中，设计了一个多模式车辆定位框架，该框架将设计具有通道状态信息（CSI）和图像的车辆。特别是，我们考虑了每辆车只能与一个BS通信的室外场景，因此，它可以将其估计的CSI上传到仅关联的BS。每个BS都配备了一组摄像机，因此它可以收集少数标记的CSI，大量未标记的CSI以及相机拍摄的图像。为了利用从图像获得的未标记的CSI数据和位置标签，我们设计了基于元学习的硬期望最大化（EM）算法。具体而言，由于我们不知道图像中未标记的CSI与多个车辆位置之间的相应关系，因此我们将训练目标的计算作为最小匹配问题。为了减少由未标记的CSI和从图像获得的车辆位置之间的不正确匹配引起的标签噪声的影响，我们在未标记的数据集中引入了加权损失函数，并研究了使用元学习算法来计算加权损失的使用。随后，根据未标记的CSI样本的加权损耗函数及其从图像获得的匹配位置标签进行更新。模拟结果表明，与不使用图像的基线相比，该方法可以将定位误差降低高达61％，而仅使用CSI指纹进行车辆定位。

### SALINA: Towards Sustainable Live Sonar Analytics in Wild Ecosystems 
[[arxiv](https://arxiv.org/abs/2410.19742)] [[cool](https://papers.cool/arxiv/2410.19742)] [[pdf](https://arxiv.org/pdf/2410.19742)]
> **Authors**: Chi Xu,Rongsheng Qian,Hao Fang,Xiaoqiang Ma,William I. Atlas,Jiangchuan Liu,Mark A. Spoljaric
> **First submission**: 2024-10-09
> **First announcement**: 2024-10-28
> **comment**: 14 pages, accepted by ACM SenSys 2024
- **标题**: 萨利纳：野生生态系统中的可持续现场声纳分析
- **领域**: 信号处理,人工智能,分布式、并行和集群计算
- **摘要**: 声纳雷达使用声波反射捕获水下对象和结构的视觉表示，这对于野生生态系统中的探索，映射和连续监视至关重要。声纳数据的实时分析对于时间敏感的应用至关重要，包括需要快速决策的环境异常检测和季节性渔业管理。但是，缺乏相关的数据集和预培训的DNN模型，再加上野生环境中的资源限制，阻碍了实时声纳分析的有效部署和连续运行。我们提出了萨利纳（Salina），这是一种可持续的现场声纳分析系统，旨在应对这些挑战。 Salina可以通过空间和时间适应来实时处理声学声纳数据，并通过强大的能量管理模块具有节能操作。萨利纳（Salina）在加拿大不列颠哥伦比亚省的两条内陆河流部署了六个月，提供了24/7的24/7水下监测，支持渔业管理和野生动植物恢复工作。通过广泛的现实测试，萨利纳（Salina）的平均精度提高了9.5％，跟踪指标提高了10.1％。能源管理模块成功处理了极端天气，防止停电并降低应急成本。这些结果为野外声学数据系统的长期部署提供了宝贵的见解。

## 地球物理学(physics.geo-ph:Geophysics)

该领域共有 1 篇论文

### An uncertainty-aware Digital Shadow for underground multimodal CO2 storage monitoring 
[[arxiv](https://arxiv.org/abs/2410.01218)] [[cool](https://papers.cool/arxiv/2410.01218)] [[pdf](https://arxiv.org/pdf/2410.01218)]
> **Authors**: Abhinav Prakash Gahlot,Rafael Orozco,Ziyi Yin,Felix J. Herrmann
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: 地下多模式CO2存储监控的不确定性感知的数字阴影
- **领域**: 地球物理学,人工智能,机器学习,计算物理
- **摘要**: 可以说，地质碳储存GC是唯一可用的净值二氧化碳排放技术，同时有望地下复杂性和储层特性的异质性需要一种系统的方法来量化不确定性，在优化生产和减轻存储风险时，包括限制和符合型号的机器型和一致性的倾向于倾向于倾向于的存储的存储风险，朝着倾向于设计和一致性的是，倾向于倾向于设计和符合型号的一步。在精心设计的现实数值模拟上引入和验证了基于数据仿真的框架，因为我们的实施是基于贝叶斯的推理，但尚未支持控制和决策，我们将我们的方法置于不确定性的数字阴影中，以表征与多模式的YERPERIDES THESEMER SHADIQUESIQUES SADITE YEXIQUES COMPIQUES COMPIQUES CORMIQUES COMPIQUES COMPIQUES COMPESIQUES COMENGER COMPERES的后分布，为GCS问题建立概率基准并吸收多模式数据，这些数据受到巨大自由度非线性多物理学的非线性多物理学的挑战，并且在评估流体流动和地震模拟以启用SBI的动态系统的SBI对数字阴影进行了培训，以启用SBI的sbi及其在数字阴影中，他们在数字阴影上进行了训练，以启用SBI，并在数字阴影中培训了一定程度的训练。完成的系统状态是在这项计算研究中可推断出的系统状态，我们观察到，可以将缺乏有关渗透率领域的知识纳入数字阴影不确定性量化的知识，这是我们知识的不确定性识别，这是不确定性意识到的概念的第一个证明。

## 基因组学(q-bio.GN:Genomics)

该领域共有 1 篇论文

### BSM: Small but Powerful Biological Sequence Model for Genes and Proteins 
[[arxiv](https://arxiv.org/abs/2410.11499)] [[cool](https://papers.cool/arxiv/2410.11499)] [[pdf](https://arxiv.org/pdf/2410.11499)]
> **Authors**: Weixi Xiang,Xueting Han,Xiujuan Chai,Jing Bai
> **First submission**: 2024-10-15
> **First announcement**: 2024-10-16
> **comment**: No comments
- **标题**: BSM：针对基因和蛋白质的小但功能强大的生物学序列模型
- **领域**: 基因组学,人工智能,机器学习
- **摘要**: 建模生物学序列（例如DNA，RNA和蛋白质）对于理解基因调节和蛋白质合成等复杂过程至关重要。但是，大多数当前模型要么关注单一类型，要么分别处理多种类型的数据，从而限制了它们捕获跨模式关系的能力。我们建议，通过学习这些方式之间的关系，该模型可以增强其对每种类型的理解。为了解决这个问题，我们介绍了BSM，这是一种小型但功能强大的混合模式生物学序列基础模型，对三种类型的数据进行了训练：RefSeq，基因相关序列和网络中交错的生物学序列。这些数据集分别捕获了遗传流，基因蛋白关系和各种生物学数据的自然共发生。通过对混合模式数据进行培训，BSM显着提高了学习效率和跨模式表示，优于仅基于单峰数据训练的模型。 BSM只有1100亿个参数，可以达到与单模式和混合模式任务中更大的模型相当的性能，并且独特地展示了混合模式任务的中文学习能力，这在现有模型中不存在。进一步缩放到27000万参数显示出更大的性能增长，突出了BSM作为多模式生物学序列建模的显着进步的潜力。

## 神经元和认知(q-bio.NC:Neurons and Cognition)

该领域共有 3 篇论文

### NECOMIMI: Neural-Cognitive Multimodal EEG-informed Image Generation with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2410.00712)] [[cool](https://papers.cool/arxiv/2410.00712)] [[pdf](https://arxiv.org/pdf/2410.00712)]
> **Authors**: Chi-Sheng Chen
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: No comments
- **标题**: Necomimi：具有扩散模型的神经认知多模式EEG信息形成的图像生成
- **领域**: 神经元和认知,机器学习
- **摘要**: Necomimi（具有扩散模型的神经认知多模式EEG信息生成）引入了一种新型框架，用于使用高级扩散模型直接从EEG信号生成图像。与以前仅通过对比度学习重点关注脑电图分类的作品不同，Necomimi将此任务扩展到图像生成。拟议的神经脑电图编码器展示了多个零射击分类任务的最先进（SOTA）性能，包括2速，4路和200速度，并在我们新提出的基于类别的评估表（CAT）中取得了最高的结果，该评分表（CAT）得分评估了基于上语上概念的EEG生成图像的质量。这项工作的一个关键发现是，该模型倾向于生成抽象或广义图像，例如景观，而不是特定的对象，突出了将噪声和低分辨率EEG数据转化为详细的视觉输出的固有挑战。此外，我们将CAT分数介绍为针对脑电图评估的新指标，并在Thingseeg数据集上建立基准。这项研究强调了脑电图生成的潜力，同时揭示了与视觉表现弥合神经活动的复杂性和挑战。

### The age of spiritual machines: Language quietus induces synthetic altered states of consciousness in artificial intelligence 
[[arxiv](https://arxiv.org/abs/2410.00257)] [[cool](https://papers.cool/arxiv/2410.00257)] [[pdf](https://arxiv.org/pdf/2410.00257)]
> **Authors**: Jeremy I Skipper,Joanna Kuc,Greg Cooper,Christopher Timmermann
> **First submission**: 2024-09-30
> **First announcement**: 2024-10-02
> **comment**: 8 Figures
- **标题**: 精神机器的时代：语言Quietus引起人工智能中意识状态的合成状态
- **领域**: 神经元和认知,人工智能,计算语言学
- **摘要**: 语言与意识有何关系？语言功能以对感知体验进行分类（例如，将互感状态标记为“快乐”）和更高级别的结构（例如，使用“ I”来表示叙事自我）。迷幻的使用和冥想可能被描述为改变的状态，这些状态会损害或故意改变语言分类的能力。例如，迷幻的现象学通常以“海洋无限”或“统一”和“自我溶解”为特征，这可能是对一个不受根深蒂固的语言类别负担的系统所期望的。如果语言分解在产生这种改变的行为中起作用，那么当注意力从语言转移时，多模式人工智能可能与这些现象学描述更加一致。我们通过比较操纵夹子和Flava模型中注意力重量后的语义嵌入空间与在操纵前的嵌入空间中的空间来比较该假设。与随机文本和包括焦虑在内的各种改变的状态相比，模型与不明智，无自我，精神和统一状态以及最少的现象经历更加一致，并且对语言和视力的关注下降。对语言的关注降低与内部，尤其是跨语义类别的不同语言模式和模糊的嵌入有关（例如，“长颈鹿”更像是“香蕉”）。这些结果为语言分类在改变意识状态的现象学中的作用提供了支持，就像那些经历了高剂量的迷幻药或集中冥想的人一样，通常会导致改善心理健康和福祉。

### Diagnosis and Pathogenic Analysis of Autism Spectrum Disorder Using Fused Brain Connection Graph 
[[arxiv](https://arxiv.org/abs/2410.07138)] [[cool](https://papers.cool/arxiv/2410.07138)] [[pdf](https://arxiv.org/pdf/2410.07138)]
> **Authors**: Lu Wei,Yi Huang,Guosheng Yin,Fode Zhang,Manxue Zhang,Bin Liu
> **First submission**: 2024-09-21
> **First announcement**: 2024-10-10
> **comment**: No comments
- **标题**: 使用融合脑连接图对自闭症谱系障碍的诊断和致病分析
- **领域**: 神经元和认知,机器学习,应用领域
- **摘要**: 我们提出了一个使用多模式磁共振成像（MRI）数据来诊断自闭症谱系障碍（ASD）的模型。我们的方法从扩散张量成像（DTI）和功能性MRI（fMRI）中整合了大脑连接性数据，该数据采用图形神经网络（GNN）进行融合图分类。为了提高诊断准确性，我们引入了一种损失功能，该功能可最大程度地提高课堂间的阶层并最大程度地减少课内边缘。我们还分析了双峰融合脑图上的网络节点中心，计算程度，子图和特征向量中心，以识别与ASD相关的病理区域。两项非参数测试评估了ASD患者和健康对照组之间这些中心的统计意义。我们的结果揭示了测试之间的一致性，但是所识别的区域在各个中心都有很大差异，这表明了不同的生理解释。这些发现增强了我们对ASD神经生物学基础的理解，并为临床诊断提供了新的方向。

## 种群与进化(q-bio.PE:Populations and Evolution)

该领域共有 1 篇论文

### A Review of BioTree Construction in the Context of Information Fusion: Priors, Methods, Applications and Trends 
[[arxiv](https://arxiv.org/abs/2410.04815)] [[cool](https://papers.cool/arxiv/2410.04815)] [[pdf](https://arxiv.org/pdf/2410.04815)]
> **Authors**: Zelin Zang,Yongjie Xu,Chenrui Duan,Yue Yuan,Jinlin Wu,Zhen Lei,Stan Z. Li
> **First submission**: 2024-10-07
> **First announcement**: 2024-10-08
> **comment**: 115 pages, 15 figures
- **标题**: 在信息融合的背景下对生物构建的综述：先验，方法，应用和趋势
- **领域**: 种群与进化,人工智能
- **摘要**: 生物树（Biotree）分析是生物学的基础工具，可以探索生物，基因和细胞之间的进化和分化关系。传统的树木构建方法虽然在早期研究中发挥了作用，但在处理现代生物学数据的日益增长的复杂性和规模方面面临重大挑战，尤其是在整合多模式数据集时。深度学习的进步（DL）通过使生物学先验知识与数据驱动模型的融合来提供变革的机会。这些方法解决了传统方法的关键局限性，从而促进了更准确，更容易解释的生物群体的构建。这篇评论重点介绍了对系统发育和分化树分析至关重要的关键生物学先验，并探讨了将这些先验整合到DL模型中以提高准确性和可解释性的策略。此外，综述系统地研究了常用的数据模式和数据库，为开发和评估多模式融合模型提供了宝贵的资源。对传统的树木建设方法进行了严格的评估，重点是其生物学假设，技术限制和可伸缩性问题。回顾了基于DL的树生成方法的最新进展，强调了它们的多模式整合和先验知识融合的创新方法。最后，该综述讨论了生物群在从系统发育到发育生物学的各种生物学学科中的各种应用，并概述了利用DL的未来趋势来推进生物培养研究。通过解决数据复杂性和先验知识集成的挑战，本综述旨在激发生物学与DL的交集的跨学科创新。

## 定量方法(q-bio.QM:Quantitative Methods)

该领域共有 3 篇论文

### GAMMA-PD: Graph-based Analysis of Multi-Modal Motor Impairment Assessments in Parkinson's Disease 
[[arxiv](https://arxiv.org/abs/2410.00944)] [[cool](https://papers.cool/arxiv/2410.00944)] [[pdf](https://arxiv.org/pdf/2410.00944)]
> **Authors**: Favour Nerrise,Alice Louise Heiman,Ehsan Adeli
> **First submission**: 2024-10-01
> **First announcement**: 2024-10-02
> **comment**: Accepted by the 6th Workshop on GRaphs in biomedicAl Image anaLysis (GRAIL) at the 27th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2024). 12 pages, 3 figures, 2 tables, Source Code: https://github.com/favour-nerrise/GAMMA-PD
- **标题**: Gamma-PD：基于图的多模式运动障碍评估帕金森氏病的分析
- **领域**: 定量方法,人工智能,机器学习,图像和视频处理,神经元和认知
- **摘要**: 医疗技术的快速发展导致多模式医学数据的指数增加，包括成像，基因组学和电子健康记录（EHRS）。图形神经网络（GNN）由于在捕获成对关系方面的出色性能而被广泛用来表示该数据。但是，多模式医学数据的异质性和复杂性仍然对标准GNN构成了重大挑战，这些挑战在学习高阶，非双向关系方面遇到了困难。本文提出了伽玛 -  PD（基于图形的多模式运动障碍评估帕金森氏病的分析），这是一种用于多模式临床数据分析的新型异质超晶融合框架。 γ-PD通过保留患者概况和症状亚型之间的高阶信息和相似性，将成像和非成像数据整合到“超网络”（患者人群图）中。我们还设计了一种基于功能的注意力加权机制，以对下游决策任务解释功能级别的贡献。我们通过帕金森氏症进步标记计划（PPMI）和私人数据集的临床数据评估我们的方法。我们证明了预测帕金森氏病的运动障碍症状的收益。我们的端到端框架还学习了患者特征子集之间的关联，以产生有关疾病和症状特征的临床相关解释。源代码可在https://github.com/favour-nerrise/gamma-pd上找到。

### Uncovering the Genetic Basis of Glioblastoma Heterogeneity through Multimodal Analysis of Whole Slide Images and RNA Sequencing Data 
[[arxiv](https://arxiv.org/abs/2410.18710)] [[cool](https://papers.cool/arxiv/2410.18710)] [[pdf](https://arxiv.org/pdf/2410.18710)]
> **Authors**: Ahmad Berjaoui,Louis Roussel,Eduardo Hugo Sanchez,Elizabeth Cohen-Jonathan Moyal
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-25
> **comment**: No comments
- **标题**: 通过整个幻灯片图像和RNA测序数据的多模式分析，揭示胶质母细胞瘤异质性的遗传基础
- **领域**: 定量方法,人工智能
- **摘要**: 胶质母细胞瘤是一种高度侵略性的脑癌形式，其特征是快速进展和预后不良。尽管治疗进展，但推动这种侵略性的基本遗传机制仍然对此知之甚少。在这项研究中，我们采用了多模式深度学习方法来研究使用联合图像/RNA-Seq分析研究胶质母细胞瘤的异质性。我们的结果揭示了与胶质母细胞瘤相关的新基因。通过利用全扫描图像和RNA-seq的组合以及引入编码RNA-Seq数据的新方法，我们确定了可能解释胶质母细胞瘤进展模式的不同遗传谱。这些发现为胶质母细胞瘤异质性的基因机制提供了新的见解，并突出了治疗干预的潜在靶标。

### MAMMAL -- Molecular Aligned Multi-Modal Architecture and Language 
[[arxiv](https://arxiv.org/abs/2410.22367)] [[cool](https://papers.cool/arxiv/2410.22367)] [[pdf](https://arxiv.org/pdf/2410.22367)]
> **Authors**: Yoel Shoshan,Moshiko Raboh,Michal Ozery-Flato,Vadim Ratner,Alex Golts,Jeffrey K. Weber,Ella Barkan,Simona Rabinovici-Cohen,Sagi Polaczek,Ido Amos,Ben Shapira,Liam Hazan,Matan Ninio,Sivan Ravid,Michael M. Danziger,Joseph A. Morrone,Parthasarathy Suryanarayanan,Michal Rosen-Zvi,Efrat Hexter
> **First submission**: 2024-10-28
> **First announcement**: 2024-10-30
> **comment**: No comments
- **标题**: 哺乳动物 - 分子对齐的多模式结构和语言
- **领域**: 定量方法,人工智能,机器学习
- **摘要**: 药物发现通常由多个步骤组成，包括确定疾病病因的靶蛋白键，证明与该靶标相互作用可以防止症状或治愈该疾病，发现小分子或生物学疗法与之相互作用，并通过复杂的所需特性的复杂景观来优化候选分子。与药物发现相关的任务通常涉及预测和产生，同时考虑了潜在相互作用的多个实体，这对典型的AI模型构成了挑战。为此，我们介绍了哺乳动物 - 分子对齐的多模式结构和语言 - 我们应用的一种方法来创建一种多种多样的多任务多任务多任务多任务基础模型，该模型从大规模的生物数据集（20亿个样本）中学习，包括各种方式，包括蛋白质，小分子和基因。我们介绍了一个及时的语法，该语法支持广泛的分类，回归和生成任务。它允许将不同的方式和实体类型组合为输入和/或输出。我们的模型处理令牌和标量的组合，并使小分子和蛋白质，性质预测以及转录组实验室测试预测产生。我们在典型的药物发现管道内的不同步骤中评估了11个不同的下游任务，该模型在9个任务中达到了新的SOTA，并且在2个任务中与SOTA相当。与使用量身定制的体系结构实现的原始SOTA性能相比，使用统一的体系结构时实现了这种性能。型号代码和预估计的权重可在https://github.com/biomedsciai/biomed-multi-alignment和https://huggingface.co/ibm/biomed.omics.bl.sm.sm.sm.m.m.ma-ted-458m中公开获得。

## 应用领域(stat.AP:Applications)

该领域共有 1 篇论文

### Analysis and Optimization of Seismic Monitoring Networks with Bayesian Optimal Experiment Design 
[[arxiv](https://arxiv.org/abs/2410.07215)] [[cool](https://papers.cool/arxiv/2410.07215)] [[pdf](https://arxiv.org/pdf/2410.07215)]
> **Authors**: Jake Callahan,Kevin Monogue,Ruben Villarreal,Tommie Catanach
> **First submission**: 2024-09-27
> **First announcement**: 2024-10-10
> **comment**: 38 pages, 19 figures. Submitted to Geophysical Journal International
- **标题**: None
- **领域**: 应用领域,机器学习,地球物理学,机器学习
- **摘要**: 监视网络越来越多地旨在吸收来自涵盖许多传感方式的大量不同传感器的数据。贝叶斯最佳实验设计（OED）旨在识别可以最佳降低不确定性并因此增加监视网络的性能的数据，传感器配置或实验。信息理论通过将实验或传感器放置的选择作为优化问题来指导，从而最大程度地提高了预期信息增益（EIG），但对于给定的先验知识和预期观察数据的模型，信息量最大化。因此，在地震声监测的背景下，我们可以通过选择传感器位置，类型和忠诚度来使用贝叶斯OED来配置传感器网络，以提高我们识别和定位地震源的能力。在这项工作中，我们开发了使用贝叶斯OED来优化传感器网络从检测到的地震阶段的到达时间数据中找到地震事件的能力所需的框架。 Bayesian OED requires four elements: 1) A likelihood function that describes the distribution of detection and travel time data from the sensor network, 2) A Bayesian solver that uses a prior and likelihood to identify the posterior distribution of seismic events given the data, 3) An algorithm to compute EIG about seismic events over a dataset of hypothetical prior events, 4) An optimizer that finds a sensor network which maximizes eig。一旦我们开发了此框架，我们就会探索许多相关的问题，例如：如何交易传感器保真度和地球模型不确定性；传感器类型，数量和位置如何影响不确定性；以及先前的模型和约束如何影响传感器放置。

## 计算(stat.CO:Computation)

该领域共有 1 篇论文

### metasnf: Meta Clustering with Similarity Network Fusion in R 
[[arxiv](https://arxiv.org/abs/2410.17976)] [[cool](https://papers.cool/arxiv/2410.17976)] [[pdf](https://arxiv.org/pdf/2410.17976)]
> **Authors**: Prashanth S Velayudhan,Xiaoqiao Xu,Prajkta Kallurkar,Ana Patricia Balbon,Maria T Secara,Adam Taback,Denise Sabac,Nicholas Chan,Shihao Ma,Bo Wang,Daniel Felsky,Stephanie H Ameis,Brian Cox,Colin Hawco,Lauren Erdman,Anne L Wheeler
> **First submission**: 2024-10-23
> **First announcement**: 2024-10-24
> **comment**: 72 pages, 22 figures, submitted to Journal of Statistical Software
- **标题**: METASNF：在R中具有相似性网络融合的元聚类
- **领域**: 计算,机器学习
- **摘要**: METASNF是一个R软件包，它使用户能够应用元聚类，这是一种通过群集本身来有效地搜索群集解决方案的方法，以基于相似性网络融合（SNF）的聚类工作流程。 SNF是一种通常用于生物医学亚型发现的多模式数据集成算法。该软件包还包含函数，以协助聚类可视化，表征和验证。该软件包可以帮助研究人员识别SNF衍生的集群解决方案，这些解决方案受上下文特定效用而指导的，而不是上下文不合时宜的质量测量。

## 机器学习(stat.ML:Machine Learning)

该领域共有 5 篇论文

### On Expert Estimation in Hierarchical Mixture of Experts: Beyond Softmax Gating Functions 
[[arxiv](https://arxiv.org/abs/2410.02935)] [[cool](https://papers.cool/arxiv/2410.02935)] [[pdf](https://arxiv.org/pdf/2410.02935)]
> **Authors**: Huy Nguyen,Xing Han,Carl Harris,Suchi Saria,Nhat Ho
> **First submission**: 2024-10-03
> **First announcement**: 2024-10-04
> **comment**: Huy Nguyen and Xing Han contributed equally to this work
- **标题**: 关于专家的专家估计，专家的层次混合：超越软磁场功能
- **领域**: 机器学习,机器学习
- **摘要**: 随着专家（MOE）体系结构在开发大型基础模型中的混合物的越来越突出，我们研究了MOE的专业变体专家的层次混合物（HMOE），它在处理复杂输入并提高目标任务的性能方面表现出色。我们的分析强调了使用Laplace门控函数比HMOE框架内传统的软式门控的优点。从理论上讲，我们证明，在HMOE模型的两个级别上应用Laplace Gating函数有助于消除由SoftMax Gating引起的不良参数相互作用，因此可以加速专家的收敛性并增强专家专业化。跨不同场景的经验验证支持这些理论主张。这包括大规模的多模式任务，图像分类和潜在领域发现和预测任务，与传统的HMOE模型相比，我们修改的HMOE模型在其中显示出巨大的性能改进。

### Provable Convergence and Limitations of Geometric Tempering for Langevin Dynamics 
[[arxiv](https://arxiv.org/abs/2410.09697)] [[cool](https://papers.cool/arxiv/2410.09697)] [[pdf](https://arxiv.org/pdf/2410.09697)]
> **Authors**: Omar Chehab,Anna Korba,Austin Stromme,Adrien Vacher
> **First submission**: 2024-10-12
> **First announcement**: 2024-10-14
> **comment**: No comments
- **标题**: Langevin动力学的几何降温的可证明的收敛性和局限性
- **领域**: 机器学习,机器学习,计算
- **摘要**: 几何回火是一种流行的方法，可以通过从一系列分布序列中取样，这些分布序列使用几何平均值插图介于易于的建议分布和目标分布之间。在本文中，当采样算法是langevin动力学时，我们从理论上研究了这种方法的声音，证明了上限和下限。我们的上限是功能不平等下文献中的第一个分析。他们主张了在连续和离散时间内恢复弹性的兰格文的收敛，并且它们的最小化导致了一对提案和目标分布的封闭形式的最佳回火计划。我们的下限表明了一个简单的情况，几何回火需要指数时间，并且进一步表明几何降温也可能遭受功能不平等和缓慢收敛的障碍，即使目标分布有良好的条件。总体而言，我们的结果表明几何降温可能无济于事，甚至可能对收敛有害。

### A spectral method for multi-view subspace learning using the product of projections 
[[arxiv](https://arxiv.org/abs/2410.19125)] [[cool](https://papers.cool/arxiv/2410.19125)] [[pdf](https://arxiv.org/pdf/2410.19125)]
> **Authors**: Renat Sergazinov,Armeen Taeb,Irina Gaynanova
> **First submission**: 2024-10-24
> **First announcement**: 2024-10-25
> **comment**: 23 pages, 7 figures
- **标题**: 使用投影产物的多视图子空间学习的光谱方法
- **领域**: 机器学习,机器学习,统计理论,计算,方法论
- **摘要**: 多视图数据提供了有关相同观测值的互补信息，多摩管和多模式传感器数据是常见示例。分析此类数据通常需要区分共享（关节）和唯一（个人）信号子空间与嘈杂的高维测量值。尽管提出了许多建议的方法，但可靠地识别关节和各个子空间的条件仍不清楚。我们严格量化了这些条件，这些条件取决于信号等级与环境维度的比率，真实子空间之间的主角度和噪声水平。我们的方法表征了从每个视图的估计子空间得出的投影矩阵产物的频谱扰动，影响子空间分离。使用这些见解，我们提供了一种易于使用且可扩展的估计算法。特别是，我们采用旋转的自举和随机矩阵理论将观察到的光谱分为关节，个体和噪声子空间。诊断图可视化这种分区，为估计性能提供实用且可解释的见解。在模拟中，我们的方法比现有方法更准确地估计关节和单个子空间。从结直肠癌患者和小鼠的营养学研究中应用的多摩学数据表明，下游预测任务的表现提高了。

### Learned Reference-based Diffusion Sampling for multi-modal distributions 
[[arxiv](https://arxiv.org/abs/2410.19449)] [[cool](https://papers.cool/arxiv/2410.19449)] [[pdf](https://arxiv.org/pdf/2410.19449)]
> **Authors**: Maxence Noble,Louis Grenioux,Marylou Gabrié,Alain Oliviero Durmus
> **First submission**: 2024-10-25
> **First announcement**: 2024-10-28
> **comment**: Accepted at ICLR 2025
- **标题**: 学习多模式分布的基于参考的扩散采样
- **领域**: 机器学习,机器学习,计算
- **摘要**: 在过去的几年中，已经提出了几种利用基于得分扩散的方法来从概率分布中采样，即没有访问精确的样品，并且仅依赖于未归一化密度的评估。所得的采样器近似近似扩散过程的时间逆转，将目标分布桥接到易于样本的碱基分布。实际上，这些方法的性能在很大程度上取决于需要准确调整地面真实样本的关键超参数。我们的工作旨在突出和解决这个基本问题，尤其是多模式分布，这对现有采样方法构成了重大挑战。在现有方法的基础上，我们介绍了基于参考的扩散采样器（LRDS），该方法专门旨在利用目标模式位置的先验知识，以绕过超参数调谐的障碍。 LRDS通过（i）在高密度空间区域的样品上学习参考扩散模型并针对多模态定制，并使用此参考模型来促进基于扩散的采样器的训练。我们通过实验表明，LRDS最佳利用目标分布的先验知识与在各种具有挑战性的分布上进行竞争算法相比。

### Graph Integration for Diffusion-Based Manifold Alignment 
[[arxiv](https://arxiv.org/abs/2410.22978)] [[cool](https://papers.cool/arxiv/2410.22978)] [[pdf](https://arxiv.org/pdf/2410.22978)]
> **Authors**: Jake S. Rhodes,Adam G. Rustad
> **First submission**: 2024-10-30
> **First announcement**: 2024-10-31
> **comment**: 8 pages, 4 figures, Accepted at ICMLA 2024
- **标题**: 基于扩散的流形比对的图集成
- **领域**: 机器学习,机器学习
- **摘要**: 来自各个观察结果的数据可以源自各种来源或模式，但通常是内在链接的。与单源数据相比，多模式数据集成可以丰富信息内容。流动对准是一种数据集成的一种形式，它寻求多个数据源的共享基本的低维表示，强调同一实体的替代表示之间的相似性。半监督的歧管比对依赖于域之间的部分已知的对应关系，无论是通过共享特征还是通过其他已知关联。在本文中，我们介绍了两种半监督的歧管比对方法。第一种方法是域结合（SPUD）的最短路径，使用已知对应关系形成统一的图形结构来建立图形边缘。通过学习域间的大地距离，SPUD创建了全球多域结构。第二种方法是MASH（通过随机跳跃的流形对齐），使用对迭代的已知对应关系来学习每个域内的局部几何形状，并通过随机步行方法来学习新的域间对应关系。通过扩散过程，MASH形成一个将异质域与统一结构联系起来的耦合矩阵。我们将SPUD和MASH与现有的半监督歧管比对方法进行了比较，并表明它们在对齐真对象和跨域分类方面的表现优于竞争方法。此外，我们展示了如何将这些方法应用于域之间的标签信息。

## 其他论文

共有 74 篇其他论文

- [Signal Processing for Haptic Surface Modeling: a Review](https://arxiv.org/abs/2409.20142)
  - **标题**: 触觉表面建模的信号处理：审查
  - **Filtered Reason**: none of cs.MM in whitelist
- [M2P2: A Multi-Modal Passive Perception Dataset for Off-Road Mobility in Extreme Low-Light Conditions](https://arxiv.org/abs/2410.01105)
  - **标题**: M2P2：在极端弱光条件下，用于越野移动性的多模式被动感知数据集
  - **Filtered Reason**: none of cs.RO in whitelist
- [MERIT: Multimodal Wearable Vital Sign Waveform Monitoring](https://arxiv.org/abs/2410.00392)
  - **标题**: 优点：多模式可穿戴生命符号波形监视
  - **Filtered Reason**: none of eess.SY,cs.AR in whitelist
- [Bayesian Intention for Enhanced Human Robot Collaboration](https://arxiv.org/abs/2410.00302)
  - **标题**: 贝叶斯的意图增强人类机器人合作
  - **Filtered Reason**: none of cs.RO in whitelist
- [Contribution of soundscape appropriateness to soundscape quality assessment in space: a mediating variable affecting acoustic comfort](https://arxiv.org/abs/2410.00667)
  - **标题**: 音景适当性对空间中音景质量评估的贡献：一种影响声学舒适的变量
  - **Filtered Reason**: none of physics.class-ph,eess.AS,cs.SD in whitelist
- [Universal Logical Quantum Photonic Neural Network Processor via Cavity-Assisted Interactions](https://arxiv.org/abs/2410.02088)
  - **标题**: 通用逻辑量子光子神经网络处理器通过空腔辅助相互作用
  - **Filtered Reason**: none of cs.ET,physics.optics,quant-ph in whitelist
- [OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition](https://arxiv.org/abs/2410.01495)
  - **标题**: OV-MER：朝向开放式摄影多模式情绪识别
  - **Filtered Reason**: none of cs.HC in whitelist
- [High and Low Resolution Tradeoffs in Roadside Multimodal Sensing](https://arxiv.org/abs/2410.01250)
  - **标题**: 路边多模式感测的高分辨率和低分辨率的权衡
  - **Filtered Reason**: none of cs.RO in whitelist
- [CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and Risk Detection of Cancer Treatment-Induced Cardiotoxicity](https://arxiv.org/abs/2410.04592)
  - **标题**: CardioAI：一种基于AI的多模式系统，用于支持症状监测和风险检测癌症治疗引起的心脏毒性
  - **Filtered Reason**: none of cs.HC in whitelist
- [Generative Co-Learners: Enhancing Cognitive and Social Presence of Students in Asynchronous Learning with Generative AI](https://arxiv.org/abs/2410.04365)
  - **标题**: 生成的共同学习者：增强学生在异步学习中的认知和社会存在与生成AI
  - **Filtered Reason**: none of cs.HC in whitelist
- [The Visualization JUDGE : Can Multimodal Foundation Models Guide Visualization Design Through Visual Perception?](https://arxiv.org/abs/2410.04280)
  - **标题**: 可视化法官：多模式基础模型可以通过视觉感知指导可视化设计吗？
  - **Filtered Reason**: none of cs.HC in whitelist
- [A Framework for Reproducible Benchmarking and Performance Diagnosis of SLAM Systems](https://arxiv.org/abs/2410.04242)
  - **标题**: 大满贯系统的可再现基准测试和性能诊断的框架
  - **Filtered Reason**: none of cs.RO in whitelist
- [Enhancing the Travel Experience for People with Visual Impairments through Multimodal Interaction: NaviGPT, A Real-Time AI-Driven Mobile Navigation System](https://arxiv.org/abs/2410.04005)
  - **标题**: 通过多模式互动来增强视觉障碍的人的旅行体验：Navigpt，一种实时AI驱动的移动导航系统
  - **Filtered Reason**: none of cs.HC in whitelist
- [TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human Action Prediction](https://arxiv.org/abs/2410.03993)
  - **标题**: TR-LLM：基于场景感知LLM的人类行动预测的集成轨迹数据
  - **Filtered Reason**: none of cs.HC in whitelist
- [MRSO: Balancing Exploration and Exploitation through Modified Rat Swarm Optimization for Global Optimization](https://arxiv.org/abs/2410.03684)
  - **标题**: MRSO：通过改良的大鼠群优化进行全球优化的探索和剥削平衡
  - **Filtered Reason**: none of cs.NE in whitelist
- [STREAMS: An Assistive Multimodal AI Framework for Empowering Biosignal Based Robotic Controls](https://arxiv.org/abs/2410.03486)
  - **标题**: 流：辅助多模式AI框架，用于赋予基于生物信号的机器人控制
  - **Filtered Reason**: none of cs.RO in whitelist
- [Level set-based inverse homogenisation of three-dimensional piezoelectric materials](https://arxiv.org/abs/2410.03148)
  - **标题**: 三维压电材料的基于水平设置的逆均质化
  - **Filtered Reason**: none of cs.DC,cs.CE in whitelist
- [FGCL: Fine-grained Contrastive Learning For Mandarin Stuttering Event Detection](https://arxiv.org/abs/2410.05647)
  - **标题**: FGCL：普通话口吃事件检测的细粒对比度学习
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [A decade of DCASE: Achievements, practices, evaluations and future challenges](https://arxiv.org/abs/2410.04951)
  - **标题**: 十年的dcase：成就，实践，评估和未来挑战
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Evaluating the Impact of Warning Modalities and False Alarms in Pedestrian Crossing Alert System](https://arxiv.org/abs/2410.06388)
  - **标题**: 评估行人交叉警报系统中警告方式和虚假警报的影响
  - **Filtered Reason**: none of cs.HC in whitelist
- [The USTC-NERCSLIP Systems for the CHiME-8 MMCSG Challenge](https://arxiv.org/abs/2410.05986)
  - **标题**: Chime-8 MMCSG挑战的USTC-SNERCSLIP系统
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [vailá: Versatile Anarcho Integrated Liberation Ánalysis in Multimodal Toolbox](https://arxiv.org/abs/2410.07238)
  - **标题**: Vailá：多模式工具箱中的多功能无政府主义综合解放
  - **Filtered Reason**: none of cs.HC in whitelist
- [FlowBotHD: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation](https://arxiv.org/abs/2410.07078)
  - **标题**: flowbothd：历史吸引的扩散器处理铰接物体操纵中的歧义
  - **Filtered Reason**: none of cs.RO in whitelist
- [Gumbel Rao Monte Carlo based Bi-Modal Neural Architecture Search for Audio-Visual Deepfake Detection](https://arxiv.org/abs/2410.06543)
  - **标题**: Gumbel Rao Monte Carlo基于双模式的神经建筑搜索视听效果检测
  - **Filtered Reason**: none of cs.CR,eess.AS,cs.SD in whitelist
- [Control System Design and Experiments for Autonomous Underwater Helicopter Docking Procedure Based on Acoustic-inertial-optical Guidance](https://arxiv.org/abs/2410.06953)
  - **标题**: 基于声学惯性光学指导的自动水下直升机对接程序的控制系统设计和实验
  - **Filtered Reason**: none of cs.RO in whitelist
- [Cyber-physical and business perspectives using Federated Digital Twins in multinational and multimodal transportation systems](https://arxiv.org/abs/2410.08479)
  - **标题**: 在跨国和多模式运输系统中使用联合数字双胞胎的网络物理和商业观点
  - **Filtered Reason**: none of cs.CE in whitelist
- [Octopus Inspired Optimization Algorithm: Multi-Level Structures and Parallel Computing Strategies](https://arxiv.org/abs/2410.07968)
  - **标题**: 章鱼灵感的优化算法：多级结构和并行计算策略
  - **Filtered Reason**: none of cs.NE in whitelist
- [Sound Zone Control Robust To Sound Speed Change](https://arxiv.org/abs/2410.07978)
  - **标题**: 声音区控制强大的声音速度变化
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Circuits and Systems for Embodied AI: Exploring uJ Multi-Modal Perception for Nano-UAVs on the Kraken Shield](https://arxiv.org/abs/2410.09054)
  - **标题**: 体现AI的电路和系统：探索Kraken Shield上纳米uavs的UJ多模式感知
  - **Filtered Reason**: none of cs.AR in whitelist
- [Contrastive Knowledge Distillation for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2410.08692)
  - **标题**: 鲁棒多模式分析的对比知识蒸馏
  - **Filtered Reason**: none of cs.MM in whitelist
- [Dual-AEB: Synergizing Rule-Based and Multimodal Large Language Models for Effective Emergency Braking](https://arxiv.org/abs/2410.08616)
  - **标题**: Dual-Aeb：基于规则和多模式的大型语言模型，以进行有效紧急制动
  - **Filtered Reason**: none of cs.RO in whitelist
- [CoHRT: A Collaboration System for Human-Robot Teamwork](https://arxiv.org/abs/2410.08504)
  - **标题**: COHRT：人类机器人团队合作系统
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [LEAD Dataset: How Can Labels for Sound Event Detection Vary Depending on Annotators?](https://arxiv.org/abs/2410.09778)
  - **标题**: 铅数据集：声音事件检测的标签如何根据注释者而有所不同？
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Technical Design Review of Duke Robotics Club's Oogway: An AUV for RoboSub 2024](https://arxiv.org/abs/2410.09684)
  - **标题**: Duke Robotics Club的Oogway的技术设计评论：RoboSub 2024的AUV
  - **Filtered Reason**: none of cs.RO in whitelist
- [Traversability-Aware Legged Navigation by Learning from Real-World Visual Data](https://arxiv.org/abs/2410.10621)
  - **标题**: 通过从现实世界的视觉数据中学习，遍历性能的腿部导航
  - **Filtered Reason**: none of cs.RO in whitelist
- [HumanFT: A Human-like Fingertip Multimodal Visuo-Tactile Sensor](https://arxiv.org/abs/2410.10353)
  - **标题**: HumanFT：类似人类的指尖多模式视觉触诊传感器
  - **Filtered Reason**: none of cs.RO in whitelist
- [Oogway: Designing, Implementing, and Testing an AUV for RoboSub 2023](https://arxiv.org/abs/2410.10900)
  - **标题**: Oogway：设计，实施和测试RoboSub 2023
  - **Filtered Reason**: none of cs.RO in whitelist
- [Multimodal Fusion with Relational Learning for Molecular Property Prediction](https://arxiv.org/abs/2410.12128)
  - **标题**: 与分子财产预测的关系学习的多模式融合
  - **Filtered Reason**: none of cs.CE in whitelist
- [PhysioFormer: Integrating Multimodal Physiological Signals and Symbolic Regression for Explainable Affective State Prediction](https://arxiv.org/abs/2410.11376)
  - **标题**: 物理学：整合多模式生理信号和可解释的情感状态预测的符号回归
  - **Filtered Reason**: none of cs.CE in whitelist
- [What Social Media Use Do People Regret? An Analysis of 34K Smartphone Screenshots with Multimodal LLM](https://arxiv.org/abs/2410.11354)
  - **标题**: 人们后悔什么社交媒体？使用多模式LLM对34K智能手机屏幕截图的分析
  - **Filtered Reason**: none of cs.HC in whitelist
- [SF-Speech: Straightened Flow for Zero-Shot Voice Clone on Small-Scale Dataset](https://arxiv.org/abs/2410.12399)
  - **标题**: SF语音：小型数据集上的零发音克隆的拉直流程
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Multimodal growth and development assessment model](https://arxiv.org/abs/2410.13647)
  - **标题**: 多模式增长和发展评估模型
  - **Filtered Reason**: none of cs.MM,cs.CE in whitelist
- [Trinity: A General Purpose FHE Accelerator](https://arxiv.org/abs/2410.13405)
  - **标题**: 三位一体：通用加速器
  - **Filtered Reason**: none of cs.CR,cs.AR in whitelist
- [Align-ULCNet: Towards Low-Complexity and Robust Acoustic Echo and Noise Reduction](https://arxiv.org/abs/2410.13620)
  - **标题**: Align-ulcnet：朝着低复杂性和强大的声学回声和降噪
  - **Filtered Reason**: none of eess.SP,eess.AS,cs.SD in whitelist
- [Evaluating Transferable Emotion Expressions for Zoomorphic Social Robots using VR Prototyping](https://arxiv.org/abs/2410.15486)
  - **标题**: 使用VR原型制作评估变形型社交机器人的可转移情绪表达
  - **Filtered Reason**: none of cs.RO,cs.HC in whitelist
- [Imprompter: Tricking LLM Agents into Improper Tool Use](https://arxiv.org/abs/2410.14923)
  - **标题**: Imfrompter：将LLM代理诱使使用不当的工具使用
  - **Filtered Reason**: none of cs.CR in whitelist
- [Diff-DAgger: Uncertainty Estimation with Diffusion Policy for Robotic Manipulation](https://arxiv.org/abs/2410.14868)
  - **标题**: Diff Dagger：通过扩散政策进行机器人操作的不确定性估计
  - **Filtered Reason**: none of cs.RO in whitelist
- [Edge Computing in Distributed Acoustic Sensing: An Application in Traffic Monitoring](https://arxiv.org/abs/2410.16278)
  - **标题**: 分布式声音传感中的边缘计算：交通监视中的应用程序
  - **Filtered Reason**: none of cs.NI,eess.AS,cs.SD in whitelist
- [ALDAS: Audio-Linguistic Data Augmentation for Spoofed Audio Detection](https://arxiv.org/abs/2410.15577)
  - **标题**: Aldas：欺骗音频检测的音频语言数据增强
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Robust Loop Closure by Textual Cues in Challenging Environments](https://arxiv.org/abs/2410.15869)
  - **标题**: 在具有挑战性的环境中，通过文本提示结束稳健的循环
  - **Filtered Reason**: none of eess.SY,cs.RO in whitelist
- [Task-oriented Robotic Manipulation with Vision Language Models](https://arxiv.org/abs/2410.15863)
  - **标题**: 通过视觉语言模型以任务为导向的机器人操作
  - **Filtered Reason**: none of cs.RO in whitelist
- [Mechanisms and Computational Design of Multi-Modal End-Effector with Force Sensing using Gated Networks](https://arxiv.org/abs/2410.17524)
  - **标题**: 使用门控网络通过力传感的多模式最终效应器的机理和计算设计
  - **Filtered Reason**: none of cs.RO in whitelist
- [DiffusionSeeder: Seeding Motion Optimization with Diffusion for Rapid Motion Planning](https://arxiv.org/abs/2410.16727)
  - **标题**: 扩散分娩者：随着快速运动计划扩散的播种运动优化
  - **Filtered Reason**: none of cs.RO in whitelist
- [Variational autoencoders stabilise TCN performance when classifying weakly labelled bioacoustics data](https://arxiv.org/abs/2410.17006)
  - **标题**: 分类弱标记的生物气流数据时，变异自动编码器稳定TCN性能
  - **Filtered Reason**: none of q-bio.QM,eess.AS,cs.SD in whitelist
- [UGotMe: An Embodied System for Affective Human-Robot Interaction](https://arxiv.org/abs/2410.18373)
  - **标题**: UGOTME：一种用于情感人类互动的体现系统
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Human-Robot Collaboration System Setup for Weed Harvesting Scenarios in Aquatic Lakes](https://arxiv.org/abs/2410.17685)
  - **标题**: 水上湖中的杂草收获场景的人机协作系统设置
  - **Filtered Reason**: none of cs.RO in whitelist
- [SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models](https://arxiv.org/abs/2410.18927)
  - **标题**: Safebench：多式模式模型的安全评估框架
  - **Filtered Reason**: none of cs.CR in whitelist
- [ChartA11y: Designing Accessible Touch Experiences of Visualizations with Blind Smartphone Users](https://arxiv.org/abs/2410.20545)
  - **标题**: Charta11y：使用盲智能手机用户设计可访问的可触摸体验
  - **Filtered Reason**: none of cs.HC in whitelist
- [Aqua-Sim Fourth Generation: Towards General and Intelligent Simulation for Underwater Acoustic Networks](https://arxiv.org/abs/2410.20698)
  - **标题**: Aqua-Sim第四代：迈向水下声学网络的一般和智能模拟
  - **Filtered Reason**: none of cs.NI in whitelist
- [A Digital Twin-based Intelligent Network Architecture for Underwater Acoustic Sensor Networks](https://arxiv.org/abs/2410.20151)
  - **标题**: 水下声学传感器网络的数字基智能网络体系结构
  - **Filtered Reason**: none of cs.NI in whitelist
- [Atrial Fibrillation Detection System via Acoustic Sensing for Mobile Phones](https://arxiv.org/abs/2410.20852)
  - **标题**: 通过声音传感手机的心房颤动检测系统
  - **Filtered Reason**: none of q-bio.QM,eess.AS,cs.SD,cs.CE in whitelist
- [Data-Efficient Low-Complexity Acoustic Scene Classification via Distilling and Progressive Pruning](https://arxiv.org/abs/2410.20775)
  - **标题**: 通过蒸馏和进行渐进的修剪数据有效的低复杂性声学场景分类
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Efficient Learned Query Execution over Text and Tables [Technical Report]](https://arxiv.org/abs/2410.22522)
  - **标题**: 有效的文本和表上的学习查询执行[技术报告]
  - **Filtered Reason**: none of cs.DB in whitelist
- [GPT-4o reads the mind in the eyes](https://arxiv.org/abs/2410.22309)
  - **标题**: GPT-4O在眼中读取思想
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [EnvoDat: A Large-Scale Multisensory Dataset for Robotic Spatial Awareness and Semantic Reasoning in Heterogeneous Environments](https://arxiv.org/abs/2410.22200)
  - **标题**: Envodat：用于机器人空间意识和语义推理的大规模多感官数据集
  - **Filtered Reason**: none of cs.RO in whitelist
- [Multimodal Semantic Communication for Generative Audio-Driven Video Conferencing](https://arxiv.org/abs/2410.22112)
  - **标题**: None
  - **Filtered Reason**: none of cs.MM in whitelist
- [Towards Data-Informed Interventions: Opportunities and Challenges of Street-level Multimodal Sensing](https://arxiv.org/abs/2410.22092)
  - **标题**: 迈向数据信息的干预措施：街道级多模式感应的机会和挑战
  - **Filtered Reason**: none of cs.HC in whitelist
- [Quality-Aware End-to-End Audio-Visual Neural Speaker Diarization](https://arxiv.org/abs/2410.22350)
  - **标题**: 质量意识的端到端音频神经扬声器诊断
  - **Filtered Reason**: none of eess.AS,cs.SD,cs.MM in whitelist
- [Timbre Difference Capturing in Anomalous Sound Detection](https://arxiv.org/abs/2410.22033)
  - **标题**: 在异常检测中捕获的音色差异
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Representational learning for an anomalous sound detection system with source separation model](https://arxiv.org/abs/2410.21797)
  - **标题**: 具有源分离模型的异常声音检测系统的代表性学习
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [VisualCoder: Guiding Large Language Models in Code Execution with Fine-grained Multimodal Chain-of-Thought Reasoning](https://arxiv.org/abs/2410.23402)
  - **标题**: 可录像带：用精细的多模式链的推理指导代码执行中的大型语言模型
  - **Filtered Reason**: none of cs.SE in whitelist
- [Exploring the Potential of Multi-modal Sensing Framework for Forest Ecology](https://arxiv.org/abs/2410.23033)
  - **标题**: 探索森林生态学多模式传感框架的潜力
  - **Filtered Reason**: none of cs.RO in whitelist
- [ISAC Prototype System for Multi-Domain Cooperative Communication Networks](https://arxiv.org/abs/2410.22956)
  - **标题**: ISAC原型系统用于多域合作通信网络
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [Leader-Follower 3D Formation for Underwater Robots](https://arxiv.org/abs/2410.23128)
  - **标题**: 领导者的领导者3D组形成
  - **Filtered Reason**: none of cs.RO in whitelist
