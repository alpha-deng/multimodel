# 2024-01 月度论文分类汇总

共有385篇相关领域论文, 另有63篇其他

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 19 篇论文

### Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education 
[[arxiv](https://arxiv.org/abs/2401.00832)] [[cool](https://papers.cool/arxiv/2401.00832)] [[pdf](https://arxiv.org/pdf/2401.00832)]
> **Authors**: Arne Bewersdorff,Christian Hartmann,Marie Hornberger,Kathrin Seßler,Maria Bannert,Enkelejda Kasneci,Gjergji Kasneci,Xiaoming Zhai,Claudia Nerdel
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: revised version 2. September 2024
- **标题**: 迈出生成人工智能的下一步：多模式大语模型在科学教育中的变革性作用
- **领域**: 人工智能,计算机与社会
- **摘要**: 在教育中，人工智能（AI）的整合（AI），尤其是基于大语言模型（LLM）的系统，在增强教学经验方面有希望。但是，能够处理多模式（GPT-4V）的多模式大语言模型（MLLM）的出现，能够处理多模式数据，包括文本，声音和视觉输入，开设了一个丰富，个性化和交互式学习景观的新时代。本文以多媒体学习理论为基础，探讨了MLLM在科学教育的中心方面的变革性作用，通过呈现典范的创新学习场景。 MLLM的可能应用可能从内容创建到为学习的量身定制的支持，促进科学实践中的能力以及提供评估和反馈。这些场景不仅限于基于文本和单模式的格式，但可以是多模式的，从而增加了个性化，可访问性和潜在的学习有效性。除了许多机会外，数据保护和道德考虑之类的挑战变得更加突出，呼吁建立强大的框架以确保负责任的集成。本文强调了采用平衡方法实施MLLM的必要性，在该技术中，该技术补充而不是取代教育者的角色，从而确保了AI在科学教育中的有效和道德使用。它要求进一步的研究探讨MLLM对教育者不断发展的作用的细微含义，并将话语扩展到科学教育之外，以延伸到其他学科。通过探索潜力，挑战和未来的影响，我们旨在为科学教育及其他方面的MLLM的变革性轨迹做出贡献。

### AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Ten Modalities via Language as a Reference Framework 
[[arxiv](https://arxiv.org/abs/2401.00546)] [[cool](https://papers.cool/arxiv/2401.00546)] [[pdf](https://arxiv.org/pdf/2401.00546)]
> **Authors**: Run Shao,Cheng Yang,Qiujun Li,Qing Zhu,Yongjun Zhang,YanSheng Li,Yu Liu,Yong Tang,Dapeng Liu,Shizhong Yang,Haifeng Li
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: 19 pages, 19 tables, 3 figures
- **标题**: AllSpark：具有十种模式通过语言作为参考框架的多模式时空通用智能模型
- **领域**: 人工智能,机器学习
- **摘要**: 利用多模式数据是理解地理对象的固有要求。但是，由于各种时空模态之间的结构和语义异质性很高，因此多模式时空数据的联合解释长期以来一直是一个极具挑战性的问题。主要的挑战在于在不同方式的凝聚力和自主权之间取决于权衡。随着方式的数量扩大，这种权衡变​​得逐渐非线性。受到人类认知系统和语言哲学的启发，五种感官的感知信号融合到语言中，我们引入了语言作为参考框架（LARF），这是构建多模式统一模型的基本原则。在此基础上，我们提出了AllSpark，这是一种多模式时空通用人工智能模型。我们的模型将十种不同的模式集成到一个统一的框架中。为了实现模态凝聚力，AllSpark引入了模态桥梁和多模式大型语言模型（LLM），以将各种模态特征映射到语言特征空间中。为了维持模态自主权，AllSpark使用特定于模态的编码器来提取各种时空模态的令牌。最后，观察模型的解释性和下游任务之间的差距，我们设计了特定于模式的提示和任务头，从而增强了模型在特定任务之间的概括能力。实验表明，语言的融合使AllSpark能够在RGB和Point Cloud模式的几杆分类任务中表现出色，而无需额外的培训，超过基线性能高达41.82 \％。源代码可在https://github.com/geox-lab/allspark上找到。

### Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy 
[[arxiv](https://arxiv.org/abs/2401.00430)] [[cool](https://papers.cool/arxiv/2401.00430)] [[pdf](https://arxiv.org/pdf/2401.00430)]
> **Authors**: Weijian Mai,Jian Zhang,Pengfei Fang,Zhijun Zhang
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: 大脑条件多模式合成：调查和分类学
- **领域**: 人工智能
- **摘要**: 在人工智能产生的内容（AIGC）时代，有条件的多模式合成技术（例如，文本到图像，文本到文本，视频，文本到原告等）正在逐渐重塑现实世界中的自然内容。多模式合成技术的关键是建立不同方式之间的映射关系。大脑信号是大脑如何解释外部信息的潜在反思，表现出与各种外部方式的独特对应关系。这种对应关系使大脑信号成为多模式含量合成的有希望的指导条件。 Brian条件多模式合成是指解码大脑信号回到感知体验，这对于开发实用的脑部计算机界面系统和揭示大脑如何感知和理解外部刺激的复杂机制至关重要。这项调查全面研究了基于AIGC的大脑条件多模式合成的新兴领域，称为AIGC-BRAIN，以描述当前的景观和未来方向。首先，引入了相关的大脑神经成像数据集，功能性大脑区域和主流生成模型，作为AIGC-脑解码和分析的基础。接下来，我们为AIGC-脑解码模型和目前的特定任务代表性工作以及详细的实施策略提供了全面的分类法，以促进比较和深入分析。然后引入质量评估，以进行定性和定量评估。最后，这项调查探讨了获得的见解，提供了当前的挑战并概述了AIGC-Brain的前景。作为该领域的首届调查，本文为AIGC-脑研究的进步铺平了道路，提供了基础概述，以指导未来的工作。

### MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries 
[[arxiv](https://arxiv.org/abs/2401.01596)] [[cool](https://papers.cool/arxiv/2401.01596)] [[pdf](https://arxiv.org/pdf/2401.01596)]
> **Authors**: Akash Ghosh,Arkadeep Acharya,Prince Jha,Aniket Gaudgaul,Rajdeep Majumdar,Sriparna Saha,Aman Chadha,Raghav Jain,Setu Sinha,Shivani Agarwal
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: ECIR 2024
- **标题**: Medsumm：一种多模式的方法，用于总结代码混合的印度英语临床查询
- **领域**: 人工智能,计算语言学
- **摘要**: 在医疗保健领域，总结患者提出的医疗问题对于改善医生互动和医疗决策至关重要。尽管医学数据的复杂性和数量增长，但该领域的当前研究体主要集中在基于文本的方法上，忽视了视觉提示的整合。同样，医学问题领域的先前工作也仅限于英语。这项工作介绍了在低资源设置中的多模式医学问题摘要的任务。为了解决这一差距，我们介绍了多模式医学代码问题摘要MMCQS数据集，该数据集将印度英语Codemixed Medical查询与视觉辅助工具相结合。这种整合丰富了患者医疗状况的表示，提供了更全面的观点。我们还提出了一个名为Medsumm的框架，该框架利用LLM和VLM的功能来完成此任务。通过利用我们的MMCQS数据集，我们演示了整合图像中的视觉信息的价值，以改善医学上详细的摘要的创建。这种多模式策略不仅改善了医疗保健决策，而且还提高了对患者查询的更深入的理解，为在个性化和响应良好的医疗服务中探索的道路铺平了道路。我们的数据集，代码和预培训模型将公开可用。

### Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human 
[[arxiv](https://arxiv.org/abs/2401.02620)] [[cool](https://papers.cool/arxiv/2401.02620)] [[pdf](https://arxiv.org/pdf/2401.02620)]
> **Authors**: Song Bai,Jie Li
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: No comments
- **标题**: 3D Generative AI中的进度和前景：包括3D人类的技术概述
- **领域**: 人工智能,图形
- **摘要**: 尽管AI生成的文本和2D图像继续扩大其领土，但第3D代表逐渐出现是一种不容忽视的趋势。自2023年以来，3D代的领域已经出现了大量的研究论文。这种增长不仅包括创建3D对象，还包括3D特征和运动产生的快速发展。几个关键因素促成了这一进展。稳定扩散中的增强性，再加上控制方法，以确保多视图一致性和现实的人类模型（如SMPL-X）有助于生产具有显着一致性和近乎现实的外观的3D模型。基于神经网络的3D存储和渲染模型的进步，例如神经辐射场（NERF）和3D高斯分裂（3DGS），已加速了神经渲染模型的效率和现实主义。此外，大语言模型的多模式功能使语言输入能够超越人类运动输出。本文旨在提供大多在2023年下半年发表的相关论文的全面概述和摘要。它将首先讨论3D中的AI生成的对象模型，然后是生成的3D人类模型，最后是生成的3D人类动作，在未来的最终范围和愿景中涉及到最终的简介和愿景。

### Agent AI: Surveying the Horizons of Multimodal Interaction 
[[arxiv](https://arxiv.org/abs/2401.03568)] [[cool](https://papers.cool/arxiv/2401.03568)] [[pdf](https://arxiv.org/pdf/2401.03568)]
> **Authors**: Zane Durante,Qiuyuan Huang,Naoki Wake,Ran Gong,Jae Sung Park,Bidipta Sarkar,Rohan Taori,Yusuke Noda,Demetri Terzopoulos,Yejin Choi,Katsushi Ikeuchi,Hoi Vo,Li Fei-Fei,Jianfeng Gao
> **First submission**: 2024-01-07
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 代理AI：测量多模式相互作用的视野
- **领域**: 人工智能,人机交互,机器学习
- **摘要**: 多模式AI系统可能会在我们的日常生活中成为无处不在的存在。使这些系统更加互动的一种有希望的方法是将它们体现为物理和虚拟环境中的代理。目前，系统利用现有的基础模型作为创建具体代理的基本构件。将代理嵌入到这种环境中可以促进模型处理和解释视觉和上下文数据的能力，这对于创建更复杂和上下文感知的AI系统至关重要。例如，可以感知用户行动，人类行为，环境对象，音频表达式以及场景的集体情感的系统，可用于在给定环境中告知和指导代理响应。为了加速基于代理的多模式智能的研究，我们将“代理AI”定义为一类交互式系统，这些系统可以感知视觉刺激，语言输入和其他环保数据，并可以产生有意义的体现动作。特别是，我们探索旨在通过纳入外部知识，多感官输入和人类反馈来改善代理的系统。我们认为，通过在接地环境中开发代理AI系统，也可以减轻大型基础模型的幻觉及其产生环境不正确的产出的趋势。代理AI的新兴领域集成了多模式相互作用的更广泛的代理方面。除了在物理世界中行动和互动的代理外，我们还设想了一个未来，人们可以轻松地创建任何虚拟现实或模拟场景，并与虚拟环境中体现的代理进行互动。

### UMIE: Unified Multimodal Information Extraction with Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2401.03082)] [[cool](https://papers.cool/arxiv/2401.03082)] [[pdf](https://arxiv.org/pdf/2401.03082)]
> **Authors**: Lin Sun,Kai Zhang,Qingyuan Li,Renze Lou
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: UMIE：统一的多模式信息提取指令调整
- **领域**: 人工智能
- **摘要**: 随着多媒体含量的普及增加，多模式信息提取（MIE）引起了极大的关注。但是，当前的MIE方法通常使用特定于任务的模型结构，从而导致跨任务的可推广性有限，并且不足以跨MIE任务的共同知识。为了解决这些问题，我们提出了统一的多模式信息提取器Umie，将三个MIE任务统一作为使用指令调整的一代问题，能够有效地提取文本和视觉提及。广泛的实验表明，我们的单一UMIE在六个MIE数据集上胜过各种三个任务的MIE数据集的各种最新方法（SOTA）。此外，深入的分析表明，乌米在零击设置，对指导变体的鲁棒性和可解释性中的强烈概括。我们的研究是迈向统一MIE模型的第一步，并启动了MIE领域内的指导调整和大型语言模型的探索。我们的代码，数据和模型可从https://github.com/zucc-ai/umie获得

### Yes, this is what I was looking for! Towards Multi-modal Medical Consultation Concern Summary Generation 
[[arxiv](https://arxiv.org/abs/2401.05134)] [[cool](https://papers.cool/arxiv/2401.05134)] [[pdf](https://arxiv.org/pdf/2401.05134)]
> **Authors**: Abhisek Tiwari,Shreyangshu Bera,Sriparna Saha,Pushpak Bhattacharyya,Samrat Ghosh
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-11
> **comment**: No comments
- **标题**: 是的，这就是我想要的！迈向多模式医学咨询有关摘要的生成
- **领域**: 人工智能,计算语言学
- **摘要**: 在过去的几年中，互联网用于与医疗保健相关的任务的使用已突飞猛进，在有效管理和处理信息方面提出了挑战，以确保其有效利用。在情感动荡和心理挑战的时刻，我们经常转向互联网作为我们最初的支持来源，选择这是由于相关的社会污名而不是与他人讨论我们的感受。在本文中，我们提出了一项新的多模式医学关注摘要（MMC）一代的任务，该任务提供了简短而精确的摘要。非语言提示，例如患者的手势和面部表情，有助于准确识别患者的关注点。医生还考虑患者的个人信息，例如年龄和性别，以适当地描述医疗状况。由患者的个人背景和视觉手势的潜在功效激发，我们提出了一个基于变压器的多任务，多模式意图识别和医学关注的摘要生成（IR-MMCSG）系统。此外，我们提出了一个多任务框架，以意图识别和医学关注医生咨询的摘要生成。我们构建了第一个多模式医学关注摘要生成（MM-Mediconsummatimation）语料库，其中包括带有医疗关注的摘要，意图，患者个人信息，医生建议和关键字的患者医生咨询。我们的实验和分析表明，（a）患者表达/手势及其个人信息在意图识别和医学关注摘要产生中的重要作用，以及（b）意图识别与患者的医学关注摘要生成数据集和源代码之间的密切相关性在https://github.com/github.com/nlp-rl/mmmcsg上可用。

### MMToM-QA: Multimodal Theory of Mind Question Answering 
[[arxiv](https://arxiv.org/abs/2401.08743)] [[cool](https://papers.cool/arxiv/2401.08743)] [[pdf](https://arxiv.org/pdf/2401.08743)]
> **Authors**: Chuanyang Jin,Yutong Wu,Jing Cao,Jiannan Xiang,Yen-Ling Kuo,Zhiting Hu,Tomer Ullman,Antonio Torralba,Joshua B. Tenenbaum,Tianmin Shu
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: ACL 2024. 26 pages, 11 figures, 7 tables
- **标题**: MMTOM-QA：多模式的思维理论问题回答
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 心理理论（汤姆）是理解人们心理状态的能力，是开发具有人类社会智力的机器的重要组成部分。最近的机器学习模型，尤其是大型语言模型，似乎显示了汤姆理解的某些方面。但是，现有的TOM基准测试使用单峰数据集 - 视频或文本。另一方面，人类汤姆不仅仅是视频或文字理解。人们可以灵活地根据概念表示（例如，目标，信念，计划）从任何可用数据中提取出来的人的思想。为了解决这个问题，我们介绍了多模式的心理问题回答理论（MMTOM-QA）基准。 MMTOM-QA全面评估了Machine Tom在多模式数据以及有关一个人在家庭环境中活动的不同类型的单峰数据。为了工程师多模式的TOM容量，我们提出了一种新颖的方法，即Bip-alm（贝叶斯逆计划通过语言模型加速）。 BIP-ALM从多模式数据提取统一表示形式，并利用语言模型进行可扩展的贝叶斯逆计划。我们对包括GPT-4在内的人类绩效，BIP-ALM和最先进的模型进行了系统的比较。实验表明，大型语言模型和大型多式联模模型仍然缺乏强大的TOM容量。另一方面，通过利用基于模型的精神推理和语言模型的力量，BIP-ALM显示出令人鼓舞的结果。

### Gemini Pro Defeated by GPT-4V: Evidence from Education 
[[arxiv](https://arxiv.org/abs/2401.08660)] [[cool](https://papers.cool/arxiv/2401.08660)] [[pdf](https://arxiv.org/pdf/2401.08660)]
> **Authors**: Gyeong-Geon Lee,Ehsan Latif,Lehong Shi,Xiaoming Zhai
> **First submission**: 2023-12-26
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: Gemini Pro被GPT-4V击败：教育的证据
- **领域**: 人工智能,计算语言学
- **摘要**: 这项研究比较了Gemini Pro和GPT-4V在教育环境中的分类性能。该研究采用视觉问题答案（VQA）技术，研究了这两种模型的能力，可以阅读基于文本的标题，然后在科学教育中自动为学生绘制的模型评分。我们使用源自学生绘制的科学模型的数据集并采用NERIF（图像反馈反馈）采用NERIF（符号增强的标题）提示方法，同时采用了定量和定性分析。研究结果表明，GPT-4V在评分准确性和二次加权Kappa方面显着优于Gemini Pro。定性分析表明，差异可能是由于模型在图像和整体图像分类性能中处理细粒文本的能力。 Gemini Pro似乎也无法像GPT-4V那样进行进一步调整NERIF方法。研究结果表明，GPT-4V在处理复杂的多模式教育任务方面具有出色的能力。研究得出的结论是，尽管这两种模型代表了AI的进步，但GPT-4V的较高性能使其成为涉及多模式数据解释的教育应用程序更合适的工具。

### GATS: Gather-Attend-Scatter 
[[arxiv](https://arxiv.org/abs/2401.08525)] [[cool](https://papers.cool/arxiv/2401.08525)] [[pdf](https://arxiv.org/pdf/2401.08525)]
> **Authors**: Konrad Zolna,Serkan Cabi,Yutian Chen,Eric Lau,Claudio Fantacci,Jurgis Pasukonis,Jost Tobias Springenberg,Sergio Gomez Colmenarejo
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: GATS：聚会 - 分类
- **领域**: 人工智能,计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 随着AI社区越来越多地采用大型模型，开发一般和灵活的工具以整合它们至关重要。我们介绍了一个新型模块，该模块可以将训练有素的基础模型（包括训练和冷冻）无缝组合到较大的多模式网络中。 GAT赋予AI系统以不同速率处理和生成多种方式的信息。与传统的微调相反，GAT允许原始组件模型保持冷冻，从而避免了它们在训练阶段失去重要知识的风险。我们通过跨游戏，机器人技术和多模式输入输出系统进行了一些实验，证明了GAT的实用性和多功能性。

### Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination 
[[arxiv](https://arxiv.org/abs/2401.08025)] [[cool](https://papers.cool/arxiv/2401.08025)] [[pdf](https://arxiv.org/pdf/2401.08025)]
> **Authors**: Syeda Nahida Akter,Aman Madaan,Sangwu Lee,Yiming Yang,Eric Nyberg
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: 18 pages, 9 figures, 12 tables
- **标题**: 自我想象：使用自我构想的多模型模型有效的单峰推理
- **领域**: 人工智能,计算语言学,机器学习
- **摘要**: 视觉模型（VLM）的潜力通常在处理基于复杂的文本问题的问题中一直没有充分利用，尤其是当这些问题可能受益于视觉表示时。通过（1）从问题中创建一个视觉图以及（2）推论他们需要采取的步骤来解决它，我们提出了自我想象，与人类通过（1）创建视觉图来解决复杂的基于文本的问题的能力引起共鸣。我们利用单一视觉模型（VLM）使用HTML生成问题的结构化表示形式，然后将HTML作为图像渲染，最后使用相同的VLM使用问题和图像来回答问题。我们的方法不需要任何其他培训数据或培训。我们使用最先进的ART（Llava-1.5和Gemini Pro）VLMS评估了三个数学任务和9个通用推理任务的方法。我们的方法促进了LLAVA-1.5和Gemini Pro在所有数学任务上的性能（平均GSM8K： +3.1％； ASDIV： +3.2％； SVAMP： +6.9％），大多数通用推理任务的表现平均为3.2％至6.0％。

### When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment 
[[arxiv](https://arxiv.org/abs/2401.07764)] [[cool](https://papers.cool/arxiv/2401.07764)] [[pdf](https://arxiv.org/pdf/2401.07764)]
> **Authors**: Minrui Xu,Dusit Niyato,Jiawen Kang,Zehui Xiong,Shiwen Mao,Zhu Han,Dong In Kim,Khaled B. Letaief
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: 当大型语言模型代理遇到6G网络时：感知，接地和对齐
- **领域**: 人工智能,网络和互联网架构
- **摘要**: 基于多模式大语言模型（LLM）的AI代理人预计将彻底改变人类计算机的互动，并在医疗保健，教育，制造业和娱乐等各个领域提供更多个性化的助理服务。在6G网络中部署LLM代理使用户可以民主地通过移动设备访问以前昂贵的AI助手服务，从而减少交互延迟并更好地保留用户隐私。然而，移动设备的有限容量限制了部署和执行本地LLM的有效性，这需要将复杂的任务卸载到在长距离交互期间在边缘服务器上运行的全局LLM。在本文中，我们为6G网络中的LLM代理提出了一个拆分学习系统，利用移动设备和边缘服务器之间的协作，其中具有不同角色的多个LLM在移动设备和Edge服务器之间分布不同，以协作执行用户代理交互式任务。在拟议的系统中，LLM代理分为感知，接地和对齐模块，促进模块间通信，以满足6G网络功能的扩展用户需求，包括集成的传感和通信，数字双胞胎以及以任务为导向的通信。此外，我们在拟议的系统中引入了一种新颖的模型缓存算法，以改善上下文中的模型利用率，从而降低了协作移动和边缘LLM代理的网络成本。

### EFO: the Emotion Frame Ontology 
[[arxiv](https://arxiv.org/abs/2401.10751)] [[cool](https://papers.cool/arxiv/2401.10751)] [[pdf](https://arxiv.org/pdf/2401.10751)]
> **Authors**: Stefano De Giorgis,Aldo Gangemi
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: EFO：情感框架本体论
- **领域**: 人工智能,计算机与社会,符号计算
- **摘要**: 情绪是各个学科中激烈辩论的主题。尽管理论和定义泛滥，但当我们谈论或分类 - 它们时，仍然尚无共识，以及如何对所涉及的不同概念进行建模。在本文中，我们提出了一个基于猫头鹰框架的情感本体：情感框架本体论（EFO）。 EFO将情感视为语义框架，并具有一系列语义角色，可捕捉情感体验的不同方面。 EFO遵循基于模式的本体设计，并与Dolce基础本体论一致。 EFO用于建模多种情感理论，可以将其作为情感本体网络中的模块交联。在本文中，我们通过将Ekman的基本情绪（BE）理论建模为EFO-BE BEE模块来体现它，并演示如何对情感情况表示自动推断。通过从Framester知识图中授予be情感框架，并从文本中实现基于图的情绪探测器来评估EFO-BE。此外，已经执行了多模式数据集的EFO整合，包括情感语音和情感表情，以进一步探究跨模式的情感语义。

### Red Teaming Visual Language Models 
[[arxiv](https://arxiv.org/abs/2401.12915)] [[cool](https://papers.cool/arxiv/2401.12915)] [[pdf](https://arxiv.org/pdf/2401.12915)]
> **Authors**: Mukai Li,Lei Li,Yuwei Yin,Masood Ahmed,Zhenguang Liu,Qi Liu
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: Working in progress
- **标题**: 红色小组视觉语言模型
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: VLM（视觉模型）扩展了LLM（大语言模型）接受多模式输入的功能。由于已经验证了LLM可以通过特定的测试用例（称为红色组合）引起LLMS产生有害或不准确的内容，因此VLM在类似情况下的表现，尤其是与文本和视觉输入的结合在一起，仍然是一个问题。为了探索这个问题，我们提出了一个新颖的红色团队数据集RTVLM，其中包括10个子任务（例如，图像误导，多模式的监狱，破坏性，面对公平等），在4个主要方面（忠诚，隐私，隐私，安全，公平，公平，公平，公平）。我们的RTVLM是第一个从这四个不同方面进行基准的当前VLM的红色团队数据集。详细的分析表明，有10个著名的开源VLM在不同程度上与红色团队斗争，而GPT-4V的性能差距高达31％。此外，我们只需使用RTVLM进行监督的微调（SFT），将红色组合对齐方式应用于Llava-V1.5，然后在RTVLM测试集中以10％的速度增强了模型的性能，在MM-HAL中为13％，在MM-Bench中，MM-Bench的下降，MM-Bench中没有明显的下降，超过了其他基于Llava的模型，该模型与其他常规的Alignerment Data有关。这表明目前的开源VLM仍然缺乏红色的团队对齐。我们的代码和数据集将是开源的。

### Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control 
[[arxiv](https://arxiv.org/abs/2401.12624)] [[cool](https://papers.cool/arxiv/2401.12624)] [[pdf](https://arxiv.org/pdf/2401.12624)]
> **Authors**: Yongjun Kim,Sejin Seo,Jihong Park,Mehdi Bennis,Seong-Lyun Kim,Junil Choi
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: No comments
- **标题**: 从面向语言到紧急沟通的知识蒸馏，用于多代理遥控器
- **领域**: 人工智能,信息论,机器学习,网络和互联网架构
- **摘要**: 在这项工作中，我们比较了基于多代理深度强化学习（MADRL）和面向语言的语义交流（LSC）建立的新兴沟通（EC），该语言通过人类语言赋予了预先训练的大语言模型（LLM）。在多模式输入数据包括位置和通道图的多模式输入数据中，EC在使用多模式数据时会产生高训练成本和挣扎，而LSC由于LLM的较大尺寸而产生高推理计算成本。为了解决它们各自的瓶颈，我们通过通过知识蒸馏（KD）指导LSC来指导EC培训，提出了一个新颖的语言引导EC（LEC）框架。模拟证实了LEC可以达到更快的旅行时间，同时避免了通道条件较差的区域，并且与EC相比，MADRL训练的收敛加速高达61.8％。

### On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks 
[[arxiv](https://arxiv.org/abs/2401.14811)] [[cool](https://papers.cool/arxiv/2401.14811)] [[pdf](https://arxiv.org/pdf/2401.14811)]
> **Authors**: Joar Skalse,Alessandro Abate
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: ef:Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, PMLR 216:1974-1984, 2023
- **标题**: 关于马尔可夫奖励表达多目标，对风险敏感和模态任务的局限性
- **领域**: 人工智能,机器学习
- **摘要**: 在本文中，我们研究了标量，马尔可夫奖励功能（RL）的表现力，并确定了它们可以表达的几个局限性。具体来说，我们查看三类RL任务。多目标RL，对风险敏感的RL和模态RL。对于每个班级，我们得出了必要和充分的条件，这些条件描述了何时可以使用标量，马尔可夫奖励来表达此类问题。此外，我们发现标量，马尔可夫奖励无法表达这三个类别中的每一个中的大多数实例。因此，我们有助于更完整地了解哪些标准奖励功能可以和无法表达的功能。除此之外，我们还呼吁人们注意将模态问题作为新的问题，因为到目前为止，在RL文献中尚未获得任何系统治疗。我们还简要概述了一些通过定制RL算法来解决我们讨论的一些问题的方法。

### Synthetic Multimodal Dataset for Empowering Safety and Well-being in Home Environments 
[[arxiv](https://arxiv.org/abs/2401.14743)] [[cool](https://papers.cool/arxiv/2401.14743)] [[pdf](https://arxiv.org/pdf/2401.14743)]
> **Authors**: Takanori Ugai,Shusaku Egami,Swe Nwe Nwe Htun,Kouji Kozaki,Takahiro Kawamura,Ken Fukuda
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: 7 pages, 2 figures,4 tables
- **标题**: 合成多模式数据集，用于在家庭环境中赋予安全性和福祉
- **领域**: 人工智能
- **摘要**: 本文介绍了日常活动的合成多模式数据集，该数据集将来自3D虚拟空间模拟器的视频数据与知识图融合在一起，描绘了活动的时空环境。该数据集是针对社会问题（KGRC4SI）的知识图推理挑战开发的，该挑战的重点是识别和解决家庭环境中的危险情况。该数据集可向公众使用，作为研究人员和从业者开发创新解决方案的宝贵资源，以确认人类行为以增强安全性和福祉

### Triple Disentangled Representation Learning for Multimodal Affective Analysis 
[[arxiv](https://arxiv.org/abs/2401.16119)] [[cool](https://papers.cool/arxiv/2401.16119)] [[pdf](https://arxiv.org/pdf/2401.16119)]
> **Authors**: Ying Zhou,Xuefeng Liang,Han Chen,Yin Zhao,Xin Chen,Lida Yu
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: 14 pages, 6 figures
- **标题**: 多模式情感分析的三重删除表示学习
- **领域**: 人工智能
- **摘要**: 由于各种方式的全面信息，尤其是互补信息，多模式学习在情感分析任务中具有重要优势。因此，许多新兴研究集中于从输入数据中解散模态不变和模式特异性表示，然后将它们融合以进行预测。但是，我们的研究表明，特定于模式的表示可能包含与任务无关或冲突的信息，从而降低了学习多模式表示的有效性。我们重新审视了分离问题，并提出了一种新型的三重解开方法Tridira，该方法将模态不变，有效的方式特异性和无效的方式特异性表示从输入数据中删除。通过仅融合模态不变和有效的特定方式表示，Tridira可以大大减轻模型培训期间跨模态无关和冲突信息的影响。在四个基准数据集上进行的广泛实验证明了我们的三重分解的有效性和概括，这表现优于SOTA方法。

## 硬件架构(cs.AR:Hardware Architecture)

该领域共有 2 篇论文

### AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology 
[[arxiv](https://arxiv.org/abs/2401.11459)] [[cool](https://papers.cool/arxiv/2401.11459)] [[pdf](https://arxiv.org/pdf/2401.11459)]
> **Authors**: Rongqing Cong,Wenyang He,Mingxuan Li,Bangning Luo,Zebin Yang,Yuchao Yang,Ru Huang,Bonan Yan
> **First submission**: 2024-01-21
> **First announcement**: 2024-01-22
> **comment**: for associated source codes, see https://bonany.cc/attentionleg
- **标题**: 注意力：用于空间尺度的大型语言模型加速器的开源构建块与内存技术的处理
- **领域**: 硬件架构,人工智能,机器学习
- **摘要**: 具有变压器体系结构的大型语言模型（LLM）在自然语言处理，多模式生成人工智能和面向代理的人工智能方面已变得惊人。自我发场模块是基于变压器的LLM中最主要的子结构。使用通用图形处理单元（GPU）进行计算，对I/O带宽构成了鲁ck的需求，以在记忆和处理单元之间传输中间计算结果。为了应对这一挑战，这项工作开发了一个完全定制的香草自我发挥的加速器，注意力为，它是构建可空间扩展LLM处理器的基本构件。 CastionLego提供了基本的实现，并具有完全注定的数字逻辑，并结合了内存处理（PIM）技术。它基于基于PIM的矩阵矢量乘法和基于查找的SoftMax设计。开源代码可在线获得：https：//bonany.cc/atterentionleg。

### LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation 
[[arxiv](https://arxiv.org/abs/2401.12224)] [[cool](https://papers.cool/arxiv/2401.12224)] [[pdf](https://arxiv.org/pdf/2401.12224)]
> **Authors**: Ruizhe Zhong,Xingbo Du,Shixiong Kai,Zhentao Tang,Siyuan Xu,Hui-Ling Zhen,Jianye Hao,Qiang Xu,Mingxuan Yuan,Junchi Yan
> **First submission**: 2023-12-28
> **First announcement**: 2024-01-23
> **comment**: 15 pages, 4 figures
- **标题**: LLM4EDA：电子设计自动化的大语言模型的新兴进度
- **领域**: 硬件架构,人工智能
- **摘要**: 在摩尔定律的驱动下，现代芯片设计的复杂性和规模正在迅速增加。电子设计自动化（EDA）已被广泛应用，以应对完整的芯片设计过程中遇到的挑战。但是，非常大规模的综合电路的演变使芯片设计耗时和资源密集，需要实质性的先前专家知识。另外，中间的人类控制活动对于寻求最佳解决方案至关重要。在系统设计阶段，电路通常用硬件说明语言（HDL）表示为文本格式。最近，大型语言模型（LLMS）在上下文理解，逻辑推理和回答生成中证明了它们的能力。由于可以用文本格式用HDL代表电路，因此有理由质疑是否可以在EDA字段中利用LLMS来实现完全自动化的芯片设计并生成具有提高功率，性能和区域（PPA）的电路。在本文中，我们介绍了一项有关LLM在EDA领域中应用的系统研究，将其归类为以下情况：1）助手聊天机器人，2）HDL和脚本生成，以及3）HDL验证和分析。此外，我们强调了未来的研究方向，重点是将LLMS应用于逻辑合成，物理设计，多模式特征提取和电路对齐。我们通过以下链接在该领域收集相关论文：https：//github.com/thinklab-sjtu/awesome-llm4eda。

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

该领域共有 2 篇论文

### Must: Maximizing Latent Capacity of Spatial Transcriptomics Data 
[[arxiv](https://arxiv.org/abs/2401.07543)] [[cool](https://papers.cool/arxiv/2401.07543)] [[pdf](https://arxiv.org/pdf/2401.07543)]
> **Authors**: Zelin Zang,Liangyu Li,Yongjie Xu,Chenrui Duan,Kai Wang,Yang You,Yi Sun,Stan Z. Li
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: 30 pages and 6 figures, plus 27 pages and 14 figures in appendices
- **标题**: 必须：最大化空间转录组学数据的潜能
- **领域**: 计算工程、金融和科学,人工智能
- **摘要**: 空间转录组学（ST）技术通过在转录组，空间和形态学中提供多模态数据，彻底改变了组织中基因表达模式的研究，为了解超出转录组学以外的组织生物学提供了机会。但是，我们确定了ST数据物种中的模态偏差现象，即不同方式对标签的不一致的贡献导致分析方法的趋势，以保留主导方式的信息。如何减轻模态偏见的不利影响以满足各种下游任务仍然是一个根本的挑战。本文介绍了多种模式结构转换，必须是一种应对挑战的新方法。必须将ST数据中包含的多模式信息有效地集成到一个均匀的潜在空间中，以为所有下游任务提供基础。它通过拓扑发现策略和拓扑融合损失函数来学习内在的局部结构，以解决不同方式之间的不一致之处。因此，这些基于拓扑的和深度学习的技术为各种分析任务提供了坚实的基础，同时协调不同的方式。必须通过性能指标和生物学意义来评估必须的有效性。结果表明，在识别组织和生物标志物的结构方面，它的优于现有的最新方法具有明显的优势。必须提供一个多功能工具包，以进行复杂的复杂生物系统分析。

### Multimodal Deep Learning of Word-of-Mouth Text and Demographics to Predict Customer Rating: Handling Consumer Heterogeneity in Marketing 
[[arxiv](https://arxiv.org/abs/2401.11888)] [[cool](https://papers.cool/arxiv/2401.11888)] [[pdf](https://arxiv.org/pdf/2401.11888)]
> **Authors**: Junichiro Niimi
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: 多式联运的口碑文本和人口统计学以预测客户评级：在营销中处理消费者异质性
- **领域**: 计算工程、金融和科学,机器学习
- **摘要**: 在营销领域中，了解消费者异质性是消费者的内在或心理差异，无法通过行为日志捕获，这一直是一个关键的挑战。但是，当今许多消费者通常在在线平台上发布其评估，这可能是消费者之间这种不可观察到的差异的宝贵来源。先前的几项研究表明了对文本方式的分析的有效性，但另一方面，此类分析可能不一定证明单独的文本有足够的预测准确性，因为它们可能不包含横截面数据（例如消费者概况数据）的信息。此外，机器学习技术的最新进展，例如大规模语言模型（LLM）和多模式学习，使同时处理各种数据集（包括文本数据和传统的横截面数据）成为可能，并且可以从多种模态中有效地获得联合表示形式。因此，这项研究构建了一种产品评估模型，该模型通过在线产品评论和消费者资料信息的多模式学习来考虑消费者的异质性。我们还使用不同的模式或超参数比较多个模型，以证明在营销分析中多模式学习的鲁棒性。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 46 篇论文

### DocLLM: A layout-aware generative language model for multimodal document understanding 
[[arxiv](https://arxiv.org/abs/2401.00908)] [[cool](https://papers.cool/arxiv/2401.00908)] [[pdf](https://arxiv.org/pdf/2401.00908)]
> **Authors**: Dongsheng Wang,Natraj Raman,Mathieu Sibue,Zhiqiang Ma,Petr Babkin,Simerjot Kaur,Yulong Pei,Armineh Nourbakhsh,Xiaomo Liu
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: 16 pages, 4 figures
- **标题**: docllm：一种用于多模式文档理解的布局感知的生成语言模型
- **领域**: 计算语言学
- **摘要**: 企业文件，例如表格，发票，收据，报告，合同和其他类似记录，通常在文本和空间方式的交集中具有丰富的语义。其复杂布局提供的视觉提示在有效理解这些文档方面起着至关重要的作用。在本文中，我们介绍了Docllm，这是传统大型语言模型（LLMS）的轻量级扩展，以考虑文本语义和空间布局，以通过视觉文档进行推理。我们的模型通过避免昂贵的图像编码器并专注于边界框信息以结合空间布局结构，这与现有的多模式LLM不同。具体而言，文本和空间方式之间的交叉对齐是通过将经典变压器中的注意机制分解为一组分离的矩阵来捕获的。此外，我们设计了一个预训练目标，该目标学会填充文本段。这种方法使我们能够解决视觉文档中经常遇到的不规则布局和异质内容。使用大规模指令数据集对预训练的模型进行微调，涵盖了四个核心文档智能任务。我们证明，我们的解决方案在所有任务中的16个数据集中的14个数据集中，我们的解决方案都优于SOTA LLM，并将5个以前看不见的数据集中的4个概括为4个。

### A Multi-Task, Multi-Modal Approach for Predicting Categorical and Dimensional Emotions 
[[arxiv](https://arxiv.org/abs/2401.00536)] [[cool](https://papers.cool/arxiv/2401.00536)] [[pdf](https://arxiv.org/pdf/2401.00536)]
> **Authors**: Alex-Răzvan Ispas,Théo Deschamps-Berger,Laurence Devillers
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: Companion Publication of the 25th International Conference onMultimodalInteraction (pp. 311-317)
- **标题**: 一种多任务，多模式的方法，用于预测分类和尺寸情绪
- **领域**: 计算语言学,人工智能
- **摘要**: 近年来，在自发对话的背景下，言语情感识别（SER）受到了广泛关注。尽管在众所周知的自然主义二元对话语料库等数据集上，Iemocap在分类和维度情绪的情况下都取得了显着结果，但很少有论文试图同时预测这两个范式。因此，在这项工作中，我们旨在通过提出一个多任务，多模式系统来预测分类和维度情绪的多任务，多模式系统。结果强调了两种类型的情绪之间的交叉指导的重要性。我们的方法包括一个多任务，多模式的体系结构，该体系结构通过自我注意来使用平行的特征改进，以达到每种模式的特征。为了融合这些功能，我们的模型引入了一组可学习的桥梁令牌，这些桥梁令牌可以借助跨注意力融合声学和语言特征。我们对10倍验证结果的分类情绪的实验可与当前的最新作用相当。在我们的配置中，与单独学习每个范式相比，我们的多任务方法提供了更好的结果。最重要的是，与以前的多任务实验相比，我们最佳性能的模型可以取得很高的成果。

### SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation for Multi-modal Intent Detection 
[[arxiv](https://arxiv.org/abs/2401.00424)] [[cool](https://papers.cool/arxiv/2401.00424)] [[pdf](https://arxiv.org/pdf/2401.00424)]
> **Authors**: Shijue Huang,Libo Qin,Bingbing Wang,Geng Tu,Ruifeng Xu
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: Accepted by ICASSP 2024
- **标题**: SDIF-DA：具有多模式意图检测数据的浅至深度交互框架
- **领域**: 计算语言学
- **摘要**: 多模式意图检测旨在利用各种方式来了解用户的意图，这对于在现实世界中的对话系统部署至关重要。多模式意图检测的两个核心挑战是（1）如何有效地对齐和融合模式的不同特征以及（2）有限的标记的多模式多模式意图训练数据。在这项工作中，我们引入了一个带有数据增强（SDIF-DA）的浅至深度交互框架，以应对上述挑战。首先，SDIF-DA利用一个浅到深的交互模块来逐步有效地对齐和融合功能，跨文本，视频和音频方式。其次，我们提出了一种基于CHATGPT的数据增强方法，以自动增加足够的培训数据。实验结果表明，SDIF-DA可以通过实现最新性能来有效地对齐和融合多模式。此外，广泛的分析表明，引入的数据增强方法可以成功地从大语言模型中提取知识。

### GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse 
[[arxiv](https://arxiv.org/abs/2401.01523)] [[cool](https://papers.cool/arxiv/2401.01523)] [[pdf](https://arxiv.org/pdf/2401.01523)]
> **Authors**: Hongzhan Lin,Ziyang Luo,Bo Wang,Ruichao Yang,Jing Ma
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: The first work to benchmark LargeMultimodalModels in safety insight on social media
- **标题**: 山羊台：通过基于模因的社会虐待来对大型多模型模型的安全见解
- **领域**: 计算语言学,人工智能
- **摘要**: 社交媒体的指数增长深刻地改变了信息的创建，传播和吸收，超过了数字时代的任何先例。遗憾的是，这种爆炸还催生了对模因的在线滥用。评估模因的负面影响非常具有挑战性，这是由于它们经常微妙而隐含的含义，而这些含义并非直接通过公开的文本和图像传达。鉴于此，大型多模型（LMM）已成为兴趣的焦点，因为它们在处理多种多模式任务方面具有出色的功能。为了回应这一发展，我们的论文旨在彻底研究各种LMM（例如GPT-4O）的能力，以辨别和回应模因中社会虐待的细微方面。我们介绍了全面的模因基准，山羊板凳，包括超过6k的不同模因，这些模因包含主题，例如隐式仇恨言论，性别歧视和网络欺凌等。利用山羊板式，我们深入研究了LMM的能力，可以准确地评估仇恨，厌恶，厌恶，进攻性，进攻性，讽刺和危害内容。我们在一系列LMM的广泛实验表明，当前模型仍然表现出安全意识的不足，对各种形式的隐式滥用表示不敏感。我们认为，这种短缺代表了实现安全人工智能的关键障碍。在https://goatlmm.github.io/上可以公开访问山羊板和随附的资源，这为在这个重要领域的持续研究做出了贡献。

### A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2401.01495)] [[cool](https://papers.cool/arxiv/2401.01495)] [[pdf](https://arxiv.org/pdf/2401.01495)]
> **Authors**: Wei Ai,FuChen Zhang,Tao Meng,YunTao Shou,HongEn Shao,Keqin Li
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: 9 pages, 3 figures
- **标题**: 基于图形对比度学习的两阶段多模式识别模型
- **领域**: 计算语言学
- **摘要**: 就人类计算机的互动而言，在对话中正确理解用户的情绪状态变得越来越重要，因此多模式情感识别（MER）的任务开始受到更多关注。但是，现有的情绪分类方法通常仅执行一次分类。句子可能会在一轮分类中被错误分类。以前的工作通常会忽略融合过程中不同形态特征之间的相似性和差异。为了解决上述问题，我们提出了一个基于图形对比度学习（TS-GCL）的两阶段情绪识别模型。首先，我们以不同的预处理方式编码原始数据集。其次，为这三个模态数据引入了图形对比度学习（GCL）策略，并具有其他结构，以学习模态内部和模式之间的相似性和差异。最后，我们两次使用MLP来实现最终的情感分类。这种分阶段的分类方法可以帮助模型更好地专注于不同级别的情感信息，从而提高模型的性能。广泛的实验表明，与以前的方法相比，TS-GCL在IEmocap和MELD数据集上具有较高的性能。

### DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever 
[[arxiv](https://arxiv.org/abs/2401.01076)] [[cool](https://papers.cool/arxiv/2401.01076)] [[pdf](https://arxiv.org/pdf/2401.01076)]
> **Authors**: Zhichao Yin,Binyuan Hui,Min Yang,Fei Huang,Yongbin Li
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: ICASSP 2024
- **标题**: DialClip：将剪辑授权作为多模式对话框检索器
- **领域**: 计算语言学
- **摘要**: 最近，在预训练的视觉模型中取得的重大进步大大提高了多模式对话系统的功能。这些模型通过对下游任务进行微调表现出了重大改进。但是，现有的预训练模型主要集中于有效捕获视觉和语言方式之间的对齐方式，通常忽略了对话环境的复杂性质。在本文中，我们提出了一个名为DialClip的参数及时调用方法，用于多模式对话框检索。具体而言，我们的方法引入了多模式上下文提示生成器，以学习上下文特征，随后将其蒸馏成预先训练的视觉模型模型剪辑中的提示。此外，我们介绍了域提示，以减轻下游对话框数据的盘式排列。为了促进各种类型的检索，我们还设计了多个专家，以学习从剪辑输出到多模式代表空间的映射，每个专家都对一种特定的检索类型负责。广泛的实验表明，DialClip通过调整总参数的0.04％来实现两个公认的基准数据集（即PhotoChat和MmDialog）上的最新性能。这些结果突出了我们提出的方法的功效和效率，强调了其推进多模式对话框检索领域的潜力。

### Social Media Ready Caption Generation for Brands 
[[arxiv](https://arxiv.org/abs/2401.01637)] [[cool](https://papers.cool/arxiv/2401.01637)] [[pdf](https://arxiv.org/pdf/2401.01637)]
> **Authors**: Himanshu Maheshwari,Koustava Goswami,Apoorv Saxena,Balaji Vasan Srinivasan
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 社交媒体准备的品牌标题生成
- **领域**: 计算语言学
- **摘要**: 社交媒体广告是品牌营销的关键，旨在吸引迷人的字幕和图片或徽标的消费者。尽管以前的研究重点是为一般图像生成字幕，但将品牌个性纳入社交媒体字幕中仍然没有探索。品牌个性被证明会影响消费者的行为和社交互动，因此被证明是营销策略的关键方面。当前的开源多模式LLM不直接适合此任务。因此，我们提出了一种管道解决方案，以协助品牌创建与图像和品牌个性保持一致的引人入胜的社交媒体字幕。我们的架构基于两个部分：A第一部分包含图像字幕模型，该模型采用了该品牌想要在线发布并给出简单英语字幕的图像； b第二部分带有生成的标题以及目标品牌个性，并输出了与人格一致的吸引人的社交媒体标题。除了品牌个性外，我们的系统还使用户灵活地提供主题标签，Instagram手柄，URL和命名的实体，他们希望包含字幕，从而使字幕与社交媒体手柄更加语义相关。对各种基线的比较评估都证明了我们方法的有效性，无论是在定性和定量上。

### Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study 
[[arxiv](https://arxiv.org/abs/2401.02147)] [[cool](https://papers.cool/arxiv/2401.02147)] [[pdf](https://arxiv.org/pdf/2401.02147)]
> **Authors**: Ziqiang Zheng,Yiwei Chen,Jipeng Zhang,Tuan-Anh Vu,Huimin Zeng,Yue Him Wong Tim,Sai-Kit Yeung
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: 51 pages, 36 figures, Repository: https://github.com/hkust-vgd/Marine_GPT-4V_Eval
- **标题**: 在海洋分析中探索GPT-4V的边界：初步案例研究
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 大型语言模型（LLMS）证明了作为通用助手回答各种查询的强大能力。连续的多模式大型语言模型（MLLM）使LLM具有感知视觉信号的能力。 GPT-4（生成预训练的变压器）的推出对研究社区产生了浓厚的兴趣。 GPT-4V（ISON）在学术界和行业领域都表现出了重要的力量，这是新的人工智能产生的焦点。尽管GPT-4V取得了巨大的成功，但在特定于领域的分析（例如海洋分析）中探索MLLM所需的特定领域知识和专业知识的关注较少。在这项研究中，我们将利用GPT-4V进行海洋分析的初步和全面的案例研究。该报告对现有的GPT-4V进行了系统的评估，评估了GPT-4V在海洋研究中的性能，并为MLLM的未来发展设定了新的标准。 GPT-4V的实验结果表明，GPT-4V产生的响应仍然远离满足海洋专业的特定领域要求。本研究中使用的所有图像和提示将在https://github.com/hkust-vgd/marine_gpt-4v_eval上找到

### CANAMRF: An Attention-Based Model for Multimodal Depression Detection 
[[arxiv](https://arxiv.org/abs/2401.02995)] [[cool](https://papers.cool/arxiv/2401.02995)] [[pdf](https://arxiv.org/pdf/2401.02995)]
> **Authors**: Yuntao Wei,Yuzhe Zhang,Shuyang Zhang,Hong Zhang
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-08
> **comment**: 6 pages, 3 figures. Pacific Rim International Conference on Artificial Intelligence. Singapore: Springer Nature Singapore, 2023
- **标题**: CANAMRF：基于注意力模式抑郁检测的基于注意力的模型
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,图像和视频处理
- **摘要**: 多模式抑郁检测是一个重要的研究主题，旨在使用多模式数据预测人类精神状态。以前的方法平等地对待不同的模态，并通过幼稚的数学操作融合每种模态，而无需测量它们之间的相对重要性，而它们无法获得下游抑郁症任务的表现良好的多模式表示。为了解决上述问题，我们提出了一个具有自适应多模式复发融合（CANAMRF）的跨模式注意网络，用于多模式抑郁症检测。 CANAMRF由多模式特征提取器，自适应多模式融合模块和混合注意模块构建。通过在两个基准数据集上的实验，CANAMRF展示了最先进的性能，强调了我们提出的方法的有效性。

### PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language Models for Medical Imaging 
[[arxiv](https://arxiv.org/abs/2401.02797)] [[cool](https://papers.cool/arxiv/2401.02797)] [[pdf](https://arxiv.org/pdf/2401.02797)]
> **Authors**: Jinlong He,Pengfei Li,Gang Liu,Genrong He,Zhaolin Chen,Shenjun Zhong
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: 12 pages, 8 figures, 12 tables
- **标题**: pefomed：用于医学成像的多模式大语言模型的参数有效微调
- **领域**: 计算语言学,人工智能
- **摘要**: 多模式大型语言模型（MLLM）代表了传统大型语言模型功能的进化扩展，从而使他们能够应对超过纯粹基于文本应用程序的范围的挑战。它利用了这些语言模型中先前编码的知识，从而增强了其在多模式上下文统治时期的适用性和功能。最近的工作调查了MLLM作为一种通用解决方案，以解决医疗多模式问题作为生成任务。在本文中，我们建议使用公共基准数据集（Public Benchmark）数据集为微调MLLM的参数有效框架（MED-VQA）和医疗报告生成（MRG）任务进行了专门验证。我们还使用5点李克特量表及其加权平均值引入了评估度量，以衡量MRG任务生成的报告的质量，其中人类和GPT-4模型都标记了比例评分。我们进一步评估了VQA和MRG任务的传统措施，GPT-4和人类评级的绩效指标的一致性。结果表明，使用GPT-4的语义相似性评估与人类注释者紧密保持一致，并提供更大的稳定性，但是与常规词汇相似性测量相比，它们揭示了差异。这质疑词汇相似性指标的可靠性，以评估MED-VQA和报告生成任务中生成模型的性能。此外，我们的微调模型大大优于GPT-4V。这表明，如果没有其他微调的多模式模型（例如GPT-4V）在医学成像任务上无法有效执行。该代码将在此处提供：https：//github.com/jinlhe/pefomed。

### High-precision Voice Search Query Correction via Retrievable Speech-text Embedings 
[[arxiv](https://arxiv.org/abs/2401.04235)] [[cool](https://papers.cool/arxiv/2401.04235)] [[pdf](https://arxiv.org/pdf/2401.04235)]
> **Authors**: Christopher Li,Gary Wang,Kyle Kastner,Heng Su,Allen Chen,Andrew Rosenberg,Zhehuai Chen,Zelin Wu,Leonid Velikovich,Pat Rondon,Diamantino Caseiro,Petar Aleksic
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: No comments
- **标题**: 高精度语音搜索查询校正通过可检索的语音文本嵌入
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 由于各种原因，例如嘈杂的音频，缺乏足够的培训数据等，自动语音识别（ASR）系统可能会遭受不良的回忆。但是，如果文本假设在语音上与成绩单真相相似，则基于ASR  - 甲型的检索可以产生差的精度。在本文中，我们通过使用从话语音频派生的嵌入来查询校正数据库来消除假设 - 审判不匹配问题；话语音频和候选校正的嵌入是由经过训练的多模式语音文本嵌入网络产生的，以放置发音音频的嵌入以及其相应的文本成绩单的嵌入在一起。在使用最近的邻居搜索找到适当的校正候选者之后，我们在将候选人添加到原始的N最佳列表中，然后将候选人用其语音文本嵌入距离进行评分。我们在候选人集中出现的笔录中显示了相对单词错误率（WER）的6％，而不会增加一般话语。

### SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2401.03945)] [[cool](https://papers.cool/arxiv/2401.03945)] [[pdf](https://arxiv.org/pdf/2401.03945)]
> **Authors**: Dong Zhang,Zhaowei Li,Pengyu Wang,Xin Zhang,Yaqian Zhou,Xipeng Qiu
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: work in progress
- **标题**: 语音：使用多模式多代理系统的人类通信模拟
- **领域**: 计算语言学
- **摘要**: 人类交流是一个复杂而多样的过程，不仅涉及多种因素，例如语言，常识和文化背景，而且还需要参与多模式信息，例如语音。大型语言模型（LLM）的多代理系统在模拟人类社会方面表现出了有希望的表现。我们可以利用基于LLM的多代理系统模拟人类的沟通吗？但是，当前基于LLM的多代理系统主要依赖文本作为主要媒介。在本文中，我们提出了Speechagents，这是一种基于多模式LLM的多模式多代理系统，旨在模拟人类的交流。 Speechagents使用多模式LLM作为个别代理的控制中心，并使用多模式信号作为代理之间交换消息的媒介。此外，我们提出了多代理调整，以增强LLM的多代理功能，而不会损害一般能力。为了加强和评估人类交流模拟的有效性，我们构建了人类通信模拟基准。实验结果表明，语音代理可以模拟以一致的内容，真实的节奏和丰富的情感模拟人类的交流对话，即使有多达25个代理商也可以表现出出色的可扩展性，这可以适用于诸如戏剧创作和音频小说生成之类的任务。代码和型号将在https：// github开源。 COM/0NOUT/SEAKINGENTS

### MERA: A Comprehensive LLM Evaluation in Russian 
[[arxiv](https://arxiv.org/abs/2401.04531)] [[cool](https://papers.cool/arxiv/2401.04531)] [[pdf](https://arxiv.org/pdf/2401.04531)]
> **Authors**: Alena Fenogenova,Artem Chervyakov,Nikita Martynov,Anastasia Kozlova,Maria Tikhonova,Albina Akhmetgareeva,Anton Emelyanov,Denis Shevelev,Pavel Lebedev,Leonid Sinev,Ulyana Isaeva,Katerina Kolomeytseva,Daniil Moskovskiy,Elizaveta Goncharova,Nikita Savushkin,Polina Mikhailova,Denis Dimitrov,Alexander Panchenko,Sergei Markov
> **First submission**: 2024-01-09
> **First announcement**: 2024-01-10
> **comment**: The paper version comparable with the release code v.1.1.0 of the benchmark MERA. ACL-2024 main track camera ready version
- **标题**: 梅拉：俄罗斯的全面评估
- **领域**: 计算语言学,人工智能
- **摘要**: 在过去的几年中，AI研究中最著名的进步之一是基础模型（FMS），以语言模型（LMS）的兴起为标题。随着模型的大小的增加，LMS展示了可测量方面的增强和新的定性特征的发展。但是，尽管研究人员的注意力和LM应用的快速增长，但仍然需要更好地理解能力，局限性和相关风险。为了解决这些问题，我们引入了对俄罗斯架构（MERA）的开放多模式评估，这是一种新的指令基准，用于评估针对俄罗斯语言的基础模型。该基准包括11个技能域中生成模型的21个评估任务，并被设计为黑盒测试，以确保排除数据泄漏。本文介绍了一种评估可以扩展到其他方式的零和少量固定指令设置中的FMS和LMS的方法。我们提出了一种评估方法，一个用于MERA评估的开源代码基础以及具有提交系统的排行榜。我们将开放性LMS评估为基线，发现它们仍然远远落后于人类水平。我们公开发布Mera来指导即将进行的研究，预测开创性的模型特征，标准化评估程序并解决潜在的社会缺点。

### TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.04471)] [[cool](https://papers.cool/arxiv/2401.04471)] [[pdf](https://arxiv.org/pdf/2401.04471)]
> **Authors**: Xue Zhang,Xiangyu Shi,Xinyue Lou,Rui Qi,Yufeng Chen,Jinan Xu,Wenjuan Han
> **First submission**: 2024-01-09
> **First announcement**: 2024-01-10
> **comment**: Work in Progress
- **标题**: TransportationGames：（多模式）大语模型的基准测试运输知识
- **领域**: 计算语言学
- **摘要**: 大型语言模型（LLM）和多模式大语模型（MLLM）表现出了出色的一般能力，甚至在许多专业领域（例如法律，经济学，运输和医学）中表现出适应性。当前，已经提出了许多特定领域的基准测试，以验证特定领域（M）LLM的性能。在各个领域中，运输在现代社会中起着至关重要的作用，因为它影响了数十亿人的经济，环境和生活质量。但是，尚不清楚LLM拥有多少流量知识（M），以及它们是否可以可靠地执行与运输相关的任务。为了解决这一差距，我们提出了TransportationGames，这是一种经过精心设计且彻底的评估基准，用于评估运输域中的LLM。通过全面考虑在现实世界中的应用程序，并参考布鲁姆分类法中的前三个级别，我们在记忆，理解和应用所选任务的运输知识方面测试了各种（M）LLMS的性能。实验结果表明，尽管某些模型在某些任务中表现良好，但总体上仍然有很大的改进空间。我们希望释放运输游戏机可以作为未来研究的基础，从而加快（M）LLM在运输领域的实施和应用。

### REBUS: A Robust Evaluation Benchmark of Understanding Symbols 
[[arxiv](https://arxiv.org/abs/2401.05604)] [[cool](https://papers.cool/arxiv/2401.05604)] [[pdf](https://arxiv.org/pdf/2401.05604)]
> **Authors**: Andrew Gritsevskiy,Arjun Panickssery,Aaron Kirtland,Derik Kauffman,Hans Gundlach,Irina Gritsevskaya,Joe Cavanagh,Jonathan Chiang,Lydia La Roux,Michelle Hung
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-11
> **comment**: 20 pages, 5 figures. For code, see http://github.com/cvndsh/rebus
- **标题**: Rebus：理解符号的强大评估基准
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,计算机与社会
- **摘要**: 我们提出了一个新的基准测试，以评估多模式大语言模型在Rebus难题中的性能。该数据集涵盖了333个基于图像的文字游戏的原始示例，并认为电影，作曲家，主要城市和食物等13个类别。为了在识别引人的单词或短语的基准上实现良好的性能，模型必须将图像识别和弦乐操纵与假设测试，多步推理以及对人类认知的理解相结合，从而实现对功能的复杂，多模式评估。我们发现GPT-4O明显优于所有其他模型，其次是专有模型优于所有其他评估模型。但是，即使是最佳模型的最终精度也仅为42 \％，在硬性难题上仅降至7％，这强调了对推理的实质性改进的需求。此外，模型很少理解难题的所有部分，几乎总是无法追溯解释正确的答案。因此，我们的基准可用于确定多模式大语模型的知识和推理中的主要缺点。

### Cross-modal Retrieval for Knowledge-based Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2401.05736)] [[cool](https://papers.cool/arxiv/2401.05736)] [[pdf](https://arxiv.org/pdf/2401.05736)]
> **Authors**: Paul Lerner,Olivier Ferret,Camille Guinaudeau
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-12
> **comment**: No comments
- **标题**: 基于知识的视觉问题回答的跨模式检索
- **领域**: 计算语言学,信息检索
- **摘要**: 基于知识的视觉问题回答有关指定实体的一项艰巨的任务，需要从多模式知识库中检索信息。指定的实体具有不同的视觉表示形式，因此很难识别。我们认为，跨模式检索可能有助于弥合实体及其描述之间的语义差距，并且最重要的是与单模式检索互补。我们通过对最近的Viquae，Infoseek和百科全书-VQA数据集的多模式双重编码器（即剪辑）进行实验提供经验证据。此外，我们研究了三种不同的策略来微调这样的模型：单模式，跨模式或联合训练。我们的方法结合了单声道和跨模式检索，它与三个数据集中的十亿参数模型具有竞争力，同时在概念上更简单，更便宜。

### An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue Assistant 
[[arxiv](https://arxiv.org/abs/2401.06807)] [[cool](https://papers.cool/arxiv/2401.06807)] [[pdf](https://arxiv.org/pdf/2401.06807)]
> **Authors**: Mohit Tomar,Abhisek Tiwari,Tulika Saha,Prince Jha,Sriparna Saha
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 生态助理：建立多模式植物护理对话助理
- **领域**: 计算语言学,人工智能
- **摘要**: 近来，人们对即将到来的环境挑战的认识越来越多，导致人们表现出对照顾环境和培养绿色生活的更强有力的奉献精神。目前耗资196亿美元的室内园艺行业反映了这种日益增长的情绪，不仅意味着货币价值，而且还表明了人类与自然界重新建立联系的深刻渴望。但是，最近的几项调查对我们护理中的植物的命运提出了启示，其中一半以上屈服于不当护理的沉默威胁。因此，对能够协助和指导个人通过植物护理的复杂性的无障碍专业知识的需求比以往任何时候都变得更加重要。在这项工作中，我们首次尝试建立植物护理助理，该助理旨在通过对话来协助植物（-ing）关注的人。我们提出了一个名为Plantational的植物护理对话数据集，该数据集包含用户和植物护理专家之间的1K对话。我们的端到端提议的方法是两个方面：（i）我们首先在各种大型语言模型（LLMS）和视觉语言模型（VLM）的帮助下对数据集进行基准测试，并通过研究教学调整（零射击和很少发射提示）和精细调整技术对这项任务的影响； （ii）最后，我们建立了一种多模式的植物护理，辅助对话生成框架，并使用门控机制结合了基于适配器的模态输注。我们对各种LLM和VLM表现出的性能进行了广泛的检查（自动评估和手动评估），这是针对这些不同模型的各自的优势和劣势的特定于域的对话响应。

### Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning 
[[arxiv](https://arxiv.org/abs/2401.06805)] [[cool](https://papers.cool/arxiv/2401.06805)] [[pdf](https://arxiv.org/pdf/2401.06805)]
> **Authors**: Yiqi Wang,Wentao Chen,Xiaotian Han,Xudong Lin,Haiteng Zhao,Yongfei Liu,Bohan Zhai,Jianbo Yuan,Quanzeng You,Hongxia Yang
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 探索多模式大语言模型（MLLM）的推理能力：一项关于多模式推理新兴趋势的全面调查
- **领域**: 计算语言学,人工智能
- **摘要**: 强大的人工智能（强大的AI）或具有抽象推理能力的人工通用智能（AGI）是下一代AI的目标。大型语言模型（LLMS）的最新进步以及多模式大语模型（MLLM）的新兴领域在广泛的多模式任务和应用程序中表现出了令人印象深刻的功能。特别是，各种MLLM具有不同的模型体系结构，培训数据和培训阶段，这些MLLM已在广泛的MLLM基准中进行了评估。这些研究在不同程度上揭示了MLLM当前功能的不同方面。但是，尚未系统地研究MLLM的推理能力。在这项调查中，我们全面回顾了多模式推理的现有评估协议，对MLLM的前沿进行了分类和说明，介绍了MLLM在推理密集型任务上的应用趋势，并最终讨论当前的实践和未来的方向。我们认为我们的调查确立了坚实的基础，并阐明了这个重要主题，多模式推理。

### LightHouse: A Survey of AGI Hallucination 
[[arxiv](https://arxiv.org/abs/2401.06792)] [[cool](https://papers.cool/arxiv/2401.06792)] [[pdf](https://arxiv.org/pdf/2401.06792)]
> **Authors**: Feng Wang
> **First submission**: 2024-01-07
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 灯塔：AGI幻觉的调查
- **领域**: 计算语言学,人工智能
- **摘要**: 随着人工智能的发展，大型模型变得越来越聪明。但是，许多研究表明，这些大型模型中的幻觉是阻碍AI研究发展的瓶颈。为了追求实现强大的人工智能，正在对AGI（人工通用情报）幻觉研究投入大量研究工作。在LLM（大语言模型）内研究幻觉方面已经进行了先前的探索。至于多模式AGI，幻觉的研究仍处于早期阶段。为了进一步在幻觉现象领域的研究进展，我们介绍了鸟类对AGI幻觉的眼光，总结了当前关于AGI幻觉的工作，并提出了一些未来研究的指示。

### Large language models in healthcare and medical domain: A review 
[[arxiv](https://arxiv.org/abs/2401.06775)] [[cool](https://papers.cool/arxiv/2401.06775)] [[pdf](https://arxiv.org/pdf/2401.06775)]
> **Authors**: Zabir Al Nazi,Wei Peng
> **First submission**: 2023-12-12
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 医疗保健和医疗领域中的大型语言模型：评论
- **领域**: 计算语言学,人工智能
- **摘要**: 医疗保健部门中大型语言模型（LLM）的部署既引发了热情和忧虑。这些模型具有出色的能力，可以提供对自由文本查询的熟练反应，这表明对专业医学知识有细微的理解。这项全面的调查深入研究了为医疗保健应用设计的现有LLM的功能，从传统的审计语言模型（PLM）到医疗保健领域的LLMS现状，阐明了其发展的轨迹。首先，我们探讨了LLM的潜力，以扩大各种医疗保健应用的效率和有效性，尤其是专注于临床语言理解任务。这些任务包括广泛的范围，从指定的实体识别和关系提取到自然语言推断，多模式医学应用，文档分类和提问。此外，我们对医疗保健领域的最新最新LLM进行了广泛的比较，同时还评估了各种开源LLM的利用，并突出了它们在医疗保健应用中的重要性。此外，我们介绍了用于评估生物医学领域中LLM的基本性能指标，从而阐明了它们的有效性和局限性。最后，我们总结了医疗保健部门中大型语言模型所面临的巨大挑战和限制，从而对其潜在的利益和缺点提供了整体观点。这篇综述提供了对医疗保健中LLM的当前格局的全面探索，以解决它们在改造医疗应用中的作用以及需要进一步研究和开发的领域。

### WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge 
[[arxiv](https://arxiv.org/abs/2401.06659)] [[cool](https://papers.cool/arxiv/2401.06659)] [[pdf](https://arxiv.org/pdf/2401.06659)]
> **Authors**: Wenbin Wang,Liang Ding,Li Shen,Yong Luo,Han Hu,Dacheng Tao
> **First submission**: 2024-01-12
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 智慧：通过融合上下文世界知识来改善多模式分析
- **领域**: 计算语言学
- **摘要**: 情感分析通过利用各种数据模式（例如文本，图像）迅速发展。但是，大多数以前的作品都依赖于表面信息，忽略了上下文世界知识的结合（例如，从给定的图像和文本对衍生出的背景信息），从而限制了他们实现更好的多峰情感分析（MSA）的能力。在本文中，我们提出了一个名为Wisdom的插入式框架，以利用从大型视觉语言模型（LVLM）引起的上下文世界知识来增强MSA。智慧利用LVLM全面分析图像和相应的文本，同时产生相关的上下文。为了减少上下文中的噪声，我们还引入了无训练的上下文融合机制。 MSA任务的各种粒度的实验始终表明，我们的方法在几种最新方法中具有实质性改进（在五种高级方法中平均 +1.96％的F1得分）。

### BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph Representation Pretraining 
[[arxiv](https://arxiv.org/abs/2401.06443)] [[cool](https://papers.cool/arxiv/2401.06443)] [[pdf](https://arxiv.org/pdf/2401.06443)]
> **Authors**: Minjun Kim,Seungwoo Song,Youhan Lee,Haneol Jang,Kyungtae Lim
> **First submission**: 2024-01-12
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: BOK-VQA：双语外部基于知识的视觉问题通过图表进行预处理回答
- **领域**: 计算语言学
- **摘要**: 诸如最近开发的GPT4之类的生成模型中的当前研究方向旨在为多模式和多语言输入找到相关的知识信息，以提供答案。在这些研究情况下，对视觉问题回答（VQA）任务的多语言评估的需求（一项代表性的多模式系统任务）已增加。因此，我们在这项研究中提出了双语外部知识VQA（BOK-VQA）数据集，可以扩展到多语言。拟议的数据包括17K图像，韩语和英语的17K问答对以及与问答内容有关的知识信息实例。我们还提出了一个框架，可以通过以图形嵌入的形式为BOK-VQA数据的知识信息进行预处理信息，从而有效地将知识信息注入VQA系统。最后，通过深入分析，我们证明了对VQA的构建培训数据中包含的知识信息的实际效果。

### Cascaded Cross-Modal Transformer for Audio-Textual Classification 
[[arxiv](https://arxiv.org/abs/2401.07575)] [[cool](https://papers.cool/arxiv/2401.07575)] [[pdf](https://arxiv.org/pdf/2401.07575)]
> **Authors**: Nicolae-Catalin Ristea,Andrei Anghel,Radu Tudor Ionescu
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: Accepted for publication in Artificial Intelligence Review
- **标题**: 级联的跨模式变压器用于音频文本分类
- **领域**: 计算语言学,机器学习,声音,音频和语音处理
- **摘要**: 语音分类任务通常需要强大的语言理解模型来掌握有用的功能，当有限的培训数据可用时，这会变得有问题。为了达到卓越的分类性能，我们建议通过使用自动语音识别（ASR）模型转录语音并通过预审预测的翻译模型转换到不同语言的语音来利用多模式表示的固有价值。因此，我们为每个数据样本获得了一个音频文本（多模式）表示。随后，我们通过新型级联的跨模式变压器（CCMT）结合了来自变压器（BERT）的语言特异性双向编码器表示。我们的模型基于两个级联变压器块。第一个结合了不同语言的特定文本特征，而第二个语言将声学特征与先前由第一个变压器块学到的多语言特征结合在一起。我们在ACM多媒体2023计算副语言学挑战的请求中采用了系统。 CCMT被宣布为获胜解决方案，分别获得投诉和请求检测的未加权平均召回（UAR）分别为65.41％和85.87％。此外，我们在语音命令V2和HarpervalleyBank对话框数据集上应用了框架，超过了以前的研究报告这些基准的研究结果。我们的代码可免费下载：https：//github.com/ristea/ccmt。

### Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering 
[[arxiv](https://arxiv.org/abs/2401.07510)] [[cool](https://papers.cool/arxiv/2401.07510)] [[pdf](https://arxiv.org/pdf/2401.07510)]
> **Authors**: Qing Li,Lei Li,Yu Li
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: 50 pages, 3 figures, 3 tables
- **标题**: 开发生物学和医学的Chatgpt：对生物医学问题的完整回答
- **领域**: 计算语言学,人工智能
- **摘要**: Chatgpt在提供医学诊断，治疗建议和其他医疗保健支持时探讨了问题回答（QA）的战略性蓝图。这是通过通过自然语言处理（NLP）和多模式范式增加医疗领域数据来实现的。通过将文本，图像，视频和其他模式的分布从通用域的分布到医疗领域，这些技术加快了医疗领域问题答案的进度（MDQA）。他们弥合人类自然语言与复杂的医学领域知识或专家手动注释之间的差距，在医学环境中处理大规模，多样，不平衡甚至未标记的数据分析方案。我们重点的核心是利用语言模型和多模式范式来回答医学问题，旨在指导研究社区为其特定的医学研究要求选择适当的机制。详细讨论了专门的任务，例如与非模态相关的问题回答，阅读理解，推理，诊断，关系提取，概率建模等以及其他与多模式相关的任务，例如视觉问题答案，图像字幕，跨模式检索，报告，报告摘要和一代。每个部分都深入研究所考虑的各个方法的复杂细节。本文强调了针对一般域方法的医疗领域探索的结构和进步，从而在不同的任务和数据集中强调了它们的应用。它还概述了未来医疗领域研究的当前挑战和机会，为在这个迅速发展的领域中持续创新和应用铺平了道路。

### Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System 
[[arxiv](https://arxiv.org/abs/2401.09150)] [[cool](https://papers.cool/arxiv/2401.09150)] [[pdf](https://arxiv.org/pdf/2401.09150)]
> **Authors**: Feng Jiang,Kuang Wang,Haizhou Li
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 桥接研究和读者：多模式自动化学术论文解释系统
- **领域**: 计算语言学
- **摘要**: 在当代信息时代，大型语言模型的出现大大加速了，科学文献的扩散正达到前所未有的水平。研究人员迫切需要有效的工具来阅读和总结学术论文，揭示大量的科学文献以及采用多种解释性方法。为了满足这种新兴的需求，自动化科学文献解释系统的作用变得至关重要。但是，商业和开源的盛行模型都面临着鲜明的挑战：它们经常忽略多模式数据，努力求助于汇总长篇文本，并且缺乏多样化的用户界面。作为回应，我们引入了一个开源多模式自动化学术论文解释系统（MMAPIS），具有三步过程阶段，并结合了LLMS以增强其功能。我们的系统首先采用混合模式预处理和对齐模块来提取纯文本，并分别从文档中提取表或数字。然后，它根据其属于的部分对这些信息对齐，以确保具有相同部分名称的数据在同一部分中分类。在此之后，我们介绍了一种层次的话语意识到的摘要方法。它利用提取的部分名称将文章划分为较短的文本段，从而通过LLM在LLMS内部和各节之间促进了具有特定提示的特定摘要。最后，我们设计了四种多样化的用户界面，包括纸张建议，多模式Q \＆A，音频广播和解释博客，可以在各种情况下广泛应用。我们的定性和定量评估强调了系统的优势，尤其是在科学摘要中，它超过了仅依赖GPT-4的解决方案。

### Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations 
[[arxiv](https://arxiv.org/abs/2401.09899)] [[cool](https://papers.cool/arxiv/2401.09899)] [[pdf](https://arxiv.org/pdf/2401.09899)]
> **Authors**: Prince Jha,Krishanu Maity,Raghav Jain,Apoorv Verma,Sriparna Saha,Pushpak Bhattacharyya
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: EACL2024
- **标题**: 模因分析：通过多模式解释增强对模因中网络欺凌的理解
- **领域**: 计算语言学
- **摘要**: 互联网模因在交流政治，心理和社会文化思想方面具有重大影响。虽然模因通常很幽默，但使用模因用于拖钓和网络欺凌行为。尽管已经开发出各种有效的基于深度学习的模型来检测进攻性多模式模因，但在解释性方面只做了几项工作。诸如通用数据保护法规的“解释权”之类的最新法律刺激了研究可解释模型的研究，而不仅仅是关注绩效。在此激励的情况下，我们介绍了{\ em MultiBully-ex}，这是第一个用于从Code-Mixed网络欺凌模因的多模式解释的基准数据集。在这里，强调视觉和文本方式，以解释为什么给定的模因是网络欺凌的原因。已经提出了一种基于对比的语言图像预处理（剪辑）基于投影的多模式共享私有多任务方法，用于视觉和文本解释模因。实验结果表明，具有多模式解释的培训可以提高产生文本理由的绩效，并更准确地确定可靠的绩效改进决策的视觉证据。

### InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance 
[[arxiv](https://arxiv.org/abs/2401.11206)] [[cool](https://papers.cool/arxiv/2401.11206)] [[pdf](https://arxiv.org/pdf/2401.11206)]
> **Authors**: Pengyu Wang,Dong Zhang,Linyang Li,Chenkun Tan,Xinghao Wang,Ke Ren,Botian Jiang,Xipeng Qiu
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 地铁：通过跨模型指导无害的推理时间对齐
- **领域**: 计算语言学
- **摘要**: 随着大型语言模型（LLM）的快速发展，它们不仅被用作通用AI助手，而且还可以通过进一步的微调来定制以满足不同应用程序的要求。当前LLMS成功的关键因素是对齐过程。当前的一致性方法，例如监督的微调（SFT）和从人类反馈（RLHF）中学习，重点是训练时间对齐，并且通常很复杂且笨拙地实施。因此，我们开发\ textbf {unuperAligner}，这是一种新型的推理时间比对方法，利用跨模型指南进行无害的对准。 EustroLigner利用从安全对准模型中提取的安全转向向量在响应有害输入时修改目标模型的激活，从而指导目标模型以提供无害的响应。实验结果表明，我们的方法可以非常有效地应用于金融，医学和数学的领域特异性模型，以及多模式大语模型（MLLMS）（例如Llava）。它大大降低了有害指令和越狱攻击的攻击成功率（ASR），同时保持下游任务几乎不变的表现。

### Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech Detection 
[[arxiv](https://arxiv.org/abs/2401.10653)] [[cool](https://papers.cool/arxiv/2401.10653)] [[pdf](https://arxiv.org/pdf/2401.10653)]
> **Authors**: Atanu Mandal,Gargi Roy,Amit Barman,Indranil Dutta,Sudip Kumar Naskar
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: Accepted in 20th International Conference on Natural Language Processing (ICON)
- **标题**: 细心融合：一种基于变压器的多模式仇恨语音检测的方法
- **领域**: 计算语言学,机器学习,声音,音频和语音处理,信号处理
- **摘要**: 随着社交媒体使用的最近激增和指数增长，对任何可恶内容的存在审查社交媒体内容至关重要。自从过去十年以来，研究人员就一直在努力工作，以区分促进仇恨和没有的内容的内容。传统上，主要重点是分析文本内容。但是，最近的研究尝试也开始识别基于音频的内容。然而，研究表明，仅依靠音频或基于文本的内容可能是无效的，因为最近的高潮表明，个人经常在语音和写作中使用讽刺。为了克服这些挑战，我们提出了一种方法来确定演讲是否促进仇恨同时利用音频和文字表示。我们的方法基于结合音频和文本采样的变压器框架，并伴随着我们自己的层，称为“ Actentive Fusion”。我们的研究结果超过了先前的最新技术，在测试集中，令人印象深刻的宏观F1得分为0.927。

### The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.12117)] [[cool](https://papers.cool/arxiv/2401.12117)] [[pdf](https://arxiv.org/pdf/2401.12117)]
> **Authors**: Kian Ahrabian,Zhivar Sourati,Kexuan Sun,Jiarui Zhang,Yifan Jiang,Fred Morstatter,Jay Pujara
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: 21 pages
- **标题**: 具有多模式大语言模型的非语言抽象推理的奇怪案例
- **领域**: 计算语言学
- **摘要**: 尽管大型语言模型（LLM）仍在新领域中采用并用于新颖的应用中，但我们正在经历新一代基础模型的涌入，即多模式大型语言模型（MLLMS）。这些模型集成了口头和视觉信息，开辟了新的可能性，以在两种方式的交集中证明更复杂的推理能力。但是，尽管MLLM的前景革命性革命性，但我们对他们的推理能力的理解是有限的。在这项研究中，我们使用Raven的渐进式矩阵来评估开源和封闭源MLLM的非语言抽象推理能力。我们的实验揭示了MLLM的此类问题的挑战性质，同时展示了开源模型和封闭源模型之间的巨大差距。我们还发现了视觉和文本感知的批判性缺点，使模型呈现出低绩效的天花板。最后，为了提高MLLM的性能，我们试验了不同的方法，例如经过思考链的提示，导致绩效的显着增强（高达100％）。我们的代码和数据集可在https://github.com/usc-isi-i2/isi-mmlm-rpm上找到。

### CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark 
[[arxiv](https://arxiv.org/abs/2401.11944)] [[cool](https://papers.cool/arxiv/2401.11944)] [[pdf](https://arxiv.org/pdf/2401.11944)]
> **Authors**: Ge Zhang,Xinrun Du,Bei Chen,Yiming Liang,Tongxu Luo,Tianyu Zheng,Kang Zhu,Yuyang Cheng,Chunpu Xu,Shuyue Guo,Haoran Zhang,Xingwei Qu,Junjie Wang,Ruibin Yuan,Yizhi Li,Zekun Wang,Yudong Liu,Yu-Hsuan Tsai,Fengji Zhang,Chenghua Lin,Wenhao Huang,Jie Fu
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: CMMMU：中国大型多学科多模式理解基准
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 随着大型多模型（LMM）的能力继续推进，评估LMM的性能是随着需求的增加而出现的。此外，在评估LMM的高级知识和推理能力（例如中文）中，还有更大的差距。我们介绍了CMMMU，这是一种新的中国大规模多学科多模式理解基准，旨在评估LMM的要求，要求在中文背景下进行大学级学科知识和故意推理。 CMMMU灵感来自MMMU的注释和分析模式。 CMMMU包括从大学考试，测验和教科书中手动收集的多模式问题，涵盖了六个核心学科：艺术与设计，商业，科学，科学，健康与医学，人文与人文科学和技术与工程，以及其同伴，MMMU。这些问题涵盖了30个主题，并包括39种高度异构图像类型，例如图表，图表，地图，表格，音乐表和化学结构。 CMMMU专注于在中国背景下具有特定于领域的知识的复杂感知和推理。我们评估了11个开源LLM和1辆专有GPT-4V（ISION）。即使是GPT-4V也只能达到42％的精度，这表明有很大的改进空间。 CMMMU将促进社区来建立下一代LMM，以实现人工智能专家，并通过提供多种语言环境来促进LMM的民主化。

### TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation 
[[arxiv](https://arxiv.org/abs/2401.12987)] [[cool](https://papers.cool/arxiv/2401.12987)] [[pdf](https://arxiv.org/pdf/2401.12987)]
> **Authors**: Taeyang Yun,Hyunkuk Lim,Jeonghwan Lee,Min Song
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-24
> **comment**: NAACL 2024 main conference
- **标题**: Telme：在对话中识别情感识别的教师领导的多模式融合网络
- **领域**: 计算语言学,机器学习,声音,音频和语音处理
- **摘要**: 对话中的情感识别（ERC）在使对话系统有效响应用户请求方面起着至关重要的作用。对话中的情绪可以通过各种方式（例如音频，视觉和文字）的表示来识别。但是，由于非语言方式对识别情绪的贡献较弱，因此多模式ERC始终被认为是一项具有挑战性的任务。在本文中，我们建议用于ERC（Telme）的教师领导的多模式融合网络。 Telme结合了跨模式知识蒸馏，以将信息从作为教师的语言模型转移到非语言学生，从而优化弱模态的功效。然后，我们使用转变的融合方法将多模式功能组合在一起，学生网络在其中支持教师。 Telme在MELD中实现了最先进的性能，这是一个用于ERC的多演讲者对话数据集。最后，我们通过其他实验证明了组件的有效性。

### KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning 
[[arxiv](https://arxiv.org/abs/2401.12863)] [[cool](https://papers.cool/arxiv/2401.12863)] [[pdf](https://arxiv.org/pdf/2401.12863)]
> **Authors**: Debjyoti Mondal,Suraj Modi,Subhadarshi Panda,Rituraj Singh,Godawari Sudhakar Rao
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: AAAI 2024
- **标题**: KAM-COT：知识增强多模式的思想链推理
- **领域**: 计算语言学,人工智能
- **摘要**: 大型语言模型（LLM）通过利用思想链（COT）来表现出令人印象深刻的自然语言处理任务表现，从而实现了逐步思考。使用多模式功能扩展LLM是最近的兴趣，但会产生计算成本，需要大量的硬件资源。为了应对这些挑战，我们提出了一个集成COT推理，知识图（KGS）和多种方式的框架，以全面了解多模式任务。 KAM-COT通过KG接地进行了两阶段的培训过程，以产生有效的理由和答案。通过在推理过程中纳入KGS的外部知识，该模型获得了更深层次的上下文理解，以减少幻觉并提高答案的质量。这种知识增强的COT推理使该模型有能力处理需要外部环境的问题，从而提供了更明智的答案。实验发现表明，KAM-COT的表现优于最先进的方法。在ScienceQA数据集上，我们的平均准确度达到93.87％，超过GPT-3.5（75.17％）的平均准确性，达到18％，GPT-4（83.99％）的平均准确度达到10％。值得注意的是，KAM-COT一次只能使用280m可训练的参数来实现这些结果，从而证明了其成本效率和有效性。

### WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2401.13919)] [[cool](https://papers.cool/arxiv/2401.13919)] [[pdf](https://arxiv.org/pdf/2401.13919)]
> **Authors**: Hongliang He,Wenlin Yao,Kaixin Ma,Wenhao Yu,Yong Dai,Hongming Zhang,Zhenzhong Lan,Dong Yu
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: Accepted to ACL 2024 (main). Code and data is released at https://github.com/MinorJerry/WebVoyager
- **标题**: WebVoyager：使用大型多模型建立端到端的Web代理
- **领域**: 计算语言学,人工智能
- **摘要**: 大型语言模型（LLMS）的快速发展导致了一个新时代，其标志是在现实世界中的自主应用程序的开发，这推动了创建高级网络代理商的创新。现有的Web代理通常仅处理一个输入模式，仅在简化的Web模拟器或静态Web快照中进行评估，从而在现实世界中极大地限制了它们的适用性。为了弥合这一差距，我们介绍了WebVoyager，这是一种创新的大型多模型模型（LMM）供电的Web代理，可以通过与现实世界中的网站进行交互来完成用户指令。此外，我们通过从15个受欢迎的网站中编译现实世界任务来建立新的基准测试，并引入一个自动评估协议，利用GPT-4V的多模式理解能力来评估开放式网络代理。我们表明，WebVoyager在我们的基准测试上达到了59.1％的任务成功率，大大超过了GPT-4（所有工具）和WebVoyager（仅文本）设置的性能，从而强调了WebVoyager的非凡能力。拟议的自动评估指标与人类判断力达成了85.3％的一致性，表明其在提供可靠和准确评估网络代理方面的有效性。

### MM-LLMs: Recent Advances in MultiModal Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.13601)] [[cool](https://papers.cool/arxiv/2401.13601)] [[pdf](https://arxiv.org/pdf/2401.13601)]
> **Authors**: Duzhen Zhang,Yahan Yu,Jiahua Dong,Chenxing Li,Dan Su,Chenhui Chu,Dong Yu
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: Accepted by ACL2024 (findings)
- **标题**: MM-llms：多模式大语言模型的最新进展
- **领域**: 计算语言学
- **摘要**: 在过去的一年中，多模式的大语言模型（MM-llms）经历了重大进步，通过成本效益的培训策略增强了现成的LLMS，以支持MM输入或输出。最终的模型不仅保留了LLMS的固有推理和决策能力，还可以增强各种MM任务。在本文中，我们提供了一项旨在促进MM-LLLS研究的综合调查。最初，我们概述了模型架构和培训管道的一般设计公式。随后，我们引入了一个涵盖126 mM-llms的分类学，每个分类法的特征是其特定的配方。此外，我们回顾了主流基准中选定的MM-LLM的性能，并总结了关键培训配方以增强MM-LLLM的效力。最后，我们探索了MM-LLM的有希望的方向，同时同时维护该领域最新开发的实时跟踪网站。我们希望这项调查有助于MM-llms域的持续发展。

### Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.13298)] [[cool](https://papers.cool/arxiv/2401.13298)] [[pdf](https://arxiv.org/pdf/2401.13298)]
> **Authors**: Hongzhan Lin,Ziyang Luo,Wei Gao,Jing Ma,Bo Wang,Ruichao Yang
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: The first work towards explainable harmful meme detection by harnessing advanced LLMs
- **标题**: 通过大语模型之间的多模式辩论来解释有害模因检测
- **领域**: 计算语言学,人工智能
- **摘要**: 社交媒体的时代充斥着互联网模因，需要清晰地掌握有害的模因。由于模因中嵌入的隐式含义，该任务提出了一个重大挑战，而模因中没有明确传达通过表面文本和图像。但是，现有的有害模因检测方法没有提出可读的解释，这些解释揭示了这种隐含的含义以支持其检测决策。在本文中，我们提出了一种可解释的方法来检测有害模因，这是通过对无害和有害立场的矛盾原理进行推理而实现的。具体而言，受到大语模型（LLM）在文本生成和推理方面的强大能力的启发，我们首先引起了LLMS之间的多模式辩论，以生成从矛盾的参数中得出的解释。然后，我们建议将一个小语言模型作为有害性推断的辩论法官进行微调，以促进有害性理由与模因中内在的多模式信息之间的多模式融合。这样，我们的模型有权利用源自无害和有害论点的多模式解释，对复杂和隐性的危害指导模式进行辩证推理。在三个公共模因数据集上进行的广泛实验表明，我们的有害模因检测方法的性能要比最先进的方法好得多，并且表现出较高的能力来解释模型预测的模因危害性。

### MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, Asr Error Detection, and Asr Error Correction 
[[arxiv](https://arxiv.org/abs/2401.13260)] [[cool](https://papers.cool/arxiv/2401.13260)] [[pdf](https://arxiv.org/pdf/2401.13260)]
> **Authors**: Jiajun He,Xiaohan Shi,Xingfeng Li,Tomoki Toda
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: Accepted by ICASSP 2024
- **标题**: MF-AED-AEC：通过利用多模式融合，ASR错误检测和ASR误差校正来识别语音情绪
- **领域**: 计算语言学,多媒体,声音,音频和语音处理
- **摘要**: 语音情感识别（SER）中普遍的方法涉及整合音频和文本信息，以全面地识别说话者的情感，而文本通常通过自动语音识别（ASR）获得。这种方法的一个基本问题是，文本模式的ASR错误可能会使SER的性能恶化。先前的研究提出了使用辅助ASR误差检测任务来适应ASR假设中每个单词的权重。但是，这种方法具有有限的改进潜力，因为它不能解决文本中语义信息的连贯性。此外，不同方式的固有异质性会导致其表示之间的分布差距，从而使其融合具有挑战性。因此，在本文中，我们结合了两个辅助任务，即ASR误差检测（AED）和ASR误差校正（AEC），以增强ASR文本的语义连贯性，并进一步介绍一种新型的多模式融合（MF）方法，以学习跨模态表示共享表示。我们将我们的方法称为MF-AED-AEC。实验结果表明，MF-AED-AEC显着优于基线模型的余量为4.1 \％。

### CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning 
[[arxiv](https://arxiv.org/abs/2401.14011)] [[cool](https://papers.cool/arxiv/2401.14011)] [[pdf](https://arxiv.org/pdf/2401.14011)]
> **Authors**: Zheqi He,Xinya Wu,Pengfei Zhou,Richeng Xuan,Guang Liu,Xi Yang,Qiannan Zhu,Hua Huang
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: No comments
- **标题**: CMMU：中文多模式多类型问题的基准理解和推理
- **领域**: 计算语言学,人工智能,多媒体
- **摘要**: 多模式的大语言模型（MLLM）取得了显着的进步，并表现出强大的知识理解和推理能力。但是，掌握特定领域的知识，这对于评估MLLM的智能至关重要，这仍然是一个挑战。当前针对领域特定知识的多模式基准集中在多项选择问题上，并且主要以英语提供，这对评估的全面性施加了限制。为此，我们介绍了CMMU，这是一种新颖的基准，用于中文中多模式和多类型问题的理解和推理。 CMMU由7个主题中的3,603个问题组成，涵盖了小学到高中的知识。这些问题可以分为三种类型：多项选择，多响应和填空，为MLLM带来了更大的挑战。此外，我们提出了一种评估策略，称为位置误差差异，用于评估多项选择问题。该策略旨在对位置偏差进行定量分析。我们与GPT4-V，Gemini-Pro和Qwen-Vl-Plus一起评估了七个开源MLLM。结果表明，CMMU对最近的MLLM构成了重大挑战。数据和代码可在https://github.com/flagopen/cmmu上找到。

### LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents 
[[arxiv](https://arxiv.org/abs/2401.15050)] [[cool](https://papers.cool/arxiv/2401.15050)] [[pdf](https://arxiv.org/pdf/2401.15050)]
> **Authors**: Ahmed Masry,Amir Hajian
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: Accepted at AAAI 2024 Workshop on AI in Finance for Social Impact
- **标题**: Longfin：长期金融领域文档的多模式文档理解模型
- **领域**: 计算语言学
- **摘要**: Document AI是一个不断增长的研究领域，该领域侧重于从扫描和数字文档中的信息理解和提取，以使日常业务运营效率更高。已经引入了许多下游任务和数据集，以促进能够解析和从各种文档类型（例如收据和扫描表格）中解析和提取信息的AI模型的培训。尽管取得了这些进步，但现有的数据集和模型都无法解决工业环境中出现的关键挑战。现有数据集主要包含由单个页面组成的简短文档，而现有模型则受到有限的最大长度约束，通常设置为512令牌。因此，这些方法在金融服务中的实际应用（文档可以跨越多个页面）受到严重阻碍。为了克服这些挑战，我们介绍了Longfin，这是一种多模式文档AI模型，能够编码多达4K令牌。我们还提出了Longforms数据集，该数据集是一个综合的财务数据集，封装了财务文件中的几个工业挑战。通过广泛的评估，我们证明了Longfin模型对Longforms数据集的有效性，超过了现有公共模型的性能，同时保持了现有单页基准的可比结果。

### Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion 
[[arxiv](https://arxiv.org/abs/2401.14717)] [[cool](https://papers.cool/arxiv/2401.14717)] [[pdf](https://arxiv.org/pdf/2401.14717)]
> **Authors**: Jinhan Wang,Long Chen,Aparna Khare,Anirudh Raju,Pranav Dheram,Di He,Minhua Wu,Andreas Stolcke,Venkatesh Ravichandran
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: To appear in IEEE ICASSP 2024
- **标题**: 用声学和大语言模型融合的转弯和回渠预测
- **领域**: 计算语言学,人工智能,机器学习,声音,音频和语音处理
- **摘要**: 我们通过将神经声学模型与大语言模型（LLM）融合在一起，提出了一种在口语对话中连续预测转弯和回音位置的方法。总机人类对话数据集上的实验表明，我们的方法始终以单个方式优于基线模型。我们还制定了一种新颖的多任务指令微调策略，以进一步从LLM编码的知识中受益，以了解任务和对话环境，从而实现其他改进。我们的方法证明了LLM和声学模型相结合的潜力，以使人与言语增强的AI代理之间的自然和对话相互作用。

### Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support 
[[arxiv](https://arxiv.org/abs/2401.14688)] [[cool](https://papers.cool/arxiv/2401.14688)] [[pdf](https://arxiv.org/pdf/2401.14688)]
> **Authors**: Xiaojun Wu,Dixiang Zhang,Ruyi Gan,Junyu Lu,Ziwei Wu,Renliang Sun,Jiaxing Zhang,Pingjian Zhang,Yan Song
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: Taiyi-Diffusion-XL Tech Report
- **标题**: TAIYI-DIFFUSION-XL：通过大型视觉模型支持提前双语文本到图像生成
- **领域**: 计算语言学
- **摘要**: 文本到图像模型的最新进展具有显着增强的图像产生能力，但在双语或中文支持中，开源模型的显着差距仍然存在。为了满足这一需求，我们提出了Taiyi-Diffusion-XL，这是一种新的中文和英语双语文本对图像模型，它是通过通过双语连续预训练的过程来扩展剪辑和稳定 - 扩散-XL的能力来开发的。这种方法包括通过将最常用的汉字集成到剪辑的令牌和嵌入图层中，并与编码扩展的绝对位置相结合，将词汇的有效扩展。此外，我们通过大型视觉语言模型来丰富文本提示，从而提供更好的图像标题并具有更高的视觉质量。这些增强功能随后应用于下游文本对图像模型。我们的经验结果表明，开发的剪辑模型在双语图像文本检索中出色。曲线，taiyi-diffusion-xl的双语图像生成能力超过了先前的模型。这项研究导致了Taiyi-Diffusion-XL模型的开发和开源，这代表了图像生成领域的显着进步，尤其是对于中文应用程序。这项贡献是解决多模式研究中对更多多样化语言支持的需求的迈出的一步。该模型和演示可在\ href {https://huggingface.co/idea-ccnl/taiyi-stable-diffusion-diffusion-xl-3.5b/}上公开获得，从而促进了该领域的进一步研究与协作。

### Engineering A Large Language Model From Scratch 
[[arxiv](https://arxiv.org/abs/2401.16736)] [[cool](https://papers.cool/arxiv/2401.16736)] [[pdf](https://arxiv.org/pdf/2401.16736)]
> **Authors**: Abiodun Finbarrs Oketunji
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: :I.2.7ACM Class:I.2.7
- **标题**: 从头开始工程大型语言模型
- **领域**: 计算语言学,计算机与社会,机器学习,软件工程
- **摘要**: 自然语言加工中深度学习的扩散（NLP）导致了能够以熟练程度理解和生成人类语言的创新技术的发展和释放。基于变压器的神经网络Atinuke通过使用唯一的配置来优化各种语言任务的性能。该体系结构交织在一起，用于处理顺序数据与注意机制，以在输入和输出之间绘制有意义的亲和力。由于其拓扑和超参数调整的配置，它可以通过提取功能和学习复杂的映射来模仿类似人类的语言。 Atinuke是模块化的，可扩展的，并与现有的机器学习管道无缝集成。高级矩阵操作（例如SoftMax，Embeddings和Multi-Head注意力）可以使文本，声学和视觉信号的细微差别处理。通过将现代深度学习技术与软件设计原理和数学理论统一，该系统实现了最新的自然语言任务结果，同时保持了可解释和稳健的态度。

### Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models 
[[arxiv](https://arxiv.org/abs/2401.16727)] [[cool](https://papers.cool/arxiv/2401.16727)] [[pdf](https://arxiv.org/pdf/2401.16727)]
> **Authors**: Ming Shan Hee,Shivam Sharma,Rui Cao,Palash Nandi,Preslav Nakov,Tanmoy Chakraborty,Roy Ka-Wei Lee
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: Accepted at EMNLP'24 (Findings)
- **标题**: 仇恨言语节奏的最新进展：多模式和大型模型的作用
- **领域**: 计算语言学
- **摘要**: 在在线交流的不断发展的景观中，主持仇恨言论（HS）提出了一个复杂的挑战，这是数字内容的多模式性质的复杂化。这项全面的调查深入研究了HS节制的最新进展，焦点大型语言模型（LLMS）和大型多模型模型（LMMS）的蓬勃发展。我们的探索始于对当前文献的彻底分析，揭示了传播HS的文本，视觉和听觉元素之间的细微相互作用。我们发现了整合这些方式的显着趋势，这主要是由于HS传播的复杂性和微妙性。 LLMS和LMMS促进的进步非常重视，这些进步已开始重新定义检测和适度能力的界限。我们确定了研究中的现有差距，尤其是在代表性不足的语言和文化的背景下，以及解决低资源环境的解决方案的需求。该调查以前瞻性的观点结束，概述了未来研究的潜在途径，包括探索新型AI方法论，适度的AI的道德治理以及更加细微的，更细微的背景感知系统的发展。这一全面的概述旨在促进进一步的研究，并促进与数字时代更为复杂，负责任和以人为中心的HS节制方法的合作努力。警告：本文包含令人反感的例子。

### Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking 
[[arxiv](https://arxiv.org/abs/2401.16575)] [[cool](https://papers.cool/arxiv/2401.16575)] [[pdf](https://arxiv.org/pdf/2401.16575)]
> **Authors**: Ivana Beňová,Jana Košecká,Michal Gregor,Martin Tamajka,Marcel Veselý,Marián Šimko
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: 9 pages of text, 11 pages total, 7 figures, 3 tables, preprint
- **标题**: 超越图像文本匹配：使用引导掩蔽在多模式变压器中的动词理解
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 主要的探测方法依赖于图像文本匹配任务的零拍性能，以获得对最近的多模式图像 - 语言变压器模型所学的表示形式的更细粒度的理解。评估是在精心策划的数据集上进行的，该数据集专注于计数，关系，属性和其他人。这项工作介绍了一种称为指导掩蔽的替代探测策略。所提出的方法使用掩蔽剂消融了不同的方式，并评估了模型以高精度预测掩盖词的能力。我们专注于研究考虑对象检测器获得的感兴趣区域（ROI）特征的多模型模型。我们使用Vilbert，LXMERT，UNITER和Visualbert上的指导掩蔽对动词的理解进行探测，并表明这些模型可以高精度预测正确的动词。这与先前的结论形成鲜明对比的是，在需要动词理解的情况下经常失败的图像文本匹配探测技术。所有实验的代码将公开可用https://github.com/ivana-13/guided_masking。

### Towards Red Teaming in Multimodal and Multilingual Translation 
[[arxiv](https://arxiv.org/abs/2401.16247)] [[cool](https://papers.cool/arxiv/2401.16247)] [[pdf](https://arxiv.org/pdf/2401.16247)]
> **Authors**: Christophe Ropers,David Dale,Prangthip Hansanti,Gabriel Mejia Gonzalez,Ivan Evtimov,Corinne Wong,Christophe Touret,Kristina Pereyra,Seohyun Sonia Kim,Cristian Canton Ferrer,Pierre Andrews,Marta R. Costa-jussà
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: arXiv admin note: substantial text overlap with arXiv:2312.05187
- **标题**: 迈向多模式和多语言翻译的红色团队
- **领域**: 计算语言学,计算机与社会
- **摘要**: 评估自然语言处理的表现变得越来越复杂。一个特别的挑战是评估数据集与直接或间接的培训数据重叠的潜力，这可能导致偏斜的结果并高估模型性能。结果，人类评估正在增加兴趣，以评估模型的性能和可靠性。一种方法是红色小组方法，该方法旨在生成模型会产生关键错误的边缘案例。尽管该方法已成为生成AI的标准实践，但其在条件AI领域的应用仍未得到探索。本文介绍了第一项关于基于人类的机器翻译（MT）的红色小组（MT）的研究，这标志着朝着理解和改善翻译模型的性能迈出的重要一步。我们研究了基于人类的红色团队和关于自动化的研究，报告了经验教训，并为翻译模型和红色团队演习提供了建议。这项开创性的工作为MT领域的研发开辟了新的途径。

### Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception 
[[arxiv](https://arxiv.org/abs/2401.16158)] [[cool](https://papers.cool/arxiv/2401.16158)] [[pdf](https://arxiv.org/pdf/2401.16158)]
> **Authors**: Junyang Wang,Haiyang Xu,Jiabo Ye,Ming Yan,Weizhou Shen,Ji Zhang,Fei Huang,Jitao Sang
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: Accepted by ICLR 2024 Workshop in Large Language Model (LLM) Agents
- **标题**: 移动设备：具有视觉感知的自动多模式移动设备代理
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 基于多模式大语言模型（MLLM）的移动设备代理正在成为流行的应用程序。在本文中，我们介绍了一种自动多模式移动设备代理Mobile-Agent。移动代理首先利用视觉感知工具准确地识别和定位应用程序前端接口中的视觉和文本元素。基于感知到的视觉上下文，它自主计划并分解复杂的操作任务，并通过操作逐步导航移动应用程序。与以前依赖于应用程序或移动系统元数据的XML文件的解决方案不同，移动设备可以以以视力为中心的方式在不同的移动操作环境之间进行更大的适应性，从而消除了对系统特定自定义的必要性。为了评估移动设备的性能，我们引入了移动设备，这是用于评估移动设备操作的基准。基于移动评估，我们对移动设备进行了全面的评估。实验结果表明，移动机构达到了显着的准确性和完成率。即使有具有挑战性的说明，例如多应用程序，移动设备仍然可以满足要求。代码和型号将在https://github.com/x-plug/mobileagent上开源。

### LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation 
[[arxiv](https://arxiv.org/abs/2401.17244)] [[cool](https://papers.cool/arxiv/2401.17244)] [[pdf](https://arxiv.org/pdf/2401.17244)]
> **Authors**: Yuan Chiang,Elvis Hsieh,Chia-Hong Chou,Janosh Riebesell
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: 32 pages, 5 figures
- **标题**: LLAMP：大型语言模型为高保真材料知识知识检索和蒸馏而实现
- **领域**: 计算语言学,材料科学,人工智能
- **摘要**: 在可靠性和可重复性至关重要的情况下，必须在科学中使用大型语言模型（LLM）的幻觉。但是，LLM固有地缺乏长期的记忆，使其成为不平凡，临时且不可避免地有偏见的任务，可以在领域特定的文献和数据上微调它们。在这里，我们介绍了LLAMP，这是一种多式模式检索生成（RAG）层次推理和作用（REACT）试剂的框架，该框架可以通过高通量工作流界面动态地与材料项目（MP）的计算和实验数据进行动态和递归相互作用。在没有微调的情况下，LLAMP展示了强大的工具使用能力，可以理解和整合材料科学概念的各种方式，即时获取相关的数据存储，过程高阶数据（例如晶体结构和弹性张量）以及简化计算材料和化学中的复杂任务。我们提出了一个简单的度量，该指标结合了不确定性和置信度估计，以评估LLAMP和Vanilla LLMS的反应自谐度。我们的基准表明，LLAMP有效地减轻了LLMS的固有偏置，抵消了似乎来自混合数据源的散装模量，电子带盖和地层的错误。我们还展示了LLAMP使用预训练的机器学习力场编辑晶体结构并运行退火分子动力学模拟的能力。该框架为探索和扩展材料信息学提供了一种直观且几乎无幻的方法，并为知识蒸馏和微调其他语言模型建立了途径。代码和现场演示可从https://github.com/chiang-yuan/llamp获得

## 密码学和安全(cs.CR:Cryptography and Security)

该领域共有 4 篇论文

### TPatch: A Triggered Physical Adversarial Patch 
[[arxiv](https://arxiv.org/abs/2401.00148)] [[cool](https://papers.cool/arxiv/2401.00148)] [[pdf](https://arxiv.org/pdf/2401.00148)]
> **Authors**: Wenjun Zhu,Xiaoyu Ji,Yushi Cheng,Shibo Zhang,Wenyuan Xu
> **First submission**: 2023-12-30
> **First announcement**: 2024-01-02
> **comment**: Appeared in 32nd USENIX Security Symposium (USENIX Security 23)
- **标题**: TPATCH：触发的物理对抗补丁
- **领域**: 密码学和安全,计算机视觉和模式识别
- **摘要**: 自动驾驶汽车越来越多地利用基于视觉的感知模块来获取有关驾驶环境并检测障碍的信息。正确的检测和分类对于确保安全驾驶决策很重要。现有作品证明了欺骗感知模型的可行性，例如对象探测器和图像分类器，并具有印刷的对抗贴剂。但是，他们中的大多数人对每辆经过的自动驾驶汽车都无与伦比。在本文中，我们提出了TPATCH，这是一种由声学信号触发的物理对抗贴片。与其他对抗斑块不同，TPATCH在正常情况下保持良性，但可以触发通过信号注射攻击对摄像机引入的设计失真引发隐藏，或改变攻击。为了避免怀疑人类驱动因素，并在现实世界中使攻击实用和强大，我们提出了一种基于内容的伪装方法和一种攻击鲁棒性增强方法来加强它。使用三个对象探测器的评估Yolo V3/V5和更快的R-CNN以及八个图像分类器证明了TPATCH在模拟和现实世界中的有效性。我们还讨论了传感器，算法和系统级别的可能防御能力。

### MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance 
[[arxiv](https://arxiv.org/abs/2401.02906)] [[cool](https://papers.cool/arxiv/2401.02906)] [[pdf](https://arxiv.org/pdf/2401.02906)]
> **Authors**: Renjie Pi,Tianyang Han,Jianshu Zhang,Yueqi Xie,Rui Pan,Qing Lian,Hanze Dong,Jipeng Zhang,Tong Zhang
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: MLLM Protector：确保MLLM的安全而不会损害性能
- **领域**: 密码学和安全,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的部署带来了一个独特的漏洞：通过视觉输入对恶意攻击的敏感性。本文调查了捍卫MLLM免受此类攻击的新颖挑战。与大型语言模型（LLM）相比，MLLM包括附加的图像模式。我们发现，图像充当了在安全统一期间未考虑的``外语''，使MLLMS更容易产生有害的响应。不幸的是，与基于文本的LLMS中考虑的离散代币相比，图像的连续性质，图像信号的连续性质表明了重大的一致性挑战，这构成了所有可能的情况。在有限的图像文本上，比广泛的基于文本的预处理要少得多，这使得Mllms更容易忘记灾难性的遗忘，以解决这些挑战，以解决这些挑战。这种方法有效地减轻了恶意视觉输入带来的风险，而不会损害MLLM的原始性能。我们的结果表明，MLLM-Protector为MLLM Security的先前未解决方面提供了强大的解决方案。

### Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning 
[[arxiv](https://arxiv.org/abs/2401.09479)] [[cool](https://papers.cool/arxiv/2401.09479)] [[pdf](https://arxiv.org/pdf/2401.09479)]
> **Authors**: Rahul Vishwakarma,Amin Rezaei
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-18
> **comment**: 2024 Design, Automation and Test in Europe Conference | The European Event for Electronic System Design & Test (accepted)
- **标题**: 使用多模式深度学习的不确定性意识性硬件特洛伊木马检测
- **领域**: 密码学和安全,人工智能,机器学习
- **摘要**: 在零信任的含糊时代，在芯片生产的各个阶段插入硬件木马的风险增加了。为了解决这个问题，已经开发了各种机器学习解决方案，以检测硬件木马。尽管大多数重点都放在统计或深度学习方法上，但木马感染的基准数量有限会影响检测准确性，并限制了检测零日木马的可能性。为了缩小差距，我们首先采用生成对抗网络以两种替代表示模式（图形和表格）放大我们的数据，以确保数据集以代表性的方式分配。此外，我们提出了一种多模式深度学习方法，以检测硬件木马并评估早期融合和晚期融合策略的结果。我们还估算了每个预测的不确定性定量指标，以实现风险意识的决策。结果不仅证实了我们提出的硬件特洛伊木马检测方法的功效，而且还为未来的研究打开了新的大门，该研究采用多模式和不确定性量化来应对其他硬件安全挑战。

### Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to Identify Trajectory Prediction Vulnerabilities for Autonomous Driving Security 
[[arxiv](https://arxiv.org/abs/2401.10313)] [[cool](https://papers.cool/arxiv/2401.10313)] [[pdf](https://arxiv.org/pdf/2401.10313)]
> **Authors**: Marsalis Gibson,David Babazadeh,Claire Tomlin,Shankar Sastry
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: 10 pages, 5 figures, 1 tables
- **标题**: 黑客预测因素意味着黑客攻击：使用灵敏度分析来识别自动驾驶安全性的轨迹预测漏洞
- **领域**: 密码学和安全,机器学习,机器人技术,系统与控制
- **摘要**: 已经证明了对基于学习的多模式轨迹预测变量的对抗性攻击。但是，关于扰动对国家历史以外的投入以及这些攻击如何影响下游计划和控制的影响，仍然存在开放问题。在本文中，我们对两个轨迹预测模型（轨迹++和代理形式）进行了灵敏度分析。分析表明，在所有输入之间，几乎所有对这两个模型的扰动敏感性都仅在最近的位置和速度状态内。我们还表明，尽管对国家历史扰动的敏感性显着性，但使用快速梯度符号方法制成的不可检测的图像图扰动可以在这两种模型中诱导大量预测误差增加，从而表明这些轨迹预测因子实际上对基于图像的攻击敏感。使用基于优化的计划者和示例从灵敏度结果制定的扰动，我们展示了这些攻击如何导致车辆突然停止中等驾驶速度。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 192 篇论文

### Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models 
[[arxiv](https://arxiv.org/abs/2401.00988)] [[cool](https://papers.cool/arxiv/2401.00988)] [[pdf](https://arxiv.org/pdf/2401.00988)]
> **Authors**: Xinpeng Ding,Jinahua Han,Hang Xu,Xiaodan Liang,Wei Zhang,Xiaomeng Li
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: 整体自主驾驶的理解是通过鸟类视图注入的多模式大型模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的兴起激发了人们对基于语言的驾驶任务的兴趣。但是，现有的研究通常集中在有限的任务上，并且经常忽略关键的多视图和时间信息，这对于强大的自主驾驶至关重要。为了弥合这些差距，我们介绍了Nuinstruct，这是一个新颖的数据集，具有17个子任务的91K多视频视频QA对，每个任务都需要整体信息（例如时间，多视图和空间），从而显着提高了挑战水平。为了获得Nuinstruct，我们提出了一种基于SQL的新方法，以自动生成指令 - 响应对，这是受人类驱动逻辑进展的启发。我们进一步介绍BEV-INMLLM，这是一种端到端的方法，用于有效地推导指令感知的Bird's-eye-View（BEV）功能（用于大型语言模型的语言）。 BEV-INMLLM集成了多视图，空间意识和时间语义，以增强MLLM在NUINSTRUCTION任务上的功能。此外，我们提出的BEV注入模块是现有MLLM的插件方法。我们对Nuinstruct的实验表明，BEV-INMLLM显着胜过现有的MLLM，例如各种任务的提高约为9％。我们计划释放我们的Nuinstruct，以进行未来的研究开发。

### COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training 
[[arxiv](https://arxiv.org/abs/2401.00849)] [[cool](https://papers.cool/arxiv/2401.00849)] [[pdf](https://arxiv.org/pdf/2401.00849)]
> **Authors**: Alex Jinpeng Wang,Linjie Li,Kevin Qinghong Lin,Jianfeng Wang,Kevin Lin,Zhengyuan Yang,Lijuan Wang,Mike Zheng Shou
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: 16 pages; Website: http://fingerrec.github.io/cosmo
- **标题**: Cosmo：与交错预训练的对比度精简多模型模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 在视觉预训练的演变中，从短文本理解转移到包含扩展的文本上下文的转变至关重要。 \ cite {flamingo，palme}的最新自回归视觉模型利用了大语言模型的长期文化能力，在很少的文本生成任务中表现出色，但在对齐任务中面临挑战。解决这一差距，我们将对比损失引入文本生成模型中，并介绍对比度流式的多模式框架（\ modelname），从战略上将语言模型分配到专用的单峰文本处理和熟练的多模式处理组件中。 \ ModelName（我们的统一框架）合并了单峰和多模式元素，增强了涉及文本和视觉数据的任务的模型性能，同时大大减少了可学习的参数。但是，这些模型需要大量的长文本数据集，但是高质量的长篇小说视频数据集的可用性仍然有限。为了弥合这一差距，这项工作介绍了\ VideoDataSetName，这是一个带有全面字幕的开幕式播放的视频文本数据集，标志着向前迈出的重要一步。为了展示其影响，我们说明了\ VideoDatasetName {}如何增强图像文本任务中的模型性能。我们的模型具有34％可学习的参数并利用72 \％的可用数据，比OpenFlamingo〜 \ cite {OpenFlamingo}证明了显着优势。例如，在4-SHOT FLICKR字幕任务中，性能显着从57.2％提高到65。\％。 \ modelName {}和\ videodatasetName {}的贡献是由14个不同的下游数据集中的显着性能提高，其中包含图像文本和视频文本任务。

### Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera 
[[arxiv](https://arxiv.org/abs/2401.00847)] [[cool](https://papers.cool/arxiv/2401.00847)] [[pdf](https://arxiv.org/pdf/2401.00847)]
> **Authors**: Jiye Lee,Hanbyul Joo
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: Accepted to CVPR 2024; Project page: https://jiyewise.github.io/projects/MocapEvery/
- **标题**: 到处都是mocap的每个人：带有智能手表和头戴式相机的轻巧运动捕获
- **领域**: 计算机视觉和模式识别,图形
- **摘要**: 我们提出了一种基于两个智能手表和一个头部安装相机的轻巧且负担得起的运动捕获方法。与使用六个或更多专家级IMU设备的现有方法相反，我们的方法更具成本效益和方便。我们的方法可以使各地的每个人都可以捕获可穿戴的运动捕获，从而在不同的环境中实现3D全身运动捕获。作为克服具有不同方式的传感器输入的极端稀疏性和模棱两可的关键思想，我们整合了从头部安装的摄像机获得的6D头姿势以进行运动估计。为了在宽敞的室内和室外场景中捕获，我们提出了一种算法来跟踪和更新地面级别的更改以定义头部姿势，并与基于多阶段变压器的回归模块相结合。我们还介绍了利用自我中心图像的视觉提示的新型策略，以进一步提高运动捕获质量，同时降低歧义。我们演示了我们在各种具有挑战性的场景上的表现，包括复杂的户外环境和日常动作，包括对象互动和多个人之间的社交互动。

### Retrieval-Augmented Egocentric Video Captioning 
[[arxiv](https://arxiv.org/abs/2401.00789)] [[cool](https://papers.cool/arxiv/2401.00789)] [[pdf](https://arxiv.org/pdf/2401.00789)]
> **Authors**: Jilan Xu,Yifei Huang,Junlin Hou,Guo Chen,Yuejie Zhang,Rui Feng,Weidi Xie
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: CVPR 2024. Project page is available at: https://jazzcharles.github.io/Egoinstructor/
- **标题**: 检索以自我为中心的视频字幕
- **领域**: 计算机视觉和模式识别
- **摘要**: 从第一人称观点视频中了解人类行为会带来重大挑战。大多数先前的方法仅在以自我为中心的视频上探索表示形式学习，同时忽略了利用现有大型第三人称视频的潜在好处。在本文中，（1）我们开发了Egoinstructor，这是一种检索型多模式字幕模型，该模型会自动检索语义相关的第三人称教学视频，以增强以Egentric视频的视频字幕。 （2）为了培训跨视图检索模块，我们设计了一条自动管道，从不同的大规模中心和外主数据集发现自我exo视频对。 （3）我们通过新颖的EgoExonce损失来训练跨视图检索模块，该模块通过将它们与描述类似动作的共享文本功能保持一致，从而使以自我为中心和外向的视频功能更加紧密地将其拉近。 （4）通过广泛的实验，我们的跨视图检索模块在七个基准测试中表现出卓越的性能。关于以自我为中心的视频字幕，Egoinstructor通过利用第三人称视频作为参考来表现出重大改进。项目页面可用：https：//jazzcharles.github.io/egoinstructor/

### Depth Map Denoising Network and Lightweight Fusion Network for Enhanced 3D Face Recognition 
[[arxiv](https://arxiv.org/abs/2401.00719)] [[cool](https://papers.cool/arxiv/2401.00719)] [[pdf](https://arxiv.org/pdf/2401.00719)]
> **Authors**: Ruizhuo Xu,Ke Wang,Chao Deng,Mei Wang,Xi Chen,Wenhui Huang,Junlan Feng,Weihong Deng
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: Accepted by Pattern Recognition
- **标题**: 深度图Denoising网络和轻巧的融合网络，用于增强3D面部识别
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着消费者深度传感器的可用性的增加，3D面部识别（FR）吸引了越来越多的关注。但是，这些传感器获得的数据通常是粗糙且嘈杂的，使其直接使用不切实际。在本文中，我们基于DeNoising隐式图像函数（DIIF）引入了创新的深度图denoising网络（DMDNET），以减少噪声并提高低质量3D FR的面部深度图像的质量。在使用DMDNET生成了干净的深度面后，我们进一步设计了一个强大的识别网络，称为轻量级深度和正常融合网络（LDNFNET），该网络结合了一个多分支融合块，以学习不同模态（例如深度和正常图像）之间的独特和互补特征。在四个不同的低质量数据库上进行的全面实验证明了我们提出的方法的有效性和鲁棒性。此外，在组合DMDNET和LDNFNET时，我们在lock3dface数据库上实现最新结果。

### Towards Efficient and Effective Text-to-Video Retrieval with Coarse-to-Fine Visual Representation Learning 
[[arxiv](https://arxiv.org/abs/2401.00701)] [[cool](https://papers.cool/arxiv/2401.00701)] [[pdf](https://arxiv.org/pdf/2401.00701)]
> **Authors**: Kaibin Tian,Yanhua Cheng,Yi Liu,Xinglin Hou,Quan Chen,Han Li
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: 通过粗到精细的视觉表示学习，迈向有效有效的文本对视频检索
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，基于剪辑的文本到视频检索方法已经历了快速发展。进化的主要方向是利用视觉和文本线索的更广泛的范围来实现对齐方式。具体而言，那些具有令人印象深刻的性能的方法通常会为句子（单词）-VIDEO（框架）相互作用设计一个沉重的融合块，而不论其过度计算的复杂性如何。然而，这些方法在特征利用率和检索效率方面并不是最佳的。为了解决这个问题，我们采用了多个视觉功能学习，确保模型在捕获培训阶段的视觉内容特征到从抽象到详细级别的视觉内容特征的全面性。为了更好地利用多粒性功能，我们在检索阶段设计了两个阶段的检索体系结构。该解决方案巧妙地平衡了检索含量的粗糙和细粒度。此外，它还达到了检索有效性和效率之间的和谐平衡。具体而言，在训练阶段，我们设计了一个无参数的文本门控互动块（TIB），以进行细粒度的视频表示学习，并嵌入了额外的Pearson约束，以优化跨模式表示学习。在检索阶段，我们使用粗粒的视频表示来快速召回Top-K候选人，然后将其重新播放，并通过细粒度的视频表示。对四个基准测试的广泛实验证明了效率和有效性。值得注意的是，我们的方法与当前最新方法的性能相当，而速度快了近50倍。

### Reviving the Context: Camera Trap Species Classification as Link Prediction on Multimodal Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2401.00608)] [[cool](https://papers.cool/arxiv/2401.00608)] [[pdf](https://arxiv.org/pdf/2401.00608)]
> **Authors**: Vardaan Pahuja,Weidi Luo,Yu Gu,Cheng-Hao Tu,Hong-You Chen,Tanya Berger-Wolf,Charles Stewart,Song Gao,Wei-Lun Chao,Yu Su
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: 12 pages, 5 figures
- **标题**: 复兴上下文：相机陷阱物种分类作为多模式知识图上的链接预测
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 相机陷阱是动物生态学的重要工具，用于生物多样性监测和保护。但是，他们的实际应用受到对新地点和看不见的位置的泛化等问题的限制。图像通常与不同形式的上下文相关联，这可能以不同的方式存在。在这项工作中，我们利用与摄像机陷阱图像相关的结构化上下文，以提高相机陷阱中物种分类任务的分布概括。例如，可以将野生动物的图片与捕获的时间和地点以及有关动物物种的结构化生物学知识有关。虽然经常被现有研究忽略，但结合这种情况为更好的图像理解提供了一些潜在的好处，例如解决数据稀缺和增强概括。但是，有效地将这种异质环境纳入视觉域是一个具有挑战性的问题。为了解决这个问题，我们提出了一个新颖的框架，将物种分类转化为多模式知识图（kg）中的链接预测。该框架使各种多模式上下文的无缝集成以进行视觉识别。我们将此框架应用于IwildCAM2020野生和快照山斑马数据集中的分布外物种分类，并通过最先进的方法实现竞争性能。此外，我们的框架提高了样本效率，以识别代表性不足的物种。

### SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge 
[[arxiv](https://arxiv.org/abs/2401.00496)] [[cool](https://papers.cool/arxiv/2401.00496)] [[pdf](https://arxiv.org/pdf/2401.00496)]
> **Authors**: Dimitrios Psychogyios,Emanuele Colleoni,Beatrice Van Amsterdam,Chih-Yang Li,Shu-Yu Huang,Yuchong Li,Fucang Jia,Baosheng Zou,Guotai Wang,Yang Liu,Maxence Boels,Jiayu Huo,Rachel Sparks,Prokar Dasgupta,Alejandro Granados,Sebastien Ourselin,Mengya Xu,An Wang,Yanan Wu,Long Bai,Hongliang Ren,Atsushi Yamada,Yuriko Harai,Yuto Ishikawa,Kazuyuki Hayashi, et al. (25 additional authors not shown)
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: SAR-RARP50：对机器人辅助的根治性前列腺切除术挑战的手术仪器和动作识别的分割
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 手术工具细分和动作识别是许多计算机辅助的干预应用程序中的基本构建基础，从外科技能评估到决策支持系统。如今，基于学习的动作识别和细分方法超过了经典方法，但是在大型注释的数据集上。此外，经常对动作识别和工具分割算法进行训练，并彼此隔离地进行预测，而无需利用潜在的交叉任务关系。随着2022 SAR-RARP50挑战的Edtovis，我们发布了第一个多模式，公开可用的Vivo，Vivo，用于手术动作识别和语义仪器的数据集，其中包含50个机器人辅助辅助自由基前列腺切除术（RARP）的50个缝合视频片段。挑战的目的是双重的。首先，使研究人员能够利用提供的数据集的规模，并在手术领域开发出健壮且高度准确的单任务识别和工具分割方法。其次，为了进一步探索基于多任务的学习方法的潜力，并确定其与单任务对应的比较优势。共有12个团队参与了挑战，贡献了7种动作识别方法，9种仪器分割技术和4种整合了动作识别和仪器细分的多任务方法。完整的SAR-RARP50数据集可在以下网址找到：https：//rdr.ucl.ac.uk/projects/sarrarp50_sementation_of_surgical_instrumentation_and_and_action_recognition_recognition_on_robot-assist-assist-assisted_radical_radical_radical_prostatoctome_prostostose_challenge_challenge/191091

### From Text to Pixels: A Context-Aware Semantic Synergy Solution for Infrared and Visible Image Fusion 
[[arxiv](https://arxiv.org/abs/2401.00421)] [[cool](https://papers.cool/arxiv/2401.00421)] [[pdf](https://arxiv.org/pdf/2401.00421)]
> **Authors**: Xingyuan Li,Yang Zou,Jinyuan Liu,Zhiying Jiang,Long Ma,Xin Fan,Risheng Liu
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: 10 pages, 12 figures, 3 tables, conference
- **标题**: 从文本到像素：一种用于红外和可见图像融合的上下文感知语义协同解决方案
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着深度学习技术的快速发展，多模式图像融合在对象检测任务中变得越来越普遍。尽管它很受欢迎，但不同来源如何描述场景内容的固有差异使融合成为一个具有挑战性的问题。当前的融合方法可以使用迭代优化或深度学习体系结构来确定两种模式之间的共享特征，并将它们整合到该共享领域，这通常忽略了模态之间的复杂语义关系，从而导致对模式间连接的表面理解，从而使次优融合的结果具有浅表性的理解。为了解决这个问题，我们介绍了一种文本引导的多模式图像融合方法，该方法利用了从文本描述中的高级语义来整合来自红外和可见图像的语义。该方法利用了各种方式的互补特征，增强了对象检测的准确性和鲁棒性。该代码手册用于增强对融合内和域间动力学的简化而简洁的描述，并进行了微调，以在检测任务中进行最佳性能。我们提出了一种双层优化策略，该策略在融合和检测的联合问题之间建立了联系，并同时优化了这两个过程。此外，我们介绍了第一个配对红外和可见图像的数据集，并带有文本提示，为将来的研究铺平了道路。在几个数据集上进行的广泛实验表明，我们的方法不仅会产生视觉上的融合结果，而且还获得了对现有方法的更高检测图，从而实现了最新的结果。

### BusReF: Infrared-Visible images registration and fusion focus on reconstructible area using one set of features 
[[arxiv](https://arxiv.org/abs/2401.00285)] [[cool](https://papers.cool/arxiv/2401.00285)] [[pdf](https://arxiv.org/pdf/2401.00285)]
> **Authors**: Zeyang Zhang,Hui Li,Tianyang Xu,Xiaojun Wu,Josef Kittler
> **First submission**: 2023-12-30
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: BUSREF：使用一组功能，红外可见图像注册和融合专注于可重建区域
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在多模式摄像机一起运行的情况下，无法避免使用非对准图像的问题。然而，现有的图像融合算法在很大程度上依赖于严格注册的输入图像对来产生更精确的融合结果，以提高下游高级视觉任务的性能。为了放松这个假设，可以尝试先注册图像。但是，现有的记录多种方式的方法具有局限性，例如复杂的结构和对重要语义信息的依赖。本文旨在解决一个名为bisref的单个框架中图像注册和融合的问题。我们专注于红外可见图像注册和融合任务（IVRF）。在此框架中，未对齐的输入图将通过三个阶段：粗登记，精细的注册和融合。可以证明，统一方法可以实现更强大的IVRF。我们还提出了一种新颖的培训和评估策略，涉及使用面具来减少非重新构造区域对损失功能的影响，从而大大提高了融合任务的准确性和鲁棒性。最后但并非最不重要的一点是，梯度感知的融合网络旨在保留互补信息。该算法的高级性能由

### HybridGait: A Benchmark for Spatial-Temporal Cloth-Changing Gait Recognition with Hybrid Explorations 
[[arxiv](https://arxiv.org/abs/2401.00271)] [[cool](https://papers.cool/arxiv/2401.00271)] [[pdf](https://arxiv.org/pdf/2401.00271)]
> **Authors**: Yilan Dong,Chunlin Yu,Ruiyang Ha,Ye Shi,Yuexin Ma,Lan Xu,Yanwei Fu,Jingya Wang
> **First submission**: 2023-12-30
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: 杂化：与混合探索的时空换步步态识别的基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的步态识别基准主要包括实验室环境中的次要衣服变化，但缺乏随时间和空间外观的持续变化。在本文中，我们提出了第一个用于换衣步态识别的野外基准CCGAIT，其中包含多种衣服的变化，室内和室外场景以及92天内的多模式统计。为了进一步解决衣服和观点变化的耦合效果，我们提出了一种混合方法混合方式，该方法可以利用时间动力学和3D人网格的预计2D信息。具体而言，我们引入了一个规范对齐的时空变压器（CA-STT）模块，以编码人类关节位置感知特征，并通过Silhouette引导的3D-2D外观投影（SILD）策略完全利用3D密集的先验。我们的贡献是双重的：我们提供了一个具有挑战性的基准CCGAIT，可捕捉在扩展和空间中逼真的外观变化，我们提出了一种混合框架混合工程，该混合框架优于先前在CCGAIT和GAIT3D基准上工作。我们的项目页面可在https://github.com/hcvlab/hybridgait上找到。

### COMMA: Co-Articulated Multi-Modal Learning 
[[arxiv](https://arxiv.org/abs/2401.00268)] [[cool](https://papers.cool/arxiv/2401.00268)] [[pdf](https://arxiv.org/pdf/2401.00268)]
> **Authors**: Lianyu Hu,Liqing Gao,Zekang Liu,Chi-Man Pun,Wei Feng
> **First submission**: 2023-12-30
> **First announcement**: 2024-01-02
> **comment**: Accepted to AAAI2024. Code is available at https://github.com/hulianyuyy/COMMA
- **标题**: 逗号：共同开设的多模式学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 预处理的大规模视觉模型（例如剪辑）在一系列下游任务上表现出了出色的概括性。但是，它们对输入文本提示的变化很敏感，并且需要选择及时模板以实现令人满意的性能。最近，已经提出了各种方法将提示作为文本输入，以避免在微调过程中进行手工制作的及时工程的要求。我们注意到这些方法在两个方面是次优的。首先，这些方法中的视觉和语言分支的提示通常是分离的或在方向上相关的。因此，两个分支的提示都没有完全相关，并且可能没有提供足够的指导来使两个分支的表示。其次，观察到，与剪辑相比，大多数以前的方法通常在可见的类上获得更好的性能，但在看不见的类别上会导致表现变性。这是因为在预处理阶段中学到的基本通用知识在微调过程中被部分遗忘。在本文中，我们提出了共同开关的多模式学习（COMMA）来处理上述局限性。尤其是，我们的方法考虑了两个分支的提示以生成提示以增强两个分支的表示对准。此外，为了减轻忘记基本知识，我们最大程度地减少了在后期变压器层中预先训练的剪辑中学习提示和手工制作的提示的嵌入之间的特征差异。我们在三个代表性的泛化任务中评估了我们的方法，对新的类别，新的目标数据集和看不见的域移动。实验结果证明了我们方法的优越性，通过对所有效率高的任务表现出良好的绩效提升。

### GazeCLIP: Enhancing Gaze Estimation Through Text-Guided Multimodal Learning 
[[arxiv](https://arxiv.org/abs/2401.00260)] [[cool](https://papers.cool/arxiv/2401.00260)] [[pdf](https://arxiv.org/pdf/2401.00260)]
> **Authors**: Jun Wang,Hao Ruan,Liangjian Wen,Yong Dai,Mingjie Wang
> **First submission**: 2023-12-30
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: GazeClip：通过文本指导的多模式学习来增强目光的估计
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉凝视估计以及其广泛的应用程序方案，在研究界吸引了越来越多的关注。尽管现有的方法仅根据图像信号推断，但视觉语言协作的最新进展表明，语言信息的整合可以显着提高各种视觉任务的性能。利用大规模对比的语言图像预训练（剪辑）模型的显着可传递性，我们解决了如何有效地将语言线索应用于凝视估计的开放而紧迫的问题。在这项工作中，我们提出了GazeClip，这是一个新颖的凝视估计框架，深入探讨了文本面协作。具体而言，我们引入了精心设计的语言描述生成器，以产生具有粗糙方向提示的文本信号。此外，我们提出了一个基于夹子的主链，以表征文本面对的凝视估计，并取决于一个细粒度的多模式融合模块，该模块对异质输入之间的复杂相互关系进行了建模。在三个具有挑战性的数据集上进行了广泛的实验，证明了Gazeclip的优势，该数据达到了最先进的准确性。我们的发现强调了使用视觉语言协作来推进凝视估计的潜力，并开放了新的途径，以进行视觉任务的多模式学习研究。实施代码和预培训模型将公开可用。

### Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2401.00127)] [[cool](https://papers.cool/arxiv/2401.00127)] [[pdf](https://arxiv.org/pdf/2401.00127)]
> **Authors**: Ashhadul Islam,Md. Rafiul Biswas,Wajdi Zaghouani,Samir Brahim Belhaouari,Zubair Shah
> **First submission**: 2023-12-29
> **First announcement**: 2024-01-02
> **comment**: 5 pages,6 figures, 4 tables, Accepted on The International Symposium on Foundation and Large Language Models (FLLM2023)
- **标题**: 推动边界：使用大型多模式探索零射击对象分类
- **领域**: 计算机视觉和模式识别,社交和信息网络
- **摘要**: 语言和视觉模型的协同作用引起了大型语言和视觉助理模型（LLVA），旨在使用户参与与基于图像的查询交织在一起的丰富对话体验。这些全面的多模型模型将视觉编码器与大语言模型（LLMS）无缝整合，以通用语言和视觉理解扩展其应用程序。大型多模型（LMM）的出现预示着人工智能（AI）援助的新时代，扩大了AI利用率的视野。本文对LMM进行了独特的视角，并使用为特定数据集设计的量身定制提示来探索其在执行图像分类任务方面的功效。我们还研究了LLVAS零射击学习能力。我们的研究包括对四个不同数据集进行的基准分析：MNIST，CATS vs。狗，赞美诗（Ants vs. Bees）和一个包括POX VS的非常规数据集。非Pox皮肤图像。我们的实验结果证明了该模型的出色性能，分类精度为85 \％，100 \％，77 \％和79 \％的分类精度，而相应的数据集则没有任何微调。为了加强我们的分析，我们评估了模型的性能后针对特定任务进行微调。在一个例子中，通过一个数据集进行了微调，其中包括有或没有自闭症儿童面孔的图像。在进行微调之前，该模型证明了55 \％的测试准确性，在微调后显着提高到83 \％。这些结果与我们先前的发现相结合，强调了LLVA及其在现实世界中的多功能应用的变革潜力。

### Glance and Focus: Memory Prompting for Multi-Event Video Question Answering 
[[arxiv](https://arxiv.org/abs/2401.01529)] [[cool](https://papers.cool/arxiv/2401.01529)] [[pdf](https://arxiv.org/pdf/2401.01529)]
> **Authors**: Ziyi Bai,Ruiping Wang,Xilin Chen
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: Accepted in NeurIPS 2023
- **标题**: 目光与重点：记忆提示多事件视频问题回答
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频问题回答（VideoQA）已成为评估代理人理解人类日常行为能力的重要工具。尽管大型视觉语言模型最近在许多多模式任务中取得了成功，但涉及多个人类对象互动事件的视频的复杂情况仍然仍然具有挑战性。相比之下，人类可以通过使用一系列情节记忆作为锚点来快速找到与问题相关的关键时刻来轻松解决它。为了模仿这种有效的推理策略，我们提出了Glance-Cocus模型。一种简单的方法是应用一个动作检测模型来预测一组动作作为关键记忆。但是，封闭的词汇中的这些动作很难推广到各种视频域。我们不必训练一个编码器，以在瞥见阶段生成一组动态事件记忆。除了使用有监督的双方匹配来获得事件记忆外，我们还设计了一种无监督的内存生成方法，以摆脱对事件注释的依赖。接下来，在聚焦阶段，这些事件记忆充当建立与高级事件概念和低级冗长视频内容之间的问题之间的相关性的桥梁。考虑到这个问题，该模型首先关注生成的关键事件内存，然后专注于通过我们设计的多层次跨注意机制推理的最相关时刻。我们对四个多事件VideoQA基准测试进行了广泛的实验，包括Star，EgotaskQA，AGQA和Next-QA。我们提出的模型取得了最先进的结果，超过了各种具有挑战性的推理任务中当前的大型模型。代码和型号可在https://github.com/byz0e/glance-focus上找到。

### Multimodal self-supervised learning for lesion localization 
[[arxiv](https://arxiv.org/abs/2401.01524)] [[cool](https://papers.cool/arxiv/2401.01524)] [[pdf](https://arxiv.org/pdf/2401.01524)]
> **Authors**: Hao Yang,Hong-Yu Zhou,Cheng Li,Weijian Huang,Jiarun Liu,Yong Liang,Guangming Shi,Hairong Zheng,Qiegen Liu,Shanshan Wang
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: No comments
- **标题**: 多模式的自我监管学习，用于病变本地化
- **领域**: 计算机视觉和模式识别
- **摘要**: 使用成像和诊断报告的多模式深度学习在医学成像诊断领域取得了令人印象深刻的进步，在缺乏足够的注释信息的情况下，在医学成像诊断领域取得了令人印象深刻的进步。尽管如此，在没有详细的位置注释的情况下准确地定位疾病仍然是一个挑战。尽管现有的方法试图利用本地信息来实现细粒的语义一致性，但它们在报告中提取综合上下文的细粒语义的能力有限。为了解决这个问题，引入了一种新方法，该方法将文本报告中的完整句子作为本地语义一致性的基本单元。这种方法将胸部X射线图像与相应的文本报告结合在一起，在全球和本地级别进行对比学习。该方法在多个数据集上获得的主要结果证实了其在病变定位任务中的功效。

### Temporal Adaptive RGBT Tracking with Modality Prompt 
[[arxiv](https://arxiv.org/abs/2401.01244)] [[cool](https://papers.cool/arxiv/2401.01244)] [[pdf](https://arxiv.org/pdf/2401.01244)]
> **Authors**: Hongyu Wang,Xiaotao Liu,Yifan Li,Meng Sun,Dian Yuan,Jing Liu
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: No comments
- **标题**: 具有模态提示的时间自适应RGBT跟踪
- **领域**: 计算机视觉和模式识别
- **摘要**: RGBT跟踪已被广泛用于机器人技术，监视处理和自动驾驶等各个领域。现有的RGBT跟踪器充分探索模板和搜索区域之间的空间信息，并根据外观匹配结果定位目标。但是，这些RGBT跟踪器对时间信息的开发非常有限，可以忽略时间信息或通过在线抽样和培训来利用它。前者努力应对对象状态的变化，而后者则忽略了空间和时间信息之间的相关性。为了减轻这些局限性，我们提出了一种新型的时间自适应RGBT跟踪框架，称为Tatrack。 Tatrack具有时空的两流结构，并通过在线更新模板捕获时间信息，其中两流结构分别是指初始模板和在线更新模板的多模式特征提取和交叉模式交互。塔特拉克（Tatrack）有助于全面利用时空信息和多模式信息，以实现目标定位。此外，我们设计了一个时空相互作用（STI）机制，该机制桥接了两个分支，并使交叉模式相互作用跨越更长的时间尺度。在三个受欢迎的RGBT跟踪基准上进行的广泛实验表明，我们的方法在实时速度下运行时实现了最先进的性能。

### IdentiFace : A VGG Based Multimodal Facial Biometric System 
[[arxiv](https://arxiv.org/abs/2401.01227)] [[cool](https://papers.cool/arxiv/2401.01227)] [[pdf](https://arxiv.org/pdf/2401.01227)]
> **Authors**: Mahmoud Rabea,Hanya Ahmed,Sohaila Mahmoud,Nourhan Sayed
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: 12 pages, 22 figures and 9 images
- **标题**: 标识：基于VGG的多模式面部生物识别系统
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 面部生物识别系统的开发为计算机视野领域的发展做出了巨大贡献。如今，总是需要开发多模式系统，该系统以有效，有意义的方式结合多种生物特征。在本文中，我们引入了“标识”，这是一种多模式的面部生物识别系统，将面部识别的核心与某些最重要的柔软生物特征特征（例如性别，面部形状和情感）相结合。我们还专注于仅使用VGG-16启发的体系结构开发系统，这些体系结构在不同子系统之间进行了较小的更改。这种统一允许跨模态更简单。它使解释任务之间的学习功能变得更加容易，从而很好地指示了面部模式和潜在连接的决策过程。对于识别问题，我们使用从FERET数据库收集的数据[1]获得了五个具有较高类内变化的五个类别的测试准确性99.2％。在性别识​​别问题中，我们在数据集中达到了99.4％，公共数据集[2]达到95.15％。我们还能够使用名人面形数据集在面形问题中实现88.03％的测试精度[3]。最后，与FER2013数据集中的相关工作相比，我们在情感任务中实现了66.13％的体面测试精度[4]。

### Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation 
[[arxiv](https://arxiv.org/abs/2401.01207)] [[cool](https://papers.cool/arxiv/2401.01207)] [[pdf](https://arxiv.org/pdf/2401.01207)]
> **Authors**: Renshuai Liu,Bowen Ma,Wei Zhang,Zhipeng Hu,Changjie Fan,Tangjie Lv,Yu Ding,Xuan Cheng
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: No comments
- **标题**: 在个性化的面部产生中同时且细粒度的表达控制
- **领域**: 计算机视觉和模式识别
- **摘要**: 在以人为本的内容产生中，预先训练的文本对图像模型难以生成用户遗嘱的肖像图像，这些肖像图像在表现出多种表达方式的同时保留了个人的身份。本文介绍了我们致力于个性化面部生成的努力。为此，我们提出了一个新型的多模式面部生成框架，能够同时表达身份表达和更细粒度的表达合成。我们的表达控制是如此复杂，以至于可以通过细粒度的情感词汇进行专业。我们设计了一个新颖的扩散模型，该模型可以同时承担面对交换和重新制定的任务。由于身份和表达的纠缠，在一个框架中分别和精确控制它们是不平凡的，因此尚未探索。为了克服这一点，我们在条件扩散模型中提出了几种创新设计，包括平衡身份和表达编码器，改进的中点采样和明确的背景条件。与最先进的文本对图，面部交换和面部重新制定方法相比，广泛的实验证明了所提出的框架的可控性和可扩展性。

### Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label Classification 
[[arxiv](https://arxiv.org/abs/2401.01181)] [[cool](https://papers.cool/arxiv/2401.01181)] [[pdf](https://arxiv.org/pdf/2401.01181)]
> **Authors**: Xuelin Zhu,Jian Liu,Dongqi Tang,Jiawei Ge,Weijia Liu,Bo Liu,Jiuxin Cao
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: No comments
- **标题**: 基于查询的知识共享开放式多标签分类
- **领域**: 计算机视觉和模式识别
- **摘要**: 识别在训练过程中未出现的标签（称为多标签零照片学习）是计算机视觉中的一项非平凡任务。为此，最近的研究试图通过知识蒸馏探索视力预训练（VLP）模型的多模式知识，从而可以以开放式摄影的方式识别看不见的标签。但是，实验证据表明，知识蒸馏是次优的，并且在看不见的标签预测中提供了有限的性能增长。在本文中，提出了一种基于查询的知识共享范式，以探索从验证的VLP模型中进行开放式多型多标签分类的多模式知识。具体而言，对一组可学习的标签 - 不足的查询令牌进行了训练，可以从输入图像中提取批判性视力知识，并在所有标签上进一步共享，从而使他们可以选择感兴趣的令牌作为视觉线索以识别识别。此外，我们提出了一个有效的及时池，以嵌入可靠的标签嵌入，并将标准排名学习重新制定为一种分类形式，以允许匹配的特征向量的幅度，这两个都显着受益于标签识别。实验结果表明，我们的框架在零弹药任务上的最先进方法分别超过了5.9％和4.5％的MAP，分别在整个NUS和开放图像上。

### AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided Diagnosis 
[[arxiv](https://arxiv.org/abs/2401.01074)] [[cool](https://papers.cool/arxiv/2401.01074)] [[pdf](https://arxiv.org/pdf/2401.01074)]
> **Authors**: Qiuhui Chen,Yi Hong
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: BIBM 2024
- **标题**: Alifuse：对齐和融合计算机辅助诊断的多模式医学数据
- **领域**: 计算机视觉和模式识别
- **摘要**: 用于诊断决策的医疗数据通常是多模式的，提供了有关受试者的全面信息。尽管计算机辅助诊断系统可以从多模式输入中受益，但有效融合此类数据仍然是一项艰巨的任务，并且是医学研究中的关键重点。在本文中，我们提出了一个基于变压器的框架，称为Alifuse，用于对齐和融合多模式医学数据。具体而言，我们采用了模式内和模式的注意机制将医学图像以及非结构化和结构化的临床记录转换为视觉和语言令牌，以学习所有成像和非成像数据的统一表示，以进行分类。此外，我们将恢复建模与对比度学习框架集成在一起，共同学习图像和文本之间的高级语义一致性，以及在另一种图像的帮助下对一种模式的低级理解。我们应用Alifuse对阿尔茨海默氏病进行分类，在五个公共数据集上实现最先进的表现，并且表现优于八个基线。

### BEV-TSR: Text-Scene Retrieval in BEV Space for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2401.01065)] [[cool](https://papers.cool/arxiv/2401.01065)] [[pdf](https://arxiv.org/pdf/2401.01065)]
> **Authors**: Tao Tang,Dafeng Wei,Zhengyu Jia,Tian Gao,Changwei Cai,Chengkai Hou,Peng Jia,Kun Zhan,Haiyang Sun,Jingchen Fan,Yixing Zhao,Fu Liu,Xiaodan Liang,Xianpeng Lang,Yang Wang
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: No comments
- **标题**: BEV-TSR：BEV空间中的文本场景检索，用于自动驾驶
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 自动驾驶行业的快速发展导致了自主驾驶数据的大量积累。因此，对检索数据提供专业优化的需求不断增长。但是，直接采用先前的图像检索方法面临几个挑战，例如缺乏全球功能表示和不足的文本检索能力，无法用于复杂的驾驶场景。为了解决这些问题，首先，我们提出了BEV-TSR框架，该框架利用描述性文本作为输入，以检索鸟类视图（BEV）空间中的相应场景。然后，为了促进复杂的场景检索，并使用大量文本描述，我们采用大型语言模型（LLM）来提取文本输入的语义特征，并结合了知识图嵌入，以增强语言嵌入的语义丰富性。为了实现BEV功能和语言嵌入之间的功能一致性，我们建议使用一组共享的可学习嵌入嵌入共享的跨模式嵌入，以弥合这两种方式之间的差距，并采用字幕生成任务来进一步增强对齐方式。此外，缺乏形成良好的检索数据集以进行有效评估。为此，我们基于广泛采用的Nuscenes数据集建立了一个多级检索数据集Nuscenes-retreval。多级Nuscenes-retreval的实验结果表明，BEV-TSR可以在场景到文本和文本到文本检索中分别达到85.78％和87.66％的TOP-1准确性，例如85.78％和87.66％。代码和数据集将可用。

### Multi-modal vision-language model for generalizable annotation-free pathology localization and clinical diagnosis 
[[arxiv](https://arxiv.org/abs/2401.02044)] [[cool](https://papers.cool/arxiv/2401.02044)] [[pdf](https://arxiv.org/pdf/2401.02044)]
> **Authors**: Hao Yang,Hong-Yu Zhou,Zhihuan Li,Yuanxu Gao,Cheng Li,Weijian Huang,Jiarun Liu,Hairong Zheng,Kang Zhang,Shanshan Wang
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 多态视觉语言模型，用于无概括的无注释病理学定位和临床诊断
- **领域**: 计算机视觉和模式识别
- **摘要**: 从医学图像自动定义病理有助于理解疾病的出现和进展，这种能力在临床诊断中至关重要。但是，现有的深度学习模型在很大程度上依赖于专家注释，并且在开放的临床环境中缺乏概括能力。在这项研究中，我们提出了一个可推广的视觉语言模型，用于无注释病理学定位（AFLOC）。 AFLOC的核心强度在于其广泛的基于语义结构的对比学习，从具有丰富图像特征的报告中，全面地将多个跨性医学概念与多种病理表达和看不见的病理表达相结合，而无需依赖专家的图像注释。我们在胸部X射线图像上演示了概念证明，并在6个不同的外部数据集中进行了广泛的实验验证，其中包括13种类型的胸部病理。结果表明，AFLOC超过了病理定位和分类方面的最新方法，甚至超过了在定位5种不同病理学方面的人类基准。此外，我们通过将其应用于视网膜底面图像来进一步验证其概括能力。我们的方法展示了AFLOC的多功能性，并强调了其在复杂临床环境中的临床诊断的适用性。

### Instruct-Imagen: Image Generation with Multi-modal Instruction 
[[arxiv](https://arxiv.org/abs/2401.01952)] [[cool](https://papers.cool/arxiv/2401.01952)] [[pdf](https://arxiv.org/pdf/2401.01952)]
> **Authors**: Hexiang Hu,Kelvin C. K. Chan,Yu-Chuan Su,Wenhu Chen,Yandong Li,Kihyuk Sohn,Yang Zhao,Xue Ben,Boqing Gong,William Cohen,Ming-Wei Chang,Xuhui Jia
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: 20 pages, 18 figures
- **标题**: 指示imagen：带有多模式指令的图像生成
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 本文介绍了指令 - 象征，该模型可以应对异质图像生成任务并跨看不见的任务进行概括。我们介绍了 *多模式指令 *用于图像生成，这是一种任务表示形式，以精确表达一系列生成意图。它使用自然语言来融合不同的方式（例如文本，边缘，样式，主题等），以便可以以统一的格式进行标准化。然后，我们通过微调具有两阶段框架的预训练的文本对图扩散模型来构建指导成像。首先，我们使用检索功能的训练对模型进行调整，以增强模型的能力，以将其生成在外部多模式上下文上。随后，我们对需要视觉的理解（例如主题驱动的生成等）进行了各种图像生成任务的改编模型，每个模型都与封装任务本质的多模式指令配对。人类对各种图像生成数据集的评估表明，指令 - 象征匹配或超过了特定于任务的模型，并证明了有希望的概括，以看不见和更复杂的任务。

### Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions 
[[arxiv](https://arxiv.org/abs/2401.01827)] [[cool](https://papers.cool/arxiv/2401.01827)] [[pdf](https://arxiv.org/pdf/2401.01827)]
> **Authors**: David Junhao Zhang,Dongxu Li,Hung Le,Mike Zheng Shou,Caiming Xiong,Doyen Sahoo
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: project page: https://showlab.github.io/Moonshot/
- **标题**: 月球：朝着可控视频生成和编辑多模式条件
- **领域**: 计算机视觉和模式识别
- **摘要**: 大多数现有的视频扩散模型（VDM）仅限于文本条件。因此，他们通常缺乏对产生视频的视觉外观和几何结构的控制。这项工作介绍了Moonshot，这是一种新的视频生成模型，该模型同时在图像和文本的多模式输入上进行条件。该模型构建在一个称为多模式视频块（MVB）的核心模块上，该模块由用于表示视频特征的常规时空层组成，以及一个脱钩的跨注意层，用于解决外观调节的图像和文本输入。此外，我们仔细设计了模型体系结构，以便可以选择与预训练的图像控制网模块集成，以进行几何视觉条件，而无需与先前的方法相比，无需额外的训练开销。实验表明，与现有模型相比，Moonshot有了多功能多模式调节机制，视觉质量和时间一致性显着改善。此外，该模型可以轻松地用于各种生成应用，例如个性化视频生成，图像动画和视频编辑，揭示了其作为可控视频生成的基本架构的潜力。模型将在https://github.com/salesforce/lavis上公开。

### Few-shot Adaptation of Multi-modal Foundation Models: A Survey 
[[arxiv](https://arxiv.org/abs/2401.01736)] [[cool](https://papers.cool/arxiv/2401.01736)] [[pdf](https://arxiv.org/pdf/2401.01736)]
> **Authors**: Fan Liu,Tianshu Zhang,Wenwen Dai,Wenwen Cai,Xiaocong Zhou,Delong Chen
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 多模式基础模型的几次改编：调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 诸如剪辑之类的多模式（视觉语言）模型正在取代传统的监督前训练模型（例如，基于Imagenet的预训练）作为新一代的视觉基础模型。这些模型具有从数十亿个Internet映像式文本对中学到的强大且一致的语义表示形式，并且可以以零拍的方式应用于各种下游任务。但是，在一些细粒的域（例如医学成像和遥感）中，多模式粉底型模型的性能通常是不足的。因此，许多研究人员已经开始为这些模型探索几乎没有射击的适应方法，逐渐得出了三种主要的技术方法：1）基于及时的方法，2）基于适配器的方法和3）基于外部知识的方法。然而，这个迅速发展的领域在没有系统地组织研究进展的情况下产生了许多结果。因此，在本调查中，我们以几种模型的几种适应方法介绍和分析了研究进步，总结了常用的数据集和实验设置，并比较了不同方法的结果。此外，由于缺乏对现有方法的可靠理论支持，我们得出了多模式模型绑定的少量适应性概括误差。该定理揭示了多模式基础模型的概括误差受三个因素的约束：域间隙，模型容量和样本量。基于此，我们从以下方面提出了三种可能的解决方案：1）自适应领域的概括，2）自适应模型选择和3）自适应知识利用率。

### Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens 
[[arxiv](https://arxiv.org/abs/2401.01674)] [[cool](https://papers.cool/arxiv/2401.01674)] [[pdf](https://arxiv.org/pdf/2401.01674)]
> **Authors**: Dengdi Sun,Yajie Pan,Andong Lu,Chenglong Li,Bin Luo
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 带有时空多模式令牌的变压器RGBT跟踪
- **领域**: 计算机视觉和模式识别
- **摘要**: 许多RGBT跟踪研究主要集中于模态融合设计，同时忽略了目标外观变化的有效处理。尽管某些方法已经引入了历史框架或保险丝并替换初始模板以包含时间信息，但它们有破坏原始目标外观并随着时间的推移积累错误的风险。为了减轻这些局限性，我们提出了一种新型的变压器RGBT跟踪方法，该方法将来自静态多模式模板和变压器中的多模式搜索区域的时空多模式代币混合在一起，以处理目标外观变化，以实现目标的RGBT跟踪。我们引入独立的动态模板令牌以与搜索区域进行交互，嵌入时间信息以解决外观变化，同时还保留了初始静态模板令牌在联合特征提取过程中的参与，以确保保留原始的可靠目标外观信息，以防止由传统时间更新引起的目标外观偏离目标。我们还使用注意机制来通过结合补充模态提示来增强多模式模板令牌的目标特征，并通过注意力机制与多模式动态模板令牌与多模式动态模板令牌相互作用，从而促进多模式实力目标变化信息的运输。我们的模块插入变压器骨干网络，并继承关节特征提取，搜索模板匹配和跨模式相互作用。在三个RGBT基准数据集上进行的广泛实验表明，与其他最先进的跟踪算法相比，所提出的方法在39.1 fps运行时保持了竞争性能。

### Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction 
[[arxiv](https://arxiv.org/abs/2401.01646)] [[cool](https://papers.cool/arxiv/2401.01646)] [[pdf](https://arxiv.org/pdf/2401.01646)]
> **Authors**: Yilan Zhang,Yingxue Xu,Jianqi Chen,Fengying Xie,Hao Chen
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 多模式癌症存活预测的典型信息瓶颈和解开
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式学习显着有益于癌症的生存预测，尤其是病理图像和基因组数据的整合。尽管多模式学习在癌症存活预测方面具有优势，但多模式数据的巨大冗余性阻止了它提取区分和紧凑的信息：（1）大量模型内任务无关的信息差异差异，差异性差异差异，尤其是对于Gigapixel全部幻灯片图像（WSIS），在Patheral of Patheral of Path in of Genomic of Genomic of genomics of genomic in of genomic of genomic sif in of genom in of genomic of genom in of genomic of genomic sif in o'冗余“问题”。（2）模式之间的重复信息主导了多模式数据的表示，这使得特定于模态的信息容易被忽略，从而导致``模式间冗余''问题。为了解决这些问题，我们提出了一个新的框架，原型信息瓶颈和解开（PIBD），该框架由原型信息瓶颈（PIB）模块组成，用于模式内冗余和原型信息分离（PID）模块（PID）模块。具体而言，提出了一系列信息瓶颈PIB，以模拟近似于不同风险水平的一堆实例的原型，可用于选择模态内的判别实例。 PID模块将多模式数据纠缠为紧凑的不同组成部分：在关节原型分布的指导下，模态和模态特定的知识。对五个癌症基准数据集进行的广泛实验证明了我们优于其他方法。

### Context-Aware Interaction Network for RGB-T Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2401.01624)] [[cool](https://papers.cool/arxiv/2401.01624)] [[pdf](https://arxiv.org/pdf/2401.01624)]
> **Authors**: Ying Lv,Zhi Liu,Gongyang Li
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: 13 pages, 7 figures, Accepted by IEEE Transactions on Multimedia 2024
- **标题**: RGB-T语义细分的上下文感知交互网络
- **领域**: 计算机视觉和模式识别
- **摘要**: RGB-T语义分割是一种用于自主驾驶场景理解的关键技术。但是，对于现有的RGB-T语义分割方法，在多个层次之间的信息相互作用中未实现对不同模式之间互补关系的有效探索。为了解决这一问题，为RGB-T语义分段提出了上下文感知的交互网络（Cainet），该网络分段构建了交互空间以利用辅助任务和全局上下文，以明确指导学习。具体而言，我们提出了一个旨在在空间和信道维度中建立多模式特征与长期背景之间的互补关系的上下文感知互补推理（CACR）模块。此外，考虑到全球上下文和详细信息的重要性，我们提出了全球上下文建模（GCM）模块和详细信息聚合（DA）模块，并介绍了特定的辅助监督，以明确指导上下文交互并完善细分图。在MFNET和PST900的两个基准数据集上进行了广泛的实验表明，拟议中的Cainet实现了最先进的性能。该代码可在https://github.com/yinglv1106/cainet上找到。

### Enhancing Representation in Medical Vision-Language Foundation Models via Multi-Scale Information Extraction Techniques 
[[arxiv](https://arxiv.org/abs/2401.01583)] [[cool](https://papers.cool/arxiv/2401.01583)] [[pdf](https://arxiv.org/pdf/2401.01583)]
> **Authors**: Weijian Huang,Cheng Li,Hong-Yu Zhou,Jiarun Liu,Hao Yang,Yong Liang,Guangming Shi,Hairong Zheng,Shanshan Wang
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 通过多尺度信息提取技术在医学视觉语言基础模型中增强表示形式
- **领域**: 计算机视觉和模式识别
- **摘要**: 医疗视觉语言基础模型的发展引起了医学和医疗领域的极大关注，因为它们在各种临床应用中有希望的前景。尽管以前的研究通常集中在单个学习量表上的特征学习上，但缺乏有关整合多规模信息的研究，这可能会阻碍这些特征之间相互加强的潜力。本文旨在通过提出一种有效利用多尺度信息以增强医学基础模型的性能的方法来弥合这一差距。所提出的方法同时利用本地，实例，模式和全局方面的特征，从而促进了模型中的全面表示学习。我们评估了所提出的方法在不同临床任务的六个开源数据集上的有效性，这表明了其增强医疗基础模型性能的能力。

### Oceanship: A Large-Scale Dataset for Underwater Audio Target Recognition 
[[arxiv](https://arxiv.org/abs/2401.02099)] [[cool](https://papers.cool/arxiv/2401.02099)] [[pdf](https://arxiv.org/pdf/2401.02099)]
> **Authors**: Zeyu Li,Suncheng Xiang,Tong Yu,Jingsheng Gao,Jiacheng Ruan,Yanping Hu,Ting Liu,Yuzhuo Fu
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: Accepted by ICIC 2024
- **标题**: Oceanship：一个用于水下音频目标识别的大型数据集
- **领域**: 计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 水下音频的识别在识别船舶运动时起着重要作用。水下目标识别任务在海洋环境保护，船舶辐射噪声，水下噪声控制和沿海船舶调度等地区具有广泛的应用。传统的UATR任务涉及训练网络以从音频数据中提取功能并预测船舶类型。当前的UATR数据集在持续时间和样本数量中都表现出缺点。在本文中，我们提出了Oceanship，这是一个大规模且多样化的水下音频数据集。该数据集包括15个类别，总持续时间为121小时，并包括全面的注释信息，例如坐标，速度，船舶类型和时间戳。我们通过在2021年至2022年之间从海洋通信网络（ONC）数据库中爬行和组织原始通信数据来编译数据集。尽管在一般音频分类中已经建立了良好的音频检索任务，但在水下音频识别的背景下尚未探索它们。利用海洋船数据集，我们介绍了一个名为Oceannet的基线模型，以进行水下音频检索。该模型的召回率为1（r@1）的准确性为67.11％，在DeepShip数据集上的召回率为5（r@5）的准确性为99.13％。

### Exploiting Polarized Material Cues for Robust Car Detection 
[[arxiv](https://arxiv.org/abs/2401.02606)] [[cool](https://papers.cool/arxiv/2401.02606)] [[pdf](https://arxiv.org/pdf/2401.02606)]
> **Authors**: Wen Dong,Haiyang Mei,Ziqi Wei,Ao Jin,Sen Qiu,Qiang Zhang,Xin Yang
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: Accepted by AAAI 2024
- **标题**: 利用两极化的材料提示进行健壮的汽车检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 汽车检测是一项重要任务，是许多自动驾驶功能的关键先决条件。由于不稳定/有限的颜色信息，景点/天气状况和场景的车辆密度构成了重大挑战，以满足对安全性高度准确的看法需求，这阻碍了库尔斯的有意义/歧视性特征的提取。在这项工作中，我们提出了一种基于学习的新型汽车检测方法，该方法利用三色线性极化为额外的提示来消除此类具有挑战性的案例。一个关键的观察是，光波的特征是在各种成像条件下强烈地描述场景对象的内在物理特性，并且与汽车材料（例如金属和玻璃）及其周围环境（例如土壤和树木）的材料的性质密切相关，从而在挑战场景中提供了可靠的和歧视性的特征。为了利用极化提示，我们首先构建了与像素对齐的RGB极化CAR检测数据集，随后我们使用该数据集训练新型的多模式融合网络。我们的CAR检测网络以请求和补充方式动态整合RGB和极化特征，并可以探索所有学习样本中汽车的内在材料特性。我们广泛验证我们的方法，并证明它的表现优于最先进的检测方法。实验结果表明，极化是汽车检测的强大提示。

### CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs 
[[arxiv](https://arxiv.org/abs/2401.02582)] [[cool](https://papers.cool/arxiv/2401.02582)] [[pdf](https://arxiv.org/pdf/2401.02582)]
> **Authors**: Daoan Zhang,Junming Yang,Hanjia Lyu,Zijian Jin,Yuan Yao,Mingkai Chen,Jiebo Luo
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: No comments
- **标题**: COCOT：对比度链，促使具有多个图像输入的大型多模型模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 在探索人工智能（AGI）的发展时，这些模型的关键任务涉及从多个图像输入中解释和处理信息。但是，在这种情况下，大型多模型模型（LMM）遇到了两个问题：（1）缺乏细粒度的感知，以及（2）倾向于将信息跨多个图像融合。我们首先广泛研究LMM在处理多个输入图像时感知细粒度细节的能力。该研究的重点是两个方面：第一个，图像对图像匹配（评估LMM是否可以有效推理并配对相关图像），第二个，多图像到文本的匹配（以评估LMM是否可以准确捕获和汇总详细的图像信息）。我们对包括GPT-4V，Gemini，OpenFlamingo和MMICL在内的各种开源和闭合型大型模型进行评估。为了增强模型性能，我们进一步开发了基于多输入多模型模型的对比度链（COCOT）提示方法。此方法要求LMMS比较多个图像输入之间的相似性和差异，然后指导模型根据确定的相似性和差异回答有关多图像输入的详细问题。我们的实验结果展示了Cocot在增强大型多模型的多图像理解能力方面的熟练程度。

### Image-based Deep Learning for Smart Digital Twins: a Review 
[[arxiv](https://arxiv.org/abs/2401.02523)] [[cool](https://papers.cool/arxiv/2401.02523)] [[pdf](https://arxiv.org/pdf/2401.02523)]
> **Authors**: Md Ruman Islam,Mahadevan Subramaniam,Pei-Chi Huang
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: 12 pages, 2 figures, and 3 tables
- **标题**: 基于图像的智能数字双胞胎的深度学习：评论
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,系统与控制
- **摘要**: 智能数字双胞胎（SDT）越来越多地通过连续数据同化来实际上复制和预测复杂物理系统的行为，从而通过控制系统的动作来优化这些系统的性能。最近，深度学习（DL）模型显着增强了SDT的功能，特别是对于诸如预测性维护，异常检测和优化等任务。在包括医学，工程和教育在内的许多领域中，SDT使用图像数据（基于图像的SDT）观察和学习系统行为并控制其行为。本文通过不断吸收物理系统的图像数据来开发基于图像的SDT的各种方法和相关挑战。本文还讨论了设计和实施SDT的DL模型所涉及的挑战，包括数据采集，处理和解释。此外，还提供了开发新的基于图像的DL方法来开发强大SDT的新方向和机会的见解。这包括使用生成模型进行数据增强，开发多模式DL模型以及探索DL与其他技术（包括5G，Edge Computing和IoT）集成的潜力。在本文中，我们描述了基于图像的SDT，该SDT能够在广泛领域中更广泛地采用数字双DT范式，以及开发新方法以提高SDT在复制，预测和优化复杂系统行为方面的能力。

### FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients 
[[arxiv](https://arxiv.org/abs/2401.02433)] [[cool](https://papers.cool/arxiv/2401.02433)] [[pdf](https://arxiv.org/pdf/2401.02433)]
> **Authors**: DaiXun Li,Weiying Xie,ZiXuan Wang,YiBing Lu,Yunsong Li,Leyuan Fang
> **First submission**: 2023-11-15
> **First announcement**: 2024-01-05
> **comment**: No comments
- **标题**: Feddiff：多模式和多客户的扩散模型驱动联合学习
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 随着成像传感器技术在遥感领域的快速发展，多模式遥感数据融合已成为土地覆盖分类任务的重要研究方向。尽管扩散模型在生成模型和图像分类任务中取得了长足的进步，但现有模型主要集中于单模式和单一晶格控制，也就是说，扩散过程由单个计算节点中的单个模态驱动。为了促进客户的异质数据的安全融合，有必要启用分布式的多模式控制，例如将组织A的高光谱数据和组织B的LIDAR数据合并在每个基站客户端上。在这项研究中，我们提出了一个多模式的协作扩散联合学习框架，称为Feddiff。我们的框架建立了双支化扩散模型特征提取设置，其中两个模态数据被输入到编码器的单独分支中。我们的关键见解是，从不同方式驱动的扩散模型就可以建立双边连接的潜在降解步骤而言是固有的。考虑到多个客户之间的私人和有效沟通的挑战，我们将扩散模型嵌入了联合学习的交流结构中，并引入了轻量级的通信模块。定性和定量实验在图像质量和条件一致性方面验证了我们框架的优越性。

### ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2401.02384)] [[cool](https://papers.cool/arxiv/2401.02384)] [[pdf](https://arxiv.org/pdf/2401.02384)]
> **Authors**: Fanqing Meng,Wenqi Shao,Quanfeng Lu,Peng Gao,Kaipeng Zhang,Yu Qiao,Ping Luo
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: Updated and corrected experimental results, removal of inappropriate experiments, and a more comprehensive experimental setup
- **标题**: ChartAssistant：通过图表到桌子的预训练和多任务指令调整的通用图表多模型模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 图表在数据可视化，了解数据模式和知情决策中起着至关重要的作用。但是，它们的图形元素（例如，条，线条）和文本组件（例如标签，传奇）的独特组合对通用多模型构成了挑战。尽管在图表数据上训练的视觉模型在理解方面表现出色，但它们在概括方面挣扎。为了应对这些挑战，我们提出了ChartAssistant，这是一种基于图表的视觉语言模型，用于通用图表理解和推理。 ChartAssistant的杠杆作用是Chartsft，这是一个综合数据集，涵盖了具有基本（例如条和派）和专业（例如雷达和气泡）图表类型的不同图表相关任务。它进行了一个两阶段的培训过程，从预先培训图表到桌子解析到对齐图表和文本，然后进行多任务指令遵循微调。这种方法使ChartAssistant能够在各种图表任务中实现竞争性能。实验结果表明，最先进的Unichart和Chartllama方法的性能取得了显着的增长，尤其是在用零拍设置的现实世界图表数据上胜过它们。代码和数据可在https://github.com/opengvlab/chartast上获得。

### LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model 
[[arxiv](https://arxiv.org/abs/2401.02330)] [[cool](https://papers.cool/arxiv/2401.02330)] [[pdf](https://arxiv.org/pdf/2401.02330)]
> **Authors**: Yichen Zhu,Minjie Zhu,Ning Liu,Zhicai Ou,Xiaofeng Mou,Jian Tang
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: The datasets were incomplete as they did not include all the necessary copyrights
- **标题**: Llava-Phi：小语言模型有效的多模式助手
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 在本文中，我们介绍了Llava-$φ$（Llava-Phi），这是一位有效的多模式助手，利用最近高级小型语言模型PHI-2的力量，以促进多模式对话。 llava-phi标志着紧凑的多模式模型领域的显着进步。它表明，即使较小的语言模型，只有2.7b参数的较小语言模型也可以有效地进行复杂的对话，以整合文本和视觉元素，只要它们接受了高质量的语料库的培训。我们的模型在包括视觉理解，推理和基于知识的感知的公开基准上提供了值得称赞的绩效。除了在多模式对话任务中的出色表现外，我们的模型还为需要实时互动的时间敏感环境和系统（例如体现的代理）开辟了新的途径。它突出了较小语言模型的潜力，以达到复杂的理解和互动水平，同时保持更高的资源效率。该项目可在{https://github.com/zhuyiche/llava-phi}上获得。

### TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection 
[[arxiv](https://arxiv.org/abs/2401.02309)] [[cool](https://papers.cool/arxiv/2401.02309)] [[pdf](https://arxiv.org/pdf/2401.02309)]
> **Authors**: Hao Sun,Mingyao Zhou,Wenjing Chen,Wei Xie
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: Accepted by AAAI-24
- **标题**: TR-DETR：联合力矩检索的任务重点变压器并突出显示检测
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 基于自然语言查询的视频力矩检索（MR）和突出显示检测（HD）是两个高度相关的任务，旨在在视频中获得相关时刻并突出显示每个视频剪辑的分数。最近，几种方法致力于建立基于DITR的网络，以共同解决MR和HD。这些方法只需在多模式特征提取和特征交互之后添加两个独立的任务头，从而实现良好的性能。然而，这些方法不足以使两个任务之间的相互关系。在本文中，我们提出了一个基于DETR（TRR）的任务重点变压器，该变压器的重点是探索MR和HD之间的固有互惠。具体而言，首先构建了局部全球多模式对齐模块，该模块是将各种模态到共享潜在空间保持一致的特征。随后，视觉功能细化旨在消除来自模态相互作用的视觉特征中的查询 -  iRrelevant信息。最后，通过利用MR和HD之间的互惠来，构建了一个任务合作模块，以完善检索管道和突出显示分数预测过程。 QVHighlights，Charades-STA和TVSUM数据集的全面实验表明，TR-DETR优于现有的最新方法。代码可在\ url {https://github.com/mingyao1120/tr-detr}中找到。

### Prompt Decoupling for Text-to-Image Person Re-identification 
[[arxiv](https://arxiv.org/abs/2401.02173)] [[cool](https://papers.cool/arxiv/2401.02173)] [[pdf](https://arxiv.org/pdf/2401.02173)]
> **Authors**: Weihao Li,Lei Tan,Pingyang Dai,Yan Zhang
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: No comments
- **标题**: 及时解耦，以重新识别文本对象
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 文本对象的重新识别（TIREID）旨在通过文本描述查询从图像库中检索目标人员。最近，诸如剪辑之类的预训练的视觉模型吸引了大量关注，并且由于其具有强大的语义概念学习和丰富的多模式知识的能力，因此已广泛用于此任务。但是，最近的基于夹的TireID方法通常依赖于整个网络的直接微调来调整剪辑模型的TireID任务。尽管这些方法在此主题上表现出竞争性的表现，但它们是最佳的，因为它们需要同时进行域的适应和任务适应。为了解决这个问题，我们试图在培训阶段将这两个过程解散。具体来说，我们介绍了及时的调整策略，以实现域的适应性，并提出了一种两阶段的培训方法，以解开与任务适应的域适应性。在第一阶段，我们将两个编码器从剪辑中冻结，而仅专注于优化提示，以减轻剪辑和下游任务的原始培训数据之间的域间隙。在第二阶段，我们维护固定的提示并微调剪辑模型，以优先捕获细粒度信息，这更适合于TireID任务。最后，我们在三个广泛使用的数据集上评估了方法的有效性。与直接微调的方法相比，我们的方法可取得重大改进。

### GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation 
[[arxiv](https://arxiv.org/abs/2401.02142)] [[cool](https://papers.cool/arxiv/2401.02142)] [[pdf](https://arxiv.org/pdf/2401.02142)]
> **Authors**: Xuehao Gao,Yang Yang,Zhenyu Xie,Shaoyi Du,Zhongqian Sun,Yang Wu
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: Accepted by IEEE Transactions on Visualization and Computer Graphics (2024)
- **标题**: 猜测：逐渐丰富文本驱动的人类运动产生的合成
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们提出了一个新型的基于扩散的生成框架，用于文本驱动的人类运动综合，该框架利用了一种逐渐丰富合成的策略（猜测为缩写）。该策略通过将详细骨骼的身体关节分组为紧密的语义接近，然后用单个身体零件节点将每个关节组替换为每个关节组，从而建立了生成目标。这样的操作递归地抽象出人的姿势，以在多个粒度水平上更粗糙和更粗糙的骨骼。随着抽象水平逐渐增加，人类运动变得越来越简洁，稳定，从而使跨模式运动合成任务受益匪浅。然后将整个文本驱动的人类运动综合问题分为多个抽象级别，并使用具有级联潜在扩散模型的多阶段生成框架解决：初始生成器首先从给定的文本描述中生成最粗糙的人类运动猜测；然后，一系列连续的发电机根据文本描述和先前的合成结果逐渐丰富了运动细节。值得注意的是，我们将猜测与提出的动态多条件融合机制进一步整合在一起，以动态平衡给定文本条件的合作效应，并在不同的生成阶段综合了粗运动及时。大规模数据集上的广泛实验验证了，从准确性，现实性和多样性方面，猜测大量优于现有的最新方法。代码可从https://github.com/xuehao-gao/guess获得。

### Bayesian Unsupervised Disentanglement of Anatomy and Geometry for Deep Groupwise Image Registration 
[[arxiv](https://arxiv.org/abs/2401.02141)] [[cool](https://papers.cool/arxiv/2401.02141)] [[pdf](https://arxiv.org/pdf/2401.02141)]
> **Authors**: Xinzhe Luo,Xin Wang,Linda Shapiro,Chun Yuan,Jianfeng Feng,Xiahai Zhuang
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: No comments
- **标题**: 贝叶斯的无监督解剖学解剖学和几何形状，以进行深层图像登记
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了多模式组图像注册的一般贝叶斯学习框架。该方法建立在图像生成过程的概率建模的基础上，在该过程中，观察到的图像的基本共同解剖结构和几何变化被显式地将其视为潜在变量。因此，通过分层贝叶斯推断实现了组的图像注册。我们提出了一种新颖的分层自动编码体系结构，以实现潜在变量的推理过程，其中可以以数学上可解释的方式明确估计注册参数。值得注意的是，这种新的范式在无监督的闭环自我重新构造过程中学习了GroupWise图像注册，从而减少了设计基于复杂图像的相似性度量的负担。计算有效的分离网络体系结构也可以固有地扩展且灵活，从而可以在具有可变大小的大规模图像组上进行群组注册。此外，通过分离学习从多模式图像中推断出的结构表示能够通过视觉语义捕获观测值的潜在解剖结构。进行了广泛的实验以验证所提出的框架，包括来自心脏，大脑和腹部医学图像的四个不同数据集。结果证明了我们方法在准确性，效率，可伸缩性和可解释性方面的优越性优于基于常规相似性的方法。

### Explore Human Parsing Modality for Action Recognition 
[[arxiv](https://arxiv.org/abs/2401.02138)] [[cool](https://papers.cool/arxiv/2401.02138)] [[pdf](https://arxiv.org/pdf/2401.02138)]
> **Authors**: Jinfu Liu,Runwei Ding,Yuhang Wen,Nan Dai,Fanyang Meng,Shen Zhao,Mengyuan Liu
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: arXiv admin note: text overlap with arXiv:2307.07977
- **标题**: 探索人类解析方式以识别行动
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于多模式的动作识别方法已使用姿势和RGB模式取得了很高的成功。但是，由于模态限制，骨骼序列缺乏外观描绘，而RGB图像却遭受了无关的噪音。为了解决这个问题，我们将人类解析特征图作为一种新型方式引入，因为它可以选择性地保留身体部位的有效语义特征，同时滤除最无关的噪声。我们提出了一个新的双分支框架，称为集合人类解析和姿势网络（EPP-NET），该框架是第一个利用骨骼和人类解析方式进行行动识别的人。第一个人类姿势分支在图形卷积网络中为稳健的骨骼提供了姿势姿势特征，而第二个人解析分支也利用描绘解析特征图图来通过卷积骨干来模型解析节日。这两个高级功能将通过晚期融合策略有效地组合，以更好地识别。对NTU RGB+D和NTU RGB+D 120基准的广泛实验始终验证我们提出的EPP-NET的有效性，这表现优于现有的动作识别方法。我们的代码可在以下网址提供：https：//github.com/liujf69/epp-net-action。

### SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment 
[[arxiv](https://arxiv.org/abs/2401.02137)] [[cool](https://papers.cool/arxiv/2401.02137)] [[pdf](https://arxiv.org/pdf/2401.02137)]
> **Authors**: Ziping Ma,Furong Xu,Jian Liu,Ming Yang,Qingpei Guo
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: No comments
- **标题**: Sycoca：对比对比字幕符，并带有细心的掩盖多模式比对
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 语言和视觉之间的多模式对齐是当前视觉模型研究中的基本话题。对比字幕符（COCA）作为代表性方法，将对比性语言图像预处理（剪辑）和图像标题（IC）集成到一个统一的框架中，从而产生了令人印象深刻的结果。剪辑对整个图像和句子的全局表示对双向约束施加了双向约束。尽管IC在本地表示上进行了单向图像到文本的生成，但它对本地文本对图像重建的任何限制都缺乏限制，这限制了与文本对齐时在细粒度上理解图像的能力。为了从全球和本地视角实现多模式对齐，本文提出了对称对比字幕符（Sycoca），该对比度会引入全球和局部表示级别的图像和文本上的双向相互作用。具体而言，我们根据ITC和IC头扩展了文本引导的蒙版图像建模（TG-MIM）头。改进的Sycoca可以进一步利用文本提示来重建上下文图像和视觉提示，以预测文本内容。在实施双向局部交互时，图像的局部内容往往与其文本描述混乱或无关。因此，我们采用细心的掩蔽策略来选择有效的图像贴片进行交互。对五项视觉任务的广泛实验，包括图像文本检索，图像捕获，视觉问题答案以及零射击/芬特图像分类，验证我们提出的方法的有效性。

### 3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding 
[[arxiv](https://arxiv.org/abs/2401.03201)] [[cool](https://papers.cool/arxiv/2401.03201)] [[pdf](https://arxiv.org/pdf/2401.03201)]
> **Authors**: Zeju Li,Chao Zhang,Xiaoyan Wang,Ruilong Ren,Yifan Xu,Ruifei Ma,Xiangde Liu
> **First submission**: 2024-01-06
> **First announcement**: 2024-01-08
> **comment**: 9 pages, 5 figures
- **标题**: 3DMIT：3D多模式指令调整场景理解
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 多模式大语言模型（MLLM）在理解视觉和语言信息方面的显着潜力已得到广泛认可。但是，与2D对应物相比，3D场景的稀缺性以及现有方法不足在理解LLMS的3D场景时，带来了重大挑战。作为响应，我们收集并构建了一个广泛的数据集，其中包括针对3D场景量身定制的75K指令 - 响应对。该数据集解决了与3D VQA，3D接地和3D对话有关的任务。为了进一步增强3D空间信息在LLM中的集成，我们引入了一种新颖而有效的及时调整范式3DMIT。该范式消除了3D场景和语言之间的对齐阶段，并使用3D模式信息扩展了指令提示，包括整个场景和分段对象。我们评估了方法在3D场景域中各种任务中的方法的有效性，并发现我们的方法是丰富LLM对3D世界的理解的战略手段。我们的代码可在https://github.com/staymylove/3DMIT上找到。

### Multimodal Informative ViT: Information Aggregation and Distribution for Hyperspectral and LiDAR Classification 
[[arxiv](https://arxiv.org/abs/2401.03179)] [[cool](https://papers.cool/arxiv/2401.03179)] [[pdf](https://arxiv.org/pdf/2401.03179)]
> **Authors**: Jiaqing Zhang,Jie Lei,Weiying Xie,Geng Yang,Daixun Li,Yunsong Li
> **First submission**: 2024-01-06
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 多模式信息vit：高光谱和激光雷达分类的信息聚集和分布
- **领域**: 计算机视觉和模式识别
- **摘要**: 在多模式的土地覆盖分类（MLCC）中，一个普遍的挑战是数据分布的冗余，在这种情况下，来自多种方式的无关信息会阻碍其独特功能的有效整合。为了解决这个问题，我们介绍了具有创新信息聚集机制的系统多模式信息VIT（MIVIT）。这种方法重新定义了冗余水平，并将绩效感知的要素整合到融合的表示中，从而促进了向前和后方方向学习语义的学习。 MIVIT在每种模式单独和融合特征的经验分布中显着降低了冗余而脱颖而出。它采用定向的注意融合（OAF）来跨水平和垂直尺寸的方式提取较浅的局部特征，以及通过长期注意力提取深层全局特征的变压器特征提取器。我们还基于共同信息提出了一个信息聚合约束（IAC），旨在删除冗余信息并保留嵌入式特征中的互补信息。此外，MIVIT中的信息分布流（IDF）通过在不同模式的特征图上分配全局分类信息来增强性能意识。该体系结构还通过轻巧独立的模态分类器来解决缺失的模态挑战，从而减少了通常与变压器关联的计算负载。我们的结果表明，Mivit模式之间的双向骨料分布机制非常有效，在三个多模式数据集中达到了95.56％的平均总准确度。这种性能超过了MLCC中当前的最新方法。可以在https://github.com/icey-zhang/mivit上访问MIVIT的代码。

### Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks 
[[arxiv](https://arxiv.org/abs/2401.03177)] [[cool](https://papers.cool/arxiv/2401.03177)] [[pdf](https://arxiv.org/pdf/2401.03177)]
> **Authors**: Qian Li,Lixin Su,Jiashu Zhao,Long Xia,Hengyi Cai,Suqi Cheng,Hengzhu Tang,Junfeng Wang,Dawei Yin
> **First submission**: 2024-01-06
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 文本视频通过变异多模式超图网络检索
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 文本视频检索是一项具有挑战性的任务，旨在确定给定文本查询的相关视频。与传统的文本检索相比，文本视频检索的主要障碍是查询的文本性质与视频内容的视觉丰富性之间的语义差距。先前的作品主要集中于通过精细汇总单词框架匹配信号来对齐查询和视频。受到人类认知过程的启发，该过程判断文本和视频之间的相关性，由于视频内容的连续且复杂的性质，判断需要高阶匹配信号。在本文中，我们提出了块级文本视频匹配，其中提取查询块以描述特定的检索单元，并且将视频块细分为视频的不同剪辑。我们将块级匹配的形式匹配为视频的查询和帧之间的N- ARY相关模型，并引入了用于N- ARY相关建模的多模式超图。通过将文本单元和视频帧表示为节点，并使用HyperEdges描绘其关系，可以构建多模式超图。这样，可以将查询和视频在高阶语义空间中对齐。此外，为了增强模型的泛化能力，提取的特征被馈入用于计算的变分推断成分，从而在高斯分布下获得了变异表示。超图和变异推理的结合使我们的模型可以捕获文本和视觉内容之间的复杂的n- ary相互作用。实验结果表明，我们提出的方法在文本视频检索任务上实现了最先进的性能。

### CaMML: Context-Aware Multimodal Learner for Large Models 
[[arxiv](https://arxiv.org/abs/2401.03149)] [[cool](https://papers.cool/arxiv/2401.03149)] [[pdf](https://arxiv.org/pdf/2401.03149)]
> **Authors**: Yixin Chen,Shuai Zhang,Boran Han,Tong He,Bo Li
> **First submission**: 2024-01-06
> **First announcement**: 2024-01-08
> **comment**: Preprint
- **标题**: CAMML：大型模型的上下文感知的多模式学习者
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项工作中，我们介绍了上下文感知的多模式学习者（CAMML），以调整大型多模型模型（LMMS）。 CAMML是一种轻巧的模块，其精心制作是将多模式上下文样本无缝整合到大型模型中，从而赋予该模型从类似，特定领域的最新信息中获得知识的能力，并做出基础的推论。重要的是，由于其层次设计，CAMML是高度可扩展的，并且可以有效地处理冗长的多模式上下文示例。基于CAMML，我们开发了两个多模型CAMML-7B和CAMML-13B，它们在一系列基准数据集中显示出出色的性能，用于多模式任务。值得注意的是，CAMML-13B在十个公认的多模式基准数据集上实现了最先进的性能，在没有任何外部资源的情况下，以明显的利润超过了Llava-1.5（13b）。此外，我们进行了广泛的烧烤研究，以检查CAMML的内部工作，并进行了定性分析，以展示其在处理现实世界中挑战性案例方面的有效性。代码和模型可在以下网址提供：https：//github.com/amazon-science/camml。

### Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2401.03145)] [[cool](https://papers.cool/arxiv/2401.03145)] [[pdf](https://arxiv.org/pdf/2401.03145)]
> **Authors**: Yuanpeng Tu,Boshen Zhang,Liang Liu,Yuxi Li,Xuhai Chen,Jiangning Zhang,Yabiao Wang,Chengjie Wang,Cai Rong Zhao
> **First submission**: 2024-01-06
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 3D工业异常检测的自学特征适应
- **领域**: 计算机视觉和模式识别
- **摘要**: 工业异常检测通常被视为无监督的任务，旨在仅使用正常训练样本来定位缺陷。最近，已经提出了许多2D异常检测方法并获得了有希望的结果，但是，仅使用2D RGB数据作为输入不足以识别不察觉到的几何表面异常。因此，在这项工作中，我们专注于多模式异常检测。具体而言，我们研究了试图利用在大规模视觉数据集（即ImageNet）构建特征数据库中预先训练的模型的早期多模式方法。而且我们从经验上发现，直接使用这些预训练的模型不是最佳的，它可能无法检测到细微的缺陷，或者将异常特征误认为正常的特征。这可能归因于目标工业数据和源数据之间的领域差距。要解决这个问题，我们提出了一种局部到全球自我监督的特征适应（LSFA）方法，以确定适应器的拟合并学习任务导向于任务的代表性，以实现Anomaly检测。 representation quality and consistency in the inference stage.Extensive experiments demonstrate that our method not only brings a significant performance boost to feature embedding based approaches, but also outperforms previous State-of-The-Art (SoTA) methods prominently on both MVTec-3D AD and Eyecandies datasets, e.g., LSFA achieves 97.1% I-AUROC on MVTec-3D, surpass previous SoTA by +3.4％。

### Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.03105)] [[cool](https://papers.cool/arxiv/2401.03105)] [[pdf](https://arxiv.org/pdf/2401.03105)]
> **Authors**: Xin He,Longhui Wei,Lingxi Xie,Qi Tian
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 合并视觉专家以解决多式模式大语模型中的信息损失
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 多模式的大语言模型（MLLM）正在经历快速增长，最近几个月产生了许多值得注意的贡献。流行的趋势涉及采用数据驱动的方法，其中收集了各种指令遵循数据集。但是，这些方法中的一个普遍的挑战仍然存在，特别是与有限的视觉感知能力有关，因为剪辑状的编码器用于从输入中提取视觉信息。尽管这些编码器已在数十亿个图像文本对上进行了预训练，但它们仍然应对信息损失困境，因为该文本字幕仅部分捕获图像中描述的内容。为了解决这一局限性，本文提出了通过experter的混合物来提高MLLM的视觉感知能力，从而提高知识增强机制。具体而言，我们引入了一种新颖的方法，该方法将多任务编码器和视觉工具纳入现有的MLLMS培训和推理管道中，旨在提供更全面，更准确的视觉输入汇总。广泛的实验已经评估了其推进MLLM的有效性，展示了通过整合视觉专家来提高的视觉感知。

### Learning Multimodal Volumetric Features for Large-Scale Neuron Tracing 
[[arxiv](https://arxiv.org/abs/2401.03043)] [[cool](https://papers.cool/arxiv/2401.03043)] [[pdf](https://arxiv.org/pdf/2401.03043)]
> **Authors**: Qihua Chen,Xuejin Chen,Chenxuan Wang,Yixiong Liu,Zhiwei Xiong,Feng Wu
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: 9 pages, 6 figures, AAAI 2024 accepted
- **标题**: 学习大规模神经元跟踪的多模式体积特征
- **领域**: 计算机视觉和模式识别
- **摘要**: 电子显微镜（EM）数据的当前神经元重建管道通常包括自动图像分割，然后进行广泛的人类专家校对。在这项工作中，我们旨在通过预测过度分段的神经元部分之间的连通性来减少人类工作量，同时考虑显微镜图像和3D形态特征，类似于人类校对工作流。为此，我们首先构建了一个名为FlyTracing的数据集，其中包含数百万个段的成对连接，扩展了整个蝇脑，这比神经元段连接的现有数据集大三个数量级。为了从连通性注释中学习复杂的生物成像特征，我们提出了一种新型的连接性对比度学习方法，以生成密集的体积EM图像嵌入。可以轻松地将学习的嵌入方式与基于任何点或体素的形态表示形式合并，以进行自动神经元跟踪。在识别整个蝇脑的分裂误差时，不同组合方案的广泛比较表明了所提出的方法的优越性，尤其是对于包含严重成像伪像的位置，例如截面缺失和未对准。该数据集和代码可在https://github.com/levishery/flywire-neuron-tracing上找到。

### Fus-MAE: A cross-attention-based data fusion approach for Masked Autoencoders in remote sensing 
[[arxiv](https://arxiv.org/abs/2401.02764)] [[cool](https://papers.cool/arxiv/2401.02764)] [[pdf](https://arxiv.org/pdf/2401.02764)]
> **Authors**: Hugo Chan-To-Hing,Bharadwaj Veeravalli
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: FUS-MAE：一种基于跨注意的数据融合方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 鉴于它们有可能减轻与策划大型卫星图像数据集相关的高标签成本的潜力，用于代表性学习的自我监督框架最近引起了遥感社区的兴趣。在多模式数据融合的领域中，尽管经常使用的对比度学习方法可以帮助弥合不同的传感器类型之间的域间隙，但它们依赖于需要专业知识和仔细设计的数据增强技术，尤其是多光谱遥感数据。避免这些局限性的一种可能但几乎没有研究的方法是使用基于掩盖图像建模的训练策略。在本文中，我们介绍了Fus-Mae，这是一种基于蒙版自动编码器的自制学习框架，该框架使用交叉注意力来执行合成孔径雷达和多光谱光学数据之间的早期和特征级数据融合 - 两种具有重要域间隙的模式。我们的经验发现表明，FUS-MAE可以有效地与针对SAR-OCTICATION数据融合的对比度学习策略竞争，并胜过其他在较大语料库中训练的蒙面的AutoEncoders框架。

### Reading Between the Frames: Multi-Modal Depression Detection in Videos from Non-Verbal Cues 
[[arxiv](https://arxiv.org/abs/2401.02746)] [[cool](https://papers.cool/arxiv/2401.02746)] [[pdf](https://arxiv.org/pdf/2401.02746)]
> **Authors**: David Gimeno-Gómez,Ana-Maria Bucur,Adrian Cosma,Carlos-David Martínez-Hinarejos,Paolo Rosso
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: Accepted at 46th European Conference on Information Retrieval (ECIR 2024)
- **标题**: 帧之间的读数：来自非语言提示的视频中的多模式抑郁检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 抑郁症是全球残疾的重要贡献者，会影响大部分人口。从社交媒体文本中检测到抑郁症的努力一直很普遍，但只有少数作品探讨了用户生成的视频内容的抑郁症检测。在这项工作中，我们通过提出一个简单而灵活的多模式时间模型来解决这一研究差距，该模型能够辨别非语言抑郁症线索与嘈杂的现实世界视频中的各种方式。我们表明，对于野外视频，使用其他高级非语言提示对于实现良好的性能至关重要，我们提取和处理的音频语音嵌入，脸部情感嵌入，面部，身体和手动地标，凝视和闪烁和闪烁的信息。通过广泛的实验，我们表明我们的模型在三个关键基准数据集上实现了最先进的结果，从而从视频中大幅度差距从视频中检测到抑郁症。我们的代码在GitHub上公开可用。

### Complementary Information Mutual Learning for Multimodality Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2401.02717)] [[cool](https://papers.cool/arxiv/2401.02717)] [[pdf](https://arxiv.org/pdf/2401.02717)]
> **Authors**: Chuyun Shen,Wenhao Li,Haoqing Chen,Xiaoling Wang,Fengping Zhu,Yuxin Li,Xiangfeng Wang,Bo Jin
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: 35 pages, 18 figures
- **标题**: 多模式医学图像细分的互补信息相互学习
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 由于医学成像的局限性和肿瘤信号的多样性，放射科医生必须利用多个模态图像进行肿瘤分割和诊断。这导致了分割中多模式学习的发展。但是，模式之间的冗余为现有的基于减法的关节学习方法带来了挑战，例如误解了模态的重要性，忽略了特定的模态信息以及增加认知载荷。这些棘手的问题最终降低了细分准确性，并增加了过度拟合的风险。本文介绍了互补信息相互学习（CIML）框架，该框架可以数学上建模并解决模式间冗余信息的负面影响。 CIML通过归纳偏见驱动的任务分解和基于消息传递的冗余过滤采用加法的想法，并消除模式间冗余信息。 CIML首先根据专家的先验知识将多模式分割任务分解为多个子任务，从而最大程度地降低了模式之间的信息依赖性。此外，CIML引入了一个方案，在该方案中，每种模式都可以通过消息传递从其他模式中提取信息。为了实现提取信息的非还原性，冗余过滤被转变为受变异信息瓶颈启发的互补信息学习。互补的信息学习过程可以通过变异推理和跨模式空间注意力有效地解决。验证任务和标准基准的数值结果表明，CIML有效地消除了模态之间的冗余信息，超过了SOTA方法的验证精度和分割效果。

### VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework for Multi-Modal 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2401.02702)] [[cool](https://papers.cool/arxiv/2401.02702)] [[pdf](https://arxiv.org/pdf/2401.02702)]
> **Authors**: Ziying Song,Guoxin Zhang,Jun Xie,Lin Liu,Caiyan Jia,Shaoqing Xu,Zhepeng Wang
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: ef:IEEE Transactions on Geoscience and Remote Sensing, vol. 61, 2023, pp. 1-12
- **标题**: VoxelNextFusion：用于多模式3D对象检测的简单，统一和有效的体素融合框架
- **领域**: 计算机视觉和模式识别
- **摘要**: LIDAR-CAMERA融合可以通过利用深度感知的LiDar点和语义丰富图像之间的互补信息来提高3D对象检测的性能。现有的基于体素的方法以一对一的方式将稀疏体素特征与密集的图像特征融合在一起时面临重大挑战，从而导致图像的优势丧失，包括语义和连续性信息，导致在长距离远距离的情况下，尤其是在长距离的情况下。在本文中，我们提出了VoxelNextFusion，这是一种专门为基于Voxel的方法设计的多模式3D对象检测框架，该框架有效地弥合了稀疏点云​​和密集图像之间的间隙。特别是，我们提出了一个基于体素的图像管道，该管道涉及将点云投射到图像上，以获得像素和补丁级特征。然后使用自我注意事项融合这些特征以获得组合表示。此外，为了解决补丁中存在的背景特征问题，我们提出了一个特征重要的模块，该模块有效地区分了前景和背景特征，从而最大程度地减少了背景特征的影响。在广泛使用的Kitti和Nuscenes 3D对象检测基准上进行了广泛的实验。值得注意的是，与KITTI测试数据集中的Voxel R-CNN基线相比

### Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging 
[[arxiv](https://arxiv.org/abs/2401.04317)] [[cool](https://papers.cool/arxiv/2401.04317)] [[pdf](https://arxiv.org/pdf/2401.04317)]
> **Authors**: Jianyang Shi,Bowen Zhang,Amartansh Dubey,Ross Murch,Liwen Jing
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: No comments
- **标题**: 视力重新构想：WiFi室内成像中的AI驱动突破
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 室内成像是机器人技术和互联网的关键任务。 WiFi作为无所不在的信号是进行被动成像并将最新信息同步到所有连接设备的有前途的候选人。这是将WiFi室内成像视为多模式图像生成任务的第一项研究工作，将测量的WiFi功率转换为高分辨率的室内图像。我们提出的WiFi-gen网络达到了形状重建精度，这是基于物理模型的反转方法实现的275％。此外，特雷切特的距离得分已显着降低了82％。为了检查该任务模型的有效性，第一个大规模数据集释放出包含80,000对WiFi信号和成像目标。我们的模型吸收了基于模型的方法的挑战，包括非线性，不确定性和不确定性，成为我们生成AI网络的大量参数。该网络还旨在最适合测量的WiFi信号和所需的成像输出。为了获得可重复性，我们将在接受后发布数据和代码。

### FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild 
[[arxiv](https://arxiv.org/abs/2401.04210)] [[cool](https://papers.cool/arxiv/2401.04210)] [[pdf](https://arxiv.org/pdf/2401.04210)]
> **Authors**: Zhi-Song Liu,Robin Courant,Vicky Kalogeiton
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: 22 pages, 14 figures
- **标题**: Funnynet-W：野外视频中有趣时刻的多模式学习
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体,声音,音频和语音处理
- **摘要**: 在观看喜剧时，自动了解有趣的时刻（即，使人们发笑的时刻）与各种特征（例如肢体语言，对话和文化）相关时具有挑战性。在本文中，我们提出了Funnynet-W，该模型依赖于跨和自我注意的视觉，音频和文本数据来预测视频中有趣的时刻。 Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a大语言模型。为了获取用于培训的标签，我们提出了一种无监督的方法，以发现和标记有趣的音频时刻。我们在五个数据集上提供实验：情景喜剧TBBT，MHD，Mustard，Friends和Ted Talk Ur-Funny。广泛的实验和分析表明，Funnynet-W成功利用了视觉，听觉和文字提示来识别有趣的时刻，而我们的发现揭示了Funnynet-W的能力，可以预测野外有趣的时刻。 Funnynet-W设置了新的艺术状态，以使用和不使用地面真相信息，并在所有数据集上使用多模式提示进行有趣的时刻检测。

### Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification 
[[arxiv](https://arxiv.org/abs/2401.04154)] [[cool](https://papers.cool/arxiv/2401.04154)] [[pdf](https://arxiv.org/pdf/2401.04154)]
> **Authors**: Wentao Zhu
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: Accepted by WACV 2024; well-formatted PDF is in https://drive.google.com/file/d/1qvW52lamsvNGMCqPS7q8g8L4NaR_LlbR/view?usp=sharing. arXiv admin note: text overlap with arXiv:2401.04023
- **标题**: 有效的选择性音频遮罩的多模式瓶颈变压器用于音频 - 视频分类
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体,声音,音频和语音处理
- **摘要**: 音频和视频是主流媒体平台中最常见的两个方式，例如YouTube。为了有效地从多模式视频中学习，在这项工作中，我们提出了一种新颖的音频视频识别方法，称为音频变性器AVT，利用视频变压器的有效时空表示，以提高动作识别的准确性。对于多模式融合，只需在跨模式变压器中串联多模式令牌，就需要大量的计算和内存资源，我们就会通过音频 - 视频瓶颈变压器降低交叉模式的复杂性。为了提高多模式变压器的学习效率，我们将自我监管的目标（即音频视频对比度学习，音频视频匹配以及遮盖的音频和视频学习）整合到AVT培训中，将其映射到多样化的音频和视频表示形式中。我们进一步提出了一个蒙版的音频段损失，以学习AVT的语义音频活动。对三个公共数据集和两个内部数据集进行的大量实验和消融研究始终证明了拟议的AVT的有效性。具体而言，AVT的表现优于先前在动力学声中的最先进的对手8％。通过利用音频信号，AVT还超过了VGGSOUND的先前最先进的视频变压器[25]。与以前最先进的多模式方法之一MBT [32]相比，AVT在拖失板方面的效率提高了1.3％，而Epic-Kitchens-100的精度提高了3.8％。

### Two-stream joint matching method based on contrastive learning for few-shot action recognition 
[[arxiv](https://arxiv.org/abs/2401.04150)] [[cool](https://papers.cool/arxiv/2401.04150)] [[pdf](https://arxiv.org/pdf/2401.04150)]
> **Authors**: Long Deng,Ziqiang Li,Bingxin Zhou,Zhongming Chen,Ao Li,Yongxin Ge
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: No comments
- **标题**: 基于对比度学习的两潮关节匹配方法，以识别几次动作
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管基于公制学习范式的射击行动识别很少，但取得了巨大的成功，但它无法解决以下问题：（1）动作关系建模不足和多模式信息的利用不足； （2）处理视频匹配问题的挑战，其长度和速度不同，视频匹配问题因视频次序不对而不是。为了解决这些问题，我们提出了一种基于对比度学习（TSJM）的两流关节匹配方法，该方法由两个模块组成：多模式对比度学习模块（MCL）和关节匹配模块（JMM）。 MCL的目的是广泛研究模式间相互信息关系，从而彻底提取模态信息以增强作用关系的建模。 JMM旨在同时解决上述视频匹配问题。在两个广泛使用的少数Shot Action识别数据集（即SSV2和动力学）上评估了所提出方法的有效性。还进行了全面的消融实验，以证实我们提出的方法的功效。

### Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video Classification 
[[arxiv](https://arxiv.org/abs/2401.04023)] [[cool](https://papers.cool/arxiv/2401.04023)] [[pdf](https://arxiv.org/pdf/2401.04023)]
> **Authors**: Wentao Zhu
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: Accepted by WACV 2024; well-formatted PDF is in https://drive.google.com/file/d/10Zo_ydJZFAm7YsxHDgTjhyc4dEJbW_dk/view?usp=sharing
- **标题**: 有效的多尺度多模式瓶颈变压器，用于音频视频分类
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体,声音,音频和语音处理
- **摘要**: 近年来，研究人员将音频和视频信号结合在一起，以应对视觉提示未很好地代表或捕获的动作的挑战。但是，如何有效利用这两种方式仍在开发中。在这项工作中，我们开发了一个多模式变压器（MMT），以利用层次表示学习。特别是，MMT由新型的多尺度音频变压器（MAT）和多尺度视频变压器组成[43]。为了学习歧视性的跨模式融合，我们进一步设计了称为Audio-Video对比损失（AVC）和模式内对比度损失（IMC）的多模式监督对比目标（IMC），以牢固地对准这两种方式。在没有外部训练数据的情况下，MMT在动力学和VGGSOUND上超过了先前的最先进方法，并在动力学和VGGSOUND上超过了7.3％和2.1％。此外，提出的垫子在三个公共基准数据集上显着优于22.2％，4.4％和4.7％的AST [28]，并且根据FLOPS的数量，效率高约3％，基于GPU存储器的使用效率高9.8％。

### RoboFusion: Towards Robust Multi-Modal 3D Object Detection via SAM 
[[arxiv](https://arxiv.org/abs/2401.03907)] [[cool](https://papers.cool/arxiv/2401.03907)] [[pdf](https://arxiv.org/pdf/2401.03907)]
> **Authors**: Ziying Song,Guoxing Zhang,Lin Liu,Lei Yang,Shaoqing Xu,Caiyan Jia,Feiyang Jia,Li Wang
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: No comments
- **标题**: Robofusion：通过SAM朝着强大的多模式3D对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式3D对象探测器致力于探索自动驾驶的安全可靠的感知系统（AD）。尽管在干净的基准测试数据集上实现了最新的（SOTA）性能，但它们倾向于忽略现实世界环境的复杂性和恶劣条件。随着视觉基础模型（VFM）的出现，提出了机会和挑战，以改善AD中多模式3D对象检测的稳健性和概括。因此，我们提出了Robofusion，这是一个强大的框架，该框架利用SAM（例如SAM）来解决噪声（OOD）噪声方案。我们首先将原始SAM改编成名为SAM-AD的广告方案。为了使SAM或SAM-AD与多模式方法对齐，然后引入AD-FPN，以提高SAM提取的图像特征。我们采用小波分解来将深度引导的图像降低，以进一步降低噪音和天气干扰。最后，我们采用自我注意的机制来适应融合功能，增强信息性功能，同时抑制过多的噪音。总而言之，Robofusion通过利用VFM的概括和鲁棒性来显着降低噪声，从而增强多模式3D对象检测的弹性。因此，Robofusion在嘈杂的场景中实现了SOTA性能，如Kitti-C和Nuscenes-C基准所证明的那样。代码可从https://github.com/adept-thu/robofusion获得。

### Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex 
[[arxiv](https://arxiv.org/abs/2401.03851)] [[cool](https://papers.cool/arxiv/2401.03851)] [[pdf](https://arxiv.org/pdf/2401.03851)]
> **Authors**: Shuxiao Ma,Linyuan Wang,Senbao Hou,Bin Yan
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: No comments
- **标题**: 与LLM一致：一种用于编码Visual Cortex fMRI活动的新的多模式训练范式
- **领域**: 计算机视觉和模式识别,神经元和认知
- **摘要**: 最近，培训的大型语言模型（例如GPT-4）的普及，遍及整个自然语言处理（NLP）和计算机视觉（CV）社区。这些LLM表现出了高级多模式的理解能力，并在各种基准测试中展示了出色的性能。 LLM已开始体现人工通用智能的特征，该特征具有增强视觉编码模型中大脑样特征的重要指导。因此，本文提出了一种新的多模式训练范式，与LLM对齐，以编码视觉皮层中的fMRI活性。基于此范式，我们培训了名为LLM-Visual编码模型（LLM-VEM）的fMRI数据中的编码模型。具体而言，我们利用LLM（Minigpt4）为所有刺激图像生成描述性文本，形成高质量的文本描述集。此外，我们使用预训练的文本编码器（剪辑）来处理这些详细的描述，从而获得文本嵌入功能。接下来，我们使用对比损失函数来最大程度地减少图像嵌入功能与文本嵌入功能之间的距离，以完成刺激图像和文本信息的对齐操作。在预先训练的LLM的帮助下，这种对齐过程有助于更好地学习视觉编码模型，从而提高精度。最终的实验结果表明，我们的训练范式显着帮助增强视觉编码模型的性能。

### A multimodal gesture recognition dataset for desktop human-computer interaction 
[[arxiv](https://arxiv.org/abs/2401.03828)] [[cool](https://papers.cool/arxiv/2401.03828)] [[pdf](https://arxiv.org/pdf/2401.03828)]
> **Authors**: Qi Wang,Fengchao Zhu,Guangming Zhu,Liang Zhang,Ning Li,Eryang Gao
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: No comments
- **标题**: 桌面人机交互的多模式手势识别数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 手势识别是自然和有效的人类交互技术的必不可少的组成部分，尤其是在桌面级别的应用中，它可以显着提高人们的生产率。但是，当前的手势识别社区缺乏适合轻量级手势捕获设备的桌面级别（顶部视角）数据集。在这项研究中，我们建立了一个名为GR4DHCI的数据集。该数据集的区别在于它的固有自然性，直观的特征和多样性。它的主要目的是作为开发桌面级便携式应用程序的宝贵资源。 GR4DHCI包含7,000多个手势样本和382,447帧，用于立体声IR和骨骼模式。我们还通过将27个不同的手位置合并到数据集中，解决桌面交互期间的手工定位方差。在GR4DHCI数据集的基础上，我们进行了一系列实验研究，结果表明，本文提出的细粒分类块可以提高模型的识别精度。预计我们在本文中提出的数据集和实验发现将推动桌面级别的手势识别研究中的进步。

### FM-AE: Frequency-masked Multimodal Autoencoder for Zinc Electrolysis Plate Contact Abnormality Detection 
[[arxiv](https://arxiv.org/abs/2401.03806)] [[cool](https://papers.cool/arxiv/2401.03806)] [[pdf](https://arxiv.org/pdf/2401.03806)]
> **Authors**: Canzong Zhou,Can Zhou,Hongqiu Zhu,Tianhao Liu
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: 2023 The 34th Chinese Process Control Conference (CPCC 2023)
- **标题**: FM-AE：用于锌电解板触点异常检测的频率掩盖的多模式自动编码器
- **领域**: 计算机视觉和模式识别
- **摘要**: 锌电解是锌冶炼中的关键过程之一，维持锌电解的稳定运行是确保生产效率和产品质量的重要因素。然而，锌电解阴极与阳极之间的接触不良是一个常见问题，导致生产效率降低和对电解细胞的损害。因此，在线监视板的接触状态对于确保生产质量和效率至关重要。为了解决这个问题，我们提出了一个端到端网络，即频率掩盖的多模式自动编码器（FM-AE）。该方法将单元电压信号和红外图像信息作为输入，并通过自动编码将两个特征融合在一起，并通过级联检测器预测板的接触状态差。实验结果表明，该提出的方法保持较高的精度（86.2％），同时具有良好的鲁棒性和泛化能力，有效地检测到锌电解细胞的接触状态差，为生产实践提供了强有力的支持。

### Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion 
[[arxiv](https://arxiv.org/abs/2401.03788)] [[cool](https://papers.cool/arxiv/2401.03788)] [[pdf](https://arxiv.org/pdf/2401.03788)]
> **Authors**: Minglong Xue,Jinhong He,Wenhai Wang,Mingliang Zhou
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: No comments
- **标题**: 通过剪贴式引导小波扩散通过剪贴式引导的低光图像增强
- **领域**: 计算机视觉和模式识别
- **摘要**: 低光图像增强技术已经取得了显着发展，但是不稳定的图像质量恢复和不令人满意的视觉感知仍然是重大挑战。为了解决这些问题，我们通过剪贴式引导的小波扩散提出了一种新颖且强大的低光图像增强方法，缩写为CFWD。具体而言，CFWD利用由多个小波变换创建的频域空间中的多模式视觉语言信息来指导增强过程。跨不同模式的多尺度监督有助于在小波扩散过程中具有语义特征的图像特征对齐，有效地弥合了降级和正常域之间的差距。此外，为了进一步促进图像细节的有效恢复，我们将基于小波变换的傅立叶变换组合在一起，并构建混合高频感知模块（HFPM），并对详细特征具有显着的感知。该模块通过引导增强结果的细粒结构恢复以实现有利的度量和感知方向的增强，从而避免了小波扩散过程的多样性混乱。公开可用的现实世界基准的广泛定量和定性实验表明，我们的方法表现优于现有的最新方法，在图像质量和噪声抑制方面取得了重大进展。该项目代码可在https://github.com/hejh8/cfwd上找到。

### SnapCap: Efficient Snapshot Compressive Video Captioning 
[[arxiv](https://arxiv.org/abs/2401.04903)] [[cool](https://papers.cool/arxiv/2401.04903)] [[pdf](https://arxiv.org/pdf/2401.04903)]
> **Authors**: Jianqiao Sun,Yudi Su,Hao Zhang,Ziheng Cheng,Zequn Zeng,Zhengjue Wang,Bo Chen,Xin Yuan
> **First submission**: 2024-01-09
> **First announcement**: 2024-01-10
> **comment**: Preprint; Under Review
- **标题**: 快照：高效快照压缩视频字幕
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频字幕（VC）是一项具有挑战性的多模式任务，因为它需要通过理解各种复杂的视频来描述语言中的场景。对于机器，传统的风险投资遵循“成像压缩对编码和捕获”管道，其中压缩是用于存储和传输的枢轴。但是，在这样的管道中，一些潜在的缺点是不可避免的，即信息冗余，导致效率低下和在抽样过程中的信息丢失。为了解决这些问题，在本文中，我们提出了一条新型的VC管道，以直接从压缩测量中生成字幕，可以通过快照压缩传感摄像头捕获，并将模型快照配音。为了更具体，从信号模拟中受益，我们可以访问我们的模型获得丰富的测量 - 视频保管数据对。此外，为了从压缩度量中更好地提取与语言相关的视觉表示，我们建议通过预先训练的剪辑与大量的语言视觉协会从视频中提取知识，以指导我们的快照学习。为了证明SNAPCAP的有效性，我们对两个广泛使用的VC数据集进行了实验。定性和定量结果都验证了我们的管道优于常规VC管道的优势。特别是，与“字幕 - 重建”方法相比，我们的快照可以更快地运行3 $ \ times $，并获得更好的字幕结果。

### Effective pruning of web-scale datasets based on complexity of concept clusters 
[[arxiv](https://arxiv.org/abs/2401.04578)] [[cool](https://papers.cool/arxiv/2401.04578)] [[pdf](https://arxiv.org/pdf/2401.04578)]
> **Authors**: Amro Abbas,Evgenia Rusak,Kushal Tirumala,Wieland Brendel,Kamalika Chaudhuri,Ari S. Morcos
> **First submission**: 2024-01-09
> **First announcement**: 2024-01-10
> **comment**: Accepted at ICLR 2024, code available at https://github.com/amro-kamal/effective_pruning
- **标题**: 基于概念簇的复杂性，有效修剪Web规模数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 利用大量的网络尺度数据集导致机器学习模型中前所未有的性能提高，但也对其培训施加了奇特的计算要求。为了提高培训和数据效率，我们在这里推动修剪大规模的多模式数据集以训练夹式模型。当今的成像网簇数据样本的最有效的修剪方法根据其嵌入和修剪的最典型样本，将其分为单独的概念。我们通过指出修剪速率应该是特定于概念的，并适应概念的复杂性，从而扩展了这种方法并改善了Laion的方法。使用简单而直观的复杂性度量，我们能够将培训成本降低到定期培训的四分之一。通过从LAION数据集进行过滤，我们发现对较小的高质量数据进行培训可以导致更高的性能，并且培训成本明显降低。更具体地说，我们能够以1.1p.p.p. p的零摄像机准确性上的LAION训练的OpencLip-vit-B32模型优于托管训练的OpencLip-vit-B32模型。而仅使用27.7％的数据和培训计算。尽管培训成本大大降低，但我们也看到了Imagenet Dist的改进。班次，检索任务和VTAB。在DataComp Medium基准上，我们获得了新的最先进的ImageHttps：//info.arxiv.org/help/help/prep#commentsnet零弹药精度，并在38个评估任务上获得了竞争性的平均零弹药精度。

### Face-GPS: A Comprehensive Technique for Quantifying Facial Muscle Dynamics in Videos 
[[arxiv](https://arxiv.org/abs/2401.05625)] [[cool](https://papers.cool/arxiv/2401.05625)] [[pdf](https://arxiv.org/pdf/2401.05625)]
> **Authors**: Juni Kim,Zhikang Dong,Pawel Polak
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-11
> **comment**: No comments
- **标题**: 面部GPS：一种量化视频中面部肌肉动力学的综合技术
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们介绍了一种新颖的方法，该方法结合了差异几何形状，光滑和光谱分析，以量化可访问的视频记录（例如在个人智能手机上捕获的视频）中量化面部肌肉活动。我们的方法强调实用性和可及性。它具有在国家安全和整形手术中应用的巨大潜力。此外，它为中风，贝尔的麻痹和声学神经瘤等医疗状况提供了远程诊断和监测。此外，它擅长于从公开到微妙的情绪来检测和分类情绪。提出的面部肌肉分析技术是深度学习方法的可解释替代方法，是面部肌电图（FEMG）的非侵入性替代品。

### Masked Attribute Description Embedding for Cloth-Changing Person Re-identification 
[[arxiv](https://arxiv.org/abs/2401.05646)] [[cool](https://papers.cool/arxiv/2401.05646)] [[pdf](https://arxiv.org/pdf/2401.05646)]
> **Authors**: Chunlei Peng,Boyu Wang,Decheng Liu,Nannan Wang,Ruimin Hu,Xinbo Gao
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-11
> **comment**: No comments
- **标题**: 蒙版属性描述嵌入布换的人重新识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 换衣服的人重新识别（CC-REID）旨在与长期换衣服的人相匹配。 CC-Reid的主要挑战是提取与衣服无关的特征，例如面部，发型，身体形状和步态。当前的研究主要集中于使用多模式生物学特征（例如轮廓和草图）对身体形状进行建模。但是，它不能完全利用原始RGB图像中隐藏的个人描述信息。考虑到某些属性描述在换衣后保持不变，我们提出了一个蒙版属性描述嵌入（制造）方法，该方法统一了CC-REID的个人视觉外观和属性描述。具体而言，处理可变服装敏感的信息（例如颜色和类型）对于有效建模而言是一项挑战。为了解决这个问题，我们在通过属性检测模型提取的个人属性描述中掩盖了衣服和颜色信息。然后将蒙版属性描述连接到各个级别的变压器块中，将其与图像的高级特征融合在一起。这种方法迫使模型丢弃服装信息。实验是在包括PRCC，LTCC，Celeb-Reid-Light和Last的几个CC-REID基准上进行的。结果表明，有效地利用了属性描述，增强了换衣服的人的重新识别性能，并与最先进的方法进行了比较。该代码可在https://github.com/moon-wh/made上找到。

### MISS: A Generative Pretraining and Finetuning Approach for Med-VQA 
[[arxiv](https://arxiv.org/abs/2401.05163)] [[cool](https://papers.cool/arxiv/2401.05163)] [[pdf](https://arxiv.org/pdf/2401.05163)]
> **Authors**: Jiawei Chen,Dingkang Yang,Yue Jiang,Yuxuan Lei,Lihua Zhang
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-11
> **comment**: ICANN, 2024
- **标题**: 小姐：MED-VQA的生成预审预后和固定方法
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 医学视觉问题回答（VQA）是一项具有挑战性的多模式任务，视觉语言预训练（VLP）模型可以有效地改善概括性能。但是，医学领域中的大多数方法都将VQA视为答案分类任务，很难将其转移到实际的应用程序方案中。此外，由于医疗图像的隐私和昂贵的注释过程，严重缺乏用于预处理的大规模医学图像文本对数据集。在本文中，我们提出了一个基于医疗VQA任务的大型多任务自我监督学习框架（MISS）。与现有方法不同，我们将医疗VQA视为生成任务。我们通过多任务学习统一文本编码器和多模式编码器，并统一图像文本功能。此外，我们提出了一种转移和符合方法，该方法使用大语言模型（LLMS）扩展了单模式图像数据集的特征空间，从而使这些传统的医学视觉现场任务数据可应用于VLP。实验表明，我们的方法通过更少的多模式数据集获得了出色的结果，并证明了生成VQA模型的优势。

### Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning 
[[arxiv](https://arxiv.org/abs/2401.06344)] [[cool](https://papers.cool/arxiv/2401.06344)] [[pdf](https://arxiv.org/pdf/2401.06344)]
> **Authors**: Weizheng Wang,Chaowei Wang,Baijian Yang,Guohua Chen,Byung-Cheol Min
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-12
> **comment**: No comments
- **标题**: Hyper-STTN：通过超图推理的人类轨迹预测的社会群体感知时空变压器网络
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在Varouls现实世界中，包括服务机器人和自动驾驶汽车在内，预测拥挤的意图和轨迹至关重要。理解环境动力学是具有挑战性的，不仅是由于建模成对的空间和时间相互作用的复杂性，而且是群体相互作用的多种影响。为了在拥挤的场景中解码全面的配对和小组互动，我们介绍了Hyper-STTN，这是一种基于超图的时空变压器网络，用于人群轨迹预测。在Hyper-STTN中，使用一组具有不同组尺寸的多尺度超图构建拥挤的小组相关性，该相关性通过基于随机行动的Hypergraph Spectral Spectral卷积捕获。此外，空间变压器适合于捕获行人在时空维度中的成对潜在相互作用。然后通过多模式变压器网络融合并对齐这些异质的群体和配对。 Hyper-STTN在5个现实世界的行人运动数据集上优于其他最先进的基线和消融模型。

### Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2401.06209)] [[cool](https://papers.cool/arxiv/2401.06209)] [[pdf](https://arxiv.org/pdf/2401.06209)]
> **Authors**: Shengbang Tong,Zhuang Liu,Yuexiang Zhai,Yi Ma,Yann LeCun,Saining Xie
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-12
> **comment**: Project page: https://tsb0601.github.io/mmvp_blog/
- **标题**: 睁大眼睛？探索多模式LLM的视觉缺陷
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉对语言足够好吗？多模式模型的最新进展主要源于大型语言模型（LLMS）的强大推理能力。但是，视觉分量通常仅取决于实例级对比语言图像预训练（剪辑）。我们的研究表明，最近的多模式LLMS（MLLM）中的视觉功能仍然表现出系统性的缺点。为了了解这些错误的根源，我们探讨了剪辑的视觉嵌入空间与仅视觉的自我监督学习之间的差距。我们识别“夹具对”  - 尽管视觉差异很明显，但剪辑视为相似的图像。使用这些对，我们构建了多模式的视觉模式（MMVP）基准。 MMVP揭示了包括GPT-4V在内的最先进系统的区域，在九个基本的视觉模式中与直接问题斗争，通常会提供错误的答案和幻觉的解释。我们进一步评估了各种基于剪辑的视觉和语言模型，并发现了挑战剪辑模型的视觉模式与多模式LLM有问题的视觉模式之间的显着相关性。作为解决这些问题的最初努力，我们提出了一种特征方法（MOF）方法，表明将视觉自我监督的学习功能与MLLM相结合可以显着增强其视觉接地能力。我们的研究共同表明，视觉表示学习仍然是一个开放的挑战，准确的视觉接地对于未来成功的多模式系统至关重要。

### Enhancing Multimodal Understanding with CLIP-Based Image-to-Text Transformation 
[[arxiv](https://arxiv.org/abs/2401.06167)] [[cool](https://papers.cool/arxiv/2401.06167)] [[pdf](https://arxiv.org/pdf/2401.06167)]
> **Authors**: Chang Che,Qunwei Lin,Xinyu Zhao,Jiaxin Huang,Liqiang Yu
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-12
> **comment**: No comments
- **标题**: 通过基于夹的图像到文本转换增强多模式理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 将输入图像转换为相应的文本解释的过程是计算机视觉和自然语言处理领域内的至关重要而复杂的努力。在本文中，我们提出了一种创新的合奏方法，该方法利用了对比的语言图像预验证模型的能力。

### GroundingGPT:Language Enhanced Multi-modal Grounding Model 
[[arxiv](https://arxiv.org/abs/2401.06071)] [[cool](https://papers.cool/arxiv/2401.06071)] [[pdf](https://arxiv.org/pdf/2401.06071)]
> **Authors**: Zhaowei Li,Qi Xu,Dong Zhang,Hang Song,Yiqing Cai,Qi Qi,Ran Zhou,Junting Pan,Zefeng Li,Van Tu Vu,Zhida Huang,Tao Wang
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-12
> **comment**: No comments
- **标题**: 接地GPT：语言增强的多模式接地模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式大型语言模型在不同方式的各种任务中都表现出了令人印象深刻的表现。但是，现有的多模式模型主要强调在每种模式中捕获全球信息，同时忽略了跨模态感知本地信息的重要性。因此，这些模型缺乏有效理解输入数据的细粒细节的能力，从而限制了它们在需要更细微的理解的任务中的表现。为了解决这一限制，有迫切需要开发模型，以使跨多种模式获得细粒度的理解，从而增强其对广泛任务的适用性。在本文中，我们提出了一种语言增强的多模式接地模型。除了像其他多模式模型一样捕获全局信息之外，我们提出的模型还擅长于任务，要求对输入中的本地信息有详细的了解。它证明了在视频中图像或时刻中特定区域的精确识别和定位。为了实现这一目标，我们设计了一个多元化的数据集构建管道，从而为模型培训提供了多模式的多范围数据集。可以在https：//github.com/lzw-lzw/groundinggpt上找到我们模型的代码，数据集和演示。

### Exploring Self- and Cross-Triplet Correlations for Human-Object Interaction Detection 
[[arxiv](https://arxiv.org/abs/2401.05676)] [[cool](https://papers.cool/arxiv/2401.05676)] [[pdf](https://arxiv.org/pdf/2401.05676)]
> **Authors**: Weibo Jiang,Weihong Ren,Jiandong Tian,Liangqiong Qu,Zhiyong Wang,Honghai Liu
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-12
> **comment**: No comments
- **标题**: 探索人类对象相互作用检测的自我和跨三个相关性
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类对象相互作用（HOI）检测在场景理解中起着至关重要的作用，旨在以<人类，对象，动作>的形式预测HOI三胞胎。现有方法主要提取多模式特征（例如外观，对象语义，人姿势），然后将它们融合在一起以直接预测HOI三胞胎。但是，这些方法中的大多数都集中在寻求自我三重的聚集，但忽略了潜在的跨三重依赖性，从而导致了动作预测的歧义。在这项工作中，我们建议探索HOI检测的自我和跨三重相关性（SCTC）。具体而言，我们将每个三重态提案视为人类，对象表示节点和动作的图表，表示边缘，以汇总自我三重相关性。另外，我们尝试通过共同考虑实例级别，语义级别和布局级别的关系来探索跨三个方面的依赖关系。此外，我们利用剪辑模型来帮助我们的SCTC通过知识蒸馏获得相互作用的特征，这为HOI检测提供了有用的动作线索。在HICO-DET和V-COCO数据集上进行了广泛的实验，验证了我们提出的SCTC的有效性。

### Concept-Guided Prompt Learning for Generalization in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2401.07457)] [[cool](https://papers.cool/arxiv/2401.07457)] [[pdf](https://arxiv.org/pdf/2401.07457)]
> **Authors**: Yi Zhang,Ce Zhang,Ke Yu,Yushun Tang,Zhihai He
> **First submission**: 2024-01-14
> **First announcement**: 2024-01-15
> **comment**: Accepted by AAAI 2024
- **标题**: 概念引导的及时学习在视觉模型中进行概括
- **领域**: 计算机视觉和模式识别
- **摘要**: 对比性语言图像预处理（剪辑）模型在建立文本和图像之间建立跨模式连接方面表现出显着的功效，从而通过微调在下游应用中产生了令人印象深刻的性能。但是，对于概括任务，诸如Coop和cocoop等剪辑的当前微调方法表明，在某些细粒度数据集上的性能相对较低。我们认识到的根本原因是，这些先前的方法仅将全局特征投射到提示中，忽略了各种视觉概念，例如颜色，形状和大小，这些概念自然可以在范围内自然转移，并在概括任务中起着至关重要的作用。为了解决这个问题，在这项工作中，我们建议概念引导的及时学习（CPL），以了解视觉模型。具体来说，我们利用了剪辑的良好学习知识来创建视觉概念缓存，以实现概念引导的提示。为了完善文本功能，我们进一步开发了将多层次视觉特征转换为文本功能的投影仪。我们观察到，这种概念引导的及时学习方法能够在视觉和语言方式之间达到增强的一致性。广泛的实验结果表明，与当前的最新方法相比，我们的CPL方法显着提高了概括能力。

### MapNeXt: Revisiting Training and Scaling Practices for Online Vectorized HD Map Construction 
[[arxiv](https://arxiv.org/abs/2401.07323)] [[cool](https://papers.cool/arxiv/2401.07323)] [[pdf](https://arxiv.org/pdf/2401.07323)]
> **Authors**: Toyota Li
> **First submission**: 2024-01-14
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: MAPNEXT：在线矢量化高清图构造的重新访问培训和缩放实践
- **领域**: 计算机视觉和模式识别
- **摘要**: 高清（HD）地图对于自动驾驶仪导航至关重要。将运行时轻质高清图构造的能力集成到自动驾驶系统中，最近出现了一个有希望的方向。在这种激增中，仅视觉的看法脱颖而出，因为摄像头仍然可以感知立体声信息，更不用说其具有吸引力的便携性和经济标志了。最新的MAPTR架构以端到端的方式解决了在线高清地图构建任务，但其潜力尚未探索。在这项工作中，我们提出了MAPTR的全面升级，并提出了MapNext，即HD Map Learning Architecture的下一代，从模型培训和扩展角度提供了重大贡献。在阐明了MAPTR的训练动力并从地图元素中利用监督的训练动力后，MapNext-Tiny将Maptr-Tiny的地图从49.0％提高到54.8％，而没有任何建筑修改。 MapNext-base享受了地图分割预训练的果实，将地图进一步提高了63.9％的地图，这已经超过了先前的Art，一种多模式的MAPTR，而$ \ sim1.8 \ times $更快。为了将性能边界提升到一个新的水平，我们就实用模型缩放得出了两个结论：增加查询有利于更大的解码器网络，以进行足够的消化；一个大的骨干稳定地提高了最终的准确性，而没有铃铛和口哨声。 MapNext-Buge以这两个经验法则为基础，在具有挑战性的Nuscenes基准中实现了最先进的表现。具体而言，我们首次将无图视觉的单模性能提高到78％以上，超过了现有方法的最佳模型。

### Self-supervised Event-based Monocular Depth Estimation using Cross-modal Consistency 
[[arxiv](https://arxiv.org/abs/2401.07218)] [[cool](https://papers.cool/arxiv/2401.07218)] [[pdf](https://arxiv.org/pdf/2401.07218)]
> **Authors**: Junyu Zhu,Lina Liu,Bofeng Jiang,Feng Wen,Hongbo Zhang,Wanlong Li,Yong Liu
> **First submission**: 2024-01-14
> **First announcement**: 2024-01-15
> **comment**: Accepted by IROS2023
- **标题**: 使用跨模式一致性的基于自我监督事件的单眼深度估计
- **领域**: 计算机视觉和模式识别
- **摘要**: 事件摄像头是一种新颖的视觉传感器，可以捕获每个像素亮度变化并输出异步``````'''''。由于具有高速分辨率，高动态范围，低的带宽，低功耗和无运动模糊，因此在这些场景中，它具有比常规摄像机的优势。因此，提出了一些来自事件的监督的单眼深度估计，以解决常规摄像机难以解决的场景。但是，深度注释是昂贵且耗时的。在本文中，为了降低注释成本，我们提出了一个基于事件的单眼深度估计框架，名为Emodepth。 Emodepth使用与像素坐标中事件对齐的强度帧的跨模式一致性来限制训练过程。此外，在推论中，仅将事件用于单眼深度预测。此外，我们设计了一个多尺度的跳过连接体系结构，以有效地融合特征，以实现深度估计，同时保持高推理速度。 MVSEC和DSEC数据集的实验表明，我们的贡献是有效的，并且准确性可以优于现有的基于事件和无监督的基于框架的方法。

### Unsupervised Domain Adaptation Using Compact Internal Representations 
[[arxiv](https://arxiv.org/abs/2401.07207)] [[cool](https://papers.cool/arxiv/2401.07207)] [[pdf](https://arxiv.org/pdf/2401.07207)]
> **Authors**: Mohammad Rostami
> **First submission**: 2024-01-14
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 使用紧凑的内部表示形式适应无监督的域
- **领域**: 计算机视觉和模式识别
- **摘要**: 解决无监督域的适应性的主要技术涉及将来自源和目标域的数据点映射到共享的嵌入空间中。对嵌入空间的映射编码器进行了训练，使嵌入空间变成域不可知论，从而允许在源域上训练的分类器在目标域上很好地概括。为了进一步增强无监督域适应性（UDA）的性能，我们开发了一种额外的技术，使源域的内部分布更加紧凑，从而提高了模型在目标域中的推广能力。我们证明，通过增加嵌入式空间中不同类别的数据表示的利润，我们可以提高UDA模型性能的不同类别。为了使内部表示更紧凑，我们估计源域的内部学习的多模式分布是高斯混合模型（GMM）。利用估计的GMM，我们增强了源域中不同类别之间的分离，从而减轻了域移位的影响。我们提供理论分析以支持我们方法的越野。为了评估我们方法的有效性，我们对广泛使用的UDA基准UDA数据集进行了实验。结果表明，我们的方法增强了模型的通用性，并且胜过现有技术。

### APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning 
[[arxiv](https://arxiv.org/abs/2401.06827)] [[cool](https://papers.cool/arxiv/2401.06827)] [[pdf](https://arxiv.org/pdf/2401.06827)]
> **Authors**: Guiming Cao,Kaize Shi,Hong Fu,Huaiwen Zhang,Guandong Xu
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-15
> **comment**: 7 pages,3 figures
- **标题**: APLE：用于多模式及时学习的代币自适应
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 预训练的视觉语言（V-L）模型为概括性竞争者之间的下游任务设定了基准。现有研究中已经探讨了V-L模型的许多特征，包括对文本输入的敏感性的挑战以及多模式提示中的调整过程。借助V-L模型（例如剪辑）的高级利用，最近的方法可以部署可学习的提示，而不是手工艺提示提高概括性能并应对上述挑战。受图层训练的启发，该训练是在图像融合中疯狂使用的，我们注意到，使用顺序训练过程适应剪辑的不同模态分支有效地促进了概括的改善。在应对多模式提示挑战的背景下，我们建议以依次的方式来调整多模式提示学习（APLE）的代币自适应，以调整两种模态提示，视觉和语言。 APLE解决了V-L模型中的挑战，以促进两种模式的迅速学习，这表明符合最先进的竞争性概括性能。最重要的是，APLE在采用V-L模型方面具有绝对优势的及时长度实验中表现出鲁棒性和有利的性能。

### Multi-Memory Matching for Unsupervised Visible-Infrared Person Re-Identification 
[[arxiv](https://arxiv.org/abs/2401.06825)] [[cool](https://papers.cool/arxiv/2401.06825)] [[pdf](https://arxiv.org/pdf/2401.06825)]
> **Authors**: Jiangming Shi,Xiangbo Yin,Yeyun Chen,Yachao Zhang,Zhizhong Zhang,Yuan Xie,Yanyun Qu
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-15
> **comment**: Accepted by ECCV2024
- **标题**: 无监督的可见红外人员重新识别的多内存匹配
- **领域**: 计算机视觉和模式识别
- **摘要**: 无监督的可见红外人员重新识别（USL-VI-REID）是一项有前途但艰巨的检索任务。 USL-VI-REID的主要挑战是有效地生成伪标签，并在不依赖任何先前的注释的情况下建立跨模式的伪标签对应。最近，聚集的伪标签方法在USL-VI-REID中引起了更多关注。但是，以前的方法没有完全利用各个细微差别，因为它们只是利用了代表身份以建立交叉模式对应关系的单个内存，从而产生了模棱两可的交叉模式对应关系。为了解决这个问题，我们为USL-VI-REID提供了一个多内存匹配（MMM）框架。我们首先设计一个跨模式聚类（CMC）模块，以通过将两个模态样本聚集在一起生成伪标记。为了关联跨模式群集伪标记，我们设计了一个多功能学习和匹配（MMLM）模块，以确保优化明确地关注单个观点的细微差别，并建立可靠的交叉模式对应关系。最后，我们设计了一个软集群级别对齐（SCA）模块，以缩小模态差距，同时通过柔软的多到许多对齐策略来减轻噪声伪标签的效果。对公共SYSU-MM01和REGDB数据集进行了广泛的实验，证明了已建立的跨模式对应关系的可靠性以及我们MMM的有效性。源代码将发布。

### Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery and Geographical Prior 
[[arxiv](https://arxiv.org/abs/2401.06550)] [[cool](https://papers.cool/arxiv/2401.06550)] [[pdf](https://arxiv.org/pdf/2401.06550)]
> **Authors**: Chuanji Shi,Yingying Zhang,Jiaotuan Wang,Xin Guo,Qiqi Zhu
> **First submission**: 2024-01-12
> **First announcement**: 2024-01-15
> **comment**: 9 pages, 9 figures
- **标题**: 通过遥感图像和地理之前的兴趣产生多模式的城市领域
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 城市区域（AOI）是指具有定义多边形边界的综合城市功能区。城市贸易的快速发展导致对高度准确和及时的AOI数据的需求不断提高。但是，现有的研究主要集中在城市规划或区域经济分析的粗粒元素功能区域，并经常忽略在现实世界中AOI的到期。他们无法满足移动Internet在线访问（O2O）业务的精确要求。这些业务需要准确地归结为特定的社区，学校或医院。在本文中，我们提出了一个全面的端到端多模式深度学习框架，旨在同时检测准确的AOI边界，并通过利用遥控感应图像以及地理位置和标题为AOITR来验证AOI的可靠性。与传统的AOI生成方法不同，例如以各个级别的路线网络分离道路网络的道路切割方法，我们的方法与依赖于像素级分类的语义分割算法不同。取而代之的是，我们的AOITR首先选择特定类别的利益点（POI），并使用它来检索相应的遥感图像和地理先验（例如入口POIS和ROAD节点）。该信息有助于基于变压器编码器架构架构来构建多模式检测模型，以回归AOI多边形。此外，我们还利用了人类移动性，附近的POI和物流地址的动态特征，并通过级联网络模块进行AOI可靠性评估。实验结果表明，我们的算法对联合（IOU）度量的交集有显着改善，从而超过了以前的方法。

### Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and Outlook 
[[arxiv](https://arxiv.org/abs/2401.06542)] [[cool](https://papers.cool/arxiv/2401.06542)] [[pdf](https://arxiv.org/pdf/2401.06542)]
> **Authors**: Ziying Song,Lin Liu,Feiyang Jia,Yadan Luo,Guoxin Zhang,Lei Yang,Li Wang,Caiyan Jia
> **First submission**: 2024-01-12
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 自动驾驶中的鲁棒性感知3D对象检测：评论和前景
- **领域**: 计算机视觉和模式识别
- **摘要**: 在现代自主驾驶领域，感知系统对于准确评估周围环境的状态是必不可少的，从而实现了知情的预测和计划。该系统的关键步骤与使用车辆安装的传感器（例如LiDAR和相机）来识别附近对象的大小，类别和位置有关的3D对象检测。尽管旨在提高检测精度和效率的3D对象检测方法激增，但文献中存在差距，该差距会系统地检查其针对环境变化，噪声和天气变化的弹性。这项研究强调了鲁棒性以及准确性和潜伏期在评估实际情况下的感知系统方面的重要性。我们的工作对仅相机，仅激光镜和多模式3D对象检测算法进行了广泛的调查，彻底评估了它们在准确性，延迟和稳健性之间的权衡，尤其是在Kitti-C和Nuscenes-C等数据集中，以确保公平比较。其中，多模式3D检测方法具有出色的鲁棒性，并引入了新的分类法以重组文献以增强清晰度。这项调查旨在对现实世界应用中3D对象检测算法的当前功能和约束提供更实际的观点，从而将未来的研究转向以鲁棒性为中心的进步。

### ModaVerse: Efficiently Transforming Modalities with LLMs 
[[arxiv](https://arxiv.org/abs/2401.06395)] [[cool](https://papers.cool/arxiv/2401.06395)] [[pdf](https://arxiv.org/pdf/2401.06395)]
> **Authors**: Xinyu Wang,Bohan Zhuang,Qi Wu
> **First submission**: 2024-01-12
> **First announcement**: 2024-01-15
> **comment**: CVPR2024
- **标题**: modaverse：有效地使用LLMS转换方式
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类具有理解各种方式并在它们之间无缝传递信息的能力。在这项工作中，我们介绍了Modaverse，这是一种多式模式大型语言模型（MLLM），能够理解和转换各种模式的内容，包括图像，视频和音频。主要的MLLM框架主要依赖于文本和非文本特征的潜在空间的对齐。这个对齐过程将接受文本数据训练的语言模型与经过多模式数据训练的编码器和解码器同步，通常需要在多个阶段进行多个投影层的广泛培训。受LLM-As-As-Agent方法论的启发，我们提出了一种新颖的输入/输出（I/O）对准机制，该机制直接在自然语言层面上运行。它将LLM的输出与生成模型的输入保持一致，避免与潜在特征对齐相关的复杂性，并将现有MLLM的多个训练阶段简化为一个高效的过程。这种概念上的进步导致数据和计算成本都大大降低。通过对几个基准进行实验，我们证明了我们的方法与最先进的状态具有可比的性能，同时在数据使用和培训期间实现了相当大的效率。

### COCO is "ALL'' You Need for Visual Instruction Fine-tuning 
[[arxiv](https://arxiv.org/abs/2401.08968)] [[cool](https://papers.cool/arxiv/2401.08968)] [[pdf](https://arxiv.org/pdf/2401.08968)]
> **Authors**: Xiaotian Han,Yiqi Wang,Bohan Zhai,Quanzeng You,Hongxia Yang
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: 可可是“所有”，您需要进行视觉指导微调
- **领域**: 计算机视觉和模式识别
- **摘要**: 在人工智能领域，多模式大型语言模型（MLLM）越来越突出。视觉指令微调（IFT）是将MLLMS输出与用户意图对齐的重要过程。以下数据是高质量和多元化的指导是此微调过程的关键。最近的研究建议通过多方面的方法构建视觉IFT数据集：使用基于规则的模板转换现有数据集，使用GPT-4进行重写注释，并利用GPT-4V进行视觉数据集伪标记。 LLAVA-1.5采用了类似的方法，并构建了Llava-Mix-665k，这是当今最简单，最广泛但最有效的IFT数据集之一。值得注意的是，如果使用此数据集对该数据集进行了适当的微调，MLLM可以在几个基准上实现最先进的性能。但是，我们注意到接受此数据集训练的模型通常很难在多轮对话框中正确遵循用户说明。此外，传统标题和VQA评估基准及其封闭形式的评估结构尚未充分评估现代开放式生成MLLM的能力。这个问题不是LLAVA-MIX-665K数据集的唯一问题，但在所有根据图像字幕或VQA源构建的IFT数据集中可能是一个潜在的问题，尽管此问题的程度可能有所不同。我们认为，注释后具有多种多样且高质量的详细说明的数据集对于MLLMS IFT来说是必不可少的，并且足以满足。在这项工作中，我们建立了一个新的IFT数据集，其中图像来自可可数据集，以及更多样化的说明。我们的实验表明，当对提出的数据集进行微调时，MLLM在单轮和多轮对话框设置中都可以在开放式评估基准上获得更好的性能。

### Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision 
[[arxiv](https://arxiv.org/abs/2401.08581)] [[cool](https://papers.cool/arxiv/2401.08581)] [[pdf](https://arxiv.org/pdf/2401.08581)]
> **Authors**: Yi Cao,Swetava Ganguli,Vipul Pandey
> **First submission**: 2023-10-15
> **First announcement**: 2024-01-17
> **comment**: Extended abstract accepted for presentation at BayLearn 2023. 3 pages, 7 figures. Abstract based on IEEE IGARSS 2023 research track paper: arXiv:2304.13143
- **标题**: 时间嵌入：可扩展的自我监督的时间表示从时空数据中学习多模式计算机视觉
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 地理空间活动时间模式与土地使用类型之间存在相关性。提出了一种新型的自我监督方法，以根据移动性活动时间序列对景观进行分层。首先，将时间序列信号转换为频域，然后通过收缩自动编码器压缩到任务不合时宜的时间嵌入到任务无关的时间嵌入，该嵌段保留了时间序列中观察到的环状时间模式。像素的嵌入方式被转换为类似图像的通道，这些通道可用于使用深层语义分割的下游地理空间任务的基于任务的多模式建模。实验表明，时间嵌入是时间序列数据的语义上有意义的表示，并且在不同任务（例如对住宅区和商业区域进行分类）中有效。时间嵌入将连续的时空运动轨迹数据转化为具有语义上有意义的图像样张量表示，可以将其组合（多模式融合）与其他数据模式相结合或可以转换为图像样张量表示（例如，用于RBG成像，远程图形的图形嵌入式，远程图像）的图像样量表示，等等。 想象。多模式计算机视觉对于训练地理空间特征检测的机器学习模型至关重要，以便保持地理空间映射服务实时最新，并且可以显着改善用户体验，最重要的是用户安全。

### MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World 
[[arxiv](https://arxiv.org/abs/2401.08577)] [[cool](https://papers.cool/arxiv/2401.08577)] [[pdf](https://arxiv.org/pdf/2401.08577)]
> **Authors**: Yining Hong,Zishuo Zheng,Peihao Chen,Yian Wang,Junyan Li,Chuang Gan
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: Project page: https://vis-www.cs.umass.edu/multiply
- **标题**: 倍数：在3D世界中以多感觉为中心的大型语言模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,机器人技术
- **摘要**: 人类具有在积极探索和与3D世界互动的同时，有能力繁殖多感官线索的混合物。但是，当前的多模式大型语言模型被动地吸收感官数据作为输入，缺乏与3D环境中对象积极互动并动态收集其多感官信息的能力。为了迎来对该领域的研究，我们提出了乘法，这是一种多感觉体现的大语言模型，可以将多感觉交互式数据（包括视觉，音频，触觉和热信息信息）纳入大语言模型，从而在单词，动作和感知之间建立相关性。为此，我们首先收集多感官宇宙，这是一个大规模的多感觉交互数据集，其中包含500K数据，通过部署LLM驱动的体现代理来参与3D环境。要在此类生成的数据上使用预训练的LLM进行指令调整，我们首先将3D场景编码为抽象为中心的以对象为中心的表示，然后引入动作令牌，以表示体现的代理在环境中采取某些动作，并在每个时间步骤中代表代表代理的多感官状态观察值的状态令牌。在推理时间内，倍数可以生成动作令牌，指示代理在环境中采取行动并获得下一个多感官状态观察。然后，通过状态令牌将观察结果附加回LLM，以生成后续文本或动作令牌。我们证明，通过涉及对象检索，工具使用，多感官字幕和任务分解的各种具体任务，将基线倍增大于基线。

### MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level Image-Concept Alignment 
[[arxiv](https://arxiv.org/abs/2401.08527)] [[cool](https://papers.cool/arxiv/2401.08527)] [[pdf](https://arxiv.org/pdf/2401.08527)]
> **Authors**: Yequan Bie,Luyang Luo,Hao Chen
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: 云母：通过多级图像概念比对来解释皮肤病变诊断
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 黑盒深度学习方法在医学图像分析领域展示了巨大的潜力。但是，医学领域的固有的严格的信任要求催化了对可解释的人工智能（XAI）的利用的研究，特别关注基于概念的方法。现有的基于概念的方法主要从单个角度（例如，全球级别）应用概念注释，忽略了子区域和嵌入在医学图像中的概念之间细微的语义关系。这导致宝贵的医疗信息的实用性不足，并可能导致模型在采用固有的可解释体系结构（例如概念瓶颈）时，无法和谐地平衡可解释性和性能。为了减轻这些缺点，我们提出了一个多模式可解释的疾病诊断框架，该框架在多个层次上精心对齐医学图像和临床相关概念，涵盖图像水平，令牌水平和概念水平。此外，我们的方法允许进行模型干预，并从人类解剖概念方面提供文本和视觉解释。三个皮肤图像数据集的实验结果表明，我们的方法虽然保持模型可解释性，但仍达到概念检测和疾病诊断的高性能和标签效率。

### Hidden flaws behind expert-level accuracy of multimodal GPT-4 vision in medicine 
[[arxiv](https://arxiv.org/abs/2401.08396)] [[cool](https://papers.cool/arxiv/2401.08396)] [[pdf](https://arxiv.org/pdf/2401.08396)]
> **Authors**: Qiao Jin,Fangyuan Chen,Yiliang Zhou,Ziyang Xu,Justin M. Cheung,Robert Chen,Ronald M. Summers,Justin F. Rousseau,Peiyun Ni,Marc J Landsman,Sally L. Baxter,Subhi J. Al'Aref,Yijia Li,Alex Chen,Josef A. Brejt,Michael F. Chiang,Yifan Peng,Zhiyong Lu
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: ef:npj Digital Medicine, 2024
- **标题**: 多模式GPT-4医学视觉的专家级别准确性背后的隐藏缺陷
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 最近的研究表明，具有视力的生成预训练的变压器4（GPT-4V）在医疗挑战任务中的表现优于人类医生。但是，这些评估主要集中在多选择问题的准确性上。我们的研究通过对GPT-4V的图像理解，医学知识的回忆和逐步多模式推理进行全面分析来扩展当前范围，当解决新英格兰医学杂志（NEJM）图像挑战时 - 旨在测试医学专业人士知识和诊断能力的成像测验。评估结果证实，GPT-4V在多选择准确性方面与人体医生的性能相对（81.6％比77.8％）。 GPT-4V在医师错误地回答（准确性超过78％）的情况下也表现良好。但是，我们发现在做出正确的最终选择（35.5％）的情况下，GPT-4V经常出现有缺陷的理由，在图像理解中最为突出（27.2％）。不管GPT-4V在多项选择问题中的高精度如何，我们的发现都强调了在将这种多模式AI模型整合到临床工作流程中之前，需要对其理由进行进一步评估。

### Multi-view Distillation based on Multi-modal Fusion for Few-shot Action Recognition(CLIP-$\mathrm{M^2}$DF) 
[[arxiv](https://arxiv.org/abs/2401.08345)] [[cool](https://papers.cool/arxiv/2401.08345)] [[pdf](https://arxiv.org/pdf/2401.08345)]
> **Authors**: Fei Guo,YiKang Wang,Han Qi,WenPing Jin,Li Zhu
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: 基于多模式融合的多视图蒸馏，用于几次动作识别（剪辑 -  $ \ mathrm {m^2} $ df）
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，很少有动作识别吸引了越来越多的关注。它通常采用元学习的范式。在这一领域，基于有限的样本，克服类和离群值的重叠分布仍然是一个具有挑战性的问题。我们认为，多模式和多视图的组合可以根据信息互补性来改善此问题。因此，我们提出了一种基于多模式融合的多视图蒸馏方法。首先，构建了查询的概率提示选择器，以根据支持的提示嵌入与查询的视觉嵌入之间的比较分数生成概率提示嵌入。其次，我们建立一个多视图。在每个视图中，我们将提示嵌入作为一致信息与视觉和全局或本地时间上下文融合在一起，以克服类和离群值的重叠分布。第三，我们对多视图的距离融合和匹配能力从一个到另一个到另一个的匹配能力的相互蒸馏，从而使模型能够对分布偏置更加稳健。我们的代码可在URL上找到：\ url {https://github.com/cofly2014/mdmf}。

### AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception 
[[arxiv](https://arxiv.org/abs/2401.08276)] [[cool](https://papers.cool/arxiv/2401.08276)] [[pdf](https://arxiv.org/pdf/2401.08276)]
> **Authors**: Yipo Huang,Quan Yuan,Xiangfei Sheng,Zhichao Yang,Haoning Wu,Pengfei Chen,Yuzhe Yang,Leida Li,Weisi Lin
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: Aesbench：图像美学知觉的多模式大语言模型的专家基准
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 有了集体的努力，多模式的大语言模型（MLLM）正在经历蓬勃发展的发展。但是，他们在图像美学感知上的表现仍然不确定，这在现实世界中非常需要。一个明显的障碍在于没有特定基准来评估MLLM在美学感知上的有效性。这种盲目的摸索可能会阻碍具有美学感知能力的更先进的MLLM的进一步发展。为了解决这一难题，我们提出了Aesbench，这是一个专家基准测试，旨在通过跨双面的精心设计来全面评估MLLM的美学感知能力。 （1）我们构建了专家标记的美学知觉数据库（EAPD），该数据库具有多元化的图像内容和专业美学专家提供的高质量注释。 （2）我们提出了一组综合标准，以从四个角度（包括感知（AESP），同理心（AESE），评估），评估（AESA）和解释（AESI）（AESI）来衡量MLLM的美学感知能力。广泛的实验结果强调了当前的MLLM仅具有基本的美学感知能力，并且MLLM和人类之间仍然存在显着差距。我们希望这项工作能够激发社区对MLLM的美学潜力进行更深入的探索。源数据将在https://github.com/yipoh/aesbench上找到。

### Human vs. LMMs: Exploring the Discrepancy in Emoji Interpretation and Usage in Digital Communication 
[[arxiv](https://arxiv.org/abs/2401.08212)] [[cool](https://papers.cool/arxiv/2401.08212)] [[pdf](https://arxiv.org/pdf/2401.08212)]
> **Authors**: Hanjia Lyu,Weihong Qi,Zhongyu Wei,Jiebo Luo
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: Accepted for publication in ICWSM 2024
- **标题**: 人与LMM：探索表情符号解释和用法中数字通信中的差异
- **领域**: 计算机视觉和模式识别
- **摘要**: 在处理多模式信息时，尤其是在社交媒体的背景下，利用大型的多模型模型（LMM）来模拟人类的行为，由于其广泛的潜力和深远的含义，引起了极大的兴趣。表情符号作为数字通信最独特的方面之一，在丰富的情感和音调方面是至关重要的。但是，了解这些高级模型（例如GPT-4V）如何在在线互动的细微差别背景下解释和采用表情符号存在显着差距。这项研究打算通过检查GPT-4V在复制类似人类表情符号的使用中的行为来弥合这一差距。这些发现揭示了人类和GPT-4V行为之间的差异，这可能是由于人类解释的主观性质以及GPT-4V以英语为中心的培训的局限性，表明文化偏见和非英国文化的代表不足。

### The Devil is in the Details: Boosting Guided Depth Super-Resolution via Rethinking Cross-Modal Alignment and Aggregation 
[[arxiv](https://arxiv.org/abs/2401.08123)] [[cool](https://papers.cool/arxiv/2401.08123)] [[pdf](https://arxiv.org/pdf/2401.08123)]
> **Authors**: Xinni Jiang,Zengsheng Kuang,Chunle Guo,Ruixun Zhang,Lei Cai,Xiao Fan,Chongyi Li
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: 魔鬼在细节中是：通过重新思考跨模式对齐和聚集来增强引导深度超分辨率
- **领域**: 计算机视觉和模式识别
- **摘要**: 引导深度超分辨率（GDSR）涉及使用同一场景的高分辨率RGB图像恢复缺失的深度细节。以前的方法在多模式输入的异质性和互补性上挣扎，并忽略了模态未对准，几何未对准和特征选择的问题。在这项研究中，我们重新考虑了GDSR网络中的一些基本组件，并提出了一个简单而有效的动态双对准和聚合网络（D2A2）。 D2A2主要由1）一个动态的双对齐模块组成，该模块可通过学习偏移来适应模态未对准，并通过学习偏移来使几何跨模式特征与几何相结合； 2）使用门控机制和像素注意力的蒙版至像素特征聚合模块，以滤除RGB功能中的无关纹理噪声，并将有用的功能与深度特征相结合。通过结合RGB和深度特征的优势，同时最大程度地减少了RGB图像引入的干扰，我们的方法与简单的重复使用和重新设计基本组​​件可以在多个基准数据集中实现最新性能。该代码可在https://github.com/jiangxinni/d2a2上找到。

### Fusion: Bayesian-based Multimodal Multi-level Fusion on Colorectal Cancer Microsatellite Instability Prediction 
[[arxiv](https://arxiv.org/abs/2401.07854)] [[cool](https://papers.cool/arxiv/2401.07854)] [[pdf](https://arxiv.org/pdf/2401.07854)]
> **Authors**: Quan Liu,Jiawen Yao,Lisha Yao,Xin Chen,Jingren Zhou,Le Lu,Ling Zhang,Zaiyi Liu,Yuankai Huo
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: 融合：基于贝叶斯的多模式多层次融合大肠癌微卫星不稳定性预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 结直肠癌（CRC）在组织病理学图像上的微观 - 卫星不稳定性（MSI）预测是一项具有挑战性的弱监督学习任务，涉及GigaiPixel图像上的多企业学习。迄今为止，放射学图像已被证明具有CRC MSI信息和有效的患者成像技术。不同的数据模式集成为提高MSI预测的准确性和鲁棒性提供了机会。尽管表示从整个幻灯片图像（WSI）学习并探索使用放射学数据的潜力取得了进展，但CRC MSI预测仍然是融合来自多种数据模式（例如病理WSI和放射学CT图像）的信息的挑战。在本文中，我们提出了$ M^{2} $ Fusion：CRC MSI的基于贝叶斯的多模式的多模式融合管道。提出的融合模型$ m^{2} $融合能够发现与单独使用单一模态以及其他融合方法相比，在模态内和跨模态内的更多新颖模式。该论文的贡献是三个方面的：（1）$ m^{2} $融合是对病理WSI的多级融合的第一个管道和MSI预测的3D放射学CT图像； （2）CT图像首次集成到CRC MSI预测的多模式融合中； （3）在基于变压器和基于CNN的方法上评估特征级融合策略。我们的方法在352个情况的交叉验证方面得到了验证，并且在AUC分数上均超过特征级别（0.8177 vs. 0.7908）或决策级融合策略（0.8177 vs. 0.7289）。

### Fusing Echocardiography Images and Medical Records for Continuous Patient Stratification 
[[arxiv](https://arxiv.org/abs/2401.07796)] [[cool](https://papers.cool/arxiv/2401.07796)] [[pdf](https://arxiv.org/pdf/2401.07796)]
> **Authors**: Nathan Painchaud,Jérémie Stym-Popper,Pierre-Yves Courand,Nicolas Thome,Pierre-Marc Jodoin,Nicolas Duchateau,Olivier Bernard
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: 12 pages + 2 pages of supplementary material, submitted to IEEE journal
- **标题**: 融合超声心动图图像和连续患者分层的医疗记录
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 深度学习可以从超声心动图序列（例如射血分数或应变）中自动且可靠地提取心脏功能描述符。这些描述符提供了细粒度的信息，这些信息与临床记录中更多的全球变量一起考虑，以评估患者的病情。利用应用于表格数据的新型变压器模型，我们提出了一种方法，该方法考虑了从病历和超声心动图中提取的所有描述符，以了解具有难以表征连续性（即高血压）的心血管病理学的表示。我们的方法首先使用特定于模式的方法将每个变量投射到其自身的表示空间中。然后将这些标准化的多模式数据表示形式馈送到变压器编码器中，该编码器通过预测临床等级的任务，将其合并为患者的全面表示。该分层任务被称为序数分类，以在表示空间中执行病理连续性。我们在239名高血压患者的队列中观察到这一连续性的主要趋势，在描述高血压对各种心脏功能描述的影响时提供了前所未有的细节。我们的分析表明，i）XTAB基金会模型的体系结构即使数据有限（少于200个培训样本）也可以达到出色的性能（98％的AUROC），ii）ii）培训之间的分层在培训（3.6％的MAE内）和III中的培训（在3.6％的MAE中）和III）模式在描述者中出现，其中一些与既定的生理学知识相一致，而其他人则可能对此有所了解。

### Multimodal Crowd Counting with Pix2Pix GANs 
[[arxiv](https://arxiv.org/abs/2401.07591)] [[cool](https://papers.cool/arxiv/2401.07591)] [[pdf](https://arxiv.org/pdf/2401.07591)]
> **Authors**: Muhammad Asif Khan,Hamid Menouar,Ridha Hamila
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: Accepted version of the paper in 19th International Conference on Computer Vision Theory and Applications (VISAPP), Rome, Italy, 27-29 Feb, 2024,
- **标题**: 多模式人群用Pix2pix Gans计数
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大多数最先进的人群计数方法都使用颜色（RGB）图像来学习人群的密度图。但是，这些方法通常难以在照明较差的密集拥挤的场景中获得更高的准确性。最近，一些研究报告了使用RGB和热图像组合的人群计数模型的准确性提高。尽管多模式数据可以带来更好的预测，但多模式数据可能并不总是事先可用。在本文中，我们建议使用生成对抗网络（GAN）自动从颜色（RGB）图像中生成热红外（TIR）图像，并使用两者都用于训练人群计数模型以达到更高的精度。我们首先使用PIX2PIX GAN网络将RGB图像转换为TIR图像。我们对几种最先进的人群计数模型和基准人群数据集的实验报告了准确性的显着提高。

### A Bi-Pyramid Multimodal Fusion Method for the Diagnosis of Bipolar Disorders 
[[arxiv](https://arxiv.org/abs/2401.07571)] [[cool](https://papers.cool/arxiv/2401.07571)] [[pdf](https://arxiv.org/pdf/2401.07571)]
> **Authors**: Guoxin Wang,Sheng Shi,Shan An,Fengmei Fan,Wenshu Ge,Qi Wang,Feng Yu,Zhiren Wang
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: Accepted by IEEE ICASSP 2024
- **标题**: 两极性融合方法用于诊断躁郁症
- **领域**: 计算机视觉和模式识别
- **摘要**: 先前关于躁郁症诊断的研究主要集中于静止状态功能磁共振成像。但是，它们的准确性无法满足临床诊断的要求。有效的多模式融合策略具有在多模式数据中应用的巨大潜力，并且可以进一步改善医学诊断模型的性能。在这项工作中，我们同时利用SMRI和fMRI数据，并提出了一种新型的躁郁症多模式诊断模型。提出的斑块金字塔提取模块提取了SMRI特征，而时空金字塔结构提取了fMRI特征。最后，它们被融合模块融合在一起，以与分类器输出诊断结果。广泛的实验表明，我们提出的方法在OpenFMRI数据集上以平衡精度从0.657到0.732优于其他方法，并实现了最新的状态。

### Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy for Temporal Sentence Grounding in Video 
[[arxiv](https://arxiv.org/abs/2401.07567)] [[cool](https://papers.cool/arxiv/2401.07567)] [[pdf](https://arxiv.org/pdf/2401.07567)]
> **Authors**: Zhaobo Qi,Yibo Yuan,Xiaowen Ruan,Shuhui Wang,Weigang Zhang,Qingming Huang
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: accepted by AAAI 2024
- **标题**: 视频中的时间句子接地的偏见冲突样本综合和对抗性删除DEBIAS策略
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频（TSGV）中接地的时间句子是由于数据集偏差问题而困扰的，这是由于输入视频或查询文本中具有相似语义组件的样本的目标矩的时间分布不平均而引起的。现有方法诉诸利用有关偏见的先验知识来人为地打破这种不均匀的分布，这仅消除了有限的重要语言偏见。在这项工作中，我们提出了偏置冲突样品合成和对抗性去除Debias策略（BSSARD），该策略通过明确利用单个模式特征与目标力矩的时间位置之间的潜在虚假相关性来动态产生偏见冲突样本。通过对抗训练，其偏见发生器不断引入偏见并产生偏见的样本以欺骗其接地模型。同时，接地模型连续消除了引入的偏见，这要求它对多模式对准信息进行建模。 BSSARD将涵盖大多数类型的耦合关系，并同时破坏语言和视觉偏见。对Charades-CD和ActivityNet-CD进行的广泛实验证明了BSSARD具有有希望的词汇能力。源代码可在https://github.com/qzhb/bssard上找到。

### MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception 
[[arxiv](https://arxiv.org/abs/2401.07529)] [[cool](https://papers.cool/arxiv/2401.07529)] [[pdf](https://arxiv.org/pdf/2401.07529)]
> **Authors**: Yuhao Wang,Yusheng Liao,Heyang Liu,Hongcheng Liu,Yu Wang,Yanfeng Wang
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: MM-SAP：在感知中评估多模式模型的自我意识的全面基准
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式大语言模型（MLLM）的最新进展已证明了视觉感知和理解方面的出色能力。但是，这些模型也遭受了幻觉的困扰，从而限制了它们作为AI系统的可靠性。我们认为，这些幻觉部分是由于模型与理解图像所能理解和不能感知的内容的斗争，这是我们所谓的自我意识。尽管它很重要，但在先前的研究中，MLLM的这一方面被忽略了。在本文中，我们旨在定义和评估感知中MLLM的自我意识。为此，我们首先在感知中介绍了知识象限，这有助于定义MLLM的了解和不了解图像的知识。使用此框架，我们提出了一种新颖的基准测试，即MLLMS（MM-SAP）的自我意识，该基准是专门设计用于评估此能力的。我们将MM-SAP应用于各种流行的MLLM，对他们的自我意识进行了全面的分析，并提供了详细的见解。实验结果表明，当前的MLLM具有有限的自我意识能力，这是对信任的MLLM发展发展的关键领域。代码和数据可在https://github.com/yhwmz/mm-sap上找到。

### One for All: Toward Unified Foundation Models for Earth Vision 
[[arxiv](https://arxiv.org/abs/2401.07527)] [[cool](https://papers.cool/arxiv/2401.07527)] [[pdf](https://arxiv.org/pdf/2401.07527)]
> **Authors**: Zhitong Xiong,Yi Wang,Fahong Zhang,Xiao Xiang Zhu
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: 5 pages
- **标题**: 所有人：迈向地球视觉的统一基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 以广泛参数和大规模数据集培训的特征的基础模型表明，在各种下游任务中，用于遥感数据具有出色的功效。当前的遥感基础模型通常专注于单一模态或特定的空间分辨率范围，从而限制了其在下游数据集的多功能性。尽管已经尝试开发多模式遥感基础模型，但它们通常采用每个模态或空间分辨率的单独的视觉编码器，因此需要在骨干中进行开关，以取决于输入数据。为了解决这个问题，我们引入了一种简单而有效的方法，称为“ net”（一对一的网络）：使用单个共享的变压器主链来用于具有不同空间分辨率的多种数据模式。使用蒙版的图像建模机制，我们使用这种简单的设计将单个变压器主链预先培训。然后，骨干模型可用于不同的下游任务，从而在地球视觉中锻造通向统一基础骨干模型的路径。在12个不同的下游任务上评估了所提出的方法，并证明了有希望的性能。

### SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model 
[[arxiv](https://arxiv.org/abs/2401.09712)] [[cool](https://papers.cool/arxiv/2401.09712)] [[pdf](https://arxiv.org/pdf/2401.09712)]
> **Authors**: Yang Zhan,Zhitong Xiong,Yuan Yuan
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: SKEYEYEGPT：通过大型语言模型调整指令调整统一遥感视觉语言任务
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型语言模型（LLM）最近已扩展到视觉语言领域，获得了令人印象深刻的一般多模式功能。但是，探索用于遥感数据（RS）数据的多模式大型语言模型（MLLM）仍处于起步阶段，并且性能并不令人满意。在这项工作中，我们介绍了Skyeyegpt，这是一种统一的多模式大型语言模型，专门为RS视觉语言理解而设计。为此，我们精心策划了RS多模式指令调整数据集，包括单任务和多任务对话说明。手动验证后，我们获得了带有968K样品的高质量RS指令遵循数据集。我们的研究表明，凭借简单而有效的设计，Skyeyegpt在不需要额外编码模块的情况下出奇地在相当不同的任务上运作良好。具体而言，在通过对齐层将RS视觉功能投影到语言域之后，它们与特定于任务的指令共同馈送到基于LLM的RS解码器中，以预测RS开放式任务的答案。此外，我们设计了一种两阶段的调谐方法，以增强不同粒度的指导跟踪和多转向对话能力。在8个数据集上进行RS视觉语言任务的实验证明了Skyeyegpt在图像级别和区域级任务（例如字幕和视觉接地）方面的优势。特别是，与GPT-4V相比，Skyeyegpt在某些定性测试中表现出令人鼓舞的结果。在线演示，代码和数据集将在https://github.com/zhanyang-nwpu/skyeyegpt中发布。

### Explainable Multimodal Sentiment Analysis on Bengali Memes 
[[arxiv](https://arxiv.org/abs/2401.09446)] [[cool](https://papers.cool/arxiv/2401.09446)] [[pdf](https://arxiv.org/pdf/2401.09446)]
> **Authors**: Kazi Toufique Elahi,Tasnuva Binte Rahman,Shakil Shahriar,Samir Sarker,Sajib Kumar Saha Joy,Faisal Muhammad Shah
> **First submission**: 2023-12-20
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 关于孟加拉模因的可解释的多模式情感分析
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 模因在数字时代已成为一种独特而有效的沟通形式，吸引了在线社区并跨越文化障碍。即使模因经常与幽默联系在一起，他们也具有传达各种情感的惊人能力，包括幸福，讽刺，挫败感等。在信息时代，理解和解释潜在模因的情感已经变得至关重要。先前的研究探索了基于文本的，基于图像和多模式的方法，从而开发了诸如Capsan和Progphate等模型，用于检测各种模因类别。但是，对孟加拉模因等低资源语言的研究仍然很少，公共可访问的数据集的可用性有限。最近的贡献包括引入备受的数据集。但是，所达到的准确性明显较低，数据集患有不平衡的分布。在这项研究中，我们使用Resnet50和Banglishbert采用了多模式方法，并获得了0.71加权F1分数的令人满意的结果，与单峰方法进行了比较，并使用了可解释的人工智能（XAI）技术进行了对模型的解释行为。

### Object Attribute Matters in Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2401.09442)] [[cool](https://papers.cool/arxiv/2401.09442)] [[pdf](https://arxiv.org/pdf/2401.09442)]
> **Authors**: Peize Li,Qingyi Si,Peng Fu,Zheng Lin,Yan Wang
> **First submission**: 2023-12-20
> **First announcement**: 2024-01-18
> **comment**: AAAI 2024
- **标题**: 对象属性在视觉问题回答中很重要
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉问题回答是一项多模式任务，需要对视觉和文本信息的共同理解。但是，仅通过注意层集成视觉和文本语义不足以全面地理解和对齐两种方式。直觉上，对象属性自然可以用作统一它们的桥梁，这在先前的研究中已经忽略了。在本文中，我们从利用对象属性的角度提出了一种新颖的VQA方法，旨在实现更好的对象级视觉语言对准和多模式场景的理解。具体而言，我们设计一个属性融合模块和对比度知识蒸馏模块。属性融合模块通过消息传递构建了多模式图神经网络，以融合属性和视觉特征。增强的对象级视觉特征有助于解决诸如计数问题之类的细粒度问题。更好的对象级视觉语言对齐有助于理解多模式场景，从而改善模型的稳健性。此外，为了增强场景的理解和分布外的性能，对比知识蒸馏模块引入了一系列隐性知识。我们通过对比损失将知识提炼为属性，从而进一步增强了属性特征的表示并促进视觉语言对齐。在六个数据集（可可QA，VQAV2，VQA-CPV2，VQA-CPV1，VQAVS和TDIUC）上进行密集实验，显示了所提出方法的优越性。

### SM$^3$: Self-Supervised Multi-task Modeling with Multi-view 2D Images for Articulated Objects 
[[arxiv](https://arxiv.org/abs/2401.09133)] [[cool](https://papers.cool/arxiv/2401.09133)] [[pdf](https://arxiv.org/pdf/2401.09133)]
> **Authors**: Haowen Wang,Zhen Zhao,Zhao Jin,Zhengping Che,Liang Qiao,Yakun Huang,Zhipeng Fan,Xiuquan Qiao,Jian Tang
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: SM $^3 $：使用多视图2D图像的自制多任务建模
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 重建现实世界对象并估算其可移动关节结构是机器人技术领域中的关键技术。先前的研究主要集中在监督方法上，依靠广泛注释的数据集来模拟有限类别的铰接对象。但是，这种方法无法有效地解决现实世界中存在的多样性。为了解决这个问题，我们提出了一种自我监督的互动感知方法，称为SM $^3 $，该方法利用了在交互前后捕获的多视图RGB图像，以模拟铰接式对象，识别可移动零件，并推断其旋转关节的参数。通过从捕获的2D图像中构造3D几何形状和纹理，SM $^3 $在重建过程中实现了可移动零件和关节参数的集成优化，从而消除了对注释的需求。此外，我们介绍了MMART数据集，Partnet-Mobility的扩展，涵盖了跨越各种类别的清晰对象的多视图和多模式数据。评估表明，SM $^3 $超过了各种类别和对象的现有基准，而在现实世界中的适应性已得到彻底验证。

### UniVG: Towards UNIfied-modal Video Generation 
[[arxiv](https://arxiv.org/abs/2401.09084)] [[cool](https://papers.cool/arxiv/2401.09084)] [[pdf](https://arxiv.org/pdf/2401.09084)]
> **Authors**: Ludan Ruan,Lei Tian,Chuanwei Huang,Xu Zhang,Xinyan Xiao
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: Univtg：迈向统一模型的视频生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于扩散的视频生成已受到广泛关注，并在学术和工业社区中取得了巨大的成功。但是，当前的努力主要集中在单键词或单任务视频生成上，例如由文本，图像或文本和图像的组合驱动的生成。这无法完全满足实际应用程序方案的需求，因为用户可能会单独或组合以灵活的方式输入图像和文本条件。为了解决这个问题，我们提出了一个统一的模式视频震级系统，该系统能够处理跨文本和图像模式的多个视频生成任务。为此，我们从生成自由的角度重新审视了系统中各种视频生成任务，并将其分为高自由和低自由的视频生成类别。对于高自由度视频的生成，我们采用多条件交叉注意来生成与输入图像或文本语义相符的视频。对于低自由度的视频生成，我们引入了有偏见的高斯噪声来替代纯净的随机高斯噪声，这有助于更好地保留输入条件的内容。我们的方法在公共学术基准MSR-VTT上实现了最低的Fréchet视频距离（FVD），超过了人类评估中当前的开源方法，并且与当前的封闭源方法GEN2相当。有关更多样本，请访问https://univg-baidu.github.io。

### CrossVideo: Self-supervised Cross-modal Contrastive Learning for Point Cloud Video Understanding 
[[arxiv](https://arxiv.org/abs/2401.09057)] [[cool](https://papers.cool/arxiv/2401.09057)] [[pdf](https://arxiv.org/pdf/2401.09057)]
> **Authors**: Yunze Liu,Changxi Chen,Zifan Wang,Li Yi
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: ef:ICRA2024
- **标题**: CrossVideo：自我监管的交叉模式对比度学习，用于云视频的理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了一种名为Crossvideo的新颖方法，该方法旨在在Point Cloud视频理解领域增强自我观察的交叉模式对比学习。由于数据稀缺和标签获取的挑战，传统的监督学习方法遇到了限制。为了解决这些问题，我们提出了一种自我监督的学习方法，该方法利用点云视频和图像视频之间的跨模式关系来获取有意义的特征表示。模式内和跨模式对比度学习技术被采用来促进对点云视频的有效理解。我们还为这两种方式提出了一种多层对比方法。通过广泛的实验，我们证明我们的方法显着超过了先前的最新方法，并且我们进行了全面的消融研究以验证我们提出的设计的有效性。

### Cross-modality Guidance-aided Multi-modal Learning with Dual Attention for MRI Brain Tumor Grading 
[[arxiv](https://arxiv.org/abs/2401.09029)] [[cool](https://papers.cool/arxiv/2401.09029)] [[pdf](https://arxiv.org/pdf/2401.09029)]
> **Authors**: Dunyuan Xu,Xi Wang,Jinyue Cai,Pheng-Ann Heng
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 跨模式指导辅助多模式学习，双重注意MRI脑肿瘤分级
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 脑肿瘤代表着世界上最致命的癌症之一，在儿童和老年人中非常普遍。早期肿瘤的类型和等级的准确鉴定在选择精确的治疗计划中起着重要作用。不同序列的磁共振成像（MRI）方案为临床医生提供了重要的矛盾信息，以识别肿瘤区域。但是，由于大量数据和脑肿瘤类型的多样性，手动评估是耗时且容易出错的。因此，对MRI自动化的脑肿瘤诊断的需求未满足。我们观察到，大学模型的预测能力受到限制，其性能在各种方式上差异很大，并且常用的模态融合方法将引入潜在的噪声，从而导致大量的性能降级。为了克服这些挑战，我们提出了一种新型的跨模式指导性多模式学习，并以双重注意来解决MRI脑肿瘤分级的任务。为了平衡模型效率和功效之间的权衡，我们采用Resnet Mix卷积作为特征提取的骨干网络。此外，对双重注意分别用于捕获空间和切片尺寸中的语义相互依赖性。为了促进模式之间的信息相互作用，我们设计了一个跨模式指南的模块，其中主要模态在训练过程中引导了其他次要方式，这可以有效地利用不同MRI模式的互补信息，并减轻可能的噪声的影响。

### Beyond the Frame: Single and mutilple video summarization method with user-defined length 
[[arxiv](https://arxiv.org/abs/2401.10254)] [[cool](https://papers.cool/arxiv/2401.10254)] [[pdf](https://arxiv.org/pdf/2401.10254)]
> **Authors**: Vahid Ahmadi Kalkhorani,Qingquan Zhang,Guanqun Song,Ting Zhu
> **First submission**: 2023-12-22
> **First announcement**: 2024-01-19
> **comment**: No comments
- **标题**: 超越框架：具有用户定义长度的单个和多个视频摘要方法
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 视频Smmarization是减少视频时间的关键方法，减少了观看/查看长视频的花费。随着PubliseHed视频每天不断增加，这种Apporach变得越来越重要。单个或多个视频可以汇总到相对较短的视频中，使用从多模式音频技术到自然语言处理方法的各种技术。视听技术可用于识别重大的视觉事件并选择最重要的部分，而NLP技术可用于评估音频成绩单并提取主要句子（时间戳）和原始视频中相应的视频帧。另一种方法是使用两个领域的最佳方法。这意味着我们可以使用视听提示以及视频成绩单来提取和总结视频。在本文中，我们将各种NLP技术（基于提取和基于扭曲的摘要）与视频处理技术相结合，以将长视频转换为单个相对短的视频。我们以用户可以指定汇总视频的相对长度的方式设计此通行费。我们还探索了将多个视频汇总和串联到单个简短视频中的方法，这将有助于在单个简短视频中获得同一主题中最重要的概念。 OUT方法表明，视频总结是一项艰巨但重要的工作，具有进一步的研发潜力，并且由于NLP模型的开发，这是可能的。

### Towards Language-Driven Video Inpainting via Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.10226)] [[cool](https://papers.cool/arxiv/2401.10226)] [[pdf](https://arxiv.org/pdf/2401.10226)]
> **Authors**: Jianzong Wu,Xiangtai Li,Chenyang Si,Shangchen Zhou,Jingkang Yang,Jiangning Zhang,Yining Li,Kai Chen,Yunhai Tong,Ziwei Liu,Chen Change Loy
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: CVPR-2024. Project Page: https://jianzongwu.github.io/projects/rovi
- **标题**: 通过多模式大型语言模型介绍语言驱动的视频
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们介绍了一项新任务 - 语言驱动的视频介绍，该视频使用自然语言说明来指导介入过程。这种方法克服了取决于手动标记的二进制面具的传统视频介绍方法的局限性，这一过程通常乏味且劳动力密集。我们通过说明（ROVI）数据集中的视频中删除对象，其中包含5,650个视频和9,091个镶嵌结果，以支持该任务的培训和评估。我们还提出了一种基于新颖的基于语言驱动的新型视频介绍框架，这是该任务的第一个端到端基线，集成了多模式的大型语言模型，以有效地理解和执行基于复杂的语言的inpainting请求。我们的全面结果展示了数据集的多功能性以及模型在各种语言实施介绍场景中的有效性。我们将公开提供数据集，代码和模型。

### MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer 
[[arxiv](https://arxiv.org/abs/2401.10208)] [[cool](https://papers.cool/arxiv/2401.10208)] [[pdf](https://arxiv.org/pdf/2401.10208)]
> **Authors**: Changyao Tian,Xizhou Zhu,Yuwen Xiong,Weiyun Wang,Zhe Chen,Wenhai Wang,Yuntao Chen,Lewei Lu,Tong Lu,Jie Zhou,Hongsheng Li,Yu Qiao,Jifeng Dai
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: 20 pages, 9 figures, 17 tables
- **标题**: MM交换：通过多模式特征同步器进行交织的图像文本生成建模
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 开发用于交织的图像文本数据的生成模型既具有研究和实用价值。它需要模型来理解交错的序列，然后生成图像和文本。但是，现有尝试受到固定数量的视觉令牌无法有效捕获图像细节的问题的限制，这在多图像场景中尤其有问题。为了解决这个问题，本文介绍了MM Interleaved，这是一个用于交织的图像文本数据的端到端生成模型。它引入了一个多尺度和多图像功能同步模块，可在生成过程中的上下文中直接访问对细粒的图像特征。在配对和交织的图像文本语料库中，MM Interleaved均已端到端进行了预先训练。通过监督的微调阶段进一步增强了它，其中该模型提高了其遵循复杂的多模式指令的能力。实验证明了按多模式指令识别视觉细节的MM间隙的多功能性，并在文本和视觉条件下生成一致的图像。代码和型号可在\ url {https://github.com/opengvlab/mm-interleaved}上找到。

### CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Re-Identification 
[[arxiv](https://arxiv.org/abs/2401.10011)] [[cool](https://papers.cool/arxiv/2401.10011)] [[pdf](https://arxiv.org/pdf/2401.10011)]
> **Authors**: Yanwei Zheng,Xinpeng Zhao,Chuanlin Lan,Xiaowei Zhang,Bowen Huang,Jibin Yang,Dongxiao Yu
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: 9 pages, 6 figures
- **标题**: CPCL：针对弱监督的基于文本的人重新识别的跨模式原型对比度学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 受弱监督的基于文本的人的重新识别（TPRE-ID）试图使用文本描述来检索目标人的图像，而无需依赖身份注释，并且更具挑战性和实用性。主要的挑战是类内差异，包括模式内特征变化和跨模式语义差距。先前的工作集中在实例级样本上，并忽略了每个内在且不变的每个人的原型特征。在此方面，我们提出了一种跨模式原型对比度学习（CPCL）方法。在实践中，CPCL首次将剪辑模型引入了弱监督的TPRE-ID，将视觉和文本实例映射到共享的潜在空间中。随后，提出的原型多模式记忆（PMM）模块通过混合跨模式匹配（HCM）模块以多到术的映射方式捕获了属于同一人的图像文本对的异质模态之间的关联。此外，异常伪标签挖掘（OPLM）模块进一步将有价值的离群样本与每种模式区分开，从而通过挖掘图像文本对之间的隐含关系来增强更可靠的簇的创建。实验结果表明，我们提出的CPCL在所有三个公共数据集上都达到了最先进的性能，在Cuhk-Pedes，ICFG-Pedes和RSTPREID数据集上，等级为11.58％，8.77％和5.25％。该代码可在https://github.com/codegallery24/cpcl上找到。

### Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation 
[[arxiv](https://arxiv.org/abs/2401.10005)] [[cool](https://papers.cool/arxiv/2401.10005)] [[pdf](https://arxiv.org/pdf/2401.10005)]
> **Authors**: Kohei Uehara,Nabarun Goswami,Hanqin Wang,Toshiaki Baba,Kohtaro Tanaka,Tomohiro Hashimoto,Kai Wang,Rei Ito,Takagi Naoya,Ryo Umagami,Yingyi Wen,Tanachai Anakewat,Tatsuya Harada
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: No comments
- **标题**: 通过明确的链条和视觉问题的生成来推进大型多模式模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 对能够解释和推理视觉内容的智能系统的需求不断增长，需要开发大型视觉和语言模型（VLM），这些模型不仅准确，而且具有明确的推理能力。本文提出了一种新的方法来开发VLM，能够根据视觉内容和文本说明进行明确的推理。我们介绍了一个可以提出问题以获取必要知识的系统，从而增强了推理过程的鲁棒性和明确性。为此，我们开发了一个由大型语言模型（LLM）产生的新型数据集，旨在促进经过思考的推理与问答机制相结合。该数据集涵盖了一系列任务，从字幕生成等常见任务到需要专业知识的专业VQA任务。此外，使用我们创建的数据集，我们对现有的VLM进行了微调。该培训使模型能够在推理过程中产生问题并执行迭代推理。结果表明，在面对模棱两可的视觉输入时，能够明确地推理并主动寻求信息，迈向了更健壮，准确和可解释的VLM。

### WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens 
[[arxiv](https://arxiv.org/abs/2401.09985)] [[cool](https://papers.cool/arxiv/2401.09985)] [[pdf](https://arxiv.org/pdf/2401.09985)]
> **Authors**: Xiaofeng Wang,Zheng Zhu,Guan Huang,Boyuan Wang,Xinze Chen,Jiwen Lu
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: project page: https://world-dreamer.github.io/
- **标题**: 世界梦：通过预测蒙版令牌迈向视频生成的通用世界模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 世界模型在理解和预测世界动态方面起着至关重要的作用，这对于视频生成至关重要。但是，现有的世界模型仅限于游戏或驾驶等特定方案，从而限制了它们捕捉一般世界动态环境复杂性的能力。因此，我们介绍了World Dreamer，这是一种开创性的世界模型，以促进对通用世界物理和动作的全面理解，从而大大增强了视频生成的能力。从大型语言模型的成功中汲取灵感，世界道德人将世界建模作为无监督的视觉序列建模挑战。这是通过将视觉输入映射到离散令牌和预测蒙面的输入来实现的。在此过程中，我们结合了多模式提示，以促进世界模型中的相互作用。我们的实验表明，世界道会在不同场景（包括自然场景和驾驶环境）上生成视频方面表现出色。 WorldDreamer在执行任务中展示了多功能性，例如文本到视频转换，图像 - 托维德合成和视频编辑。这些结果强调了世界梦在捕获各种一般世界环境中动态元素方面的有效性。

### Improving fine-grained understanding in image-text pre-training 
[[arxiv](https://arxiv.org/abs/2401.09865)] [[cool](https://papers.cool/arxiv/2401.09865)] [[pdf](https://arxiv.org/pdf/2401.09865)]
> **Authors**: Ioana Bica,Anastasija Ilić,Matthias Bauer,Goker Erdogan,Matko Bošnjak,Christos Kaplanis,Alexey A. Gritsenko,Matthias Minderer,Charles Blundell,Razvan Pascanu,Jovana Mitrović
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: 26 pages
- **标题**: 改善图像文本预训练中的细粒度理解
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 我们引入了稀疏的细粒对比对准（SPARC），这是一种从图像文本对中预处理更细粒的多模式表示的简单方法。鉴于多个图像补丁通常与单个单词相对应，我们建议为标题中的每个令牌学习图像补丁的分组。为了实现这一目标，我们在图像贴片和语言令牌之间使用稀疏的相似性度量，并为每个令牌的语言组嵌入的视觉嵌入作为补丁的加权平均值。然后，通过细粒度的序列损失仅取决于单个样本，不需要其他批处理样本作为负面，将令牌和语言组的视觉嵌入嵌入。这使得可以以廉价的方式学习更多详细的信息。 SPARC将这种细粒度的损失与全局图像和文本嵌入之间的对比损失结合在一起，以学习同时编码全球和本地信息的表示形式。我们彻底评估了我们提出的方法，并在依靠粗粒信息的图像级任务上表现出相对于竞争方法的提高性能，例如分类以及依靠细粒度信息的区域级任务，例如检索，对象检测和分割。此外，SPARC改善了基础视觉模型中的模型忠诚和字幕。

### Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.09861)] [[cool](https://papers.cool/arxiv/2401.09861)] [[pdf](https://arxiv.org/pdf/2401.09861)]
> **Authors**: Li Sun,Liuan Wang,Jun Sun,Takayuki Okatani
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: 7 pages, 7 figures
- **标题**: 时间洞察力增强：减轻多模式模型中的时间幻觉
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式大语言模型（MLLM）的最新进展显着增强了对多媒体内容的理解，从而汇集了各种方式，例如文本，图像和视频。但是，这些模型面临的关键挑战，尤其是在处理视频输入时，发生了幻觉 - 错误的看法或解释，尤其是在事件级别。这项研究介绍了一种创新的方法，可以解决MLLM中事件级别的幻觉，重点介绍视频内容中的特定时间理解。我们的方法利用了一个新颖的框架，该框架从事件查询和提供的视频中提取并利用事件的特定信息来完善MLLM的响应。我们提出了一种独特的机制，将按需事件查询分解为标志性动作。随后，我们采用诸如clip和blip2之类的模型来预测事件发生的特定时间戳。我们使用Charades-STA数据集进行的评估表明，时间幻觉显着降低，并提高了与事件相关的响应的质量。这项研究不仅为解决MLLM的关键局限性提供了新的观点，而且还为在与时间相关问题的背景下评估MLLM的定量测量方法提供了贡献。

### CLIP Model for Images to Textual Prompts Based on Top-k Neighbors 
[[arxiv](https://arxiv.org/abs/2401.09763)] [[cool](https://papers.cool/arxiv/2401.09763)] [[pdf](https://arxiv.org/pdf/2401.09763)]
> **Authors**: Xin Zhang,Xin Zhang,YeMing Cai,Tianzhi Jia
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: CLIP model, KNN, image-to-prompts
- **标题**: 图像的剪辑模型基于顶级邻居的文本提示
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 近年来，文本对图像合成是多模式生成的一个子场。我们为图像到prompt生成提出了一种具有成本效益的方法，该方法利用生成模型生成文本提示，而无需大量带注释的数据。我们将方法分为两个阶段：在线阶段和离线阶段。我们结合了剪辑模型和k-nearest邻居（KNN）算法。提出的系统由两个主要部分组成：脱机任务和在线任务。我们的方法在这些模型中拥有最高的度量0.612，比夹子，夹子 + KNN（前10个）高0.013、0.055、0.011。

### SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition 
[[arxiv](https://arxiv.org/abs/2401.09759)] [[cool](https://papers.cool/arxiv/2401.09759)] [[pdf](https://arxiv.org/pdf/2401.09759)]
> **Authors**: Hao Wang,Shuhei Kurita,Shuichiro Shimizu,Daisuke Kawahara
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: 3rd Workshop on Advances in Language and Vision Research (ALVR 2024)
- **标题**: slideavsr：用于视听语音识别的纸张解释视频数据集
- **领域**: 计算机视觉和模式识别,多媒体,声音,音频和语音处理
- **摘要**: 视听语音识别（AVSR）是自动语音识别（ASR）的多模式扩展，使用视频作为音频的补充。在AVSR中，已经针对面部特征（例如唇部阅读）的数据集进行了大量努力，而它们通常在评估更广泛的环境中的图像理解能力方面经常不足。在本文中，我们使用科学论文解释视频构建了Slideavsr，这是一个AVSR数据集。 Slideavsr提供了一种新的基准测试，其中模型在演示文稿录音上的幻灯片上抄录了语音话语。众所周知，在没有参考文本的情况下转录的论文中经常存在的技术术语是具有挑战性的，我们的SlideAVSR数据集聚焦于AVSR问题的新方面。作为一个简单而有效的基准，我们提出了DocWhisper，即AVSR模型，可以参考幻灯片中的文本信息，并确认其对SlideAvsr的有效性。

### Weakly-Supervised Semantic Segmentation of Circular-Scan, Synthetic-Aperture-Sonar Imagery 
[[arxiv](https://arxiv.org/abs/2401.11313)] [[cool](https://papers.cool/arxiv/2401.11313)] [[pdf](https://arxiv.org/pdf/2401.11313)]
> **Authors**: Isaac J. Sledge,Dominic M. Byrne,Jonathan L. King,Steven H. Ostertag,Denton L. Woods,James L. Prater,Jermaine L. Kennedy,Timothy M. Marston,Jose C. Principe
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: Submitted to the IEEE Journal of Oceanic Engineering
- **标题**: 循环扫描，合成孔径图像的弱监督语义分割
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: 我们提出了一个弱监督的框架，用于圆形扫描合成孔径（CSAS）图像的语义分割。我们框架的第一部分是在图像级标签上以有监督的方式训练的，以发现每个图像中的一组半比较，空间上的区域区域。然后评估每个区域的分类不确定性。然后选择那些不确定性最低的区域在框架的第二部分中被弱标记为像素水平的分割种子。每个种子的范围都根据无监督的，信息理论损失的结构化预测正规化剂逐步调整。这个重塑过程使用多尺度，自适应的功能来描述本地图像内容中特定类别的过渡。在我们的框架的各个部分中插入了内容可调的记忆，以便它可以利用先前看到的图像的功能来提高相关图像的细分性能。我们使用现实世界中的CSA图像评估了弱监督的框架，其中包含十多个海底类别和十个目标类别。我们表明，我们的框架的性能与九个完全监督的深网。我们的框架还胜过11个最好的弱监督的深层网络。在自然图像进行预培训时，我们可以实现最先进的表现。自然图像和声纳图像的平均绝对性能差距与次数弱弱监督的网络的平均性能差距远远超过百分之十。发现此差距具有统计学意义。

### M2-CLIP: A Multimodal, Multi-task Adapting Framework for Video Action Recognition 
[[arxiv](https://arxiv.org/abs/2401.11649)] [[cool](https://papers.cool/arxiv/2401.11649)] [[pdf](https://arxiv.org/pdf/2401.11649)]
> **Authors**: Mengmeng Wang,Jiazheng Xing,Boyuan Jiang,Jun Chen,Jianbiao Mei,Xingxing Zuo,Guang Dai,Jingdong Wang,Yong Liu
> **First submission**: 2024-01-21
> **First announcement**: 2024-01-22
> **comment**: ef:AAAI2024
- **标题**: M2-CLIP：视频动作识别的多模式，多任务改编框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，大规模视觉识别的模型（如剪辑）的兴起，以及参数效率高效的填充技术（PEFT）的兴起，在视频动作识别中吸引了很大的吸引力。然而，普遍的方法倾向于优先考虑强大的监督性能，而牺牲了模型在转移过程中的概括能力。在本文中，我们介绍了一种新型的多式模式，多任务剪辑改编框架，名为\名称，以应对这些挑战，从而确保高监督性能和可靠的可传递性。首先，为了增强各个模态体系结构，我们将多模式适配器引入视觉和文本分支。具体而言，我们设计了一种新型的视觉TED适应器，该适应器执行全局时间增强和局部时间差异建模，以提高视觉编码器的时间表示功能。此外，我们采用文本编码器适配器来加强语义标签信息的学习。其次，我们设计了一个具有丰富的监督信号的多任务解码器，以熟练满足多模式框架内强制性监督性能和概括的需求。实验结果验证了我们方法的功效，证明了在监督学习中表现出色的表现，同时在零拍摄的情况下保持强烈的概括。

### Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss 
[[arxiv](https://arxiv.org/abs/2401.11633)] [[cool](https://papers.cool/arxiv/2401.11633)] [[pdf](https://arxiv.org/pdf/2401.11633)]
> **Authors**: Jordan Shipard,Arnold Wiliem,Kien Nguyen Thanh,Wei Xiang,Clinton Fookes
> **First submission**: 2024-01-21
> **First announcement**: 2024-01-22
> **comment**: 15 pages
- **标题**: Zoom-shot：快速有效地无监督的夹子转移到具有多模式损失的视觉编码器
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉和语言的融合通过视觉模型（VLM）的出现使计算机视觉发生了变革。但是，现有VLM的资源密集型性质构成了重大挑战。我们需要一种可访问的方法来开发下一代VLM。为了解决这个问题，我们提出了Zoom-shot，这是一种新颖的方法，用于将夹子的零拍功能转移到任何预训练的视觉编码器中。我们通过利用专门设计的多模式损耗函数来利用剪辑潜在空间中存在的多模式信息（即文本和图像）来做到这一点。这些损失函数是（1）周期矛盾损失，以及（2）我们新颖的迅速引入的知识蒸馏损失（PG-KD）。 PG-KD将知识蒸馏的概念与夹子的零弹分类结合在一起，以捕获文本和图像特征之间的相互作用。有了我们的多模式损失，我们在剪辑潜在空间和预训练的视觉编码器的潜在空间之间训练$ \ textbf {线性映射} $，仅用于$ \ textbf {single epoch} $。此外，变焦射击是完全无监督的，并使用$ \ textbf {nofeed} $ data进行了训练。我们测试了一系列视觉编码器的零射击功能，在粗糙和细粒度的分类数据集上增强了作为新的VLM的增强功能，在此问题域中表现出色。在消融中，我们发现Zoom-shot允许在培训期间进行数据和计算之间的权衡；并且我们的最先进结果可以通过将20个时期的训练从20％的成像网训练数据降低到1％。所有代码和模型均可在GitHub上找到。

### Self-Supervised Bird's Eye View Motion Prediction with Cross-Modality Signals 
[[arxiv](https://arxiv.org/abs/2401.11499)] [[cool](https://papers.cool/arxiv/2401.11499)] [[pdf](https://arxiv.org/pdf/2401.11499)]
> **Authors**: Shaoheng Fang,Zuhong Liu,Mingyu Wang,Chenxin Xu,Yiqi Zhong,Siheng Chen
> **First submission**: 2024-01-21
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 跨模式信号的自我监督的鸟类视图运动预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 以自我监督的方式学习密集的鸟类视图（BEV）运动流是针对机器人技术和自动驾驶的一项新兴研究。当前的自我监督方法主要依赖于点云之间的点对应关系，这可能会引入假流和不一致的问题，从而阻碍了模型学习准确和现实的运动的能力。在本文中，我们介绍了一种新颖的跨模式自我监督训练框架，该培训框架通过利用多模式数据来获得监督信号来有效解决这些问题。我们设计了三个创新的监督信号，以保留场景运动的固有特性，包括掩盖的倒角距离损失，分段刚度损失和时间一致性损失。通过广泛的实验，我们证明了我们提出的自我监管框架优于运动预测任务的所有以前的自我审视方法。

### Exploring Missing Modality in Multimodal Egocentric Datasets 
[[arxiv](https://arxiv.org/abs/2401.11470)] [[cool](https://papers.cool/arxiv/2401.11470)] [[pdf](https://arxiv.org/pdf/2401.11470)]
> **Authors**: Merey Ramazanova,Alejandro Pardo,Humam Alwassel,Bernard Ghanem
> **First submission**: 2024-01-21
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 探索多模式的egocentric数据集中缺失的方式
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的视频理解对于分析以自我为中心的视频至关重要，在该视频中，整合多个感官信号会显着增强动作识别和力矩定位。但是，由于隐私问题，效率要求或硬件故障等因素，实际应用通常会努力应对不完整的方式。解决这个问题时，我们的研究深入研究了缺失模式对以自我为中心行动识别的影响，尤其是在基于变压器的模型中。我们介绍了一种新颖的概念 - 邀请模式令牌（MMT），即使缺乏模态，也证明在EGO4D，Epic-kitchens和Epic-sounds的数据集中有效的策略。我们的方法减轻了性能损失，将其从原始$ \ sim 30 \％$下降到仅$ \ sim 10 \％$时，当测试集的一半是模态分配时。通过广泛的实验，我们证明了MMT对不同训练方案的适应性及其在处理缺失方式的优势与当前方法相比。我们的研究贡献了全面的分析和一种创新的方法，为现实世界中更有弹性的多模式系统开辟了途径。

### LLMRA: Multi-modal Large Language Model based Restoration Assistant 
[[arxiv](https://arxiv.org/abs/2401.11401)] [[cool](https://papers.cool/arxiv/2401.11401)] [[pdf](https://arxiv.org/pdf/2401.11401)]
> **Authors**: Xiaoyu Jin,Yuan Shi,Bin Xia,Wenming Yang
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: LLMRA：基于多模式的大型模型恢复助手
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大型语言模型（MLLM）由于广泛的知识以及强大的感知和发电能力而对各种任务产生重大影响。但是，对于将MLLM应用于低级视力任务，仍然是一个开放的研究问题。在本文中，我们提出了一个简单的基于MLLM的图像恢复框架，以解决此差距，即基于多模式的基于多模型的恢复助手（LLMRA）。我们利用MLLM的令人印象深刻的功能来获得通用图像恢复的降解信息。通过采用预审预定的多模式大型语言模型和视觉语言模型，我们生成文本描述并将其编码为上下文嵌入，其中包含降级信息的降级图像。通过提出的上下文增强模块（CEM）和基于降解上下文的变压器网络（DC形式），我们将这些上下文嵌入到恢复网络中，从而有助于更准确，可调节的图像恢复。基于与用户的对话，我们的方法利用MLLM的图像降级先验，提供了低级属性的输入低质量图像的描述和同时还原的高质量图像。广泛的实验证明了我们的LLMRA在通用图像恢复任务中的出色表现。

### UniM-OV3D: Uni-Modality Open-Vocabulary 3D Scene Understanding with Fine-Grained Feature Representation 
[[arxiv](https://arxiv.org/abs/2401.11395)] [[cool](https://papers.cool/arxiv/2401.11395)] [[pdf](https://arxiv.org/pdf/2401.11395)]
> **Authors**: Qingdong He,Jinlong Peng,Zhengkai Jiang,Kai Wu,Xiaozhong Ji,Jiangning Zhang,Yabiao Wang,Chengjie Wang,Mingang Chen,Yunsheng Wu
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: Accepted by IJCAI 2024
- **标题**: UNIM-OV3D：UNI-MODAILATY开放式Vocabulary 3D场景理解，并具有细粒度代表
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D开放式摄影场景的理解旨在认识到基本标签空间以外的任意新型类别。但是，现有作品不仅无法充分利用3D域中的所有可用模式信息，而且在表示每种模式的特征方面缺乏足够的粒度。在本文中，我们提出了一个统一的多模式3D开放式视频场景理解网络，即Unim-ov3d，该网络将点云与图像，语言和深度保持一致。为了更好地整合点云的全球和本地特征，我们设计了一个分层云特征提取模块，该模块可以学习全面的细粒特征表示。此外，为了促进从字幕中学习粗到1点的点语音表示，我们建议利用层次结构3D字幕对，利用3D场景各种观点的几何约束。广泛的实验结果表明，我们方法在开放式语义语义和实例分段中的有效性和优势，在室内和室外基准上都达到了最新的性能，例如扫描，Scannet200，S3IDS和Nuscenes。代码可在https://github.com/hithqd/unim-ov3d上找到。

### Prompting Large Vision-Language Models for Compositional Reasoning 
[[arxiv](https://arxiv.org/abs/2401.11337)] [[cool](https://papers.cool/arxiv/2401.11337)] [[pdf](https://arxiv.org/pdf/2401.11337)]
> **Authors**: Timothy Ossowski,Ming Jiang,Junjie Hu
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 促使大型视觉模型用于组成推理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 诸如剪辑之类的视觉模型在将文本和图像编码为对齐的嵌入式中显示出令人印象深刻的功能，从而可以在共享嵌入空间中检索多模式数据。但是，这些基于嵌入的模型在有效地匹配具有类似visio语言构图的图像和文本时仍面临挑战，这是由于它们在最近的Winoground数据集中的性能所证明的。在本文中，我们认为这种限制源于两个因素：使用单个向量表示复杂的多模式数据，以及在这些基于嵌入的方法中缺乏逐步推理。为了解决这个问题，我们使用一种新颖的生成方法做出探索性步骤，该方法促使大型视觉模型（例如GPT-4）描绘图像并执行组成推理。我们的方法在Winoground数据集上优于其他基于嵌入的方法，并在增强最佳描述时可以进一步提高10％的准确性。

### Unifying Visual and Vision-Language Tracking via Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2401.11228)] [[cool](https://papers.cool/arxiv/2401.11228)] [[pdf](https://arxiv.org/pdf/2401.11228)]
> **Authors**: Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Jinpeng Zhang,Mengxue Kang
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 通过对比度学习统一视觉和视觉跟踪
- **领域**: 计算机视觉和模式识别
- **摘要**: 单个对象跟踪旨在根据不同模态引用指定的状态（包括初始边界框（Bbox），自然语言（NL）或两者（NL+Bbox），将目标对象定位在视频序列中。由于不同模态之间的差距，大多数现有的跟踪器都是为这些参考设置的单个或部分设计而设计的，并在特定方式上过度专业。不同的是，我们提出了一个称为UVLTRACK的统一跟踪器，该跟踪器可以同时处理具有相同参数的所有三个参考设置（Bbox，NL，NL+Bbox）。拟议的Uvltrack具有多种优点。首先，我们为联合视觉和语言特征学习设计了一种模态统一的功能提取器，并提出了多模式的对比损失，以将视觉和语言特征对准统一的语义空间。其次，提出了一个模态自适应盒头，该盒子的头部充分利用目标引用，以动态地与视频上下文进行动态变化的方案特征，并以对比的方式区分目标，从而在不同的参考设置中实现稳健的性能。广泛的实验结果表明，UVLTRACK在七个视觉跟踪数据集，三个视觉语言跟踪数据集和三个视觉接地数据集上实现了有希望的性能。代码和模型将在https://github.com/openspaceai/uvltrack上开源。

### Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images 
[[arxiv](https://arxiv.org/abs/2401.11170)] [[cool](https://papers.cool/arxiv/2401.11170)] [[pdf](https://arxiv.org/pdf/2401.11170)]
> **Authors**: Kuofeng Gao,Yang Bai,Jindong Gu,Shu-Tao Xia,Philip Torr,Zhifeng Li,Wei Liu
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: Accepted by ICLR 2024
- **标题**: 用冗长的图像诱导大型视觉模型的高能延迟
- **领域**: 计算机视觉和模式识别,密码学和安全
- **摘要**: 大型视觉模型（VLM）（例如GPT-4）在各种多模式任务中都取得了出色的性能。但是，VLM的部署需要大量的能耗和计算资源。一旦攻击者恶意诱导了VLMS推断期间高能量消耗和潜伏时间（能源潜伏成本），它将耗尽计算资源。在本文中，我们探讨了有关VLM的可用性的这种攻击表面，并旨在在VLMS推断期间引起高能量延迟成本。我们发现，可以通过最大化生成的序列的长度来操纵VLMS推断期间的高能延迟成本。为此，我们提出了详细的图像，目的是制定不可察觉的扰动以诱导VLMS在推理过程中产生长句子。具体而言，我们设计了三个损失目标。首先，提出了损失，以延迟序列末端（EOS）令牌的发生，其中EOS令牌是VLMS停止产生进一步令牌的信号。此外，提出了不确定性损失和令牌多样性损失，以增加每个产生的令牌的不确定性以及分别为整个生成序列的所有令牌之间的多样性，这可以打破令牌级别和序列级别的输出依赖性。此外，提出了时间重量调节算法，可以有效地平衡这些损失。广泛的实验表明，与MS-Coco和Imagenet数据集中的原始图像相比，我们的详细图像可以将生成序列的长度增加7.87次和8.56次，这给各种应用带来了潜在的挑战。我们的代码可从https://github.com/kuofenggao/verbose_images获得。

### Make-A-Shape: a Ten-Million-scale 3D Shape Model 
[[arxiv](https://arxiv.org/abs/2401.11067)] [[cool](https://papers.cool/arxiv/2401.11067)] [[pdf](https://arxiv.org/pdf/2401.11067)]
> **Authors**: Ka-Hei Hui,Aditya Sanghi,Arianna Rampini,Kamal Rahimi Malekshan,Zhengzhe Liu,Hooman Shayani,Chi-Wing Fu
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 成型形状：100万尺度的3D形型号
- **领域**: 计算机视觉和模式识别,图形
- **摘要**: 在培训自然语言和图像的大型生成模型中已经取得了重大进展。然而，3D生成模型的进步受到其对培训的大量资源需求的阻碍，以及效率低下，非压缩和表现力较低的表示。本文介绍了Make-A Shape，这是一种新的3D生成模型，旨在在广泛的规模上进行有效的培训，能够利用100万个公开可用的形状。从技术方面来说，我们首先通过制定子带系数过滤方案来有效利用系数关系，首先对小波树表示进行紧凑的编码形状。然后，我们通过设计子带系数包装方案来使表示形式在低分辨率网格中布局来使表示形式通过扩散模型产生。此外，我们得出了子带自适应训练策略，以训练我们的模型，以有效地学习生成粗糙和细节的小波系数。最后，我们扩展了框架，以通过其他输入条件来控制，以使其能够从各种模态（例如单/多视图图像，点云和低分辨率体素体）生成形状。在我们广泛的实验集中，我们演示了各种应用，例如无条件的生成，形状完成和有条件的生成，以广泛的方式产生。我们的方法不仅超过了能够提供高质量结果的最新技术，而且还可以在几秒钟内有效地产生形状，通常在大多数情况下仅在2秒钟内实现这一目标。我们的源代码可在https://github.com/autodeskailab/make-a-shape上找到。

### Exploring scalable medical image encoders beyond text supervision 
[[arxiv](https://arxiv.org/abs/2401.10815)] [[cool](https://papers.cool/arxiv/2401.10815)] [[pdf](https://arxiv.org/pdf/2401.10815)]
> **Authors**: Fernando Pérez-García,Harshita Sharma,Sam Bond-Taylor,Kenza Bouzid,Valentina Salvatelli,Maximilian Ilse,Shruthi Bannur,Daniel C. Castro,Anton Schwaighofer,Matthew P. Lungren,Maria Teodora Wetscherek,Noel Codella,Stephanie L. Hyland,Javier Alvarez-Valle,Ozan Oktay
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: ef:Nature Machine Intelligence (2025)
- **标题**: 探索可扩展的医疗图像编码超出文本监督
- **领域**: 计算机视觉和模式识别
- **摘要**: 被证明是从图像中提取具有语义意义的特征的一种有价值的方法，它是计算机视觉和医学成像域中多模式系统中的基础元素。但是，计算的特征受文本中包含的信息的限制，文本在医学成像中尤其有问题，放射科医生描述的发现着重于特定的观察结果。由于对个人健康信息泄漏的担忧，配对成像文本数据的稀缺性稀缺使这一挑战更加复杂。在这项工作中，我们从根本上挑战了对学习通用生物医学成像编码器的语言监督的依赖。我们介绍了Rad-Dino，这是一种仅在单峰生物医学成像数据上进行的生物医学图像编码器预训练，该数据比在各种基准测试范围内获得的最先进的生物医学监督模型相比获得了相似或更高的性能。具体而言，在标准成像任务（分类和语义分割）以及视觉校准任务（图像中的文本报告生成）上评估了学习表示的质量。为了进一步证明语言监督的弊端，我们表明，Rad-Dino的特征与其他医学记录（例如性别或年龄）相比，比语言监督的模型更好，这在放射学报告中通常没有提及。最后，我们进行了一系列消融，以确定Rad-Dino表现的因素。值得注意的是，我们观察到Rad-Dino的下游性能随训练数据的数量和多样性而良好，表明仅图像的监督是训练基础生物医学图像编码器的可扩展方法。可以在https://huggingface.co/microsoft/rad-dino上获得在公开可用数据集上训练的Rad-Dino的模型权重。

### Removal then Selection: A Coarse-to-Fine Fusion Perspective for RGB-Infrared Object Detection 
[[arxiv](https://arxiv.org/abs/2401.10731)] [[cool](https://papers.cool/arxiv/2401.10731)] [[pdf](https://arxiv.org/pdf/2401.10731)]
> **Authors**: Tianyi Zhao,Maoxun Yuan,Feng Jiang,Nan Wang,Xingxing Wei
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: 11pages, 10figures
- **标题**: 删除然后选择：RGB-Infrared对象检测的粗到细节融合视角
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，利用可见的（RGB）和热红外（IR）图像的对象检测引起了广泛的关注，并已广泛实现在各种领域。通过利用RGB和IR图像之间的互补属性，对象检测任务可以在从白天到夜间环境中在各种照明条件上实现可靠且可靠的对象定位。大多数现有的多模式对象检测方法将RGB和IR图像直接输入深度神经网络，从而导致次要检测性能。我们认为，这个问题不仅源于与有效整合多模式信息相关的挑战，而且还源于RGB和IR模式中冗余特征的存在。每种方式的冗余信息将加剧融合不精确的问题。为了解决这个问题，我们从人类大脑的处理多模式信息的机制中汲取了灵感，并提出了一种新颖的粗到精细的视角，以净化和融合两种方式的特征。具体来说，遵循此视角，我们设计了一个冗余频谱删除模块，以删除每种模式中的干扰信息，并设计一个动态特征选择模块，以精细选择特征融合的所需功能。为了验证粗到十五融合策略的有效性，我们构建了一个称为去除的新对象检测器，然后选择检测器（RSDET）。在三个RGB-IR对象检测数据集上进行了广泛的实验，验证了我们方法的出色性能。

### MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning 
[[arxiv](https://arxiv.org/abs/2401.10727)] [[cool](https://papers.cool/arxiv/2401.10727)] [[pdf](https://arxiv.org/pdf/2401.10727)]
> **Authors**: Chenyu Wang,Weixin Luo,Qianyu Chen,Haonan Mai,Jindi Guo,Sixun Dong,Xiaohua,Xuan,Zhengxin Li,Lin Ma,Shenghua Gao
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: 21 pages, 9 figures, 10 tables
- **标题**: MLLM-Tool：一种用于工具代理学习的多模式大语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，大型语言模型（LLM）在自然语言理解和发电任务中的惊人表现引发了大量探索，将其用作中央控制器来构建代理系统。多项研究着重于将LLM桥接到外部工具以扩展应用程序方案。但是，当前LLM的感知工具使用能力仅限于单个文本查询，这可能会导致理解用户的真实意图的歧义。有望通过感知视觉或听觉的指令的信息来消除LLM。因此，在本文中，我们提出了MLLM-Tool，该系统包含开源LLMS和多模式编码器，以便学习的LLM可以意识到多模式输入指令，然后正确选择功能匹配的工具。为了促进对模型功能的评估，我们收集了一个数据集，该数据集由HuggingFace中的多模式输入工具组成。数据集的另一个重要特征是，由于存在相同的功能和同义函数，我们的数据集还包含相同指令的多个潜在选择，该功能为同一查询提供了更多的潜在解决方案。实验表明，我们的MLLM-Tool能够为多模式说明推荐适当的工具。代码和数据可在https://github.com/mllm-tool/mllm-tool上找到。

### Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge 
[[arxiv](https://arxiv.org/abs/2401.10712)] [[cool](https://papers.cool/arxiv/2401.10712)] [[pdf](https://arxiv.org/pdf/2401.10712)]
> **Authors**: Haibo Wang,Weifeng Ge
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: Accepted by ECCV'24
- **标题**: 问答提示：通过采矿提问来发现丰富的视觉线索，提示VQA需要多样化的世界知识
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 随着多模式大语言模型的突破，回答需要先进的推理能力和世界知识的复杂视觉问题已成为开发AI模型的重要测试。但是，为AI模型配备强大的跨模式推理能力仍然具有挑战性，因为人类的认知方案尚未系统地理解。在本文中，我们认为，如果我们可以尽可能多地收集给定图像中的视觉线索，我们将更准确地识别图像，更好地理解问题，更轻松地回忆相关知识，最后推理答案。我们通过在图像中挖掘提问 - 答案将它们挖掘出来，并将其发送到多模式大语言模型作为提示，从而发现这些丰富的视觉线索。我们调用建议的方法问答提示。具体来说，我们首先将图像解答对以及训练集中的相应问题作为输入和输出来训练视觉问题生成模型。然后，我们使用图像标记模型来识别各种实例，并将包装的图像标签对发送到视觉问题生成模型中，以用提取的图像标签作为答案生成相关问题。最后，我们用视觉吸引提示模块编码这些生成的问题 - 答案对，并将它们发送到预先训练的多模式大型语言模型中，以理解最终答案。实验结果表明，与最先进的方法相比，我们的问答提示可以在挑战性的视觉问题上取得了重大改进，该问题回答数据集需要对多样化的世界知识进行推理，例如OK-VQA和A-OKVQA。

### Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering 
[[arxiv](https://arxiv.org/abs/2401.10711)] [[cool](https://papers.cool/arxiv/2401.10711)] [[pdf](https://arxiv.org/pdf/2401.10711)]
> **Authors**: Haibo Wang,Chenghang Lai,Yixuan Sun,Weifeng Ge
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: accepted by ACM Multimedia 2024
- **标题**: 弱监督的高斯对比接地，具有大型多模式用于视频问题的模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 视频问题回答（VideoQA）旨在根据视频中观察到的信息回答自然语言问题。尽管大型多模型模型（LMM）在图像语言的理解和推理中取得了成功，但它们仅将均匀采样的框架作为视觉输入而忽略了与问题相关的视觉线索来处理视频QA。此外，在现有的VideoQA数据集中没有针对问题的时间戳的人类注释。鉴于此，我们提出了一个新颖的弱监督框架，以实施LMM，以将关键问题作为视觉输入来推理答案。具体而言，我们首先将问题和答案对作为事件描述融合，以找到多个关键帧作为目标矩和伪标记，并与剪辑模型的视觉语言对齐能力。通过这些伪标记的密钥框架作为较弱的监督，我们设计了一个基于高斯轻型的对比接地（GCG）模块。 GCG学习了多个高斯功能，以表征视频的时间结构，并将关键问题框架作为积极矩作为LMM的视觉输入。对几个基准测试的广泛实验验证了我们框架的有效性，与以前的最新方法相比，我们实现了实质性改进。

### DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval 
[[arxiv](https://arxiv.org/abs/2401.10588)] [[cool](https://papers.cool/arxiv/2401.10588)] [[pdf](https://arxiv.org/pdf/2401.10588)]
> **Authors**: Xiangpeng Yang,Linchao Zhu,Xiaohan Wang,Yi Yang
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: AAAI2024, Code will be available at https://github.com/knightyxp/DGL
- **标题**: DGL：用于文本视频检索的动态全局及时提示调整
- **领域**: 计算机视觉和模式识别
- **摘要**: 文本视频检索是找到文本查询最相关的视频的关键多模式任务。尽管经过验证的模型在该领域表现出了令人印象深刻的潜力，但由于模型尺寸的增加而完全填充这些模型的成本上升的成本继续构成问题。为了应对这一挑战，迅速调整已成为替代方案。但是，现有作品在调整预贴的图像文本模型到下游视频文本任务时仍然遇到两个问题：（1）视觉编码器只能编码帧级别的功能，并且无法提取全球级别的常规视频信息。 （2）为视觉和文本编码器配备分离的提示，无法减轻视觉文本模态差距。为此，我们提出了DGL，这是一种跨模式的动态及时调整方法，并具有全球 - 本地视频的关注。与以前的及时调整方法相反，我们采用共享的潜在空间来生成局部级文本和框架提示，以鼓励模式间相互作用。此外，我们建议在全球本地注意机制中对视频进行建模，以从迅速调整的角度捕获全球视频信息。广泛的实验表明，当仅调整0.67％的参数时，我们的跨模式调整策略DGL表现优于表现，或者与MSR-VTT，VATEX，LSMDC和Activitynet Netnet数据集的完全易登录方法相媲美。代码将在https://github.com/knightyxp/dgl上找到

### Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences 
[[arxiv](https://arxiv.org/abs/2401.10529)] [[cool](https://papers.cool/arxiv/2401.10529)] [[pdf](https://arxiv.org/pdf/2401.10529)]
> **Authors**: Xiyao Wang,Yuhang Zhou,Xiaoyu Liu,Hongjin Lu,Yuancheng Xu,Feihong He,Jaehong Yoon,Taixi Lu,Gedas Bertasius,Mohit Bansal,Huaxiu Yao,Furong Huang
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: 27 pages, 23 figures
- **标题**: 纪念品：多模式大语言模型推理的全面基准，图像序列
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 多模式的大语言模型（MLLM）表现出在处理各种视觉语言任务方面的熟练程度。但是，当前的MLLM基准主要是设计用于基于有关单个图像的静态信息评估推理的，而现代MLLM从图像序列中推断出对理解我们不断变化的世界至关重要的能力的研究较少。为了应对这一挑战，本文介绍了Mementos，这是一种新的基准测试，旨在评估MLLMS的顺序图像推理能力。 Mementos具有4,761种不同长度的不同图像序列。我们还采用GPT-4辅助方法来评估MLLM推理性能。通过仔细评估包括GPT-4V和Gemini在内的九个最近的MLLM，我们发现它们很难准确描述有关给定图像序列的动态信息，通常会导致对象及其相应行为的幻觉/虚假陈述。我们的定量分析和案例研究确定了影响MLLM的顺序图像推理的三个关键因素：对象与行为幻觉之间的相关性，同时存在行为的影响以及行为幻觉的复杂影响。我们的数据集可从https://github.com/umd-huang-lab/mementos获得。

### Training-Free Action Recognition and Goal Inference with Dynamic Frame Selection 
[[arxiv](https://arxiv.org/abs/2401.12471)] [[cool](https://papers.cool/arxiv/2401.12471)] [[pdf](https://arxiv.org/pdf/2401.12471)]
> **Authors**: Ee Yeo Keat,Zhang Hao,Alexander Matyasko,Basura Fernando
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: 通过动态框架选择的无训练动作识别和目标推断
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们介绍了VIDTFS，这是一种无训练的开放式视频视频目标和动作推理框架，将冷冻视觉基础模型（VFM）和大型语言模型（LLM）与新颖的动态框架选择模块结合在一起。我们的实验表明，提出的框架选择模块可显着提高框架的性能。我们在四个广泛使用的视频数据集上验证了拟议的VIDTF的性能，包括Crosstask，Coin，UCF101和ActivityNet，涵盖了目标推理和行动识别任务，而无需任何培训或进行任何培训或精心调整。结果表明，VIDTF的表现优于预测和指令调整的多模式模型，该模型直接堆叠LLM和VFM，用于下游视频推理任务。我们的适应性具有适应性的VIDTF显示了对新的无培训视频推理任务推广的未来潜力。

### Exploration and Improvement of Nerf-based 3D Scene Editing Techniques 
[[arxiv](https://arxiv.org/abs/2401.12456)] [[cool](https://papers.cool/arxiv/2401.12456)] [[pdf](https://arxiv.org/pdf/2401.12456)]
> **Authors**: Shun Fang,Ming Cui,Xing Feng,Yanan Zhang
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: 基于NERF的3D场景编辑技术的探索和改进
- **领域**: 计算机视觉和模式识别,人工智能,图形
- **摘要**: 在提议后的几年中，学者们很快就接受了Nerf的高质量综合能力，并且在3D场景表示和综合中取得了重大进展。但是，高计算成本限制了场景的直观和高效编辑，这使得Nerf在现场编辑领域的开发面临许多挑战。本文回顾了近年来在现场或对象编辑字段中NERF上学者的初步探索，主要改变了新合成场景中场景或对象的形状和纹理； through the combination of residual models such as GaN and Transformer with NeRF, the generalization ability of NeRF scene editing has been further expanded, including realizing real-time new perspective editing feedback, multimodal editing of text synthesized 3D scenes, 4D synthesis performance, and in-depth exploration in light and shadow editing, initially achieving optimization of indirect touch editing and detail representation in complex scenes.目前，大多数NERF编辑方法都集中在间接点的接触点和材料上，但是在处理更复杂或更大的3D场景时，很难平衡准确性，广度，效率和质量。克服这些挑战可能成为未来NERF 3D场景编辑技术的方向。

### Multi-modal News Understanding with Professionally Labelled Videos (ReutersViLNews) 
[[arxiv](https://arxiv.org/abs/2401.12419)] [[cool](https://papers.cool/arxiv/2401.12419)] [[pdf](https://arxiv.org/pdf/2401.12419)]
> **Authors**: Shih-Han Chou,Matthew Kowal,Yasmin Niknam,Diana Moyano,Shayaan Mehdi,Richard Pito,Cheng Zhang,Ian Knopke,Sedef Akinli Kocak,Leonid Sigal,Yalda Mohsenzadeh
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: 具有专业标签的视频（Reutersvilnews）的多模式新闻理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管在视频语言理解的领域取得了进展，但当前的最新算法仍然有限地了解高级抽象（例如新闻导向视频）的视频的能力。另外，人类很容易从视频和语言合并信息，以推断出超越像素中可视观察到的信息。一个例子是观看新闻报道，在该故事中，活动的背景可以像事件本身一样在理解故事中所扮演重要角色。为了在算法中设计这种能力的解决方案，我们对路透社新闻社收集的内部数据集进行了大规模分析，称为路透社视频语言新闻（Reutersvilnews）数据集，重点介绍了高级视频的理解，并强调了长期新闻。 Reutersvilnews数据集由新闻行业专业人员收集和标记的长期新闻视频在几年内收集和标记，并包含来自世界各地的著名新闻报道。每个视频都涉及一个故事，并包含实际事件的动作镜头，与活动相关的人的访谈，附近地区的录像等等。 Reutersvilnews数据集包含来自七个主题类别的视频：灾难，金融，娱乐，健康，政治，体育和杂项以及从高级到低级的注释，标题字幕，视频描述，高级故事描述，关键字，关键字和位置。我们首先对Reutersvilnews的数据集统计数据进行分析与以前的数据集相比。然后，我们为四个不同的视频语言任务进行最新方法。结果表明，以新闻为导向的视频是当前视频语言理解算法的重大挑战，我们通过在设计方法来解决Reutersvilnews数据集时提供了未来的指示。

### OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection 
[[arxiv](https://arxiv.org/abs/2401.12344)] [[cool](https://papers.cool/arxiv/2401.12344)] [[pdf](https://arxiv.org/pdf/2401.12344)]
> **Authors**: Fatema-E Jannat,Sina Gholami,Minhaj Nur Alam,Hamed Tabkhi
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: 12 pages, 7 figures, 6 tables
- **标题**: OCT-SELFNET：一个具有多模式数据集的自我监督框架，用于广泛性和健壮的视网膜疾病检测
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 尽管AI产生了革命性的影响以及经过本地训练的算法的发展，但从医学AI中的多模式数据中实现了广泛的广泛性学习仍然是一个重大挑战。这一差距阻碍了可扩展的医疗AI解决方案的实际部署。在应对这一挑战时，我们的研究贡献了一个自制的健壮机器学习框架，即Oct-Selfnet，用于使用光学相干断层扫描（OCT）图像来检测眼部疾病。在这项工作中，将来自各个机构的各种数据集结合在一起，以实现更全面的代表性。我们的方法使用两阶段训练方法解决了问题，该方法通过为现实世界中的临床部署提供了解决方案，从而将自我监督预处理和监督的微调与面罩自动编码器与面罩自动编码器结合在一起。在三个具有不同编码器骨架，低数据设置，看不见的数据设置以及增强效果的三个数据集上进行的广泛实验表明，我们的方法的表现优于基线模型，通过始终如一的AUC-ROC性能超过77％，而基线模型超过了54％。此外，就AUC-PR度量而言，我们提出的方法超过42％，与基线相比，绩效至少增长了10％，仅超过33％。这有助于我们理解方法的潜力，并强调其在临床环境中的实用性。

### Multimodal Data Curation via Object Detection and Filter Ensembles 
[[arxiv](https://arxiv.org/abs/2401.12225)] [[cool](https://papers.cool/arxiv/2401.12225)] [[pdf](https://arxiv.org/pdf/2401.12225)]
> **Authors**: Tzu-Heng Huang,Changho Shin,Sui Jiet Tay,Dyah Adila,Frederic Sala
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-23
> **comment**: Appeared in the Workshop of Towards the Next Generation of Computer Vision Datasets (TNGCV) on ICCV 2023
- **标题**: 通过对象检测和滤波器合奏的多模式数据策划
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 我们提出了一种策划多模式数据的方法，我们在2023 Datacomp竞争过滤轨道中使用该方法。我们的技术结合了对象检测和基于监督的结合。在我们方法中的两个步骤中的第一个步骤中，我们采用开箱即用的零击对象检测模型来提取颗粒信息并产生各种过滤器设计。在第二步中，我们采用薄弱的监督来进行整体过滤规则。与表现最好的基线相比，这种方法可提高4％的性能，在写作时在小型轨道中产生最高的位置。此外，在中等规模的轨道中，我们通过简单地将现有基准在弱监督下结合现有基准来取得4.2％的提高。

### Automated facial recognition system using deep learning for pain assessment in adults with cerebral palsy 
[[arxiv](https://arxiv.org/abs/2401.12161)] [[cool](https://papers.cool/arxiv/2401.12161)] [[pdf](https://arxiv.org/pdf/2401.12161)]
> **Authors**: Álvaro Sabater-Gárriz,F. Xavier Gaya-Morey,José María Buades-Rubio,Cristina Manresa Yee,Pedro Montoya,Inmaculada Riquelme
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: 自动化面部识别系统使用深度学习进行脑瘫成人的疼痛评估
- **领域**: 计算机视觉和模式识别
- **摘要**: 背景：具有神经系统疾病的人，尤其是那些自我报告能力和面部表情改变的人的疼痛评估，提出了挑战。现有的措施依靠照顾者的直接观察，缺乏灵敏度和特异性。在脑瘫中，疼痛是一种常见的合并症，可靠的评估方案至关重要。因此，在诊断出这种类型的患者疼痛时，拥有识别面部表情的自动系统可能会有很大帮助。目的：1）在患有脑瘫的个体中建立面部疼痛表达的数据集，以及2）开发一种基于对该人群的疼痛评估的深度学习的自动面部识别系统。方法：在三个疼痛图像数据库中训练了十个神经网络，包括UNBC-MCMASTER肩部疼痛表达档案数据库，多模式强度疼痛数据集和特拉华州疼痛数据库。此外，创建了一个策划的数据集（CPPAIN），由109个脑瘫患者的109个预处理的面部疼痛表达图像组成，并由两名物理治疗师使用面部动作编码系统观察量表进行了分类。结果：InceptionV3在CP-PAIN数据集上表现出令人鼓舞的性能，其准确度为62.67％，F1得分为61.12％。可解释的人工智能技术揭示了跨模型疼痛识别的一致的基本特征。结论：这项研究证明了深度学习模型在神经系统疾病和沟通障碍的人群中的强大疼痛检测的潜力。创建特定于脑瘫的较大数据集将进一步提高模型的准确性，从而提供有价值的工具来辨别微妙和特质的疼痛表达式。获得的见解可能扩展到其他复杂的神经系统条件。

### SignVTCL: Multi-Modal Continuous Sign Language Recognition Enhanced by Visual-Textual Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2401.11847)] [[cool](https://papers.cool/arxiv/2401.11847)] [[pdf](https://arxiv.org/pdf/2401.11847)]
> **Authors**: Hao Chen,Jiaze Wang,Ziyu Guo,Jinpeng Li,Donghao Zhou,Bian Wu,Chenyong Guan,Guangyong Chen,Pheng-Ann Heng
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: signvtcl：通过视觉文本对比学习增强的多模式连续手语识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 手语识别（SLR）在促进听力受损社区的交流中起着至关重要的作用。 SLR是一项弱监督的任务，在整个视频中都用光泽注释，这使得识别视频段中相应的光泽是一项挑战。最近的研究表明，SLR中的主要瓶颈是由于大规模数据集的可用性有限而导致的训练不足。为了应对这一挑战，我们提出了SignVTCL，这是一种通过视觉文本对比学习增强的多模式连续手语识别框架，它利用了多模式数据的全部潜力和语言模型的概括能力。 SignVTCL同时集成了多模式数据（视频，关键点和光流），以训练统一的视觉主链，从而产生更多可靠的视觉表示。此外，SignVTCL包含一种视觉文本对齐方法，其中包含光泽级别和句子级别对齐，以确保在单个光泽和句子级别上的视觉特征和光泽之间的精确对应关系。每天在三个数据集（Phoenix-2014，Phoenix-2014T和CSL）上进行的实验结果表明，与以前的方法相比，SIGNVTCL可实现最先进的结果。

### Collaborative Position Reasoning Network for Referring Image Segmentation 
[[arxiv](https://arxiv.org/abs/2401.11775)] [[cool](https://papers.cool/arxiv/2401.11775)] [[pdf](https://arxiv.org/pdf/2401.11775)]
> **Authors**: Jianjian Cao,Beiya Dai,Yulin Li,Xiameng Qin,Jingdong Wang
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: 用于引用图像细分的协作位置推理网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 给定图像和自然语言表达式作为输入，引用图像分割的目的是将表达式所述实体的前景面具分割。现有方法主要集中于视觉和语言之间的互动学习，以增强全球上下文推理的多模式表示。但是，直接在像素级空间中预测会导致定位崩溃和分割结果差。它的主要挑战在于如何显式建模实体定位，尤其是对于非征服实体。在本文中，我们通过拟议的新颖的行相互作用（ROCO）执行协作位置推理网络（CPRN）来解决这个问题，并指导整体互动（HOLI）模块。具体而言，Roco分别将视觉特征分别汇总到行和列特征分别对应两个方向轴。它提供了一种细粒度的匹配行为，可以感知语言特征和两个脱钩的视觉特征之间的关联，以在层次结构上执行位置推理。 HOLI通过交叉模式的注意机制整合了两种模式的特征，这在Roco的定位信息指南下抑制了无关紧要的冗余性。因此，随着ROCO和HOLI模块的结合，CPRN捕获了位置推理的视觉细节，以便模型可以实现更准确的分割。据我们所知，这是明确侧重于位置推理建模的第一部作品。我们还验证了三个评估数据集中的建议方法。它始终优于现有的最新方法。

### Multi-level Cross-modal Alignment for Image Clustering 
[[arxiv](https://arxiv.org/abs/2401.11740)] [[cool](https://papers.cool/arxiv/2401.11740)] [[pdf](https://arxiv.org/pdf/2401.11740)]
> **Authors**: Liping Qiu,Qin Zhang,Xiaojun Chen,Shaotian Cai
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: 用于图像群集的多级交叉模式对齐
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 最近，已经采用了跨模式预处理模型来生产有意义的伪标记，以监督图像聚类模型的训练。但是，跨模式预训练模型中的许多错误比对可能会产生质量不佳的伪标记和降解聚类性能。 To solve the aforementioned issue, we propose a novel \textbf{Multi-level Cross-modal Alignment} method to improve the alignments in a cross-modal pretraining model for downstream tasks, by building a smaller but better semantic space and aligning the images and texts in three levels, i.e., instance-level, prototype-level, and semantic-level.理论结果表明，我们提出的方法会收敛，并提出有效的手段来降低我们方法的预期聚类风险。五个基准数据集的实验结果清楚地显示了我们新方法的优势。

### Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2401.11708)] [[cool](https://papers.cool/arxiv/2401.11708)] [[pdf](https://arxiv.org/pdf/2401.11708)]
> **Authors**: Ling Yang,Zhaochen Yu,Chenlin Meng,Minkai Xu,Stefano Ermon,Bin Cui
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: ICML 2024. Project: https://github.com/YangLing0818/RPG-DiffusionMaster
- **标题**: 掌握文本到图像扩散：使用多模式LLMS重新审议，计划和生成
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 扩散模型在文本到图像生成和编辑中表现出非凡的性能。但是，现有方法在处理复杂文本提示时通常会面临挑战，涉及多个具有多个属性和关系的对象。在本文中，我们提出了一个全新的无培训文本到图像生成/编辑框架，即重新捕获，计划和生成（RPG），以利用多模式LLMS的强大想象链推理能力增强文本到图像扩散模型的组成。我们的方法采用MLLM作为全球规划师，将复杂图像生成复杂图像的过程分解为子区域内的多个简单生成任务。我们提出了互补的区域扩散，以使区域构成生成。此外，我们以闭环的方式将文本引导的图像生成和编辑整合在拟议的RPG中，从而增强了概括能力。广泛的实验表明，我们的RPG优于最先进的文本对图像扩散模型，包括DALL-E 3和SDXL，尤其是在多类别对象组成和文本图像上的语义对齐中。值得注意的是，我们的RPG框架与各种MLLM体系结构（例如Minigpt-4）和扩散骨架（例如ControlNet）表现出广泛的兼容性。我们的代码可在以下网址找到：https：//github.com/yangling0818/rpg-diffusionmaster

### MLLMReID: Multimodal Large Language Model-based Person Re-identification 
[[arxiv](https://arxiv.org/abs/2401.13201)] [[cool](https://papers.cool/arxiv/2401.13201)] [[pdf](https://arxiv.org/pdf/2401.13201)]
> **Authors**: Shan Yang,Yongfei Zhang
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: No comments
- **标题**: MLLMREID：多模式的基于大语模型的人重新识别
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 多模式大语模型（MLLM）在许多任务中都取得了令人满意的结果。但是，迄今为止，尚未探索他们在里德（Reid）任务（REID）任务的表现。本文将调查如何适应REID的任务。一个直观的想法是用REID Image-Text数据集微调MLLM，然后将其视觉编码器用作REID的骨架。但是，仍然存在两个明显的问题：（1）设计REID的说明，MLLM可能会过度拟合特定的说明，并且设计各种说明将导致更高的成本。 （2）微调MLLM的视觉编码器时，它不会与REID任务同步训练。结果，视觉编码器微调的有效性不能直接反映在REID任务的性能中。为了解决这些问题，本文提出了MLLMREID：基于多模式的大型模型REID。首先，我们提出了共同的指导，这是一种简单的方法，它利用LLMS继续写作的本质能力，避免复杂而多样化的指导设计。其次，我们提出了一个基于多任务的基于学习的同步模块，以确保MLLM的视觉编码器与REID任务同步训练。实验结果证明了我们方法的优越性。

### Free Form Medical Visual Question Answering in Radiology 
[[arxiv](https://arxiv.org/abs/2401.13081)] [[cool](https://papers.cool/arxiv/2401.13081)] [[pdf](https://arxiv.org/pdf/2401.13081)]
> **Authors**: Abhishek Narayanan,Rushabh Musthyala,Rahul Sankar,Anirudh Prasad Nistala,Pranav Singh,Jacopo Cirrone
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: 6 pages and 4 figures
- **标题**: 放射学中的免费形式医学视觉问题回答
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 医学领域中的视觉问题回答（VQA）提出了一个独特的，跨学科的挑战，结合了计算机视觉，自然语言处理和知识表示等领域。尽管它的重要性，但在医学VQA方面的研究很少，自2018年以来才获得势头。解决这一差距，我们的研究深入研究了放射学图像的有效代表以及多模式表示的联合学习，超过了现有方法。我们创新地增强了SLAKE数据集，使我们的模型能够响应更多样化的问题，而不仅限于放射学或病理图像的直接内容。我们的模型在具有较不复杂的体系结构的情况下达到了79.55％的前1个精度，表现出与当前最新模型相当的性能。这项研究不仅可以进步医疗VQA，而且还为诊断环境中的实际应用开辟了途径。

### On the Efficacy of Text-Based Input Modalities for Action Anticipation 
[[arxiv](https://arxiv.org/abs/2401.12972)] [[cool](https://papers.cool/arxiv/2401.12972)] [[pdf](https://arxiv.org/pdf/2401.12972)]
> **Authors**: Apoorva Beedu,Harish Haresamudram,Karan Samel,Irfan Essa
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: No comments
- **标题**: 关于基于文本的输入方式的功效，以进行行动预期
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,图像和视频处理
- **摘要**: 由于潜在的未来行动的多样性和规模，预期未来的行动是一项高度挑战的任务。但是，来自不同方式的信息有助于缩小合理的行动选择。每种方式都可以为模型提供多样化，通常是互补的环境。虽然以前的多模式方法利用了视频和音频等方式的信息，但我们主要探讨了动作和对象的文本描述如何通过提供其他上下文提示，例如有关环境及其内容来导致更准确的动作预期。我们提出了一个多模式对比度预期变压器（M-CAT），这是一种视频变压器体系结构，可以从多模式特征和动作和对象的文本描述中共同学习。我们在两个阶段训练模型，该模型首先学会了将视频剪辑与未来动作的描述对齐，并随后进行了微调以预测未来的动作。与现有方法相比，M-CAT具有从两种类型的文本输入中学习其他上下文的优点：在预训练期间对未来动作的丰富描述，以及在模态特征融合过程中检测到的对象和动作的文本描述。通过广泛的实验评估，我们证明了我们的模型在epickitchens数据集上的先前方法优于先前的方法，并证明使用动作和对象的简单文本描述有助于更有效的动作预期。此外，我们研究了通过文本获得的对象和动作信息的影响，并进行大量消融。

### FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units 
[[arxiv](https://arxiv.org/abs/2401.12862)] [[cool](https://papers.cool/arxiv/2401.12862)] [[pdf](https://arxiv.org/pdf/2401.12862)]
> **Authors**: Shaoheng Fang,Rui Ye,Wenhao Wang,Zuhong Liu,Yuxiao Wang,Yafei Wang,Siheng Chen,Yanfeng Wang
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: No comments
- **标题**: FEDRSU：在路边单位上的场景流量估算的联合学习
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 路边单元（RSU）可以通过车辆到设施（V2X）通信显着提高自动驾驶汽车的安全性和鲁棒性。目前，单个RSU的使用主要集中在实时推理和V2X协作上，同时忽略了RSU传感器收集的高质量数据的潜在价值。整合来自众多RSU的大量数据可以为模型培训提供丰富的数据来源。但是，没有地面真理注释和传输大量数据的困难是完全利用这一隐藏价值的两个不可避免的障碍。在本文中，我们介绍了Fedrsu，这是一个创新的联合学习框架，用于自我监督场景流程估计。在Fedrsu，我们提出了一个经常性的自我划分训练范式，在每个时间戳中，每个时间戳的场景都可以通过其随后的将来的多模式观察来监督。 Fedrsu的另一个关键组成部分是联合学习，其中多个设备在保留本地和私人培训数据的同时协作训练ML模型。借助经常出现的自我监督学习范式的力量，FL能够利用RSU的无数数据不足的数据。为了验证FedRsu框架，我们构建了一个大规模的多模式数据集RSU-SF。数据集由17个RSU客户端组成，涵盖了各种方案，模式和传感器设置。根据RSU-SF，我们表明Fedrsu可以大大提高其模型性能，并在不同的FL场景下提供全面的基准。据我们所知，我们为FL社区提供了第一个现实世界中的LIDAR相机多模式数据集和基准。

### MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under Uncertainty 
[[arxiv](https://arxiv.org/abs/2401.12761)] [[cool](https://papers.cool/arxiv/2401.12761)] [[pdf](https://arxiv.org/pdf/2401.12761)]
> **Authors**: Tim Brödermann,David Bruggemann,Christos Sakaridis,Kevin Ta,Odysseas Liagouris,Jason Corkill,Luc Van Gool
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: Dataset available at http://muses.vision.ee.ethz.ch
- **标题**: 缪斯女神：用于驾驶不确定性的多传感器语义感知数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 在自动驾驶汽车中实现5级驱动自动化，必须具有强大的语义视觉感知系统，能够在不同条件下从不同传感器中解析数据。但是，现有的语义感知数据集通常缺乏自动驾驶汽车通常使用的重要非相机方式，或者它们不利用此类方式来帮助和改善具有挑战性的条件下的语义注释。为了解决这个问题，我们介绍了Muses，这是多传感器语义感知数据集，用于在不确定性下在不良条件下驱动。缪斯女神包括在不同天气和照明下捕获的2500张图像的2D全景注释的同步多模式记录。数据集集成了框架摄像头，雷达，雷达，事件摄像头和IMU/GNSS传感器。我们新的两阶段全面注释协议在地面真理中捕获了类级别和实例级别的不确定性，并实现了我们引入的不确定性意识性综合分段的新任务，以及标准的语义和全倾段。在各种视觉条件下评估模型，缪斯既证明对训练有效，又是挑战性的，它为多模式和不确定性感知的密集语义感知开辟了新的途径。我们的数据集和基准标准可在https://muses.vision.ee.eethz.ch上公开获取。

### ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation 
[[arxiv](https://arxiv.org/abs/2401.12665)] [[cool](https://papers.cool/arxiv/2401.12665)] [[pdf](https://arxiv.org/pdf/2401.12665)]
> **Authors**: Shengze Li,Jianjian Cao,Peng Ye,Yuhan Ding,Chongjun Tu,Tao Chen
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: 17 pages,17 figures
- **标题**: Clipsam：零摄像异常分段的剪辑和SAM协作
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近，诸如夹子和SAM之类的基础模型表现出有希望的零拍异常分割（ZSA）任务的表现。但是，基于夹的或基于SAM的ZSA方法仍然遭受不可忽略的关键缺点：1）剪辑主要集中在不同输入之间的全局特征对齐，导致对局部异常部分的不精确分段； 2）SAM倾向于在没有适当及时限制的情况下生成许多冗余面具，从而产生复杂的后处理要求。在这项工作中，我们对ZSAS的剪辑和SAM协作框架提出了创新的建议。 ClipsAM背后的见解是采用剪贴画的语义理解能力，以实现异常定位和粗糙分割，这进一步用作SAM的及时约束，以完善异常分割结果。从详细的角度来看，我们引入了至关重要的统一多尺度跨模式相互作用（UMCI）模块，用于与多个夹子的视觉特征相互作用，以推理异常位置。然后，我们设计了一种新型的多层面掩码细化（MMR）模块，该模块利用位置信息作为多层提示，以使SAM获取层次层面并将其合并。广泛的实验验证了我们方法的有效性，从而在MVTEC-AD和VISA数据集上实现了最佳分割性能。

### UniHDA: A Unified and Versatile Framework for Multi-Modal Hybrid Domain Adaptation 
[[arxiv](https://arxiv.org/abs/2401.12596)] [[cool](https://papers.cool/arxiv/2401.12596)] [[pdf](https://arxiv.org/pdf/2401.12596)]
> **Authors**: Hengjia Li,Yang Liu,Yuqi Lin,Zhanwei Zhang,Yibo Zhao,weihang Pan,Tu Zheng,Zheng Yang,Yuchun Jiang,Boxi Wu,Deng Cai
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: No comments
- **标题**: Unihda：多模式混合域适应的统一且通用的框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，生成域的适应取得了显着的进步，使我们能够将预训练的发电机调整到新的目标域。但是，现有方法只需将发电机调整到一个单个目标域，仅限于单个模态，无论是文本驱动还是图像驱动。此外，他们不能与源域保持良好的一致性，这阻碍了多样性的继承。在本文中，我们建议使用来自多个域中的多模式引用的生成混合域适应的unihda，a \ textbf {unified}和\ textbf {versatile}框架。我们使用夹编码器将多模式引用投影到统一的嵌入空间中，然后线性地插值从多个目标域的方向向量，以实现混合域的适应性。为了确保与源域的\ textBf {一致性}，我们提出了一种新型的跨域空间结构（CSS）损失，该损失在源和目标发生器之间保持详细的空间结构信息。实验表明，改编的生成器可以将逼真的图像与各种属性组成合成逼真的图像。此外，我们的框架是生成器，不合时宜的，并且对多个发电机，例如stylegan，eg3d和扩散模型。

### NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis 
[[arxiv](https://arxiv.org/abs/2401.12568)] [[cool](https://papers.cool/arxiv/2401.12568)] [[pdf](https://arxiv.org/pdf/2401.12568)]
> **Authors**: Chongke Bi,Xiaoxing Liu,Zhilei Liu
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: Accepted by ICASSP 2024
- **标题**: nerf-ad：神经辐射场，具有基于注意的说话面部综合的基于注意力的分离
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 由音频驱动的说话面部合成是多维信号处理和多媒体领域的当前研究热点之一。神经辐射场（NERF）最近已被带到该研究领域，以增强生成的面孔的现实主义和3D效应。但是，大多数现有的基于NERF的方法要么负担复杂的学习任务负担，同时缺少有监督的多模式特征融合的方法，要么无法精确地将音频映射到与语音运动有关的面部区域。这些原因最终导致现有方法产生不准确的唇形。本文将一部分NERF学习任务移动，并通过NERF提出了带有基于注意力的分解（NERF-AD）的会说话的面部合成方法。特别是，引入了基于注意力的解开模块，以使用与语音相关的面部动作单元（AU）信息将面部置于音频和身份面中。为了准确地调节音频如何影响说话的面孔，我们只融合了音频与音频功能。此外，AU信息还用于监督这两种方式的融合。广泛的定性和定量实验表明，我们的NERF-AD在生成现实的说话面部视频（包括图像质量和唇部同步）方面优于最先进的方法。要查看视频结果，请参阅https://xiaoxingliu02.github.io/nerf-ad。

### MambaMorph: a Mamba-based Framework for Medical MR-CT Deformable Registration 
[[arxiv](https://arxiv.org/abs/2401.13934)] [[cool](https://papers.cool/arxiv/2401.13934)] [[pdf](https://arxiv.org/pdf/2401.13934)]
> **Authors**: Tao Guo,Yinuo Wang,Shihao Shu,Diansheng Chen,Zhouping Tang,Cai Meng,Xiangzhi Bai
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: No comments
- **标题**: MAMBAMORPH：基于MAMBA的医疗MR-CT可变形注册框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 在不同的方式上捕获体素的空间对应关系对于医学图像分析至关重要。但是，目前的注册方法在注册准确性和临床适用性方面不够实用。在本文中，我们介绍了Mambamorph，这是一种新型的多模式可变形的注册框架。具体而言，Mambamorph利用基于MAMBA的注册模块和细粒度但简单的功能提取器，分别用于有效的长距离通信建模和高维功能学习。此外，我们开发了一个通知的大脑MR-CT注册数据集SR-REG，以解决多模式注册中数据的稀缺性。为了验证Mambamorph的多模式注册功能，我们对SR-REG数据集和公共T1-T2数据集进行了定量实验。两个数据集上的实验结果表明，Mambamorph在注册准确性方面显着优于当前基于学习的注册方法。进一步的研究强调了基于MAMBA的注册模块和轻巧功能提取器的效率，这些效率可达到显着的注册质量，同时保持合理的计算成本和速度。我们认为，Mambamorph具有在医学图像注册中实用应用的巨大潜力。 Mambamorph的代码可在以下网址提供：https：//github.com/guo-stone/mambamorph。

### Knowledge Guided Entity-aware Video Captioning and A Basketball Benchmark 
[[arxiv](https://arxiv.org/abs/2401.13888)] [[cool](https://papers.cool/arxiv/2401.13888)] [[pdf](https://arxiv.org/pdf/2401.13888)]
> **Authors**: Zeyu Xi,Ge Shi,Xuefen Li,Junchi Yan,Zun Li,Lifang Wu,Zilin Liu,Liang Wang
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: No comments
- **标题**: 知识指导的实体感知视频字幕和篮球基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管最近出现了视频字幕模型，但如何使用特定实体名称和细粒度的动作生成文本描述远未解决，但是它具有很棒的应用，例如篮球现场直播。在本文中，提出了一个新的多模式知识图支持的篮球基准，用于视频字幕。具体来说，我们构建了一个多模式篮球游戏知识图（KG_NBA_2022），以提供视频之外的其他知识。然后，一个基于KG_NBA_2022的多模式篮球游戏视频字幕（VC_NBA_2022）数据集，该数据集包含9种类型的细粒射击事件和286个玩家的知识（即图像和名称）。我们根据篮球实时文本广播中的coder-decoder表单中的候选播放器列表开发了知识指导的实体感知视频字幕网络（KEANET）。视频中的时间上下文信息是通过引入双向GRU（BI-GRU）模块来编码的。实体感知的模块旨在建模玩家之间的关系并突出关键参与者。对多个运动基准测试的广泛实验表明，基尼特有效利用外观知识和优于高级视频字幕模型。拟议的数据集和相应的代码将很快公开可用

### Toward Robust Multimodal Learning using Multimodal Foundational Models 
[[arxiv](https://arxiv.org/abs/2401.13697)] [[cool](https://papers.cool/arxiv/2401.13697)] [[pdf](https://arxiv.org/pdf/2401.13697)]
> **Authors**: Xianbing Zhao,Soujanya Poria,Xuejiao Li,Yixin Chen,Buzhou Tang
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-25
> **comment**: Under Review
- **标题**: 使用多模式的基础模型进行强大的多模式学习
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 现有的多模式情感分析任务在很大程度上取决于训练和测试集是完整的多模式数据的假设，而这种假设可能难以持有：在实际情况下，多模式数据通常是不完整的。因此，在具有随机缺失模态的方案中，强大的多模式模型是高度首选的。最近，基于夹的多模式基础模型通过学习图像和文本对的一致性跨模式语义，在众多多模式任务上表现出了令人印象深刻的性能，但是多模式的基础模型也无法直接解决涉及模态缺失的方案。为了减轻此问题，我们提出了一个简单有效的框架，即TRML，使用多模式的基础模型来强大的多模式学习。 TRML采用生成的虚拟模式来替换缺失的模态，并在生成和缺失模态之间对齐语义空间。具体而言，我们设计了缺少的模态推理模块，以生成虚拟模量并替换缺失的模态。我们还设计了一个语义匹配的学习模块，以使生成和缺少模式的语义空间保持一致。在完全模态的提示下，我们的模型通过利用对齐的跨模式语义空间来捕获缺失模态的语义。实验证明了我们方法在三个多模式情感分析基准数据集CMU-MOSI，CMU-MOSEI和MELD上的优越性。

### How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability 
[[arxiv](https://arxiv.org/abs/2401.13641)] [[cool](https://papers.cool/arxiv/2401.13641)] [[pdf](https://arxiv.org/pdf/2401.13641)]
> **Authors**: Ivan DeAndres-Tame,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez,Javier Ortega-Garcia
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: ef:IEEE Access, February 2024
- **标题**: 面对生物识别的Chatgpt有多好？首次了解识别，柔软的生物识别和解释性
- **领域**: 计算机视觉和模式识别,人工智能,计算机与社会,机器学习
- **摘要**: 大型语言模型（LLM），例如由OpenAI开发的GPT，已经显示出惊人的结果，引入了我们社会的快速变化。 Chatgpt的发布已经加强了这一点，该发布使任何人都可以与LLMs简单的对话方式进行交互，而无需在所需的现场经验。结果，Chatgpt已迅速应用于许多不同的任务，例如代码和歌曲作者，教育，虚拟助手等，对未经培训的任务显示出令人印象深刻的结果（零射击学习）。本研究旨在根据最近的GPT-4多模式LLM探索ChatGPT的能力，以实现面部生物识别的任务。特别是，我们分析了CHATGPT执行诸如面部验证，软生物测定法估计以及结果解释性之类的任务的能力。在人类场景中，CHATGPT对于进一步提高自动决策的解释性和透明度可能非常有价值。实验是为了评估Chatgpt的性能和鲁棒性，使用流行的公共基准，并将结果与​​该领域的最新方法进行比较。这项研究中获得的结果表明，LLMS（例如Chatgpt）对面部生物识别技术的潜力，尤其是为了增强解释性。出于可重复性原因，我们在Github中发布了所有代码。

### Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild 
[[arxiv](https://arxiv.org/abs/2401.13627)] [[cool](https://papers.cool/arxiv/2401.13627)] [[pdf](https://arxiv.org/pdf/2401.13627)]
> **Authors**: Fanghua Yu,Jinjin Gu,Zheyuan Li,Jinfan Hu,Xiangtao Kong,Xintao Wang,Jingwen He,Yu Qiao,Chao Dong
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: This paper has been accepted by CVPR 2024
- **标题**: 扩大到卓越的扩展：练习野外照片真实图像修复的模型缩放
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们介绍了Supir（扩展图像恢复），这是一种开创性的图像恢复方法，可利用生成性先验和模型扩展的力量。 Supir利用多模式技术和先进的生成性先验，标志着智能和现实的图像恢复的重大进步。作为Supir中的关键催化剂，模型缩放大大提高了其功能，并展示了图像恢复的新潜力。我们收集一个数据集，其中包括2000万个高分辨率的高质量图像，用于模型培训，每个图像都具有描述性文本注释。 Supir提供了以文本提示为指导的图像，扩大其应用范围和潜力的能力。此外，我们引入了负质量提示，以进一步提高感知质量。我们还开发了一种恢复引导的抽样方法，以抑制基于生成的恢复中遇到的保真度问题。实验证明了Supir的出色恢复效果及其通过文本提示来操纵恢复的新颖能力。

### Serial fusion of multi-modal biometric systems 
[[arxiv](https://arxiv.org/abs/2401.13418)] [[cool](https://papers.cool/arxiv/2401.13418)] [[pdf](https://arxiv.org/pdf/2401.13418)]
> **Authors**: Gian Luca Marcialis,Paolo Mastinu,Fabio Roli
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: ef:IEEE International Workshop on Biometric Measurements and Systems for Security and Medical Applications (BioMS2010), September, 9, 2010, Taranto (Italy), ISBN: 978-1-4244-6302-2
- **标题**: 多模式生物识别系统的连续融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 到目前为止，尚未对多个生物识别匹配器的连续或顺序融合进行融合。但是，这种方法在广泛采用的平行方法方面具有一些优势。在本文中，我们根据作者先前的工作提出了一个新的理论框架，用于评估此类系统的性能。从理论上评估了性能方面的好处，以及模型参数计算中的估计错误。根据NIST生物识别评分集1进行的初步实验，从其优缺点的角度分析了模型。

### UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion 
[[arxiv](https://arxiv.org/abs/2401.13388)] [[cool](https://papers.cool/arxiv/2401.13388)] [[pdf](https://arxiv.org/pdf/2401.13388)]
> **Authors**: Wei Li,Xue Xu,Jiachen Liu,Xinyan Xiao
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: Accepted by ACL 2024, Main Conference, Long Paper
- **标题**: UNIMO-G：通过多模式条件扩散产生统一的图像
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的文本到图像扩散模型主要从文本提示中生成图像。但是，文本描述的固有简洁性在忠实地通过复杂的细节（例如特定的实体或场景）忠实地综合图像时提出了挑战。本文介绍了Unimo-G，这是一种简单的多模式条件扩散框架，该框架在多模式的提示下运行，并带有交织的文本和视觉输入，这表明了文本驱动和主题驱动的图像生成的统一能力。 UNIMO-G包括两个核心组件：用于编码多模式提示的多模式大语言模型（MLLM），以及一个有条件的DeNo deofusing扩散网络，用于基于编码的多模态输入生成图像。我们利用两阶段的训练策略有效地训练框架：首先在大规模的文本图像对上进行预训练，以开发有条件的图像生成功能，然后使用多模式提示进行指导调整以实现统一的图像产生能力。涉及语言接地和图像分割的精心设计的数据处理管道用于构建多模式提示。 UniMO-G在文本到图像生成和零摄像主体驱动的合成中都表现出色，并且在复杂的多模式提示中生成高保真图像，涉及多个图像实体。

### InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions 
[[arxiv](https://arxiv.org/abs/2401.13313)] [[cool](https://papers.cool/arxiv/2401.13313)] [[pdf](https://arxiv.org/pdf/2401.13313)]
> **Authors**: Ryota Tanaka,Taichi Iki,Kyosuke Nishida,Kuniko Saito,Jun Suzuki
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: Accepted by AAAI2024; project page: https://github.com/nttmdlab-nlp/InstructDoc
- **标题**: 指示：用于使用说明的视觉文档理解的零击概括的数据集
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 我们研究了通过人工写的说明在现实世界文档上完成各种视觉文档理解（VDU）任务的问题。为此，我们提出了TenfersDoc，这是第一个大规模收藏的30个公开可用的VDU数据集，每个数据集都具有统一格式的不同指令，其中涵盖了12个任务，其中包括开放文档类型/格式。此外，为了增强VDU任务上的概括性能，我们设计了一个新的基于指令的文档阅读和理解模型，指示DR，该模型通过可训练的桥接模块连接文档图像，图像编码器和大型语言模型（LLMS）。实验表明，通过给定的说明，指示可以有效地适应新的VDU数据集，任务和域，并且在没有特定培训的情况下优于现有的多模式LLMS和CHATGPT。

### ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2401.13311)] [[cool](https://papers.cool/arxiv/2401.13311)] [[pdf](https://arxiv.org/pdf/2401.13311)]
> **Authors**: Rohan Wadhawan,Hritik Bansal,Kai-Wei Chang,Nanyun Peng
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: ef:PMLR 235:49733-49787, 2024
- **标题**: 上下文：在大型多模型中评估上下文敏感文本富含视觉推理
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 许多现实世界的任务都需要代理来共同推理文本和视觉对象（例如，在公共空间中导航），我们将其称为上下文敏感的文本含量丰富的视觉推理。具体而言，这些任务需要了解文本与图像中的视觉元素相互作用的上下文。但是，缺乏现有的数据集来基准在上下文敏感的文本富含视觉推理上最先进的多模型模型的功能。在本文中，我们介绍了上下文，这是一个新颖的数据集，其中包含人工制作的说明，这些说明需要上下文敏感的文本图像。我们进行实验，以评估14个基础模型（GPT-4V，Gemini-Pro-Vision，Llava-Next）的性能并建立人类绩效基线。此外，我们对模型响应进行人体评估，并观察到GPT-4V（当前表现最佳的大型多模型）和人类绩效之间的显着性能差距为30.8％。我们的细粒度分析表明，GPT-4V遇到了解释与时间相关的数据和信息图表的困难。但是，它表明了理解抽象视觉上下文（例如模因和引号）的熟练程度。最后，我们的定性分析发现了导致性能不佳的各种因素，包括缺乏精确的视觉感知和幻觉。我们的数据集，代码和排行榜可以在项目页面上找到https://con-textual.github.io/

### ChatterBox: Multi-round Multimodal Referring and Grounding 
[[arxiv](https://arxiv.org/abs/2401.13307)] [[cool](https://papers.cool/arxiv/2401.13307)] [[pdf](https://arxiv.org/pdf/2401.13307)]
> **Authors**: Yunjie Tian,Tianren Ma,Lingxi Xie,Jihao Qiu,Xi Tang,Yuan Zhang,Jianbin Jiao,Qi Tian,Qixiang Ye
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: 17 pages, 6 tables, 9 figurs. Code, data, and model are available at: https://github.com/sunsmarterjie/ChatterBox
- **标题**: Chatterbox：多轮多模式引用和接地
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项研究中，我们为一个名为Multomodal多模式的参考和接地（MRG）的新任务建立了一个基线，为实例级别的多模式对话打开了一个有希望的方向。为此，我们提出了一个新的基准和有效的视觉语言模型。名为CB-300K的新基准测试跨越了挑战，包括多轮对话，多个实例之间的复杂空间关系以及一致的推理，这超出了现有基准测试中所示。所提出的名为Chatterbox的模型利用两分支架构来协作处理视觉和语言任务。通过引导实例区域，语言分支可以获得感知参考信息的能力。同时，Chatterbox将视觉分支中的查询嵌入到令牌接收器中以进行视觉接地。设计了两阶段的优化策略，同时使用CB-300K和辅助外部数据来提高模型的稳定性和实例级别的理解能力。实验表明，Chatterbox在定量和定性上都优于MRG中的现有模型，这为具有复杂和精确相互作用的多模式对话方案铺平了新的道路。代码，数据和模型可在以下网址提供：https：//github.com/sunsmarterjie/chatterbox。

### Dynamic Traceback Learning for Medical Report Generation 
[[arxiv](https://arxiv.org/abs/2401.13267)] [[cool](https://papers.cool/arxiv/2401.13267)] [[pdf](https://arxiv.org/pdf/2401.13267)]
> **Authors**: Shuchang Ye,Mingyuan Meng,Mingjian Li,Dagan Feng,Usman Naseem,Jinman Kim
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: No comments
- **标题**: 医疗报告的动态追溯学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 自动化报告的生成有可能大大减少与耗时的医疗报告过程相关的工作量。最近的生成表示学习方法已显示出在整合医学报告生成的视觉和语言方式方面的希望。但是，当受过训练的端到端训练并直接应用于医学图像到文本的一代时，它们面临两个重大挑战：i）难以准确捕获微妙但至关重要的病理细节，以及ii）依靠推断过程中视觉和文本输入的依赖，从而导致性能在仅出现图像时零射击中的零射击降解。为了应对这些挑战，本研究提出了一种新型的多模式动态追溯学习框架（DTRACE）。具体而言，我们引入了一种追溯机制，以监督生成内容的语义有效性和动态学习策略，以适应各种比例的图像和文本输入，从而使文本生成不依赖于推断期间的两种模式的输入。通过监督模型从互补的对应物中恢复蒙面的语义信息，可以增强跨模式知识的学习。在两个基准数据集IU-XRAY和MIMIC-CXR上进行的广泛实验表明，所提出的DTRACE框架优于医疗报告生成的最先进方法。

### Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities 
[[arxiv](https://arxiv.org/abs/2401.14405)] [[cool](https://papers.cool/arxiv/2401.14405)] [[pdf](https://arxiv.org/pdf/2401.14405)]
> **Authors**: Yiyuan Zhang,Xiaohan Ding,Kaixiong Gong,Yixiao Ge,Ying Shan,Xiangyu Yue
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: CVPR 2024. Code and models are available at https://github.com/AILab-CVC/M2PT
- **标题**: 多模式途径：通过其他模态通过无关数据改善变压器
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 我们建议通过来自其他模式的数据来改善特定模式的变压器，例如，使用音频或点云数据集改进成像网模型。我们想强调，目标方式的数据样本与其他模式无关，这将我们的方法与使用配对（例如剪辑）或不同模态的交错数据区分开了我们的方法。我们提出了一种名为多模式途径的方法论 - 给定了目标方式和为其设计的变压器，我们使用辅助变压器训练了使用另一种模式和构造途径的数据训练的辅助变压器，以连接两个模型的组件，以便两个模型都可以处理目标模态的数据。这样，我们利用了从两种模式获得的变压器的通用序列对序列建模能力。作为具体的实现，我们像往常一样使用特定于模态的令牌和特定于任务的头部，但是通过所提出的跨模式重新参数化的方法利用辅助模型的变压器块，从而利用了辅助权重，而无需任何推理成本。在图像，点云，视频和音频识别任务上，我们观察到来自其他模式的无关数据的显着和一致的性能改进。代码和型号可在https://github.com/ailab-cvc/m2pt上找到。

### JUMP: A joint multimodal registration pipeline for neuroimaging with minimal preprocessing 
[[arxiv](https://arxiv.org/abs/2401.14250)] [[cool](https://papers.cool/arxiv/2401.14250)] [[pdf](https://arxiv.org/pdf/2401.14250)]
> **Authors**: Adria Casamitjana,Juan Eugenio Iglesias,Raul Tudela,Aida Ninerola-Baizan,Roser Sala-Llonch
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: No comments
- **标题**: 跳跃：最小预处理的神经影像学的联合多模式注册管道
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一条管道，用于对神经影像模式的无偏多模式登记，并且预处理最少。尽管典型的多模式研究需要使用多种独立的处理管道，但具有不同的选项和超参数，但我们提出了一个单一的结构化框架来共同处理不同的图像方式。基于最先进的学习技术的使用可以快速推断，这使得提出的方法适用于每个会话数量不同的大规模和/或多方面数据集。该管道目前与结构MRI，静止状态fMRI和淀粉样蛋白宠物图像一起使用。我们在病例对照研究中使用了衍生生物标志物的预测能力，并研究了不同图像方式之间的跨模式关系。该代码可以在https：//github.com/acasamitjana/jump中找到。

### LanDA: Language-Guided Multi-Source Domain Adaptation 
[[arxiv](https://arxiv.org/abs/2401.14148)] [[cool](https://papers.cool/arxiv/2401.14148)] [[pdf](https://arxiv.org/pdf/2401.14148)]
> **Authors**: Zhenbin Wang,Lei Zhang,Lituan Wang,Minjuan Zhu
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: 20 pages, 8 figures
- **标题**: 兰达：语言引导的多源域适应
- **领域**: 计算机视觉和模式识别
- **摘要**: 当将知识从多个标记的源域转移到未标记的目标域时，多源域适应性（MSDA）旨在减轻数据分布的变化。但是，现有的MSDA技术假定目标域图像可用，但忽略了图像丰富的语义信息。因此，一个空旷的问题是，在没有目标域图像的情况下，是否只能通过文本提示来指导MSDA。通过采用具有联合图像和语言嵌入空间的多模式模型，我们提出了一种基于最佳转移理论的新型语言指导的MSDA方法，称为Landa，该方法促进了多个源域向新目标域的传递，仅需要对目标域的文本描述，而无需单个目标域，即使单个目标域，同时保留任务范围，同时又需要一个单个目标域，同时又需要。我们使用一套相关的基准进行了跨不同传输方案的广泛实验，这表明Landa在目标和源域中都超过了标准的微调和集合方法。

### CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion 
[[arxiv](https://arxiv.org/abs/2401.14066)] [[cool](https://papers.cool/arxiv/2401.14066)] [[pdf](https://arxiv.org/pdf/2401.14066)]
> **Authors**: Nisha Huang,Weiming Dong,Yuxin Zhang,Fan Tang,Ronghui Li,Chongyang Ma,Xiu Li,Changsheng Xu
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: No comments
- **标题**: Creativessynth：基于多模式扩散的视觉艺术的创造性混合和综合
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大规模的文本到图像生成模型取得了令人印象深刻的进步，展示了它们合成各种高质量图像的能力。但是，将这些模型改编成艺术图像编辑提出了两个重大挑战。首先，用户难以制作文本提示，这些提示会精心详细介绍输入图像的视觉元素。其次，普遍的模型在影响特定区域的修改时经常破坏整体艺术风格，从而使凝聚力和美学统一的艺术品的达成复杂化。为了克服这些障碍，我们构建了创新的统一框架创意合成，该框架基于扩散模型，能够在艺术图像生成领域协调多模式输入和多任务。通过将多模式特征与自定义的注意机制集成在一起，Creativesynth通过反转和实时样式转移将现实世界的语义内容进口到艺术领域。这允许对图像样式和内容进行精确操纵，同时保持原始模型参数的完整性。严格的定性和定量评估强调了创造力的表现在增强艺术图像的忠诚度并保留其先天美学的本质方面表现出色。通过弥合生成模型与艺术技巧之间的差距，freativesynth成为一种定制的数字调色板。

### GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2401.14032)] [[cool](https://papers.cool/arxiv/2401.14032)] [[pdf](https://arxiv.org/pdf/2401.14032)]
> **Authors**: Butian Xiong,Zhuo Li,Zhen Li
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: IJCAI2024 submit, 8 pages
- **标题**: 高斯（Gauu-scene）：大规模3D重建数据集的场景重建基准，使用高斯分裂
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们使用新开发的3D表示方法，高斯越野的新型大规模重建基准介绍了一种新颖的大型场景重建基准，在我们扩展的U-Scene数据集中。 U-Scene涵盖了一个半公里的一个半平方公里，其中包含一个全面的RGB数据集和LiDAR地面真相。为了进行数据采集，我们使用了配备高准确Zenmuse L1 LIDAR的MATRIX 300无人机，从而实现了精确的屋顶数据收集。该数据集提供了城市和学术环境的独特融合，用于高级空间分析对话超过1.5 km $^2 $。我们对使用高斯脱落的U-Scene的评估包括跨各种新颖观点的详细分析。我们还将这些结果与从我们的准确点云数据集中得出的结果并列，突出了重大差异，强调了组合多模式信息的重要性

### An Extensible Framework for Open Heterogeneous Collaborative Perception 
[[arxiv](https://arxiv.org/abs/2401.13964)] [[cool](https://papers.cool/arxiv/2401.13964)] [[pdf](https://arxiv.org/pdf/2401.13964)]
> **Authors**: Yifan Lu,Yue Hu,Yiqi Zhong,Dequan Wang,Yanfeng Wang,Siheng Chen
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: Accepted by ICLR 2024. The code and data are open-sourced at https://github.com/yifanlu0227/HEAL
- **标题**: 开放异质协作感知的可扩展框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 协作感知旨在通过促进多个代理之间的数据交换来减轻单药感知（例如遮挡）的局限性。但是，大多数当前作品都考虑了所有代理使用身份传感器和感知模型的均匀情况。实际上，与现有代理合作时，异质的代理类型可能会不断出现，并且不可避免地会面临域间隙。在本文中，我们引入了一个新的开放异构问题：如何将新的异质代理类型纳入协作感知，同时确保高感知性能和低集成成本？为了解决这个问题，我们提出了异质联盟（HEAL），这是一个新颖的可扩展协作感知框架。 HEAL首先通过新型的多尺度前景感知金字塔融合网络建立了具有初始代理的统一特征空间。当异类的新代理以先前看不见的方式或模型出现时，我们将它们与创新的后退对齐保持与已建立的统一空间保持一致。此步骤仅涉及对新代理类型的个人培训，从而呈现出极低的培训成本和高可扩展性。为了丰富代理的数据异质性，我们带来了一个具有更多种传感器类型的新型大型数据集OPV2V-H。 OPV2V-H和DAIR-V2X数据集的广泛实验表明，HEAL在整合3种新的代理类型时，HEL会超过性能的SOTA方法，同时将训练参数降低91.5％。我们进一步实施一个综合代码库，网址：https：//github.com/yifanlu0227/heal

### Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA 
[[arxiv](https://arxiv.org/abs/2401.15847)] [[cool](https://papers.cool/arxiv/2401.15847)] [[pdf](https://arxiv.org/pdf/2401.15847)]
> **Authors**: Yue Fan,Jing Gu,Kaiwen Zhou,Qianqi Yan,Shan Jiang,Ching-Chen Kuo,Xinze Guan,Xin Eric Wang
> **First submission**: 2024-01-28
> **First announcement**: 2024-01-29
> **comment**: ACL 2024
- **标题**: 松饼还是奇瓦瓦？具有多工具VQA的挑战多模式的大型语言模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 多工具图像，通常被视为网络屏幕截图，海报等，遍及我们的日常生活。这些图像的特征是它们在不同的布局中构成多个亚法，从而有效地将信息传达给人们。旨在构建高级多模式AI应用程序，例如了解复杂场景并通过网页导航的代理，多工会视觉推理的技能至关重要，并且对这方面的模型进行了全面的评估。因此，我们介绍了多工具视觉问题回答（MultipanelVQA），这是一个新颖的基准，其中包括6,600个问题，答案和多工具图像，这些图像在理解多工具图像时特别挑战了模型。我们的评估表明，即使人类在这些问题上可以达到约99％的准确性，但在多工程基准基准中的问题对最先进的多模式大语模型（MLLM）构成了重大挑战。独特的是，多工vqa基准特征合成生成的多工具图像专门为隔离和评估了各种因素的影响，例如布局，对MLLMS的Multipanel图像理解能力。结果，除了基准MLLM在理解多工厂图像方面的能力外，我们还分析了多工具图像的各种因素，这些因素使用合成数据影响MLLM的性能，并为增强提供见解。代码和数据在https://sites.google.com/view/multipanelvqa/home上发布。

### LCV2: An Efficient Pretraining-Free Framework for Grounded Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2401.15842)] [[cool](https://papers.cool/arxiv/2401.15842)] [[pdf](https://arxiv.org/pdf/2401.15842)]
> **Authors**: Yuhan Chen,Lumei Su,Lihua Chen,Zhiwei Lin
> **First submission**: 2024-01-28
> **First announcement**: 2024-01-29
> **comment**: 21 pages,9 figures
- **标题**: LCV2：一个有效的无训练框架，用于接地视觉问题
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在本文中，提出了LCV2模块化方法，用于在视觉多模式结构域中的接地视觉问题回答任务。这种方法依赖于冻结的大语言模型（LLM）作为现成的VQA模型与现成的视觉接地（VG）模型之间的中间调解人，其中LLM根据设计的提示在两个模块之间转换并传达两个模块之间的文本信息。 LCV2在不需要任何预训练过程的情况下建立一个集成的插件框架。可以在低计算资源下部署该框架来进行VQA接地任务。该框架中的模块化模型允许使用各种最新的预训练模型应用，具有与时俱进的巨大潜力。实验实现是在约束的计算和内存资源下进行的，评估了所提出的方法在包括GQA，CLEVR和VIZWIZ-VQA-GRODGER在内的基准数据集上的性能。使用基线方法进行比较分析证明了LCV2的强大竞争力。

### Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance 
[[arxiv](https://arxiv.org/abs/2401.15687)] [[cool](https://papers.cool/arxiv/2401.15687)] [[pdf](https://arxiv.org/pdf/2401.15687)]
> **Authors**: Qingcheng Zhao,Pengyu Long,Qixuan Zhang,Dafei Qin,Han Liang,Longwen Zhang,Yingliang Zhang,Jingyi Yu,Lan Xu
> **First submission**: 2024-01-28
> **First announcement**: 2024-01-29
> **comment**: Project Page: https://sites.google.com/view/media2face
- **标题**: Media2Face：带有多模式指导的共同语音面部动画生成
- **领域**: 计算机视觉和模式识别,图形
- **摘要**: 言语中3D面部动画的综合引起了人们的关注。由于高质量的4D面部数据和众多宣布丰富的多模式标签的稀缺性，以前的方法通常会受到现实主义有限和缺乏词汇条件的影响。我们通过三部曲解决了这一挑战。我们首先将广义的神经参数面部资产（GNPFA）引入了高度概括的表达空间，解耦表达式和身份。然后，我们利用GNPFA从大量视频中提取高质量表达式和准确的头部姿势。这介绍了M2F-D数据集，这是一个大型，多样化和扫描级别的共同言论3D面部动画数据集，具有畅通无阻的情感和风格标签。最后，我们提出了Media2Face，这是GNPFA潜在空间中的一个扩散模型，用于共同语音面部动画生成，接受音频，文本和图像的丰富多模式指南。广泛的实验表明，我们的模型不仅可以实现面部动画综合的高保真度，而且还扩大了3D面部动画中表达和风格适应性的范围。

### Distilling Privileged Multimodal Information for Expression Recognition using Optimal Transport 
[[arxiv](https://arxiv.org/abs/2401.15489)] [[cool](https://papers.cool/arxiv/2401.15489)] [[pdf](https://arxiv.org/pdf/2401.15489)]
> **Authors**: Muhammad Haseeb Aslam,Muhammad Osama Zeeshan,Soufiane Belharbi,Marco Pedersoli,Alessandro Koerich,Simon Bacon,Eric Granger
> **First submission**: 2024-01-27
> **First announcement**: 2024-01-29
> **comment**: No comments
- **标题**: 蒸馏使用最佳传输的特权多模式信息以表达表达式识别
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式表达识别的深度学习模型由于能够学习互补和冗余的语义信息，因此在受控的实验室环境中达到了显着的性能。但是，这些模型在野外挣扎，主要是因为用于培训的方式不可用和质量。实际上，在测试时只能提供一部分训练时间方式。使用特权信息学习，使模型可以从培训期间可用的其他模式中利用数据。已经提出了最先进的知识蒸馏（KD）方法，以将信息从多个教师模型（每种模式训练）提炼到一个共同的学生模型。这些特权的KD方法通常使用点对点匹配，但没有明确的机制来捕获通过引入特权模式形成的教师表示空间中的结构信息。在两个具有挑战性的问题上进行了实验 - 对BioVID数据集（序数分类）的疼痛估计以及对AFFWILD2数据集（回归）的唤醒价值预测。结果表明，我们提出的方法可以超过这些问题的最先进的特权KD方法。模态和融合体系结构之间的多样性表明PKDOT是模态和模型 - 不合时宜的。

### Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks 
[[arxiv](https://arxiv.org/abs/2401.15275)] [[cool](https://papers.cool/arxiv/2401.15275)] [[pdf](https://arxiv.org/pdf/2401.15275)]
> **Authors**: Yuliang Cai,Mohammad Rostami
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: No comments
- **标题**: 动态变压器架构，用于持续学习多模式任务
- **领域**: 计算机视觉和模式识别
- **摘要**: 变压器神经网络越来越多地替代不同数据模式中广泛应用程序中的先前架构。微调大型预训练的变压器神经网络的规模和计算需求不断增加，这对这些模型的广泛采用构成了重大挑战。为了应对这一挑战，持续学习（CL）作为解决方案出现，通过促进知识的转移，跨任务依次到达自主学习代理。但是，当前的CL方法主要集中于仅基于视觉或基于语言的学习任务。我们提出了一个基于变压器的CL框架，重点是学习视觉和语言，称为视觉和语言（Val）任务。由于变压器在其他模式下的成功，我们的体系结构有可能在多模式学习设置中使用。在我们的框架中，我们从将额外参数引入基础变压器中受益于每个任务的网络。结果，我们启用动态模型扩展以在序列中学习多个任务。我们还使用知识蒸馏来从相关的过去经验中受益，以更有效地学习当前任务。我们提出的方法，即专注的多模式持续学习（TAM-CL），可以在减轻灾难性遗忘的问题的同时交换任务之间的信息。值得注意的是，我们的方法是可扩展的，会产生最小的内存和时间开销。 TAM-CL在挑战多模式任务上实现最新的（SOTA）性能

### GeoDecoder: Empowering Multimodal Map Understanding 
[[arxiv](https://arxiv.org/abs/2401.15118)] [[cool](https://papers.cool/arxiv/2401.15118)] [[pdf](https://arxiv.org/pdf/2401.15118)]
> **Authors**: Feng Qi,Mian Dai,Zixian Zheng,Chao Wang
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-29
> **comment**: No comments
- **标题**: 地理编码器：授权多模式图理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 本文介绍了GeodeCoder，这是一种专门的多模式模型，旨在处理地图中的地理空间信息。 GeodeCoder建立在Beitgpt体系结构上，并结合了用于图像和文本处理的专业专家模块。在图像侧，GeodeCoder将Gaode Amap用作基础基础图，该基础图固有地包含有关道路和建筑形状，相对位置和其他属性的基本细节。通过利用渲染技术，该模型无缝地集成了外部数据和功能，例如符号标记，驱动器轨迹，热图和用户定义的标记，从而消除了对额外功能工程的需求。 GeodeCoder的文本模块接受各种上下文文本和问题提示，以GPT风格生成文本输出。此外，基于GPT的模型允许以端到端的方式培训和执行同一模型中的多个任务。为了增强MAP认知并使地理编码器能够获取有关北京地理实体分布的知识，我们设计了八个基本的地理空间任务，并使用大型文本图像样本对模型进行了预处理。随后，对三个下游任务进行了快速的微调，从而显着提高了性能。 GeodeCoder模型展示了对地图元素及其相关操作的全面理解，从而在不同的业务场景中可以有效且高质量地应用各种地理空间任务。

### From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities 
[[arxiv](https://arxiv.org/abs/2401.15071)] [[cool](https://papers.cool/arxiv/2401.15071)] [[pdf](https://arxiv.org/pdf/2401.15071)]
> **Authors**: Chaochao Lu,Chen Qian,Guodong Zheng,Hongxing Fan,Hongzhi Gao,Jie Zhang,Jing Shao,Jingyi Deng,Jinlan Fu,Kexin Huang,Kunchang Li,Lijun Li,Limin Wang,Lu Sheng,Meiqi Chen,Ming Zhang,Qibing Ren,Sirui Chen,Tao Gui,Wanli Ouyang,Yali Wang,Yan Teng,Yaru Wang,Yi Wang,Yinan He, et al. (11 additional authors not shown)
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: No comments
- **标题**: 从GPT-4到双子座及以后：通过四种方式评估MLLM关于普遍性，可信赖性和因果关系的景观
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大型语言模型（MLLM）在产生有关多模式内容的合理响应方面表现出了令人印象深刻的能力。但是，即使最强大的OpenAI的GPT-4和Google的Gemini已被部署，最近基于MLLM的应用程序的表现与广泛公众的期望之间仍然存在很大差异。本文通过一项定性研究的镜头来努力增强对差距的理解，该研究对四种模式的近期专有和开源MLLM的普遍性，可信度和因果推理能力：IE，文本，代码，图像和视频，最终旨在提高MLLM的透明度。我们认为，这些属性是定义MLLM可靠性的几个代表性因素，并支持各种下游应用程序。具体而言，我们评估了封闭源GPT-4和Gemini以及6个开源LLM和MLLM。总体而言，我们评估了230个手动设计的案例，然后将定性结果汇总为12个分数（即，4个模式时间3属性）。总的来说，我们发现了14个经验发现，可用于了解专有和开源MLLM的功能和局限性，以实现更可靠的下游多模式应用程序。

### Memory-Inspired Temporal Prompt Interaction for Text-Image Classification 
[[arxiv](https://arxiv.org/abs/2401.14856)] [[cool](https://papers.cool/arxiv/2401.14856)] [[pdf](https://arxiv.org/pdf/2401.14856)]
> **Authors**: Xinyao Yu,Hao Sun,Ziwei Niu,Rui Qin,Zhenjia Bai,Yen-Wei Chen,Lanfen Lin
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: No comments
- **标题**: 内存为文本图像分类的内存启发的时间提示互动
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 近年来，大规模的预训练的多模型模型（LMM）通常会出现以整合视觉和语言方式，从而在各种自然语言处理和计算机视觉任务中取得了巨大成功。但是，LMM的规模不断增长，导致对这些模型进行下游任务的微调成本巨大。因此，研究了基于及时的互动策略，以更有效地对齐方式。在此CONTEX中，我们提出了一种受人体记忆策略启发的基于新颖的基于及时的多模式相互作用策略，即记忆启发的时间及时互动（MITP）。我们提出的方法涉及两个阶段，如人类记忆策略：收购阶段以及巩固和激活阶段。我们利用中间层上的时间提示来模仿获得阶段，利用基于相似性的及时交互来模仿内存整合，并采用及时生成策略来模仿内存激活。论文的主要优势在于，我们与中间层上的迅速向量相互作用，以利用模式之间的足够信息交换，可压缩训练的参数和内存使用情况。我们在几个数据集中获得了竞争结果，具有相对较小的内存使用和2.0m可训练的参数（约占预训练的基础模型的1％）。

### Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring 
[[arxiv](https://arxiv.org/abs/2401.14838)] [[cool](https://papers.cool/arxiv/2401.14838)] [[pdf](https://arxiv.org/pdf/2401.14838)]
> **Authors**: Dan Lin,Philip Hann Yung Lee,Yiming Li,Ruoyu Wang,Kim-Hui Yap,Bingbing Li,You Shing Ngim
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: No comments
- **标题**: 基于车辆机舱监控的双重特征转移的多模式动作识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 驾驶员行动识别（DAR）对于车辆舱监测系统至关重要。在现实世界中，车厢配备具有不同方式的相机是常见的。但是，很少研究汽车舱内DAR任务的多模式融合策略。在本文中，我们提出了一种基于双重特征转移的新颖而有效的多模式驱动器动作识别方法，名为DFS。 DFS首先通过执行模态特征交互来整合跨模态的互补特征。同时，DFS通过时间框架之间的特征移动来实现单个模式内的邻居特征传播。为了学习通用模式并提高模型效率，DFS共享多种模式之间提取阶段。已经进行了广泛的实验，以验证驱动器\＆ACT数据集上提出的DFS模型的有效性。结果表明，DFS实现了良好的性能并提高了多模式驱动器动作识别的效率。

### Synchformer: Efficient Synchronization from Sparse Cues 
[[arxiv](https://arxiv.org/abs/2401.16423)] [[cool](https://papers.cool/arxiv/2401.16423)] [[pdf](https://arxiv.org/pdf/2401.16423)]
> **Authors**: Vladimir Iashin,Weidi Xie,Esa Rahtu,Andrew Zisserman
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: Extended version of the ICASSP 24 paper. Project page: https://www.robots.ox.ac.uk/~vgg/research/synchformer/ Code: https://github.com/v-iashin/Synchformer
- **标题**: 同步器：稀疏提示的有效同步
- **领域**: 计算机视觉和模式识别,机器学习,多媒体,声音,音频和语音处理
- **摘要**: 我们的目标是视听同步，重点是“野外”视频，例如在YouTube上的视频，在YouTube上，同步提示可能很少。我们的贡献包括一种新型的视听同步模型，以及将通过多模式段级对比度预训练从同步建模中提取的训练。这种方法在密集和稀疏设置中都达到了最新的性能。我们还将同步模型培训扩展到音频集以一个数百万级的“野外”数据集，研究证据归因技术以解释性，并探索了同步模型的新功能：音频 - 视觉同步性。

### InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model 
[[arxiv](https://arxiv.org/abs/2401.16420)] [[cool](https://papers.cool/arxiv/2401.16420)] [[pdf](https://arxiv.org/pdf/2401.16420)]
> **Authors**: Xiaoyi Dong,Pan Zhang,Yuhang Zang,Yuhang Cao,Bin Wang,Linke Ouyang,Xilin Wei,Songyang Zhang,Haodong Duan,Maosong Cao,Wenwei Zhang,Yining Li,Hang Yan,Yang Gao,Xinyue Zhang,Wei Li,Jingwen Li,Kai Chen,Conghui He,Xingcheng Zhang,Yu Qiao,Dahua Lin,Jiaqi Wang
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: Code and models are available at https://github.com/InternLM/InternLM-XComposer
- **标题**: Internlm-Xcomposer2：掌握视觉上的自由形式的文本图像组成和理解大型模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 我们介绍了InternLM-Xcomposer2，这是一种在自由形式的文本图像组成和理解中卓越的尖端视觉语言模型。该模型超出了传统的视觉语言理解，从概述，详细的文本规范和参考图像等各种输入中熟练地制作了交织的文本图像内容，从而启用了高度可自定义的内容创建。 InternLM-Xcomposer2提出了一种部分洛拉（PLORA）方法，该方法专门用于图像令牌，以保持预训练的语言知识的完整性，在精确的视觉理解和文本构成与文学人才之间达到平衡。实验结果证明了基于InterLM2-7B的InternLM-XComposer2在生产高质量的长培根多模式内容及其出色的视觉理解性能方面的优势，在各种基准测试中，它不仅超过了现有的多模型，而且还匹配了GPT-4V和GENSEMINTIMENT，它不仅超过了现有的多模型。这突出了其在多模式理解领域的杰出熟练程度。 https://github.com/internlm/internlm/internlm-xcomposer公开获得了带有7B参数的InternLM-XCOMPOSER2模型系列。

### PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology 
[[arxiv](https://arxiv.org/abs/2401.16355)] [[cool](https://papers.cool/arxiv/2401.16355)] [[pdf](https://arxiv.org/pdf/2401.16355)]
> **Authors**: Yuxuan Sun,Hao Wu,Chenglu Zhu,Sunyi Zheng,Qizi Chen,Kai Zhang,Yunlong Zhang,Dan Wan,Xiaoxiao Lan,Mengyue Zheng,Jingxiong Li,Xinheng Lyu,Tao Lin,Lin Yang
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: 27 pages, 12 figures
- **标题**: pathmmu：用于理解和推理病理学的庞大多模式专家级别基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型模型的出现在AI中具有巨大的潜力，尤其是病理学。但是，缺乏专业的高质量基准阻碍了他们的发展和精确的评估。为了解决这个问题，我们介绍了大型多模型（LMM）的最大和最高质量专家验证的病理基准Pathmmu。它包括33,428个多模式的多选择问题和来自各种来源的24,067张图像，每个图像都伴随着正确答案的解释。 Pathmmu的构建利用GPT-4V的高级功能，利用30,000多个图像扣对来丰富字幕并在级联过程中生成相应的Q＆AS。值得注意的是，为了最大化Pathmmu的权威，我们邀请七位病理学家根据PathMMU的验证和测试集的严格标准对每个问题进行审查，同时为Pathmmu设置专家级的性能基准。我们进行了广泛的评估，包括对14个开源和4个封闭式LMM的零拍评估及其对腐败形象的稳健性。我们还微调了代表性LMM，以评估其对Pathmmu的适应性。经验发现表明，高级LMM与具有挑战性的PATHMMU基准斗争，其表现最佳的LMM，GPT-4V，仅实现了49.8％的零球性能，显着低于人类病理学家证明的71.8％。经过微调后，显着较小的开源LMM可以胜过GPT-4V，但仍然没有病理学家所显示的专业知识。我们希望Pathmmu能够提供有价值的见解，并促进更专业的下一代LMMS病理学的发展。

### LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs 
[[arxiv](https://arxiv.org/abs/2401.16160)] [[cool](https://papers.cool/arxiv/2401.16160)] [[pdf](https://arxiv.org/pdf/2401.16160)]
> **Authors**: Shaoxiang Chen,Zequn Jie,Lin Ma
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: No comments
- **标题**: llava-mole：洛拉专家的稀疏混合物，用于减轻指令登录MLLMS中的数据冲突
- **领域**: 计算机视觉和模式识别
- **摘要**: 在各种图像文本指令数据上进行的指令对获得多种模式大型语言模型（MLLM）的关键，并且指令数据的不同配置可以导致具有不同功能的填充模型。但是，我们发现，在从不同域中混合指令数据时，数据冲突是不可避免的，这可能会导致特定域任务的性能下降。为了解决这个问题，我们建议采用有效的专家（MOE）设计的混合物，该设计是洛拉专家（mole）的稀疏混合物（MOLE），以供指导MLLM。在变压器层中，我们通过创建专门针对MLP层的一组LORA专家来扩展流行的低级自适应方法（LORA）方法，并根据路由功能将每个令牌路由到Top-1专家，从而允许从不同领域的代币进行自适应选择。由于洛拉专家被稀少地激活，因此与原始洛拉方法相比，训练和推理成本大致保持稳定。通过用我们的Moe设计代替Llava-1.5的普通Lora，我们的最终型号称为Llava-mole。广泛的实验证明，LLAVA摩尔在将多个不同的指令数据集与各种配置混合在一起时可以有效地减轻数据冲突问题，并在强大的普通 - 洛拉碱基上实现一致的性能提高。最重要的是，在混合数据集上，Llava-mole甚至可以胜过训练两倍的纯洛拉基线。

### Find the Cliffhanger: Multi-Modal Trailerness in Soap Operas 
[[arxiv](https://arxiv.org/abs/2401.16076)] [[cool](https://papers.cool/arxiv/2401.16076)] [[pdf](https://arxiv.org/pdf/2401.16076)]
> **Authors**: Carlo Bretti,Pascal Mettes,Hendrik Vincent Koops,Daan Odijk,Nanne van Noord
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: MMM24
- **标题**: 查找Cliffhanger：肥皂剧中的多模式拖车
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 创建预告片需要仔细挑选并从更长的视频中拼凑出简短的诱人时刻，这使其成为一项具有挑战性且耗时的任务。这需要基于视觉和对话信息的选择时刻。我们介绍了一种多模式方法，用于预测拖车，以帮助编辑从长篇视频中选择值得拖车的时刻。我们在新引入的肥皂剧数据集上介绍了结果，表明预测拖车是一项艰巨的任务，从多模式信息中受益。代码可从https://github.com/carlobretti/cliffhanger获得

### MoE-LLaVA: Mixture of Experts for Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2401.15947)] [[cool](https://papers.cool/arxiv/2401.15947)] [[pdf](https://arxiv.org/pdf/2401.15947)]
> **Authors**: Bin Lin,Zhenyu Tang,Yang Ye,Jinfa Huang,Junwu Zhang,Yatian Pang,Peng Jin,Munan Ning,Jiebo Luo,Li Yuan
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: update author
- **标题**: Moe-llava：大型视觉模型的专家的混合物
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近的进步表明，扩展大型视觉模型（LVLM）有效地改善了下游任务性能。但是，现有的缩放方法使所有模型参数都能为计算中的每个令牌都活跃，从而带来了大规模的培训和推断成本。在这项工作中，我们为LVLM提出了一种简单而有效的培训策略。这种策略是在多模式稀疏学习中创新的，绩效降解的常见问题，因此构建了一个稀疏的模型，这些模型具有少量的参数，但持续的计算成本。此外，我们介绍了Moe-llava，这是一种基于MoE的稀疏LVLM体系结构，它在部署过程中只能通过路由器唯一地激活Top-K专家，使其余专家不活跃。广泛的实验表明，Moe-llava在各种视觉理解和对象幻觉基准中的显着性能。值得注意的是，Moe-llava只有大约3B稀疏激活的参数，在各种视觉理解数据集上展示了与Llava-1.5-7b相当的性能，甚至超过了物体幻觉基准中的LLAVA-1.5-13B。通过Moe-llava，我们旨在为稀疏的LVLM建立基线，并为未来的研究提供有价值的见解，以开发更有效，有效的多模式学习系统。代码在https://github.com/pku-yuangroup/moe-llava上发布。

### M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining 
[[arxiv](https://arxiv.org/abs/2401.15896)] [[cool](https://papers.cool/arxiv/2401.15896)] [[pdf](https://arxiv.org/pdf/2401.15896)]
> **Authors**: Qingpei Guo,Furong Xu,Hanxiao Zhang,Wang Ren,Ziping Ma,Lin Ju,Jian Wang,Jingdong Chen,Ming Yang
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: No comments
- **标题**: M2编码器：通过大规模预处理提高双语图像文本理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 诸如剪辑之类的视觉基础模型已彻底改变了人工智能领域。然而，由于大规模预处理数据集的相对稀缺性，支持多语言的VLM模型，例如中文和英语的模型滞后。为此，我们介绍了一个全面的双语（中文）数据集BM-6B，具有超过60亿个图像文本对，旨在增强多模式基础模型，以很好地了解两种语言的图像。为了处理如此规模的数据集，我们提出了一种新颖的分组聚合方法，用于对比损失计算，从而减少了沟通开销和GPU内存的需求，从而促进了训练速度的60％。我们预先一系列双语图像文本基础模型，具有增强的BM-6B的精细颗粒理解能力，结果模型，被称为$ M^2 $  - 编码器（发音为“ M平方”），以两种语言为多模式检索和分类任务设置新基准。值得注意的是，我们最大的$ M^2 $ -Ancoder-10b型号在零摄像机分类设置下，ImageNet上的TOP-1精度为88.5％，Imagenet-CN的88.7％的精度为80.7％，分别超过了先前报道的SOTA方法，分别超过2.2％和21.1％。 $ m^2 $  - 编码系列代表了迄今为止最全面的双语图像文本基础模型之一，因此我们正在向研究社区提供进一步的探索和开发。

### Through-Wall Imaging based on WiFi Channel State Information 
[[arxiv](https://arxiv.org/abs/2401.17417)] [[cool](https://papers.cool/arxiv/2401.17417)] [[pdf](https://arxiv.org/pdf/2401.17417)]
> **Authors**: Julian Strohmayer,Rafael Sterzinger,Christian Stippel,Martin Kampel
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: Added link to source code repository
- **标题**: 基于WiFi通道状态信息的壁成像
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 这项工作提出了一种开创性的方法，用于在通过壁式场景中从WiFi通道状态信息（CSI）中综合图像。利用wifi的优势，例如成本效益，照明不变性和墙壁穿透能力，我们的方法可以视觉监视房间边界以外的室内环境，而无需相机。更一般地，它通过解锁执行基于图像的下游任务（例如视觉活动识别）的选项来提高WiFi CSI的可解释性。为了实现从wifi csi到图像的跨模式翻译，我们依赖于适合我们问题细节的多模式变分自动编码器（VAE）。我们通过消融建筑配置和重建图像的定量/定性评估来广泛评估我们提出的方法。我们的结果证明了我们方法的生存能力，并突出了其实用应用的潜力。

### YTCommentQA: Video Question Answerability in Instructional Videos 
[[arxiv](https://arxiv.org/abs/2401.17343)] [[cool](https://papers.cool/arxiv/2401.17343)] [[pdf](https://arxiv.org/pdf/2401.17343)]
> **Authors**: Saelyne Yang,Sunghyun Park,Yunseok Jang,Moontae Lee
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: AAAI 2024
- **标题**: YTCOMMENTQA：教学视频中的视频问题答案性
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 教学视频为各种任务提供了详细的操作指南，观众经常提出有关内容的问题。解决这些问题对于理解内容至关重要，但是很难收到即时答案。尽管已经开发了许多计算模型来用于视频响应（视频质量质量质量标准）任务，但它们主要是根据基于视频内容生成的问题进行培训的，旨在从内容中产生答案。但是，在现实情况下，用户可能会提出超出视频信息界限的问题，从而强调确定视频是否可以提供答案的必要性。由于视频的多模式性质，视频内容可以通过视频内容来回答问题，在视频中，视觉和口头信息是交织在一起的。为了弥合这一差距，我们提出了YTCommentQa数据集，该数据集包含YouTube自然产生的问题，按它们的答复性和所需的方式来回答 - 视觉，脚本或两者兼而有之。具有回答性分类任务的实验证明了YTCommentQA的复杂性，并强调需要理解视觉和脚本信息在视频推理中的综合作用。该数据集可从https://github.com/lgresearch/ytcommentqa获得。

### Achieving More Human Brain-Like Vision via Human EEG Representational Alignment 
[[arxiv](https://arxiv.org/abs/2401.17231)] [[cool](https://papers.cool/arxiv/2401.17231)] [[pdf](https://arxiv.org/pdf/2401.17231)]
> **Authors**: Zitong Lu,Yile Wang,Julie D. Golomb
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: No comments
- **标题**: 通过人类脑电图的代表对齐来实现更多类似人脑的视力
- **领域**: 计算机视觉和模式识别,机器学习,神经和进化计算,神经元和认知
- **摘要**: 尽管人工智能方面的进步，对象识别模型仍在模仿人类大脑的视觉信息处理方面仍然落后。最近的研究强调了使用神经数据模仿大脑处理的潜力。但是，这些通常依赖于非人类受试者的侵入性神经记录，从而在理解人类视觉感知方面留下了危险的差距。在解决这一差距时，我们首次介绍了“ RE（表演）Al（点火）网络”，该视觉模型与基于非侵入性脑电图的人类大脑活动一致，表明与人脑表示的相似性明显更高。我们的创新图像到脑多层编码框架通过优化多个模型层并使模型能够有效学习并模仿对象类别和不同方式的人类大脑的视觉表示模式，从而提高了人类神经对齐方式。我们的发现表明，Realnet代表了弥合人造和人类视觉之间差距的突破，并为更多类似大脑的人工智能系统铺平了道路。

### Self-Supervised Representation Learning for Nerve Fiber Distribution Patterns in 3D-PLI 
[[arxiv](https://arxiv.org/abs/2401.17207)] [[cool](https://papers.cool/arxiv/2401.17207)] [[pdf](https://arxiv.org/pdf/2401.17207)]
> **Authors**: Alexander Oberstrass,Sascha E. A. Muenzing,Meiqi Niu,Nicola Palomero-Gallagher,Christian Schiffer,Markus Axer,Katrin Amunts,Timo Dickscheid
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: No comments
- **标题**: 3D-PLI中神经纤维分布模式的自学表示学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 对人脑中组织原理的全面理解还需要神经纤维结构的可量化描述符。三维偏振光成像（3D-PLI）是一种微观成像技术，可以洞悉具有高分辨率的髓鞘神经纤维的细粒组织。表征在3D-PLI中观察到的纤维结构的描述符将实现下游分析任务，例如多模式相关研究，聚类和映射。但是，尚无观察者与3D-PLI中纤维架构无关的表征的最佳实践。为此，我们提出了完全数据驱动的方法的应用，以使用自我监督的表示学习来表征3D-PLI图像中的神经纤维结构。我们引入了一个3D-Context对比度学习（CL-3D）的目标，该目标利用了3D重建体积的组织学脑部构成纹理示例的空间邻域，以对对比度学习进行样品阳性对。我们将这种采样策略与专门设计的图像增强结合在一起，以获得3D-PLI参数图中典型变化的稳健性。对于三d猴脑的3D重建枕叶，证明了该方法。我们表明，提取的特征对神经纤维的不同构型高度敏感，但对连续的脑部切片之间由组织学处理引起的变化有良好的变化。我们证明了它们用于检索均匀纤维架构簇的实际适用性，并为纤维架构（例如U光纤）特定组件的交互选择模板（例如U光纤）进行数据挖掘。

### Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning 
[[arxiv](https://arxiv.org/abs/2401.17186)] [[cool](https://papers.cool/arxiv/2401.17186)] [[pdf](https://arxiv.org/pdf/2401.17186)]
> **Authors**: Bang Yang,Yong Dai,Xuxin Cheng,Yaowei Li,Asif Raza,Yuexian Zou
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: Accepted by AAAI'2024, 15 pages (with appendix), 7 figures, 10 tables
- **标题**: 通过持续的语言学习在剪辑中拥抱语言包容性和多样性
- **领域**: 计算机视觉和模式识别,人工智能,信息检索
- **摘要**: 近年来，视觉语言预训练的模型（VL-PTM）进行了高级多模式研究，但它们以几种语言（例如英语）的掌握限制了它们在更广泛的社区中的适用性。为此，人们对通过联合学习设置开发多语言VL模型的兴趣越来越大，但是由于昂贵的成本和数据可用性，这可能是不现实的。在这项工作中，我们建议通过连续的语言学习（CLL）扩展VL-PTMS的语言能力，其中模型需要逐步更新其语言知识，而不会遭受灾难性遗忘（CF）。我们通过引入一个称为Cll-CLIP的模型来开始我们的研究，该模型以剪辑为基础，该剪辑是一种盛行的VL-PTM，它已经获得了图像 - 英语对齐。具体而言，CLL-CLIP包含一个可扩展的令牌嵌入层来处理语言差异。它仅训练令牌嵌入以提高内存稳定性，并在跨模式和跨语性目标下进行优化，以学习图像和多语言文本之间的对齐方式。为了减轻协变量转移和词汇重叠所提出的CF，我们进一步提出了一种新颖的方法，以确保在初始化过程中所有令牌嵌入的分布相同，并在训练过程中正规化令牌嵌入学习。我们构建了一个基于MSCOCO和XM3600数据集的36种语言的CLL基准测试，然后评估多语言图像文本检索性能。广泛的实验验证了Cll-CLIP的有效性，并表明我们的方法可以在XM3600上的文本到图像平均召回@1增加6.7％，并始终提高各种最新方法。我们的代码和数据可在\ url {https://github.com/yangbang18/clfm}中获得。

### Fourier Prompt Tuning for Modality-Incomplete Scene Segmentation 
[[arxiv](https://arxiv.org/abs/2401.16923)] [[cool](https://papers.cool/arxiv/2401.16923)] [[pdf](https://arxiv.org/pdf/2401.16923)]
> **Authors**: Ruiping Liu,Jiaming Zhang,Kunyu Peng,Yufan Chen,Ke Cao,Junwei Zheng,M. Saquib Sarfraz,Kailun Yang,Rainer Stiefelhagen
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: Accepted to IEEE IV 2024. The source code is publicly available at https://github.com/RuipingL/MISS
- **标题**: 傅立叶提示调谐模态分段的场景细分
- **领域**: 计算机视觉和模式识别,机器人技术,图像和视频处理
- **摘要**: 从多种方式整合信息可以增强自动驾驶汽车中场景感知系统的鲁棒性，从而提供了更全面和可靠的感官框架。但是，多模式分割中的模态不完整仍然不足。在这项工作中，我们建立了一个名为“模态”场景分割（MISS）的任务，该任务涵盖了系统级模态缺勤和传感器级模态错误。为了避免多模式融合中主要的模式依赖，我们引入了缺失的模态开关（MMS）策略，以主动管理训练期间缺失的模态。在完整和不完整的测试方案中，利用位批量抽样的采样可增强模型的性能。此外，我们介绍了傅立叶及时调整（FPT）方法，将代表性的光谱信息纳入有限数量的可学习提示中，以维持与所有错过场景的稳健性。类似于微调效果，但可调参数较少（1.1％）。广泛的实验证明了我们提出的方法的功效，表明在缺少模态的先前最先进的参数效率方法上，改善了5.84％MIOU。源代码可在https://github.com/ruipingl/miss上公开获得。

### EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain 
[[arxiv](https://arxiv.org/abs/2401.16822)] [[cool](https://papers.cool/arxiv/2401.16822)] [[pdf](https://arxiv.org/pdf/2401.16822)]
> **Authors**: Wei Zhang,Miaoxin Cai,Tong Zhang,Yin Zhuang,Xuerui Mao
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: No comments
- **标题**: EarthGPT：遥感域中多模式的通用多模式大型语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大型语言模型（MLLM）在自然图像域内的视觉和视觉任务中表现出了显着的成功。由于自然传感和遥感图像之间的重要多样性，RS域中MLLM的发展仍处于婴儿阶段。为了填补空白，本文提出了一个名为Earthgpt的先锋MLLM EarthGPT，该先锋局部集成了各种多传感器RS解释任务。在Earthgpt中，开发了三种关键技术，包括视觉增强的感知机制，跨模式相互理解方法以及RS域中多传感器多任务的统一指令调整方法。更重要的是，构建了一个名为MMRS-1M的数据集，该数据集具有大规模多传感器多模式RS RS指令跟踪的构造，包括基于34个现有的不同RS数据集的1M图像text对，以及包括光学，光学，综合光圈雷达（SAR）和Infrared等多传感器图像。 MMRS-1M数据集解决了RS专家知识上MLLM的缺点，并刺激RS域中MLLM的开发。进行了广泛的实验，证明了与其他专家模型和MLLM相比，Earthgpt在各种RS视觉解释任务中的出色表现，证明了拟议的Earthgpt的有效性，并为开放设定的推理任务提供了多功能范式。

## 计算机与社会(cs.CY:Computers and Society)

该领域共有 2 篇论文

### Characteristics and prevalence of fake social media profiles with AI-generated faces 
[[arxiv](https://arxiv.org/abs/2401.02627)] [[cool](https://papers.cool/arxiv/2401.02627)] [[pdf](https://arxiv.org/pdf/2401.02627)]
> **Authors**: Kai-Cheng Yang,Danishjeet Singh,Filippo Menczer
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: 18 pages, 6 figures; added a new dataset and made some minor revisions
- **标题**: 具有AI生成的面孔的假社交媒体概况的特征和流行率
- **领域**: 计算机与社会,人工智能,社交和信息网络
- **摘要**: 生成人工智能（AI）的最新进展引起了人们对创造令人信服的假社交媒体帐户的潜力的担忧，但缺乏经验证据。在本文中，我们使用生成对抗网络（GAN）为其个人资料图片生成的人体对Twitter（X）帐户进行系统分析。我们提供了一个1,420个此类帐户的数据集，并表明它们用于传播骗局，垃圾邮件和放大协调的消息以及其他不真实的活动。利用gan生成的面孔的特征 - 始终如一的眼睛 - 并用人类注释来补充它，我们设计了一种有效的方法来识别野生中甘恩生成的特征。将此方法应用于Active Twitter用户的随机样本，我们估计使用GAN生成的面孔在0.021％至0.044％之间的剖面率的下限，约为每日活跃帐户。这些发现强调了多模式生成AI构成的新兴威胁。我们发布了检测方法的源代码以及收集的数据，以促进进一步的研究。此外，我们提供实用的启发式方法，以帮助社交媒体用户识别此类帐户。

### Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity 
[[arxiv](https://arxiv.org/abs/2401.07348)] [[cool](https://papers.cool/arxiv/2401.07348)] [[pdf](https://arxiv.org/pdf/2401.07348)]
> **Authors**: Claudio Novelli,Federico Casolari,Philipp Hacker,Giorgio Spedicato,Luciano Floridi
> **First submission**: 2024-01-14
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 欧盟法律中的生成AI：责任，隐私，知识产权和网络安全
- **领域**: 计算机与社会,人工智能
- **摘要**: 生成AI的出现，特别是通过Chatgpt等大型语言模型（LLM）及其继任者的出现，标志着AI景观的范式转变。 Advanced LLMS表现出多模式，处理各种数据格式，从而扩大了它们的应用范围。但是，这些模型的复杂性和紧急自主权引发了可预测性和法律合规性的挑战。本文深入研究了欧盟环境中生成AI和LLM的法律和法规含义，分析了责任，隐私，知识产权和网络安全方面。它批判性地研究了现有和拟议的欧盟立法的充分性，包括《人工智能法》（AIA）草案，以应对一般AI尤其是LLMS所带来的独特挑战。本文确定了立法框架中的潜在差距和缺点，并提出了建议，以确保生成模型的安全和合规部署，以确保它们与欧盟不断发展的数字景观和法律标准保持一致。

## 图形(cs.GR:Graphics)

该领域共有 1 篇论文

### Diverse Part Synthesis for 3D Shape Creation 
[[arxiv](https://arxiv.org/abs/2401.09384)] [[cool](https://papers.cool/arxiv/2401.09384)] [[pdf](https://arxiv.org/pdf/2401.09384)]
> **Authors**: Yanran Guan,Oliver van Kaick
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 3D形状创建的各种零件合成
- **领域**: 图形,计算机视觉和模式识别,机器学习
- **摘要**: 在过去的几年中，引入了使用神经网络以基于部分表示形式合成3D形状的方法。这些方法表示形状作为零件的图或层次结构，并启用了各种应用，例如形状采样和重建。但是，当前方法不允许根据用户偏好轻松地再生单个形状零件。在本文中，我们研究了允许用户为单个零件生成多种不同建议的技术。具体而言，我们尝试了多模式深的生成模型，这些模型允许对形状零件进行采样不同的建议，并专注于以前关于形状合成的工作中未考虑的模型。为了提供对这些技术的比较研究，我们介绍了一种在基于部分表示中合成3D形状的方法，并评估了此合成方法中的所有零件建议技术。在我们的方法中受到先前工作的启发，形状以隐式函数的形式表示为一组部分，然后将其放在空间中以形成最终形状。基于隐式解码器和空间变压器的神经网络体系结构启用了此表示中的合成。我们通过评估其在生成零件建议时的性能来比较各种多模式模型。我们的贡献是通过定性和定量评估来展示多模式零件生成的新技术的表现最佳，并且基于表现最好的技术的合成方法使用户可以在重新构造形状时保持3D形状中生成的零件，同时保持3D形状中生成的零件。

## 人机交互(cs.HC:Human-Computer Interaction)

该领域共有 7 篇论文

### Effects of Multimodal Explanations for Autonomous Driving on Driving Performance, Cognitive Load, Expertise, Confidence, and Trust 
[[arxiv](https://arxiv.org/abs/2401.04206)] [[cool](https://papers.cool/arxiv/2401.04206)] [[pdf](https://arxiv.org/pdf/2401.04206)]
> **Authors**: Robert Kaufman,Jean Costa,Everlyne Kimani
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: 14 pages, published in Scientific Reports
- **标题**: 多模式解释对自动驾驶的影响对驾驶性能，认知负荷，专业知识，信心和信任的影响
- **领域**: 人机交互,人工智能,机器人技术
- **摘要**: 自主驾驶的进步为AI辅助驾驶指导提供了机会，直接解决了对人类驾驶改善的关键需求。 AI讲师应该如何传达信息以促进学习？在一个前实验（n = 41）中，我们测试了AI教练在绩效驾驶专家说明后建模的解释性通信的影响。参与者分为四（4）个组，以评估AI教练解释的两个（2）个维度：信息类型（'What'和'为什么''型解释）和演示方式（听觉和视觉）。我们比较不同的解释技术如何通过观察学习影响驾驶绩效，认知负荷，信心，专业知识和信任。通过面试，我们描述了参与者的学习过程。结果表明，AI教练可以有效地向新手讲授绩效驾驶技能。我们发现信息的类型和方式会影响性能成果。在成功学习的方式方面的差异归因于信息如何指导注意力，减轻不确定性以及参与者经历的过载。结果表明，在设计有效的HMI通信时，应选择有效的，适合模态的解释，而HMI通信可以在不压倒性的情况下进行指导。此外，结果支持需要将沟通与人类学习和认知过程保持一致。我们为未来的自动驾驶汽车HMI和AI教练设计提供了八种设计含义。

### Exploring of Discrete and Continuous Input Control for AI-enhanced Assistive Robotic Arms 
[[arxiv](https://arxiv.org/abs/2401.07118)] [[cool](https://papers.cool/arxiv/2401.07118)] [[pdf](https://arxiv.org/pdf/2401.07118)]
> **Authors**: Max Pascher,Kevin Zinta,Jens Gerken
> **First submission**: 2024-01-13
> **First announcement**: 2024-01-15
> **comment**: Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction
- **标题**: 探索AI增强辅助机器人臂的离散和连续输入控制
- **领域**: 人机交互,人工智能,机器人技术
- **摘要**: 机器人臂是针对运动障碍者的家庭护理中不可或缺的一部分，使他们能够独立地进行日常生活（ADL）的活动，从而减少对人类护理人员的依赖。这些协作机器人要求用户管理多个自由度（DOF），以掌握和操纵对象等任务。常规输入设备（通常仅限于两个DOF）需要频繁且复杂的模式开关来控制单个DOF。带有馈送多模式反馈的现代自适应控制减少了整体任务完成时间，模式开关的数量和认知负载。尽管可用的输入设备多种多样，但它们在具有辅助机器人技术的自适应环境中的有效性尚未得到彻底评估。这项研究通过将它们集成到辅助机器人技术，对其进行评估并通过初步研究为未来发展提供经验见解来探索三种不同的输入设备。

### Self context-aware emotion perception on human-robot interaction 
[[arxiv](https://arxiv.org/abs/2401.10946)] [[cool](https://papers.cool/arxiv/2401.10946)] [[pdf](https://arxiv.org/pdf/2401.10946)]
> **Authors**: Zihan Lin,Francisco Cruz,Eduardo Benitez Sandoval
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-22
> **comment**: Australasian Conference on Robotics and Automation (ACRA). 2023
- **标题**: 对人类机器人互动的自我背景感知的情感感知
- **领域**: 人机交互,人工智能
- **摘要**: 情绪识别在人类机器人相互作用的各个领域都起着至关重要的作用。在与人类的长期互动中，机器人需要连续，准确地做出反应，但是，主流情绪识别方法主要集中在短期情绪识别上，而无视感知情绪的环境。人类认为，上下文信息和不同的上下文会导致完全不同的情绪表达。在本文中，我们介绍了自我上下文感知模型（SCAM），该模型（SCAM）采用二维情感坐标系统来锚定和重新标记不同的情绪。同时，它结合了其独特的信息保留结构和上下文损失。这种方法在音频，视频和多模式之间取得了重大改进。在听觉方式中，准确性有了显着的增强，从63.10％上升到72.46％。同样，视觉方式表现出了提高的精度，从77.03％增加到80.82％。在多模式中，准确性的高度从77.48％到78.93％。将来，我们将通过心理学实验来验证机器人对机器人的可靠性和可用性。

### VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games 
[[arxiv](https://arxiv.org/abs/2401.12133)] [[cool](https://papers.cool/arxiv/2401.12133)] [[pdf](https://arxiv.org/pdf/2401.12133)]
> **Authors**: He Zhang,Xinyang Li,Yuanxi Sun,Xinyi Fu,Christine Qiu,John M. Carroll
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: Accepted to IEEE VR 2024
- **标题**: VRMN-BD：VR站立互动游戏中沉浸式人类恐惧反应的多模式自然行为数据集
- **领域**: 人机交互,计算机视觉和模式识别,机器学习
- **摘要**: 在元时代，理解和识别情绪是重要且具有挑战性的问题。在虚拟现实（VR）环境中，理解，识别和预测恐惧是人类的基本情绪之一，在沉浸式游戏开发，场景发展和下一代虚拟虚拟人类计算器的互动应用中起着至关重要的作用。在本文中，我们使用VR恐怖游戏作为一种媒介来分析23名玩家的多模式数据（姿势，音频和生理信号），从而分析恐惧情绪。我们使用基于LSTM的模型以6级分类（无恐惧和五个不同级别的恐惧）和2级分类（没有恐惧和恐惧）的精度为65.31％和90.47％的精度来预测恐惧。我们构建了一个沉浸式人类恐惧反应（VRMN-BD）的多模式自然行为数据集，并将其与现有的相关高级数据集进行了比较。结果表明，我们的数据集在收集方法，数据量表和受众范围方面的限制较少。在VR站立交互环境中，我们在针对恐惧和行为的多模式数据集方面是独一无二的。此外，我们讨论了这项工作对社区和应用的影响。数据集和预培训模型可在https://github.com/kindopstar/vrmn-bd上找到。

### MINT: A wrapper to make multi-modal and multi-image AI models interactive 
[[arxiv](https://arxiv.org/abs/2401.12032)] [[cool](https://papers.cool/arxiv/2401.12032)] [[pdf](https://arxiv.org/pdf/2401.12032)]
> **Authors**: Jan Freyberg,Abhijit Guha Roy,Terry Spitz,Beverly Freeman,Mike Schaekermann,Patricia Strachan,Eva Schnider,Renee Wong,Dale R Webster,Alan Karthikesalingam,Yun Liu,Krishnamurthy Dvijotham,Umesh Telang
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: 15 pages, 7 figures
- **标题**: 薄荷：使多模式和多图像AI模型交互的包装器
- **领域**: 人机交互,人工智能
- **摘要**: 在诊断过程中，医生纳入了包括成像和病史在内的多模式信息 - 同样，医疗AI的发展也越来越多地成为多模式。在本文中，我们应对一个更加微妙的挑战：医生会采用有针对性的病史来获得最相关的信息；我们如何使AI做同样的事情？我们开发了一种名为Mint的包装方法（使您的模型交互式）自动确定哪些信息在每个步骤中最有价值的信息最有价值，并仅询问最有用的信息。我们证明了包裹皮肤病预测模型的薄荷的功效，其中多种模式深网使用了多个图像和一组可选答案（即标准元数据问题（即结构化病史）），以提供鉴别诊断。我们表明，薄荷可以确定是否需要元数据输入，如果是，下一步要问的问题。我们还证明，当收集多个图像时，薄荷可以识别其他图像是否有益，如果是，则可以捕获哪种类型的图像。我们表明，在保持预测性能的同时，薄荷分别将所需的元数据和图像输入的数量分别减少了82％和36.2％。使用现实世界AI皮肤病系统数据，我们表明需要更少的输入可以保留可能无法完成系统提交的用户并在没有诊断的情况下放下。定性示例表明，薄荷可以密切模仿临床工作流程的分步决策过程，以及对于直截了当的情况而言，这与更困难，模棱两可的情况有何不同。最后，我们证明了薄荷对不同的基础多模型分类器的鲁棒性，并且可以轻松地适应用户需求而无需重新训练。

### Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers 
[[arxiv](https://arxiv.org/abs/2401.16123)] [[cool](https://papers.cool/arxiv/2401.16123)] [[pdf](https://arxiv.org/pdf/2401.16123)]
> **Authors**: Amr Gomaa,Guillermo Reyes,Michael Feld,Antonio Krüger
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: Accepted for publication in the Proceedings of the 29th International Conference on Intelligent User Interfaces (IUI'24), March 18--21, 2024, in Greenville, SC, USA
- **标题**: 寻找更好的合适吗？增量学习的多模式对象参考框架适应各个驱动程序
- **领域**: 人机交互,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 汽车行业朝着自动化和半自动化的车辆的快速发展使传统的车辆互动方法（例如基于触摸的和语音命令系统）不足，无法扩大相关的相关任务，例如车辆之外的引用对象。因此，研究已转向手势输入（例如，手，凝视和头部姿势手势），作为驾驶过程中更合适的相互作用方式。但是，由于驾驶和个体变化的动态性质，驱动程序的手势输入性能存在显着差异。从理论上讲，这种固有的可变性可以通过大量数据驱动的机器学习模型来调节，但普遍的方法论倾向于受到约束的，经过训练的单个训练模型，用于对象引用。这些模型显示出有限的能力，可以不断适应单个驱动因素的不同行为以及各种驾驶场景的行为。为了解决这个问题，我们建议\ textit {icregress}，这是一种基于回归的新型增量学习方法，可适应不断变化的行为以及从事驾驶和引用对象双重任务的驾驶员的独特特征。我们建议使用连续的终身学习来增强驾驶员体验，安全性和便利性，为多模式手势界面提供一种更个性化和适应性的解决方案。使用引用用例的车式对象对我们的方法进行了评估，并强调了相对于单个训练的模型的增量学习模型的优越性，这些模型在各种驾驶员特征上，例如握手，驾驶经验和众多驾驶条件。最后，为了促进可重复性，缓解部署并促进进一步的研究，我们将方法作为开放式框架\ url {https://github.com/amrgomaealhady/icregress}。

### GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual AI for Smart Eyewear 
[[arxiv](https://arxiv.org/abs/2401.17217)] [[cool](https://papers.cool/arxiv/2401.17217)] [[pdf](https://arxiv.org/pdf/2401.17217)]
> **Authors**: Robert Konrad,Nitish Padmanaban,J. Gabriel Buckmaster,Kevin C. Boyle,Gordon Wetzstein
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: Project video: https://youtu.be/AuDFHHTK_m8
- **标题**: Gazegpt：使用凝视着智能眼镜的凝视范围的上下文AI增强人类能力
- **领域**: 人机交互,计算机视觉和模式识别
- **摘要**: 多模式大语模型（LMM）在世界知识和解决问题的能力中表现出色。通过使用面向世界的摄像头和上下文AI，新兴的智能配件旨在提供人类和LMM之间的无缝接口。但是，这些可穿戴的计算系统缺乏对用户注意的了解。我们将Gegegpt作为上下文AI的新用户交互范式介绍。 Gazegpt使用眼向跟踪来帮助LMM了解用户正在关注的面向世界相机中的哪个对象。使用广泛的用户评估，我们表明，这种凝视的机制比替代方案更快，更准确。它可以通过显着提高狗分类任务的准确性来增强人类能力；并且它始终被认为是上下文AI的头部或身体驱动选择机制更自然的。此外，我们原型的各种应用程序方案表明，作为未来AI驱动的个人助理的一部分，Gazegpt对用户可能具有重要价值。

## 信息检索(cs.IR:Information Retrieval)

该领域共有 5 篇论文

### GPT-4V(ision) is a Generalist Web Agent, if Grounded 
[[arxiv](https://arxiv.org/abs/2401.01614)] [[cool](https://papers.cool/arxiv/2401.01614)] [[pdf](https://arxiv.org/pdf/2401.01614)]
> **Authors**: Boyuan Zheng,Boyu Gou,Jihyung Kil,Huan Sun,Yu Su
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: GPT-4V（ISION）是通才的网络代理
- **领域**: 信息检索,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 最新的大型多模型模型（LMM）的开发，尤其是GPT-4V（ISION）和GEMINI，一直在迅速将多模型模型的能力边界扩展到传统任务之外，例如图像字幕和视觉询问答案。在这项工作中，我们探讨了像GPT-4V一样作为通才网络代理这样的LMM的潜力，可以按照自然语言说明在任何给定的网站上完成任务。我们提出了SEEACCT，这是一种通才的网络代理，它利用LMM的力量来集成视觉理解和在网络上的作用。我们评估了最近的Mind2Web基准。除了在缓存网站上进行标准离线评估外，我们还可以通过开发一个允许在实时网站上运行Web代理的工具来启用新的在线评估设置。我们表明，GPT-4V为Web代理提供了巨大的潜力 - 如果我们将其文本计划手动将其文本计划纳入网站上的操作，则可以成功完成51.1个任务。这实际上超过了仅针对Web代理的专门调整的诸如GPT-4或较小型号（Flan-T5和Blip-2）（Flan-T5和Blip-2）（Flan-T5和Blip-2）（例如GPT-4或较小的LLM）的表现。但是，基础仍然是一个重大挑战。现有的LMM接地策略（例如套装提示）对网络代理而言无效，我们在本文中制定的最佳接地策略既利用了HTML结构和视觉效果。然而，甲骨文接地仍然存在很大的差距，留出了足够的空间以进一步改进。所有代码，数据和评估工具均可在https://github.com/osu-nlp-group/seeact上找到。

### Generative Multi-Modal Knowledge Retrieval with Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.08206)] [[cool](https://papers.cool/arxiv/2401.08206)] [[pdf](https://arxiv.org/pdf/2401.08206)]
> **Authors**: Xinwei Long,Jiali Zeng,Fandong Meng,Zhiyuan Ma,Kaiyan Zhang,Bowen Zhou,Jie Zhou
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: Accepted to AAAI 2024
- **标题**: 具有大语言模型的生成多模式知识检索
- **领域**: 信息检索,计算语言学
- **摘要**: 具有多模式查询的知识检索在支持知识密集型多模式应用中起着至关重要的作用。但是，现有方法在其有效性和训练效率方面面临挑战，尤其是在培训和整合多个猎犬以处理多模式查询时。在本文中，我们为多模式知识检索提出了一个创新的端到端生成框架。我们的框架利用了一个事实，即大型语言模型（LLM）也可以有效地充当虚拟知识库，即使接受有限的数据培训。我们通过两步过程检索知识：1）生成与查询相关的知识线索，以及2）通过使用知识线索搜索数据库来获取相关文档。特别是，我们首先引入了一种对象感知的前缀调整技术，以指导多元透明的视觉学习。然后，我们使用LLM来捕获跨模式相互作用，将多层视觉特征与LLM的文本特征空间保持一致。随后，我们使用统一格式构建指导数据，用于模型培训。最后，我们提出了知识引导的生成策略，以在解码步骤中施加先前的约束，从而促进了独特的知识线索的产生。通过在三个基准测试的实验中，与强基础相比，所有评估指标的显着改善范围从3.0％到14.6％。

### A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems 
[[arxiv](https://arxiv.org/abs/2401.10279)] [[cool](https://papers.cool/arxiv/2401.10279)] [[pdf](https://arxiv.org/pdf/2401.10279)]
> **Authors**: Sean Tucker
> **First submission**: 2024-01-12
> **First announcement**: 2024-01-19
> **comment**: 20 pages, 11 figures, 3 appendices
- **标题**: 大语言模型中的地理空间位置嵌入方法的系统评价：空间AI系统的途径
- **领域**: 信息检索,人工智能,计算语言学
- **摘要**: 地理空间位置嵌入（GLE）有助于大型语言模型（LLM）同化并分析空间数据。地理空间人工智能（GEOAI）中GLE的出现是由于我们复杂的当代空间中更深层的地理空间意识以及LLM在生成AI中提取深度含义的成功而引起的。我们在Google Scholar，Science Direct和Arxiv搜索了有关地理空间位置嵌入和LLM的论文，并审查了旨在通过LLM获得更深入的空间“了解”的文章。我们筛选了304个标题，30个摘要和18个全文论文，这些论文揭示了四个GLE主题 - 实体位置嵌入（ELE），文档位置嵌入（DLE），序列位置嵌入（SLE）和令牌位置嵌入（TLE）。综合是表格和叙述，包括“空间”和“ LLM”之间的对话对话。尽管GLES通过叠加空间数据有助于空间理解，但他们强调需要促进空间方式和广义推理的复杂性。 GLES表示需要在模型体系结构中嵌入空间知识的空间基础/语言模型（SLM）。 SLM框架推进了空间人工智能系统（SPAIS），建立了映射到物理空间的空间向量空间（SVS）。由此产生的空间灌输语言模型是独一无二的。它同时代表了实际空间和具有AI能力的空间，为AI天然地理位置，分析和多模式为空间人工智能系统（SPAI）铺平了道路。

### A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model 
[[arxiv](https://arxiv.org/abs/2401.10934)] [[cool](https://papers.cool/arxiv/2401.10934)] [[pdf](https://arxiv.org/pdf/2401.10934)]
> **Authors**: Hao Yang,Jianxin Yuan,Shuai Yang,Linhe Xu,Shuo Yuan,Yifan Zeng
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 稳定扩散模型的新的创意生成管道，用于点击率
- **领域**: 信息检索,人工智能
- **摘要**: 在在线广告方案中，卖家通常会创建多种创意来提供全面的演示，这对于提出最吸引人的设计至关重要，以最大程度地提高点击率（CTR）。但是，卖家通常很难考虑用户对创意设计的偏好，与基于人工智能（AI）的方法相比，美学和数量相对较低。传统的基于AI的方法仍然面临着同样的问题，即在设计师的审美知识有限的同时不考虑用户信息。实际上，融合用户信息，生成的创意者可能会更具吸引力，因为不同的用户可能具有不同的偏好。为了优化结果，然后通过名为Creative排名模型的另一个模块对传统方法中的生成的创意者进行排名。考虑用户功能，排名模型可以预测每个创意的CTR分数。但是，以上两个阶段被视为两个不同的任务，并被分别优化。在本文中，我们提出了一条新的自动创意生成管道，以进行点击率（CG4CTR），目的是在创意生成阶段改善CTR。我们的贡献有4个部分：1）首先将稳定扩散模式用于在线广告领域的创意生成任务。提出了一个自循环生成管道，以确保训练的收敛性。 2）及时模型旨在为不同的用户群体生成个性化的创意，这可以进一步改善多样性和质量。 3）奖励模型全面考虑了图像和文本的多模式特征，以提高创意排名任务的有效性，并且在自我循环管道中也至关重要。 4）在线和离线实验中获得的重大好处验证了我们提出的方法的重要性。

### SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval 
[[arxiv](https://arxiv.org/abs/2401.13478)] [[cool](https://papers.cool/arxiv/2401.13478)] [[pdf](https://arxiv.org/pdf/2401.13478)]
> **Authors**: Siwei Wu,Yizhi Li,Kang Zhu,Ge Zhang,Yiming Liang,Kaijing Ma,Chenghao Xiao,Haoran Zhang,Bohao Yang,Wenhu Chen,Wenhao Huang,Noura Al Moubayed,Jie Fu,Chenghua Lin
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: camera-ready version for ACL 2024 Findings
- **标题**: Scimmir：基准测试科学多模式信息检索
- **领域**: 信息检索,计算语言学,计算机视觉和模式识别,多媒体
- **摘要**: 多模式信息检索（MMIR）是一个快速发展的领域，在该领域中，通过先进的表示学习和跨模式一致性研究取得了重大进展，尤其是在图像文本配对中。但是，当前用于评估科学领域中图像文本配对中MMIR性能的基准测试表明差距显着，其中以学术语言描述的图表和表图像通常不起作用。为了弥合这一差距，我们通过利用开放式纸张收集来提取与科学领域相关的数据来开发专门的科学MMIR（SCIMMIR）基准。该基准包括530k精心策划的图像文本对，并从科学文档中的图形和表中提取。我们进一步注释了图像文本对，并使用两级子集分类层次结构注释，以促进对基准的更全面的评估。我们对突出的多模式图像捕获和视觉语言模型（例如剪辑和BLIP）进行了零射和微调评估。我们的分析为MMIR提供了科学领域的关键见解，包括预训练和微调设置的影响以及视觉和文本编码器的影响。我们所有的数据和检查点均在https://github.com/wusiwei0410/scimmir上公开获取。

## 信息论(cs.IT:Information Theory)

该领域共有 1 篇论文

### HawkRover: An Autonomous mmWave Vehicular Communication Testbed with Multi-sensor Fusion and Deep Learning 
[[arxiv](https://arxiv.org/abs/2401.01822)] [[cool](https://papers.cool/arxiv/2401.01822)] [[pdf](https://arxiv.org/pdf/2401.01822)]
> **Authors**: Ethan Zhu,Haijian Sun,Mingyue Ji
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: submitted to IEEE conferences for future publications
- **标题**: Hawkrover：具有多传感器融合和深度学习的自动MMWave车辆通信测试
- **领域**: 信息论,计算机视觉和模式识别
- **摘要**: 连接和自动化的车辆（骑士）已成为一种可以改变我们日常生活的变革性技术。当前，毫米波（MMWAVE）频段被确定为有前途的CAV连接解决方​​案。尽管它可以提供很高的数据速率，但他们的实现面临许多挑战，例如在MMWave信号传播和移动性管理过程中衰减高。现有的解决方案必须启动试点信号以测量通道信息，然后应用信号处理以将最佳窄光束计算到接收器端以保证足够的信号功率。这个过程需要大量的开销和时间，因此不适合车辆。在这项研究中，我们提出了一个自主和低成本测试床，以收集广泛的共同置换的MMWave信号和其他传感器数据，例如LIDAR（光检测和射程），摄像机，超声波，超声波等，传统上是“自动化”的，以促进MMWave车辆通讯。直观地，这些传感器可以在车辆周围构建3D图，并且可以估算信号传播路径，从而通过飞行员信号消除过程。这种多模式数据融合与AI一起有望在``连接''研究中带来重大进展。

## 机器学习(cs.LG:Machine Learning)

该领域共有 41 篇论文

### Balanced Multi-modal Federated Learning via Cross-Modal Infiltration 
[[arxiv](https://arxiv.org/abs/2401.00894)] [[cool](https://papers.cool/arxiv/2401.00894)] [[pdf](https://arxiv.org/pdf/2401.00894)]
> **Authors**: Yunfeng Fan,Wenchao Xu,Haozhao Wang,Jiaqi Zhu,Song Guo
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: 10 pages, 5 figures 4 tables
- **标题**: 通过跨模式渗透平衡的多模式联合学习
- **领域**: 机器学习,计算机视觉和模式识别,多媒体
- **摘要**: 联合学习（FL）的基础是通过协作培训神经网络的分布式计算的进步，而无需公开客户的原始数据。当前的FL范式主要关注单模式数据，而从分布式多模式数据中利用知识仍然很大程度上没有探索。 Existing multimodal FL (MFL) solutions are mainly designed for statistical or modality heterogeneity from the input side, however, have yet to solve the fundamental issue,"modality imbalance", in distributed conditions, which can lead to inadequate information exploitation and heterogeneous knowledge aggregation on different modalities.In this paper, we propose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework that有效地通过从全球主导方式转移的知识转移来有效地减轻了模式失衡和知识异质性。为了避免由于仅模仿主要模式的行为而导致的弱模式中的信息丢失，我们设计了两项项目模块，以整合主要模态的知识，同时仍促进弱模态的局部特征剥削。此外，我们还引入了班级温度适应方案，以在不同类别上实现公平的表现。进行了广泛数据集的广泛实验，并为我们提供了令人欣慰的确认，以完全探索MFL中每种模式的信息的拟议框架。

### Overcome Modal Bias in Multi-modal Federated Learning via Balanced Modality Selection 
[[arxiv](https://arxiv.org/abs/2401.00403)] [[cool](https://papers.cool/arxiv/2401.00403)] [[pdf](https://arxiv.org/pdf/2401.00403)]
> **Authors**: Yunfeng Fan,Wenchao Xu,Haozhao Wang,Fushuo Huo,Jinyu Chen,Song Guo
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: Accepted by ECCV24, 23 pages
- **标题**: 通过平衡模态选择克服多模式联合学习中的模态偏见
- **领域**: 机器学习,计算机视觉和模式识别,多媒体
- **摘要**: 选择适当的客户参加每个联合学习（FL）回合对于有效利用广泛的分布式数据至关重要。现有的客户选择方法仅考虑分布式单模式数据的挖掘，但是，它们在多模式FL（MFL）中的有效性可能会降低，因为模态不平衡问题不仅会阻碍协作的本地培训，而且会导致严重的全球模态偏差。我们从经验上表明，具有某种单一模式的本地培训可能对全球模型造成更多的贡献，而不是所有局部模式的培训。为了有效利用分布式多种方式，我们提出了一个新型的MFL平衡模态选择框架（BMSFED）来克服模态偏差。一方面，我们在本地培训期间引入了模态增强损失，以减轻基于总体全球原型的本地失衡。另一方面，我们提出了旨在选择具有巨大多样性并同时实现全球模态平衡的局部模态子集的模态选择。我们对视听，彩色灰色和前后数据集进行了广泛的实验，展示了BMSF的优越性及其在多模式数据开发中的有效性。

### Natural Language Processing and Multimodal Stock Price Prediction 
[[arxiv](https://arxiv.org/abs/2401.01487)] [[cool](https://papers.cool/arxiv/2401.01487)] [[pdf](https://arxiv.org/pdf/2401.01487)]
> **Authors**: Kevin Taylor,Jerry Ng
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-03
> **comment**: 13 pages, 13 figures
- **标题**: 自然语言处理和多模式股票价格预测
- **领域**: 机器学习,计算语言学
- **摘要**: 在财务决策领域，预测股票价格是关键的。人工智能技术，例如长期短期存储网络（LSTMS），支持矢量机（SVM）和自然语言处理（NLP）模型，通常用于预测上述价格。与传统的原始货币价值使用相比，本文将股票百分比变化作为培训数据，重点是分析公开发布的新闻文章。百分比变化的选择旨在为模型提供有关价格波动的重要性以及对特定股票的总体价格变化影响的背景。该研究采用专业的BERT自然语言处理模型来预测股票价格趋势，并特别强调各种数据模式。结果展示了具有小型自然语言处理模型的此类策略的能力，以准确预测整体股票趋势，并强调某些数据功能和特定于部门的数据的有效性。

### A Physics-guided Generative AI Toolkit for Geophysical Monitoring 
[[arxiv](https://arxiv.org/abs/2401.03131)] [[cool](https://papers.cool/arxiv/2401.03131)] [[pdf](https://arxiv.org/pdf/2401.03131)]
> **Authors**: Junhuan Yang,Hanchen Wang,Yi Sheng,Youzuo Lin,Lei Yang
> **First submission**: 2024-01-06
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 物理引导的生成AI工具包，用于地球物理监测
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,信号处理,地球物理学
- **摘要**: 全波形反演（FWI）在地球科学中起着至关重要的作用来探索地下。它利用地震波来成像地下速度图。随着机器学习（ML）技术的发展，使用ML进行FWI任务的数据驱动方法已经出现，与传统的基于物理学的方法相比，具有增强的准确性和降低的计算成本。但是，地球科学中的一个普遍挑战，即无特点的数据，严重限制了ML的效力。在模型修剪期间，问题变得更糟，这是环境复杂性而导致的地球科学的一步。为了解决这个问题，我们介绍了Edgeo Toolkit，该工具包采用以物理原理为指导的基于扩散的模型来生成高保真速度图。该工具包使用声波方程来生成相应的地震波形数据，从而促进了修剪的ML模型的微调。我们的结果表明，在各种修剪比率上，MAE和MSE的SSIM得分有了显着改善。值得注意的是，使用Edgeo生成的数据微调的ML模型可产生速度图的较高质量，尤其是在表示无特点的特征，表现优于其他现有方法。

### Multi-Modal Federated Learning for Cancer Staging over Non-IID Datasets with Unbalanced Modalities 
[[arxiv](https://arxiv.org/abs/2401.03609)] [[cool](https://papers.cool/arxiv/2401.03609)] [[pdf](https://arxiv.org/pdf/2401.03609)]
> **Authors**: Kasra Borazjani,Naji Khosravan,Leslie Ying,Seyyedali Hosseinalipour
> **First submission**: 2024-01-07
> **First announcement**: 2024-01-08
> **comment**: Published in IEEE Transactions on Medical Imaging (TMI), DOI: https://doi.org/10.1109/TMI.2024.3450855
- **标题**: 多模式联合学习，用于在非IID数据集上进行癌症分期
- **领域**: 机器学习,人工智能
- **摘要**: 通过医学图像分析使用机器学习（ML）进行癌症分期已引起了医学学科的重大兴趣。当伴随着创新的联邦学习（FL）框架时，ML技术可以进一步克服与患者数据暴露有关的隐私问题。鉴于患者记录中经常存在各种数据模式，因此在多模式学习框架中利用FL为癌症分期提供了相当大的希望。但是，多模式FL的现有作品通常认为所有数据收集机构都可以访问所有数据模式。这种过度简化的方法忽略了仅访问系统内数据模式的机构。在这项工作中，我们介绍了一种新型的FL体系结构，旨在不仅适合数据样本的异质性，还可以容纳跨机构数据模式的固有异质性/不均匀性/不均匀性。我们阐明了与FL系统内不同数据模式相关的不同收敛速度相关的挑战。随后，我们提出了一种解决方案来解决这些挑战，通过设计分布式梯度混合和邻近感知的客户加权策略为多模式FL量身定制。为了显示我们方法的优越性，我们使用癌症基因组计划（TCGA）数据库进行实验，以考虑不同的癌症类型和三种数据方式：mRNA序列，组织病理学图像数据和临床信息。我们的结果进一步揭示了基于类型与类型的异质性对模型性能的影响和严重性，从而扩大了多模式FL文献中数据异质性概念的观点。

### Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting 
[[arxiv](https://arxiv.org/abs/2401.03397)] [[cool](https://papers.cool/arxiv/2401.03397)] [[pdf](https://arxiv.org/pdf/2401.03397)]
> **Authors**: Sina Ehsani,Elina Sergeeva,Wendy Murdy,Benjamin Fox
> **First submission**: 2024-01-07
> **First announcement**: 2024-01-08
> **comment**: 9 pages, 6 figures, to be published
- **标题**: 预测天空：飞行级别乘客流量预测的新型模型
- **领域**: 机器学习,人工智能,应用领域
- **摘要**: 准确预测航班乘客流量在航空公司运营中至关重要，从而影响了从定价到路线优化的关键决策。这项研究介绍了一种新型的多模式深度学习方法，以预测飞行级别的客运流量的挑战，与传统模型相比，准确的准确性提高了。利用美国航空公司的广泛数据集，我们的模型摄入了历史交通数据，票价关闭信息和季节性属性，特定于每次飞行。我们提出的神经网络集成了复发性神经网络（RNN）和卷积神经网络（CNN）的优势，从而利用数据中的时间模式和空间关系来增强预测性能。对于我们的模型的成功至关重要，是一种全面的数据处理策略。我们构建3D张量来表示数据，应用仔细掩盖策略来反映现实世界动态，并采用数据增强技术来丰富我们培训集的多样性。结果在结果中证明了我们方法的功效：与传统基准相比，我们的模型表明，平均误差（MSE）的33 \％改善。因此，这项研究突出了深度学习技术和细致数据处理在推进飞行流量预测领域的重要潜力。

### Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues 
[[arxiv](https://arxiv.org/abs/2401.03171)] [[cool](https://papers.cool/arxiv/2401.03171)] [[pdf](https://arxiv.org/pdf/2401.03171)]
> **Authors**: Qiang Li,Yufeng Wu,Zhan Xu,Hefeng Zhou
> **First submission**: 2024-01-06
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 基于人口普查调查和一般生活问题的青少年抑郁症风险预测的探索
- **领域**: 机器学习,人工智能,计算机与社会
- **摘要**: 在当代社会中，生活和工作的不断升级使心理障碍引起了现代健康问题的最前沿，这一问题已被19. COVID-19- 19岁大流行进一步强调。青少年抑郁症的普遍性正在稳步增加，依靠量表或访谈的传统诊断方法尤其不足以发现年轻人的抑郁症。解决这些挑战时，已经出现了许多基于AI的基于AI的方法来协助诊断心理健康问题。但是，这些方法中的大多数围绕具有尺度的基本问题或使用多模式识别等多模式的方法。基于日常习惯和行为的抑郁症风险的诊断仅限于小规模的定性研究。我们的研究利用青少年人口普查数据来预测抑郁症的风险，重点是儿童抑郁症的经历及其日常生活情况。我们引入了一种方法，用于管理严重不平衡的高维数据和针对数据结构特征量身定制的自适应预测方法。此外，我们为自动在线学习和数据更新提供了基于云的体系结构。这项研究利用了2020年至2022年公开可用的NSCH青年人口普查数据，其中包括近15万个数据条目。我们进行了基本数据分析和预测实验，证明了对标准机器学习和深度学习算法的显着改善。这肯定了我们的数据处理方法在处理不平衡医疗数据方面的广泛适用性。考虑到更广泛的用户需求，我们的研究与典型的预测方法研究不同，提出了一种全面的建筑解决方案。

### AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis 
[[arxiv](https://arxiv.org/abs/2401.03040)] [[cool](https://papers.cool/arxiv/2401.03040)] [[pdf](https://arxiv.org/pdf/2401.03040)]
> **Authors**: Kebin Wu,Wenbin Li,Xiaofei Xiao
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: 8 pages, 2 figures
- **标题**: 事故网：交通事故分析的大型多模式基础模型
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,分布式、并行和集群计算
- **摘要**: 交通事故分析是提高公共安全和制定道路法规的关键。传统方法虽然广泛使用，但通常受到手动分析过程，主观决策，单模式输出以及与敏感数据相关的隐私问题的限制。本文介绍了交通事故分析的基础模型，该概念结合了多模式输入数据，以自动重建事故过程视频，并使用动态详细信息重建事故过程，此外，还提供了具有多模式输出的多任务分析。意外设备的设计具有多模式提示，并提供了以任务为导向的适应性的反馈，混合培训模式以利用标记和未标记的数据以及用于数据隐私的Edge-Cloud Split Configuration。为了充分实现该模型的功能，我们提出了一些研究机会。本文是填补传统交通事故分析方法的差距的垫脚石，并吸引了研究社区的关注，以自动，客观和隐私的交通事故分析。

### H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses 
[[arxiv](https://arxiv.org/abs/2401.02905)] [[cool](https://papers.cool/arxiv/2401.02905)] [[pdf](https://arxiv.org/pdf/2401.02905)]
> **Authors**: Haidong Gu,Nathan Gaw,Yinan Wang,Chancellor Johnstone,Christine Beauchene,Sophia Yuditskaya,Hrishikesh Rao,Chun-An Chou
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: Paper accepted in Human-Centric Representation Learning workshop at AAAI 2024 (https://hcrl-workshop.github.io/2024/)
- **标题**: H2G2-NET：用于发现多模式生理反应的分层异构图生成网络框架
- **领域**: 机器学习,人工智能,信号处理
- **摘要**: 使用多模式生理信号发现人类的认知和情绪状态，在各种研究应用中引起了人们的关注。人体的生理反应受人类认知的影响，通常用于分析认知态。从网络科学的角度来看，这些异质生理方式在图形结构中的相互作用可能会提供深刻的信息，以支持认知状态的预测。但是，没有线索可以在异质方式之间获得确切的连通性，并且存在亚模式的层次结构。现有的图形神经网络旨在学习具有预定义图结构的非层次同质图；他们没有从没有预定义的图结构的情况下从分层的多模式生理数据中学习。为此，我们提出了一个层次的异构图生成网络（H2G2-NET），该网络自动以无域知识学习图形结构，以及以端到端方式上的层次异质图上的强大表示。我们在Cogpilot数据集上验证了由多模式生理信号组成的建议方法。广泛的实验表明，我们提出的方法的预测准确性优于最先进的GNN。

### Enhancing Acute Kidney Injury Prediction through Integration of Drug Features in Intensive Care Units 
[[arxiv](https://arxiv.org/abs/2401.04368)] [[cool](https://papers.cool/arxiv/2401.04368)] [[pdf](https://arxiv.org/pdf/2401.04368)]
> **Authors**: Gabriel D. M. Manalu,Mulomba Mukendi Christian,Songhee You,Hyebong Choi
> **First submission**: 2024-01-09
> **First announcement**: 2024-01-10
> **comment**: 9 pages, 2 tables
- **标题**: 通过在重症监护病房中整合药物特征来增强急性肾脏损伤预测
- **领域**: 机器学习
- **摘要**: 急性肾损伤（AKI）预测与肾毒性药物或对肾功能不利的药物之间的关系是在重症监护环境中尚待探讨的。研究差距的一个因素是对重症监护病房（ICU）环境中药物形态的有限研究，这是因为将处方数据处理到相应的药物表示中的挑战，并且缺乏对这些药物表示的全面理解。这项研究通过提出一种新型方法来解决这一差距，该方法利用患者处方数据作为一种方法来改善AKI预测的现有模型。我们基于电子健康记录（EHR）数据的研究，提取相关的患者处方信息，并将其转换为我们研究的选定药物表示，即扩展连接性指纹（ECFP）。此外，我们采用了一种独特的多模式方法，开发机器学习模型和适用于临床药物表示的1D卷积神经网络（CNN），建立了一项程序，任何先前预测AKI的研究尚未使用。这些发现通过整合药物嵌入和其他患者队列特征来展示AKI预测的显着改善。 By using drug features represented as ECFP molecular fingerprints along with common cohort features such as demographics and lab test values, we achieved a considerable improvement in model performance for the AKI prediction task over the baseline model which does not include the drug representations as features, indicating that our distinct approach enhances existing baseline techniques and highlights the relevance of drug data in predicting AKI in the ICU setting

### VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition 
[[arxiv](https://arxiv.org/abs/2401.05531)] [[cool](https://papers.cool/arxiv/2401.05531)] [[pdf](https://arxiv.org/pdf/2401.05531)]
> **Authors**: John Fischer,Marko Orescanin,Eric Eckstrand
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-11
> **comment**: Published in IEEE Access
- **标题**: vi-pann：利用转移学习和不确定性感知的变异推断，以改善音频模式识别的概括
- **领域**: 机器学习,声音,音频和语音处理
- **摘要**: 转移学习（TL）是一种越来越流行的训练深度学习模型（DL）模型，它通过培训培训各种大规模数据集的基础模型来利用知识，以用于在下游任务上使用较少的域或特定于任务的数据。文献富含TL技术和应用。但是，大部分研究都利用了确定性的DL模型，这些模型通常是未校准的，并且缺乏传达预测中认知（模型）不确定性的能力。与确定性的对应物不同，贝叶斯DL（BDL）模型通常经过良好校准，为预测提供了认知不确定性的访问，并且能够实现竞争性的预测性能。在这项研究中，我们提出了变异推断预训练的音频神经网络（VI-PANNS）。 vi-panns是流行的Resnet-54体系结构的变异推理变体，该变体在Audioset（一个大规模的音频事件检测数据集）上进行了预训练。我们使用ESC-50，URBANSOUND8K和DCASE2013数据集将知识从VI-PANN转移到其他下游声学分类任务时，评估了所得不确定性的质量。我们首次证明，可以将校准的不确定性信息以及从上游任务传输校准的不确定性信息，以增强模型执行下游任务的能力。

### CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification 
[[arxiv](https://arxiv.org/abs/2401.06194)] [[cool](https://papers.cool/arxiv/2401.06194)] [[pdf](https://arxiv.org/pdf/2401.06194)]
> **Authors**: Shubham Gupta,Nandini Saini,Suman Kundu,Debasis Das
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-12
> **comment**: No comments
- **标题**: CRISISKAN：危机事件分类的知识注入知识和可解释的多模式关注网络
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 社交媒体的普遍使用已成为实时信息（例如图像，文本或两者）的新兴来源，以识别各种事件。尽管图像和基于文本的事件分类的迅速增长，但最新的（SOTA）模型发现，由于编码不一致而导致的图像特征和文本模式之间的语义差距构成挑战。同样，模型的黑盒本质无法解释模型在高风险情况下（例如大流行）中建立信任的结果。此外，在社交媒体帖子上施加的一词限制可能会对特定事件引起偏见。为了解决这些问题，我们提出了Crissiskan，这是一个新颖的知识融合且可解释的多模式关注网络，需要与Wikipedia的外部知识结合图像和文本，以对危机事件进行分类。为了丰富对文本信息的特定于上下文的理解，我们使用建议的Wiki提取算法整合了Wikipedia知识。随之而来的是，在集成视觉和文本数据时，还实施了一个引导的跨意义模块，以填补语义差距。为了确保可靠性，我们采用了一种特定于模型的方法，称为梯度加权类激活映射（GRAD-CAM），该方法对所提出模型的预测提供了强有力的解释。关于危机数据集进行的综合实验在各种特定于危机的任务和设置中产生了深入的分析。结果，Crisiskan的表现优于现有的SOTA方法，并在可解释的多模式事件分类的领域中提供了一种新颖的看法。

### An attempt to generate new bridge types from latent space of PixelCNN 
[[arxiv](https://arxiv.org/abs/2401.05964)] [[cool](https://papers.cool/arxiv/2401.05964)] [[pdf](https://arxiv.org/pdf/2401.05964)]
> **Authors**: Hongjun Zhang
> **First submission**: 2024-01-11
> **First announcement**: 2024-01-12
> **comment**: 7 pages, 8 figures
- **标题**: 尝试从PixelCNN的潜在空间生成新的桥梁类型
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 尝试使用生成人工智能技术生成新的桥梁类型。基于Python编程语言，TensorFlow和Keras Deep Learning Platform Platform框架，使用三跨横梁桥，拱形桥和悬架桥的对称结构化图像数据集，构建和训练。该模型可以捕获图像的统计结构，并在给出上一个像素时计算下一个像素的概率分布。从获得的潜在空间采样中，可以生成与训练数据集不同的新桥梁类型。 Pixelcnn可以根据人类原始桥梁的类型有机地结合不同的结构组件，从而创建具有一定程度的人类原始能力的新桥梁类型。自回归模型无法理解序列的含义，而多模式模型则结合了回归和自回归模型以了解序列。多模型模型应该是未来实现人工通用智能的方式。

### Domain Adaptation for Sustainable Soil Management using Causal and Contrastive Constraint Minimization 
[[arxiv](https://arxiv.org/abs/2401.07175)] [[cool](https://papers.cool/arxiv/2401.07175)] [[pdf](https://arxiv.org/pdf/2401.07175)]
> **Authors**: Somya Sharma,Swati Sharma,Rafael Padilha,Emre Kiciman,Ranveer Chandra
> **First submission**: 2024-01-13
> **First announcement**: 2024-01-15
> **comment**: Neurips workshop on Tackling Climate Change 2023
- **标题**: 使用因果和对比的约束最小化可持续土壤管理的域适应
- **领域**: 机器学习
- **摘要**: 监测有机物是维持土壤健康的关键，并可以帮助可持续的土壤管理实践。虽然基于传感器的土壤信息提供了更高的前景和对有机物变化的可靠见解，但采样和测量传感器数据的成本较高。我们提出了一个多模式，可扩展的框架，可以通过遥感数据估算有机物，这是一个更容易获得的数据源，同时利用稀疏的土壤信息来改善概括。使用传感器数据，我们保留了传感器属性和有机物之间的基本因果关系。同时，我们利用数据中的固有结构，并使用对比度学习训练模型以区分域之间。这种因果关系最小化可确保改善对其他领域的概括和适应。我们还通过识别对于改善概括重要的属性来阐明框架的解释性。确定影响有机物的这些关键土壤属性将有助于标准化数据收集工作的努力。

### AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media 
[[arxiv](https://arxiv.org/abs/2401.08825)] [[cool](https://papers.cool/arxiv/2401.08825)] [[pdf](https://arxiv.org/pdf/2401.08825)]
> **Authors**: Alessandro Gambetti,Qiwei Han
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: Aigen-Foodreview：社交媒体上机器生成的餐厅评论和图像的多模式数据集
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 用户生成内容（UGC）形式的在线评论显着影响消费者的决策。但是，不仅人类虚假内容，而且机器生成的内容的普遍问题挑战了UGC的可靠性。大型语言模型（LLM）的最新进展可能为制造不可分割的假产生内容的方式铺平了道路，成本要低得多。利用OpenAI的GPT-4-Turbo和Dall-E-2型号，我们制作了Aigen-Foodeview，这是一个由20,144个餐厅评论图像对的多模式数据集，分为真实和机器生成。我们探索单峰和多模式检测模型，使用Flava达到99.80％的多模式精度。我们使用可读性和摄影理论的属性分别为评论和图像进行评分和图像，以可扩展和可解释的检测模型中的手工制作的功能证明了它们的实用性，并且具有可比的性能。该论文通过开源数据集并发布伪造的审核探测器来做出贡献，建议其在单峰和多模式的伪造审查检测任务中使用，并评估合成数据与真实数据中的语言和视觉特征。

### MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor 
[[arxiv](https://arxiv.org/abs/2401.08619)] [[cool](https://papers.cool/arxiv/2401.08619)] [[pdf](https://arxiv.org/pdf/2401.08619)]
> **Authors**: Etienne Goffinet,Raghvendra Mall,Ankita Singh,Rahul Kaushik,Filippo Castiglione
> **First submission**: 2023-12-05
> **First announcement**: 2024-01-17
> **comment**: Patent pending: U.S. Provisional Application No. 63/603,952
- **标题**: Mate-Pred：基于多模式注意的TCR  -  ePitope相互作用预测指标
- **领域**: 机器学习,人工智能
- **摘要**: T细胞受体和表位之间的准确结合亲和力预测果断地有助于制定成功的免疫疗法策略。一些最先进的计算方法通过整合进化特征来实现深度学习技术，以将细胞受体和表位序列的氨基酸残基转换为数值值，而其他一些方法则采用预训练的语言模型来总结氨基酸残基在氨基酸残基水平上的嵌入量以获得序列智能表示。在这里，我们提出了一种高度可靠的新颖方法Mate-Pred，该方法对T细胞受体和表位结合亲和力的基于多模式注意的预测进行了多模式。比较伴侣预示并与其他深度学习模型进行了比较，这些模型利用了T细胞受体和表位的多模式表示。在提出的方法中，蛋白质的文本表示嵌入了预先训练的双向编码器模型，并与另外两种方式结合在一起：a）一组全面的选定物理化学特性； b）预测序列中氨基酸残基之间3D距离的接触图。 Mate-Pred证明了多模型模型在实现最新性能中的潜力（与基准相比，+8.4 \％MCC，+5.5 \％AUC）并有效地从氨基酸残基中捕获上下文，物理化学和结构信息。 Mate-Pred的表现将其潜在应用于各种药物发现制度。

### Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data 
[[arxiv](https://arxiv.org/abs/2401.08567)] [[cool](https://papers.cool/arxiv/2401.08567)] [[pdf](https://arxiv.org/pdf/2401.08567)]
> **Authors**: Yuhui Zhang,Elaine Sui,Serena Yeung-Levy
> **First submission**: 2024-01-16
> **First announcement**: 2024-01-17
> **comment**: Published at ICLR 2024
- **标题**: 连接，崩溃，腐败：使用单模式数据学习跨模式任务
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 由于配对的多模式数据有限，建筑物跨模式应用程序具有挑战性。最近的工作表明，利用预训练的多模式对比表示空间可以从单模式数据中学到交叉模式任务。这是基于以下假设：对比优化使不同模态的嵌入方式可互换。但是，由于存在模态差距的多模式对比空间的几何形状，该假设的探索尚未探索。在我们的研究中，我们提供了对该空间几何形状的理论解释，并引入了三步方法$ c^3 $（连接，崩溃，腐败），以弥合模态差距，增强了嵌入式的互换性。我们的$ c^3 $方法显着改善了从单模式数据中的跨模式学习，从而在零摄像图像 /音频 /视频字幕和文本对象生成上实现最先进的结果。

### A Survey of Resource-efficient LLM and Multimodal Foundation Models 
[[arxiv](https://arxiv.org/abs/2401.08092)] [[cool](https://papers.cool/arxiv/2401.08092)] [[pdf](https://arxiv.org/pdf/2401.08092)]
> **Authors**: Mengwei Xu,Wangsong Yin,Dongqi Cai,Rongjie Yi,Daliang Xu,Qipeng Wang,Bingyang Wu,Yihao Zhao,Chen Yang,Shihe Wang,Qiyang Zhang,Zhenyan Lu,Li Zhang,Shangguang Wang,Yuanchun Li,Yunxin Liu,Xin Jin,Xuanzhe Liu
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: 资源有效的LLM和多模式基础模型的调查
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **摘要**: 大型基础模型，包括大型语言模型（LLM），视觉变压器（VIT），扩散和基于LLM的多模式模型，正在彻底改变整个机器学习生命周期，从培训到部署。但是，这些模型提供的多功能性和性能方面的重大进步在硬件资源方面取得了巨大成本。为了以可扩展和环境可持续的方式支持这些大型模型的增长，人们非常重视制定资源有效的策略。这项调查深入研究了此类研究的重要性，研究了算法和系统方面。它提供了从现有文献中收集的全面分析和有价值的见解，其中包括从最先进的模型架构以及培训/服务算法到实用的系统设计和实现的广泛主题。这项调查的目的是对当前方法如何应对大型基础模型带来的资源挑战，并有可能激发该领域的未来突破。

### eipy: An Open-Source Python Package for Multi-modal Data Integration using Heterogeneous Ensembles 
[[arxiv](https://arxiv.org/abs/2401.09582)] [[cool](https://papers.cool/arxiv/2401.09582)] [[pdf](https://arxiv.org/pdf/2401.09582)]
> **Authors**: Jamie J. R. Bennett,Aviad Susman,Yan Chak Li,Gaurav Pandey
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: EIPY：使用异构合奏的多模式数据集成的开源Python软件包
- **领域**: 机器学习
- **摘要**: 在本文中，我们介绍了EIPY  - 一个开源Python软件包，用于开发有效的多模式异质合奏以进行分类。 EIPY同时提供了一个严格且用户友好的框架，可通过系统地使用嵌套的交叉验证来系统地评估其性能，从而比较和选择表现最佳的多模式数据集成和预测建模方法。该软件包旨在利用类似Scikit-Learn的估计器作为组件来构建多模式预测模型。在https://eipy.readthedocs.io上维护了EIPY的最新用户指南，包括API参考和教程。该项目的主要存储库可以在https://github.com/gauravpandeylab/eipy的GitHub上找到。

### Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning 
[[arxiv](https://arxiv.org/abs/2401.09517)] [[cool](https://papers.cool/arxiv/2401.09517)] [[pdf](https://arxiv.org/pdf/2401.09517)]
> **Authors**: Junhao Wen,Mathilde Antoniades,Zhijian Yang,Gyujoon Hwang,Ioanna Skampardoni,Rongguang Wang,Christos Davatzikos
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 尺寸神经影像学内表型：通过机器学习的疾病异质性的神经生物学表示
- **领域**: 机器学习,图像和视频处理,定量方法
- **摘要**: 机器学习已越来越多地用于获得个性化的神经影像学特征，以诊断，预后和对神经精神和神经退行性疾病的治疗的反应。因此，它通过鉴定疾病亚型在各种大脑表型措施中存在显着差异，从而有助于更好地理解疾病异质性。 In this review, we first present a systematic literature overview of studies using machine learning and multimodal MRI to unravel disease heterogeneity in various neuropsychiatric and neurodegenerative disorders, including Alzheimer disease, schizophrenia, major depressive disorder, autism spectrum disorder, multiple sclerosis, as well as their potential in transdiagnostic settings.随后，我们总结了相关的机器学习方法，并讨论了一种新兴范式，我们称之为尺寸神经影像型内表型（DNE）。 DNE将神经精神和神经退行性疾病的神经生物学异质性分解为低维但有益的，定量的大脑表型表述，是一种可靠的中间表型（即内型型），主要反映了潜在的遗传学和遗传学。最后，我们讨论当前发现的潜在临床意义，并设想未来的研究途径。

### Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer 
[[arxiv](https://arxiv.org/abs/2401.09181)] [[cool](https://papers.cool/arxiv/2401.09181)] [[pdf](https://arxiv.org/pdf/2401.09181)]
> **Authors**: Junhao Zheng,Qianli Ma,Zhen Liu,Binquan Wu,Huawen Feng
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 超越反遗嘱：多模式持续指导调谐，正向转移正向转移
- **领域**: 机器学习
- **摘要**: 多模式连续教学调整（MCIT）使多模式大语模型（MLLM）能够不断地满足新兴要求而无需昂贵的再培训。 MCIT面临两个主要障碍：灾难性的遗忘（遗忘了旧知识）和负面转移（在未来任务的执行情况下降低）。尽管现有的方法极大地缓解了灾难性的遗忘，但它们仍然遭受负面转移的损失。我们通过在输入嵌入中执行奇异值分解（SVD），发现不同输入嵌入的巨大差异。这种差异导致模型学习旧任务和预训练的任务无关的信息，从而导致灾难性的遗忘和负面的远期转移。为了解决这些问题，我们提出了一种基于正向转移的正向转移（FWD-Prompt）的及时调整，这是一种基于及时的方法，该方法将迅速梯度投射到残留空间，以最大程度地减少任务之间的干扰以及预先训练的子空间，以重用预培训的知识。我们的实验表明，FWD-Prompt可以在更新参数和不需要旧样本的同时实现最先进的性能。我们的研究阐明了根据指令调整范式不断适应新任务的潜力，并鼓励未来的研究探索MCIT。

### Developing an AI-based Integrated System for Bee Health Evaluation 
[[arxiv](https://arxiv.org/abs/2401.09988)] [[cool](https://papers.cool/arxiv/2401.09988)] [[pdf](https://arxiv.org/pdf/2401.09988)]
> **Authors**: Andrew Liang
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: No comments
- **标题**: 开发基于AI的集成系统，用于蜜蜂健康评估
- **领域**: 机器学习,计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 蜜蜂在世界食品供应中授粉约占三分之一，但由于包括农药和害虫在内的多个因素，在过去的十年中，蜜蜂菌落震惊地下降了近40％。监测蜂箱的传统方法，例如人类检查，是主观的，破坏性的和耗时的。为了克服这些局限性，人工智能已被用来评估蜂巢健康。但是，以前的研究缺乏端到端的解决方案，主要依赖于单个来源的数据，即蜜蜂图像或声音。这项研究介绍了一个由Bee对象检测和健康评估组成的综合系统。此外，它利用视觉和音频信号的组合来分析蜜蜂行为。开发了基于注意力的多模式神经网络（AMNN），以适应各种信号的关键特征，以进行准确的蜜蜂健康评估。 AMNN的总体准确性为92.61％，超过了八个现有的单信号卷积神经网络和复发性神经网络。在保持有效的处理时间的同时，它的表现优于最佳基于图像的模型，而基于声音的最高模型则优于13.98％。此外，它提高了预测鲁棒性，在所有四种评估的健康状况中，F1得分高于90％。该研究还表明，音频信号比评估蜜蜂健康的图像更可靠。通过将AMNN与图像和声音数据无缝整合到全面的蜜蜂健康监测系统中，该方法为早期发现蜜蜂疾病和蜜蜂菌落保存提供了更有效和非侵入性的解决方案。

### Projected Belief Networks With Discriminative Alignment for Acoustic Event Classification: Rivaling State of the Art CNNs 
[[arxiv](https://arxiv.org/abs/2401.11199)] [[cool](https://papers.cool/arxiv/2401.11199)] [[pdf](https://arxiv.org/pdf/2401.11199)]
> **Authors**: Paul M. Baggenstoss,Kevin Wilkinghoff,Felix Govaers,Frank Kurth
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: 15 Pages. Submitted to IEEE-TNNLS
- **标题**: 具有歧视性一致性的预测信念网络，以进行声学事件分类：与艺术的竞争状态CNNS
- **领域**: 机器学习,声音,音频和语音处理
- **摘要**: 预测的信念网络（PBN）是一个基于馈送前向神经网络（FFNN）的生成随机网络。生成功能通过通过FFNN“备份”来运行。 PBN是两个网络，一个在向前方向运行的FFNN，并且是向后方向运行的生成网络。两个网络基于相同的参数集，具有自身的成本功能，并且可以单独或共同培训。因此，PBN有可能具有歧视性和生成性分类器的最佳品质。为了实现这一潜力，对每个类别进行了单独的PBN训练，最大程度地提高了给定类别的生成似然函数，同时最大程度地降低了FFNN对“所有其他类”的判别成本。该技术称为判别一致性（PBN-DA），将似然函数的轮廓与决策界限保持一致，并获得了极大的分类性能，与最先进的歧视性网络相媲美。可以使用隐藏的Markov模型（HMM）作为PBN的组成部分进一步改进该方法，称为PBN-DA-HMM。本文提供了PBN，PBN-DA和PBN-DA-HMM的全面处理。此外，还提供了两个新的分类实验的结果。第一个实验使用空气声事件，第二个实验使用了由海洋哺乳动物调用组成的水下声学数据。在这两个实验中，PBN-DA-HMM作为CNN的状态均具有可比性或更好的性能，并且与CNN结合使用时达到了两个误差降低的因子。

### Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation 
[[arxiv](https://arxiv.org/abs/2401.11648)] [[cool](https://papers.cool/arxiv/2401.11648)] [[pdf](https://arxiv.org/pdf/2401.11648)]
> **Authors**: Heejoon Koo
> **First submission**: 2024-01-21
> **First announcement**: 2024-01-22
> **comment**: Accepted to EACL 2024 (The 18th Conference of the European Chapter of the Association for Computational Linguistics)
- **标题**: 接下来访问以医学代码为中心的多模式对比EHR建模的诊断预测与层次正则化
- **领域**: 机器学习,人工智能,信息检索
- **摘要**: 使用电子健康记录（EHR）预测下一次访问诊断是医疗保健的重要任务，对于为医疗保健提供者和患者设计积极的未来计划至关重要。尽管如此，许多先前的研究还没有充分解决EHR数据固有的异质和层次特征，不可避免地会导致次优性能。为此，我们提出了Necho，这是一种新型的以医学代码为中心的多模式对比性EHR学习框架，并具有层次正则化。首先，我们使用量身定制的网络设计和一对双峰对比损失整合了包括医疗法规，人口统计信息和临床注释的多方面信息，所有这些信息都围绕医疗代码表示。我们还使用医学本体论中的父母级别信息来确定特定于模式的编码器，以学习EHR数据的层次结构。一系列关于模拟III数据的实验证明了我们方法的有效性。

### Automated Fusion of Multimodal Electronic Health Records for Better Medical Predictions 
[[arxiv](https://arxiv.org/abs/2401.11252)] [[cool](https://papers.cool/arxiv/2401.11252)] [[pdf](https://arxiv.org/pdf/2401.11252)]
> **Authors**: Suhan Cui,Jiaqi Wang,Yuan Zhong,Han Liu,Ting Wang,Fenglong Ma
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: Accepted by SDM 2024
- **标题**: 多模式电子健康记录的自动融合以获得更好的医疗预测
- **领域**: 机器学习,人工智能
- **摘要**: 医疗机构的电子健康记录（EHR）系统的广泛采用已经产生了大量的医疗数据，从而通过深度学习技术为改善医疗服务提供了重要的机会。但是，现实世界中EHR数据中的复杂和多样化的方式和特征结构对深度学习模型设计构成了巨大的挑战。为了应对EHR数据中的多模式挑战，当前方法主要依赖于基于直觉和经验经验的手工制作的模型体系结构，从而导致次优模型体系结构和有限的性能。因此，为了自动化用于采矿EHR数据的模型设计过程，我们提出了一种名为AUTOFM的新型神经体系结构搜索（NAS）框架，该框架可以自动搜索用于编码各种输入方式和融合策略的最佳模型体系结构。我们对现实世界多模式EHR数据和预测任务进行了彻底的实验，结果表明，我们的框架不仅可以对现有最新方法进行显着的性能提高，而且还可以有效地发现有意义的网络体系结构。

### Density Adaptive Attention is All You Need: Robust Parameter-Efficient Fine-Tuning Across Multiple Modalities 
[[arxiv](https://arxiv.org/abs/2401.11143)] [[cool](https://papers.cool/arxiv/2401.11143)] [[pdf](https://arxiv.org/pdf/2401.11143)]
> **Authors**: Georgios Ioannides,Aman Chadha,Aaron Elkins
> **First submission**: 2024-01-20
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 密度自适应的关注就是您所需要的：跨多种方式稳健参数有效的微调
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别,声音,音频和语音处理,信号处理
- **摘要**: 我们提出了多头密度自适应注意机制（DAAM），这是一种新型的概率注意框架，可用于参数有效的微调（PEFT）和密度自适应变压器（DAT），旨在增强跨多种方式，包括语音，文本和视觉的多种方式。 DAAM将可学习的平均值和方差整合到其注意机制中，该机制在多头框架中实现，使其能够集体对任何概率分布进行建模，以动态重新校准特征意义。该方法显示出显着的改进，尤其是在高度非平稳数据的情况下，超过了模型性能中最新的注意力技术，精度的准确性高达约20％（ABS。）。从经验上讲，DAAM在各种任务中表现出卓越的适应性和功效，包括语音，图像分类和文本分类中的情绪识别，从而在跨多种方式处理数据中建立了其鲁棒性和多功能性。此外，我们介绍了重要性因素，这是一种新的基于学习的指标，可增强接受基于DAAM方法训练的模型的解释性。

### Robust Multi-Modal Density Estimation 
[[arxiv](https://arxiv.org/abs/2401.10566)] [[cool](https://papers.cool/arxiv/2401.10566)] [[pdf](https://arxiv.org/pdf/2401.10566)]
> **Authors**: Anna Mészáros,Julian F. Schumann,Javier Alonso-Mora,Arkady Zgonnikov,Jens Kober
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: No comments
- **标题**: 强大的多模式密度估计
- **领域**: 机器学习,机器学习
- **摘要**: 概率密度函数的估计是科学和工程学中的基本问题。但是，已经证明诸如核密度估计（KDE）之类的常见方法缺乏鲁棒性，而在多模式估计问题中尚未评估更复杂的方法。在本文中，我们提出了罗马（强大的多模式估计量），这是一种非参数估计方法，它旨在解决估计多模式，非正态和高度相关分布的挑战。罗马利用聚类将一组多模式的样本划分为多个单模式的样本，然后在单个多模式估计中结合了单个簇获得的简单KDE估计值。我们比较了我们对最新方法的密度估计方法以及罗马消融的方法，这不仅表明它不仅胜过建立的方法，而且对各种分布也更强大。我们的结果表明，罗马可以克服其他估计器所表现出的过度安装和过度平滑的问题。

### Tensor-view Topological Graph Neural Network 
[[arxiv](https://arxiv.org/abs/2401.12007)] [[cool](https://papers.cool/arxiv/2401.12007)] [[pdf](https://arxiv.org/pdf/2401.12007)]
> **Authors**: Tao Wen,Elynn Chen,Yuzhou Chen
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: Accepted at AISTATS 2024
- **标题**: 张量视图拓扑图神经网络
- **领域**: 机器学习,人工智能
- **摘要**: 图形分类是图形结构数据的重要学习任务。图神经网络（GNN）最近在图形学习中引起了人们的注意，并且在许多重要的图形问题中显示出显着改善。尽管有最先进的表现，但现有的GNN仅使用来自每个节点周围非常有限的社区的本地信息，却遭受了多模式信息的丢失和过度计算的开销。为了解决这些问题，我们提出了一种新颖的张量视图拓扑图神经网络（TTG-NN），这是一类简单而有效的拓扑深度学习，建立在持续的同源性，图形卷积和张量操作的基础上。这种新方法结合了张量学习，以同时捕获张量视图拓扑（TT），以及有关本地和全球级别的张量图图（TG）结构信息。在计算上，为了充分利用图形拓扑和结构，我们提出了两个灵活的TT和TG表示模块，它们可以解散特征特征量张量聚集和转换，并学习以更少的计算来保留多模式结构。从理论上讲，我们在我们提出的张量转换层（TTL）的样本外和样本中平均近似误差上得出了高概率边界。实际数据实验表明，所提出的TTG-NN优于各种图基准的20种最先进方法。

### Benchmarking Large Multimodal Models against Common Corruptions 
[[arxiv](https://arxiv.org/abs/2401.11943)] [[cool](https://papers.cool/arxiv/2401.11943)] [[pdf](https://arxiv.org/pdf/2401.11943)]
> **Authors**: Jiawei Zhang,Tianyu Pang,Chao Du,Yi Ren,Bo Li,Min Lin
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: Technical report
- **标题**: 基准针对常见腐败的大型多模型模型
- **领域**: 机器学习,计算语言学,密码学和安全,计算机视觉和模式识别,多媒体
- **摘要**: 该技术报告旨在通过专门检查遭受共同腐败时的产出的自持持续性来填补评估大型多模型（LMM）的缺陷。我们研究了文本，图像和语音之间的跨模式相互作用，包括四个基本的一代任务：文本到图像，图像到文本，文本到语音和语音到文本。我们创建了一个名为MMCBench的全面基准，该基准涵盖了100多个流行的LMM（完全超过150个模型检查点）。在共同腐败下进行彻底的评估对于实际部署至关重要，并有助于更好地理解尖端LMM的可靠性。基准代码可从https://github.com/sail-sg/mmcbench获得

### Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality 
[[arxiv](https://arxiv.org/abs/2401.13898)] [[cool](https://papers.cool/arxiv/2401.13898)] [[pdf](https://arxiv.org/pdf/2401.13898)]
> **Authors**: Huy Q. Le,Chu Myaet Thwal,Yu Qiao,Ye Lin Tun,Minh N. H. Nguyen,Eui-Nam Huh,Choong Seon Hong
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: 14 pages, 8 figures, 11 tables
- **标题**: 基于跨模式原型的多模式联合学习在严重缺失的方式下
- **领域**: 机器学习
- **摘要**: 多模式联合学习（MFL）已成为一个分散的机器学习范式，允许多个具有不同方式的客户在跨不同数据源的全球模型进行培训而无需共享其私人数据的情况下进行协作。但是，挑战（例如数据异质性和严重缺失的方式）对MFL的鲁棒性构成了至关重要的障碍，从而显着影响了全球模型的性能。在现实世界应用中（例如自动驾驶）中缺失模式的发生通常来自传感器故障等因素，在培训过程中带来了知识差距。具体而言，在本地培训阶段缺乏模式会引入未对准，这是由于少量填补的情况而导致的。因此，在全球模型中实现强大的概括是必须的，尤其是在与数据不完整的客户打交道时。在本文中，我们提出了$ \ textbf {多模式联合交叉原型学习（MFCPL）} $，这是一种在严重缺失模式下的新型MFL方法。我们的MFCPL利用完整的原型以与跨模式的正则化和特定于模态特定水平和跨模式对比机制提供了模态共享水平的多种模态知识。此外，我们的方法还引入了跨模式对齐方式，以提供特定于模态特征的正规化，从而提高了整体性能，尤其是在涉及严重缺失模态的情况下。通过对三个多模式数据集进行的大量实验，我们证明了MFCPL在缓解数据异质性挑战和严重缺失模态的挑战，同时改善MFL的整体性能和鲁棒性。

### VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks 
[[arxiv](https://arxiv.org/abs/2401.13649)] [[cool](https://papers.cool/arxiv/2401.13649)] [[pdf](https://arxiv.org/pdf/2401.13649)]
> **Authors**: Jing Yu Koh,Robert Lo,Lawrence Jang,Vikram Duvvur,Ming Chong Lim,Po-Yu Huang,Graham Neubig,Shuyan Zhou,Ruslan Salakhutdinov,Daniel Fried
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: Accepted to ACL 2024. 24 pages. Project page: https://jykoh.com/vwa
- **标题**: VisualWebarena：在现实的视觉网络任务上评估多模式的代理
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 能够在网络上计划，推理和执行操作的自主代理为自动化计算机任务的自动化途径提供了有希望的途径。但是，大多数现有基准主要集中在基于文本的代理上，忽略了许多需要视觉信息才能有效解决的自然任务。鉴于大多数计算机界面都符合人类的感知，因此视觉信息通常会以仅限文本模型有效利用的方式来增强文本数据。为了弥合这一差距，我们介绍了VisualWebarena，这是一种基准测试，旨在评估多模式Web代理在现实\ TextIt {视觉扎根的任务}上的性能。 VisualWebarena由一组不同的基于Web的任务组成，可评估自动多模式代理的各种功能。要执行此基准测试，代理需要准确处理图像文本输入，解释自然语言说明并在网站上执行操作以实现用户定义的目标。我们对基于LLM的最先进的自主剂进行了广泛的评估，包括多种模型。通过广泛的定量和定性分析，我们确定了仅文本LLM代理的几个局限性，并揭示了最先进的多模式语言代理能力的差距。 VisualWebarena提供了一个评估多模式自动语言代理的框架，并为建立更强大的自主代理提供了洞察力。我们的代码，基线模型和数据可在https://jykoh.com/vwa上公开获取。

### Classification of Radiologically Isolated Syndrome and Clinically Isolated Syndrome with Machine-Learning Techniques 
[[arxiv](https://arxiv.org/abs/2401.13301)] [[cool](https://papers.cool/arxiv/2401.13301)] [[pdf](https://arxiv.org/pdf/2401.13301)]
> **Authors**: V Mato-Abad,A Labiano-Fontcuberta,S Rodriguez-Yanez,R Garcia-Vazquez,CR Munteanu,J Andrade-Garda,A Domingo-Santos,V Galan Sanchez-Seco,Y Aladro,ML Martinez-Gines,L Ayuso,J Benito-Leon
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: 24 pages, 2 tables
- **标题**: 通过机器学习技术的放射学孤立综合征和临床孤立综合征的分类
- **领域**: 机器学习
- **摘要**: 背景和目的：通过磁共振成像（MRI）的意外检测在无症状的白质病变受试者的大脑中，暗示了多发性硬化症（MS）的放射学分离综合征（RIS）。作为早期MS之间的差异[即临床分离的综合征（CIS）]和RIS是临床事件的发生，由于存在放射学诊断标准，因此在不干扰MRI的情况下改善对亚临床形式的检测是合乎逻辑的。我们的目标是使用机器学习分类方法来确定有助于区分RIS患者与CI患者的形态计量指标。方法：我们通过组合17例RIS患者的MRI生物标志物（皮质厚度，皮质和皮质下灰质和白质完整性），使用了多模式3-T MRI方法。结果：预测顺式和RI诊断的最佳模型是基于仅使用三个功能的天真贝叶斯，装袋和多层感知器分类器：左侧中间额回的体积和右Amygdala和右舌旋流的分数各向异性值。天真的贝叶斯获得了最高精度[总体分类，0.765;接收器操作特征（AUROC）下的面积为0.782]。结论：应用于多模式MRI数据的机器学习方法可能会区分MS（CIS和RIS）的最早临床表达，精度为78％。关键字：包装；多层感知;天真的贝叶斯分类器；临床上孤立的综合征；扩散张量成像；机器学习；磁共振成像；多发性硬化症;放射学孤立综合征。

### MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning 
[[arxiv](https://arxiv.org/abs/2401.14199)] [[cool](https://papers.cool/arxiv/2401.14199)] [[pdf](https://arxiv.org/pdf/2401.14199)]
> **Authors**: Junwei Su,Shan Wu,Jinhui Li
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: No comments
- **标题**: MTRGL：通过多模式的时间关系图辨别有效的时间相关性
- **领域**: 机器学习,普通经济学,交易和市场微观结构
- **摘要**: 在这项研究中，我们探讨了深度学习和金融市场应用的协同作用，重点是配对交易。这种市场中立的策略是定量融资不可或缺的，并且易于先进的深度学习技术。配对交易中的一个关键挑战是辨别实体之间的时间相关性，需要整合各种数据模式。解决这个问题，我们介绍了一个新颖的框架，多模式的时间关系图学习（MTRGL）。 MTRGL将时间序列数据和离散功能结合到时间图中，并采用基于内存的时间图神经网络。这种方法将时间相关标识重新构架为时间图链接预测任务，该任务已显示出经验成功。我们在现实世界数据集上的实验证实了MTRGL的出色表现，强调了其在完善自动化对交易策略方面的承诺。

### Validation of artificial neural networks to model the acoustic behaviour of induction motors 
[[arxiv](https://arxiv.org/abs/2401.15377)] [[cool](https://papers.cool/arxiv/2401.15377)] [[pdf](https://arxiv.org/pdf/2401.15377)]
> **Authors**: F. J. Jimenez-Romero,D. Guijo-Rubio,F. R. Lara-Raya,A. Ruiz-Gonzalez,C. Hervas-Martinez
> **First submission**: 2024-01-27
> **First announcement**: 2024-01-29
> **comment**: No comments
- **标题**: 验证人工神经网络以建模感应电动机的声学行为
- **领域**: 机器学习,声音,音频和语音处理
- **摘要**: 在过去的十年中，电动电动机的音质是研究领域的热门话题。特别是，由于其应用大量，人口暴露于噪声排放引起的身体和心理不适。因此，有必要最大程度地减少其对人群的心理影响。通过这种方式，这项工作的主要目标是评估使用多任务人工神经网络作为建模技术，用于同时预测感应电机的心理声学参数。使用了几种输入，例如电机功率信号的电量和极点数量，而不是将电动机的噪声与环境噪声分开。提出了两种不同类型的人工神经网络，以使用等效的声压，响度，粗糙度和清晰度作为输出来评估感应电动机的声学质量。具体来说，已经考虑了两个不同的拓扑：简单模型和更复杂的模型。前者更容易解释，而后者则导致更高的准确性，因为它们隐藏了因果关系。为了关注简单的可解释模型，产品单元神经网络取得了最佳结果：用于MSE和SEP。该产品单元模型的主要好处是它的简单性，因为仅使用了10个输入变量，概述了多任务人工神经网络的有效传输机制来提取多个任务的共同特征。最后，对使用最佳产品单元神经网络完成的感应电动机的声学质量进行了深入分析。

### Analog and Multi-modal Manufacturing Datasets Acquired on the Future Factories Platform 
[[arxiv](https://arxiv.org/abs/2401.15544)] [[cool](https://papers.cool/arxiv/2401.15544)] [[pdf](https://arxiv.org/pdf/2401.15544)]
> **Authors**: Ramy Harik,Fadi El Kalach,Jad Samaha,Devon Clark,Drew Sander,Philip Samaha,Liam Burns,Ibrahim Yousif,Victor Gadow,Theodros Tarekegne,Nitol Saha
> **First submission**: 2024-01-27
> **First announcement**: 2024-01-29
> **comment**: 11 pages, datasets for Future Factories
- **标题**: 在未来工厂平台上获取的模拟和多模式制造数据集
- **领域**: 机器学习
- **摘要**: 本文介绍了两个行业级数据集，这些数据集于12月11日和2023年12月12日在南卡罗来纳大学的未来工厂实验室收集。这些数据集由制造装配线生成，该制造装配线利用工业标准在执行机构，控制机制和传输方面。这两个数据集都是通过连续30个小时（带有较小的过滤）的组装线同时生成的，并从整个系统中配备的传感器中收集数据。在操作过程中，还通过手动删除最终组装所需的零件，将缺陷引入组装操作中。生成的数据集包括时间序列模拟数据集，另一个是时间序列多模式数据集，该数据集在模拟数据旁边包含系统的图像。这些数据集的生成是为了提供工具，以进一步研究增强制造业的智能。实际制造数据集可能会稀缺，更不用说存在异常或缺陷的数据集了。因此，这些数据集希望解决这一差距，并为研究人员提供建立和培训适用于制造业的人工智能模型的基础。最后，这些数据集是未来工厂实验室已发表的数据的第一次迭代，可以进一步调整以适合更多的研究人员，需要前进。

### On the generalization capacity of neural networks during generic multimodal reasoning 
[[arxiv](https://arxiv.org/abs/2401.15030)] [[cool](https://papers.cool/arxiv/2401.15030)] [[pdf](https://arxiv.org/pdf/2401.15030)]
> **Authors**: Takuya Ito,Soham Dan,Mattia Rigotti,James Kozloski,Murray Campbell
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: ICLR 2024
- **标题**: 关于通用多模式推理期间神经网络的概括能力
- **领域**: 机器学习
- **摘要**: 变压器的出现导致了大型语言模型（LLM）的发展，这似乎证明了类似人类的能力。为了评估这类模型的一般性以及多种基础神经网络架构与多模式领域的一般性，我们评估并比较了它们的多模式概括能力。我们介绍了多模式的问题解答基准，以评估三种特定类型的分布（OOD）概括性能：分散术术的概括（在存在分散术的存在中），系统的组成概括（对新任务排列的概括）和生产性组成的概括（对更复杂的任务结构）。我们发现，跨模型架构（例如RNN，变形金刚，感知者等），具有多个注意力层的模型，或在输入域之间利用交叉注意机制的模型，表现更好。我们的积极结果表明，对于多模式干扰物和系统的概括，跨模式的关注或更深的注意层模型是整合多模式输入所需的关键建筑特征。另一方面，这些架构特征都没有导致富有成效的概括，这表明了对特定类型的多模式概括的现有体系结构的基本限制。这些结果证明了用于多模式推理的现代神经模型基础的特定建筑组成部分的优势和局限性。最后，我们提供了通用COG（GCOG），这是一种具有多种多模式分割的可配置基准测试，以供将来的研究探索。

### Communication-Efficient Multimodal Federated Learning: Joint Modality and Client Selection 
[[arxiv](https://arxiv.org/abs/2401.16685)] [[cool](https://papers.cool/arxiv/2401.16685)] [[pdf](https://arxiv.org/pdf/2401.16685)]
> **Authors**: Liangqi Yuan,Dong-Jun Han,Su Wang,Devesh Upadhyay,Christopher G. Brinton
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: arXiv admin note: text overlap with arXiv:2310.07048
- **标题**: 沟通高效的多模式联合学习：联合方式和客户选择
- **领域**: 机器学习,分布式、并行和集群计算
- **摘要**: 多模式联合学习（FL）旨在丰富客户在多种模式中收集测量的FL设置中的模型培训。但是，对多模式FL的关键挑战仍然没有解决，尤其是在异质网络设置中：（i）每个客户收集的方式集将是多种多样的，（ii）通信限制阻止客户将其所有经过本地训练的模式上传到服务器。在本文中，我们提出了具有联合模式和客户选择（MMFEDMC）的多模式联合学习，这是一种新的FL方法，可以应对多模式环境中上述挑战。联合选择算法结合了两个主要组成部分：（a）每个客户的模态选择方法，它称重（i）通过沙普利价值分析（Shapley Value Analysis）衡量的模态的影响，（ii）模型模型大小作为跨度的频率大小作为沟通范围，以（iii）对（iii）（iii）模式模型更新的频率，表示敏捷性的频率，以表示的敏捷性，以增强概括性。 （b）基于每个客户端的局部丢失模式模型的端口选择策略。五个现实世界数据集的实验证明了MMFEDMC达到与多个基线的可比精度的能力，同时将通信开销降低了20倍以上。我们的方法论的演示视频可在https://liangqiy.com/mmfedmc/上找到。

### MLEM: Generative and Contrastive Learning as Distinct Modalities for Event Sequences 
[[arxiv](https://arxiv.org/abs/2401.15935)] [[cool](https://papers.cool/arxiv/2401.15935)] [[pdf](https://arxiv.org/pdf/2401.15935)]
> **Authors**: Viktor Moskvoretskii,Dmitry Osin,Egor Shvetsov,Igor Udovichenko,Maxim Zhelnin,Andrey Dukhovny,Anna Zhimerikina,Evgeny Burnaev
> **First submission**: 2024-01-29
> **First announcement**: 2024-01-30
> **comment**: 11 pages, 9 figures
- **标题**: MLEM：生成和对比度学习，作为事件序列的不同方式
- **领域**: 机器学习,人工智能
- **摘要**: 这项研究探讨了自我监督学习技术在事件序列中的应用。它是银行，电子商务和医疗保健等各种应用程序中的关键方式。但是，关于事件序列的自我监督学习的研究有限，图像，文本和语音等其他领域的方法可能不容易转移。为了确定最合适的方法，我们对先前确定的表现最佳方法进行了详细的比较分析。我们发现对比度和生成方法都不是优越的。我们的评估包括分类事件序列，预测下一个事件以及评估嵌入质量。这些结果进一步强调了将这两种方法结合起来的潜在优势。鉴于该域中的混合模型缺乏研究，我们最初从另一个领域调整了基线模型。但是，在观察其表现不佳的情况下，我们开发了一种称为多模式学习事件模型（MLEM）的新颖方法。 MLEM将对比度学习和生成建模视为不同但互补的方式，使它们的嵌入保持一致。我们的研究结果表明，将对比度和生成方法结合在一起，将MLEM与MLEM相结合，在多个指标之间取得了出色的性能。

### Enhancing Score-Based Sampling Methods with Ensembles 
[[arxiv](https://arxiv.org/abs/2401.17539)] [[cool](https://papers.cool/arxiv/2401.17539)] [[pdf](https://arxiv.org/pdf/2401.17539)]
> **Authors**: Tobias Bischoff,Bryan Riel
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: No comments
- **标题**: 使用合奏增强基于得分的抽样方法
- **领域**: 机器学习,计算
- **摘要**: 我们在基于分数的采样方法中介绍合奏，以开发无梯度的近似采样技术，以利用粒子集合的集体动力学来计算近似反向扩散漂移。我们介绍了基本方法，强调了它与生成扩散模型和先前引入的Föllmer采样器的关系。我们通过各种示例证明了集合策略的功效，范围从低维度到中等程度采样问题，包括多模式和高度非高斯概率分布，并与诸如螺母之类的传统方法进行比较。我们的发现突出了整体策略的潜力，即在无法使用梯度的情况下对复杂概率分布进行建模。最后，我们在地球物理科学中的贝叶斯反演问题的背景下展示了其应用。

### Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models 
[[arxiv](https://arxiv.org/abs/2401.17139)] [[cool](https://papers.cool/arxiv/2401.17139)] [[pdf](https://arxiv.org/pdf/2401.17139)]
> **Authors**: Lai Wei,Zhiquan Tan,Chenghai Li,Jindong Wang,Weiran Huang
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: Accepted by NeurIPS 2024
- **标题**: DIFF-ERANK：一种基于排名的新型指标，用于评估大语言模型
- **领域**: 机器学习,人工智能,计算语言学,信息论
- **摘要**: 大型语言模型（LLMS）已将自然语言处理转换，并将其强大功能扩展到多模式域。随着LLMS继续前进，为其评估发展多样化和适当的指标至关重要。在本文中，我们介绍了一种基于信息理论和几何原理的新型基于等级的指标，Diff-Erank。 Diff-Erank通过分析其隐藏表示形式来评估LLM，从而定量衡量了它们在培训期间消除冗余信息的有效程度。我们演示了diff-erank在单模式（例如语言）和多模式设置中的适用性。对于语言模型，我们的结果表明，Diff-Erank随模型大小的增加而增加，并且与常规指标（例如损失和准确性）良好相关。在多模式环境中，我们提出了一种基于Erank的一致性评估方法，并验证当代的多模式LLMS基于我们的方法表现出强烈的对齐性能。我们的代码可在https://github.com/waltonfuture/diff-erank上公开获取。

### Multi-modal Representation Learning for Cross-modal Prediction of Continuous Weather Patterns from Discrete Low-Dimensional Data 
[[arxiv](https://arxiv.org/abs/2401.16936)] [[cool](https://papers.cool/arxiv/2401.16936)] [[pdf](https://arxiv.org/pdf/2401.16936)]
> **Authors**: Alif Bin Abdul Qayyum,Xihaier Luo,Nathan M. Urban,Xiaoning Qian,Byung-Jun Yoon
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: No comments
- **标题**: 多模式表示学习，用于从离散低维数据的连续天气模式的跨模式预测
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 世界正在寻找不会污染环境的清洁和可再生能源，以减少有助于全球变暖的温室气体排放。风能不仅可以减少温室排放，而且还满足对能源的不断增长的需求。为了有效利用风能，解决风数据分析中以下三个挑战至关重要。首先，改善各种气候条件下的数据解决方案，以确保大量信息以评估潜在能源。其次，为从传感器/仿真收集的数据实施降低技术以有效管理和存储大型数据集。第三，将风数据从一个空间规范到另一种空间规范，特别是在数据采集可能是不切实际或昂贵的情况下。我们提出了一种基于深度学习的方法，以实现从不连续的风数据以及降低数据维度的多模式连续分辨率数据预测。

## 多媒体(cs.MM:Multimedia)

该领域共有 2 篇论文

### Learning Audio Concepts from Counterfactual Natural Language 
[[arxiv](https://arxiv.org/abs/2401.04935)] [[cool](https://papers.cool/arxiv/2401.04935)] [[pdf](https://arxiv.org/pdf/2401.04935)]
> **Authors**: Ali Vosoughi,Luca Bondi,Ho-Hsiang Wu,Chenliang Xu
> **First submission**: 2024-01-10
> **First announcement**: 2024-01-11
> **comment**: Accepted at ICASSP 2024
- **标题**: 从反事实自然语言学习音频概念
- **领域**: 多媒体,计算语言学,声音,音频和语音处理
- **摘要**: 传统的音频分类依赖于预定义的类，缺乏从自由形式文本中学习的能力。最近的方法从原始音频文本对中解锁了学习音频文本的嵌入，描述了自然语言的音频。尽管最近进步，但几乎没有探索系统的方法来训练模型以识别替代方案中的声音事件和来源，例如在类似情况下将烟花与户外活动中的枪击区分开。这项研究介绍了音频域中的因果推理和反事实分析。我们使用反事实实例，并将它们包括在不同方面的模型中。我们的模型考虑了来自人类注销参考文本的声学特征和声源信息。为了验证模型的有效性，我们利用多个音频字幕进行了预训练。然后，我们通过几个常见的下游任务进行评估，证明了该方法的优点是在音频域中利用反事实信息的第一批作品之一。具体而言，基于开放语言的音频检索任务中的前1位准确性增加了43％以上。

### An Open Software Suite for Event-Based Video 
[[arxiv](https://arxiv.org/abs/2401.17151)] [[cool](https://papers.cool/arxiv/2401.17151)] [[pdf](https://arxiv.org/pdf/2401.17151)]
> **Authors**: Andrew C. Freeman
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: No comments
- **标题**: 基于事件的视频的开放软件套件
- **领域**: 多媒体,计算机视觉和模式识别
- **摘要**: 虽然传统的视频表示是围绕离散图像帧组织的，但基于事件的视频是一种完全放弃图像框架的新范式。相反，像素样品在时间上是异步的，并且彼此独立。到目前为止，研究人员还缺乏一个凝聚力的软件框架来探索基于事件的视频的表示，压缩和应用。我介绍了ad $δ$ er软件套件以填补此空白。该框架包括用于将基于框架和多模式事件的视频源转码的公用事业，转换为通用表示，费率控制机制，有损压缩，应用支持以及用于转码和播放的交互式GUI。在本文中，我描述了这些各种软件组件及其用法。

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

该领域共有 1 篇论文

### Multimodal Optimization with k-Cluster Big Bang-Big Crunch Algorithm and Postprocessing Methods for Identification and Quantification of Optima 
[[arxiv](https://arxiv.org/abs/2401.06153)] [[cool](https://papers.cool/arxiv/2401.06153)] [[pdf](https://arxiv.org/pdf/2401.06153)]
> **Authors**: Kemal Erdem Yenin,Reha Oguz Sayin,Kuzey Arar,Kadir Kaan Atalay,Fabio Stroppa
> **First submission**: 2023-12-21
> **First announcement**: 2024-01-12
> **comment**: 18 pages
- **标题**: 使用K群集大爆炸量紧缩算法和识别和量化的多模式优化
- **领域**: 神经和进化计算,人工智能
- **摘要**: 在工程问题中经常遇到多模式优化，尤其是在寻求不同的替代解决方案时。进化算法可以有效地解决多模式优化，这要归功于它们的特征，例如人口概念，探索/开发以及适合并行计算。本文研究了鲜为人知的优化器，即Big Bang-Big Crunch（BBBC）算法，适用于多模式优化。我们扩展了BBBC，并提出了基于聚类的多模式优化器K-BBBC。此外，我们将两种后处理方法介绍给（i）在一组检索的解决方案（即种群）中识别局部最佳选择，以及（ii）量化针对预期的最佳选择（即成功率）。我们的结果表明，K-BBC的表现良好，即使有大量Optima（以$ 379 $ OPTIMA进行测试）和高维度（以$ 32 $的决策变量进行测试），但对于许多本地Optima的问题（即CEC'2013 Benchmark Marksk Set设置）而言，它在计算上变得太昂贵了）。与其他多模式优化方法相比，在基本多模式函数测试时，它在准确性（在搜索和客观空间中）和成功率（正确检索了Optima的数量）方面优于它们，尤其是在应用精英函数时；但是，它需要知道问题的最佳数量，这使得在Niching竞争测试CEC'2013进行测试时，其性能降低。最后，我们通过将其成功率与实际验证率进行比较来验证了我们提出的后处理方法：结果表明，这些方法可用于通过正确识别Optima并提供成功的指示来评估多模式优化算法的性能 - 无需了解Optima位于搜索空间中。

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

该领域共有 2 篇论文

### LMaaS: Exploring Pricing Strategy of Large Model as a Service for Communication 
[[arxiv](https://arxiv.org/abs/2401.02675)] [[cool](https://papers.cool/arxiv/2401.02675)] [[pdf](https://arxiv.org/pdf/2401.02675)]
> **Authors**: Panlong Wu,Qi Liu,Yanjie Dong,Fangxin Wang
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: LMAA：探索大型模型的定价策略作为通信的服务
- **领域**: 网络和互联网架构,计算机科学与博弈论,机器学习
- **摘要**: 下一代的交流被认为是智能的交流，可以取代传统的符号通信，在这些沟通中，考虑到来源和渠道的高度凝结的语义信息将以高效率提取和传输。最近流行的大型模型（例如GPT4和增强学习技术）为智能沟通奠定了坚实的基础，并在不久的将来促使其实际部署。鉴于这些多模式大型语言模型的“一次训练并广泛使用”的特征，我们认为在这种情况下，付款方式适合使用，将其称为“大型模型”（LMAAS）。但是，在异质和动态的客户环境中，交易和定价问题非常复杂，这使得定价优化问题在寻求手持解决方案时具有挑战性。在本文中，我们旨在填补这一空白，并将LMAA市场交易作为Stackelberg游戏，并具有两个步骤。第一步，我们优化了卖方的定价决定，并提出了一种迭代模型定价（IMP）算法，该算法通过推理客户的未来租赁决策来优化大型模型的价格，该决策能够实现近乎最佳的定价解决方案。在第二步中，我们通过设计强大的选择和租赁（RSR）算法来优化客户的选择决策，该算法可以保证具有严格的理论证明是最佳的。广泛的实验证实了我们算法的有效性和鲁棒性。

### Deep Learning in Physical Layer: Review on Data Driven End-to-End Communication Systems and their Enabling Semantic Applications 
[[arxiv](https://arxiv.org/abs/2401.12800)] [[cool](https://papers.cool/arxiv/2401.12800)] [[pdf](https://arxiv.org/pdf/2401.12800)]
> **Authors**: Nazmul Islam,Seokjoo Shin
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-24
> **comment**: ef:IEEE Open Journal of the Communications Society, vol. 5, pp. 4207-4240, 2024
- **标题**: 物理层中的深度学习：审查数据驱动的端到端通信系统及其启用语义应用程序
- **领域**: 网络和互联网架构,机器学习
- **摘要**: 深度学习（DL）通过引入DataDriven端到端（E2E）学习彻底改变了无线通信系统，其中物理层（PHY）被转换为DL体系结构以实现峰值优化。利用DL进行E2E优化，可以显着提高其在复杂的无线环境中的适应性和性能，满足5G及以后的高级网络系统的需求。此外，数据驱动的PHY优化的这种演变还启用了各种模式的高级语义应用程序，包括文本，图像，音频，视频和多模式传输。这些应用程序将沟通从比特级提升到语义级的智能，使其能够辨别背景和意图。尽管PHY作为DL体系结构，在启用语义通信（SEMCOM）系统中起着至关重要的作用，但整合E2E通信和SEMCOM系统的全面研究仍然显着尚未得到充分兴奋。这突出了这些综合领域的新颖性和潜力，将其标记为有前途的研究领域。因此，本文对E2E通信系统的数据驱动的PHY的新兴领域进行了全面综述，并强调了它们在跨各种方式启用语义应用中的作用。它还确定了主要的挑战和潜在的研究方向，是E2E通信和SEMCOM系统DL未来进步的关键指南。

## 机器人技术(cs.RO:Robotics)

该领域共有 12 篇论文

### General-purpose foundation models for increased autonomy in robot-assisted surgery 
[[arxiv](https://arxiv.org/abs/2401.00678)] [[cool](https://papers.cool/arxiv/2401.00678)] [[pdf](https://arxiv.org/pdf/2401.00678)]
> **Authors**: Samuel Schmidgall,Ji Woong Kim,Alan Kuntz,Ahmed Ezzat Ghazi,Axel Krieger
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: 通用基础模型，用于增加机器人辅助手术的自主权
- **领域**: 机器人技术,机器学习,组织和器官
- **摘要**: 端到端机器人学习的主要范式集中在优化特定于任务的目标，以解决单个机器人问题，例如拾取对象或达到目标位置。但是，关于机器人技术中高容量模型的最新研究表明，有望在视频演示的大量和任务不合时宜的数据集中接受培训。这些模型显示出令人印象深刻的对看不见的情况的概括水平，尤其是数据量和模型复杂性量表。从数据中学习的手术机器人系统由于有几个原因而努力与机器人学习的其他领域一样迅速前进：（1）缺乏现有的大型开源数据来训练模型，（2）在手术过程中这些机器人在手术中无法使用的柔软性变形，因为在手术过程中，这些机器人无法与临床进行临床的损害，因为该机器人在验证的情况下进行了验证和视觉危害，但要验证了验证的机器人验证和（3）的生物学危害（3））更广泛的安全措施。这篇观点文章旨在通过开发用于手术机器人的多模式，多任务，多任务，视觉语言操作模型来增加机器人辅助手术中的机器人自主权的途径。最终，我们认为手术机器人是从通用模型中受益的独特位置，并为在机器人辅助手术中提高自主权提供了三种指导行动。

### On Time-Indexing as Inductive Bias in Deep RL for Sequential Manipulation Tasks 
[[arxiv](https://arxiv.org/abs/2401.01993)] [[cool](https://papers.cool/arxiv/2401.01993)] [[pdf](https://arxiv.org/pdf/2401.01993)]
> **Authors**: M. Nomaan Qureshi,Ben Eisner,David Held
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 在深度RL中作为归纳偏置的时间指数，以进行连续操作任务
- **领域**: 机器人技术,人工智能
- **摘要**: 在解决复杂的操纵任务的同时，操纵政策通常需要学习一组各种技能来完成这些任务。一组技能通常是多模式的 - 每个技能可能具有相当明显的行动和状态分布。标准的深层政策学习算法通常将策略模型为具有单个输出头（确定性或随机性）的深神经网络。该结构要求网络在内部学习模式之间切换，这可能会导致样本效率降低和性能差。在本文中，我们探讨了一个简单的结构，该结构有利于许多操纵任务所需的技能学习。具体来说，我们提出了一个政策体系结构，该策略架构在固定持续时间内依次执行不同的动作头，从而能够学习原始技能，例如达到和掌握。我们对Metaworld任务的经验评估表明，这种简单的结构的表现优于标准的政策学习方法，强调了其提高技能获取的潜力。

### Object-Centric Instruction Augmentation for Robotic Manipulation 
[[arxiv](https://arxiv.org/abs/2401.02814)] [[cool](https://papers.cool/arxiv/2401.02814)] [[pdf](https://arxiv.org/pdf/2401.02814)]
> **Authors**: Junjie Wen,Yichen Zhu,Minjie Zhu,Jinming Li,Zhiyuan Xu,Zhengping Che,Chaomin Shen,Yaxin Peng,Dong Liu,Feifei Feng,Jian Tang
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: accepted to ICRA2024
- **标题**: 机器人操纵以中心的以对象为中心的指令
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 人类通过认识到对象在其观察中的身份和位置来解释场景。为了使机器人执行\ enquote {pick and plots}之类的任务，了解对象是什么以及它们所在的位置至关重要。虽然在文献中已经广泛讨论了前者，该文献使用大型语言模型丰富文本描述，但后者仍然没有被忽略。在这项工作中，我们介绍了\ textit {以对象为中心的指令增强（OCI）}框架，以增强具有位置提示的高语义和信息密集的语言指令。我们利用多模式的大语言模型（MLLM）将对象位置的知识编织到自然语言指令中，从而帮助策略网络掌握用于多功能操作的动作。此外，我们提出了一种功能重复使用机制，以将现成预训练的MLLM的视觉语言功能整合到策略网络中。通过一系列模拟和现实世界的机器人任务，我们证明了通过我们的增强说明训练的机器人操纵器模仿政策优于仅依靠传统语言指示的人。

### Large Language Models for Robotics: Opportunities, Challenges, and Perspectives 
[[arxiv](https://arxiv.org/abs/2401.04334)] [[cool](https://papers.cool/arxiv/2401.04334)] [[pdf](https://arxiv.org/pdf/2401.04334)]
> **Authors**: Jiaqi Wang,Zihao Wu,Yiwei Li,Hanqi Jiang,Peng Shu,Enze Shi,Huawen Hu,Chong Ma,Yiheng Liu,Xuhui Wang,Yincheng Yao,Xuan Liu,Huaqin Zhao,Zhengliang Liu,Haixing Dai,Lin Zhao,Bao Ge,Xiang Li,Tianming Liu,Shu Zhang
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-09
> **comment**: No comments
- **标题**: 机器人技术的大型语言模型：机遇，挑战和观点
- **领域**: 机器人技术,人工智能
- **摘要**: 大型语言模型（LLM）经历了重大扩展，并且越来越多地整合在各个领域。值得注意的是，在机器人任务计划的领域中，LLMS利用其先进的推理和语言理解能力来根据自然语言指示制定精确有效的行动计划。但是，对于机器人与复杂环境相互作用的具体任务，仅文本LLMS通常由于缺乏与机器人视觉感知的兼容性而面临挑战。这项研究概述了LLM和多模式LLM的新兴整合到各种机器人任务中。此外，我们提出了一个框架，该框架利用多模式GPT-4V通过自然语言指令和机器人视觉感知的结合来增强具体的任务计划。我们的结果基于不同的数据集，表明GPT-4V有效地增强了体现任务中的机器人性能。对各种机器人任务的LLM和多模式LLM的广泛调查和评估丰富了以LLM为中心的以LLM体现的智能的理解，并为弥合人类机器人环境互动的差距提供了前瞻性的见解。

### Singing the Body Electric: The Impact of Robot Embodiment on User Expectations 
[[arxiv](https://arxiv.org/abs/2401.06977)] [[cool](https://papers.cool/arxiv/2401.06977)] [[pdf](https://arxiv.org/pdf/2401.06977)]
> **Authors**: Nathaniel Dennler,Stefanos Nikolaidis,Maja Matarić
> **First submission**: 2024-01-12
> **First announcement**: 2024-01-15
> **comment**: Presented at the RSS Workshop on Social Intelligence in Humans and Robots, 2023
- **标题**: 唱人体电动：机器人实施例对用户期望的影响
- **领域**: 机器人技术,人工智能,人机交互
- **摘要**: 用户开发了机器人的心理模型，以概念化他们可以与这些机器人进行哪种互动。概念化通常是在与机器人互动之前形成的，仅基于观察机器人的物理设计。结果，必须理解由物理设计形成的概念化，以了解用户打算如何与机器人互动。我们建议使用机器人实施例的多模式特征来预测用户对给定机器人的社交和物理能力的期望。我们表明，使用此类功能提供有关机器人的一般心理模型的信息，这些信息跨社会交互式机器人概括。我们描述了如何为使用社会互动机器人工作的研究人员将这些模型纳入交互设计和物理设计中。

### Relative Pose for Nonrigid Multi-Perspective Cameras: The Static Case 
[[arxiv](https://arxiv.org/abs/2401.09140)] [[cool](https://papers.cool/arxiv/2401.09140)] [[pdf](https://arxiv.org/pdf/2401.09140)]
> **Authors**: Min Li,Jiaqi Yang,Laurent Kneip
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 非辅助多镜相机的相对姿势：静态情况
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 具有潜在的非重叠视野的多角度相机已成为许多应用程序中的重要外观感知方式，例如智能车辆，无人机和混合现实耳机。在这项工作中，我们挑战了这些情况下的基本假设之一，即多摄像机钻机是刚性的。更具体地说，我们正在考虑在不同的空间取向中估算静态非刚性钻机之间相对姿势的问题，同时考虑到重力对系统的影响。每个相机和身体中心之间的可变形物理连接通过简单的悬臂模型近似，并插入了广义的外两极约束中。我们的结果使我们得出了一个重要的见解：变形模型的潜在参数（这意味着两种视图中的重力向量）变得可以观察到。我们对两种不同算法的噪声，异常值和钻机刚度的所有变量的可观察性进行简洁分析。第一个是仅视觉替代方案，而第二个则利用额外的重力测量值。总而言之，我们证明了在现实世界中的示例中感知重力的能力，并讨论了实际含义。

### Autonomous Catheterization with Open-source Simulator and Expert Trajectory 
[[arxiv](https://arxiv.org/abs/2401.09059)] [[cool](https://papers.cool/arxiv/2401.09059)] [[pdf](https://arxiv.org/pdf/2401.09059)]
> **Authors**: Tudor Jianu,Baoru Huang,Tuan Vo,Minh Nhat Vu,Jingxuan Kang,Hoan Nguyen,Olatunji Omisore,Pierre Berthet-Rayne,Sebastiano Fichera,Anh Nguyen
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: Code: https://github.com/airvlab/cathsim
- **标题**: 带有开源模拟器和专家轨迹的自动导管插入术
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 血管内机器人在学术界和工业中都积极开发。然而，封闭源模拟器和物理幻象的广泛使用通常会阻碍自主导管插入术的进展。此外，由于昂贵的医疗程序，收购用于训练机器机器人的大型数据集通常是不可行的。在本章中，我们介绍了Cathsim，Cathsim是第一个用于解决这些局限性的血管内干预的开源模拟器。 Cathsim强调实时性能，以实现学习算法的快速发展和测试。我们验证Cathsim反对真正的机器人，并表明我们的模拟器可以成功模仿真正的机器人的行为。根据Cathsim，我们开发了一个多模式的专家导航网络，并证明了其在下游血管内导航任务中的有效性。密集的实验结果表明，Cathsim有可能在自主导管插入术场中显着加速研究。我们的项目可在https://github.com/airvlab/cathsim上公开获取。

### Adaptive Kalman-Informed Transformer 
[[arxiv](https://arxiv.org/abs/2401.09987)] [[cool](https://papers.cool/arxiv/2401.09987)] [[pdf](https://arxiv.org/pdf/2401.09987)]
> **Authors**: Nadav Cohen,Itzik Klein
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: No comments
- **标题**: 自适应卡尔曼信息变压器
- **领域**: 机器人技术,人工智能,系统与控制
- **摘要**: 扩展的卡尔曼过滤器（EKF）是导航应用中传感器融合的广泛采用方法。 EKF的关键方面是在线确定过程噪声协方差矩阵，反映了模型不确定性。尽管常见的EKF实现假设过程噪声恒定，但在实际情况下，过程噪声会变化，导致估计状态的不准确性，并可能导致过滤器差异。提出了基于模型的自适应EKF方法，并证明了应对这种情况的性能改进，突出了需要强大的自适应方法。在本文中，我们得出了一种自适应的卡尔曼知识变压器（A-KIT），旨在在线学习不同的过程噪声协方差。 A-KIT建立在EKF的基础上，利用了SET变压器的众所周知能力，包括固有的降噪和捕获数据中非线性行为的能力。此方法适用于任何涉及EKF的应用。在一项案例研究中，我们证明了A-KIT在多普勒速度对数和惯性传感器之间非线性融合中的有效性。这是使用从地中海自动水下汽车上的传感器记录的真实数据来完成的。我们表明，A-Kit的表现使常规EKF的表现超过49.5％，基于模型的自适应EKF平均比位置准确性平均高35.4％。

### Multimodal Visual-Tactile Representation Learning through Self-Supervised Contrastive Pre-Training 
[[arxiv](https://arxiv.org/abs/2401.12024)] [[cool](https://papers.cool/arxiv/2401.12024)] [[pdf](https://arxiv.org/pdf/2401.12024)]
> **Authors**: Vedant Dave,Fotios Lygerakis,Elmar Rueckert
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: No comments
- **标题**: 通过自我监督的对比预训练，多模式的视觉训练表示学习
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 机器人技术的快速发展的领域需要采用促进多种方式融合的方法。具体而言，当涉及到与有形对象进行互动时，有效地结合视觉和触觉感觉数据是理解和导航物理世界的复杂动态的关键，从而使对不断变化的环境的更细微和适应性的响应更加细微，更适应能力。然而，在合并这两种感觉方式方面的许多早期工作都依赖于使用人类标记的数据集的监督方法。本文引入了Mvitac，这是一种新型方法，一种利用对比性学习以将视觉和触摸感与自我服务的方式整合和触摸感。通过使用两个感觉输入，MVITAC利用学习表征的内部和模式间损失，从而增强了材料属性分类和更熟练的掌握预测。通过一系列实验，我们展示了我们的方法的有效性及其优于现有的最新自我监管和监督技术。在评估我们的方法论时，我们专注于两个不同的任务：材料分类和掌握成功预测。我们的结果表明，MVITAC促进了改进的模态编码器的发展，从线性探测评估中证明了更强大的表示形式。

### Single and bi-layered 2-D acoustic soft tactile skin (AST2) 
[[arxiv](https://arxiv.org/abs/2401.14292)] [[cool](https://papers.cool/arxiv/2401.14292)] [[pdf](https://arxiv.org/pdf/2401.14292)]
> **Authors**: Vishnu Rajendran,Simon Parsons,Amir Ghalamzan E
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: IEEE Robosoft conference 2024 (accepted)
- **标题**: 单层和双层二-D声学软触觉皮肤（AST2）
- **领域**: 机器人技术,人工智能
- **摘要**: 本文旨在为声学软触觉（AST）皮肤提出创新且具有成本效益的设计，其主要目标是显着提高了2-D触觉功能估计的准确性。现有的挑战在于使用具有成本效益的解决方案实现精确的触觉特征估计，尤其是关于接触几何特性的估计。我们假设，通过在感应表面以下2层中的专用声通道来利用声能并分析振幅调制，我们可以有效地解码在感觉表面上的相互作用，从而改善触觉特征估计。我们的方法涉及负责发出和接收声学信号的硬件组件的独特分离，从而导致模块化且高度可定制的皮肤设计。实际测试证明了这一新型设计的有效性，在估计接触正常力（MAE <0.8 N），2D接触定位（MAE <0.7 mm）和接触表面直径（MAE <0.3 mm）方面达到了显着的精度。总之，AST皮肤具有创新的设计和模块化架构，成功地解决了触觉功能估计的挑战。提出的结果展示了其精确估计各种触觉功能的能力，使其成为机器人应用的实用且具有成本效益的解决方案。

### MResT: Multi-Resolution Sensing for Real-Time Control with Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2401.14502)] [[cool](https://papers.cool/arxiv/2401.14502)] [[pdf](https://arxiv.org/pdf/2401.14502)]
> **Authors**: Saumya Saxena,Mohit Sharma,Oliver Kroemer
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: CoRL'23, Project website: http://tinyurl.com/multi-res-realtime-control
- **标题**: MREST：通过视觉语言模型进行实时控制的多分辨率传感
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **摘要**: 利用各种空间和时间分辨率的传感方式可以提高机器人操纵任务的性能。多空间分辨率传感提供了在不同的空间尺度捕获的层次信息，并可以启用粗略和精确的运动。同时进行多个颞分辨率传感使代理能够表现出高反应性和实时控制。在这项工作中，我们提出了一个框架，MREST（多分辨率变压器），用于学习可推广的语言条件的多任务策略，该策略使用不同能力的网络在不同的空间和时间分辨率下使用感应，以有效地执行精确和反应性任务的实时控制。我们利用现成的预测视觉模型在低频全局特征以及小型非预言模型上运行，以适应高频本地反馈。通过在3个域（粗糙，精确和动态操纵任务）中进行的广泛实验，我们表明，与最近的多任务基准相比，我们的方法显着改善（平均为2倍）。此外，我们的方法很好地概括了目标对象的视觉和几何变化和变化的相互作用力。

### M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation 
[[arxiv](https://arxiv.org/abs/2401.17032)] [[cool](https://papers.cool/arxiv/2401.17032)] [[pdf](https://arxiv.org/pdf/2401.17032)]
> **Authors**: Fotios Lygerakis,Vedant Dave,Elmar Rueckert
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: Project website: https://sites.google.com/view/M2CURL/home
- **标题**: M2CURL：通过自我监督的代表性学习机器人操纵的样品效率多模式增强学习
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **摘要**: 多模式增强学习（RL）最关键的方面之一是不同观察方式的有效整合。从这些模态得出的强大而准确的表示是提高RL算法的鲁棒性和样品效率的关键。但是，RL设置中的Visuotactile数据的学习表示构成了重大挑战，特别是由于数据的高维度以及将视觉和触觉输入与动态环境和任务目标相关联的复杂性。为了应对这些挑战，我们提出了多模式对比度无监督的强化学习（M2CURL）。我们的方法采用了一种新型的多模式自学学习技术，该技术可以学习有效的表示，并有助于更快的RL算法收敛。我们的方法对RL算法不可知，因此可以与任何可用的RL算法进行集成。我们在触觉健身房2模拟器上评估了M2Curl，并表明它可以显着提高不同操纵任务的学习效率。与没有我们表示学习方法的标准RL算法相比，与标准RL算法相比，每集的收敛速度更快和累积奖励更高，这证明了这一点。

## 符号计算(cs.SC:Symbolic Computation)

该领域共有 1 篇论文

### Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework 
[[arxiv](https://arxiv.org/abs/2401.09748)] [[cool](https://papers.cool/arxiv/2401.09748)] [[pdf](https://arxiv.org/pdf/2401.09748)]
> **Authors**: Tianhao Chen,Pengbo Xu,Haibiao Zheng
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: No comments
- **标题**: 引导OTS-FUNCIMG预训练模型（BOTFIP） - 综合符号回归框架
- **领域**: 符号计算,人工智能,机器学习
- **摘要**: 在科学计算领域，许多解决问题的方法倾向于仅关注过程和最终结果，即使在科学的AI中，数据背后也缺乏深层的多模式信息挖掘，而缺少类似于图像text域中的多模式框架。在本文中，我们将符号回归（SR）作为我们的焦点，并从图像培养域中的Blip模型中汲取灵感，提出了基于功能图像（Funcimg）和命名的BootsTrapplate bootstrapplate bootsTrapping OTS-FAMPATE OTS-FUNSTAPPATE OTS-FUNSCIMG-FUNSCIMG预先训练模型（BOTFIP）的科学计算多模式框架（funcimg）。在SR实验中，我们验证了BotFip在低复杂性SR问题中的优势，从而展示了其潜力。作为MED框架，BotFip在更广泛的科学计算问题中对未来的应用有望。

## 声音(cs.SD:Sound)

该领域共有 10 篇论文

### Exploring Multi-Modal Control in Music-Driven Dance Generation 
[[arxiv](https://arxiv.org/abs/2401.01382)] [[cool](https://papers.cool/arxiv/2401.01382)] [[pdf](https://arxiv.org/pdf/2401.01382)]
> **Authors**: Ronghui Li,Yuqin Dai,Yachao Zhang,Jun Li,Jian Yang,Jie Guo,Xiu Li
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-03
> **comment**: No comments
- **标题**: 探索音乐驱动舞蹈一代中的多模式控制
- **领域**: 声音,计算机视觉和模式识别,音频和语音处理
- **摘要**: 现有音乐驱动的3D舞蹈生成方法主要集中于高质量的舞蹈生成，但在一代过程中缺乏足够的控制。为了解决这些问题，我们提出了一个统一的框架，能够产生高质量的舞蹈运动并支持多模式控制，包括流派控制，语义控制和空间控制。首先，我们将舞蹈生成网络从舞蹈控制网络中解脱出来，从而避免添加其他控制信息时舞蹈质量的降解。其次，我们为不同的控制信息设计了特定的控制策略，并将它们集成到统一的框架中。实验结果表明，拟议的舞蹈生成框架在运动质量和可控性方面优于最先进的方法。

### MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition 
[[arxiv](https://arxiv.org/abs/2401.03424)] [[cool](https://papers.cool/arxiv/2401.03424)] [[pdf](https://arxiv.org/pdf/2401.03424)]
> **Authors**: He Wang,Pengcheng Guo,Pan Zhou,Lei Xie
> **First submission**: 2024-01-07
> **First announcement**: 2024-01-08
> **comment**: 5 pages, 3 figures Accepted at ICASSP 2024
- **标题**: MLCA-AVSR：多层交叉注意融合基于视听语音识别
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 尽管自动语音识别（ASR）系统在嘈杂的环境中显着降低，但视听语音识别（AVSR）系统旨在用噪声不变的视觉提示补充音频流并改善系统的鲁棒性。但是，当前的研究主要集中于融合良好的模式特征，例如特定于模态编码器的输出，而无需考虑模式特征学习过程中的上下文关系。在这项研究中，我们提出了一种基于多层跨科融合的AVSR（MLCA-AVSR）方法，该方法通过在不同级别的音频/视觉编码器中融合来促进每种模态的表示。 MISP2022-AVSR挑战数据集的实验结果显示了我们所提出的系统的功效，在评估集中达到了串联的最小置换字符错误率（CPCER）为30.57％，并且与以前的第二个系统相比，在挑战中排名第二。在融合了多个系统之后，我们提出的方法超过了第一名系统，在此数据集中建立了29.13％的新SOTA CPCER。

### Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2401.03000)] [[cool](https://papers.cool/arxiv/2401.03000)] [[pdf](https://arxiv.org/pdf/2401.03000)]
> **Authors**: Muhammad Muaz,Nathan Paull,Jahnavi Malagavalli
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 桥接方式：知识蒸馏和掩盖训练，用于将多模式情感识别转化为单模式，仅语音的情感识别
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **摘要**: 本文提出了一种创新的方法，可以解决将多模式情感识别模型转化为更实用和资源效率的单模式的挑战，特别是专门针对仅语音的情感识别。识别语音信号的情绪是一项至关重要的任务，在人类计算机互动，情感计算和心理健康评估中的应用。但是，现有的最新模型通常依赖于多模式输入，这些输入包括来自多种来源的信息，例如面部表情和手势，在现实情况下可能不容易获得或可行。为了解决这个问题，我们提出了一个新颖的框架，该框架利用知识蒸馏和掩盖的培训技术。

### Music Genre Classification: A Comparative Analysis of CNN and XGBoost Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms 
[[arxiv](https://arxiv.org/abs/2401.04737)] [[cool](https://papers.cool/arxiv/2401.04737)] [[pdf](https://arxiv.org/pdf/2401.04737)]
> **Authors**: Yigang Meng
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-10
> **comment**: No comments
- **标题**: 音乐流派分类：使用MEL频率的CNN和XGBoost方法进行比较分析
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 近年来，各种精心设计的算法都授权音乐平台根据一个人的喜好提供内容。音乐流派是通过各个方面定义的，包括声学特征和文化考虑。音乐流派分类与基于内容的过滤效果很好，该过滤建议基于音乐与用户相似的内容。鉴于一个相当大的数据集，一个前提是使用机器学习或深度学习方法自动注释，可以有效地对音频文件进行分类。系统的有效性在很大程度上取决于功能和模型选择，因为不同的架构和特征可以互相促进并产生不同的结果。在这项研究中，我们进行了一项比较研究，研究了三种模型的性能：拟议的卷积神经网络（CNN），具有完全连接的层（FC）的VGG16，以及在不同特征上的极端梯度提升（XGBOOST）方法：30秒的MEL Spectrampon图和3秒钟的Mel-Feel-Frequencence Cepency Cepstral Ceeffseft（Mfccs）。结果表明，MFCC XGBoost模型的表现优于其他模型。此外，在数据预处理阶段应用数据分割可以显着提高CNN的性能。

### Ultra-lightweight Neural Differential DSP Vocoder For High Quality Speech Synthesis 
[[arxiv](https://arxiv.org/abs/2401.10460)] [[cool](https://papers.cool/arxiv/2401.10460)] [[pdf](https://arxiv.org/pdf/2401.10460)]
> **Authors**: Prabhav Agrawal,Thilo Koehler,Zhiping Xiu,Prashant Serai,Qing He
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: Accepted for ICASSP 2024
- **标题**: 高质量语音综合的超轻量级神经差异DSP Vocoder
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 神经声码器对原始音频波形进行建模并合成高质量的音频，但是即使是MB-Melgan和LPCNet等高效的音频，也无法在诸如SmartGlass这样的低端设备上实时运行。纯数字信号处理（DSP）的Vocoder可以通过轻质快速傅立叶变换（FFT）实现，因此，比任何神经声码器都要快。 DSP Vocoder通常会由于消耗了声带的近似表示的过度平滑声学模型预测，因此通常会获得较低的音频质量。在本文中，我们提出了一种超轻质差异DSP（DDSP）VOCODER，该Vocoder使用与DSP Vocoder共同优化的声学模型，并在没有提取的声音谱系功能的情况下学习。该模型的音频质量可与神经声码器相当，高平均MO为4.36，同时作为DSP Vocoder有效。我们的C ++实施无需任何硬件特异性优化，在15 mflops上，在拖鞋方面超过了MB-Melgan的340次，并且仅在2GHZ Intel Xeon Xeon Xeon CPU上运行单线读取，仅实现仅VOCODER的RTF，总RTF为0.044。

### Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition 
[[arxiv](https://arxiv.org/abs/2401.09880)] [[cool](https://papers.cool/arxiv/2401.09880)] [[pdf](https://arxiv.org/pdf/2401.09880)]
> **Authors**: Fréjus A. A. Laleye,Mikaël A. Mousse
> **First submission**: 2024-01-18
> **First announcement**: 2024-01-19
> **comment**: No comments
- **标题**: 基于注意力的自动行为的复发性神经网络，使母鸡识别
- **领域**: 声音,人工智能,计算语言学,机器学习,音频和语音处理
- **摘要**: 现代家禽养殖的利益之一是养生母鸡的发声，其中包含有关健康行为的非常有用的信息。该信息被用作健康和福祉指标，可帮助育种者更好地监测母鸡，这涉及早期发现问题以进行快速，更有效的干预措施。在这项工作中，我们将重点放在识别饲养母鸡的呼叫类型的声音分析上，以提出强大的对其行为表征的系统，以进行更好的监视。为此，我们首先收集并注释了Hen Call信号，然后根据时间和频域特征的组合设计了最佳的声学表征。然后，我们使用这些功能来构建基于复发性神经网络的多标签分类模型，以将语义类别分配给表征躺着的母鸡行为的发声。结果表明，基于时间和频域特征的组合，获得了我们模型的总体性能，该特征的结合使用了频率域特征，该模型获得了最高的F1分数（F1 = 92.75），在模型上获得了17％的增长，而在比较的垃圾方法中，该模型的增长率为8％。

### Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach 
[[arxiv](https://arxiv.org/abs/2401.10747)] [[cool](https://papers.cool/arxiv/2401.10747)] [[pdf](https://arxiv.org/pdf/2401.10747)]
> **Authors**: Weide Liu,Huijing Zhan,Hao Chen,Fengmao Lv
> **First submission**: 2023-12-28
> **First announcement**: 2024-01-22
> **comment**: We request to withdraw our paper from the archive due to significant errors identified in the analysis and conclusions. Upon further review, we realized that these errors undermine the validity of our findings. We plan to conduct additional research to correct these issues and resubmit a revised version in the future
- **标题**: 多模式的情感分析，缺少模式：一种知识转移方法
- **领域**: 声音,人工智能,计算语言学,机器学习,音频和语音处理
- **摘要**: 多模式情感分析旨在通过视觉，语言和声学提示来确定个人表达的情绪。但是，大多数现有的研究工作都假定所有模式在培训和测试过程中都可以使用，这使其算法容易受到缺失的模态情况的影响。在本文中，我们提出了一个新颖的知识转移网络，以在不同的方式之间翻译不同的模式，以重建缺失的音频方式。此外，我们开发了一种跨模式注意机制，以保留重建和观察到的情感预测方式的最大信息。在三个公开数据集上进行的广泛实验表明，对基准进行了显着改进，并获得了与以前的多种模式监督相当的结果。

### Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting 
[[arxiv](https://arxiv.org/abs/2401.13498)] [[cool](https://papers.cool/arxiv/2401.13498)] [[pdf](https://arxiv.org/pdf/2401.13498)]
> **Authors**: Hounsu Kim,Soonbeom Choi,Juhan Nam
> **First submission**: 2024-01-24
> **First announcement**: 2024-01-25
> **comment**: Accepted to ICASSP 2024
- **标题**: 具有特异性输入表示和扩散支出的表达式吉他声音综合
- **领域**: 声音,人工智能,机器学习,音频和语音处理,信号处理
- **摘要**: 综合性能吉他声音是一项高度挑战性的任务，这是由于表达式的多态和较高的变异性。最近，经常使用通用MIDI输入，深层生成模型在合成音乐得分的表达性复音乐器声音方面表现出了令人鼓舞的结果。在这项工作中，我们提出了一种具有自定义的输入表示仪器的表达式吉他声音合成模型，我们称之为吉特罗。我们使用基于扩散的门廊实施建议的方法，该方法可以长期一致性生成音频。为了克服缺乏MIDI/AUDIOPAIRED数据集，我们不仅使用了现有的吉他数据集，而且还使用了从高质量的基于样本的吉他合成器中收集的数据。通过定量和定性评估，我们表明我们所提出的模型具有比基线模型更高的音频质量，并且比以前的主要作品产生更现实的音色声音。

### AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations 
[[arxiv](https://arxiv.org/abs/2401.15164)] [[cool](https://papers.cool/arxiv/2401.15164)] [[pdf](https://arxiv.org/pdf/2401.15164)]
> **Authors**: Naresh Kumar Devulapally,Sidharth Anand,Sreyasee Das Bhattacharjee,Junsong Yuan,Yu-Ping Chang
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: No comments
- **标题**: 娱乐：在小组对话中，对说话者情感识别的自适应多模式分析
- **领域**: 声音,计算机视觉和模式识别,机器学习,多媒体,音频和语音处理
- **摘要**: 分析小组对话期间的个人情绪对于发展能够自然人机相互作用的智能代理至关重要。尽管可靠的情绪识别技术取决于不同的方式（文本，音频，视频），但这些方式与受个人独特的行为模式影响的动态跨模式相互作用之间的固有异质性使情感识别的任务非常具有挑战性。在小组设置中，这种困难变得更加复杂，在小组设置中，情感及其时间的演变不仅受个人的影响，而且还受观众反应和正在进行的对话的背景等外部环境的影响。为了应对这一挑战，我们提出了一个多模式的注意网络，该网络通过共同学习其交互式特定模式的外围和中央网络来捕获各种空间抽象的跨模式相互作用。提出的人通过其外围键值对在模式特异性中央查询网络的每一层中注入跨模式的关注。然后使用自适应融合技术组合所得的交叉模式特异性描述符，该技术使模型能够将判别性和互补模式特异性的数据模式集成到实例特定的多模式描述符中。鉴于以一系列话语表示的对话，拟议的娱乐模型将空间和时间特征凝结成两个密集的描述符：扬声器级别和话语级别。这不仅有助于提供更好的分类性能（大规模公共数据集中的加权F1提高3-5％，准确性提高了5-7％），而且还可以帮助用户了解模型通过其多模式解释性可视化模块进行的每个情绪预测背后的推理。

### PBSCR: The Piano Bootleg Score Composer Recognition Dataset 
[[arxiv](https://arxiv.org/abs/2401.16803)] [[cool](https://papers.cool/arxiv/2401.16803)] [[pdf](https://arxiv.org/pdf/2401.16803)]
> **Authors**: Arhan Jain,Alec Bunn,Austin Pham,TJ Tsai
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: 19 pages, 6 figures, to be published in Transactions of the International Society for Music Information Retrieval
- **标题**: PBSCR：钢琴盗版得分作曲家识别数据集
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 本文激励，描述并介绍了PBSCR数据集，用于研究作曲家对古典钢琴音乐的认可。我们的目标是设计一个数据集，该数据集促进了适合现代建筑和培训实践的作曲家认可的大规模研究。为了实现这一目标，我们在IMSLP上利用了丰富的乐谱图像和丰富的元数据，使用先前提出的功能表示形式称为Bootleg Score来编码相对于员工线的表情位置，并以极为简单的格式（2D二进制图像）呈现数据来鼓励快速探索和迭代。该数据集本身包含40,000个62x64盗版得分图像，用于9级识别任务，100,000 62x64盗版得分图像100级识别任务的盗版得分图像以及29,310个未标记的可变长度练习式得分图像。标记的数据以反映MNIST图像的形式呈现，以使其非常容易以有效的方式可视化，操纵和训练模型。我们包括相关信息，将每个盗版得分图像与其基本的原始乐谱图像连接起来，我们在所有钢琴上从IMSLP刮擦，组织和编译元数据，以促进多模式研究，并允许方便链接到其他数据集。我们发布了基线的结果，可以进行监督和低弹性的设置，以供将来的工作与之相比，我们讨论了PBSCR数据特别适合促进研究的开放研究问题。

## 社交和信息网络(cs.SI:Social and Information Networks)

该领域共有 1 篇论文

### VGA: Vision and Graph Fused Attention Network for Rumor Detection 
[[arxiv](https://arxiv.org/abs/2401.01759)] [[cool](https://papers.cool/arxiv/2401.01759)] [[pdf](https://arxiv.org/pdf/2401.01759)]
> **Authors**: Lin Bai,Caiyan Jia,Ziying Song,Chaoqun Cui
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: VGA：视觉和图形融合注意力网络以进行谣言检测
- **领域**: 社交和信息网络,计算语言学,计算机视觉和模式识别,多媒体
- **摘要**: 随着社交媒体的发展，谣言已经在社交媒体平台上广泛传播，对社会造成了巨大伤害。除了文本信息外，许多谣言还使用操纵的图像或在图像中隐藏文本信息来欺骗人们并避免被检测到，这使得多模式的谣言检测是一个关键问题。大多数多模式谣言检测方法主要集中于提取源主张及其相应图像的特征，同时忽略了谣言及其传播结构的评论。这些评论和结构暗示着人群的智慧，被证明对揭露谣言至关重要。此外，这些方法通常仅以基本方式提取视觉特征，很少考虑图像中的篡改或文本信息。因此，在这项研究中，我们提出了一个新颖的视野和图形融合的注意网络（VGA），以谣言检测来利用帖子之间的传播结构，以获取人群的意见并进一步探索视觉篡改的特征，以及隐藏在图像中的文本信息。我们在三个数据集上进行了广泛的实验，表明VGA可以有效地检测多模式的谣言，并且胜过最先进的方法。

## 音频和语音处理(eess.AS:Audio and Speech Processing)

该领域共有 8 篇论文

### Detecting the presence of sperm whales echolocation clicks in noisy environments 
[[arxiv](https://arxiv.org/abs/2401.00900)] [[cool](https://papers.cool/arxiv/2401.00900)] [[pdf](https://arxiv.org/pdf/2401.00900)]
> **Authors**: Guy Gubnitsky,Roee Diamant
> **First submission**: 2023-12-31
> **First announcement**: 2024-01-02
> **comment**: 10 pages and 10 figures
- **标题**: 在嘈杂的环境中检测晶石鲸的存在回声定位点击
- **领域**: 音频和语音处理,机器学习,声音
- **摘要**: Sperm Whales（Physter Macrocephalus）带有一系列冲动，单击的声音，称为回声定位点击。这些点击的特征是多元结构（MPS），该结构是一种独特的模式。在这项工作中，我们将MPS的稳定性用作检测指标，以识别和分类噪声环境中的点击存在。为了区分噪声瞬变并处理与多个抹香鲸的同时发射，我们的方法簇簇一个时间序列的MPS测量，同时删除了无法满足点相互间隔，持续时间和光谱限制的潜在点击。结果，我们的方法可以处理高噪声瞬变和低信噪比。使用三个数据集检查了我们的检测方法的性能：来自地中海的七个月记录，其中包含手动验证的环境噪声；从多米尼加岛收集的几天手动标记数据，其中包含多个抹香鲸的大约40,000次点击；以及来自巴哈马的数据集，其中包含1,203个单一语鲸的单击。与两个基准检测器的结果相比，观察到精确和召回之间的更好的权衡以及虚假检测率的显着降低，尤其是在嘈杂的环境中。为了确保可重复性，我们将提供标记点击的数据库以及实施代码。

### Enhancing dysarthria speech feature representation with empirical mode decomposition and Walsh-Hadamard transform 
[[arxiv](https://arxiv.org/abs/2401.00225)] [[cool](https://papers.cool/arxiv/2401.00225)] [[pdf](https://arxiv.org/pdf/2401.00225)]
> **Authors**: Ting Zhu,Shufei Duan,Camille Dingam,Huizhi Liang,Wei Zhang
> **First submission**: 2023-12-30
> **First announcement**: 2024-01-02
> **comment**: No comments
- **标题**: 通过经验模式分解和Walsh-Hadamard变换增强构音词语音特征代表
- **领域**: 音频和语音处理,人工智能,信号处理
- **摘要**: 质心言语包含声带和声带的病理特征，但到目前为止，它们尚未包含在传统的声学特征集中。而且，语音的非线性和非平稳性被忽略了。在本文中，我们提出了一种称为Whfemd的构音障碍语音的功能增强算法。它结合了经验模式分解（EMD）和快速的Walsh-Hadamard变换（FWHT），以增强特征。使用提出的算法，首先执行构音障碍语音的快速傅立叶变换，然后进行EMD以获得内在模式函数（IMF）。之后，FWHT用于输出新系数，并根据IMF，功率频谱密度和增强的Gammatone频率Cepstral系数提取统计特征。为了评估所提出的方法，我们对包括UA语音和Torgo在内的两个公共病理语音数据库进行了实验。结果表明，我们的算法在分类中的表现要好。我们分别取得了13.8％（UA语音）和3.84％（Torgo）的改善。此外，将不平衡的分类算法掺入以解决数据不平衡已导致识别精度提高12.18％。该算法有效地解决了违反语音中不平衡数据集和非线性的挑战，并同时提供了对声带和区域的局部病理特征的强大表示。

### A unified multichannel far-field speech recognition system: combining neural beamforming with attention based end-to-end model 
[[arxiv](https://arxiv.org/abs/2401.02673)] [[cool](https://papers.cool/arxiv/2401.02673)] [[pdf](https://arxiv.org/pdf/2401.02673)]
> **Authors**: Dongdi Zhao,Jianbo Ma,Lu Lu,Jinke Li,Xuan Ji,Lei Zhu,Fuming Fang,Ming Liu,Feijun Jiang
> **First submission**: 2024-01-05
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 统一的多通道远场语音识别系统：将神经光束与基于注意的端到端模型相结合
- **领域**: 音频和语音处理,人工智能,声音
- **摘要**: 远场语音识别是一项具有挑战性的任务，通常使用信号处理波束形成噪声和干扰问题。但是由于对环境假设的严重依赖，通常发现该性能通常受到限制。在本文中，我们提出了一个统一的多通道远场语音识别系统，该系统结合了基于神经和变压器的聆听，咒语，参加（LAS）语音识别系统，该系统将进一步扩展到端到端的语音识别系统，以进一步包括语音增强。然后，将共同培训此类框架以优化感兴趣的最终目标。具体而言，已经采用了分量的复杂线性投影（FCLP）来形成神经光束。然后比较几种结合外观方向的汇总策略，以找到最佳方法。此外，源方向的信息还集成在波束形成中，以探索源方向作为先验的有用性，通常在多模式的情况下可用。进行了不同麦克风阵列几何形状的实验，以评估针对麦克风阵列间距方差的鲁棒性。大型内部数据库用于评估所提出的框架的有效性，与强大的基线相比，所提出的方法可获得19.26 \％的改善。

### Construction and Evaluation of Mandarin Multimodal Emotional Speech Database 
[[arxiv](https://arxiv.org/abs/2401.07336)] [[cool](https://papers.cool/arxiv/2401.07336)] [[pdf](https://arxiv.org/pdf/2401.07336)]
> **Authors**: Zhu Ting,Li Liangqi,Duan Shufei,Zhang Xueying,Xiao Zhongzhe,Jia Hairng,Liang Huizhi
> **First submission**: 2024-01-14
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 普通话多模式情感语音数据库的构建和评估
- **领域**: 音频和语音处理,人工智能,声音,信号处理
- **摘要**: 设计和建立了一个多模式的情感语音普通话数据库，包括发音运动学，声学，声门和面部微表达式，从语料库设计，主题选择，记录详细信息和数据处理的方面进行了详细的详细介绍。信号上标有离散的情感标签（中性，快乐，冷漠，愤怒，悲伤，悲伤）和维度情感标签（愉悦，唤醒，统治）。在本文中，维度注释的有效性通过维数注释数据的统计分析来验证。验证注释者的SCL-90量表数据已验证并与PAD注释数据进行分析，以探索注释中异常现象与注释者心理状态之间的内部关系。为了验证数据库的语音质量和情感歧视，本文使用3种基本模型，CNN和DNN的基本模型来计算这七种情绪的识别率。结果表明，单独使用声学数据时，七个情绪的平均识别率约为82％。单独使用震颤数据时，平均识别率约为72％。仅使用运动学数据，平均识别率也达到55.7％。因此，数据库具有高质量，可以用作语音分析研究的重要来源，尤其是用于多模式情感语音分析的任务。

### Multilingual acoustic word embeddings for zero-resource languages 
[[arxiv](https://arxiv.org/abs/2401.10543)] [[cool](https://papers.cool/arxiv/2401.10543)] [[pdf](https://arxiv.org/pdf/2401.10543)]
> **Authors**: Christiaan Jacobs
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-22
> **comment**: PhD thesis
- **标题**: 零资源语言的多语言声词嵌入
- **领域**: 音频和语音处理,计算语言学,声音
- **摘要**: 这项研究解决了为缺乏标记数据的零资源语言开发语音应用的挑战。它专门使用声词嵌入（awe） - 可变持续性语音段的固定维表示 - 采用多语言转移，其中使用了几种资源丰富的语言的标记数据。该研究介绍了一个新的神经网络，该网络的表现优于零资源语言的现有敬畏模型。它探索了选择资源良好的语言的影响。 Awes应用于在斯瓦希里语无线电广播中用于仇恨言语检测的关键字点介绍系统，在现实世界中证明了稳健性。此外，新颖的语义敬畏模型可改善示例搜索的语义查询。

### Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms 
[[arxiv](https://arxiv.org/abs/2401.12238)] [[cool](https://papers.cool/arxiv/2401.12238)] [[pdf](https://arxiv.org/pdf/2401.12238)]
> **Authors**: Iran R. Roman,Christopher Ick,Sivan Ding,Adrian S. Roman,Brian McFee,Juan P. Bello
> **First submission**: 2024-01-19
> **First announcement**: 2024-01-23
> **comment**: 5 pages, 4 figures, 1 table, to be presented at ICASSP 2024 in Seoul, South Korea
- **标题**: 空间Scaper：一个库，用于模拟和增强声音景观，以在逼真的房间内定位和检测
- **领域**: 音频和语音处理,机器学习,声音
- **摘要**: 声音事件的本地化和检测（SELD）是机器侦听的重要任务。重大进步依赖于模拟数据，并在特定房间中具有合理的事件和强烈的时空标签。通过将空间定位的房间脉冲响应（RIRS）带有声波形以将声音事件放置在音景中，可以模拟SELD数据。但是，RIR需要在特定房间内进行手动收集。我们提出了空间Scaper，这是一个用于SELD数据模拟和增强的库。与现有的工具相比，空间雪人通过尺寸和墙壁吸收等参数模仿虚拟房间。这允许前景和背景音源的参数化位置（包括运动）。空间示例还包括可应用于现有数据的数据增强管道。作为一个案例研究，我们使用空间扫描件将房间添加到dcase seld数据中。通过我们的数据训练模型，导致渐进式绩效提高了声学多样性的直接功能。这些结果表明，空间Scaper对于训练强大的SELD模型很有价值。

### Acoustic characterization of speech rhythm: going beyond metrics with recurrent neural networks 
[[arxiv](https://arxiv.org/abs/2401.14416)] [[cool](https://papers.cool/arxiv/2401.14416)] [[pdf](https://arxiv.org/pdf/2401.14416)]
> **Authors**: François Deloche,Laurent Bonnasse-Gahot,Judit Gervain
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-26
> **comment**: 15 pages, 7 figures
- **标题**: 语音节奏的声学特征：超越重复神经网络的指标
- **领域**: 音频和语音处理,机器学习,声音
- **摘要**: 长期以来根据其感知的节奏属性描述了语言。相关的类型论在心理语言学中引起了人们的关注，因为它们部分预测了新生儿歧视语言的能力，并提供了有关成年听众如何处理非本地语言的见解。尽管节奏指标在支持语言节奏类别的存在方面取得了相对的成功，但定量研究尚未捕获与语音节奏相关的时间正常的全部复杂性。我们认为，深度学习提供了一种强大的模式识别方法，以推动语音节奏的声学基础的特征。为了探讨这一假设，我们通过21种语言的大型语音记录数据库在语言标识任务上培训了一个中型的复发性神经网络。该网络可以访问振幅信封和识别声音段的变量，假设该信号传达了语音信息，但可以保留韵律特征。该网络能够在40％的情况下识别10秒记录的语言，并且该语言在三分之二的情况下是前3个猜测。可视化方法表明，通过网络激活构建的表示与语音节奏类型一致，尽管所产生的地图比压力和音节定时式语言之间的两个分离的簇要复杂。我们通过确定网络激活与已知语音节奏指标之间的相关性进一步分析了该模型。这些发现说明了深度学习工具的潜力，可以通过识别和探索语言相关的声学特征空间来提高我们对语音节奏的理解。

### Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers 
[[arxiv](https://arxiv.org/abs/2401.15018)] [[cool](https://papers.cool/arxiv/2401.15018)] [[pdf](https://arxiv.org/pdf/2401.15018)]
> **Authors**: Kerlos Atia Abdalmalak,Ascensión Gallardo-Antol'in
> **First submission**: 2024-01-26
> **First announcement**: 2024-01-29
> **comment**: ef:Neural Computing and Applications 29 (2018) 637-651
- **标题**: 通过使用功能组合和并行结构分类器来增强独立于文本的扬声器验证系统
- **领域**: 音频和语音处理,人工智能,机器学习,声音
- **摘要**: 说话者验证（SV）系统主要涉及两个单独的阶段：特征提取和分类。在本文中，我们探讨了这两个模块，目的是在嘈杂条件下提高说话者验证系统的性能。一方面，最合适的声学特征的选择是执行强大扬声器验证的关键因素。拟议系统中使用的声学参数为：MEL频率sepstral系数（MFCC），其第一和第二个衍生物（Deltas和Delta-deltas），树皮频率CEPSTRAL系数（BFCC），感知性线性预测（PLP），以及相对频谱变换 - 感知线性预测 -  Rasta-Plasta-Plasta-Plasta-plpp。在本文中，讨论了先前功能的不同组合的完整比较。另一方面，常规支持向量机（SVM）分类器的主要弱点是使用通用传统内核函数来计算数据点之间的距离。但是，SVM的内核功能对其性能有很大影响。在这项工作中，我们提出了两个基于SVM的分类器与不同内核函数的组合：线性内核和高斯径向基函数（RBF）内核与逻辑回归（LR）分类器。该组合是通过平行结构方法进行的，其中考虑采取最终决定的不同投票规则。结果表明，通过将合并的特征与合并的分类器一起使用干净的语音或在噪声的存在下，可以显着改善SV系统的性能。最后，为了在嘈杂的环境中更加增强系统，提出了将多音噪声去除技术包含作为预处理阶段的。

## 图像和视频处理(eess.IV:Image and Video Processing)

该领域共有 15 篇论文

### MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for Chest X-Ray Image Classification 
[[arxiv](https://arxiv.org/abs/2401.00728)] [[cool](https://papers.cool/arxiv/2401.00728)] [[pdf](https://arxiv.org/pdf/2401.00728)]
> **Authors**: Saurabh Agarwal,K. V. Arya,Yogesh Kumar Meena
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-02
> **comment**: 19 pages
- **标题**: MultifusionNet：深神经网络的多模式融合用于胸部X射线图像分类
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 胸部X射线成像是用于识别肺部疾病的关键诊断工具。但是，这些图像的手动解释是耗时的，容易出错。利用卷积神经网络（CNN）的自动化系统在提高胸部X射线图像分类的准确性和效率方面表现出了希望。虽然先前的工作主要集中在最终卷积层中使用特征图，但有必要探索利用其他层来改善疾病分类的好处。从有限的医疗图像数据集中提取强大的功能仍然是一个至关重要的挑战。在本文中，我们提出了一种新型的基于深度学习的多层融合模型，该模型强调从不同层中提取特征并融合它们。我们的疾病检测模型考虑了每一层捕获的歧视性信息。此外，我们提出了不同大小的特征图（FDSFM）模块的融合，以有效合并不同层的特征图。提出的模型分别达到三级和两级分类的明显更高的精度为97.21％和99.60％。拟议的多层多模式融合模型以及FDSFM模块具有准确的疾病分类的希望，也可以扩展到胸部X射线图像中的其他疾病分类。

### Modality Exchange Network for Retinogeniculate Visual Pathway Segmentation 
[[arxiv](https://arxiv.org/abs/2401.01685)] [[cool](https://papers.cool/arxiv/2401.01685)] [[pdf](https://arxiv.org/pdf/2401.01685)]
> **Authors**: Hua Han,Cheng Li,Lei Xie,Yuanjing Feng,Alou Diakite,Shanshan Wang
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 视网膜式视觉途径分段的模态交换网络
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 通过确定途径内的破坏或异常，对视网膜生成视觉途径（RGVP）的准确分割有助于诊断和治疗视觉障碍。但是，RGVP的复杂解剖结构和连通性使得实现准确的分割具有挑战性。在这项研究中，我们提出了一个新型的模态交换网络（ME-NET），该网络有效地利用了多模式磁共振（MR）成像信息来增强RGVP分割。我们的ME-NET有两个主要贡献。首先，我们引入了一种有效的多模式软交换技术。具体而言，我们设计了一个通道和空间混合的注意模块，以在T1加权和分数各向异性MR图像之间交换模态信息。其次，我们提出了一个交叉融合模块，该模块进一步增强了两种方式之间的信息融合。实验结果表明，就RGVP分割性能而言，我们的方法优于现有的最新方法。

### LESEN: Label-Efficient deep learning for Multi-parametric MRI-based Visual Pathway Segmentation 
[[arxiv](https://arxiv.org/abs/2401.01654)] [[cool](https://papers.cool/arxiv/2401.01654)] [[pdf](https://arxiv.org/pdf/2401.01654)]
> **Authors**: Alou Diakite,Cheng Li,Lei Xie,Yuanjing Feng,Hua Han,Shanshan Wang
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: Lesen：基于多参数MRI的视觉途径分割的标签有效的深度学习
- **领域**: 图像和视频处理,机器学习
- **摘要**: 最近的研究表明，在基于多参数MRI的视觉途径（VP）分割中深度学习的潜力。但是，获取标记的数据进行培训是费力且耗时的。因此，在标记样品有限的情况下开发有效的算法至关重要。在这项工作中，我们提出了一种具有自我同步（Lesen）的标签有效的深度学习方法。 Lesen纳入了受监督和无监督的损失，使学生和教师的模型能够相互学习，形成一个自我调整的教师框架。此外，我们引入了可靠的未标记样品选择（RUSS）机制，以进一步提高Lesen的有效性。与最新技术相比，我们对人类连接项目（HCP）数据集的实验证明了我们方法的出色性能，从而在临床和研究环境中进行了多模式VP细分，以全面分析。该实施代码将在以下网址提供：https：//github.com/aldiak/semi-supervised-multimodal-visual-pathway- delineation。

### Multi-modal Learning with Missing Modality in Predicting Axillary Lymph Node Metastasis 
[[arxiv](https://arxiv.org/abs/2401.01553)] [[cool](https://papers.cool/arxiv/2401.01553)] [[pdf](https://arxiv.org/pdf/2401.01553)]
> **Authors**: Shichuan Zhang,Sunyi Zheng,Zhongyi Shui,Honglin Li,Lin Yang
> **First submission**: 2024-01-03
> **First announcement**: 2024-01-04
> **comment**: No comments
- **标题**: 在预测腋窝淋巴结转移时缺少模态的多模式学习
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 多模式学习在医学图像分析中引起了广泛的关注。使用多模式数据，整个幻灯片图像（WSI）和临床信息可以改善深度学习模型在腋窝淋巴结转移的诊断中的性能。但是，由于隐私问题，资源有限，缺乏互操作性等，临床信息在临床实践中并不容易收集。尽管患者的选择可以确保培训设置为具有模型开发的多模式数据，但在测试期间可能会出现临床信息的缺失。这通常会导致性能降解，这限制了诊所中多模式的使用。为了减轻这个问题，我们提出了一个双向蒸馏框架，该框架由多模式分支和单模式分支组成。单模式分支从多模式分支中获取完整的多模式知识，而多模式从单模式中学习了WSI的强大特征。我们在早期乳腺癌的淋巴结转移的公共数据集上进行实验，以验证该方法。我们的方法不仅可以在测试集上以0.861的AUC实现最先进的性能，而无需丢失数据，而且当缺失模态的速率为80 \％时，AUC的AUC为0.842。这显示了该方法在处理多模式数据和缺失模式方面的有效性。这种模型有可能改善具有腋窝淋巴转移性状态的早期乳腺癌患者的治疗决策。

### Demonstration of an Adversarial Attack Against a Multimodal Vision Language Model for Pathology Imaging 
[[arxiv](https://arxiv.org/abs/2401.02565)] [[cool](https://papers.cool/arxiv/2401.02565)] [[pdf](https://arxiv.org/pdf/2401.02565)]
> **Authors**: Poojitha Thota,Jai Prakash Veerla,Partha Sai Guttikonda,Mohammad S. Nasr,Shirin Nilizadeh,Jacob M. Luber
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: No comments
- **标题**: 表现出对病理成像多模式视觉语言模型的对抗性攻击
- **领域**: 图像和视频处理,计算机视觉和模式识别,组织和器官
- **摘要**: 在医学人工智能的背景下，本研究探讨了在有针对性攻击下的病理学语言图像预处理（PLIP）模型，即视觉语言基础模型。我们的调查利用九种组织类型的凯瑟结肠数据集使用7,180个H＆E图像，采用了预计的梯度下降（PGD）对抗扰动攻击，以故意引起错误分类。结果表明，在操纵PLIP的预测方面取得了100％的成功率，强调了其对对抗性扰动的敏感性。对抗性实例的定性分析深入研究了可解释性挑战，阐明了对抗性操纵引起的预测的细微变化。这些发现有助于对医学成像中视觉语言模型的可解释性，领域的适应性和可信度的关键见解。该研究强调了强大防御能力的紧迫需求，以确保AI模型的可靠性。可以在https://github.com/jaiprakash1824/vlm_adv_attack上找到此实验的源代码。

### Segment Anything Model for Medical Image Segmentation: Current Applications and Future Directions 
[[arxiv](https://arxiv.org/abs/2401.03495)] [[cool](https://papers.cool/arxiv/2401.03495)] [[pdf](https://arxiv.org/pdf/2401.03495)]
> **Authors**: Yichi Zhang,Zhenrong Shen,Rushi Jiao
> **First submission**: 2024-01-07
> **First announcement**: 2024-01-08
> **comment**: No comments
- **标题**: 细分医学图像细分模型：当前的应用和未来方向
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 由于提示的固有灵活性，基础模型已成为自然语言处理和计算机视觉领域的主要力量。最近引入了任何模型（SAM），表示及时驱动的范式的显着扩展到图像分割的域中，从而引入了许多先前未探索的功能。但是，鉴于自然图像和医学图像之间存在很大的区别，其在医学图像分割中的应用仍然不确定。在这项工作中，我们提供了旨在扩展SAM对医学图像分割任务的功效的最新努力的全面概述，涵盖了经验基准和方法论的适应性。此外，我们探索了SAM在医学图像细分中的作用的未来研究方向的潜在途径。虽然将SAM直接应用于医疗图像分割并未在到目前为止的多模式和多目标医学数据集上产生令人满意的性能，但从这些努力中收集的许多见解是塑造医学图像分析领域基础模型的轨迹的宝贵指导。为了支持正在进行的研究努力，我们维护了一个活跃的存储库，该存储库包含最新的纸张列表和https://github.com/yichizhang98/sam4mis的开源项目的简洁摘要。

### HA-HI: Synergising fMRI and DTI through Hierarchical Alignments and Hierarchical Interactions for Mild Cognitive Impairment Diagnosis 
[[arxiv](https://arxiv.org/abs/2401.06780)] [[cool](https://papers.cool/arxiv/2401.06780)] [[pdf](https://arxiv.org/pdf/2401.06780)]
> **Authors**: Xiongri Shen,Zhenxi Song,Linling Li,Min Zhang,Lingyan Liang Honghai Liu,Demao Deng,Zhiguo Zhang
> **First submission**: 2024-01-02
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: ha-hi：通过层次对齐和层次相互作用协同fMRI和DTI，用于轻度认知障碍诊断
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 使用多模式磁共振成像（MRI）的轻度认知障碍（MCI）和主观认知下降（SCD）的早期诊断是研究的关键领域。尽管已采用了功能性MRI（fMRI）和扩散张量成像（DTI）的各种区域和连通性特征来开发诊断模型，但大多数研究都将这些特征整合在一起而没有充分解决它们的一致性和相互作用。这限制了完全利用联合特征和方式的协同贡献的潜力。为了解决这一差距，我们的研究介绍了用于MCI和SCD分类的新型分层比对（HA-HI）方法，利用了fMRI和DTI的综合优势。 Ha-hi通过对齐各种特征类型并层次最大化其相互作用来有效地学习重要的MCI-或SCD相关的区域和连通性特征。此外，为了增强我们方法的可解释性，我们开发了协同激活图（SAM）技术，揭示了指示MCI/SCD的关键大脑区域和连接。对ADNI数据集和我们自我收集的数据的全面评估表明，Ha-Hi在诊断MCI和SCD方面胜过其他现有方法，使其成为早期检测的潜在至关重要且可解释的工具。该方法的实现可在https://github.com/ici-bci/dual-mri-ha-hi.git上公开访问。

### Multimodal Neuroimaging Attention-Based architecture for Cognitive Decline Prediction 
[[arxiv](https://arxiv.org/abs/2401.06777)] [[cool](https://papers.cool/arxiv/2401.06777)] [[pdf](https://arxiv.org/pdf/2401.06777)]
> **Authors**: Jamie Vo,Naeha Sharif,Ghulam Mubashar Hassan
> **First submission**: 2023-12-20
> **First announcement**: 2024-01-15
> **comment**: No comments
- **标题**: 多模式神经成像基于注意力的认知下降预测的体系结构
- **领域**: 图像和视频处理,人工智能
- **摘要**: 早期发现阿尔茨海默氏病必须确保早期治疗并改善患者预后。因此，已经进行了广大研究，以检测AD及其中间阶段，轻度认知障碍（MCI）。但是，从正常的认知条件下预测向AD和MCI的转化有很小的文献。最近，多项研究应用了卷积神经网络（CNN），这些神经网络（CNN）整合了磁共振成像（MRI）和正电子发射断层扫描（PET）来对MCI和AD进行分类。但是，在这些作品中，MRI和PET特征的融合仅通过串联而实现，从而导致缺乏交叉模式相互作用。在本文中，我们提出了一种新型的基于注意力的CNN结构MNA-NET，以预测认知正常（CN）个体是否会在10年内发展MCI还是AD。为了解决以前作品中神经影像模式之间缺乏相互作用的相互作用，MNA-NET利用注意机制形成了MRI和PET图像的共享表示。提出的MNA-NET在OASIS-3数据集中进行了测试，并能够预测转换为MCI或AD的CN个体，其精度为83％，真为80％，真正的正率为86％。通过使用注意机制，新的最新水平结果提高了5％和10％的准确性和真正的负率。这些结果证明了所提出的模型在融合不同神经影像模式的融合中预测认知障碍和基于注意力的机制的潜力，以改善认知能力下降的预测。

### Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in Remote Sensing 
[[arxiv](https://arxiv.org/abs/2401.07782)] [[cool](https://papers.cool/arxiv/2401.07782)] [[pdf](https://arxiv.org/pdf/2401.07782)]
> **Authors**: Jakob Hackstein,Gencer Sumbul,Kai Norman Clasen,Begüm Demir
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: Accepted at the IEEE Transactions on Geoscience and Remote Sensing. Our code is available at https://github.com/jakhac/CSMAE
- **标题**: 在遥感中探索传感器敏捷图像检索的蒙版自动编码器
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 通过蒙版自动编码器（MAE）的自我监督学习最近引起了遥感（RS）图像表示学习的极大关注，因此从不断增长的RS图像档案中体现了基于内容的图像检索（CBIR）的重要潜力。但是，现有的RS基于MAE的CBIR研究假设所考虑的RS图像是由单个图像传感器获取的，因此仅适用于单模式CBIR问题。尚未探索MAE对跨传感器CBIR的有效性，该跨传感器CBIR旨在搜索不同图像模式的语义相似图像。在本文中，我们迈出的第一步探讨了MAE在Rs中对传感器不可稳定的CBIR的有效性。为此，我们介绍了一个系统概述，介绍了在CBIR的上下文中，在多传感器RS图像档案中利用蒙版的图像建模（表示为跨传感器蒙版的自动编码器[CSMAES]），以利用蒙版的图像建模。根据适用于香草Mae的不同调整，我们引入了不同的CSMAE模型。我们还对这些CSMAE模型提供了广泛的实验分析。我们最终得出了一个指南，以利用Rs中的单模式和跨模式CBIR问题利用蒙版的图像建模。这项工作的代码可在https://github.com/jakhac/csmae上公开获得。

### DeepThalamus: A novel deep learning method for automatic segmentation of brain thalamic nuclei from multimodal ultra-high resolution MRI 
[[arxiv](https://arxiv.org/abs/2401.07751)] [[cool](https://papers.cool/arxiv/2401.07751)] [[pdf](https://arxiv.org/pdf/2401.07751)]
> **Authors**: Marina Ruiz-Perez,Sergio Morell-Ortega,Marien Gadea,Roberto Vivo-Hernando,Gregorio Rubio,Fernando Aparici,Mariam de la Iglesia-Vaya,Thomas Tourdias,Pierrick Coupé,José V. Manjón
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: Deepthalamus：一种新型的深度学习方法，用于从多模式超高分辨率MRI自动分割脑丘脑核
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 丘脑在多种神经病理学中的含义使其成为体积分析的兴趣结构。在目前的工作中，我们设计并实施了一个多模式的深度神经网络，以在超高分辨率下（0.125 mm3）对丘脑核进行分割。当前工具要么以标准分辨率（1 mm3）运行，要么使用单差数据。为了实现所提出的目标，首先，使用超高分辨率T1，T2和白质（WMN）图像创建了半二核分割的丘脑核的数据库。然后，一种新型的基于深度学习的策略旨在获得自动分割，并接受了培训，以使用半佩心的方法来提高其鲁棒性和顾问性。将所提出的方法与相关的最先进方法进行了比较，该方法在分割质量和效率方面都显示出竞争性结果。为了使所提出的方法完全可供科学界使用，还提出了一个能够使用单色标准分辨率T1图像的完整管道。

### Automatic 3D Multi-modal Ultrasound Segmentation of Human Placenta using Fusion Strategies and Deep Learning 
[[arxiv](https://arxiv.org/abs/2401.09638)] [[cool](https://papers.cool/arxiv/2401.09638)] [[pdf](https://arxiv.org/pdf/2401.09638)]
> **Authors**: Sonit Singh,Gordon Stevenson,Brendan Mein,Alec Welsh,Arcot Sowmya
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 使用融合策略和深度学习的自动3D多模式超声分割
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 目的：超声是用于临床实践中诊断和筛查的最常用的医学成像方式。由于其安全性，无创性和可移植性，超声是妊娠胎儿评估的主要成像方式。当前的超声处理方法是手动的或半自动的，因此艰苦，耗时且容易出现错误，自动化将在解决这些挑战方面大有帮助。对早期妊娠期胎盘变化的自动鉴定可以促进诸如胎儿生长限制和前宾夕化诸如目前仅在胎龄晚期发现的疾病的潜在疗法，从而有可能阻止围产期发病率和死亡率。方法：我们提出了一种自动三维多模式（B模式和功率多普勒）超声分割对人胎盘的超声分割，并使用深度学习与不同的融合策略相结合。我们收集的包含BMODE和Power Doppler超声扫描的数据进行了400项研究。结果：我们评估了基于标准重叠和基于边界的指标的胎盘分割的不同融合策略和最先进的图像分割网络。我们发现，B模式和功率多普勒扫描的形式的多模式信息胜过任何单个模态。此外，我们发现在数据级融合的B模式和功率多普勒输入扫描可提供最佳结果，平均骰子相似性系数（DSC）为0.849。结论：我们得出的结论是，将B模式和功率多普勒扫描组合的多模式方法有效地从3D超声扫描以完全自动化的方式分割，并且对数据集的质量变化非常有力。

### Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM) 
[[arxiv](https://arxiv.org/abs/2401.09019)] [[cool](https://papers.cool/arxiv/2401.09019)] [[pdf](https://arxiv.org/pdf/2401.09019)]
> **Authors**: Hongruixuan Chen,Jian Song,Naoto Yokoya
> **First submission**: 2024-01-17
> **First announcement**: 2024-01-18
> **comment**: No comments
- **标题**: 通过段的任何模型（SAM）在光学遥感图像和地图数据之间进行更改检测
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,多媒体
- **摘要**: 无监督的多模式变化检测是时间敏感的任务和全面的多阶乘地球监测的关键。在这项研究中，我们探索了两个关键的遥感数据源之间无监督的多模式变化检测：光学高分辨率图像和OpenStreetMap（OSM）数据。具体来说，我们建议利用视觉基础模型分割任何模型（SAM）来解决我们的任务。利用SAM的异常零转移能力，可以获得光学图像的高质量分割图。因此，我们可以直接比较所谓分割域中的这两个异质数据形式。然后，我们介绍了指导SAM细分过程的两种策略：“无预告片”和“框/掩码提示”方法。这两种策略旨在检测一般情况下的土地覆盖变化，并分别在现有背景中确定新的土地覆盖物体。三个数据集的实验结果表明，与代表性无监督的多模式变更检测方法相比，所提出的方法可以实现更具竞争力的结果。

### DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI 
[[arxiv](https://arxiv.org/abs/2401.12074)] [[cool](https://papers.cool/arxiv/2401.12074)] [[pdf](https://arxiv.org/pdf/2401.12074)]
> **Authors**: Sergio Morell-Ortega,Marina Ruiz-Perez,Marien Gadea,Roberto Vivo-Hernando,Gregorio Rubio,Fernando Aparici,Maria de la Iglesia-Vaya,Gwenaelle Catheline,Pierrick Coupé,José V. Manjón
> **First submission**: 2024-01-22
> **First announcement**: 2024-01-23
> **comment**: 20 pages
- **标题**: 深渊：一种使用超高分辨率多模式MRI的小脑叶分割的深度学习方法
- **领域**: 图像和视频处理,计算机视觉和模式识别,神经元和认知
- **摘要**: 本文介绍了一种新型的多模式和高分辨率的人脑小脑小叶分割方法。与当前以标准分辨率运行的工具（$ 1 \ text {mm}^{3} $）或使用单模式数据不同，提出的方法通过使用多模式和超高分辨率（$ 0.125 \ $ 0.125 \ text {mm}^{3} $培训数据，可以改善小脑叶分割。为了开发该方法，首先，创建了一个半自动标记的小脑小叶的数据库，以使用超高分辨率T1和T2 MR图像训练所提出的方法。然后，已经设计和开发了深层网络的合奏，从而使所提出的方法可以在复杂的小脑小叶分割任务中表现出色，从而提高了精度，同时是在内存效率的同时。值得注意的是，我们的方法通过探索替代体系结构来偏离传统的U-NET模型。我们还将深度学习与经典的机器学习方法结合在一起，这些方法结合了从多ATLAS细分的先验知识，从而提高了精度和鲁棒性。最后，已开发了一条名为Deepceres的新在线管道，以便向科学界提供了拟议的方法，该方法仅在标准分辨率下仅作为输入作为单个T1 MR映像。

### Dual-Domain Coarse-to-Fine Progressive Estimation Network for Simultaneous Denoising, Limited-View Reconstruction, and Attenuation Correction of Cardiac SPECT 
[[arxiv](https://arxiv.org/abs/2401.13140)] [[cool](https://papers.cool/arxiv/2401.13140)] [[pdf](https://arxiv.org/pdf/2401.13140)]
> **Authors**: Xiongchao Chen,Bo Zhou,Xueqi Guo,Huidong Xie,Qiong Liu,James S. Duncan,Albert J. Sinusas,Chi Liu
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: 11 Pages, 10 figures, 4 tables
- **标题**: 双域的粗到精细渐进估计网络，用于同时降级，限量重建和心脏SPECT的衰减校正
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 单光子发射计算机断层扫描（SPECT）广泛用于诊断冠状动脉疾病。低剂量（LD）SPECT旨在最大程度地减少辐射暴露，但导致图像噪声增加。限量视图（LV）SPECT（例如最新的GE Myospect ES系统）可以加速扫描并减少硬件费用，但会降低重建精度。此外，计算机断层扫描（CT）通常用于得出衰减图（$μ$ -MAPS）进行心脏SPECT的衰减校正（AC），但它将引入额外的辐射暴露和SPECT-CT错误对准。尽管已经开发了各种方法来仅专注于SPECT中的LD DeNoising，LV重建或无CT AC，但同时解决这些任务的解决方案仍然具有挑战性和探索。此外，必须探索在这些相互关联的任务中融合跨域和跨模式信息的潜力，以进一步提高每个任务的准确性。因此，我们提出了一个双域的粗到精细的渐进网络（Dudocfnet），这是一种用于同时LD denoising，LV重建和CT无CT $μ$ $ $ $ $ $ $ $ $ $ $ $ $ MAP生成心脏SPECT的多任务学习方法。 Dudocfnet中的配对双域网络使用多层融合机制级联，用于跨域和交叉模式融合。在投影和图像域中都应用了两阶段的渐进式学习策略，以实现光谱投影的粗到最新估计和CT衍生的$μ$图。我们的实验表明，与现有的单一或多任务学习方法相比，在各种迭代和LD水平下，与现有的单一或多任务学习方法相比，Dudocfnet在估计预测，产生$ $ $ $ $ $图和AC重建方面的精确度。此工作的源代码可在https://github.com/xiongchaochen/dudocfnet-multitask上获得。

### Predicting Hypoxia in Brain Tumors from Multiparametric MRI 
[[arxiv](https://arxiv.org/abs/2401.14171)] [[cool](https://papers.cool/arxiv/2401.14171)] [[pdf](https://arxiv.org/pdf/2401.14171)]
> **Authors**: Daniele Perlo,Georgia Kanli,Selma Boudissa,Olivier Keunen
> **First submission**: 2024-01-25
> **First announcement**: 2024-01-26
> **comment**: 7 pages, 2 figures
- **标题**: 预测来自多参数MRI的脑肿瘤缺氧
- **领域**: 图像和视频处理,人工智能
- **摘要**: 本研究论文提出了一种使用多参数磁共振成像（MRI）的新方法来预测脑肿瘤中缺氧的方法。缺氧是氧气水平低的疾病，是与预后不良有关的恶性脑肿瘤的共同特征。氟甲替唑正电子发射断层扫描（FMISO PET）是一种在体内检测缺氧的良好方法，但昂贵且不广泛。我们的研究建议使用MRI（一种更容易访问，更具成本效益的成像方式）来预测FMISO PET信号。我们研究了对ACRIN 6684数据集训练的深度学习模型（DL），该数据集包含来自脑肿瘤患者的MRI和FMISO PET图像的资源。我们训练有素的模型有效地了解了MRI特征与相应的FMISO PET信号之间的复杂关系，从而仅通过MRI扫描来预测缺氧。结果表明，预测和实际FMISO PET信号之间的相关性很强，总体PSNR得分高于29.6，SSIM得分大于0.94，这证实了MRI是脑肿瘤中缺氧预测的有希望的选择。这种方法可以显着提高临床环境中缺氧检测的可及性，并有可能进行及时和有针对性的治疗。

## 信号处理(eess.SP:Signal Processing)

该领域共有 1 篇论文

### Representation Learning for Wearable-Based Applications in the Case of Missing Data 
[[arxiv](https://arxiv.org/abs/2401.05437)] [[cool](https://papers.cool/arxiv/2401.05437)] [[pdf](https://arxiv.org/pdf/2401.05437)]
> **Authors**: Janosch Jungo,Yutong Xiang,Shkurta Gashi,Christian Holz
> **First submission**: 2024-01-08
> **First announcement**: 2024-01-11
> **comment**: Paper accepted in Human-Centric Representation Learning workshop at AAAI 2024 (https://hcrl-workshop.github.io/2024/)
- **标题**: 在丢失数据的情况下，用于基于可穿戴的应用的表示形式学习
- **领域**: 信号处理,人工智能,机器学习
- **摘要**: 可穿戴设备不断收集传感器数据，并使用它来推断个人的行为，例如睡眠，体育锻炼和情绪。尽管该领域的兴趣和进步很大，但由于数据质量低和数据注释有限，在现实世界环境中对多模式传感器数据进行建模仍然具有挑战性。在这项工作中，我们研究了代表性学习，以推出缺失的可穿戴数据，并将其与最先进的统计方法进行比较。我们研究了变压器模型在具有不同掩盖比的10个生理和行为信号上的性能。我们的结果表明，变形金刚的表现优于基线，用于丢失更频繁的信号数据插入数据，但对于单调信号而言却不多。我们进一步研究了归纳策略和掩盖口粮对下游分类任务的影响。我们的研究为基于掩盖的自我监督学习任务的设计和开发提供了见解，并主张采用基于混合的插入策略来应对可穿戴设备中缺少数据的挑战。

## 基因组学(q-bio.GN:Genomics)

该领域共有 1 篇论文

### Integrate Any Omics: Towards genome-wide data integration for patient stratification 
[[arxiv](https://arxiv.org/abs/2401.07937)] [[cool](https://papers.cool/arxiv/2401.07937)] [[pdf](https://arxiv.org/pdf/2401.07937)]
> **Authors**: Shihao Ma,Andy G. X. Zeng,Benjamin Haibe-Kains,Anna Goldenberg,John E Dick,Bo Wang
> **First submission**: 2024-01-15
> **First announcement**: 2024-01-17
> **comment**: No comments
- **标题**: 整合任何OMIC：朝着全基因组数据整合进行患者分层
- **领域**: 基因组学,机器学习,定量方法
- **摘要**: 高通量的OMICS分析进步具有大大增强的癌症患者分层。但是，多摩尼克集成中的不完整数据提出了一个重大挑战，因为传统方法诸如样本排除或插补通常会损害生物学多样性和依赖性。此外，通常会忽略将具有部分OMIC数据准确分类为现有子类型的新患者对新患者进行分类的关键任务。为了解决这些问题，我们介绍了Integrao（集成任何OMIC），这是一个无监督的框架，用于集成不完整的多摩斯数据并分类新样本。 Integrao首先结合了来自不同OMICS来源的部分重叠的患者图，并利用图形神经网络产生统一的患者嵌入。我们在涉及六种OMIC模式的五个癌症队列中进行的系统评估表明，Integrao对缺失数据的鲁棒性及其在对新样本进行部分概况分类的准确性。一项急性髓样白血病病例研究进一步验证了其在不完整数据集中发现生物学和临床异质性的能力。 Integrao处理异质和不完整数据的能力使其成为精确肿瘤学的重要工具，为患者表征提供了整体方法。

## 分子网络(q-bio.MN:Molecular Networks)

该领域共有 1 篇论文

### Multi-Modal Representation Learning for Molecular Property Prediction: Sequence, Graph, Geometry 
[[arxiv](https://arxiv.org/abs/2401.03369)] [[cool](https://papers.cool/arxiv/2401.03369)] [[pdf](https://arxiv.org/pdf/2401.03369)]
> **Authors**: Zeyu Wang,Tianyi Jiang,Jinhuan Wang,Qi Xuan
> **First submission**: 2024-01-06
> **First announcement**: 2024-01-08
> **comment**: 8 pages, 3 figures
- **标题**: 分子属性预测的多模式表示学习：序列，图，几何形状
- **领域**: 分子网络,机器学习,生物分子
- **摘要**: 分子特性预测是指用某些生化特性标记分子的任务，在药物发现和设计过程中起关键作用。最近，随着机器学习的发展，基于深度学习的分子财产预测已成为解决传统方法的资源密集型性质的解决方案，从而引起了极大的关注。其中，分子表示学习是分子性质预测性能的关键因素。并且已经提出了许多基于序列的，基于图和几何的方法。但是，大多数现有研究仅着眼于一种用于学习分子表示的方式，未能全面捕获分子特征和信息。在本文中，提出了一个新型的多模式表示学习模型，该学习模型集成了序列，图和几何特性，用于分子属性预测，称为SGGRL。具体而言，我们设计了一个融合层来融合不同方式的表示。此外，为了确保跨模态的一致性，对SGGRL进行了训练，以最大程度地提高同一分子的表示相似性，同时最大程度地减少不同分子的相似性。为了验证SGGRL的有效性，使用了七个分子数据集和几个基线进行评估和比较。实验结果表明，在大多数情况下，SGGRL始终优于基准。这进一步强调了SGGRL全面捕获分子信息的能力。总体而言，拟议的SGGRL模型通过利用多模式表示学习来提取多种多样和全面的分子见解来彻底改变分子性能预测的潜力。我们的代码在https://github.com/vencent-won/sggrl上发布。

## 神经元和认知(q-bio.NC:Neurons and Cognition)

该领域共有 3 篇论文

### Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data 
[[arxiv](https://arxiv.org/abs/2401.01383)] [[cool](https://papers.cool/arxiv/2401.01383)] [[pdf](https://arxiv.org/pdf/2401.01383)]
> **Authors**: Michalis Pistos,Gang Li,Weili Lin,Dinggang Shen,Islem Rekik
> **First submission**: 2024-01-01
> **First announcement**: 2024-01-03
> **comment**: No comments
- **标题**: 使用稀缺数据来预测婴儿大脑的连通性与联合多trajectory GNN
- **领域**: 神经元和认知,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 对于确定早期大脑连通性发展的动态，对婴儿脑网络的复杂演变的理解是关键的。现有的深度学习解决方案遭受了三个主要局限性。首先，它们不能推广到多条件预测任务，其中每个图轨迹对应于特定的成像模态或连接性类型（例如T1-W MRI）。其次，现有模型需要广泛的培训数据集，以实现令人满意的性能，这通常具有挑战性。第三，他们没有有效利用不完整的时间序列数据。为了解决这些限制，我们介绍了FedGMTE-NET ++，这是一个基于图形的多个项目的多个项目的演化网络。利用联邦的力量，我们在有限的数据集中汇总了不同医院的本地学习。结果，我们在保留数据隐私的同时，提高了每个医院的本地生成模型的性能。 The three key innovations of FedGmTE-Net++ are: (i) presenting the first federated learning framework specifically designed for brain multi-trajectory evolution prediction in a data-scarce environment, (ii) incorporating an auxiliary regularizer in the local objective function to exploit all the longitudinal brain connectivity within the evolution trajectory and maximize data utilization, (iii) introducing a two-step imputation process, comprising基于初步的KNN的预先完成，然后是插图步骤，该步骤采用回归器来提高相似性得分和完善的推荐。我们的全面实验结果表明，与基准方法相比，从单基线图中，脑多孔图预测中FedGMTE-NET ++的表现要优于表现。

### Multi-Modal Cognitive Maps based on Neural Networks trained on Successor Representations 
[[arxiv](https://arxiv.org/abs/2401.01364)] [[cool](https://papers.cool/arxiv/2401.01364)] [[pdf](https://arxiv.org/pdf/2401.01364)]
> **Authors**: Paul Stoewer,Achim Schilling,Andreas Maier,Patrick Krauss
> **First submission**: 2023-12-22
> **First announcement**: 2024-01-03
> **comment**: No comments
- **标题**: 基于训练后继表示的神经网络的多模式认知图
- **领域**: 神经元和认知,人工智能,机器学习,神经和进化计算
- **摘要**: 认知图是关于大脑如何有效地组织记忆并从中检索背景的一个概念。内嗅 - 海马复合物与情节和关系记忆处理以及空间导航很大程度上涉及，并被认为可以通过位置和网格细胞构建认知图。为了利用认知图的有希望的属性，我们使用后继表示建立了一个多模式神经网络，该神经网络能够模拟将细胞动力学和认知图表示。在这里，我们使用由图像和单词嵌入组成的多模式输入。该网络了解新的输入与培训数据库之间的相似之处，从而成功地表示认知图。随后，该网络的预测可用于从一种模式推断为$ 90 \％$准确性。因此，提出的方法可能是改善当前AI系统的基础，以更好地理解环境和出现对象的不同方式。因此，当类似的信息发生较少的信息时，可以从学习的认知图推断出其他信息时，特定方式与某些相遇的关联可能会导致新情况下的上下文意识。认知图由大脑中的内hinal-Hampocampal复合物所代表，从记忆中组织和检索上下文，表明像Chatgpt这样的大型语言模型（LLM）可以利用类似的体系结构作为高级处理中心的作用，类似于Hippocampus在Cortex Hiersarchy中的运作方式。最后，通过利用多模式输入，LLM可以潜在地弥合不同形式的数据（例如图像和单词）之间的差距，为上下文意识和通过学习的关联来铺平了途径，以解决AI中的基础问题。

### Detection of Auditory Brainstem Response Peaks Using Image Processing Techniques in Infants with Normal Hearing Sensitivity 
[[arxiv](https://arxiv.org/abs/2401.17317)] [[cool](https://papers.cool/arxiv/2401.17317)] [[pdf](https://arxiv.org/pdf/2401.17317)]
> **Authors**: Amir Majidpour,Samer Kais Jameel,Jafar Majidpour,Houra Bagheri,Tarik A. Rashid,Ahmadreza Nazeri,Mahshid Moheb Aleaba
> **First submission**: 2024-01-21
> **First announcement**: 2024-01-30
> **comment**: No comments
- **标题**: 使用图像处理技术在具有正常听力敏感性的婴儿中检测听觉脑干反应峰值
- **领域**: 神经元和认知,计算机视觉和模式识别,声音,音频和语音处理,图像和视频处理
- **摘要**: 简介：测量听觉的脑干反应（ABR）是为了找到正常听力的儿童中脑干级别的听觉神经系统的完整性。听觉诱发电位（AEP）是使用声学刺激产生的。解释这些波浪需要能力避免误诊听力问题。使用计算机视觉自动化ABR测试标签可能会减少人为错误。方法：使用两只耳朵正常听力的26名儿童的ABR测试结果。建议一种新的方法来自动计算不同强度（分贝）波的峰值。该过程需要使用颜色阈值方法从Audera设备中获取波浪图像，使用图像区域分析仪应用程序将每个波段作为单个波浪图像分割为单个波浪图像，并使用图像处理（IP）技术将所有波浪图像转换为波浪，并最终计算每个波的潜伏期，以诊断出每个波的峰值来诊断疾病。结果：图像处理技术分别能够准确地检测诊断领域的1、3和5波（0.82），（0.98）和（0.98），波浪1、3和5的精度分别为（0.32），（0.32），（0.97），（0.87）和（0.87）。该评估在阈值部分也很好地工作，82.7％正确地检测到了ABR波。结论：我们的发现表明，通过使用技术自动检测和标记ABR波，可以使听力学测试电池套件更加准确，快速和无错误。

## 一般财务(q-fin.GN:General Finance)

该领域共有 1 篇论文

### Multimodal Gen-AI for Fundamental Investment Research 
[[arxiv](https://arxiv.org/abs/2401.06164)] [[cool](https://papers.cool/arxiv/2401.06164)] [[pdf](https://arxiv.org/pdf/2401.06164)]
> **Authors**: Lezhi Li,Ting-Yu Chang,Hai Wang
> **First submission**: 2023-12-23
> **First announcement**: 2024-01-12
> **comment**: No comments
- **标题**: 基础投资研究的多模式Gen-AI
- **领域**: 一般财务,机器学习
- **摘要**: 该报告概述了金融投资行业的一项变革性倡议，在该计划中，正在重新构想传统的决策过程，其中包含劳动密集型的任务，例如通过大量文件进行筛选。利用语言模型，我们的实验旨在使信息汇总和投资思想产生自动化。我们试图评估微调方法对基本模型（LLAMA2）的有效性，以实现特定的应用程序级目标，包括提供有关事件对公司和行业的影响的见解，了解市场状况关系，了解市场状况关系，从而产生与投资者保持一致的投资思想，并通过库存建议和详细的解释来形成股票的投资想法以及对股票建议的结果。通过最先进的生成建模技术，最终目标是开发AI代理原型，使人类投资者摆脱重复任务，并允许专注于高级战略思维。该项目涵盖了多样化的语料库数据集，包括研究报告，投资备忘录，市场新闻和广泛的时间序列市场数据。我们进行了三个实验，在Llama2_7b_hf_chat上应用了无监督和监督的Lora微调作为基本模型，并在GPT3.5模型上进行了指导微调。统计和人类评估都表明，微调版本在解决文本建模，摘要，推理和金融领域问题方面表现更好，这表明了提高金融领域中决策过程的关键步骤。可以在GitHub上找到该项目的代码实现：https：//github.com/firenze11/finance_lm。

## 计算(stat.CO:Computation)

该领域共有 2 篇论文

### Ensemble-Based Annealed Importance Sampling 
[[arxiv](https://arxiv.org/abs/2401.15645)] [[cool](https://papers.cool/arxiv/2401.15645)] [[pdf](https://arxiv.org/pdf/2401.15645)]
> **Authors**: Haoxuan Chen,Lexing Ying
> **First submission**: 2024-01-28
> **First announcement**: 2024-01-29
> **comment**: 33 pages, 13 figures
- **标题**: 基于合奏的退火重要性抽样
- **领域**: 计算,机器学习,数值分析,计算物理,机器学习
- **摘要**: 在计算科学和统计数据中，从多模式分布中的采样是一个基本且具有挑战性的问题。在为此任务提出的各种方法中，一种流行的方法是退火重要性采样（AIS）。在本文中，我们通过将基于集合的AIS与基于人群的蒙特卡洛方法相结合以提高其效率来提出基于合奏的AIS版本。通过跟踪沿起始分布和目标分布之间的某个延续路径的集合而不是单个粒子，我们利用合奏中的相互作用来鼓励探索未发现的模式。具体而言，我们的主要思想是利用Snooker算法或进化蒙特卡洛中使用的遗传算法。我们讨论如何实现所提出的算法并得出一个偏微分方程，该方程在连续的时间和平均范围限制下管理集合的演变。我们还测试了所提出的算法在各种连续和离散分布上的效率。

### Leveraging Nested MLMC for Sequential Neural Posterior Estimation with Intractable Likelihoods 
[[arxiv](https://arxiv.org/abs/2401.16776)] [[cool](https://papers.cool/arxiv/2401.16776)] [[pdf](https://arxiv.org/pdf/2401.16776)]
> **Authors**: Xiliang Yang,Yifei Xiong,Zhijian He
> **First submission**: 2024-01-30
> **First announcement**: 2024-01-31
> **comment**: 28 pages, 4 figures
- **标题**: 利用嵌套的MLMC进行连续神经后验估计，具有棘手的可能性
- **领域**: 计算,机器学习,机器学习
- **摘要**: 最近已经提出了顺序神经后估计（SNPE）技术，用于处理具有棘手的可能性的基于模拟的模型。他们致力于使用基于神经网络的条件密度估计器从适应性的模拟中学习后部。作为一种SNPE技术，Greenberg等人提出的自动后验变换（APT）方法。 （2019年）的表现显着，并扩展到高维数据。但是，APT方法具有对棘手归一化常数的对数期望的计算，即嵌套期望。尽管提出了通过离散归一化常数来解决这一问题，但分析学习的融合仍然具有挑战性。在本文中，我们提出了一种嵌套的APT方法来估计所嵌套的期望。这有助于建立收敛分析。由于损耗函数及其梯度的嵌套估计量有偏差，因此我们利用无偏的多级蒙特卡洛（MLMC）估计器进行依据。为了进一步减少公正估计器的过度差异，本文还考虑了偏见和平均成本之间的权衡，从而开发了一些截断的MLMC估计器。提供了在中等维度中使用多模态近似复合后期的数值实验。

## 机器学习(stat.ML:Machine Learning)

该领域共有 2 篇论文

### Simulation-Based Inference with Quantile Regression 
[[arxiv](https://arxiv.org/abs/2401.02413)] [[cool](https://papers.cool/arxiv/2401.02413)] [[pdf](https://arxiv.org/pdf/2401.02413)]
> **Authors**: He Jia
> **First submission**: 2024-01-04
> **First announcement**: 2024-01-05
> **comment**: 9+13 pages, 8+8 figures, ICML 2024
- **标题**: 基于仿真的推断与分位数回归
- **领域**: 机器学习,宇宙学和非银河系天体物理学,天体物理学仪器和方法,机器学习
- **摘要**: 我们提出了神经分位数估计（NQE），这是一种基于条件分位数回归的新型基于模拟的推断（SBI）方法。 NQE自动加工学习每个后维的单个维数分位数，以数据和先前的后尺寸为条件。通过使用单调立方体遗迹样条插值预测的分位数来获得后样品，并针对尾巴行为和多模式分布进行特定处理。我们使用局部累积密度函数（CDF）引入了贝叶斯可信区域的替代定义，比传统的最高后密度区域（HPDR）提供的评估要快得多。如果模拟预算有限和/或已知的模型错误指定，则可以将后处理校准步骤集成到NQE中，以确保后验估计的无偏见，并可以忽略不计。我们证明，NQE在各种基准问题上实现了最新的性能。

### Assessment of Sports Concussion in Female Athletes: A Role for Neuroinformatics? 
[[arxiv](https://arxiv.org/abs/2401.13045)] [[cool](https://papers.cool/arxiv/2401.13045)] [[pdf](https://arxiv.org/pdf/2401.13045)]
> **Authors**: Rachel Edelstein,Sterling Gutterman,Benjamin Newman,John Darrell Van Horn
> **First submission**: 2024-01-23
> **First announcement**: 2024-01-24
> **comment**: No comments
- **标题**: 评估女运动员体育脑震荡：神经信息学的角色？
- **领域**: 机器学习,机器学习,应用领域,方法论
- **摘要**: 在过去的十年中，女运动员中与运动有关的脑震荡的复杂性很容易显而易见。诊断脑震荡的传统临床方法在应用于女运动员时会受到限制，通常无法捕获大脑结构和功能的细微变化。在这项工作中，先进的神经信息技术和机器学习模型已成为宝贵的资产。尽管这些技术已被广泛用于理解男运动员的脑震荡，但我们理解其对女运动员的有效性仍然存在很大的差距。机器学习具有出色的数据分析能力，为弥合这种赤字提供了有希望的途径。通过利用机器学习的力量，研究人员可以将观察到的表型神经影像学数据与性别特定的生物学机制联系起来，从而揭示女性运动员脑震荡的奥秘。此外，机器学习中的嵌入方法可以检查大脑体系结构及其在常规解剖参考框架之外的改变。反过来，使研究人员可以更深入地了解脑震荡，治疗反应和恢复过程的动态。为了确保女运动员获得应有的最佳护理，研究人员必须采用先进的神经影像学技术和精致的机器学习模型。这些工具可以深入研究负责女运动员的神经元功能障碍引起的脑震荡症状的基本机制。本文努力解决在女运动员人群中多模式神经影像设计和机器学习方法中性别差异的关键问题，最终确保他们在面对脑震荡挑战时获得所需的量身定制护理。

## 其他论文

共有 63 篇其他论文

- [E-chat: Emotion-sensitive Spoken Dialogue System with Large Language Models](https://arxiv.org/abs/2401.00475)
  - **标题**: e-chat：具有大语言模型的情感敏感对话系统
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Specific Emitter Identification Based on Joint Variational Mode Decomposition](https://arxiv.org/abs/2401.01503)
  - **标题**: 基于联合变异模式分解的特定发射极标识
  - **Filtered Reason**: none of cs.CR in whitelist
- [Enhancing Zero-Shot Multi-Speaker TTS with Negated Speaker Representations](https://arxiv.org/abs/2401.02014)
  - **标题**: 用否定的扬声器表示，增强零击的多演讲者TT
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [MULTI-CASE: A Transformer-based Ethics-aware Multimodal Investigative Intelligence Framework](https://arxiv.org/abs/2401.01955)
  - **标题**: 多案例：基于变压器的多式联运智能智能框架
  - **Filtered Reason**: none of cs.HC,cs.MM in whitelist
- [Towards Weakly Supervised Text-to-Audio Grounding](https://arxiv.org/abs/2401.02584)
  - **标题**: 迈向弱监督的文本对原告的基础
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Modal smoothing for analysis of room reflections measured with spherical microphone and loudspeaker arrays](https://arxiv.org/abs/2401.03458)
  - **标题**: 模态平滑，以分析用球形麦克风和扬声器阵列测量的房间反射
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Single-Microphone Speaker Separation and Voice Activity Detection in Noisy and Reverberant Environments](https://arxiv.org/abs/2401.03448)
  - **标题**: 嘈杂和混响环境中的单微粒扬声器分离和语音活动检测
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Multichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-Modal Speech Representation](https://arxiv.org/abs/2401.03468)
  - **标题**: 多通道AV-WAV2VEC2：学习多通道多模式语音表示的框架
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition](https://arxiv.org/abs/2401.03429)
  - **标题**: Merbench：多模式情感识别的统一评估基准
  - **Filtered Reason**: none of cs.HC in whitelist
- [The RoSiD Tool: Empowering Users to Design Multimodal Signals for Human-Robot Collaboration](https://arxiv.org/abs/2401.03088)
  - **标题**: ROSID工具：授权用户设计人机协作的多模式信号
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Message-Passing Receiver for OCDM over Multi-Lag Multi-Doppler Channels](https://arxiv.org/abs/2401.04358)
  - **标题**: 通过多链多型多普勒通道上的OCDM的消息接收器
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [Autonomous robotic re-alignment for face-to-face underwater human-robot interaction](https://arxiv.org/abs/2401.04320)
  - **标题**: 自主机器人重新对准面对面的水下人类机器人互动
  - **Filtered Reason**: none of cs.RO in whitelist
- [FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for Acoustic Echo Cancellation](https://arxiv.org/abs/2401.04283)
  - **标题**: FADI-AEC：基于快速得分的扩散模型，由远端信号引导以取消声音回声
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Recovering the 3D UUV Position using UAV Imagery in Shallow-Water Environments](https://arxiv.org/abs/2401.03938)
  - **标题**: 在浅水环境中使用无人机图像恢复3D UUV位置
  - **Filtered Reason**: none of cs.RO in whitelist
- [Inverse Nonlinearity Compensation of Hyperelastic Deformation in Dielectric Elastomer for Acoustic Actuation](https://arxiv.org/abs/2401.03850)
  - **标题**: 介电弹性体高弹性变形的逆非线性补偿用于声学启动
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [An audio-quality-based multi-strategy approach for target speaker extraction in the MISP 2023 Challenge](https://arxiv.org/abs/2401.03697)
  - **标题**: MISP 2023挑战中的目标扬声器提取的基于音频质量的多策略方法
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [A Multi-Modal Approach Based on Large Vision Model for Close-Range Underwater Target Localization](https://arxiv.org/abs/2401.04595)
  - **标题**: 基于大型视觉模型的多模式方法，用于近距离水下目标定位
  - **Filtered Reason**: none of cs.RO in whitelist
- [Augmented Reality User Interface for Command, Control, and Supervision of Large Multi-Agent Teams](https://arxiv.org/abs/2401.05665)
  - **标题**: 增强现实用户界面，用于大型多代理团队的命令，控制和监督
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Localizing Acoustic Energy in Sound Field Synthesis by Directionally Weighted Exterior Radiation Suppression](https://arxiv.org/abs/2401.05809)
  - **标题**: 通过定向加权的外部辐射抑制在声场合成中定位声能
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Intuitive Control of Scraping and Rubbing Through Audio-tactile Synthesis](https://arxiv.org/abs/2401.05757)
  - **标题**: 直观地控制刮擦和通过音频摩擦摩擦
  - **Filtered Reason**: none of physics.class-ph,eess.AS,cs.SD in whitelist
- [Cross-Modality and Within-Modality Regularization for Audio-Visual DeepFake Detection](https://arxiv.org/abs/2401.05746)
  - **标题**: 跨模式和模式内正规化，用于视听深膜检测
  - **Filtered Reason**: none of cs.MM in whitelist
- [Probability-based Distance Estimation Model for 3D DV-Hop Localization in WSNs](https://arxiv.org/abs/2401.05709)
  - **标题**: WSN中3D DV-HOP定位的基于概率的距离估计模型
  - **Filtered Reason**: none of cs.NI,eess.SP in whitelist
- [Multimodal Language and Graph Learning of Adsorption Configuration in Catalysis](https://arxiv.org/abs/2401.07408)
  - **标题**: 催化中吸附配置的多模式语言和图形学习
  - **Filtered Reason**: none of cs.CE in whitelist
- [Graded modal logic and counting message passing automata](https://arxiv.org/abs/2401.06519)
  - **标题**: 分级模态逻辑和计数消息传递自动机
  - **Filtered Reason**: none of cs.LO,cs.DC in whitelist
- [A Logic for Repair and State Recovery in Byzantine Fault-tolerant Multi-agent Systems](https://arxiv.org/abs/2401.06451)
  - **标题**: 拜占庭式耐断层多代理系统的维修和状态恢复的逻辑
  - **Filtered Reason**: none of cs.DC in whitelist
- [CLIPRerank: An Extremely Simple Method for Improving Ad-hoc Video Search](https://arxiv.org/abs/2401.08449)
  - **标题**: Cliprerank：一种非常简单的改进临时视频搜索的方法
  - **Filtered Reason**: none of cs.MM in whitelist
- [Multimodal assessment of best possible self as a self-regulatory activity for the classroom](https://arxiv.org/abs/2401.08424)
  - **标题**: 多模式评估最佳自我作为课堂的自我调节活动
  - **Filtered Reason**: none of cs.HC in whitelist
- [Certifiable Mutual Localization and Trajectory Planning for Bearing-Based Robot Swarm](https://arxiv.org/abs/2401.07784)
  - **标题**: 基于轴承的机器人群的可认证的相互定位和轨迹计划
  - **Filtered Reason**: none of cs.RO in whitelist
- [POE: Acoustic Soft Robotic Proprioception for Omnidirectional End-effectors](https://arxiv.org/abs/2401.09382)
  - **标题**: POE：全向终端效应的声学软机器人本体感受
  - **Filtered Reason**: none of cs.RO in whitelist
- [Can Synthetic Data Boost the Training of Deep Acoustic Vehicle Counting Networks?](https://arxiv.org/abs/2401.09308)
  - **标题**: 合成数据可以提高深度媒介物计数网络的训练吗？
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Knight Watch: Ubiquitous Computing Enhancements To Sleep Quality With Acoustic Analysis](https://arxiv.org/abs/2401.08991)
  - **标题**: 骑士手表：通过声学分析，无处不在的计算对睡眠质量的增强
  - **Filtered Reason**: none of cs.HC in whitelist
- [CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot](https://arxiv.org/abs/2401.09388)
  - **标题**: Cognitivedog：大型基于多模型的系统，将视觉和语言转化为四足机器人的动作
  - **Filtered Reason**: none of cs.RO in whitelist
- [Your blush gives you away: detecting hidden mental states with remote photoplethysmography and thermal imaging](https://arxiv.org/abs/2401.09145)
  - **标题**: 您的腮红给您带来了：通过远程照相的图像和热成像检测隐藏的心理状态
  - **Filtered Reason**: none of cs.CY in whitelist
- [Detecting Post-Stroke Aphasia Via Brain Responses to Speech in a Deep Learning Framework](https://arxiv.org/abs/2401.10291)
  - **标题**: 在深度学习框架中，通过大脑对语音的反应来检测势后失语症
  - **Filtered Reason**: none of eess.SP,eess.AS,cs.SD in whitelist
- [PhotoScout: Synthesis-Powered Multi-Modal Image Search](https://arxiv.org/abs/2401.10464)
  - **标题**: Photoscout：综合供电的多模式图像搜索
  - **Filtered Reason**: none of cs.HC in whitelist
- [Acoustic Disturbance Sensing Level Detection for ASD Diagnosis and Intelligibility Enhancement](https://arxiv.org/abs/2401.11832)
  - **标题**: ASD诊断和清晰度提高的声学干扰感测水平检测
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Harmonic Detection from Noisy Speech with Auditory Frame Gain for Intelligibility Enhancement](https://arxiv.org/abs/2401.11829)
  - **标题**: 从嘈杂的语音和听觉框架增益的谐波检测以提高可理解性
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Multi-Objective Multi-mode Time-Cost Tradeoff modeling in Construction Projects Considering Productivity Improvement](https://arxiv.org/abs/2401.12388)
  - **标题**: 在建筑项目中，多目标多模式的时间成本权衡建模考虑生产率提高
  - **Filtered Reason**: none of cs.CE,math.OC in whitelist
- [An Exploratory Study of Multimodal Physiological Data in Jazz Improvisation Using Basic Machine Learning Techniques](https://arxiv.org/abs/2401.12266)
  - **标题**: 使用基本机器学习技术对爵士即兴创作中多模式生理数据的探索性研究
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model for Multimodal Processing](https://arxiv.org/abs/2401.12264)
  - **标题**: COAVT：由认知启发的统一音频 - 视频文本预训练模型，用于多模式处理
  - **Filtered Reason**: none of eess.AS,cs.SD,cs.MM,eess.IV in whitelist
- [VirtuWander: Enhancing Multi-modal Interaction for Virtual Tour Guidance through Large Language Models](https://arxiv.org/abs/2401.11923)
  - **标题**: Virtuwander：通过大语言模型增强虚拟旅游指导的多模式互动
  - **Filtered Reason**: none of cs.HC in whitelist
- [The Conversation is the Command: Interacting with Real-World Autonomous Robot Through Natural Language](https://arxiv.org/abs/2401.11838)
  - **标题**: 对话是命令：通过自然语言与现实世界的自主机器人互动
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [MInD: Improving Multimodal Sentiment Analysis via Multimodal Information Disentanglement](https://arxiv.org/abs/2401.11818)
  - **标题**: 思维：通过多模式信息分离改善多模式分析
  - **Filtered Reason**: none of cs.MM in whitelist
- [Identity-Driven Multimedia Forgery Detection via Reference Assistance](https://arxiv.org/abs/2401.11764)
  - **标题**: 通过参考辅助，身份驱动的多媒体伪造检测
  - **Filtered Reason**: none of cs.MM in whitelist
- [From Numbers to Words: Multi-Modal Bankruptcy Prediction Using the ECL Dataset](https://arxiv.org/abs/2401.12652)
  - **标题**: 从数字到单词：使用ECL数据集的多模式破产预测
  - **Filtered Reason**: none of q-fin.CP,cs.CE in whitelist
- [Bayesian adaptive learning to latent variables via Variational Bayes and Maximum a Posteriori](https://arxiv.org/abs/2401.13766)
  - **标题**: 贝叶斯自适应学习对潜在变量通过变异贝叶斯和最大后验
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Machine Learning for Shipwreck Segmentation from Side Scan Sonar Imagery: Dataset and Benchmark](https://arxiv.org/abs/2401.14546)
  - **标题**: 从侧面扫描声纳图像进行沉船细分的机器学习：数据集和基准测试
  - **Filtered Reason**: none of cs.RO in whitelist
- [MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor Fusion Perception Systems](https://arxiv.org/abs/2401.14314)
  - **标题**: 多点：用于测试多传感器融合感知系统的物理意识对象插入
  - **Filtered Reason**: none of cs.SE in whitelist
- [Leveraging Foundation Models for Crafting Narrative Visualization: A Survey](https://arxiv.org/abs/2401.14010)
  - **标题**: 利用基础模型来制作叙事可视化：调查
  - **Filtered Reason**: none of cs.HC in whitelist
- [CMA-ES with Learning Rate Adaptation](https://arxiv.org/abs/2401.15876)
  - **标题**: 具有学习率适应的CMA-E
  - **Filtered Reason**: none of cs.NE,math.OC in whitelist
- [IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models](https://arxiv.org/abs/2401.15559)
  - **标题**: IntentTuner：一个交互式框架，用于在微调文本到图像生成模型中整合人类意图
  - **Filtered Reason**: none of cs.HC in whitelist
- ["May I Speak?": Multi-modal Attention Guidance in Social VR Group Conversations](https://arxiv.org/abs/2401.15507)
  - **标题**: “我可以说吗？”：社会VR小组对话中的多模式关注指导
  - **Filtered Reason**: none of cs.HC in whitelist
- [Learning Online Belief Prediction for Efficient POMDP Planning in Autonomous Driving](https://arxiv.org/abs/2401.15315)
  - **标题**: 在自动驾驶中学习在线信念预测，以实现有效的POMDP计划
  - **Filtered Reason**: none of cs.RO in whitelist
- [Automatically Detecting Confusion and Conflict During Collaborative Learning Using Linguistic, Prosodic, and Facial Cues](https://arxiv.org/abs/2401.15201)
  - **标题**: 使用语言，韵律和面部提示在协作学习过程中自动检测混乱和冲突
  - **Filtered Reason**: none of cs.HC in whitelist
- [Multimodality in Group Communication Research](https://arxiv.org/abs/2401.15194)
  - **标题**: 小组交流研究中的多模式
  - **Filtered Reason**: none of cs.HC in whitelist
- [LaMI: Large Language Models for Multi-Modal Human-Robot Interaction](https://arxiv.org/abs/2401.15174)
  - **标题**: Lami：多模式人类机器人互动的大型语言模型
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [RABBIT: A Robot-Assisted Bed Bathing System with Multimodal Perception and Integrated Compliance](https://arxiv.org/abs/2401.15159)
  - **标题**: 兔子：具有多模式感知和综合合规性的机器人辅助床浴系统
  - **Filtered Reason**: none of cs.RO in whitelist
- [Chaotic Encryption for 10-Gb Ethernet Optical Links](https://arxiv.org/abs/2401.15138)
  - **标题**: 10 GB以太网光学链接的混沌加密
  - **Filtered Reason**: none of eess.SP,cs.CR in whitelist
- [LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering](https://arxiv.org/abs/2401.14857)
  - **标题**: Liv-Gaussmap：实时3D辐射现场图渲染的LIDAR惯性 - 视觉融合
  - **Filtered Reason**: none of cs.RO in whitelist
- [ReLoki: Infrastructure-free Distributed Relative Localization using On-board UWB Antenna Arrays](https://arxiv.org/abs/2401.16599)
  - **标题**: Reloki：使用板载UWB天线阵列的无基础架构分布式相对定位
  - **Filtered Reason**: none of cs.RO in whitelist
- [CognitiveOS: Large Multimodal Model based System to Endow Any Type of Robot with Generative AI](https://arxiv.org/abs/2401.16205)
  - **标题**: 认知：大型基于多模型的系统，可赋予任何类型的机器人具有生成AI的机器人
  - **Filtered Reason**: none of cs.RO in whitelist
- [Masked Audio Modeling with CLAP and Multi-Objective Learning](https://arxiv.org/abs/2401.15953)
  - **标题**: 用鼓掌和多目标学习掩盖音频建模
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [ROAMER: Robust Offroad Autonomy using Multimodal State Estimation with Radar Velocity Integration](https://arxiv.org/abs/2401.17404)
  - **标题**: ROAMER：使用雷达速度积分的多模式状态估计的强大越野自主权
  - **Filtered Reason**: none of cs.RO in whitelist
