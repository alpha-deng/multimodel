# 2025-01 月度论文分类汇总

共有529篇相关领域论文, 另有42篇其他

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 43 篇论文

### Probabilistic Mission Design in Neuro-Symbolic Systems 
[[arxiv](https://arxiv.org/abs/2501.01439)] [[cool](https://papers.cool/arxiv/2501.01439)] [[pdf](https://arxiv.org/pdf/2501.01439)]
> **Authors**: Simon Kohaut,Benedict Flade,Daniel Ochs,Devendra Singh Dhami,Julian Eggert,Kristian Kersting
> **First submission**: 2024-12-25
> **First announcement**: 2025-01-03
> **comment**: arXiv admin note: text overlap with arXiv:2406.03454
- **标题**: 神经符号系统中的概率任务设计
- **领域**: 人工智能,机器人技术
- **摘要**: Advanced Air Mobility（AAM）是一个不断增长的领域，它需要在智能车辆中准确建模法律概念和限制。此外，AAM的任何实施都需要面对固有动态和不确定的人居住空间所带来的挑战。尽管如此，视觉线（BVLOS）以外的无人飞机系统（UAS）的使用是一项可爱的任务，有望显着增强当今的物流和紧急响应能力。为了应对这些挑战，我们提出了一种概率和神经符号架构，以以可解释和适应性的方式编码不确定的空间关系和嘈杂感知的法律框架和专家知识。更具体地说，我们演示了概率任务设计（PROMIS），该系统体系结构将地理空间和感官数据与声明性的，混合概率的逻辑程序（HPLP）联系起来，以推理代理的状态空间及其合法性。结果，Promis生成了概率的任务景观（PML），该景观量化了代理商的信念，即在其导航空间中满足了一套任务条件。扩展了有关Promis的推理能力和计算特征的先前工作，我们显示了它与有效的机器学习模型（例如大语言模型（LLM）和基于变压器的视觉模型）的集成。因此，我们的实验是Promis使用多模式输入数据的应用以及我们的方法如何应用于许多重要的AAM方案。

### CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries 
[[arxiv](https://arxiv.org/abs/2501.01282)] [[cool](https://papers.cool/arxiv/2501.01282)] [[pdf](https://arxiv.org/pdf/2501.01282)]
> **Authors**: Shudong Liu,Yiqiao Jin,Cheng Li,Derek F. Wong,Qingsong Wen,Lichao Sun,Haipeng Chen,Xing Xie,Jindong Wang
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: Technical report; 26 pages
- **标题**: Culturevlm：表征和改善对100多个国家的视觉模型的文化理解
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLMS）具有先进的人类相互作用，但由于以西方为主的以西方为中心的培训数据而偏见，与文化理解相互作用，通常误解了符号，手势和人工制品。在本文中，我们构建了文化词，这是一个大规模的多模式基准，涵盖了19、682个文化概念，188个国家/地区，15种文化概念和3种问题类型，目的是表征和改善VLMS的多元文化理解能力。然后，我们提出了Culturevlm，这是在我们的数据集中微调的一系列VLM，以在文化理解中取得显着的绩效提高。我们对16个模型的评估揭示了明显的差异，在西方概念中的表现更强，在非洲和亚洲环境中的结果较弱。对我们的培养文化进行微调增强了文化感知，表明跨文化，跨占领和跨数据集的概括，而无需牺牲模型的一般VLM基准。我们进一步介绍了关于文化概括和遗忘的见解。我们希望这项工作可以为更公平和文化意识的多式联运系统奠定基础。

### Beyond Text: Implementing Multimodal Large Language Model-Powered Multi-Agent Systems Using a No-Code Platform 
[[arxiv](https://arxiv.org/abs/2501.00750)] [[cool](https://papers.cool/arxiv/2501.00750)] [[pdf](https://arxiv.org/pdf/2501.00750)]
> **Authors**: Cheonsu Jeong
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: 22 pages, 27 figures
- **标题**: 超越文本：使用无代码平台实施多模式的大型语言模型驱动的多代理系统
- **领域**: 人工智能
- **摘要**: 这项研究建议设计和实施基于LLM的多模式多代理系统（MAS）利用无代码平台来解决与企业中AI采用相关的实际约束和重大进入障碍。高级AI技术（例如大型语言模型（LLM））通常由于其技术复杂性和高度实施成本而构成挑战，这使得许多组织难以采用。为了克服这些局限性，本研究开发了一种基于无代码的多代理系统，旨在使用户在没有编程知识的情况下轻松构建和管理AI系统。该研究研究了各种用例，以验证AI在业务流程中的适用性，包括从基于图像的注释，基于图像的基于抹布的问题缠绕系统，基于文本的图像生成以及使用图像和提示的视频生成的代码。这些系统降低了AI采用的障碍，不仅赋予了专业开发人员的能力，而且还赋予了普通用户的利用AI，以显着提高生产率和效率。通过证明无代码平台的可伸缩性和可访问性，这项研究可以提高企业中AI技术的民主化，并验证多机构系统的实际适用性，最终有助于广泛采用AI在各个行业中。

### Artificial Intelligence in Creative Industries: Advances Prior to 2025 
[[arxiv](https://arxiv.org/abs/2501.02725)] [[cool](https://papers.cool/arxiv/2501.02725)] [[pdf](https://arxiv.org/pdf/2501.02725)]
> **Authors**: Nantheera Anantrasirichai,Fan Zhang,David Bull
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: This is an updated review of our previous paper (see https://doi.org/10.1007/s10462-021-10039-7)
- **标题**: 创意产业中的人工智能：2025年之前的进步
- **领域**: 人工智能
- **摘要**: 人工智能（AI）的快速进步，尤其是生成AI和大型语言模型（LLMS），通过启用创新的内容创建，增强工作流以及使创意工具的访问使创新的创建，增强工作流程和民主化，从而对创意产业产生了深远的影响。本文探讨了自2022年我们先前的评论以来的重大技术转变，强调了这些发展如何扩大了创造机会和效率。这些技术进步增强了文本对图像，文本对视频和多模式生成技术的能力。特别是，LLM中的主要突破已经在对话式AI中建立了新的基准，而图像发生器的进步已经彻底改变了内容的创建。我们还讨论了AI集成到后期制作工作流程中，该工作流已经大大加速和完善的传统过程。尽管有这些创新，但由于对创意内容的沟通流量的需求，仍然存在挑战，尤其是对于媒体行业。因此，我们在本文中包括数据压缩和质量评估。此外，我们强调了能够解决多个创造性任务的统一AI框架的趋势，并强调了人类监督对减轻AI生成的不准确性的重要性。最后，我们探索了AI在创意部门中的未来潜力，强调需要应对新兴挑战以最大程度地提高其利益，同时解决相关风险。

### Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition 
[[arxiv](https://arxiv.org/abs/2501.03230)] [[cool](https://papers.cool/arxiv/2501.03230)] [[pdf](https://arxiv.org/pdf/2501.03230)]
> **Authors**: Hao Fei,Shengqiong Wu,Wei Ji,Hanwang Zhang,Meishan Zhang,Mong-Li Lee,Wynne Hsu
> **First submission**: 2024-05-07
> **First announcement**: 2025-01-07
> **comment**: Accepted by ICML 2024
- **标题**: 思想视频：从感知到认知的分步视频推理
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 现有的视频理解研究仍然努力在复杂的视频中实现深入的理解和推理，这主要是由于对两个关键瓶颈的探索不足：细粒度的时空感知性理解和认知级别的视频场景理解。本文通过提出一种新颖的解决方案来弥合差距。我们首先介绍了一种新型的视频多模式大型语言模型（MLLM），该模型，该模型通过整合视频时空时空场景图（STSG）表示，从而实现了精细的像素级时空视频接地。然后，我们在运动式的基础上制定了一个思想视频（fot）推理框架。投票继承了思想链（COT）核心，将复杂的任务分解为更简单，可管理的子问题，并逐步解决它们从低级像素感知到高级认知解释。各种复杂视频基准测试的广泛实验表明，我们的整体框架显着提高了现有的最新框架。据我们所知，这是成功实施实现人类视频推理的COT技术的首次尝试，在该技术将其扩展到更广泛的视频理解场景方面，我们表现出巨大的潜力。项目在https://haofei.vip/vot上开放

### Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches 
[[arxiv](https://arxiv.org/abs/2501.03151)] [[cool](https://papers.cool/arxiv/2501.03151)] [[pdf](https://arxiv.org/pdf/2501.03151)]
> **Authors**: Alhassan Mumuni,Fuseini Mumuni
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: 人工通用情报（AGI）的大型语言模型：基础原理和方法的调查
- **领域**: 人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 基于大规模预处理基础模型（PFM）的生成人工智能（AI）系统，例如视觉语言模型，大语言模型（LLMS），扩散模型和视觉模型（VLA）模型，已经证明了在各种领域和上下文中求解复杂且真正的非琐事AI问题的能力。尤其是多模式的大语言模型（MLLM）从广泛而多样化的数据源中学习，允许世界上富裕和细微的表现形式，从而提供广泛的功能，包括推理能力，参与有意义的对话；与人类和其他代理商合作，共同解决复杂的问题；并了解人类的社会和情感方面。尽管这一令人印象深刻的壮举，但在大规模数据集中训练的最先进的LLM的认知能力仍然是肤浅而脆弱的。因此，通用LLM的通才能力受到严重限制。需要解决许多基本问题 - 实施例，符号接地，因果关系和记忆，以使LLMS获得人级的一般智能。这些概念与人类认知更加一致，并为LLM提供了固有的类似人类的认知特性，这些特性支持实现物理上可行的，语义上有意义的，灵活的，更具概括性的知识和智慧。在这项工作中，我们讨论了上述基础问题和调查最先进的方法，用于在LLMS中实施这些概念。具体而言，我们讨论了如何利用实施例，符号接地，因果关系和记忆的原理来以有机方式获得人工通用智能（AGI）。

### Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment 
[[arxiv](https://arxiv.org/abs/2501.03012)] [[cool](https://papers.cool/arxiv/2501.03012)] [[pdf](https://arxiv.org/pdf/2501.03012)]
> **Authors**: Pegah Khayatan,Mustafa Shukor,Jayneel Parekh,Matthieu Cord
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: The first three authors contributed equally
- **标题**: 分析多模式LLMS转向对齐的微型表示表示转移
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式LLM在理解多模式输入方面达到了显着的水平，推动了广泛的研究以开发越来越强大的模型。但是，对理解和解释这些模型的基本机制的关注要少得多。大多数现有的解释性研究仅在其最终状态中检查这些模型，从而忽略了训练过程中发生的动态代表性变化。在这项工作中，我们系统地分析了隐藏状态表示的演变，以揭示微调如何改变模型的内部结构，以专门研究新的多模式任务。使用基于概念的方法，我们将隐藏状态映射到可解释的视觉和文本概念，使我们能够随着培训的进展来追踪跨模态的编码概念的变化。我们还证明了使用转移向量来捕获这些概念的变化。这些换档向量使我们能够通过移动原始模型中的那些概念来恢复微调的概念。最后，我们探讨了我们发现对模型转向的实际影响，表明我们可以在没有任何培训的情况下调整多模式LLMS行为，例如修改答案类型，字幕样式或将模型偏向特定答案。我们的工作阐明了多模式表示如何通过微调演变，并为解释多模式任务中的模型适应提供了新的观点。该项目的代码可在https://github.com/mshukor/xl-vlms上公开获取。

### SenseRAG: Constructing Environmental Knowledge Bases with Proactive Querying for LLM-Based Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2501.03535)] [[cool](https://papers.cool/arxiv/2501.03535)] [[pdf](https://arxiv.org/pdf/2501.03535)]
> **Authors**: Xuewen Luo,Fan Ding,Fengze Yang,Yang Zhou,Junnyong Loo,Hwa Hui Tew,Chenxi Liu
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: This paper has been accepted for presentation at WACV Workshop LLMAD 2025
- **标题**: Senserag：通过主动查询基于LLM的自动驾驶，构建环境知识基础
- **领域**: 人工智能,机器人技术
- **摘要**: 这项研究通过利用大语言模型（LLMS）的上下文推理能力来解决对自主驾驶（AD）增强情境意识的关键需求。与传统的感知系统依赖于刚性的基于标签的注释，它将实时的多模式传感器数据集成到统一的，LLMS可读的知识库中，使LLMS能够动态理解和响应复杂的驾驶环境。为了克服LLM的固有延迟和模态局限性，主动检索型生成（RAG）是为AD设计的，并结合了一系列的促进链接机制，确保了快速且富有上下文的理解。使用现实世界中的全能（V2X）数据集的实验结果表明，感知和预测性能有了显着改善，突出了该框架在下一代AD系统中增强安全性，适应性和决策的潜力。

### MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation 
[[arxiv](https://arxiv.org/abs/2501.04614)] [[cool](https://papers.cool/arxiv/2501.04614)] [[pdf](https://arxiv.org/pdf/2501.04614)]
> **Authors**: Daniele Molino,Francesco Di Feola,Eliodoro Faiella,Deborah Fazzini,Domiziana Santucci,Linlin Shen,Valerio Guarrasi,Paolo Soda
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: Medcodi-M：多模式医学数据生成的多项目基础模型
- **领域**: 人工智能,机器学习
- **摘要**: 人工智能正在彻底改变医疗实践，提高诊断准确性和医疗保健。但是，它在医疗环境中的适应仍然面临着与数据可用性和隐私限制有关的重大挑战。合成数据已成为减轻这些问题的有前途的解决方案，在保留隐私的同时解决数据稀缺性。最近，潜在扩散模型已成为生成高质量合成数据的强大工具。同时，不同方式的整合引起了人们的兴趣，强调了能够处理多模式医学数据的模型的需求。现有的方法难以整合互补信息，并且缺乏同时产生方式的能力。为了应对这一挑战，我们提出了一种6.77亿参数模型Medcodi-M，该模型是为多模式医学数据生成而设计的，该模型遵循基础模型范式，利用对比度学习和大量数据，以构建共享的潜在空间，以捕获不同数据模式之间的关系。此外，我们介绍了多项目培训技术，该技术在不同的环境下大大提高了Medcodi-M的一代。我们广泛验证了Medcodi-M：首先，我们将其基准与Mimic-CXR数据集的五个竞争对手进行基准测试，这是一种用于胸部X射线和放射学报告生成的最新数据集。其次，我们与专家放射科医生进行视觉图灵测试，以评估生成数据的现实主义和临床相关性，从而确保与现实世界情景保持一致。最后，我们评估了Medcodi-M在解决医学领域的主要挑战方面的实用性，例如匿名，数据稀缺和不平衡学习。结果很有希望，证明了Medcodi-M在医疗环境中的适用性。项目页面位于https://cosbidev.github.io/medcodi-m/。

### InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection 
[[arxiv](https://arxiv.org/abs/2501.04575)] [[cool](https://papers.cool/arxiv/2501.04575)] [[pdf](https://arxiv.org/pdf/2501.04575)]
> **Authors**: Yuhang Liu,Pengxiang Li,Zishu Wei,Congkai Xie,Xueyu Hu,Xinchen Xu,Shengyu Zhang,Xiaotian Han,Hongxia Yang,Fei Wu
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: 14 pages, 7 figures, work in progress
- **标题**: Inviguiagent：具有本地推理和反思的多模式通才GUI剂
- **领域**: 人工智能,计算语言学,人机交互
- **摘要**: 由多模式大语言模型（MLLMS）提供动力的图形用户界面（GUI）代理，在计算设备（例如计算机和手机）上显示了任务自动化的巨大潜力。但是，现有的代理在多步推理和依赖文本注释方面面临挑战，从而限制了其有效性。我们介绍了\ textit {infiguiagent}，这是一种基于MLLM的GUI代理，训练有两个阶段的监督微调管道。第1阶段增强了GUI理解和基础等基本技能，而第2阶段则使用合成的数据整合了层次的推理和期望反射推理技能，以实现代理的本地推理能力。 \ textit {infiguiagent}在几个GUI基准上实现了竞争性能，突出了本地推理技能在增强GUI相互作用中对自动化任务的影响。资源可在\ url {https://github.com/reallm-labs/infiguiagent}中获得。

### Multimodal-to-Text Prompt Engineering in Large Language Models Using Feature Embeddings for GNSS Interference Characterization 
[[arxiv](https://arxiv.org/abs/2501.05079)] [[cool](https://papers.cool/arxiv/2501.05079)] [[pdf](https://arxiv.org/pdf/2501.05079)]
> **Authors**: Harshith Manjunath,Lucas Heublein,Tobias Feigl,Felix Ott
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: :68T30; 68T05ACM Class:H.1; H.5; I.4.9; I.4.10
- **标题**: 多模式到文本提示在大语言模型中使用用于GNSS干扰表征的功能嵌入式工程
- **领域**: 人工智能,信号处理
- **摘要**: 大型语言模型（LLMS）是在包括NLP，信息检索和推荐系统在内的各个领域应用的高级AI系统。尽管具有适应性和效率，但尚未广泛探索信号处理任务的LLM，尤其是在全球导航卫星系统（GNSS）干扰监控的领域。 GNSS干扰监控对于确保车辆定位在道路上的可靠性至关重要，这是许多应用程序的关键要求。但是，基于GNSS的定位容易受到干扰设备的干扰，这可能会损害其准确性。主要目的是识别，分类和减轻这些干扰。解释GNSS快照和相关的干扰物，由于固有的复杂性，包括多径效应，多种干扰类型，不同的传感器特征和卫星星座提出了重大挑战。在本文中，我们从大型GNSS数据集中提取功能，并使用LLAVA从广泛的知识库中检索相关信息。我们采用及时的工程来解释干扰和环境因素，并利用T-SNE分析功能嵌入。我们的发现表明，所提出的方法能够在GNSS上下文中进行视觉和逻辑推理。此外，我们的管道在干涉分类​​任务中优于最先进的机器学习模型。

### A General Retrieval-Augmented Generation Framework for Multimodal Case-Based Reasoning Applications 
[[arxiv](https://arxiv.org/abs/2501.05030)] [[cool](https://papers.cool/arxiv/2501.05030)] [[pdf](https://arxiv.org/pdf/2501.05030)]
> **Authors**: Ofir Marom
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: 15 pages, 7 figures
- **标题**: 用于多模式的基于案例的推理应用程序的一般检索增强生成框架
- **领域**: 人工智能,计算语言学
- **摘要**: 基于病例的推理（CBR）是一种基于经验的解决问题的方法，在该方法中，解决了解决案例的存储库来解决新案例。最近的研究表明，具有检索功能的大型语言模型（LLMS）可以通过检索类似情况并将其用作LLM查询的其他上下文来支持CBR管道的检索和重复使用阶段。大多数研究都集中在纯文本应用上，但是，在许多现实世界中，案例的组成部分是多模式的。在本文中，我们介绍了MCBR-rag，这是一种用于多模式CBR应用的一般抹布框架。 MCBR-rag框架将非文本案例组件转换为基于文本的表示形式，从而允许：1）学习可以索引以进行检索的特定应用程序的潜在表示，以及2）通过将所有情况组合在一起以获得更好的上下文来丰富提供给LLM的查询。我们通过在简化的Math-24应用程序和更复杂的后山应用程序上进行的实验来证明MCBR-rag的有效性。我们的经验结果表明，与没有提供上下文信息的基线LLM相比，MCBR-rag提高了发电质量。

### Unveiling the Potential of Text in High-Dimensional Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2501.07048)] [[cool](https://papers.cool/arxiv/2501.07048)] [[pdf](https://arxiv.org/pdf/2501.07048)]
> **Authors**: Xin Zhou,Weiqing Wang,Shilin Qu,Zhiqiang Zhang,Christoph Bergmeir
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: Accepted by NeurIPS24 TSALM Workshop
- **标题**: 在高维时间序列预测中揭示文本的潜力
- **领域**: 人工智能
- **摘要**: 传统上，时间序列预测集中在单变量和多元数值数据上，经常忽略合并多模式信息，尤其是文本数据的好处。在本文中，我们提出了一个新颖的框架，该框架将时间序列模型与大语言模型集成在一起，以改善高维时间序列的预测。受多模型模型的启发，我们的方法将时间序列和双重式结构中的文本数据结合在一起。信息融合会创建一个全面的表示，然后通过线性层进行处理以生成最终的预测。广泛的实验表明，合并文本可以增强高维时间序列的预测性能。这项工作为多模式时间序列预测的进一步研究铺平了道路。

### Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification 
[[arxiv](https://arxiv.org/abs/2501.06827)] [[cool](https://papers.cool/arxiv/2501.06827)] [[pdf](https://arxiv.org/pdf/2501.06827)]
> **Authors**: Shijing Chen,Mohamed Reda Bouadjenek,Shoaib Jameel,Usman Naseem,Basem Suleiman,Flora D. Salim,Hakim Hacid,Imran Razzak
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: 11 pages, 7 figures, 2 tables, and accepted by COLING 2025
- **标题**: 利用分类法和LLM进行改进的多模式分层分类
- **领域**: 人工智能
- **摘要**: 多级分层分类（MLHC）解决了在复杂的多层类结构中对项目进行分类的挑战。但是，传统的MLHC分类器通常依赖具有独立输出层的骨干模型，这些模型倾向于忽略类之间的层次关系。这种监督可能导致不一致的预测违反了基本的分类法。利用大型语言模型（LLMS），我们提出了一种新型的分类法限制的过渡性LLM-Agnostic框架进行多模式分类。这种进步的基石是模型在层次层面上执行一致性的能力。我们对MEP-3M数据集的评估 - 与常规LLM结构相比，具有各种层次级别的多模式电子商务产品数据集证明了性能的显着改善。

### ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation 
[[arxiv](https://arxiv.org/abs/2501.06598)] [[cool](https://papers.cool/arxiv/2501.06598)] [[pdf](https://arxiv.org/pdf/2501.06598)]
> **Authors**: Xuanle Zhao,Xianzhen Luo,Qi Shi,Chi Chen,Shuo Wang,Wanxiang Che,Zhiyuan Liu,Maosong Sun
> **First submission**: 2025-01-11
> **First announcement**: 2025-01-13
> **comment**: 13 pages, 6 figures
- **标题**: 图表编码器：用于图表到代码的多模式大型语言模型
- **领域**: 人工智能
- **摘要**: 多模式大语言模型（MLLM）在图表理解任务中表现出了显着的功能。但是，用文本描述解释图表通常会导致信息丢失，因为它无法完全捕获图表中嵌入的密集信息。相比之下，将图表分解为代码提供了无损表示，可以有效地包含所有关键细节。尽管现有的开源MLLM在图表理解任务方面取得了成功，但是当将其应用于图表到编码任务时，它们仍然面临两个主要挑战。为了应对这些挑战，我们提出了第一个专用的图表到代码MLLM，将\ textbf {ChartCoder}提出，它利用代码LLMS作为语言骨架来增强生成的代码的可执行性。此外，我们介绍\ textbf {Chart2Code-160K}，这是第一个用于图表到代码生成的大规模且多样的数据集，并提出了\ textbf {textbf {thippet of-thought（sot）}方法，它将直接对代码对代码的数据转换为进一步的数据。实验表明，ChartCoder只有7b参数，超过了图表到代码基准的现有开源MLLM，从而实现了出色的图表恢复和代码兴奋性。我们的代码将在https://github.com/thunlp/chartcoder上找到。

### A Multimodal Social Agent 
[[arxiv](https://arxiv.org/abs/2501.06189)] [[cool](https://papers.cool/arxiv/2501.06189)] [[pdf](https://arxiv.org/pdf/2501.06189)]
> **Authors**: Athina Bikaki,Ioannis A. Kakadiaris
> **First submission**: 2024-12-11
> **First announcement**: 2025-01-13
> **comment**: 9 pages
- **标题**: 多式联运社会代理人
- **领域**: 人工智能,计算语言学
- **摘要**: 近年来，大型语言模型（LLMS）在常识性推理任务中表现出了显着的进步。这种能力对于理解社会动态，互动和交流至关重要。但是，将计算机与这些社交能力集成的潜力仍然相对尚未探索。但是，将计算机与这些社交能力集成的潜力仍然相对尚未探索。本文介绍了Musa，这是一种基于LLM的多模式的代理商，该代理分析了量身定制的文本丰富的社交内容，以解决选定的以人为中心的内容分析任务，例如问答，视觉问题答案，标题生成和分类。它使用计划，推理，行动，优化，批评和完善策略来完成任务。我们的方法表明，Musa可以自动化和改善社会内容分析，从而帮助各种应用程序的决策过程。我们已经评估了我们的代理商的答案，标题生成和内容分类任务中的能力。穆萨的表现比我们的基线要好得多。

### Visual Language Models as Operator Agents in the Space Domain 
[[arxiv](https://arxiv.org/abs/2501.07802)] [[cool](https://papers.cool/arxiv/2501.07802)] [[pdf](https://arxiv.org/pdf/2501.07802)]
> **Authors**: Alejandro Carrasco,Marco Nedungadi,Enrico M. Zucchelli,Amit Jain,Victor Rodriguez-Fernandez,Richard Linares
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Updated version of the paper presented in 2025 AIAA SciTech. https://arc.aiaa.org/doi/10.2514/6.2025-1543
- **标题**: 视觉语言模型是空间域中的操作剂
- **领域**: 人工智能,空间物理学
- **摘要**: 本文探讨了视觉模型（VLM）作为空间域中的操作员代理的应用，重点介绍了软件和硬件操作范式。在大型语言模型（LLM）及其多模式扩展的基础上，我们研究了VLM如何增强空间任务中的自主控制和决策。在软件上下文中，我们在Kerbal空间程序差异游戏（KSPDG）模拟环境中采用VLM，使代理可以解释图形用户界面的视觉屏幕截图以执行复杂的轨道操作。在硬件上下文中，我们将VLM与配备摄像机的机器人系统集成在一起，以检查和诊断物理空间对象，例如卫星。我们的结果表明，VLM可以有效地处理视觉和文本数据，以生成上下文适当的动作，与传统方法和非偶像LLM在模拟任务中竞争，并在现实世界应用程序中表现出希望。

### From Screens to Scenes: A Survey of Embodied AI in Healthcare 
[[arxiv](https://arxiv.org/abs/2501.07468)] [[cool](https://papers.cool/arxiv/2501.07468)] [[pdf](https://arxiv.org/pdf/2501.07468)]
> **Authors**: Yihao Liu,Xu Cao,Tingting Chen,Yankai Jiang,Junjie You,Minghua Wu,Xiaosong Wang,Mengling Feng,Yaochu Jin,Jintai Chen
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: 56 pages, 11 figures, manuscript accepted by Information Fusion
- **标题**: 从屏幕到场景：对医疗保健体现AI的调查
- **领域**: 人工智能
- **摘要**: 全球医疗保健系统在效率，可及性和个性化方面面临持续的挑战。体现的AI（EMAI）由现代AI技术（例如多模式大语言模型和世界模型）提供支持，代表着一种变革性的边境，提供了增强的自主权，并具有与物理世界互动以应对这些挑战的能力。作为一个跨学科和快速发展的研究领域，“医疗保健中的EMAI”跨越了各种领域，例如算法，机器人技术和生物医学。这种复杂性强调了及时审查和分析以跟踪进步，应对挑战和促进跨学科合作的重要性。在本文中，我们提供了EMAI医疗保健“大脑”的全面概述，其中我们介绍了基础AI AI算法，以进行感知，驱动，计划和记忆，并专注于介绍涉及临床干预，日常护理和陪伴，基础设施支持以及生物医学研究的医疗保健应用程序。尽管有希望，但诸如安全问题，模拟平台和现实世界应用之间的差距，缺乏标准化的基准和跨学科范围内的不均匀进展的关键挑战所阻碍了EMAI用于医疗保健的发展。我们讨论技术障碍并探讨道德考虑，并为EMAI在医疗保健中的未来提供了前瞻性的看法。还引入了针对EMAI系统的智能水平的层次结构框架，以指导进一步的发展。通过提供系统的见解，这项工作旨在激发创新和实际应用，为智能，以患者为中心的医疗保健的新时代铺平道路。

### Lifelong Learning of Large Language Model based Agents: A Roadmap 
[[arxiv](https://arxiv.org/abs/2501.07278)] [[cool](https://papers.cool/arxiv/2501.07278)] [[pdf](https://arxiv.org/pdf/2501.07278)]
> **Authors**: Junhao Zheng,Chengming Shi,Xidi Cai,Qiuke Li,Duzhen Zhang,Chenxing Li,Dong Yu,Qianli Ma
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: 46 pages
- **标题**: 基于大语模型的代理人的终身学习：路线图
- **领域**: 人工智能
- **摘要**: 终身学习，也称为持续或增量学习，是通过使系统能够在动态环境中持续适应人工通用智能（AGI）的关键组成部分。尽管大型语言模型（LLM）在自然语言处理中表现出了令人印象深刻的功能，但现有的LLM代理通常是为静态系统设计的，并且缺乏随着时间的推移而适应新挑战的能力。这项调查是第一个系统地总结将终身学习纳入LLM基础代理的潜在技术。我们将这些代理的核心组件分为三个模块：多模式输入积分的感知模块，用于存储和检索不断发展的知识的存储模块以及与动态环境的接地相互作用的动作模块。我们强调了这些支柱如何共同实现持续适应，减轻灾难性遗忘并改善长期绩效。这项调查为研究人员和从业人员提供了路线图，以发展LLM代理商的终身学习能力，从而提供有关新兴趋势，评估指标和应用程序方案的见解。相关文献和资源可在\ href {this url} {https://github.com/qianlima-lab/awesome-lifelong-llm-agent}中获得。

### Natural Language-Assisted Multi-modal Medication Recommendation 
[[arxiv](https://arxiv.org/abs/2501.07166)] [[cool](https://papers.cool/arxiv/2501.07166)] [[pdf](https://arxiv.org/pdf/2501.07166)]
> **Authors**: Jie Tan,Yu Rong,Kangfei Zhao,Tian Bian,Tingyang Xu,Junzhou Huang,Hong Cheng,Helen Meng
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: 10 pages
- **标题**: 自然语言辅助多模式药物建议
- **领域**: 人工智能
- **摘要**: 组合药物建议（CMR）是医疗保健的一项基本任务，它为临床医生提供了为具有复杂健康状况的患者提供更精确处方的机会，尤其是在长期医疗服务的情况下。先前的研究工作试图从电子健康记录（EHRS）中提取有意义的信息，以促进组合药物的建议。现有的基于学习的方法进一步考虑了药物的化学结构，但忽略了清晰描述功能的文本药物描述。此外，从患者的EHR中得出的文本知识在很大程度上仍未得到充分利用。为了解决这些问题，我们介绍了自然语言辅助的多模式药物建议（NLA-MMR），这是一个多模式对齐框架，旨在从患者的观点和药物视图中学习知识。具体而言，NLA-MMR将CMR提出为患者和药物方式的对齐问题。在这种情况下，我们采用验证的语言模型（PLM）来提取有关患者和药物的内域知识，是两种方式的基础代表。在用药方式中，我们利用化学结构和文本描述来创建药物表示。在患者方式中，我们根据诊断，程序和症状的文本描述生成患者的表征。在三个公开访问数据集上进行的广泛实验表明，NLA-MMR实现了新的最先进的性能，Jaccard得分的平均平均提高4.72％。我们的源代码可在https://github.com/jtan1102/nla-mmr_cikm_2024上公开获得。

### CureGraph: Contrastive Multi-Modal Graph Representation Learning for Urban Living Circle Health Profiling and Prediction 
[[arxiv](https://arxiv.org/abs/2501.07157)] [[cool](https://papers.cool/arxiv/2501.07157)] [[pdf](https://arxiv.org/pdf/2501.07157)]
> **Authors**: Jinlin Li,Xiao Zhou
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: CUREGRAPH：对比度的多模式图表表示城市生活圈的学习健康分析和预测
- **领域**: 人工智能
- **摘要**: 在邻里水平上，老年人对健康状况下降的早期发现和预测对于城市规划和公共卫生政策制定至关重要。尽管现有研究肯定了生活环境与健康结果之间的联系，但大多数研究依赖于单个数据模式或多模式信息的简单特征串联，从而限制了它们全面介绍面向健康的城市环境的能力。为了填补这一空白，我们提出了Curegraph，这是城市健康预测的对比多模式表示学习框架，该学习框架采用基于图形的技术来推断每个社区城市生活圈子中老年人中常见慢性疾病的普遍性。 Curegraph利用丰富的多模式信息，包括居民区及其周围兴趣点的照片和文本评论，以产生城市社区的嵌入。通过将预训练的视觉和文本编码与图形建模技术整合在一起，Curegraph捕获了跨模式的空间依赖性，从而对针对老年人健康考虑的城市环境有了全面的了解。关于现实世界数据集的广泛实验表明，在老年人疾病风险预测任务中，Curegraph平均将最佳基线提高了$ 28 \％$。此外，该模型可以识别阶段的慢性疾病进展，并支持跨社区的比较公共卫生分析，为可持续的城市发展和增强的生活质量提供可行的见解。该代码可在https://github.com/jinlin2021/curegraph上公开获取。

### ADAM-1: AI and Bioinformatics for Alzheimer's Detection and Microbiome-Clinical Data Integrations 
[[arxiv](https://arxiv.org/abs/2501.08324)] [[cool](https://papers.cool/arxiv/2501.08324)] [[pdf](https://arxiv.org/pdf/2501.08324)]
> **Authors**: Ziyuan Huang,Vishaldeep Kaur Sekhon,Ouyang Guo,Mark Newman,Roozbeh Sadeghian,Maria L. Vaida,Cynthia Jo,Doyle Ward,Vanni Bucci,John P. Haran
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: 16 pages, 16 figures
- **标题**: ADAM-1：阿尔茨海默氏症检测和微生物组临床数据集成的AI和生物信息学
- **领域**: 人工智能
- **摘要**: 阿尔茨海默氏病分析1（ADAM）是一种多机语言模型（LLM）框架，旨在整合和分析多模式数据，包括微生物组概况，临床数据集和外部知识基础，以增强对阿尔茨海默氏病（AD）的理解和检测（AD）。 ADAM-1通过利用检索型生成（RAG）技术及其多代理体系结构，综合了来自不同数据源的见解，并使用文学驱动的证据将发现进行了上下文化。对XGBoost的比较评估显示，平均F1得分相似，但ADAM-1的差异显着降低，突出了其稳健性和一致性，尤其是在小型实验室数据集中。尽管目前针对二进制分类任务量身定制，但未来的迭代旨在纳入其他数据模式，例如神经影像学和生物标志物，以扩大阿尔茨海默氏症研究和诊断的可扩展性和适用性。

### CG-MER: A Card Game-based Multimodal dataset for Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2501.08182)] [[cool](https://papers.cool/arxiv/2501.08182)] [[pdf](https://arxiv.org/pdf/2501.08182)]
> **Authors**: Nessrine Farhat,Amine Bohi,Leila Ben Letaifa,Rim Slama
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: 8 pages, 2 figures and 4 tables. Sixteenth International Conference on Machine Vision (ICMV 2023), Yerevan, Armenia
- **标题**: CG-MER：基于卡游戏的多模式数据集用于情绪识别
- **领域**: 人工智能,计算机视觉和模式识别,人机交互
- **摘要**: 情感计算领域在探索情绪与新兴技术之间的关系方面取得了重大进步。本文通过引入专门为情感识别而设计的综合法国多模式数据集提出了对这一领域的新颖而宝贵的贡献。数据集涵盖了三种主要方式：面部表情，语音和手势，为情感提供了整体观点。此外，数据集有可能结合其他方式，例如自然语言处理（NLP），以扩大情绪识别研究的范围。该数据集是通过让参与者参加纸牌游戏会话来策划的，在此期间，他们在回答各种问题时被提示表达一系列情绪。该研究包括10个会议，有20名参与者（9名女性和11名男性）。该数据集是进一步研究情感识别的宝贵资源，并为探索人类情感与数字技术之间的复杂联系提供了途径。

### LLM-Ehnanced Holonic Architecture for Ad-Hoc Scalable SoS 
[[arxiv](https://arxiv.org/abs/2501.07992)] [[cool](https://papers.cool/arxiv/2501.07992)] [[pdf](https://arxiv.org/pdf/2501.07992)]
> **Authors**: Muhammad Ashfaq,Ahmed R. Sadik,Tommi Mikkonen,Muhammad Waseem,Niko Mäkitalo
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: LLM-Enhnanced Holonic Architecture用于临时可伸缩SOS
- **领域**: 人工智能,新兴技术,多代理系统,软件工程
- **摘要**: 随着现代系统系统（SOS）变得越来越适应性和人性化，传统的建筑通常很难支持互操作性，可重构性和有效的人类系统互动。本文通过促进SOS艺术园艺结构的状态来解决这些挑战，并为支持这些适应性需求提供了两项主要贡献。首先，我们为霍姆提出了一个分层的体系结构，其中包括推理，通信和功能层。这种设计通过改善数据交换和集成来促进异质组成系统之间的无缝互操作性。其次，受到智能制造原则的启发，我们介绍了专业的霍隆，即主管，计划者，任务和资源圆顶，旨在增强SOS的适应性和可重构性。这些专业的霍姆在其推理层中利用大型语言模型来支持决策并确保实时适应性。我们通过一项针对智能城市运输的3D移动案例研究来证明我们的方法，展示了其管理复杂的多模式SOS环境的潜力。此外，我们提出了评估方法来评估体系结构效率和可扩展性，从而通过模拟和现实世界实施为未来的经验验证奠定了基础。

### Text Semantics to Flexible Design: A Residential Layout Generation Method Based on Stable Diffusion Model 
[[arxiv](https://arxiv.org/abs/2501.09279)] [[cool](https://papers.cool/arxiv/2501.09279)] [[pdf](https://arxiv.org/pdf/2501.09279)]
> **Authors**: Zijin Qiu,Jiepeng Liu,Yi Xia,Hongtuo Qi,Pengkun Liu
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 灵活设计的文本语义：一种基于稳定扩散模型的住宅布局生成方法
- **领域**: 人工智能
- **摘要**: 基于AI的住宅布局设计的灵活性仍然是一个重大挑战，因为基于规则的启发式方法和基于图的传统方法通常缺乏灵活性，并且需要用户的大量设计知识。为了解决这些局限性，我们提出了一种基于稳定扩散模型的跨模式设计方法，用于生成灵活的住宅布局。该方法为学习目标提供了多种输入类型，从而允许用户指定边界和布局。它结合了自然语言作为设计约束，并引入了ControlNet，以通过两种不同的途径使稳定的布局生成。我们还提出了一个将设计专业知识封装在知识图中的方案，并将其转化为自然语言，从而提供了可解释的设计知识表示。输入选项的这种可理解性和多样性使专业人士和非专业人士可以直接表达设计要求，从而提高灵活性和可控性。最后，即使有关房间区域或连接不完整的特定语义信息，实验在多模式约束下验证所提出的方法的灵活性更好。

### SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector 
[[arxiv](https://arxiv.org/abs/2501.08814)] [[cool](https://papers.cool/arxiv/2501.08814)] [[pdf](https://arxiv.org/pdf/2501.08814)]
> **Authors**: Kyeongryul Lee,Heehyeon Kim,Joyce Jiyoung Whang
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: 6 pages, 2 figures, 1 tables. AI for Public Missions (AIPM) Workshop at the 39th AAAI Conference on Artificial Intelligence (AAAI 2025)
- **标题**: SAIF：评估公共部门生成AI风险的综合框架
- **领域**: 人工智能,计算语言学,计算机与社会
- **摘要**: 在公共部门迅速采用了生成AI，其中包括从自动公共援助到福利服务和移民过程的各种应用程序，强调了其变革潜力，同时强调了对彻底风险评估的紧迫需求。尽管存在日益增长的存在，但对公共部门中与AI驱动系统相关的风险的评估仍然不足。在基于来自各种政府政策和公司准则的AI风险的既定分类学基础上，我们研究了公共部门中生物AI带来的关键风险，同时扩展了范围以说明其多模式能力。此外，我们提出了一个系统的数据生成框架，用于评估生成AI（SAIF）的风险。 SAIF涉及四个关键阶段：分解风险，设计场景，应用越狱方法以及探索及时类型。它确保了系统的及时数据的系统性和一致的生成，从而促进了全面的评估，同时为减轻风险提供了坚实的基础。此外，SAIF旨在适应新兴的越狱方法和不断发展的及时类型，从而实现有效的响应以无法预见的风险情况。我们认为，这项研究可以在促进生成AI与公共部门的安全和负责任的整合中发挥至关重要的作用。

### Exploring the Implementation of AI in Early Onset Interviews to Help Mitigate Bias 
[[arxiv](https://arxiv.org/abs/2501.09890)] [[cool](https://papers.cool/arxiv/2501.09890)] [[pdf](https://arxiv.org/pdf/2501.09890)]
> **Authors**: Nishka Lal,Omar Benkraouda
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: No comments
- **标题**: 探索在早期访谈中实施AI的实施，以帮助减轻偏见
- **领域**: 人工智能
- **摘要**: 本文调查了人工智能（AI）在早期招聘访谈中的应用，以减少固有的偏见，特别是情感偏见。传统的访调员通常会遇到几种偏见，包括访调员偏见，社会可取性效果，甚至确认偏见。反过来，这导致了非包装的招聘实践，以及劳动力较少的劳动力。这项研究进一步分析了当今市场中存在的各种AI干预措施，例如多模式平台和交互式候选评估工具，以评估AI当前在早期招聘中的市场使用情况。但是，本文旨在使用开发的独特的AI系统来转录和分析访谈动态，该系统强调技能和知识而不是情感情感。结果表明，AI有效地使情感驱动的偏见最小化了41.2％，这表明其在公司的招聘过程中革命性的权力以提高股票和效率。

### The Goofus & Gallant Story Corpus for Practical Value Alignment 
[[arxiv](https://arxiv.org/abs/2501.09707)] [[cool](https://papers.cool/arxiv/2501.09707)] [[pdf](https://arxiv.org/pdf/2501.09707)]
> **Authors**: Md Sultan Al Nahian,Tasmia Tasrin,Spencer Frazier,Mark Riedl,Brent Harrison
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: Accepted by International Conference on Machine Learning and Applications (ICMLA) 2024. Main Conference, Long Paper
- **标题**: 傻瓜和勇敢的故事语料库以实用价值对齐
- **领域**: 人工智能
- **摘要**: 价值观或原则是人类社会的关键要素，它会根据公认的标准社会规则来影响人们行事和发挥作用，以维持社会秩序。随着人工智能系统在人类社会中变得无处不在，这是一个主要问题，他们可能违反这些规范或价值观并可能造成伤害。因此，为防止故意或无意的伤害，预计AI系统将采取与这些原则保持一致的行动。展示这种行为的培训系统很困难，并且通常需要专门的数据集。这项工作提出了一个多模式数据集，该数据集说明了通过自然语言和艺术图像描述的现实情况中规范性和非规范行为。该培训集包含旨在教幼儿社会原则的精选图像集。我们认为，鉴于这一事实，这是用于培训社会规范代理的理想数据集。

### YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents in Augmented Reality Tasks 
[[arxiv](https://arxiv.org/abs/2501.09355)] [[cool](https://papers.cool/arxiv/2501.09355)] [[pdf](https://arxiv.org/pdf/2501.09355)]
> **Authors**: Saptarashmi Bandyopadhyay,Vikas Bahirwani,Lavisha Aggarwal,Bhanu Guda,Lin Li,Andrea Colaco
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: Preprint
- **标题**: Yeti（尚未干预）多模式AI代理在增强现实任务中的主动干预措施
- **领域**: 人工智能,计算机视觉和模式识别,新兴技术,多代理系统
- **摘要**: 多模式AI代理是AI模型，具有交互式和合作协助人类用户解决日常任务的能力。增强现实（AR）磨损设备可以通过为AI代理提供以自我为中心的多模式（音频和视频）观察能力来唯一地改善用户的体验来解决程序日常任务。这样的AR功能可以帮助AI代理人看到并聆听用户采取的动作，这些操作与人类用户的多模式能力有关。现有的AI代理，大型语言模型（LLM）或多模式视觉模型（VLM）本质上是反应性的，这意味着模型无法在不阅读或聆听人类用户的提示的情况下采取行动。另一方面，AI代理人的积极性可以帮助人类用户检测并纠正代理观察到的任务中的任何错误，在用户正确执行任务时鼓励他们，或者只是与用户进行对话 - 类似于人类的教学或协助用户。我们尚未介入（Yeti）多模式的代理人着重于确定可能需要代理人主动干预的情况的研究问题。这使代理商可以理解何时可以与人类用户进行对话，以帮助用户纠正使用AR等任务（例如烹饪）的错误。我们的Yeti代理学习了基于连续视频帧上结构相似性（SSIM）的可解释概念（SSIM）理解信号。我们还定义了AI代理可以学会识别的对齐信号是否与用户对任务的操作相对应的视频帧是否与预期操作一致。我们的AI代理使用这些信号来确定何时应该主动干预。我们将我们的结果比较了全天师多模式基准的主动干预实例，以指导用户完成程序任务的专家代理。

### MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science 
[[arxiv](https://arxiv.org/abs/2501.10768)] [[cool](https://papers.cool/arxiv/2501.10768)] [[pdf](https://arxiv.org/pdf/2501.10768)]
> **Authors**: Erle Zhu,Yadi Liu,Zhe Zhang,Xujun Li,Jin Zhou,Xinjie Yu,Minlie Huang,Hongning Wang
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: 地图：推进专家级物理科学中的多模式推理
- **领域**: 人工智能
- **摘要**: 当前的多模式大语模型（MLLM）在广泛的文本和图像语料库中进行了预先培训，在一般的视觉推理任务中显示出强大的功能。但是，他们的性能仍然缺乏物理领域，这些域需要基于多模式信息来理解具有复杂物理结构和定量分析的图表。为了解决这个问题，我们开发了一个新的框架，该框架将基于MLLM的物理感知和仿真（MAPS）称为多模式科学推理。地图将专家级的多模式推理任务分解为通过物理感知模型（PPM）的物理图理解，并通过模拟器通过物理知识推理。 PPM模块是通过使用配对物理图和相应模拟语言描述的精心设计的合成数据来微调视觉语言模型获得的。在推理阶段，地图集成了PPM提供的输入图的仿真语言描述，以及通过使用MLLM的仿真过程获得的结果，以得出基本的基本原理和最终答案。使用我们收集的大学级电路分析问题进行了验证，地图显着提高了MLLM的推理准确性，并胜过所有现有模型。结果证实地图为增强MLLM的多模式科学推理能力提供了有希望的方向。发表本文后，我们将发布用于实验的代码，模型和数据集。

### Kimi k1.5: Scaling Reinforcement Learning with LLMs 
[[arxiv](https://arxiv.org/abs/2501.12599)] [[cool](https://papers.cool/arxiv/2501.12599)] [[pdf](https://arxiv.org/pdf/2501.12599)]
> **Authors**: Kimi Team,Angang Du,Bofei Gao,Bowei Xing,Changjiu Jiang,Cheng Chen,Cheng Li,Chenjun Xiao,Chenzhuang Du,Chonghua Liao,Chuning Tang,Congcong Wang,Dehao Zhang,Enming Yuan,Enzhe Lu,Fengxiang Tang,Flood Sung,Guangda Wei,Guokun Lai,Haiqing Guo,Han Zhu,Hao Ding,Hao Hu,Hao Yang,Hao Zhang, et al. (69 additional authors not shown)
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: 25 pages
- **标题**: Kimi K1.5：使用LLMS缩放加强学习
- **领域**: 人工智能,机器学习
- **摘要**: 在下一代币预测的情况下进行语言模型已被证明对缩放计算有效，但仅限于可用培训数据的数量。扩展加强学习（RL）为继续改善人工智能的新轴解锁了新轴，并承诺大型语言模型（LLMS）可以通过学习奖励来探索培训来扩展其培训数据。但是，先前发表的工作并未产生竞争成果。鉴于此，我们报告了Kimi K1.5的培训实践，这是我们接受RL培训的最新多模式LLM，包括其RL培训技术，多模式数据配方和基础架构优化。长上下文缩放和改进的策略优化方法是我们方法的关键要素，它可以建立一个简单，有效的RL框架，而无需依赖更复杂的技术，例如蒙特卡洛树搜索，价值功能和过程奖励模型。值得注意的是，我们的系统在多个基准和模式之间实现了最新的推理性能 - 例如，Aime上的77.5，在数学500上为96.2，在CodeForces上为94-1％，在Mathvista上为74.9，在Mathvista上 - 匹配Openai的O1。此外，我们提出了有效的long2short方法，这些方法使用长期训练技术来改善短型模型，从而产生最先进的短秘密推理结果，例如，在Aime上进行60.8，在Aime上，在Math500上的94.6，在LiveCodeBench上的94.6，在LiveCodeBench上 - 超过现有的短型型号，例如GPT-4O和Claude Sonnet 3.5（ +550％）（ +550％）（ +550％）（ +550％）。

### Bridging Visualization and Optimization: Multimodal Large Language Models on Graph-Structured Combinatorial Optimization 
[[arxiv](https://arxiv.org/abs/2501.11968)] [[cool](https://papers.cool/arxiv/2501.11968)] [[pdf](https://arxiv.org/pdf/2501.11968)]
> **Authors**: Jie Zhao,Kang Hao Cheong,Witold Pedrycz
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: 桥接可视化和优化：图形结构化优化的多模式大语言模型
- **领域**: 人工智能,机器学习
- **摘要**: 由于其非线性和错综复杂的性质，图形结构化的组合挑战本质上是困难的，通常使传统的计算方法无效或昂贵。但是，通过利用我们先天的空间推理能力的视觉表述，人类可以更自然地应对这些挑战。在这项研究中，我们建议将图形转换为图像，以准确地保留其高阶结构特征，从而彻底改变了用于求解图形结构化组合任务的表示。这种方法允许机器在应对复杂的组合挑战时模仿类似人类的处理。通过将由多模式大语言模型（MLLM）提供支持的创新范式与简单的搜索技术相结合，我们旨在开发一个新颖有效的框架来解决此类问题。我们对MLLM的调查跨越了各种基于图的任务，从影响最大化等组合问题到网络拆卸的顺序决策，以及解决六个基本与图形相关的问题。我们的发现表明，MLLM具有出色的空间智能和处理这些问题的独特能力，从而大大促进了机器以类似于人类认知的深度和直觉来理解和分析图形结构化数据的潜力。这些结果还意味着，将MLLM与简单的优化策略集成在一起可能会形成一种新颖而有效的方法，用于导航图形结构化的组合挑战，而无需复杂的衍生，计算要求训练和微调。

### Distributed Multi-Agent Coordination Using Multi-Modal Foundation Models 
[[arxiv](https://arxiv.org/abs/2501.14189)] [[cool](https://papers.cool/arxiv/2501.14189)] [[pdf](https://arxiv.org/pdf/2501.14189)]
> **Authors**: Saaduddin Mahmud,Dorian Benhamou Goldfajn,Shlomo Zilberstein
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 使用多模式基础模型的分布式多代理协调
- **领域**: 人工智能,机器学习,多代理系统
- **摘要**: 分布式约束优化问题（DCOPS）为多代理协调提供了有力的框架，但通常依赖于劳动密集型的手动问题构建。为了解决这个问题，我们介绍了VL-DCOPS，该框架利用大型多模式模型（LFM）自动从视觉和语言指令中生成约束。然后，我们引入了用于求解VL-DCOPS的试剂原型的频谱：从将某些算法决定委托给LFM的神经符号剂到完全取决于LFM的完全依赖于LFM进行协调的完全神经药物。我们使用最先进的LLM（大语言模型）和VLMS（视觉语言模型）在三个新颖的VL-DCOP任务上评估了这些代理原型，并比较了它们各自的优势和缺点。最后，我们讨论这项工作如何扩展到DCOP文献中更广泛的边界挑战。

### Diffusion-based Hierarchical Negative Sampling for Multimodal Knowledge Graph Completion 
[[arxiv](https://arxiv.org/abs/2501.15393)] [[cool](https://papers.cool/arxiv/2501.15393)] [[pdf](https://arxiv.org/pdf/2501.15393)]
> **Authors**: Guanglin Niu,Xiaowei Zhang
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: The version of a full paper accepted to DASFAA 2025
- **标题**: 基于扩散的分层负抽样，用于多模式知识图完成
- **领域**: 人工智能,计算语言学
- **摘要**: 多模式知识图完成（MMKGC）旨在解决多模式知识图（MMKGS）中缺少知识的关键问题，以解决其更好的应用程序。但是，先前的MMGKC和负抽样（NS）方法都忽略了多模式信息的使用，从而从各种语义水平和硬度水平产生了多样化和高质量的负三元，从而限制了训练MMKGC模型的有效性。因此，我们提出了一种针对MMKGC任务量身定制的新型基于扩散的层次负抽样（DHNS）方案，该方案通过利用基于扩散的基于扩散的层次嵌入产生（DIFFHEG）来应对产生高质量负三元三元的挑战，从而逐渐在实体和关系以及多束式的语义上逐渐条件。此外，我们制定了负三重自适应训练（NTAT）策略，该策略会动态调整与合成的负三元三元素的硬度水平相关的训练边缘，从而促进了更强大，更有效的学习程序，以区分正面和负三元。在三个MMKGC基准数据集上进行的广泛实验表明，我们的框架的表现优于几种最先进的MMKGC模型和负抽样技术，这说明了我们的DHNS在培训MMKGC模型中的有效性。本文的源代码和数据集可在https://github.com/ngl567/dhns上找到。

### A Causality-aware Paradigm for Evaluating Creativity of Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.15147)] [[cool](https://papers.cool/arxiv/2501.15147)] [[pdf](https://arxiv.org/pdf/2501.15147)]
> **Authors**: Zhongzhan Huang,Shanshan Zhong,Pan Zhou,Shanghua Gao,Marinka Zitnik,Liang Lin
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: Accepted by TPAMI. arXiv admin note: text overlap with arXiv:2312.02439
- **标题**: 一种因果关系，用于评估多模式模型的创造力
- **领域**: 人工智能,人机交互
- **摘要**: 最近，已经开发了许多基准来评估大语言模型（LLMS）的逻辑推理能力。但是，由于创造力的主观，多样化和数据筛选性质，尤其是在多模式的情况下，评估LLMS同样重要的创意能力是具有挑战性的。在本文中，我们考虑了评估多模式LLM的创造力的全面管道，重点是合适的评估平台和方法。首先，我们找到了Oogiri游戏，这是一项以创造力为导向的任务，需要幽默，关联思维以及对文本，图像或两者兼而有之意外响应的能力。该游戏与现代多模式LLM的投入输出结构非常吻合，并从丰富的高质量，人类宣传的创意响应的存储库中受益，使其成为研究LLM创造力的理想平台。接下来，除了使用Oogiri游戏进行排名和选择等标准评估之外，我们建议Lotbench（一种交互式，因果关系感知评估框架）进一步解决标准评估中的一些内在风险，例如信息泄漏和有限的可解释性。拟议的Lotbench不仅可以更有效地量化LLM的创造力，而且还可以看到基本的创造性思维过程。我们的结果表明，尽管大多数LLM都表现出限制的创造力，但LLMS和人类之间的性能差距并非无法克服。此外，我们观察到来自多模式认知基准MMMU和Lotbench的结果之间的密切相关性，但与传统创造力指标的联系仅较弱。这表明Lotbench可以更好地与人类的认知理论保持一致，从而在创造力的早期阶段强调了认知是关键的基础，并可以弥合各种概念。 https://lotbench.github.io

### Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models 
[[arxiv](https://arxiv.org/abs/2501.16215)] [[cool](https://papers.cool/arxiv/2501.16215)] [[pdf](https://arxiv.org/pdf/2501.16215)]
> **Authors**: Huayu Li,Xiwen Chen,Ci Zhang,Stuart F. Quan,William D. S. Killgore,Shu-Fen Wung,Chen X. Chen,Geng Yuan,Jin Lu,Ao Li
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 在医疗时间序列上增强多模式大语言模型的视觉检查能力，并具有支持性的保融和可解释的小型专业模型
- **领域**: 人工智能,机器学习,信号处理
- **摘要**: 大型语言模型（LLM）在视觉检查医学时序列数据方面具有显着的功能，可实现与人类临床医生相当的能力。但是，它们的广泛范围限制了特定于域的精度，并且专有权重阻碍了专用数据集的微调。相比之下，小型专业模型（SSM）在目标任务中表现出色，但缺乏复杂的临床决策所需的上下文推理。为了应对这些挑战，我们提出了CONMIL（将多个实例学习），这是一种与LLM无缝集成的决策支持SSM。通过使用多个实例学习（MIL）来识别临床意义的信号段和校准设置值输出的共形预测，Conmil增强了LLMS用于医疗时间序列分析的解释能力。实验结果表明，CONMIL显着提高了最先进的LLM的性能，例如ChatGpt4.0和Qwen2-VL-7B。具体而言，与独立的46.13％和13.16％的独立LLM精度相比，在心律失常检测和睡眠分期中，自信样品的自信样品中支持的QWEN2-VL-7B获得了94.92％和96.82％的精度。这些发现凸显了Conmil在特定于任务的精度和更广泛的上下文推理上的潜力，从而实现了更可靠和可解释的AI驱动临床决策支持。

### Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding 
[[arxiv](https://arxiv.org/abs/2501.17310)] [[cool](https://papers.cool/arxiv/2501.17310)] [[pdf](https://arxiv.org/pdf/2501.17310)]
> **Authors**: Yun-Shiuan Chuang,Nikunj Harlalka,Sameer Narendran,Alexander Cheung,Sizhe Gao,Siddharth Suresh,Junjie Hu,Timothy T. Rogers
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: No comments
- **标题**: 探测LLM世界模型：通过人群解码的智慧增强猜测
- **领域**: 人工智能,人机交互
- **摘要**: 猜测是进行近似数量估计的任务，是一个常见的现实挑战。但是，在大型语言模型（LLM）和视觉语言模型（VLMS）研究中，它在很大程度上被忽略了。我们介绍了一个新颖的猜测数据集，大理石。该数据集要求一个数据集估计有或没有伴随图像的情况下，可以估计有多少个项目（例如大理石）适合容器（例如，一个单杯测量杯）。受到``人群的智慧''（WOC）的社会科学概念的启发 - 从人群中汲取了估计值的中位数，这在猜测中被证明有效，我们建议``WOC解码'''LLM猜测''''''''''我们表明，LLMS/VLM在猜测方面表现良好，这表明它们具有一定程度的猜测“世界模型”。此外，与人类绩效类似，WOC解码方法提高了LLM/VLM猜测精度。此外，在多模式条件下包含图像可以增强模型性能。这些结果突出了WOC解码策略对LLMS/VLM的价值和位置猜测，作为评估LLMS/VLMS世界模型的探针。由于LLMS的世界模型是许多现实世界任务的基本先决条件，例如，人类团队的组合，我们的发现对AI社区具有广泛的影响。

### SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training 
[[arxiv](https://arxiv.org/abs/2501.17161)] [[cool](https://papers.cool/arxiv/2501.17161)] [[pdf](https://arxiv.org/pdf/2501.17161)]
> **Authors**: Tianzhe Chu,Yuexiang Zhai,Jihan Yang,Shengbang Tong,Saining Xie,Dale Schuurmans,Quoc V. Le,Sergey Levine,Yi Ma
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: Website at https://tianzhechu.com/SFTvsRL
- **标题**: SFT记忆，RL概括：基础模型后培训的比较研究
- **领域**: 人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 监督的微调（SFT）和增强学习（RL）是基础模型的广泛使用的培训技术。但是，它们在增强模型概括功能中的作用尚不清楚。本文研究了SFT和RL在泛化和记忆方面的差异，重点是基于文本的规则变体和视觉变体。我们介绍了一家算术推理卡游戏的通用点，并采用V-irl（一种现实世界的导航环境）来评估使用SFT训练和RL训练的模型如何在文本和视觉域中都看不见变体。我们表明，RL，尤其是在接受基于结果的奖励培训时，可以概括基于规则的文本和视觉变体。相比之下，SFT倾向于记住培训数据并努力概括分布场景。进一步的分析表明，RL提高了该模型的基本视觉识别能力，从而有助于其在视觉域中增强概括。尽管RL具有出色的概括，但我们表明SFT对于有效的RL培训仍然至关重要。 SFT稳定了模型的输出格式，从而使后续RL能够实现其性能提高。这些发现证明了RL在复杂的多模式任务中获取可推广知识的能力。

### Revisit Mixture Models for Multi-Agent Simulation: Experimental Study within a Unified Framework 
[[arxiv](https://arxiv.org/abs/2501.17015)] [[cool](https://papers.cool/arxiv/2501.17015)] [[pdf](https://arxiv.org/pdf/2501.17015)]
> **Authors**: Longzhong Lin,Xuewu Lin,Kechun Xu,Haojian Lu,Lichao Huang,Rong Xiong,Yue Wang
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: No comments
- **标题**: 重访混合模型用于多机构模拟：统一框架内的实验研究
- **领域**: 人工智能,多代理系统,机器人技术
- **摘要**: 模拟在评估自主驾驶系统中起着至关重要的作用，在自主驾驶系统中，实际的多代理行为是关键方面。在多代理模拟中，主要挑战包括行为多模式和闭环分布变化。在这项研究中，我们重新审视用于生成多模式剂行为的混合模型，该模型可以涵盖包括连续混合模型和类似GPT的离散模型在内的主流方法。此外，我们引入了针对混合模型定制的闭环样品生成方法，以减轻分布变化。在统一混合模型〜（UNIMM）框架中，我们从模型和数据角度识别关键配置。我们对各种模型配置进行系统检查，包括正组件匹配，连续回归，预测范围和组件数量。此外，我们对数据配置的研究突出了闭环样品在实现现实模拟中的关键作用。为了将闭环样品的好处扩展到更广泛的混合模型中，我们进一步解决了快捷方式学习和非政策学习问题。利用我们探索的见解，UNIMM框架中提出的不同变体（包括离散，无锚和基于锚的模型）都在WOSAC基准上实现了最新性能。

### Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling 
[[arxiv](https://arxiv.org/abs/2501.17811)] [[cool](https://papers.cool/arxiv/2501.17811)] [[pdf](https://arxiv.org/pdf/2501.17811)]
> **Authors**: Xiaokang Chen,Zhiyu Wu,Xingchao Liu,Zizheng Pan,Wen Liu,Zhenda Xie,Xingkai Yu,Chong Ruan
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: Research paper. arXiv admin note: text overlap with arXiv:2410.13848
- **标题**: Janus-Pro：通过数据和模型缩放的统一多模式理解和生成
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 在这项工作中，我们介绍了先前作品Janus的高级版本Janus-Pro。具体而言，Janus-Pro结合了（1）优化的训练策略，（2）扩展的训练数据，（3）扩展到更大的模型大小。通过这些改进，Janus-Pro在多模式的理解和文本对图像遵循能力方面都取得了重大进步，同时还提高了文本对图像生成的稳定性。我们希望这项工作将激发该领域的进一步探索。代码和模型公开可用。

### Reflections on "Can AI Understand Our Universe?" 
[[arxiv](https://arxiv.org/abs/2501.17507)] [[cool](https://papers.cool/arxiv/2501.17507)] [[pdf](https://arxiv.org/pdf/2501.17507)]
> **Authors**: Yu Wang
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: Invited talk at the 17th Marcel Grossmann Meeting, associated with arXiv:2404.10019, to be published in the International Journal of Modern Physics D
- **标题**: 对“人工智能可以理解我们的宇宙”的思考吗？
- **领域**: 人工智能,高能天体物理现象,天体物理学仪器和方法
- **摘要**: 本文简要讨论了AI的哲学和技术方面。它着重于两个理解的概念：直觉和因果关系，并突出显示了三种AI技术：变形金刚，经过思考的推理和多模式处理。我们预计，这些技术代表有前途的进步，原则上AI可以形成理解。

### Semantic Web and Creative AI -- A Technical Report from ISWS 2023 
[[arxiv](https://arxiv.org/abs/2501.18542)] [[cool](https://papers.cool/arxiv/2501.18542)] [[pdf](https://arxiv.org/pdf/2501.18542)]
> **Authors**: Raia Abu Ahmad,Reham Alharbi,Roberto Barile,Martin Böckling,Francisco Bolanos,Sara Bonfitto,Oleksandra Bruns,Irene Celino,Yashrajsinh Chudasama,Martin Critelli,Claudia d'Amato,Giada D'Ippolito,Ioannis Dasoulas,Stefano De Giorgis,Vincenzo De Leo,Chiara Di Bonaventura,Marco Di Panfilo,Daniil Dobriy,John Domingue,Xuemin Duan,Michel Dumontier,Sefika Efeoglu,Ruben Eschauzier,Fakih Ginwa,Nicolas Ferranti, et al. (52 additional authors not shown)
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: Technical Report
- **标题**: 语义网络和创意AI- ISWS 2023的技术报告
- **领域**: 人工智能
- **摘要**: 国际语义网络研究学校（ISWS）是一个为期一周的密集计划，旨在将参与者浸入该领域。本文档报告了由十个学生团队进行的合作努力，每个团队由高级研究员作为他们的导师指导，参加了ISWS 2023。每个团队为创意AI的主题提供了不同的观点，并由一系列研究问题证实，这是他们调查的主要主题。 2023年的ISWS专注于语义网络技术与创意AI的交集。 ISWS 2023探索了语义Web技术与创意AI之间的各种交集。重点的关键领域是LLM作为知识工程的支持工具的潜力。 Participants also delved into the multifaceted applications of LLMs, including legal aspects of creative content production, humans in the loop, decentralised approaches to multimodal generative AI models, nanopublications and AI for personal scientific knowledge graphs, commonsense knowledge in automatic story and narrative completion, generative AI for art critique, prompt engineering, automatic music composition, commonsense prototyping and conceptual blending, and引起隐性知识。随着大型语言模型和语义技术的不断发展，新的令人兴奋的前景正在出现：创造性表达与事实知识之间的界限变得越来越渗透和多孔的未来，从而导致知识世界既有信息又鼓舞人心。

### MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding 
[[arxiv](https://arxiv.org/abs/2501.18362)] [[cool](https://papers.cool/arxiv/2501.18362)] [[pdf](https://arxiv.org/pdf/2501.18362)]
> **Authors**: Yuxin Zuo,Shang Qu,Yifei Li,Zhangren Chen,Xuekai Zhu,Ermo Hua,Kaiyan Zhang,Ning Ding,Bowen Zhou
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: MEDXPERTQA：基准测试专家级的医学推理和理解
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 我们介绍了MEDXPERTQA，这是一种高度挑战性和全面的基准，用于评估专家级的医学知识和高级推理。 MEDXPERTQA包括4,460个问题，涵盖17个专业和11个身体系统。它包括两个子集，文本评估的文本和用于多模式评估的MM。值得注意的是，MM引入了专家级考试问题，其中包含各种图像和丰富的临床信息，包括患者记录和检查结果，将其与传统的医学多模式基准分开，并具有由图像标题产生的简单QA对。 MEDXPERTQA应用了严格的过滤和增强，以解决MEDQA等现有基准的难度不足，并结合了专业委员会的问题以提高临床相关性和全面性。我们执行数据综合以减轻数据泄漏风险并进行多轮专家审查，以确保准确性和可靠性。我们评估了MEDXPERTQA上的16个领先模型。此外，医学与现实世界的决策密切相关，为评估数学和代码以外的推理能力提供了丰富而代表性的环境。为此，我们开发了一个面向推理的子集，以促进对O1样模型的评估。

## 硬件架构(cs.AR:Hardware Architecture)

该领域共有 1 篇论文

### EXION: Exploiting Inter- and Intra-Iteration Output Sparsity for Diffusion Models 
[[arxiv](https://arxiv.org/abs/2501.05680)] [[cool](https://papers.cool/arxiv/2501.05680)] [[pdf](https://arxiv.org/pdf/2501.05680)]
> **Authors**: Jaehoon Heo,Adiwena Putra,Jieon Yoon,Sungwoong Yune,Hangyeol Lee,Ji-Hoon Kim,Joo-Young Kim
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: To appear in 2025 IEEE International Symposium on High-Performance Computer Architecture (HPCA 2025)
- **标题**: 派别：利用扩散模型的介质和识字输出稀疏性
- **领域**: 硬件架构,人工智能,机器学习
- **摘要**: 在过去的几年中，扩散模型已成为新颖的AI解决方案，从文本提示产生了多种模式输出。尽管具有能力，但它们在计算方面仍面临挑战，例如由于其迭代架构而导致的过多潜伏期和能源消耗。尽管可以应用专门用于变压器加速的先前工作，但扩散模型的迭代性质仍未解决。在本文中，我们介绍了第一个SW-HW共同设计的扩散加速器，它通过利用扩散模型中唯一的介入和内部介质输出稀疏性来解决计算挑战。为此，我们提出了两个SW级优化。首先，我们介绍了FFN恢复算法，该算法在不同的迭代（Inter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-Iter-tase）中识别并跳过了冗余计算。其次，我们使用一种修改的急切预测方法，该方法采用两步前一步检测来准确预测注意力评分，跳过迭代中不必要的计算（内题性稀疏性）。我们还引入了一种名为Conmerge的新型数据压实机制，该机制可以通过将稀疏矩阵凝结为紧凑形式来增强HW利用率。最后，它具有专用的HW体系结构，该体系结构支持上述稀疏性诱导算法，从而将高输出稀疏转化为提高的能源效率和性能。为了验证范围的可行性，我们首先证明它对各种多模式扩散模型的准确性没有影响。然后，我们在服务器和边缘级别设置中实例化阶段，并将其与GPU的性能与类似规格进行比较。我们的评估表明，与服务器GPU相比，迁移的性能和能源效率显着提高了3.2-379.3X和45.1-3067.6倍，与Edge GPU相比，与Server GPU相比，与服务器GPU相比和42.6-1090.9x和196.9-4668.2x倍。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 59 篇论文

### Advancing Singlish Understanding: Bridging the Gap with Datasets and Multimodal Models 
[[arxiv](https://arxiv.org/abs/2501.01034)] [[cool](https://papers.cool/arxiv/2501.01034)] [[pdf](https://arxiv.org/pdf/2501.01034)]
> **Authors**: Bin Wang,Xunlong Zou,Shuo Sun,Wenyu Zhang,Yingxu He,Zhuohan Liu,Chengwei Wei,Nancy F. Chen,AiTi Aw
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: Open-Source: https://github.com/AudioLLMs/Singlish
- **标题**: 推进Singlish的理解：用数据集和多模式弥合差距
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: Singlish是一种基于英语的克里奥尔语，是多语言和多元文化背景中语言研究的重点。但是，其口头形式仍然没有被逐渐置换，限制了对其语言结构和应用的见解。为了解决这一差距，我们标准化并注释了最大的口语Singlish语料库，并介绍了多任务国家演讲语料库（MNSC）。这些数据集支持各种任务，包括自动语音识别（ASR），口头问题答案（SQA），口语对话摘要（SDS）和副语言问题答案（PQA）。我们释放标准化的分裂和人为验证的测试集，以促进进一步的研究。此外，我们提出了Singaudiollm，这是一种多任务多模型，利用多模式大型语言模型同时处理这些任务。实验揭示了我们对独特背景的模型的适应性，与其他Audiollms和级联解决方案相比，实现了最先进的性能，并优于10-30％的先前模型。

### Negative to Positive Co-learning with Aggressive Modality Dropout 
[[arxiv](https://arxiv.org/abs/2501.00865)] [[cool](https://papers.cool/arxiv/2501.00865)] [[pdf](https://arxiv.org/pdf/2501.00865)]
> **Authors**: Nicholas Magal,Minh Tran,Riku Arakawa,Suzanne Nie
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 与积极的模态辍学
- **领域**: 计算语言学,机器学习
- **摘要**: 本文旨在通过使用积极的模态辍学来记录一种改善多模式共学习的有效方法。我们发现，通过使用侵略性模态辍学，我们能够将负阴性共学习（NCL）逆转到正共学习（PCL）。积极的模态辍学可用于“准备”一个多式模型，用于单峰部署，并在负共学习过程中大大提高了模型性能，在某些实验中，我们看到准确性增长了20％。我们还针对PCL进行了基准测试我们的模态辍学技术，以表明我们的模态掉落技术可以改善PCL期间的共学习，尽管它没有像NCL期间那样具有实质性效果。 github：https：//github.com/nmagal/modality_drop_for_colearning

### Decoding the Flow: CauseMotion for Emotional Causality Analysis in Long-form Conversations 
[[arxiv](https://arxiv.org/abs/2501.00778)] [[cool](https://papers.cool/arxiv/2501.00778)] [[pdf](https://arxiv.org/pdf/2501.00778)]
> **Authors**: Yuxuan Zhang,Yulong Li,Zichen Yu,Feilong Tang,Zhixiang Lu,Chong Li,Kang Dang,Jionglong Su
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: 7pages
- **标题**: 解码流程：长期对话中情绪因果关系分析的因果关系
- **领域**: 计算语言学,计算机与社会
- **摘要**: 长期的因果推理试图在扩展时间序列数据中发现因果关系，但受到复杂的依赖性和验证因果关系的挑战的阻碍。为了解决大规模语言模型（例如GPT-4）在捕获扩展对话中复杂的情感因果关系中的局限性，我们提出了Causemotion，这是一个基于检索型发电（RAG）和多载融合的长期情感因果推理框架。与仅依靠文本信息的传统方法不同，Causemotion通过结合音频衍生的功能 - 声音情感，情感强度和语音速率文本模式来丰富语义表示。通过将抹布与滑动窗口机制相结合，它可以有效检索和利用上下文相关的对话段，从而可以推断复杂的情感因果关系链，跨越了多个对话转弯。为了评估其有效性，我们构建了第一个专门用于长期情感因果推理的基准数据集，其中包含超过70圈的对话。实验结果表明，提出的基于抹布的多模式综合方法，大大提高了情感理解的深度和大规模语言模型的因果推理能力。与因果关系集成的GLM-4相比，因果准确性提高了8.7％，并且超过GPT-4O的1.2％。此外，在公开可用的DIAASQ数据集中，Causemotion-GLM-4实现最先进的方法可以提高准确性，F1分数和因果推理的准确性。

### Exploring the Implicit Semantic Ability of Multimodal Large Language Models: A Pilot Study on Entity Set Expansion 
[[arxiv](https://arxiv.org/abs/2501.00330)] [[cool](https://papers.cool/arxiv/2501.00330)] [[pdf](https://arxiv.org/pdf/2501.00330)]
> **Authors**: Hebin Wang,Yangning Li,Yinghui Li,Hai-Tao Zheng,Wenhao Jiang,Hong-Gee Kim
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: ICASSP 2025
- **标题**: 探索多模式大语言模型的隐性语义能力：一项针对实体集扩展的试点研究
- **领域**: 计算语言学,人工智能,信息检索
- **摘要**: 多模式大语言模型（MLLM）的快速发展为现实世界应用中的各种任务带来了重大改进。但是，LLM在提取隐式语义信息时仍具有一定的局限性。在本文中，我们将MLLM应用于多模式实体集扩展（MESE）任务，该任务旨在扩展少数具有属于同一语义类别的新实体的种子实体，并为每个实体提供多模式信息。我们通过MESE任务探索MLLM的功能，以了解实体级别粒度的隐性语义信息，并引入了列表排名方法LUSAR，该方法将本地分数映射到全球排名。我们的Lusar在MESE任务上的性能显示出显着改善，这标志着生成MLLM在ESE任务中首次使用并扩展了ListWise排名的适用性。

### A Breadth-First Catalog of Text Processing, Speech Processing and Multimodal Research in South Asian Languages 
[[arxiv](https://arxiv.org/abs/2501.00029)] [[cool](https://papers.cool/arxiv/2501.00029)] [[pdf](https://arxiv.org/pdf/2501.00029)]
> **Authors**: Pranav Gupta
> **First submission**: 2024-12-20
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 南亚语言中文本处理，语音处理和多模式研究的广度目录
- **领域**: 计算语言学,信息检索,机器学习
- **摘要**: 我们回顾了南亚语言的最新文献（2022-2024年10月），涉及基于文本的语言处理，多模式模型和语音处理，并提供了重点分析，重点分析，重点介绍了21种低资源的南亚语言，即Saraiki，Assamese，Assamese，Balochi，Balochi，Bhojpuri，bhojpuri，bodo，bodo，burmese，burmese burmese，burmesgarhi，dynia，dynata，ddjnata，dyandarhi，ddjanta gyrhhi，dyanda gyanda，dyanda ghatta，克什米尔人，康卡尼，卡西，马拉雅拉姆语，梅蒂（Meitei），尼泊尔（Nepali），odia，pashto，rajasthani，sindhi和telugu。我们使用逐步的方法来确定趋势，挑战和未来的研究方向，该方法结合了基于大语言模型（LLM）的相关性分类和聚类。我们的目标是向有兴趣使用南亚语言合作的NLP研究人员提供有关南亚语言技术最近发展的广度概述。

### Multi-LLM Collaborative Caption Generation in Scientific Documents 
[[arxiv](https://arxiv.org/abs/2501.02552)] [[cool](https://papers.cool/arxiv/2501.02552)] [[pdf](https://arxiv.org/pdf/2501.02552)]
> **Authors**: Jaeyoung Kim,Jongho Lee,Hong-Jun Choi,Ting-Yao Hsu,Chieh-Yang Huang,Sungchul Kim,Ryan Rossi,Tong Yu,Clyde Lee Giles,Ting-Hao 'Kenneth' Huang,Sungchul Choi
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: Accepted to AAAI 2025 AI4Research Workshop
- **标题**: 科学文档中的多LLL协作标题生成
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 科学人物字幕是一项复杂的任务，需要生成上下文适当的视觉内容描述。但是，现有方法通常通过利用不完整的信息而陷入困境，仅将任务视为图像到文本或文本摘要问题。这种限制阻碍了充分捕获必要细节的高质量标题的产生。此外，来自Arxiv论文的现有数据包含低质量的标题，对培训大语言模型（LLMS）提出了重大挑战。在本文中，我们介绍了一个称为Multi-LLM协作人物字幕生成（MLBCAP）的框架，以通过利用专门的LLM为不同的子任务来解决这些挑战。我们的方法在三个关键模块中展开：（质量评估）我们利用多模式LLM来评估培训数据的质量，从而使低质量标题的过滤。 （不同的字幕生成）然后，我们采用了一项策略，即在字幕任务上进行微调/提示多个LLM，以生成候选标题。 （判断）最后，我们促使一家著名的LLM从候选人中选择最高质量的标题，然后完善剩余的不准确性。人类评估表明，我们的方法的列表比人写的标题更好，强调了其有效性。我们的代码可在https://github.com/teamreboott/mlbcap上找到

### Towards Multimodal Metaphor Understanding: A Chinese Dataset and Model for Metaphor Mapping Identification 
[[arxiv](https://arxiv.org/abs/2501.02434)] [[cool](https://papers.cool/arxiv/2501.02434)] [[pdf](https://arxiv.org/pdf/2501.02434)]
> **Authors**: Dongyu Zhang,Shengcheng Yin,Jingwei Yu,Zhiyao Wu,Zhen Li,Chengpei Xu,Xiaoxia Wang,Feng Xia
> **First submission**: 2025-01-04
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 迈向多模式的隐喻理解：一种中文数据集和用于隐喻映射标识的模型
- **领域**: 计算语言学
- **摘要**: 隐喻在人类交流中起着至关重要的作用，但由于涉及认知复杂性，他们的理解仍然是自然语言处理（NLP）的重大挑战。根据概念隐喻理论（CMT），隐喻将目标域映射到源域上，理解该映射对于掌握隐喻的性质至关重要。尽管现有的NLP研究集中在隐喻检测和隐喻表达的情感分析等任务上，但人们对识别源和目标域之间映射的复杂过程的关注有限。此外，在文献中，非英语多模式比喻资源在很大程度上被忽略了，阻碍了人们对隐喻解释所涉及的关键要素的更深入的了解。为了解决这一差距，我们开发了一个中国多模式的隐喻广告数据集（即CM3D），其中包括对特定目标和源域的注释。该数据集旨在促进对隐喻理解的进一步研究，尤其是在非英语语言中。此外，我们提出了一个基于促进的（COT）基于促进的隐喻映射识别模型（CPMMIM），该模拟人类的认知过程以识别这些映射。从COT推理和双层优化（BLO）中汲取灵感，我们将任务视为层次识别问题，从而实现了更准确，更可解释的隐喻映射。我们的实验结果证明了CPMMIM的有效性，突出了其在NLP中推进隐喻理解的潜力。我们的数据集和代码都可以公开使用，以鼓励该领域的进一步进步。

### Multimodal Contrastive Representation Learning in Augmented Biomedical Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2501.01644)] [[cool](https://papers.cool/arxiv/2501.01644)] [[pdf](https://arxiv.org/pdf/2501.01644)]
> **Authors**: Tien Dang,Viet Thanh Duy Nguyen,Minh Tuan Le,Truong-Son Hy
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 增强生物医学知识图中的多模式对比度表示学习
- **领域**: 计算语言学,机器学习
- **摘要**: 生物医学知识图（BKG）整合了多种数据集，以阐明生物医学领域内的复杂关系。这些图表上的有效链接预测可以发现有价值的连接，例如潜在的新型药物疾病关系。我们介绍了一种新型的多模式方法，该方法将专业语言模型（LMS）与图形对比度学习（GCL）统一嵌入，以增强实体关系，同时采用知识图嵌入（KGE）模型来捕获实体关系以进行有效的链接预测。为了解决现有BKG中的限制，我们提出了PrimeKG ++，这是一个包含多模式数据的丰富知识图，包括每个实体类型的生物学序列和文本描述。通过将语义和关系信息结合在统一表示中，我们的方法表明了强烈的概括性，即使是看不见的节点，也可以实现准确的链接预测。关于PrimeKG ++和药品银行靶标相互作用数据集的实验结果证明了我们方法在各种生物医学数据集中的有效性和鲁棒性。我们的源代码，预培训模型和数据可在https://github.com/hysonlab/biomedkg上公开获取。

### Multimodal Multihop Source Retrieval for Web Question Answering 
[[arxiv](https://arxiv.org/abs/2501.04173)] [[cool](https://papers.cool/arxiv/2501.04173)] [[pdf](https://arxiv.org/pdf/2501.04173)]
> **Authors**: Navya Yarrabelly,Saloni Mittal
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: arXiv admin note: text overlap with arXiv:2010.03604 by other authors
- **标题**: Web问题回答的多模式多台面源检索
- **领域**: 计算语言学,人工智能
- **摘要**: 这项工作涉及对多模式多搭扣问答（QA）的学习和推理的挑战。我们根据句子的语义结构提出了一个图形推理网络，以学习多源推理路径，并找到图像和文本模式的支持事实以回答问题。在本文中，我们研究了图结构对多模式多跳多跳的问题的重要性。我们的分析以WebQA为中心。我们构建了一个强大的基线模型，该模型可以使用成对分类任务找到相关来源。我们确定，通过适当使用预训练模型的功能表示形式，图结构有助于改善多模式多模式的多跳问答。我们指出，图形结构和邻接矩阵都是任务相关的先验知识，并且可以利用图形结构来提高任务的检索性能。实验和可视化分析表明，图形网络上的消息传播或整个图形结构可以用令牌的跨注意替代大量的多模式变压器。我们证明了我们方法的适用性，并显示了\ textbf {4.6 $ \％$}的性能增益，尽管它是一个非常轻的模型，但在变压器基线上检索了F1SCORE。我们进一步证明了我们的模型对大规模检索设置的适用性。

### "Yeah Right!" -- Do LLMs Exhibit Multimodal Feature Transfer? 
[[arxiv](https://arxiv.org/abs/2501.04138)] [[cool](https://papers.cool/arxiv/2501.04138)] [[pdf](https://arxiv.org/pdf/2501.04138)]
> **Authors**: Benjamin Reichman,Kartik Talamadupula
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: “是的！” -  LLM是否表现出多模式特征转移？
- **领域**: 计算语言学
- **摘要**: 人类交流是一种多方面和多模式的技能。沟通需要了解表面级文本内容和交流的内涵意图。在人类中，学习超出表面水平的范围是从学习语音中学习交流意图开始。一旦人类在口头交流中获得这些技能，他们便将这些技能转移到书面交流中。在本文中，我们评估了语音+文本模型和文本模型的能力，该模型和文本模型特别强调了人类到人类的对话，以使这种多模式转移。我们专门测试了这些模型，以检测秘密欺骗性交流的能力。我们发现，没有特殊的提示语音+文本LLM在执行此任务时具有优势。同样，我们发现以这种技能为优势，以人类对人类对话培训的LLM也有优势。

### URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics 
[[arxiv](https://arxiv.org/abs/2501.04686)] [[cool](https://papers.cool/arxiv/2501.04686)] [[pdf](https://arxiv.org/pdf/2501.04686)]
> **Authors**: Ruilin Luo,Zhuofan Zheng,Yifan Wang,Yiyao Yu,Xinzhe Ni,Zicheng Lin,Jin Zeng,Yujiu Yang
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: Fix typos and add results. 27 pages, 11 tables, 17 figures. Models, training data and code have been open-sourced. Project url: https://ursa-math.github.io
- **标题**: URSA：在多模式数学中理解和验证经过思考的推理
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 经过思考链（COT）推理被广泛用于增强大语言模型（LLMS）的数学推理能力。引入COT轨迹的过程监督引发了有关改进测试时间缩放的讨论，从而解开了这些模型的系统2风格的思维功能。但是，在多模式数学推理中，高质量的婴儿床训练数据的稀缺性阻碍了现有模型实现故意的推理和细粒度的验证。在这项工作中，我们提出了一个新颖的框架，该框架将系统2风格的思维引入多模式数学推理。我们引入了一个三模块COT数据合成过程，该过程集成了COT蒸馏，轨迹格式重写和格式统一。此过程生成MMATHCOT-1M，这是一种高质量的COT推理指令微调数据集。此外，我们实施了双视轨迹标记自动化，该自动化既针对视觉接地忠诚度和演绎链有效性，从而导致双重脉冲1.1M数据集。接受MMATHCOT-1M培训的URSA-8B型号在六个流行的推理基准上，在类似尺寸的多模式LLMS中实现了新的最先进（SOTA）性能。在双重训练中，对DualMath-1.1M数据集进行了进一步培训URSA-RM-8B，这是一种增强URSA-8B测试时间性能并超过强闭合封闭源多模式MLLM（如GPT-4O）的验证者。模型权重，培训数据和代码已开源：https：//github.com/ursa-math/ursa-math。

### OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis 
[[arxiv](https://arxiv.org/abs/2501.04561)] [[cool](https://papers.cool/arxiv/2501.04561)] [[pdf](https://arxiv.org/pdf/2501.04561)]
> **Authors**: Run Luo,Ting-En Lin,Haonan Zhang,Yuchuan Wu,Xiong Liu,Min Yang,Yongbin Li,Longze Chen,Jiaming Li,Lei Zhang,Yangyi Chen,Hamid Alinejad-Rokny,Fei Huang
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: OpenOMNI：通过渐进的多模式对齐和实时自我意识的情感语音综合，推进开源综合大型语言模型
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 综合学习的最新进展已大大改善了图像，文本和语音的理解和产生，但是这些发展仍主要局限于专有模型。缺乏高质量的全象数据集和实时情感语音综合的挑战极大地阻碍了开源研究中的进步。为了解决这些限制，我们介绍了\名称，这是一个两阶段的培训框架，该框架集成了综合一致性和语音生成，以开发最先进的综合大型语言模型。在对齐阶段，预先训练的语音模型对文本图像任务进行了进一步的培训，从而实现了（接近）从视觉到语音的零射门概括，优于在三模式数据集中训练的模型。在语音生成阶段，对具有直接偏好优化的语音任务进行了轻巧的解码器，从而使实时情感语音综合具有高保真性。实验表明，\名称超过了综合，视觉语言和语音语言基准的最新模型。尽管使用了较小的训练样本和较小的模型尺寸（7b vs. 7x8b），但它在综合型模型VITA上的综合型中实现了4分的绝对改进。此外，\名称在非自动回归模式下以<1s延迟的延迟来实现实时语音生成，与自回归方法相比，推理时间减少了5倍，并将情感分类精度提高了7.7 \％

### Multimodal Graph Constrastive Learning and Prompt for ChartQA 
[[arxiv](https://arxiv.org/abs/2501.04303)] [[cool](https://papers.cool/arxiv/2501.04303)] [[pdf](https://arxiv.org/pdf/2501.04303)]
> **Authors**: Yue Dai,Soyeon Caren Han,Wei Liu
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 多模式图对比度学习并提示ChartQA
- **领域**: 计算语言学
- **摘要**: 由于图表元素的复杂分布以及嵌入在基础数据中的隐式模式，ChartQA提出了重大挑战。在本章中，我们为图表开发了一个联合多模式场景图，明确表示图表元素及其相关模式之间的关系。我们提出的多模式场景图由两个组件组成：一个视觉图和一个文本图，每个图形旨在捕获图表中的结构和语义信息。为了统一这些不同模式的表示，我们引入了一种多模式的对比度学习方法，该方法通过在多模式图中代表相同对象的节点之间的相似性来学习统一表示形式。可以将学习的图表表示无缝地并入变压器解码器中，作为软提示。此外，鉴于在零拍摄方案中对多模式大语言模型（MLLM）的需求日益增长，因此我们设计了对MLLMS的Thebough（COT）提示以减少幻觉。我们在ChartQA，OpenCQA和ChartX等公共基准上测试了这两种方法，证明了性能的提高并验证了我们提出的方法的有效性。

### Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.05662)] [[cool](https://papers.cool/arxiv/2501.05662)] [[pdf](https://arxiv.org/pdf/2501.05662)]
> **Authors**: Zheqi Lv,Wenkai Wang,Jiawei Wang,Shengyu Zhang,Fei Wu
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 高效多模式模型的级联自我评估增强培训
- **领域**: 计算语言学,人工智能
- **摘要**: 有效的多模式大语言模型（EMLLM）最近迅速发展。结合经过思考链（COT）推理和逐步的自我评估已提高了其性能。但是，有限的参数通常会阻碍EMLLM在推理过程中有效使用自我评估。关键挑战包括综合评估数据，确定其数量，优化培训和推理策略以及选择适当的提示。为了解决这些问题，我们引入了自我评估增强培训（SEAT）。 SEAT使用更强大的EMLLM进行COT推理，数据选择和评估生成，然后使用合成数据训练EMLLM。但是，处理长时间的提示和维持COT推理质量是有问题的。因此，我们提出了级联的自我评估增强培训（CAS-SEAT），该培训将冗长提示分解为较短的，特定于任务的级联提示，并降低了资源有限设置的成本。在数据合成过程中，我们采用开源7B参数EMLLM，并用简短提示注释一个小数据集。实验表明，CAS座位显着提高了Emllms的自我评估能力，分别在MathVista，Math-V和We-Math数据集上提高了19.68％，55.57％和46.79％的绩效。此外，我们的CAS座数据集是增强EMLLM自我评估的未来研究的宝贵资源。

### Modality-Invariant Bidirectional Temporal Representation Distillation Network for Missing Multimodal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2501.05474)] [[cool](https://papers.cool/arxiv/2501.05474)] [[pdf](https://arxiv.org/pdf/2501.05474)]
> **Authors**: Xincheng Wang,Liejun Wang,Yinfeng Yu,Xinxin Jiao
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-10
> **comment**: Accepted for publication by 2025 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)
- **标题**: 模式不变的双向时间表示蒸馏网络缺失多模式分析
- **领域**: 计算语言学,人工智能,机器学习,声音,音频和语音处理
- **摘要**: 多模式情感分析（MSA）整合了各种方式（文本，音频和视频），以全面分析和理解个人的情绪状态。但是，不完整数据的现实世界普遍存在对MSA构成了重大挑战，这主要是由于丢失了模态的随机性。此外，多模式数据中的异质性问题尚待有效解决。为了应对这些挑战，我们介绍了模式不变的双向时间表示蒸馏网络（MITR-DNET），以缺少多模式分析。 MITR-DNET采用了一种蒸馏方法，其中完整的模式教师模型指导缺失的模式学生模型，从而确保在缺少模态的情况下稳健性。同时，我们开发了模式不变的双向时间表示学习模块（MIB-TRL）以减轻异质性。

### LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.05468)] [[cool](https://papers.cool/arxiv/2501.05468)] [[pdf](https://arxiv.org/pdf/2501.05468)]
> **Authors**: Pouria Rouzrokh,Moein Shariatnia
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-10
> **comment**: 31 pages, 5 figures, 5 tables
- **标题**: Lattereview：使用大语言模型的系统审核自动化的多代理框架
- **领域**: 计算语言学
- **摘要**: 系统的文献综述和荟萃分析对于综合研究见解至关重要，但是由于筛查，评估和数据提取的迭代过程，它们仍然是时间密集型和劳动力密集的。本文介绍并评估了Lattereview，这是一个基于Python的框架，该框架利用大型语言模型（LLMS）和多代理系统来自动化系统审核过程的关键要素。 Lattereview旨在简化工作流程，同时维护严格，将模块化代理用于标题和抽象筛选，相关性评分和结构化数据提取等任务。这些代理在精心策划的工作流程中运行，支持基于用户反馈的顺序和并行审查回合，动态决策以及迭代精致。 Lattereview的体系结构集成了LLM提供商，从而使基于云和本地托管的模型兼容。该框架支持诸如结合外部环境，多模式评论，基于Pydantic的结构化输入和输出的验证以及用于处理大型数据集的异步编程之类的功能。该框架可在GitHub存储库中获得，并提供详细的文档和可安装的软件包。

### GLaM-Sign: Greek Language Multimodal Lip Reading with Integrated Sign Language Accessibility 
[[arxiv](https://arxiv.org/abs/2501.05213)] [[cool](https://papers.cool/arxiv/2501.05213)] [[pdf](https://arxiv.org/pdf/2501.05213)]
> **Authors**: Dimitris Kouremenos,Klimis Ntalianis
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: 9 pages, 4 figures
- **标题**: Glam-sign：希腊语言多模式唇读，具有集成的手语可访问性
- **领域**: 计算语言学,人工智能
- **摘要**: 希腊语言多模式唇读，具有集成的手语可访问性（Glam-sign）[1]是可访问性和多模式AI的开创性资源，旨在支持聋人和听力障碍（DHH）个体。它是根据Feelit Project [2]开发的，它集成了高分辨率音频，视频，文本转录和希腊语手语翻译，例如实时手语翻译和增强的字幕同步等应用程序。尽管它的主要重点是促进希腊旅游业的包容性，但其适应性延伸到教育，医疗保健和公共服务。未来的进步将提高单词级的精度和对其他语言的可扩展性，并得到先进的AI方法和与各种利益相关者的合作的支持。该数据集强调了多模式资源在弥合沟通差距，促进创新并为道德AI和包容性技术设定基准的变革潜力。

### MinMo: A Multimodal Large Language Model for Seamless Voice Interaction 
[[arxiv](https://arxiv.org/abs/2501.06282)] [[cool](https://papers.cool/arxiv/2501.06282)] [[pdf](https://arxiv.org/pdf/2501.06282)]
> **Authors**: Qian Chen,Yafeng Chen,Yanni Chen,Mengzhe Chen,Yingda Chen,Chong Deng,Zhihao Du,Ruize Gao,Changfeng Gao,Zhifu Gao,Yabin Li,Xiang Lv,Jiaqing Liu,Haoneng Luo,Bin Ma,Chongjia Ni,Xian Shi,Jialong Tang,Hui Wang,Hao Wang,Wen Wang,Yuxuan Wang,Yunlan Xu,Fan Yu,Zhijie Yan, et al. (11 additional authors not shown)
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: Work in progress. Authors are listed in alphabetical order by family name
- **标题**: Minmo：一种用于无缝语音互动的多模式大语言模型
- **领域**: 计算语言学,人工智能,人机交互,声音,音频和语音处理
- **摘要**: 大型语言模型（LLM）和多模式语音文本模型的最新进展为无缝的语音互动奠定了基础，从而实现了实时，自然和类似人类的对话。语音交互的先前模型被归类为本地和对齐。本地模型将语音和文本处理在一个框架中整合，但要困扰诸如序列长度和预训练不足之类的问题。对齐模型保持文本LLM功能，但通常受小型数据集的限制和对语音任务的关注狭窄。在这项工作中，我们介绍了Minmo，Minmo是一种多模式的大语言模型，具有大约8B参数，用于无缝的语音交互。我们解决了先前对齐的多峰模型的主要局限性。我们通过多个语音到文本对齐，文本到语音对齐，语音到语音对齐和双工互动对齐方式进行多个训练Minmo，以140万小时的多种语音数据和广泛的语音任务进行。经过多阶段的培训，MINMO在维持文本LLM的功能的同时，在各种基准中实现了各种基准的最新性能，并促进了全双工对话，也就是说，用户与系统之间的同时进行了双向交流。此外，我们提出了一个新颖而简单的语音解码器，它在语音生成中的表现优于先前的模型。 MINMO的增强指令遵循的功能支持基于用户说明的控制语音生成，并具有各种细微差别，包括情绪，方言和口语率以及模仿特定的声音。对于Minmo而言，语音到文本延迟约为100ms，理论上的全双工延迟约为600ms，实践中的延迟约为800ms。 MINMO项目网页是https://funaudiollm.github.io/minmo，代码和模型将很快发布。

### Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.05767)] [[cool](https://papers.cool/arxiv/2501.05767)] [[pdf](https://arxiv.org/pdf/2501.05767)]
> **Authors**: You Li,Heyu Huang,Chi Chen,Kaiyu Huang,Chao Huang,Zonghao Guo,Zhiyuan Liu,Jinan Xu,Yuhua Li,Ruixuan Li,Maosong Sun
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: 21 pages, 8 figures
- **标题**: Migician：在多模式模型中揭示自由形式多图像接地的魔力
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的最新进步显着改善了对单个图像的细粒度感知和多个图像的一般理解。但是，现有的MLLM仍然面临在复杂的多图像方案中实现精确基础的挑战。为了解决这个问题，我们首先探索了一个经过思考链（COT）框架，该框架将单像接地与多图像理解集成在一起。尽管部分有效，但由于其非端到端的性质，它仍然不稳定，并且难以捕获抽象的视觉信息。因此，我们介绍了Migician，这是第一个能够在多个图像上进行自由形式和准确接地的多图像接地模型。为了支持这一点，我们介绍了Mgrounding-630K数据集，该数据集包括用于从现有数据集派生的几个多图像接地任务以及新生成的新生成的自由形式接地指令遵循数据的数据。此外，我们提出了MIG BENCH，这是一种专门设计用于评估多图像接地能力的综合基准。实验结果表明，我们的模型达到了明显优越的多图像接地能力，表现优于最佳现有MLLM，甚至超过了更大的70B模型。我们的代码，模型，数据集和基准在https://migician-vg.github.io/上完全开源。

### Large Language Models for Knowledge Graph Embedding Techniques, Methods, and Challenges: A Survey 
[[arxiv](https://arxiv.org/abs/2501.07766)] [[cool](https://papers.cool/arxiv/2501.07766)] [[pdf](https://arxiv.org/pdf/2501.07766)]
> **Authors**: Bingchen Liu,Xin Li
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: 知识图嵌入技术，方法和挑战的大型语言模型：调查
- **领域**: 计算语言学,人工智能
- **摘要**: 大型语言模型（LLM）由于其出色的表现而引起了各个领域的大量关注，旨在培训数亿或更多参数，以了解和生成自然语言。随着LLM的出色性能变得显而易见，它们越来越多地应用于知识图嵌入（KGE）相关任务以改善处理结果。作为自然语言处理（NLP）领域的深度学习模型，它学习了大量文本数据，以预测下一个单词或生成与给定文本有关的内容。但是，最近在不同类型的KGE相关场景（例如多模式KGE）和根据其任务特征开放KGE的情况下，LLMS被不同程度调用。在本文中，我们研究了在不同类型的KGE方案中执行与LLMS相关的任务的广泛方法。为了更好地比较各种方法，我们在分类中总结了每个KGE方案。除了分类方法外，我们还提供了该方法及其源代码链接的表格概述，以进行更直接的比较。在文章中，我们还讨论了主要使用这些方法的应用，并为开发该新研究领域的开发提出了几个前瞻性方向。

### Imagine while Reasoning in Space: Multimodal Visualization-of-Thought 
[[arxiv](https://arxiv.org/abs/2501.07542)] [[cool](https://papers.cool/arxiv/2501.07542)] [[pdf](https://arxiv.org/pdf/2501.07542)]
> **Authors**: Chengzu Li,Wenshan Wu,Huanyu Zhang,Yan Xia,Shaoguang Mao,Li Dong,Ivan Vulić,Furu Wei
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: 11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables including references and appendices)
- **标题**: 想象一下在太空中推理的同时：多模式可视化的思想
- **领域**: 计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 事实证明，经过思考链（COT）提示非常有效地增强了大语言模型（LLM）和多模式大语言模型（MLLM）的复杂推理。但是，它在复杂的空间推理任务中挣扎。尽管如此，人类的认知范围超出了语言，从而使能够以文字和图像进行思考的非凡能力。受此机制的启发，我们提出了一种新的推理范式，多模式可视化（MVOT）。它通过生成其推理轨迹的图像可视化来实现MLLM的视觉思考。为了确保高质量的可视化，我们将令牌差异引入自回归的MLLM。这种创新显着提高了视觉连贯性和忠诚度。我们通过几个动态的空间推理任务来验证这种方法。实验结果表明，MVOT证明了跨任务的竞争性能。此外，在COT失败的最具挑战性的情况下，它在最具挑战性的场景中表现出了强大而可靠的改进。最终，MVOT为视觉思维可以有效地补充口头推理的复杂推理任务建立了新的可能性。

### Boosting Text-To-Image Generation via Multilingual Prompting in Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2501.07086)] [[cool](https://papers.cool/arxiv/2501.07086)] [[pdf](https://arxiv.org/pdf/2501.07086)]
> **Authors**: Yongyu Mu,Hengyu Li,Junxin Wang,Xiaoxuan Zhou,Chenglong Wang,Yingfeng Luo,Qiaozhi He,Tong Xiao,Guocheng Chen,Jingbo Zhu
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Accepted to ICASSP 2025
- **标题**: 通过多语言提示在大型多模型中增强文本对图像生成
- **领域**: 计算语言学
- **摘要**: 以前的有关用于文本对图像（T2I）生成的大型多模型（LMM）的工作重点是丰富内部文化学习（ICL）的输入空间。这包括提供一些演示和优化图像描述以更详细和逻辑。但是，随着对更复杂和灵活的图像描述的需求不断增长，增强对ICL范式内输入文本的理解仍然是关键但毫无疑问的区域。在这项工作中，我们通过构建旨在利用LMM的多语言功能的平行多语言提示来扩展这一研究。更具体地说，我们将输入文本转换为几种语言，并为模型提供原始文本和翻译。在3个基准的两个LMM上进行的实验表明，我们的方法PMT2I在一般，组成和细粒度的评估中，尤其是在人类的偏好比对方面都能达到卓越的性能。此外，凭借生成更多样化的图像的优势，PMT2I在与重新依给方法合并时的表现明显优于基线提示。我们的代码和并行多语言数据可在https://github.com/takagi97/pmt2i上找到。

### A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following 
[[arxiv](https://arxiv.org/abs/2501.08187)] [[cool](https://papers.cool/arxiv/2501.08187)] [[pdf](https://arxiv.org/pdf/2501.08187)]
> **Authors**: Yin Fang,Xinle Deng,Kangwei Liu,Ningyu Zhang,Jingyang Qian,Penghui Yang,Xiaohui Fan,Huajun Chen
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: 37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell, Models: https://huggingface.co/zjunlp/Instructcell-chat, https://huggingface.co/zjunlp/InstructCell-instruct
- **标题**: 多式联运AI副铜，用于单细胞分析，并具有以下说明
- **领域**: 计算语言学,人工智能,计算工程、金融和科学,人机交互,机器学习,细胞行为
- **摘要**: 大型语言模型在解释复杂的自然语言指令方面表现出色，使他们能够执行各种任务。在生命科学中，单细胞RNA测序（SCRNA-SEQ）数据是“细胞生物学的语言”，可在单细胞水平上捕获复杂的基因表达模式。但是，通过常规工具与这种“语言”进行互动通常是效率低下且不直觉的，对研究人员构成了挑战。为了解决这些局限性，我们提出了指Guenthcell，这是一种多模式的AI副铜，它利用自然语言作为媒介进行更直接和灵活的单细胞分析。我们构建了一个综合的多模式指令数据集，该数据集将基于文本的指令与来自不同组织和物种的SCRNA-SEQ轮廓配对。在此基础上，我们开发了一种多模式的单元语言体系结构，能够同时解释和处理这两种模式。指导电视使研究人员能够完成关键任务，例如细胞类型注释，有条件的伪细胞产生和药物敏感性预测 - 使用直接的自然语言命令。广泛的评估表明，指导圈始终达到或超过现有的单细胞基础模型的性能，同时适应各种实验条件。更重要的是，TenchCell提供了一种可访问，直观的工具，用于探索复杂的单细胞数据，降低技术障碍并实现更深入的生物学见解。

### Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention for Enabled Classification 
[[arxiv](https://arxiv.org/abs/2501.08085)] [[cool](https://papers.cool/arxiv/2501.08085)] [[pdf](https://arxiv.org/pdf/2501.08085)]
> **Authors**: Hui Lee,Singh Suniljit,Yong Siang Ong
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: 动态多模式情感分析：利用跨模式的关注以进行启用分类
- **领域**: 计算语言学,机器学习
- **摘要**: 本文探讨了多模式情感分析模型的开发，该模型集成了文本，音频和视觉数据以增强情感分类。目的是通过捕获这些方式之间的复杂相互作用来改善情绪检测，从而实现更准确和细微的情感解释。该研究在基于变压器的架构中评估了三种特征融合策略 - 晚期融合，早期融合和多头注意。使用CMU-Mosei数据集进行了实验，该数据集包括具有情感分数标记的同步文本，音频和视觉输入。结果表明，早期融合的表现明显优于晚期融合，达到71.87 \％的准确性，而多头注意方法可提供边际改进，达到72.39 \％。研究结果表明，在过程的早期整合方式会增强情感分类，而注意机制在当前框架中的影响可能有限。未来的工作将集中于精炼功能融合技术，结合时间数据以及探索动态功能加权以进一步提高模型性能。

### Dynamic Knowledge Integration for Enhanced Vision-Language Reasoning 
[[arxiv](https://arxiv.org/abs/2501.08597)] [[cool](https://papers.cool/arxiv/2501.08597)] [[pdf](https://arxiv.org/pdf/2501.08597)]
> **Authors**: Julian Perry,Surasakdi Siripong,Thanakorn Phonchai
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 动态知识整合，以增强视力语言推理
- **领域**: 计算语言学
- **摘要**: 大型视觉模型（LVLM）在多模式任务中表现出了令人印象深刻的功能，但是它们的性能通常受到缺乏外部知识整合的限制，从而限制了他们处理知识密集型任务的能力，例如视觉问题答案和推理。为了应对这一挑战，我们提出了一种新颖的方法，是针对大型视觉模型（AKGP-LVLM）的自适应知识引导的预处理，该方法将结构化和非结构化的知识动态纳入了训练和微调过程中。我们的方法采用知识编码器来表示外部知识，检索机制，以选择与任务相关的信息以及动态适配器，以有效地对齐多模式和知识表示。我们在四个基准数据集上评估了我们的方法，这表明对最新模型的性能改善了。此外，人类评估突出了我们模型输出的出色正确性和相关性。广泛的分析证实了AKGP-LVLM的鲁棒性，效率和可扩展性，使其成为现实世界知识密集型任务的引人注目的解决方案。

### AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis 
[[arxiv](https://arxiv.org/abs/2501.11170)] [[cool](https://papers.cool/arxiv/2501.11170)] [[pdf](https://arxiv.org/pdf/2501.11170)]
> **Authors**: Alireza Ghahramani Kure,Mahshid Dehghani,Mohammad Mahdi Abootorabi,Nona Ghazizadeh,Seyed Arshan Dalili,Ehsaneddin Asgari
> **First submission**: 2025-01-19
> **First announcement**: 2025-01-20
> **comment**: Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)
- **标题**: AIMA在Semeval-2024任务3：简单而强大的情感原因对成对分析
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: SEMEVAL-2024任务3介绍了两个子任务，重点是在会话环境中进行情绪原因对提取。子任务1围绕着提取文本情感原因对的提取，其中原因是在对话中定义和注释为文本跨度。相反，子任务2扩展了分析，以包含多模式线索，包括语言，音频和视觉，并确认可能在文本数据中不明确表示原因的实例。我们提出的情绪原因分析模型被精心构成三个核心段：（i）嵌入提取，（ii）发现对后，（iii）在找到对后使用QA导致提取。我们的模型利用最新的技术并在特定于任务的数据集上进行微调，可以有效地揭开错综复杂的对话动力学网络，并提取微妙的线索，象征着情感表达中的因果关系。我们的团队AIMA在Semeval-2024 Task 3比赛中表现出色。我们在子任务1中排名第十，在23个团队中排名第六。

### Leveraging Chain of Thought towards Empathetic Spoken Dialogue without Corresponding Question-Answering Data 
[[arxiv](https://arxiv.org/abs/2501.10937)] [[cool](https://papers.cool/arxiv/2501.10937)] [[pdf](https://arxiv.org/pdf/2501.10937)]
> **Authors**: Jingran Xie,Shun Lei,Yue Yu,Yang Xiang,Hui Wang,Xixin Wu,Zhiyong Wu
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: Accepted by ICASSP 2025
- **标题**: 在没有相应的提问数据的情况下，利用思想链条朝着善解人意的口语对话
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 善解人意的对话对于自然的人类计算机互动至关重要，使对话系统可以以更个性化和情感上的方式做出响应，从而提高用户满意度和参与度。大型语言模型（LLM）的出现通过利用其强大能力并在多模式领域显示出潜力，从而彻底改变了对话的产生。许多研究已将语音与基于文本的LLM相结合，以将语音问题作为输入和输出文本响应。但是，缺乏包括语音样式信息到监督微调（SFT）的口头提问数据集限制了这些系统的性能。结果，尽管这些系统在理解语音内容方面表现出色，但它们通常很难产生善解人意的反应。作为回应，我们提出了一种新的方法，该方法规避了对提问数据的需求，称为听力，感知和表达（LPE）。我们的方法采用了两个阶段的培训过程，最初指导LLM聆听内容并感知语音的情感方面。随后，我们利用经过思考链（COT）促使该模型根据倾听的口语内容和感知的情感提示来解锁该模型的潜力。我们采用实验来证明拟议方法的有效性。据我们所知，这是利用COT进行基于语音的对话的首次尝试。

### BAP v2: An Enhanced Task Framework for Instruction Following in Minecraft Dialogues 
[[arxiv](https://arxiv.org/abs/2501.10836)] [[cool](https://papers.cool/arxiv/2501.10836)] [[pdf](https://arxiv.org/pdf/2501.10836)]
> **Authors**: Prashant Jayannavar,Liliang Ren,Marisa Hudspeth,Charlotte Lambert,Ariel Cordes,Elizabeth Kaplan,Anjali Narayan-Chen,Julia Hockenmaier
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: BAP V2：Minecraft对话中的增强的任务框架以遵循指令
- **领域**: 计算语言学,人工智能
- **摘要**: 能够在物理世界中理解和执行指示的互动代理长期以来一直是AI研究的核心目标。 Minecraft合作建筑任务（MCBT）为实现这一目标提供了一种这样的设置（Narayan-Chen，Jayannavar和Hockenmaier 2019）。这是一款两人游戏，建筑师（a）指示建筑商（b）在模拟的世界环境中构建目标结构。我们专注于具有有限的培训数据（Jayannavar，Narayan-Chen和Hockenmaier 2020），在给定的多模式游戏环境中预测正确的动作序列的具有挑战性的构建器行动预测（BAP）子任务。我们仔细研究了BAP任务的评估和数据，发现了关键挑战，并在这两个方面做出了重大改进，以提出BAP V2，这是该任务的升级版本。这将使未来的工作能够在其上取得更有效和有意义的进步。它包括：（1）增强的评估基准，其中包括更清洁的测试集，更公平，更有见地的指标，以及（2）通过新型Minecraft对话和模拟MCBT产生的其他合成训练数据。我们表明，即使使用相对简单的训练方法，合成数据也可以用于训练更多的性能和健壮的神经模型。展望未来，此类数据对于培训更复杂的，渴望数据的深度变压器模型以及越来越大的LLM也可能至关重要。尽管建模不是这项工作的主要重点，但我们也说明了我们的数据和培训方法对基于LLM和变压器的简单模型的影响，从而验证了我们方法的鲁棒性，并为更高级的体系结构和LLM奠定了基础。

### MSTS: A Multimodal Safety Test Suite for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.10057)] [[cool](https://papers.cool/arxiv/2501.10057)] [[pdf](https://arxiv.org/pdf/2501.10057)]
> **Authors**: Paul Röttger,Giuseppe Attanasio,Felix Friedrich,Janis Goldzycher,Alicia Parrish,Rishabh Bhardwaj,Chiara Di Bonaventura,Roman Eng,Gaia El Khoury Geagea,Sujata Goswami,Jieun Han,Dirk Hovy,Seogyeong Jeong,Paloma Jeretič,Flor Miriam Plaza-del-Arco,Donya Rooein,Patrick Schramowski,Anastassia Shaitarova,Xudong Shen,Richard Willats,Andrea Zugarini,Bertie Vidgen
> **First submission**: 2025-01-17
> **First announcement**: 2025-01-20
> **comment**: under review
- **标题**: MST：视觉模型的多模式安全测试套件
- **领域**: 计算语言学
- **摘要**: 视觉语言模型（VLMS）是处理图像和文本输入的，越来越多地集成到聊天助手和其他消费者AI应用程序中。但是，如果没有适当的保障措施，VLM可能会提供有害建议（例如如何自我伤害）或鼓励不安全的行为（例如食用药物）。尽管存在这些明显的危害，但到目前为止，几乎没有工作评估了VLM安全性和多模式输入产生的新型风险。为了解决这一差距，我们介绍了MSTS，这是VLMS的多模式安全测试套件。 MSTS包括40个细粒危险类别的400个测试提示。每个测试提示都由文本和仅组合揭示其完整不安全含义的图像组成。有了MST，我们在几个开放式VLM中发现了明确的安全问题。我们还发现一些VLM偶然安全，这意味着它们是安全的，因为它们甚至无法理解简单的测试提示。我们将MST转化为十种语言，显示非英语提示提高了不安全模型响应的速度。我们还显示，仅用文本而不是多模式提示进行测试时，模型要更安全。最后，我们探讨了VLM安全评估的自动化，即使是缺少最佳的安​​全分类器。

### The Value of Nothing: Multimodal Extraction of Human Values Expressed by TikTok Influencers 
[[arxiv](https://arxiv.org/abs/2501.11770)] [[cool](https://papers.cool/arxiv/2501.11770)] [[pdf](https://arxiv.org/pdf/2501.11770)]
> **Authors**: Alina Starovolsky-Shitrit,Alon Neduva,Naama Appel Doron,Ella Daniel,Oren Tsur
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 一无所有的价值：由Tiktok影响者表达的人类价值的多模式提取
- **领域**: 计算语言学,计算机与社会,社交和信息网络
- **摘要**: 通过互动和暴露，将社会和个人价值观传播给年轻一代。传统上，儿童和青少年从父母，教育者或同龄人那里学会了价值观。如今，社交平台是一个重要的渠道，通过这些渠道，青年（和成年人）将信息作为娱乐的主要媒介，也可能是他们学习不同价值观的媒介。在本文中，我们从针对儿童和青少年的在线影响者上传的Tiktok电影中提取隐式价值。我们策划了数百部Tiktok电影的数据集，并根据Schwartz的个人价值观理论对它们进行了注释。然后，我们尝试了一系列蒙版和大型语言模型，探索如何检测值。具体而言，我们考虑了两个管道 - 从视频中直接提取值，以及一种2步方法，其中首先将视频转换为详细脚本，然后提取值。达到最先进的结果，我们发现2步方法的性能明显优于直接方法，并且使用可训练的蒙版语言模型作为第二步大大胜过了许多大型语言模型的几次应用。我们进一步讨论了微调的影响，并比较了不同模型对tiktok中存在或矛盾的值的性能。最后，我们共享了Tiktok视频的第一个价值观的数据集。我们的结果为在基于视频的社交平台中进一步研究影响和价值传播的方式铺平了道路。

### Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks 
[[arxiv](https://arxiv.org/abs/2501.11733)] [[cool](https://papers.cool/arxiv/2501.11733)] [[pdf](https://arxiv.org/pdf/2501.11733)]
> **Authors**: Zhenhailong Wang,Haiyang Xu,Junyang Wang,Xi Zhang,Ming Yan,Ji Zhang,Fei Huang,Heng Ji
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 移动代理-E：复杂任务的自我发展的移动助手
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 智能手机在现代生活中变得必不可少，但是在移动设备上进行复杂的任务通常仍然令人沮丧。大型多模型模型（LMM）的移动剂的最新进展证明了在移动环境中感知和行动的能力。但是，当前的方法面临着重大局限性：它们在满足现实世界中的人类需求，在推理密集型和长途任务方面挣扎以及缺乏从以前的经验中学习和改进的机制。为了克服这些挑战，我们介绍了移动设施-E，这是一个能够通过过去的经验进行自我进化的层次多代理框架。根据层次结构，我们是指高级计划和低级行动执行的明确分离。该框架包括一个经理，负责通过将复杂的任务分解为子目标，而四个下属代理（感知者，操作员，动作反射器和NoteTaker）分别处理精细的视觉感知，即时操作执行，错误验证，错误验证和信息汇总。移动代理-E还具有一个新型的自我进化模块，该模块保持持续的长期记忆，包括提示和快捷方式。技巧是一般指导，也是从先前任务中学到的有关如何有效与环境互动的经验教训。快捷方式是可重复使用的，可执行的原子操作序列，适用于特定子例程。包括技巧和快捷方式的包含有助于持续的性能和效率。除了这个框架之外，我们还介绍了Mobile-eval-E，这是一种新的基准测试，具有复杂的移动任务，需要长期摩托车，多应用交互。经验结果表明，移动代理-E比以前三个基础模型骨架的先前最先进的方法实现了22％的绝对改善。项目页面：https：//x-plug.github.io/mobileagent。

### Question-to-Question Retrieval for Hallucination-Free Knowledge Access: An Approach for Wikipedia and Wikidata Question Answering 
[[arxiv](https://arxiv.org/abs/2501.11301)] [[cool](https://papers.cool/arxiv/2501.11301)] [[pdf](https://arxiv.org/pdf/2501.11301)]
> **Authors**: Santhosh Thottingal
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 无幻觉知识访问的问题与问题检索：Wikipedia和Wikidata问题的方法回答
- **领域**: 计算语言学,人工智能
- **摘要**: 本文通过从密集的矢量嵌入商店中进行“问题与问题”的匹配和检索，介绍了一种对Wikipedia和Wikidata等知识库等知识库回答的方法。我们没有嵌入文档内容，而是使用指令调整的LLM为每个逻辑内容单元生成一组全面的问题。这些问题被矢量装饰和存储，映射到相应的内容。然后将用户查询的向量嵌入与此问题向量存储相匹配。最高的相似性得分导致直接检索相关的文章内容，从而消除了答案生成的需求。对于相关问题对，我们的方法实现了高余弦的相似性（> 0.9），从而实现了高度精确的检索。这种方法提供了几种优势，包括计算效率，快速响应时间和提高可扩展性。我们证明了它对Wikipedia和Wikidata的有效性，包括通过Wikidata的结构化事实检索，包括多媒体的多媒体内容，开辟了新的途径，以解决多模式问题。

### RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering 
[[arxiv](https://arxiv.org/abs/2501.13297)] [[cool](https://papers.cool/arxiv/2501.13297)] [[pdf](https://arxiv.org/pdf/2501.13297)]
> **Authors**: Yang Bai,Christan Earl Grant,Daisy Zhe Wang
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: Accepted by NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text and images, has gained significant attention in information retrieval (IR) and natural language processing (NLP). Traditional ranking methods rely on small encoder-based language models, which are incompatible with modern decoder-based generative large language models (LLMs) that have advanced various NLP tasks. To bridge this gap, we propose RAMQA, a unified framework combining learning-to-rank methods with generative permutation-enhanced ranking techniques. We first train a pointwise multi-modal ranker using LLaVA as the backbone. Then, we apply instruction tuning to train a LLaMA model for re-ranking the top-k documents using an innovative autoregressive multi-task learning approach. Our generative ranking model generates re-ranked document IDs and specific answers from document candidates in various permutations. Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant improvements over strong baselines, highlighting the effectiveness of our approach. Code and data are available at: https://github.com/TonyBY/RAMQA

### Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning 
[[arxiv](https://arxiv.org/abs/2501.13042)] [[cool](https://papers.cool/arxiv/2501.13042)] [[pdf](https://arxiv.org/pdf/2501.13042)]
> **Authors**: Bohao Yang,Yingji Zhang,Dong Liu,André Freitas,Chenghua Lin
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: No comments
- **标题**: 表源很重要吗？基准测试和改进多模式科学表的理解和推理
- **领域**: 计算语言学
- **摘要**: 最近的大型语言模型（LLMS）具有高级的表格理解功能，但依赖于将表转换为文本序列。虽然多模式大型语言模型（MLLM）启用了直接的视觉处理，但由于固定的输入图像分辨率和数值不足的推理功能，它们在处理科学表中面临限制。我们提出了一个全面的框架，用于通过动态输入图像分辨率的多模式科学表理解和推理。我们的框架由三个关键组成部分组成：（1）MMSCI-PRE，这是一个针对52K科学表结构识别样本的特定域结构学习数据集，（2）MMSCI-INS，一个基于三个表任务的12K样品的教学调整数据集，以及（3）MMSCI-EVAL，MMSCI-EVAL，MMSCI-EVAL，MMSCI-EVAL，3,114测试的基本量。广泛的实验表明，与150k通用域表相比，我们针对52K科学表图像的域特异性方法可以达到较高的性能，从而强调了数据质量与数量相比的重要性。我们提出的具有动态输入分辨率的基于表格的MLLM在一般表理解和数值推理能力方面都显示出显着改善，并对持有数据集进行了强烈的概括。我们的代码和数据可在https://github.com/bernard-yang/mmsci_table上公开获取。

### Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.13976)] [[cool](https://papers.cool/arxiv/2501.13976)] [[pdf](https://arxiv.org/pdf/2501.13976)]
> **Authors**: Akash Bonagiri,Lucen Li,Rajvardhan Oak,Zeerak Babar,Magdalena Wojcieszak,Anshuman Chhabra
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-24
> **comment**: This paper is in submission and under peer review
- **标题**: 迈向更安全的社交媒体平台：使用大语言模型的可扩展和表现的有害内容适度
- **领域**: 计算语言学,人工智能,计算机与社会,社交和信息网络
- **摘要**: 社交媒体平台上有害内容的普遍性给用户和社会带来了重大风险，因此需要更有效，可扩展的内容审核策略。当前的方法依赖于人类主持人，监督分类器和大量培训数据，并且经常在可扩展性，主观性和有害内容的动态性质（例如，暴力内容，危险挑战趋势等）中挣扎。为了弥合这些差距，我们利用大型语言模型（LLMS）通过封闭式学习来进行几乎没有动态的内容。通过对多个LLM的广泛实验，我们证明了我们的几种方法可以超越现有的专有基准（透视和openai适度），以及先前最新的几次学习方法，以识别危害。我们还结合了视觉信息（视频缩略图），并评估不同的多模式技术是否改善了模型性能。我们的结果强调了采用基于LLM的方法在线可扩展和动态有害内容审核的重大好处。

### Redundancy Principles for MLLMs Benchmarks 
[[arxiv](https://arxiv.org/abs/2501.13953)] [[cool](https://papers.cool/arxiv/2501.13953)] [[pdf](https://arxiv.org/pdf/2501.13953)]
> **Authors**: Zicheng Zhang,Xiangyu Zhao,Xinyu Fang,Chunyi Li,Xiaohong Liu,Xiongkuo Min,Haodong Duan,Kai Chen,Guangtao Zhai
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: MLLMS基准的冗余原则
- **领域**: 计算语言学,人工智能
- **摘要**: 随着多模式大型语言模型（MLLM）的快速迭代以及该领域的不断发展的需求，每年产生的基准数量已飙升为数百人。快速增长不可避免地导致了基准之间的显着冗余。因此，至关重要的是退后一步并批判性地评估冗余的当前状态，并提出构建有效MLLM基准的目标原则。在本文中，我们从三个关键角度关注冗余：1）基准能力维度的冗余，2）测试问题数量的冗余，以及3）特定域内的跨基准冗余。通过在20多个基准中进行数百个MLLM的性能的全面分析，我们旨在定量衡量冗余水平在于现有的MLLM评估，提供有价值的见解，以指导MLLM基准的未来开发，并提供策略以有效地提高和解​​决冗余问题。

### Fanar: An Arabic-Centric Multimodal Generative AI Platform 
[[arxiv](https://arxiv.org/abs/2501.13944)] [[cool](https://papers.cool/arxiv/2501.13944)] [[pdf](https://arxiv.org/pdf/2501.13944)]
> **Authors**: Fanar Team,Ummar Abbas,Mohammad Shahmeer Ahmad,Firoj Alam,Enes Altinisik,Ehsannedin Asgari,Yazan Boshmaf,Sabri Boughorbel,Sanjay Chawla,Shammur Chowdhury,Fahim Dalvi,Kareem Darwish,Nadir Durrani,Mohamed Elfeky,Ahmed Elmagarmid,Mohamed Eltabakh,Masoomali Fatehkia,Anastasios Fragkopoulos,Maram Hasanain,Majd Hawasly,Mus'ab Husaini,Soon-Gyo Jung,Ji Kim Lucas,Walid Magdy,Safa Messaoud, et al. (17 additional authors not shown)
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-24
> **comment**: :I.2.0; D.2.0
- **标题**: FANAR：以阿拉伯语为中心的多式联运AI平台
- **领域**: 计算语言学,人工智能
- **摘要**: 我们介绍Fanar，这是一个支持语言，语音和图像生成任务的以阿拉伯语为中心的多模式生成AI系统的平台。 Fanar的核心是Fanar Star和Fanar Prime，这是两种功能强大的阿拉伯语大型语言模型（LLMS），它们在类似尺寸的型号的基准良好的基准上最好。 Fanar Star是一个7B（十亿）参数模型，在近1万亿清洁和重复删除的阿拉伯语，英语和代码令牌上从头开始训练。 Fanar Prime是一个9B参数模型，该模型在同一1万亿代币集的Gemma-2 9b基本模型上持续训练。两种模型均已同时部署和设计，以解决通过定制的编排透明路由的不同类型的提示。 FANAR平台提供了许多其他功能，包括用于处理宗教提示的定制伊斯兰检索增强发电系统（RAG）系统，这是用于汇总有关培训前数据截止日期后发生的有关当前或最近事件的信息的重新度破布。该平台提供了其他认知能力，包括内部双语语音识别，支持多种阿拉伯语方言，语音和图像生成，以更好地反映区域特征。最后，Fanar提供了一种归因服务，可用于验证基于事实的生成内容的真实性。 FANAR的设计，开发和实施完全是在哈马德·本·哈利法大学的卡塔尔计算研究所（QCRI）进行的，并由卡塔尔通信和信息技术部赞助，以实现主权AI技术开发。

### The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama with Vision-Aware and Function-Calling Capabilities 
[[arxiv](https://arxiv.org/abs/2501.13921)] [[cool](https://papers.cool/arxiv/2501.13921)] [[pdf](https://arxiv.org/pdf/2501.13921)]
> **Authors**: MediaTek Research,:,Chan-Jan Hsu,Chia-Sheng Liu,Meng-Hsi Chen,Muxi Chen,Po-Chun Hsu,Yi-Chang Chen,Da-Shan Shiu
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 微风2的模型：传统的中国LLM，基于Llama，具有视觉吸引和功能功能的功能
- **领域**: 计算语言学
- **摘要**: Llama-Breeze2（以下称为Breeze2）是一套由高级多模式模型组成的套件，可在3B和8B参数配置中使用，专门设计用于增强传统的中文表示。在Llama 3.2模型家族的基础上，我们继续对Breeze2进行预培训，以增强传统中国人的语言和文化遗产。除了语言建模功能外，我们还可以通过功能呼叫和远景理解功能大大增强模型。据我们所知，在本出版物时，没有引起推理的提示，Breeze2是传统的中国功能呼叫和图像理解中最强的表现模型。 Breeze2的有效性是在各种任务中基准的，包括台湾一般知识，跟随教学，长篇小说，功能呼叫和愿景理解。我们将在Llama 3.2社区许可下公开发布所有Breeze2模型。我们还使用移动应用程序展示了在移动平台上运行的模型的功能，我们也可以开源。

### Parameter-Efficient Fine-Tuning for Foundation Models 
[[arxiv](https://arxiv.org/abs/2501.13787)] [[cool](https://papers.cool/arxiv/2501.13787)] [[pdf](https://arxiv.org/pdf/2501.13787)]
> **Authors**: Dan Zhang,Tao Feng,Lilong Xue,Yuandong Wang,Yuxiao Dong,Jie Tang
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: 25 pages, 6 figures, 7 tables
- **标题**: 基础模型的参数有效微调
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 这项调查深入研究了基础模型（FMS）中参数效率微调（PEFT）领域。 PEFT是一种经济高效的微调技术，在努力达到最佳下游任务性能的同时，最大程度地降低了参数和计算复杂性。 FMS，例如Chatgpt，Dall-E和Llava，专门研究语言理解，生成任务和多模式任务，对涵盖文本，图像和视频的各种数据集进行了培训。 FMS的多样性指导PEFT的各种适应策略。因此，该调查旨在提供应用于不同FMS的PEFT技术的全面概述，并解决了解技术，趋势和应用方面的关键差距。我们首先提供FMS和PEFT的详细开发。随后，我们会系统地回顾各种FMS中PEFT的关键类别和核心机制，以提供对趋势的全面理解。我们还探索了各种FMS的最新应用，以证明PEFT的多功能性，从而阐明了系统的PEFT方法与一系列FMS的整合。此外，我们确定了未来改善PEFT的潜在研究和开发方向。这项调查为寻求理解和利用PEFT跨FMS的新移民和专家提供了宝贵的资源。所有审核的论文均以\ url {https://github.com/thudm/awesome-parameter-felmeter-fine-fine-for-for-foundation-models}列出。

### LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.13652)] [[cool](https://papers.cool/arxiv/2501.13652)] [[pdf](https://arxiv.org/pdf/2501.13652)]
> **Authors**: Yizheng Sun,Yanze Xin,Hao Li,Jingyuan Sun,Chenghua Lin,Riza Batista-Navarro
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: Accepted to NAACL 2025 Findings
- **标题**: LVPruning：一种有效但简单的语言引导的视觉图令牌修剪方法，用于多模式大型语言模型
- **领域**: 计算语言学
- **摘要**: 多模式的大型语言模型（MLLM）通过整合视觉和文本模式取得了巨大的成功。但是，由于处理了大量的视觉令牌，它们会产生大量的计算开销，从而限制了它们在资源受限环境中的实用性。我们介绍了MLLMS的语言引导的视觉令牌修剪（LVPRUNING），这是一种有效而简单的方法，可大大减轻计算负担，同时保留模型性能。 LVPruning采用跨意义模块来根据其与语言代币的互动来计算视觉令牌的重要性，从而确定修剪哪个。重要的是，可以在不修改原始MLLM参数的情况下集成LVPRUNING，这使LVPruning易于应用或删除。我们的实验表明，LVPruning可以通过LLAVA-1.5的中层有效地降低90％的视觉令牌，从而导致推理TERA浮点数每秒（TFlops）的推理TERA浮点数降低62.1％（TFLOPS），平均性能损失的平均性能损失仅为0.45％。

### ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models 
[[arxiv](https://arxiv.org/abs/2501.13397)] [[cool](https://papers.cool/arxiv/2501.13397)] [[pdf](https://arxiv.org/pdf/2501.13397)]
> **Authors**: Kangjie Zheng,Junwei Yang,Siyue Liang,Bin Feng,Zequn Liu,Wei Ju,Zhiping Xiao,Ming Zhang
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: 30 pages, 12 figures
- **标题**: EXLM：重新考虑[蒙版]令牌在蒙版语言模型中的影响
- **领域**: 计算语言学,机器学习
- **摘要**: 蒙面语言模型（MLMS）在许多自我监督的表示任务中取得了杰出的成功。通过将输入序列的部分随机掩盖以[mask]令牌的形式随机掩盖部分，并学会根据其余上下文重建原始内容来训练MLM。本文探讨了[面具]令牌对MLMS的影响。分析研究表明，掩盖令牌可以引入损坏的语义问题，其中损坏的上下文可能传达出多种模棱两可的含义。这个问题也是影响MLMS在下游任务上的性能的关键因素。基于这些发现，我们提出了一种新颖的增强的文本MLM，EXLM。我们的方法在输入上下文中扩展了[蒙版]令牌，并模拟了这些扩展状态之间的依赖性。这种增强增加了上下文能力，并使模型能够捕获更丰富的语义信息，从而有效地减轻了预训练期间损坏的语义问题。实验结果表明，EXLM在文本建模和微笑建模任务中都可以取得重大的性能提高。进一步的分析证实，EXLM通过上下文增强了语义表示，并有效地降低了在MLMS中通常观察到的语义多模式。

### Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts 
[[arxiv](https://arxiv.org/abs/2501.15688)] [[cool](https://papers.cool/arxiv/2501.15688)] [[pdf](https://arxiv.org/pdf/2501.15688)]
> **Authors**: Haodi Ma,Dzmitry Kasinets,Daisy Zhe Wang
> **First submission**: 2025-01-26
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 基于变压器的多模式知识图完成，具有链接感知上下文
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 多模式知识图完成（MMKGC）旨在通过利用来自各种模式的信息以及结构数据的信息来预测多模式知识图（MMKGS）中的缺失链接。现有的MMKGC方法主要扩展传统知识图嵌入（KGE）模型，这些模型通常需要为每个实体创建一个嵌入。这会导致大型模型大小和效率低下，以整合多模式信息，尤其是对于现实图形。同时，基于变压器的模型在知识图完成（KGC）中表现出了竞争性能。但是，他们对单模知识的关注限制了他们利用跨模式信息的能力。最近，大型视觉模型（VLM）在跨模式任务中显示出潜力，但受到高训练成本的限制。在这项工作中，我们提出了一种新颖的方法，该方法将基于变压器的KGE模型与预训练的VLM产生的跨模式上下文集成在一起，从而将其适用性扩展到MMKGC。具体而言，我们采用预先训练的VLM将相关的视觉信息从实体及其邻居转换为文本序列。然后，我们将kgc作为序列到序列任务，用生成的跨模式上下文对模型进行微调。与传统的KGE方法相比，这种简单但有效的方法显着降低了模型大小，同时在多个大型数据集中实现竞争性能，并以最小的高参数调整。

### Baichuan-Omni-1.5 Technical Report 
[[arxiv](https://arxiv.org/abs/2501.15368)] [[cool](https://papers.cool/arxiv/2501.15368)] [[pdf](https://arxiv.org/pdf/2501.15368)]
> **Authors**: Yadong Li,Jun Liu,Tao Zhang,Tao Zhang,Song Chen,Tianpeng Li,Zehuan Li,Lijun Liu,Lingfeng Ming,Guosheng Dong,Da Pan,Chong Li,Yuanbo Fang,Dongdong Kuang,Mingrui Wang,Chenglin Zhu,Youwei Zhang,Hongyu Guo,Fengyu Zhang,Yuran Wang,Bowen Ding,Wei Song,Xu Li,Yuqi Huo,Zheng Liang, et al. (68 additional authors not shown)
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: Baichuan-Omni-1.5技术报告
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 我们介绍了Baichuan-Omni-1.5，这是一种Omni-Modal模型，不仅具有Omni-Modal理解功能，而且还提供了端到端的音频生成功能。为了在不损害任何模式的能力的情况下实现流利和高质量的互动，我们优先优先优化三个关键方面。首先，我们为多模式数据建立了全面的数据清洁和合成管道，获得了约500B高质量数据（文本，音频和视觉）。其次，旨在从音频中捕获语义和声学信息的音频tokenizer（Baichuan-Audio-Tokenizer），从而实现了无缝集成并增强了与MLLM的兼容性。最后，我们设计了一种多阶段训练策略，该策略逐步整合了多模式对齐和多任务微调，从而确保了各种方式的有效协同作用。 Baichuan-Omni-1.5在全面的Omni-Modal功能方面，领导当代模型（包括GPT4O-Mini和MiniCPM-O 2.6）。值得注意的是，它的结果与在各种多模式医学基准中的QWEN2-VL-72B等领先模型相当。

### Figurative-cum-Commonsense Knowledge Infusion for Multimodal Mental Health Meme Classification 
[[arxiv](https://arxiv.org/abs/2501.15321)] [[cool](https://papers.cool/arxiv/2501.15321)] [[pdf](https://arxiv.org/pdf/2501.15321)]
> **Authors**: Abdullah Mazhar,Zuhair hasan shaik,Aseem Srivastava,Polly Ruhnke,Lavanya Vaddavalli,Sri Keshav Katragadda,Shweta Yadav,Md Shad Akhtar
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: Accepted for oral presentation at The Web Conference (WWW) 2025
- **标题**: 多模式心理健康模因分类的象征性兼常见知识输液
- **领域**: 计算语言学,社交和信息网络
- **摘要**: 在过去的几年中，通过非传统手段（例如模因）表达了精神健康症状的表达，通过模因中的比喻性复杂性，用户经常突出他们的心理健康斗争。尽管人类依靠常识知识来解释这些复杂的表达，但当前的多模式模型（MLMS）努力捕获模因中固有的这些象征性方面。为了解决这一差距，我们介绍了一个新颖的数据集Axiom，该数据集源自GAD焦虑问卷，该数据将模因分为六种细粒度焦虑症状。接下来，我们提出了一个常识性和富含域的框架M3H，以增强MLMS解释具有比喻性语言和常识性知识的能力。总体目标仍然是首先理解，然后对模因中表达的心理健康症状进行分类。我们对6种竞争基线（具有20种变化）进行基准M3H，表明定量和定性指标的改进，包括详细的人类评估。我们观察到加权F1公制的明显改善4.20％和4.66％。为了评估普遍性，我们在公共数据集上进行了广泛的实验，以恢复，以识别抑郁症状，并提供了一项广泛的消融研究，突出了每个模块在两个数据集中的贡献。我们的发现揭示了现有模型的局限性以及使用常识来增强象征性理解的优势。

### Cross-modal Context Fusion and Adaptive Graph Convolutional Network for Multimodal Conversational Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2501.15063)] [[cool](https://papers.cool/arxiv/2501.15063)] [[pdf](https://arxiv.org/pdf/2501.15063)]
> **Authors**: Junwei Feng,Xueyan Fan
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 跨模式上下文融合和自适应图卷积网络，用于多模式对话情感识别
- **领域**: 计算语言学
- **摘要**: 情绪识别在人类计算机，营销，医疗保健和其他领域中具有广泛的应用。近年来，深度学习技术的发展为情感识别提供了新的方法。在此之前，已经提出了许多情绪识别方法，包括多模式情感识别方法，但是这些方法忽略了不同输入方式之间的相互干扰，并且很少关注说话者之间的方向对话。因此，本文提出了一种新的多模式情感识别方法，包括跨模态上下文融合模块，自适应图卷积编码模块和情感分类模块。交叉模态上下文模块包括一个交叉模态对齐模块和上下文融合模块，该模块用于减少不同输入模态之间相互干扰引入的噪声。自适应图卷积模块构建了一个对话关系图，用于提取说话者之间的依赖性和自依赖性。我们的模型已经超过了公开可用基准数据集的一些最新方法，并获得了高识别精度。

### AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.15021)] [[cool](https://papers.cool/arxiv/2501.15021)] [[pdf](https://arxiv.org/pdf/2501.15021)]
> **Authors**: Zunhai Su,Wang Shen,Linge Li,Zhe Chen,Hanyu Wei,Huangqi Yu,Kehong Yuan
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: AKVQ-VL：视觉模型的注意力感知KV缓存自适应2位量化
- **领域**: 计算语言学
- **摘要**: 视觉模型（VLMS）在多模式任务中表现出色。但是，过度长的多模式输入会导致超大键值（KV）缓存，从而导致大量内存消耗和I/O瓶颈。大型语言模型（LLMS）的先前KV量化方法可能会减轻这些问题，但忽略了多模式令牌的注意力显着性差异，从而导致次优性能。在本文中，我们调查了VLM中的注意力值显着性模式，并提出了AKVQ-VL。 AKVQ-VL利用拟议的文本平台注意力（TSA）和枢轴 -  token-salient Poasine（PSA）模式来适应分配位预算。此外，实现极低的量化量化需要有效解决KV张量的异常值。 AKVQ-VL利用WALSH-HADAMARD变换（WHT）来构建无距离的KV缓存，从而降低了量化难度。对12个长篇文化和多模式任务的2位量化的评估表明，AKVQ-VL维持甚至提高了精度，表现优于LLM面向的方法。 AKVQ-VL可以将峰值存储器的使用量减少2.13倍，支撑高达3.25倍较大的批次尺寸和2.46倍的吞吐量。

### Dynamic Adaptation of LoRA Fine-Tuning for Efficient and Task-Specific Optimization of Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.14859)] [[cool](https://papers.cool/arxiv/2501.14859)] [[pdf](https://arxiv.org/pdf/2501.14859)]
> **Authors**: Xiaoxuan Liao,Chihang Wang,Shicheng Zhou,Jiacheng Hu,Hongye Zheng,Jia Gao
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 洛拉微调的动态适应大型语言模型的高效和特定任务优化
- **领域**: 计算语言学,机器学习
- **摘要**: 本文介绍了一种针对大型语言模型摩尔拉的新型方法。该方法从标准的低级适应框架中构建，进一步增加了动态适应机制，以提高效率和性能。动态LORA的关键贡献在于其自适应重量分配机制以及基于输入特征的自适应策略。这些增强功能允许更精确的微调过程，该过程更适合特定任务。传统的LORA方法使用静态适配器设置，而不是考虑模型层的不同重要性。相反，动态洛拉引入了一种机制，该机制可以动态评估该层在微调过程中的重要性。该评估使适配器参数的重新分配能够符合每个任务的唯一需求，从而导致更好的优化结果。灵活性的另一个增益来自对输入特征分布的考虑，这有助于模型在面对复杂和多样化的数据集时更好地推广。联合方法不仅可以提高每个任务上的性能，还可以提高模型的概括能力。在基准数据集（例如胶水）的实验中验证了动态洛拉的效率，结果令人惊讶。更具体地说，该方法以87.3％的F1得分达到88.1％的精度。明显的是，这些改进的计算成本略有上涨：比标准洛拉（Standard Lora）多0.1％。性能和效率位置之间的平衡将动态洛拉作为微调LLM的实用，可扩展的解决方案，尤其是在资源约束的情况下。为了进一步，它的适应性使其成为更高级应用程序（包括多模式任务）的有前途的基础。

### 3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose Diffusion via Rectified Flow 
[[arxiv](https://arxiv.org/abs/2501.16698)] [[cool](https://papers.cool/arxiv/2501.16698)] [[pdf](https://arxiv.org/pdf/2501.16698)]
> **Authors**: Yueen Ma,Yuzheng Zhuang,Jianye Hao,Irwin King
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: Preprint. Work in progress
- **标题**: 3D-MOE：Experts多模式LLM的混合物，用于3D视觉和通过整流流量扩散
- **领域**: 计算语言学,计算机视觉和模式识别,机器人技术
- **摘要**: 长期以来，3D视觉和空间推理被认为是准确地感知我们的三维世界的优选，尤其是与基于2D图像的传统视觉推理相比。由于很难收集高质量的3D数据，因此该领域的研究直到最近才获得动力。随着强大的大型语言模型（LLM）的出现，在过去的几年中，已经开发了3D视觉的多模式LLM。但是，这些模型中的大多数主要关注3D数据的视觉编码器。在本文中，我们提出将现有的密集激活的LLMS转换为专家的混合物（MOE）模型，这些模型已被证明对多模式数据处理有效。除了利用这些模型的指导遵循功能外，我们还通过连接扩散头Pose-Dit，进一步实现了体现的任务计划，该姿势姿势dit采用了新颖的整流流量扩散调度程序。 3D问答和任务规划任务的实验结果表明，我们的3D-MOE框架通过更少的激活参数实现了提高的性能。

### MME-Industry: A Cross-Industry Multimodal Evaluation Benchmark 
[[arxiv](https://arxiv.org/abs/2501.16688)] [[cool](https://papers.cool/arxiv/2501.16688)] [[pdf](https://arxiv.org/pdf/2501.16688)]
> **Authors**: Dongyi Yi,Guibo Zhu,Chenglin Ding,Zongshu Li,Dong Yi,Jinqiao Wang
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: 9 pages,2 figures
- **标题**: MME-Industry：跨境多模式评估基准
- **领域**: 计算语言学
- **摘要**: 随着多模式大语言模型（MLLM）的快速发展，出现了许多评估基准。但是，对它们在各种工业应用中的绩效的全面评估仍然有限。在本文中，我们介绍了MME-Industry，这是一种专门用于评估工业环境中MLLM的新型基准。基准包括21个不同的域，包括1050个问题解答，每个领域有50个问题。为了确保数据完整性并防止公共数据集的潜在泄漏，所有问答对均由域专家手动制作和验证。此外，通过合并可以直接回答的非敬礼问题以及需要专业领域知识的任务，可以有效地增强基准的复杂性。此外，我们同时提供中文和英语版本的基准，从而对MLLM跨这些语言的功能进行了比较分析。我们的发现为MLLM的实用工业应用提供了宝贵的见解，并阐明了未来模型优化研究的有希望的方向。

### Contextual Reinforcement in Multimodal Token Compression for Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.16658)] [[cool](https://papers.cool/arxiv/2501.16658)] [[pdf](https://arxiv.org/pdf/2501.16658)]
> **Authors**: Naderdel Piero,Zacharias Cromwell,Nathaniel Wainwright,Matthias Nethercott
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 大语模型的多模式令牌压缩中的上下文加强
- **领域**: 计算语言学,人工智能
- **摘要**: 有效的令牌压缩仍然是扩展模型处理日益复杂和多样化数据集的关键挑战。引入了基于上下文加固的新型机制，通过相互依存和语义相关性动态地调整令牌的重要性。这种方法可以大大减少令牌用法，同时保留信息表示的质量和连贯性。该方法结合了基于图的算法和自适应加权，可捕获跨文本和多模式数据的微妙的上下文关系，从而确保下游任务中的稳健对齐和性能。跨不同领域的评估揭示了准确性和语义保留的显着改善，尤其是对于需要详细跨模式相互作用的任务。记忆使用分析表明，尽管有其他强化过程，但在开销中的开销很小。通过错误分布分析，进一步验证了性能增长，与基线模型相比，语义损失和句法不一致降低。模块化体系结构可确保与各种开源框架的兼容性，从而促进可扩展的实现现实世界应用程序。这些发现突出了上下文加强在重新定义代币管理策略和推进大规模模型设计的潜力。

### An LLM Benchmark for Addressee Recognition in Multi-modal Multi-party Dialogue 
[[arxiv](https://arxiv.org/abs/2501.16643)] [[cool](https://papers.cool/arxiv/2501.16643)] [[pdf](https://arxiv.org/pdf/2501.16643)]
> **Authors**: Koji Inoue,Divesh Lala,Mikey Elmers,Keiko Ochi,Tatsuya Kawahara
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 多模式多党对话中收件人识别的LLM基准测试
- **领域**: 计算语言学,人工智能,声音,音频和语音处理
- **摘要**: 处理多方对话是推进口语对话系统的重要一步，需要开发特定于多方互动的任务。为了应对这一挑战，我们正在构建一个多模式的多方对话库（三参与者）讨论。本文重点介绍了收件人识别的任务，并确定正在向谁讲话要进行下一个回合，这是多方对话系统独有的关键组成部分。用收件人信息注释了一部分语料库，表明在大约20％的会话转弯中指示了显式收件人。为了评估任务的复杂性，我们在收件人识别中基准了大型语言模型（GPT-4O）的性能。结果表明，GPT-4O仅在偶然的情况下才能达到准确性，这突显了多方对话中收件人识别的挑战。这些发现凸显了需要进一步研究以增强大语言模型在理解和导航多方对话动态的复杂性方面的能力。

### CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2501.16629)] [[cool](https://papers.cool/arxiv/2501.16629)] [[pdf](https://arxiv.org/pdf/2501.16629)]
> **Authors**: Jinlan Fu,Shenzhen Huangfu,Hao Fei,Xiaoyu Shen,Bryan Hooi,Xipeng Qiu,See-Kiong Ng
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: Accepted by ICLR 2025
- **标题**: 芯片：多模式LLMS的跨模式层次直接优化优化
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 尽管具有令人印象深刻的能力，但多模式的大型语言模型（MLLM）仍然在幻觉上挣扎。最近的研究试图通过使用基于文本响应的偏好对应用直接偏好优化（DPO）来缓解这种情况。但是，我们对表示分布的分析表明，多模式DPO努力努力使图像和文本表示并区分幻觉和非抗解描述。为了应对这些挑战，在这项工作中，我们提出了一个跨模式层次直接偏好优化（芯片），以解决这些局限性。我们在DPO框架中介绍了一个视觉偏好优化模块，使MLLM可以同时从文本和视觉偏好中学习。此外，我们提出了一个层次的文本偏好优化模块，该模块允许模型在多个颗粒水平上捕获偏好，包括响应，段和令牌级别。我们通过定量和定性分析来评估CHIP，并在多个基准测试中进行了结果，证明了其在减少幻觉方面的有效性。在对象HALBENCH数据集上，CHIP在减少幻觉中的表现优于DPO，基于基本模型松饼和LLAVA模型，相对点的改善分别提高了52.7％和55.5％的相对点。我们将所有数据集和代码公开可用：https：//github.com/lvugai/chip。

### Towards Explainable Multimodal Depression Recognition for Clinical Interviews 
[[arxiv](https://arxiv.org/abs/2501.16106)] [[cool](https://papers.cool/arxiv/2501.16106)] [[pdf](https://arxiv.org/pdf/2501.16106)]
> **Authors**: Wenjie Zheng,Qiming Xie,Zengzhi Wang,Jianfei Yu,Rui Xia
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: 21 pages
- **标题**: 为临床访谈的可解释的多模式抑郁识别
- **领域**: 计算语言学
- **摘要**: 最近，临床访谈的多模式抑郁识别（MDRC）最近引起了相当大的关注。现有的MDRC研究主要集中于改善任务绩效并取得了重大发展。但是，对于临床应用，模型透明度至关重要，以前的工作忽略了决策过程的解释性。为了解决这个问题，我们建议对临床访谈（EMDRC）任务进行可解释的多模式抑郁识别，该任务旨在通过总结症状并发现潜在的原因来提供抑郁症识别的证据。鉴于访谈者参与者的互动情况，EMDRC的目标是根据八个项目的患者健康调查表抑郁量表（PHQ-8）构建参与者的症状，并预测其抑郁症的严重性。为了解决EMDRC任务，我们基于现有的MDRC数据集构建了一个新数据集。此外，我们利用PHQ-8并提出了PHQ感知的多模式多任务学习框架，该框架捕获了与症状相关的语义信息，以帮助生成对话级别的摘要。我们注释数据集的实验结果证明了我们提出的方法比基线系统在EMDRC任务上的优越性。

### NUS-Emo at SemEval-2024 Task 3: Instruction-Tuning LLM for Multimodal Emotion-Cause Analysis in Conversations 
[[arxiv](https://arxiv.org/abs/2501.17261)] [[cool](https://papers.cool/arxiv/2501.17261)] [[pdf](https://arxiv.org/pdf/2501.17261)]
> **Authors**: Meng Luo,Han Zhang,Shengqiong Wu,Bobo Li,Hong Han,Hao Fei
> **First submission**: 2024-08-22
> **First announcement**: 2025-01-29
> **comment**: 2nd place at SemEval-2024 Task 3, Subtask 2, to appear in SemEval-2024 proceedings
- **标题**: NUS-EMO在SEMEVAL-2024任务3：在对话中进行多模式情感分析的指令调用LLM
- **领域**: 计算语言学
- **摘要**: 本文介绍了为Semeval-2024的任务3开发的系统架构：对话中的多模式情感原因分析。我们的项目针对子任务2的挑战，该挑战致力于多模式情感因素对情感类别（MECPE-cat）的提取，并构建了一个针对此任务的独特挑战量身定制的双组分系统。我们将任务分为两个子任务：对话中的情感识别（ERC）和情感因子对提取（ECPE）。为了解决这些子任务，我们利用了大语言模型（LLMS）的能力，这些模型始终在各种自然语言处理任务和域中展示了最先进的表现。最重要的是，我们为LLMS设计一种情感因素的教学方法，以通过相应的因果理性来增强情绪的感知。我们的方法使我们能够熟练地浏览MECPE-CAT的复杂性，达到任务的加权平均34.71％的F1得分，并确保排行榜上的第二名。复制我们的实验的代码和元数据都可以公开使用。

### Multimodal Magic Elevating Depression Detection with a Fusion of Text and Audio Intelligence 
[[arxiv](https://arxiv.org/abs/2501.16813)] [[cool](https://papers.cool/arxiv/2501.16813)] [[pdf](https://arxiv.org/pdf/2501.16813)]
> **Authors**: Lindy Gan,Yifan Huang,Xiaoyang Gao,Jiaming Tan,Fujun Zhao,Tao Yang
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: 21 pages,7 figures.1 table
- **标题**: 多模式的魔术抬高抑郁症检测，并融合文本和音频智能
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 这项研究提出了一个基于教师学生建筑的创新多模式融合模型，以增强抑郁分类的准确性。我们设计的模型通过引入多头注意力机制和加权多模式转移学习来解决特征融合和模态重量分配中传统方法的局限性。在文本和听觉教师模型的指导下，利用DAIC-WOZ数据集（学生融合模型）实现了分类准确性的显着提高。消融实验表明，所提出的模型的F1得分为99。1％在测试集中，大大优于单峰和常规方法。我们的方法有效地捕获了文本和音频功能之间的互补性，同时动态调整教师模型以增强概括能力的贡献。实验结果突出了所提出的框架在处理复杂多模式数据中的鲁棒性和适应性。这项研究为抑郁症分析中的多模式大型学习提供了一种新颖的技术框架，为解决现有方法的局限性提供了新的见解，以模态融合和特征提取。

### Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare Scripts 
[[arxiv](https://arxiv.org/abs/2501.17785)] [[cool](https://papers.cool/arxiv/2501.17785)] [[pdf](https://arxiv.org/pdf/2501.17785)]
> **Authors**: Yu-Fei Shih,Zheng-Lin Lin,Shu-Kai Hsieh
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: 7 pages, 3 figures
- **标题**: 对字形的推理：LLM对稀有脚本的解密评估
- **领域**: 计算语言学,机器学习
- **摘要**: 我们探讨了LVLM和LLM在Unicode中未编码的稀有脚本中的功能。我们介绍了一种新颖的方法，用于构建涉及此类脚本的语言难题的多模式数据集，并利用语言glyphs的令牌化方法。我们的方法包括用于LVLM的图片方法和LLMS的描述方法，使这些模型能够应对这些挑战。我们在语言难题上使用突出的模型GPT-4O，Gemini和Claude 3.5十四行诗进行实验。我们的发现揭示了当前AI方法在语言解密中的优势和局限性，强调了Unicode编码对模型性能的影响以及通过描述对视觉语言代币建模的挑战。我们的研究促进了对AI在语言解密的潜力的理解，并强调了进一步研究的需求。

### Exploring Vision Language Models for Multimodal and Multilingual Stance Detection 
[[arxiv](https://arxiv.org/abs/2501.17654)] [[cool](https://papers.cool/arxiv/2501.17654)] [[pdf](https://arxiv.org/pdf/2501.17654)]
> **Authors**: Jake Vasilakes,Carolina Scarton,Zhixue Zhao
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: Submitted to the International AAAI Conference on Web and Social Media (ICWSM) 2025
- **标题**: 探索视觉语言模型用于多模式和多语言姿势检测
- **领域**: 计算语言学,人工智能
- **摘要**: 社交媒体的全球影响力扩大了信息的传播，突出了对强大的自然语言处理任务的需求，例如跨语言和方式的立场检测。先前的研究主要集中在仅文本输入上，而留下了多模式的场景，例如涉及图像和文本的情况，相对不受欢迎。同时，近年来，多模式职位的流行率显着增加。尽管最先进的视觉语言模型（VLMS）表现出了希望，但它们在多模式和多语言姿势检测任务上的表现仍然很大程度上尚未进行。本文在新扩展的数据集上评估了最新的VLM，该数据集涵盖了七种语言和多模式输入，研究了它们对视觉提示的使用，特定于语言的性能和交叉模式的交互。我们的结果表明，VLM通常更依赖文本而不是图像来进行立场检测，并且这种趋势始终存在于语言上。此外，VLMS与其他视觉内容相比，VLMS明显更多地依赖图像中包含的文本。关于多语言性，所研究的模型倾向于在语言上产生一致的预测，无论它们是否明确多种语言，尽管有些异常值与宏F1，语言支持和模型大小不协调。

### Tonguescape: Exploring Language Models Understanding of Vowel Articulation 
[[arxiv](https://arxiv.org/abs/2501.17643)] [[cool](https://papers.cool/arxiv/2501.17643)] [[pdf](https://arxiv.org/pdf/2501.17643)]
> **Authors**: Haruki Sakajo,Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: Accepted to NAACL 2025
- **标题**: 舌头：探索语言模型的理解元音发音
- **领域**: 计算语言学,人工智能
- **摘要**: 元音主要以舌头位置为特征。人类通过自己的经验和明确的客观观察（例如使用MRI）发现了元音发音的这些特征。有了这些知识和我们的经验，我们可以解释和理解舌头位置和元音之间的关系，这些知识有助于语言学习者学习发音。由于语言模型（LMS）经过大量包括语言和医学领域的数据培训，因此我们的初步研究表明，LM能够解释元音的发音机制。但是，尚不清楚多模式LMS（例如Vision LMS）是否将文本信息与视觉信息保持一致。出现一个问题：LMS是否将真实的舌头位置与元音相关？在这项研究中，我们从现有的实时MRI数据集中创建了视频和图像数据集，并研究了LMS是否可以使用基于视觉的信息来理解基于舌头位置的元音发音。我们的发现表明，当提供参考示例的情况下，在没有它们的情况下，LMS提供了元音和舌头位置的潜力。我们的数据集构建代码可在GitHub上找到。

### Divergent Emotional Patterns in Disinformation on Social Media? An Analysis of Tweets and TikToks about the DANA in Valencia 
[[arxiv](https://arxiv.org/abs/2501.18640)] [[cool](https://papers.cool/arxiv/2501.18640)] [[pdf](https://arxiv.org/pdf/2501.18640)]
> **Authors**: Iván Arcos,Paolo Rosso,Ramón Salaverría
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-31
> **comment**: ef:Proceedings of the 17th International Conference on Agents and Artificial Intelligence (ICAART 2025), Porto, Portugal, February 23-25, 2025
- **标题**: 社交媒体上虚假信息的情绪模式分歧？对Valencia的Dana的推文和Tiktoks的分析
- **领域**: 计算语言学,计算机与社会,社交和信息网络
- **摘要**: 这项研究调查了达娜（Dana）活动期间社交媒体平台上的虚假信息的传播（达娜（Dana）是西班牙的首字母缩写。虚假信息和值得信赖的内容。此外，GPT-4O的几次注释方法与手动标签达成了实质性的一致性（Cohen的Kappa为0.684）。情绪分析表明，X上的虚假信息主要与增加的悲伤和恐惧有关，而在Tiktok上，它与更高水平的愤怒和厌恶有关。使用LIWC词典的语言分析表明，值得信赖的内容利用了更多的清晰和事实语言，而虚假信息则采用否定，感知词和个人轶事来显得可信。 Tiktok帖子的音频分析强调了不同的模式：值得信赖的音频具有更明亮的音调，机器人或单调的叙述，促进了清晰度和信誉，而虚假信息则利用了音频变化，情感深度和操纵性的音乐元素来扩大互动。在检测模型中，SVM+TF-IDF达到了最高的F1得分，并且具有有限的数据表现出色。将音频功能纳入Roberta-large-BNE中提高了精度和F1得分，超过了其仅本文的准确性和SVM的精度。 GPT-4O几射线也表现良好，展示了大型语言模型自动虚假信息检测的潜力。这些发现证明了利用文本和音频功能以改善Tiktok等多模式平台上的虚假信息检测的重要性。

## 密码学和安全(cs.CR:Cryptography and Security)

该领域共有 7 篇论文

### Dynamic Feature Fusion: Combining Global Graph Structures and Local Semantics for Blockchain Fraud Detection 
[[arxiv](https://arxiv.org/abs/2501.02032)] [[cool](https://papers.cool/arxiv/2501.02032)] [[pdf](https://arxiv.org/pdf/2501.02032)]
> **Authors**: Zhang Sheng,Liangliang Song,Yanbin Wang
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 动态功能融合：结合全局图结构和区块链欺诈检测的本地语义
- **领域**: 密码学和安全,人工智能,软件工程
- **摘要**: 区块链技术的出现促进了金融领域广泛采用智能合约。但是，当前的欺诈检测方法在捕获交易网络中的全局结构模式和交易数据中嵌入的局部语义关系时表现出局限性。大多数现有的模型都集中在结构信息或语义特征上，从而在检测复杂的欺诈模式时次优性能。在本文中，我们提出了一个动态功能融合模型，该模型结合了基于图形的表示学习和语义特征提取区块链欺诈检测。具体来说，我们构建全局图表来建模帐户关系并从交易数据中提取本地上下文特征。引入了动态多模式融合机制，以自适应地整合这些特征，从而使模型能够有效地捕获结构和语义欺诈模式。我们进一步开发了全面的数据处理管道，包括图形构造，时间功能增强和文本预处理。大规模实际区块链数据集的实验结果表明，我们的方法在精度，F1分数和召回指标上的表现优于现有基准。这项工作突出了整合结构关系和语义相似性以在强大的欺诈检测中的重要性，并为保护区块链系统提供了可扩展的解决方案。

### Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency 
[[arxiv](https://arxiv.org/abs/2501.04931)] [[cool](https://papers.cool/arxiv/2501.04931)] [[pdf](https://arxiv.org/pdf/2501.04931)]
> **Authors**: Shiji Zhao,Ranjie Duan,Fengxiang Wang,Chi Chen,Caixin Kang,Jialing Tao,YueFeng Chen,Hui Xue,Xingxing Wei
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 通过洗牌不一致的越狱多模式模型
- **领域**: 密码学和安全,人工智能,计算语言学
- **摘要**: 多模式的大语言模型（MLLM）取得了令人印象深刻的性能，并已在商业应用中实用，但它们仍然具有潜在的安全机制脆弱性。越狱攻击是红色小组的方法，旨在绕过安全机制并发现MLLM的潜在风险。现有的MLLM的越狱方法通常通过复杂的优化方法或精心设计的图像和文本提示绕过模型的安全机制。尽管取得了一些进展，但他们对商业封闭源MLLM的攻击成功率低。与以前的研究不同，我们从经验上发现，MLLM的理解能力和安全性的安全能力之间存在混乱不一致。也就是说，从理解能力的角度来看，MLLM可以很好地理解改组的有害文本图像指令。但是，从安全能力的角度来看，可以轻松地绕开这些改组的有害说明，从而导致有害的反应。然后，我们创新提出了一场名为Si-Attack的文本图像越狱攻击。具体而言，为了充分利用洗牌不一致并克服了随机性，我们采用基于查询的黑盒优化方法来根据有毒法官模型的反馈选择最有害的洗牌输入。一系列实验表明，Si-Attack可以改善攻击在三个基准测试上的表现。特别是，Si-Attack显然可以提高商业MLLM的攻击成功率，例如GPT-4O或Claude-3.5-Sonnet。

### Playing Devil's Advocate: Unmasking Toxicity and Vulnerabilities in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.09039)] [[cool](https://papers.cool/arxiv/2501.09039)] [[pdf](https://arxiv.org/pdf/2501.09039)]
> **Authors**: Abdulkadir Erol,Trilok Padhi,Agnik Saha,Ugur Kursuncu,Mehmet Emin Aktas
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 演奏魔鬼的拥护者：揭露大型视力模型中的毒性和脆弱性
- **领域**: 密码学和安全,人工智能,计算机与社会
- **摘要**: 大型视觉模型（LVLM）的快速发展具有增强的功能，从而提供了从内容创造到生产率增强的潜在应用。尽管具有创新的潜力，但LVLM仍表现出脆弱性，尤其是在产生潜在的有毒或不安全的反应时。恶意行为者可以利用这些脆弱性以自动化（或半）方式传播有毒内容，从而通过战略性地制作的提示来利用LVLM对欺骗的敏感性，而无需微观或计算密集型程序。尽管与LVLM相关的红色团队努力和固有的潜在风险，但探索LVLMS的漏洞仍然是偏生的，尚未以系统的方式完全解决。这项研究系统地研究了开源LVLM的脆弱性，包括Llava，TenchBlip，Fuyu和Qwen，使用对抗性及时的迅速策略，这些策略模拟了由社会理论所告知的现实世界中社会操纵策略。我们的发现表明，（i）毒性和侮辱是最普遍的行为，平均比率分别为16.13％和9.75％。 （ii）QWEN-VL-CHAT，LLAVA-V1.6-VICUNA-7B和TERCENTBLIP-VICUNA-7B是最脆弱的模型，表现出21.50％，18.30％和17.90％的毒性反应率，以及13.40％，11.70％和10.10％和10.10％和10.10％和10.10％和10.10％和10.10％; （iii）提示结合深度幽默和多模式有毒及时完成的​​策略显着提高了这些脆弱性。尽管为了安全性进行了微调，但这些模型仍会在带有对抗性输入的提示时以不同程度的毒性产生含量，强调了迫切需要增强安全机制和LVLM开发中强大的护栏。

### Multimodal Techniques for Malware Classification 
[[arxiv](https://arxiv.org/abs/2501.10956)] [[cool](https://papers.cool/arxiv/2501.10956)] [[pdf](https://arxiv.org/pdf/2501.10956)]
> **Authors**: Jonathan Jiang,Mark Stamp
> **First submission**: 2025-01-19
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: 恶意软件分类的多模式技术
- **领域**: 密码学和安全,机器学习
- **摘要**: 恶意软件的威胁是计算机网络和系统的严重问题，突出了对准确的分类技术的需求。在这项研究中，我们基于Windows便携式可执行文件（PE）文件格式的结构化性质，尝试使用多模式机器学习方法进行恶意软件分类。具体而言，我们在从PE标头提取的功能上训练支持向量机（SVM），长短期内存（LSTM）和卷积神经网络（CNN）模型，我们在从PE文件的其他部分中提取的功能上训练了这些相同的模型，并在从整个PE文件中提取的功能上提取的每个模型训练了每个模型。然后，我们使用组件模型的输出层概率作为特征向量来训练这些基线模型的九个标头段组合中的每个模型。我们将基线案例与这些多模式组合进行比较。在我们的实验中，我们发现最好的多模式模型优于基线案例中最好的，这表明在Windows PE文件的不同部分上训练单独的模型可能是有利的。

### Intelligent Code Embedding Framework for High-Precision Ransomware Detection via Multimodal Execution Path Analysis 
[[arxiv](https://arxiv.org/abs/2501.15836)] [[cool](https://papers.cool/arxiv/2501.15836)] [[pdf](https://arxiv.org/pdf/2501.15836)]
> **Authors**: Levi Gareth,Maximilian Fairbrother,Peregrine Blackwood,Lucasta Underhill,Benedict Ruthermore
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 通过多模式执行路径分析，用于高精度勒索软件检测的智能代码嵌入框架
- **领域**: 密码学和安全,人工智能
- **摘要**: 现代威胁景观随着越来越复杂，挑战传统检测方法的挑战，并需要能够解决复杂的对抗策略的创新解决方案。开发了一个新颖的框架，以通过多模式执行路径分析来识别勒索软件活动，集成高维嵌入和动态启发式推导机制，以捕获各种攻击变体的行为模式。该方法表现出很高的适应性，有效地减轻了勒索软件家族通常采用的混淆策略和多态性特征来逃避检测。全面的实验评估显示，与基线技术相比，精确，召回和准确度指标的显着进步，尤其是在可变的加密速度和混淆的执行流的条件下。该框架实现了可扩展和计算高效的性能，从而确保了从资源受限环境到高性能基础架构的一系列系统配置的鲁棒适用性。值得注意的发现包括降低假阳性率和增强的检测潜伏期，即使对于采用复杂加密机制的勒索软件系列也是如此。模块化设计允许无缝整合其他方式，从而为新兴威胁媒介提供了可扩展性和对未来的防止。定量分析进一步强调了该系统的能源效率，强调了其在具有严格操作限制的环境中部署的实用性。结果强调了整合先进的计算技术和动态适应性以保护数字生态系统免受日益复杂的威胁的重要性。

### Membership Inference Attacks Against Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.18624)] [[cool](https://papers.cool/arxiv/2501.18624)] [[pdf](https://arxiv.org/pdf/2501.18624)]
> **Authors**: Yuke Hu,Zheng Li,Zhihao Liu,Yang Zhang,Zhan Qin,Kui Ren,Chun Chen
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-31
> **comment**: Accepted by USENIX'25; 22 pages, 28 figures;
- **标题**: 对视觉模型的会员推断攻击
- **领域**: 密码学和安全,人工智能
- **摘要**: 构建基于预先训练的视觉编码器和大型语言模型（LLM）的视觉语言模型（VLMS）显示出了出色的多模式理解和对话能力，将它们定位为下一项技术革命的催化剂。但是，尽管大多数VLM研究都侧重于增强多模式的相互作用，但数据滥用和泄漏的风险在很大程度上没有探索。这促使需要对VLMS中此类风险进行全面调查。在本文中，我们通过会员推理攻击（MIA）的镜头对VLM中的滥用和泄漏检测进行了首次分析。具体而言，我们专注于VLM的指令调整数据，该数据更可能包含敏感或未经授权的信息。为了解决现有MIA方法的局限性，我们引入了一种新型方法，该方法根据一组样本及其对温度的敏感性（VLMS中的独特参数）渗透成员资格。基于此，我们提出了四种会员推理方法，每种方法都针对不同级别的背景知识量身定制，最终达到了最具挑战性的情况。我们的全面评估表明，这些方法可以准确地确定成员资格状态，例如，实现大于0.8的AUC，靶向一个小型集，该集合仅由Llava上的5个样品组成。

### BounTCHA: A CAPTCHA Utilizing Boundary Identification in AI-extended Videos 
[[arxiv](https://arxiv.org/abs/2501.18565)] [[cool](https://papers.cool/arxiv/2501.18565)] [[pdf](https://arxiv.org/pdf/2501.18565)]
> **Authors**: Lehao Lin,Ke Wang,Maha Abdallah,Wei Cai
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: 22 pages, 15 figures
- **标题**: Bountcha：在AI扩展视频中使用边界标识的验证码
- **领域**: 密码学和安全,人工智能,人机交互
- **摘要**: 近年来，人工智能（AI）尤其是多模式大语言模型（MLLM）的快速发展使其能够了解文本，图像，视频和其他多媒体数据，从而使AI系统能够根据人提供的提示执行各种任务。但是，AI驱动的机器人越来越能够绕过大多数现有的验证码系统，对Web应用程序构成了重大安全威胁。这使得新验证机制的设计成为紧迫的优先事项。我们观察到人类对视频的转变和突然变化高度敏感，而当前的AI系统仍在努力理解和对这种情况有效地做出反应。基于此观察，我们设计和实施了Bountcha，这是一种验证机制，它利用了人类对视频过渡和破坏中边界的看法。通过利用AI的功能通过提示扩展原始视频，我们引入了意外的曲折和更改，以创建用于用于验证码目的的简短视频的管道。我们开发一个原型，并进行实验，以收集有关人类时间偏见的数据。该数据是区分人类用户和机器人的基础。此外，我们对Bountcha进行了详细的安全性分析，证明了其对各种攻击的韧性。我们希望Bountcha能够充当强大的防御，并在AI驱动的时代维护数百万个网络应用程序。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 270 篇论文

### Google is all you need: Semi-Supervised Transfer Learning Strategy For Light Multimodal Multi-Task Classification Model 
[[arxiv](https://arxiv.org/abs/2501.01611)] [[cool](https://papers.cool/arxiv/2501.01611)] [[pdf](https://arxiv.org/pdf/2501.01611)]
> **Authors**: Haixu Liu,Penghao Jiang,Zerui Tao
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: Google就是您所需要的：光多模式多任务分类模型的半监督转移学习策略
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着数字图像数据的数量的增加，图像分类的有效性会加剧。这项研究介绍了一个可靠的多标签分类系统，旨在将多个标签分配给单个图像，以解决可能与多个类别相关联的图像的复杂性（1至19，不包括12）。我们提出了一个多模式分类器，该分类器将高级图像识别算法与自然语言处理（NLP）模型合并，并结合了融合模块以整合这些独特的模式。整合文本数据的目的是通过提供上下文理解，即视觉分析无法完全捕获，以提高标签预测的准确性。我们提出的分类模型结合了卷积神经网络（CNN），用于使用NLP技术进行图像处理，用于分析文本描述（即字幕）。这种方法包括严格的训练和验证阶段，每个模型组件通过消融实验进行了验证和分析。初步结果证明了分类器的准确性和效率，突出了其作为自动图像标记系统的潜力。

### SAFER: Sharpness Aware layer-selective Finetuning for Enhanced Robustness in vision transformers 
[[arxiv](https://arxiv.org/abs/2501.01529)] [[cool](https://papers.cool/arxiv/2501.01529)] [[pdf](https://arxiv.org/pdf/2501.01529)]
> **Authors**: Bhavna Gopal,Huanrui Yang,Mark Horton,Yiran Chen
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 更安全：清晰度意识到层选择性固定，以增强视觉变压器的鲁棒性
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉变压器（VIT）已成为高级计算机视觉应用程序和多模式基础模型中的必不可少的骨干。尽管有优势，VIT仍然容易受到对抗性扰动的影响，可与卷积神经网络（CNN）的脆弱性相当。此外，大型参数计数和VIT的复杂体系结构使它们特别容易易于过度拟合，通常会损害清洁和对抗性的精度。本文通过一种新颖的，层次选择的微调方法来缓解对抗性过度的VIT：更安全。我们没有优化整个模型，而是识别并选择性地微调了最容易过度拟合的小部分层，在冻结模型的其余部分的同时，对这些层施加了清晰度感知的最小化。我们的方法一致地增强了基线方法的清洁和对抗精度。典型的改进约为5％，在某些情况下，各种VIT架构和数据集的增长幅度高达20％。

### Multi-Modal Video Feature Extraction for Popularity Prediction 
[[arxiv](https://arxiv.org/abs/2501.01422)] [[cool](https://papers.cool/arxiv/2501.01422)] [[pdf](https://arxiv.org/pdf/2501.01422)]
> **Authors**: Haixu Liu,Wenning Wang,Haoxiang Zheng,Penghao Jiang,Qirui Wang,Ruiqing Yan,Qiuzhuang Sun
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: INFORMS 2024 Data Challenge Competition
- **标题**: 多模式视频功能提取用于普及预测
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 这项工作旨在使用视频本身及其相关功能来预测短视频的普及。受欢迎程度是通过四个关键参与度量指标来衡量的：视图计数，例如计数，评论计数和共享计数。这项研究采用具有不同体系结构和培训方法的视频分类模型作为骨干网络，以提取视频模态特征。同时，将清洁的视频字幕纳入了经过精心设计的及时框架，以及视频，作为视频到文本生成模型的输入，该模型生成了详细的基于文本的视频内容理解。然后，使用预训练的BERT模型将这些文本编码为向量。基于上述六组向量，为四个预测指标中的每个指标中的每个媒介进行了训练。此外，该研究根据视频和表格数据进行了数据挖掘和功能工程，构建了实用特征，例如主题标签出现的总频率，提及外观的总频率，视频持续时间，框架计数，帧数，帧速率，帧速率和总时间在线。训练了多个机器学习模型，并选择了最稳定的XGBOOST。最后，对神经网络和XGBoost模型的预测进行平均以获得最终结果。

### Hierarchical Alignment-enhanced Adaptive Grounding Network for Generalized Referring Expression Comprehension 
[[arxiv](https://arxiv.org/abs/2501.01416)] [[cool](https://papers.cool/arxiv/2501.01416)] [[pdf](https://arxiv.org/pdf/2501.01416)]
> **Authors**: Yaxian Wang,Henghui Ding,Shuting He,Xudong Jiang,Bifan Wei,Jun Liu
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: AAAI 2025
- **标题**: 用于广义参考表达理解的分层对齐增强的自适应接地网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项工作中，我们解决了广义推荐表达理解（GREC）的具有挑战性的任务。与侧重于单目标表达式的经典参考表达理解（REC）相比，GREC通过进一步涵盖无目标和多目标表达式将范围扩展到更实用的环境。现有的REC方法在处理GREC中遇到的复杂案例时面临挑战，这主要是由于它们的固定输出和多模式表示中的局限性。为了解决这些问题，我们提出了一个针对GREC的层次结构增强的自适应接地网络（HIEA2G），该网络可以灵活地处理各种类型的参考表达式。首先，提出了层次多模式的语义对准（HMSA）模块，以结合三个级别的比对，包括单词对象，短语 - 对象和文本图像对齐。它使跨多个级别的层次交叉模式相互作用能够获得全面，健壮的多模式理解，从而极大地增强了复杂病例的接地能力。然后，为了解决GREC中不同数量的目标对象，我们引入了一个自适应接地计数器（AGC），以动态确定输出目标的数量。此外，AGC在AGC中采用了辅助对比损失来增强对象计数能力，从而吸收具有相同计数的多模式特征，并将其推翻具有不同计数的功能。广泛的实验结果表明，HIEA2G在具有挑战性的GREC任务上以及其他四个任务，包括REC，短语接地，参考表达细分（RES）以及广义的参考表达细分（GRES），在具有挑战性的GREC任务上实现了新的最新性能，证明了提议HIEA2G的显着优势和普遍性。

### SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization 
[[arxiv](https://arxiv.org/abs/2501.01245)] [[cool](https://papers.cool/arxiv/2501.01245)] [[pdf](https://arxiv.org/pdf/2501.01245)]
> **Authors**: Yongle Huang,Haodong Chen,Zhenbang Xu,Zihan Jia,Haozhou Sun,Dian Shao
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: AAAI 2025; Code: https://github.com/KyleHuang9/SeFAR
- **标题**: SEFAR：半监督的细粒度识别，具有时间扰动和学习稳定
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 人类的行动理解对于多模式系统的发展至关重要。尽管最新的发展是由强大的大语言模型（LLM）驱动的，目的是足够涵盖各种类别，但他们经常忽略对更具体功能的需求。在这项工作中，我们解决了更具挑战性的精细动作识别（FAR）的任务，该任务侧重于较短的时间持续时间内的详细语义标签（例如，“ Salto salto ting with 1 Awt tuck”）。鉴于注释细粒标签的高成本和微调LLM所需的大量数据，我们建议采用半监督学习（SSL）。我们的框架Sefar结合了几种创新设计，以应对这些挑战。具体来说，为了捕获足够的视觉细节，我们构建了双级时间元素作为更有效的表示，基于我们，我们设计了一种新的强大增强策略，以通过涉及适度的时间扰动来为教师学习学习范式。此外，为了处理教师模型对远方的预测中的高度不确定性，我们建议适应性调节以稳定学习过程。实验表明，Sefar在两个数据范围的两个远处数据集上实现了最新的性能。在两个经典的粗粒数据集UCF101和HMDB51上，它还优于其他半监督方法。进一步的分析和消融研究验证了我们的设计的有效性。此外，我们表明我们的Sefar提取的功能在很大程度上可以促进多模式基础模型了解细粒和特定领域的语义的能力。

### Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants 
[[arxiv](https://arxiv.org/abs/2501.01243)] [[cool](https://papers.cool/arxiv/2501.01243)] [[pdf](https://arxiv.org/pdf/2501.01243)]
> **Authors**: Lixiong Qin,Shilong Ou,Miaoxuan Zhang,Jiangning Wei,Yuhang Zhang,Xiaoshuai Song,Yuchen Liu,Mei Wang,Weiran Xu
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: 50 pages, 14 figures, 41 tables. Submitted to ICLR 2025
- **标题**: 面对面的板凳：多模式助手的面部和人类理解的全面基准
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 面孔和人类是社交互动的关键要素，被广泛包含在日常照片和视频中。因此，对面孔和人类的深刻了解将使多模式助手能够提高响应质量并扩大应用范围。目前，多模式助理社区缺乏对面部和人类理解能力的全面和科学评估。在本文中，我们首先提出了包括三个能力级别的层次能力分类学。然后，基于此分类法，我们从面部和人类社区中公开可用的数据集收集图像和注释，并建立半自动数据管道，以为新基准提供问题。最后，获得的面部板凳包括一个开发套装，其中有900个问题和一个带有1800个问题的测试集，支持英语和中文。我们通过面对面的基础进行了25多种主流多模式大型语言模型（MLLM）的评估，重点是能力之间的相关性，目标相对位置对性能的影响以及促使性能的思想链（COT）的影响（COT）的影响。此外，受到多模式代理的启发，我们还探讨了MLLM的哪些能力需要由专业模型补充。

### Asymmetric Reinforcing against Multi-modal Representation Bias 
[[arxiv](https://arxiv.org/abs/2501.01240)] [[cool](https://papers.cool/arxiv/2501.01240)] [[pdf](https://arxiv.org/pdf/2501.01240)]
> **Authors**: Xiyuan Gao,Bing Cao,Pengfei Zhu,Nannan Wang,Qinghua Hu
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: Accepted by AAAI 2025
- **标题**: 针对多模式表示偏置的不对称加强
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式学习的优势在于它有能力从各种来源整合信息，提供丰富而全面的见解。但是，在实际情况下，多模式系统通常面临动态方式贡献的挑战，不同方式的优势可能随环境而变化，从而导致多模式学习的次优性能。当前方法主要增强了弱模态以平衡多模式表示偏差，从部分模式的角度来看，这不可避免地优化，很容易导致绩效下降的主导方式。为了解决这个问题，我们提出了一种针对多模式表示偏置（ARM）的不对称加固方法。我们的手臂动态增强了弱模态，同时通过有条件的相互信息保持了代表主导方式的能力。此外，我们提供了深入的分析，即优化某些模式可能会导致信息丢失并防止利用多模式数据的全部优势。通过探索模式之间的优势和缩小贡献差距，我们显着提高了多模式学习的表现，在减轻不平衡的多模式学习方面取得了显着进步。

### Conditional Consistency Guided Image Translation and Enhancement 
[[arxiv](https://arxiv.org/abs/2501.01223)] [[cool](https://papers.cool/arxiv/2501.01223)] [[pdf](https://arxiv.org/pdf/2501.01223)]
> **Authors**: Amil Bhagat,Milind Jain,A. V. Subramanyam
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: 6 pages, 5 figures, 4 tables, The first two authors contributed equally
- **标题**: 有条件的一致性引导图像翻译和增强
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 一致性模型已成为扩散模型的有希望的替代品，通过单步样本生成提供了高质量的生成能力。但是，它们在多域图像翻译任务中的应用，例如跨模式翻译和低光图像增强，在很大程度上尚未探索。在本文中，我们通过合并其他条件输入来介绍有条件的一致性模型（CCM），以进行多域图像翻译。我们通过引入特定任务的条件输入来实现这些修改，从而指导转换过程，从而确保生成的输出保留来自相应输入域中的结构和上下文信息。我们在10个不同的数据集上评估CCM，证明了它们在跨多个领域产生高质量翻译图像方面的有效性。代码可在https://github.com/amilbhagat/conditional-consistency-models上找到。

### Real-time Cross-modal Cybersickness Prediction in Virtual Reality 
[[arxiv](https://arxiv.org/abs/2501.01212)] [[cool](https://papers.cool/arxiv/2501.01212)] [[pdf](https://arxiv.org/pdf/2501.01212)]
> **Authors**: Yitong Zhu,Tangyao Li,Yuyang Wang
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 虚拟现实中的实时跨模式Cyber​​sickness预测
- **领域**: 计算机视觉和模式识别,人机交互
- **摘要**: Cyber​​sickness仍然是对沉浸式虚拟现实（VR）体验广泛采用的重大障碍，因为它可以极大地破坏用户的参与和舒适性。研究表明，Cyber​​sickness可以显着反映在头部和眼睛跟踪数据中，以及其他生理数据（例如TMP，EDA和BMP）。尽管采用了CNN和LSTM等深度学习技术，但这些模型通常很难捕获多种数据模式之间的复杂相互作用，并且缺乏实时推理的能力，从而限制了它们的实际应用。在解决这一差距时，我们提出了一个轻巧的模型，该模型利用具有稀疏自我注意力的基于变压器的编码器来处理生物信号特征和一个用于视频功能提取的PP-TSN网络。然后，这些功能通过跨模式融合模块集成，创建了视频感知的生物信号表示，该表示基于视觉和生物信号输入支持Cyber​​sickness预测。我们的模型经过轻巧的框架，在包含眼睛和头部跟踪数据，生理数据和VR视频的公共数据集上进行了验证，并在Cyber​​sickness预测中证明了最先进的性能，仅使用VR视频输入获得了93.13 \％的高精度。这些发现表明，我们的方法不仅可以实现有效的实时网络智能预测，而且还解决了VR环境中长期存在的模态相互作用问题。这一进步为VR中多模式数据集成的未来研究奠定了基础，这可能会导致更个性化，舒适且广泛访问的VR体验。

### Towards Interactive Deepfake Analysis 
[[arxiv](https://arxiv.org/abs/2501.01164)] [[cool](https://papers.cool/arxiv/2501.01164)] [[pdf](https://arxiv.org/pdf/2501.01164)]
> **Authors**: Lixiong Qin,Ning Jiang,Yang Zhang,Yuhan Qiu,Dingheng Zeng,Jiani Hu,Weihong Deng
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 进行互动深层分析
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的DeepFake分析方法主要基于判别模型，这大大限制了其应用程序方案。本文旨在通过对多模式大语言模型（MLLM）进行指导调整来探索交互式深层分析。这将面临诸如缺乏数据集和基准测试以及培训效率低的挑战。 To address these issues, we introduce (1) a GPT-assisted data construction process resulting in an instruction-following dataset called DFA-Instruct, (2) a benchmark named DFA-Bench, designed to comprehensively evaluate the capabilities of MLLMs in deepfake detection, deepfake classification, and artifact description, and (3) construct an interactive deepfake analysis system called DFA-GPT, as a strong baseline for the community,具有低级适应（LORA）模块。该数据集和代码将在https://github.com/lxq1000/dfa-sinstruct中提供，以促进进一步的研究。

### 3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer 
[[arxiv](https://arxiv.org/abs/2501.01163)] [[cool](https://papers.cool/arxiv/2501.01163)] [[pdf](https://arxiv.org/pdf/2501.01163)]
> **Authors**: Jiajun Deng,Tianyu He,Li Jiang,Tianyu Wang,Feras Dayoub,Ian Reid
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 3D-LALAVA：使用Omni SuperPoint变压器的通才3D LMM
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的3D大型多模型模型（3D LMM）在基于3D的对话和推理中显示出巨大的潜力。但是，如何进一步增强3D LMM，以实现精细的场景理解并促进灵活的人类代理互动仍然是一个挑战性的问题。在这项工作中，我们介绍了3D-llava，这是一个简单而强大的3D LMM，旨在充当理解，推理和与3D世界互动的智能助手。与依赖复杂管道的现有最佳方法不同，例如离线多视图功能提取或其他特定于任务的头部-3D-llava采用了具有集成体系结构的简约设计，并且仅将点云作为输入。在3D-llava的核心是一个新的Omni SuperPoint变压器（OST），它集成了三个功能：（1）一个可转换和选择视觉令牌的视觉功能选择器，（2）视觉提示编码器，将交互式视觉提示嵌入到视觉代码空间中，以及（3）基于3D型效果的引用掩码解码器，以3D型效果描述了todstextsstextresss todstextsstect。这种多功能的OST通过杂交预处理以获得感知先验并将其作为将3D数据桥接到LLM桥接的视觉连接器所赋予的能力。进行统一的指令调整后，我们的3D-lalava在各种基准测试中报告了令人印象深刻的结果。该代码和模型将被发布以促进未来的探索。

### Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning 
[[arxiv](https://arxiv.org/abs/2501.01120)] [[cool](https://papers.cool/arxiv/2501.01120)] [[pdf](https://arxiv.org/pdf/2501.01120)]
> **Authors**: Jian Lang,Zhangtao Cheng,Ting Zhong,Fan Zhou
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: 9 pages, 8 figures. Accepted by AAAI 2025. Codes are released at https://github.com/Jian-Lang/RAGPT
- **标题**: 检索启动的动态及时调整，以进行多模式学习
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 具有不完整方式的多模式学习是实用且具有挑战性的。最近，研究人员专注于通过应用可学习的提示在缺失的模态条件下提高预训练的多模式变压器（MMT）的鲁棒性。但是，这些基于及时的方法面临着几个局限性：（1）不完整的模态为特定于任务的推断提供了受限的模态提示，（2）虚拟内容引起的内容引起了信息丢失并引入噪声，并且（3）静态提示是实例 - 势不可挡，为各种缺失条件提供了有限的知识。为了解决这些问题，我们提出了Ragpt，这是一个新颖的检索动态及时调整框架。破布包含三个模块：（i）多通道检索器，通过模式内检索策略来识别相似的实例，（ii）缺失的模态生成器，该模态生成器使用检索到的上下文恢复缺失的信息，（iii）上下文意识到的求职者，从相关的实例中捕获上下文知识，并从相​​关的实例中产生动态提示，以使动态提示更加增强，以增强MAL的功能。在三个现实世界数据集上进行的广泛实验表明，破烂在处理不完整的模态问题中始终优于所有竞争基线。我们的工作代码和基于及时的基线的代码可在https://github.com/jian-lang/ragpt上找到。

### BatStyler: Advancing Multi-category Style Generation for Source-free Domain Generalization 
[[arxiv](https://arxiv.org/abs/2501.01109)] [[cool](https://papers.cool/arxiv/2501.01109)] [[pdf](https://arxiv.org/pdf/2501.01109)]
> **Authors**: Xiusheng Xu,Lei Qi,Jingyang Zhou,Xin Geng
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: Accepted by IEEE TCSVT
- **标题**: Batstyler：推进无源域概括的多类样式生成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 无源域的概括（SFDG）旨在开发一个模型，该模型在不依赖任何源域而在看不见的域上执行。但是，由于培训数据的不可用，该实施仍然受到限制。 SFDG的研究集中于基于多种模式的关节空间的多模式模型和样式合成的知识转移，从而消除了对源域图像的依赖性。但是，现有作品主要适用于多域和较少类别的配置，但是多域和多类别配置的性能相对较差。此外，在多类方案中，样式合成的效率也会恶化。如何有效地合成足够多样化的数据并将其应用于多类别配置是具有更大实际价值的方向。在本文中，我们提出了一种称为Batstyler的方法，该方法可用于提高多类方案中样式合成的能力。 BatStyler由两个模块组成：粗糙的语义产生和统一样式的生成模块。粗糙的语义生成模块提取了粗粒语义，以防止在多类别配置中压缩样式多样性学习的空间，而统一样式生成模块则提供了一个样式模板，这些模板均匀地分布在空间中并实施并行训练。广泛的实验表明，我们的方法在较小的类别数据集上表现出可比的性能，同时超过了多类数据集上的最新方法。

### Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs 
[[arxiv](https://arxiv.org/abs/2501.01042)] [[cool](https://papers.cool/arxiv/2501.01042)] [[pdf](https://arxiv.org/pdf/2501.01042)]
> **Authors**: Linhao Huang,Xue Jiang,Zhiqiang Wang,Wentao Mo,Xi Xiao,Bo Han,Yongjie Yin,Feng Zheng
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 基于图像的多模式作为入侵者：对基于视频的MLLM的可转移多模式攻击
- **领域**: 计算机视觉和模式识别,密码学和安全,机器学习
- **摘要**: 基于视频的多模式大型语言模型（V-MLLM）显示了视频文本多模式任务中对对抗示例的脆弱性。但是，对抗性视频转移到了看不见的模型（一种常见且实用的现实世界情景），但没有探索。在本文中，我们开创了对跨V-MLLM的对抗视频样本的可传递性的调查。我们发现，现有的对抗性攻击方法在用于V-MLLMS的黑盒设置中应用时面临重大限制，我们将其归因于以下缺点：（1）在扰动视频功能中缺乏概括，（2）仅关注稀疏的键性键盘框架，并且（3）未能集成多模态信息。为了解决这些局限性，并加深对黑盒情景中V-MLLM漏洞的理解，我们介绍了图像到视频MLLM（I2V-MLLM）攻击。在I2V-MLLM中，我们利用基于图像的多模式模型（IMM）作为替代模型来制作对抗性视频样本。多模式相互作用和时间信息被整合在一起，以破坏潜在空间内的视频表示形式，从而提高了对抗性转移性。此外，引入了一种扰动传播技术来处理不同的未知框架采样策略。实验结果表明，我们的方法可以生成对抗性示例，这些示例在多个视频文本多模式任务上在不同的V-MLLM上表现出强大的可传递性。与对这些模型的白盒攻击相比，我们的黑盒攻击（使用BLIP-2作为替代模型）实现了竞争性能，MSVD-QA的平均攻击成功率为55.48％，视频QA任务的MSRVTT-QA分别为58.26％。我们的代码将在接受后发布。

### 2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining 
[[arxiv](https://arxiv.org/abs/2501.00958)] [[cool](https://papers.cool/arxiv/2501.00958)] [[pdf](https://arxiv.org/pdf/2501.00958)]
> **Authors**: Wenqi Zhang,Hang Zhang,Xin Li,Jiashuo Sun,Yongliang Shen,Weiming Lu,Deli Zhao,Yueting Zhuang,Lidong Bing
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: Under review
- **标题**: 课堂上的2。5年：一本多式联运教科书
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 与图像文本对数据相比，交织的Corpora使视觉语言模型（VLM）像人类一样自然地理解世界。但是，这种现有的数据集从网页上爬了出来，面临着诸如知识密度低，图像文本关系宽松以及图像之间逻辑连贯性差的挑战。另一方面，互联网主持了大量的教学视频（例如，在线几何课程），这些视频被人类广泛用于学习基础学科，但是这些宝贵的资源在VLM培训中仍未充满信心。在本文中，我们介绍了具有更丰富的基础知识的高质量\ textbf {多模式教科书} copus。它收集了超过2.5年的教学视频，总计22,000个小时。我们首先使用LLM传播的分类法系统收集教学视频。然后，我们从视频中逐步提取并完善视觉范围（关键框架），音频（ASR）和文本知识（OCR），并根据时间顺序将其作为图像文本交织语料库组织。与同行相比，我们以视频为中心的教科书提供了更连贯的上下文，更丰富的知识和更好的图像文本对齐方式。实验证明了其出色的预处理表现，尤其是在知识和推理密集型任务（如ScienceQA和Mathvista）中。此外，在我们的教科书上预先训练的VLM表现出了出色的交织上下文意识，并在其几乎没有弹奏的上下文中利用视觉和文本提示来解决任务解决。我们的代码可在https://github.com/damo-nlp-sg/multimodal_textbook上找到。

### Multiscaled Multi-Head Attention-based Video Transformer Network for Hand Gesture Recognition 
[[arxiv](https://arxiv.org/abs/2501.00935)] [[cool](https://papers.cool/arxiv/2501.00935)] [[pdf](https://arxiv.org/pdf/2501.00935)]
> **Authors**: Mallika Garg,Debashis Ghosh,Pyari Mohan Pradhan
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: ef:IEEE Signal Processing Letters ( Volume: 30), 2023
- **标题**: 多头式多头基于注意的视频变压器网络，用于手势识别
- **领域**: 计算机视觉和模式识别,人机交互
- **摘要**: 由于签名者手的姿势，大小和形状的变化，动态的手势识别是具有挑战性的研究领域之一。在这封信中，提出了用于动态手势识别的多标记多头注意视频变压器网络（MSMHA-VTN）。使用Transformer Multiscaled Head注意模型提取多尺度特征的锥体层次结构。提出的模型对变压器的每个头部采用不同的注意力维度，使其能够在多尺度上提供注意力。此外，除了单一模态外，还检查了使用多种模态的识别性能。广泛的实验证明了提出的MSMHA-VTN的出色性能，其总体精度分别为88.22 \％和99.10 \％，分别在NVMENTURE和BRIAREO数据集上。

### Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model 
[[arxiv](https://arxiv.org/abs/2501.00895)] [[cool](https://papers.cool/arxiv/2501.00895)] [[pdf](https://arxiv.org/pdf/2501.00895)]
> **Authors**: Chenyang Liu,Keyan Chen,Rui Zhao,Zhengxia Zou,Zhenwei Shi
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: Text2Erth：使用全局规模数据集解锁文本驱动的遥感图像生成和基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 生成基础模型具有先进的大规模文本驱动的自然图像生成，成为各个垂直领域的重要研究趋势。但是，在遥感领域中，仍然缺乏对大规模文本图像（Text2Image）生成技术的研究。现有的遥感图像文本数据集的规模很小，仅限于特定的地理区域和场景类型。此外，现有的Text2Image方法还在努力实现全球规模，多分辨率可控且无限制的图像生成。为了应对这些挑战，本文提出了两个关键的贡献：GIT-10M数据集和Text2earth基础模型。 GIT-10M是一个全球尺度的图像文本数据集，包含1000万个图像文本对，比以前最大的数据集大5倍。该数据集涵盖了广泛的地理场景，并包含分辨率信息，大小和多样性都超过了现有数据集。在GIT-10M的基础上，我们提出了Text2Erth，这是一个基于扩散框架的13亿个参数生成基础模型，以建模全球规模的遥感场景。 Text2Earth集成了解决方案指导机制，使用户能够指定图像分辨率。提出了一种动态条件适应策略，用于培训和推断以提高图像质量。 text2earth以零拍的文本图像生成效果出色，并在多个任务中演示了强大的概括和灵活性，包括无限的场景构造，图像编辑和跨模式图像生成。这种强大的功能超过了以前的模型，仅限于基本固定尺寸和有限的场景类型。在以前的基准数据集上，Text2Erth优于先前的模型，其改进为+26.23 FID和 +20.95％零弹药Cls-OA Metric.our Project Pagg as \ url {https://chen-yang-liu.github.io.io/text2earth}

### Multimodal Large Models Are Effective Action Anticipators 
[[arxiv](https://arxiv.org/abs/2501.00795)] [[cool](https://papers.cool/arxiv/2501.00795)] [[pdf](https://arxiv.org/pdf/2501.00795)]
> **Authors**: Binglu Wang,Yao Tian,Shunzhou Wang,Le Yang
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 多模式大型模型是有效的动作预测者
- **领域**: 计算机视觉和模式识别
- **摘要**: 长期行动预期的任务需要解决方案，这些解决方案可以在长时间内有效地对时间动态进行建模，同时深入了解动作的固有语义。传统方法主要依赖于经常性单元或变压器层来捕获长期依赖性，通常在应对这些挑战方面缺乏。大型语言模型（LLM）具有强大的顺序建模功能和广泛的常识性知识，为长期行动预期提供了新的机会。在这项工作中，我们介绍了ActionLlm框架，这是一种新颖的方法，将视频序列视为连续的令牌，并利用LLMS预测未来的动作。我们的基线模型通过设置未来的令牌，结合动作调整模块并将文本解码器层减少到线性层，从而简单地实现了直接的动作预测，而无需复杂说明或冗余说明，我们的基线模型简化了LLM体系结构。为了进一步利用LLM的常识性推理，我们预测观察到的帧的动作类别，并使用顺序的文本线索来指导语义理解。此外，我们引入了一个跨模式相互作用块，旨在探索每种模态内的特异性并捕获视觉和文本方式之间的相互作用，从而增强多模式调整。基准数据集上的广泛实验证明了拟议的ActionLLM框架的优越性，鼓励在行动预期的背景下探索LLM的有希望的方向。代码可在https://github.com/2tianyao1/actionllm.git上找到。

### A Study on Context Length and Efficient Transformers for Biomedical Image Analysis 
[[arxiv](https://arxiv.org/abs/2501.00619)] [[cool](https://papers.cool/arxiv/2501.00619)] [[pdf](https://arxiv.org/pdf/2501.00619)]
> **Authors**: Sarah M. Hooper,Hui Xue
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: Published at ML4H 2024
- **标题**: 一项有关生物医学图像分析的上下文长度和有效变压器的研究
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 生物医学成像方式通常会产生高分辨率的多维图像，从而对深度神经网络构成计算挑战。当由于自我发挥操作员而训练变压器时，这些计算挑战会变得更加复杂，该操作员随上下文的长度二次扩展。尽管缺乏对该主题的系统评估，但长篇文化模型的最新发展有可能减轻这些困难，并使变形金刚在大型生物医学图像中更有效地应用。在这项研究中，我们研究了上下文长度对生物医学图像分析的影响，并评估了最近提出的长篇文化模型的性能。我们首先策划了一套生物医学成像数据集，包括2D和3D数据，用于分割，降解和分类任务。然后，我们使用Vision Transformer和Swin Transformer分析了上下文长度对网络性能的影响，通过改变贴片大小和注意力窗口大小。我们的发现揭示了上下文长度与性能之间的牢固关系，尤其是对于像素级预测任务。最后，我们表明，最近的长篇小说模型在保持可比性的同时表现出显着提高效率，尽管我们强调了差距仍然存在。这项工作强调了在生物医学成像中使用长篇文化模型的潜力和挑战。

### Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method 
[[arxiv](https://arxiv.org/abs/2501.00584)] [[cool](https://papers.cool/arxiv/2501.00584)] [[pdf](https://arxiv.org/pdf/2501.00584)]
> **Authors**: Zhenpeng Huang,Xinhao Li,Jiaqi Li,Jing Wang,Xiangyu Zeng,Cheng Liang,Tao Wu,Xi Chen,Liang Li,Limin Wang
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 在线视频理解：一种全面的基准和记忆扬声器的方法
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 多模式大语模型（MLLM）在离线视频理解中显示出很大的进步。但是，将这些模型应用于实际情况，例如自动驾驶和人类计算机的互动，由于需要进行连续的在线视频流进行实时处理，因此提出了独特的挑战。为此，本文从三个角度提出了系统的努力：评估基准，模型架构和培训策略。首先，我们介绍了OVBench，这是一种全面的提问基准，专门旨在评估模型在在线视频环境中感知，记忆和理性的能力。它在三种时间上下文中具有六种核心任务类型，即来自不同数据集的三个核心 - 现在和未来形成的16个子任务。其次，我们提出了一个新的金字塔记忆库（PMB），该存储库有效地保留了视频流中的关键时空信息。第三，我们提出了一个脱机到线学习范式，设计了用于在线视频数据的交织对话格式，并构建了针对在线视频培训的指令调整数据集。该框架导致了VideoChat-Online的开发，这是一个可用于在线视频理解的强大而有效的模型。尽管计算成本较低和效率较高，但在流行的离线视频基准和OVBENCH上，视频聊天在线的脱机和在线模型的表现优于现有的最新模型，这表明了我们的模型体系结构和培训策略的有效性。

### VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling 
[[arxiv](https://arxiv.org/abs/2501.00574)] [[cool](https://papers.cool/arxiv/2501.00574)] [[pdf](https://arxiv.org/pdf/2501.00574)]
> **Authors**: Xinhao Li,Yi Wang,Jiashuo Yu,Xiangyu Zeng,Yuhan Zhu,Haian Huang,Jianfei Gao,Kunchang Li,Yinan He,Chenting Wang,Yu Qiao,Yali Wang,Limin Wang
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: VideoChat-Flash：长篇小写视频建模的层次结构压缩
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 长篇小说视频建模对于多模式大语言模型（MLLM）至关重要，使它们能够处理电影，在线视频流等等。尽管有进步，但由于难以有效理解非常长的视频环境，因此处理长视频仍然具有挑战性。本文旨在从模型架构，培训数据，培训策略和评估基准的各个方面解决此问题。首先，我们提出了一种新型的分层视频令牌压缩（HICO）方法，该方法利用长视频中的视觉冗余来压缩从剪辑级到视频级别的长视频上下文，从而大大减少了计算，同时保留了基本细节，实现了大约1/50的极端压缩比，几乎没有性能损失。其次，我们介绍了一个多阶段的短期学习计划，一个名为Longvid的真实世界的大规模数据集，以及充满挑战的``多跳针中的video-haystack''基准。最后，我们构建了一个功能强大的视频MLLM，名为VideoChat-Flash，该视频在2B和7B型号的主流长度和短视频基准测试中都表现出领先的性能。在开源型号中，它首先在NIAH的10,000帧中获得99.1％的精度。

### Fine-grained Video-Text Retrieval: A New Benchmark and Method 
[[arxiv](https://arxiv.org/abs/2501.00513)] [[cool](https://papers.cool/arxiv/2501.00513)] [[pdf](https://arxiv.org/pdf/2501.00513)]
> **Authors**: Yifan Xu,Xinhao Li,Yichun Yang,Rui Huang,Limin Wang
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 细粒度视频检索：一种新的基准和方法
- **领域**: 计算机视觉和模式识别,信息检索,机器学习
- **摘要**: 感知细粒度的空间和时间信息的能力对于视频语言检索至关重要。但是，由于缺乏详细的注释，现有的视频检索基准（例如MSRVTT和MSVD）无法有效评估视频模型（VLMS）的细粒度检索能力。为了解决这个问题，我们提出了Fiber，这是用于视频检索的文本的细粒基准，其中包含从填充数据集中采购的1,000个视频。独特的是，我们的光纤基准为每个视频提供了详细的人类通知的空间注释和时间注释，从而可以独立评估VLMS在视频检索任务上的空间和时间偏见。此外，我们采用一种文本嵌入方法来解锁对多模式大语言模型（MLLM）的细粒度视频语言理解的能力。令人惊讶的是，实验结果表明，我们的视频大语言编码器（VLLE）在传统基准上的基于夹的模型相当，并且具有较低时空偏置的细粒度表示能力更强。项目页面：https：//fiber-bench.github.io。

### CNC: Cross-modal Normality Constraint for Unsupervised Multi-class Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2501.00346)] [[cool](https://papers.cool/arxiv/2501.00346)] [[pdf](https://arxiv.org/pdf/2501.00346)]
> **Authors**: Xiaolei Wang,Xiaoyang Wang,Huihui Bai,Eng Gee Lim,Jimin Xiao
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: Accepted by AAAI 2025
- **标题**: CNC：无监督多级异常检测的跨模式正态性约束
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 现有的基于无监督蒸馏的方法依赖于编码和解码特征之间的差异来定位测试图像中的异常区域。但是，仅在普通样本上训练的解码器仍然可以重建异常贴片的特征，从而降低了性能。在无监督的多级异常检测任务中，此问题特别明显。我们将这种行为归因于解码器的过度普通化（OG）：多级训练中斑块模式的多样性可显着增加，从而增强了对正常斑块的模型概括，但也无意间将其对异常斑块的概括扩大。为了减轻OG，我们提出了一种新颖的方法，该方法利用了类不足的可学习提示来捕获各种视觉模式的常见文本正常性，然后应用它们以指导解码的特征朝着正常的文本表示形式，从而抑制了解码器在异常模式上的过度遗传。为了进一步提高性能，我们还引入了一个封闭式的Experts模块，以专门处理多种贴片模式，并在多级训练中减少它们之间的相互干扰。我们的方法在MVTEC AD和VISA数据集上实现了竞争性能，证明了其有效性。

### OVGaussian: Generalizable 3D Gaussian Segmentation with Open Vocabularies 
[[arxiv](https://arxiv.org/abs/2501.00326)] [[cool](https://papers.cool/arxiv/2501.00326)] [[pdf](https://arxiv.org/pdf/2501.00326)]
> **Authors**: Runnan Chen,Xiangyu Sun,Zhaoqing Wang,Youquan Liu,Jiepeng Wang,Lingdong Kong,Jiankang Deng,Mingming Gong,Liang Pan,Wenping Wang,Tongliang Liu
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: ovgaussian：可概括的3D高斯分割，开放词汇
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 使用3D高斯（3DG）表示的开放式摄影场景吸引了人们的关注。但是，现有的方法主要将知识从大型2D视觉模型逐一将3DG提升为3DG，从而限制了在训练场景中开放式摄影查询的功能，因此缺乏对新颖场景的普遍性。在这项工作中，我们提出了\ textbf {ovgaussian}，这是一种基于3D \ textbf {gaussian}的表示。我们首先构建一个基于3DG的大规模3D场景数据集，称为\ textbf {seggaussian}，该数据为高斯点和多视图图像提供了详细的语义和实例注释。为了促进跨场景的语义概括，我们介绍了可推广的语义栅格化（GSR），该语义栅格化（GSR）利用3D神经网络来学习和预测每个3D高斯观点的语义属性，在该点可以将语义属性呈现为多视图一致的2D语义图。在接下来的情况下，我们提出了一个跨模式一致性学习（CCL）框架，该框架利用塞加斯主义者中2D图像和3D高斯的开放式视频记录注释来训练能够跨基于高斯的3D场景的开放式语义语义段的3D神经网络。实验结果表明，Ovgaussian显着胜过基线方法，这些方法表现出强大的跨场，跨域和新型视图概括能力。代码和Seggaussian数据集将发布。 （https://github.com/runnanchen/ovgaussian）。

### OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning 
[[arxiv](https://arxiv.org/abs/2501.00321)] [[cool](https://papers.cool/arxiv/2501.00321)] [[pdf](https://arxiv.org/pdf/2501.00321)]
> **Authors**: Ling Fu,Biao Yang,Zhebin Kuang,Jiajun Song,Yuzhe Li,Linghao Zhu,Qidi Luo,Xinyu Wang,Hao Lu,Mingxin Huang,Zhang Li,Guozhi Tang,Bin Shan,Chunhui Lin,Qi Liu,Binghong Wu,Hao Feng,Hao Liu,Can Huang,Jingqun Tang,Wei Chen,Lianwen Jin,Yuliang Liu,Xiang Bai
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: OCRBENCH V2：改进的基准测试，用于评估视觉文本本地化和推理上的大型多模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近，大型多模型（LMMS）的光学特征识别（OCR）能力得分最近引起了人们的兴趣日益增长的兴趣。现有的基准测试表明，LMM在文本识别中的表现令人印象深刻。但是，它们在某些具有挑战性的任务上的能力，例如文本本地化，手写内容提取和逻辑推理，仍然没有被忽视。要弥合这一差距，我们介绍了Ocrbench V2，这是一种大型双语文本中心的基准标准，目前是最全面的任务集（比以前的多场景基准OCRBENCH多4倍（任务多4倍），这是场景最广泛的覆盖，包括31种不同的场景（包括街头场景，公式，公式，公式，详细信息），以及详尽的范围，包括10，000年级的人，以及彻底的人类，以及彻底的人，以及详细信息，以及一定的人，以及详细信息，以及Onlifiatified，On of of of of of of of of of of of of of of of of of of of of of of of）。提问对和很大比例的困难样本。在仔细地对OCRBENCH V2进行了最新的LMM基准测试之后，我们发现22个LMMS中有20分低于50（总计100），并且受到了五种限制的限制，包括较少遇到的文本识别，细粒度的感知，布局感知，复杂的元素解析，复杂的元素解析和逻辑推理。基准和评估脚本可在https://github.com/yuliang-liu/multimodalocr上找到。

### Dual Diffusion for Unified Image Generation and Understanding 
[[arxiv](https://arxiv.org/abs/2501.00289)] [[cool](https://papers.cool/arxiv/2501.00289)] [[pdf](https://arxiv.org/pdf/2501.00289)]
> **Authors**: Zijie Li,Henry Li,Yichun Shi,Amir Barati Farimani,Yuval Kluger,Linjie Yang,Peng Wang
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 统一图像产生和理解的双重扩散
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 扩散模型在文本到图像生成方面取得了巨大的成功，但仍落后于视觉理解任务，这是一个以自回归视觉模型为主导的领域。我们为多模式理解和产生提出了一个大规模且完全的端到端扩散模型，可显着改善现有基于扩散的多模式模型，并且是支持视觉模型功能的完整套件的第一个此类模型。受到多模式扩散变压器（MM-DIT）和离散扩散语言建模的最新进展的启发，我们利用了一个跨模式的最大似然估计框架，该框架同时训练图像和文本在单个损失功能下共同训练的有条件的可能性，这是通过单个损失功能在两个分支中通过两个扩散变形物的分支进行了反映。最终的模型非常灵活，并且能够执行各种任务，包括图像生成，字幕和视觉问题回答。与最近的统一图像理解和生成模型相比，我们的模型获得了竞争性能，这表明了多模式扩散建模是自动回归下一步预测模型的有希望的替代品。

### MLLM-as-a-Judge for Image Safety without Human Labeling 
[[arxiv](https://arxiv.org/abs/2501.00192)] [[cool](https://papers.cool/arxiv/2501.00192)] [[pdf](https://arxiv.org/pdf/2501.00192)]
> **Authors**: Zhenting Wang,Shuming Hu,Shiyu Zhao,Xiaowen Lin,Felix Juefei-Xu,Zhuowei Li,Ligong Han,Harihar Subramanyam,Li Chen,Jianfa Chen,Nan Jiang,Lingjuan Lyu,Shiqing Ma,Dimitris N. Metaxas,Ankit Jain
> **First submission**: 2024-12-30
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: MLLM-AS-A-A-Gudge用于图像安全，无人标记
- **领域**: 计算机视觉和模式识别,计算语言学,计算机与社会,机器学习
- **摘要**: 随着在线平台上的视觉媒体的兴起，图像内容安全已成为一个重大挑战。同时，在AI生成的内容（AIGC）的时代，许多图像生成模型都能产生有害内容，例如包含性或暴力物质的图像。因此，基于既定的安全规则确定此类不安全的图像至关重要。鉴于其强大的模式识别能力，预训练的多模式大语模型（MLLM）在这方面具有潜力。现有方法通常使用人体标记的数据集微调MLLM，但是带来了一系列缺点。首先，依靠人类注释来按照复杂和详细的准则标记数据既昂贵又富有劳动力。此外，安全判断系统的用户可能需要经常更新安全规则，从而使基于人类的注释进行微调更具挑战性。这提出了一个研究问题：我们可以通过使用预定义的安全构成（一组安全规则）在零弹设置中查询MLLM来检测不安全的图像？我们的研究表明，简单地查询预训练的MLLM不会产生令人满意的结果。缺乏效力源于诸如安全规则的主观性，冗长宪法的复杂性以及模型中固有的偏见等因素。为了应对这些挑战，我们提出了一种基于MLLM的方法，包括对安全规则进行客观化，评估规则和图像之间的相关性，并根据逻辑上完整但简化的安全规则的前提前提链进行快速判断，并对安全规则进行简化的先进链，并进行更深入的链条推理，并使用层压链条的链条链条进行。实验结果表明，我们的方法对于零击图像安全判断任务非常有效。

### EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models 
[[arxiv](https://arxiv.org/abs/2501.02699)] [[cool](https://papers.cool/arxiv/2501.02699)] [[pdf](https://arxiv.org/pdf/2501.02699)]
> **Authors**: Andrés Villa,Juan León Alcázar,Motasem Alfarra,Vladimir Araujo,Alvaro Soto,Bernard Ghanem
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: 12 pages, 4 figures, 8 tables
- **标题**: Eagle：增强的视觉接地可最大程度地减少教学多模型中的幻觉
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型语言模型和视觉变压器已经表现出令人印象深刻的零击功能，从而在下游任务中可转移。这些模型的融合导致了具有增强教学能力的多模式架构。尽管结合了庞大的图像和语言预训练，但这些多模式体系结构通常会产生与图像数据中的地面真相偏离的响应。这些故障案例称为幻觉。缓解幻觉的当前方法通常集中于正规化语言组件，改善融合模块或结合多个视觉编码器以改善视觉表示。在本文中，我们通过直接增强视觉组件的功能来解决幻觉问题。我们的方法名为Eagle，对LLM或Fusion模块完全不可知，它是一种后期方法，可改善视觉编码器的接地和语言对齐。我们表明，对原始对比预训练任务的直接重新重新制定会导致改进的视觉编码器，可以将其纳入教学多模式体系结构，而无需进行其他教学培训。结果，Eagle在多个具有挑战性的基准和任务中的幻觉显着减少。

### Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs? 
[[arxiv](https://arxiv.org/abs/2501.02669)] [[cool](https://papers.cool/arxiv/2501.02669)] [[pdf](https://arxiv.org/pdf/2501.02669)]
> **Authors**: Simon Park,Abhishek Panigrahi,Yun Cheng,Dingli Yu,Anirudh Goyal,Sanjeev Arora
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 从简单的视觉推理到概括：我们可以减轻VLM中的方式失衡吗？
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 虽然视觉语言模型（VLM）在视觉问题回答（VQA）和图像字幕之类的任务中令人印象深刻，但它们将多步推理应用于图像的能力却滞后，从而引起了对模态失衡或脆弱性的认识。为了系统地研究此类问题，我们引入了一个合成框架，用于评估VLMS执行算法视觉推理（AVR）的能力，包括三个任务：表读数，网格导航和视觉类比。每个都有两个难度，简单和硬的级别，甚至简单的版本对于边境VLM来说都是困难的。我们寻求培训对任务的简单版本的培训，以改善相应的艰巨任务的性能，即S2H概括。这个合成框架（每个任务也都有仅文本版本，可以量化模态不平衡，以及如何受到培训策略的影响。消融强调了使用自动回归训练时明确的图像转换转换在促进S2H概括方面的重要性。我们还报告了这种现象的机理研究结果，包括梯度比对的度量，似乎可以识别培训策略，以促进更好的S2H概括。

### Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention Using Graph-Based and Vision Backbone-Extracted Features 
[[arxiv](https://arxiv.org/abs/2501.02649)] [[cool](https://papers.cool/arxiv/2501.02649)] [[pdf](https://arxiv.org/pdf/2501.02649)]
> **Authors**: Haixu Liu,Penghao Jiang,Zerui Tao,Muyan Wan,Qiuzhuang Sun
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: CVPR GeolifeCLEF
- **标题**: Tighnari：基于分层跨注意的多模式植物物种使用基于图和视觉骨干的特征
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在特定时空环境中预测植物物种组成在生物多样性管理和保护以及改善物种识别工具中起着重要作用。我们的工作利用了在欧洲特定时空环境中进行的88,987个植物调查记录。我们还使用相应的卫星图像，时间序列数据，气候时间序列以及其他栅格的环境数据，例如土地覆盖，人足迹，生物气候和土壤变量作为训练数据，以训练模型，以预测4,716种植物调查的结果。我们根据图结构提出了一种特征构造和结果校正方法。通过比较实验，我们选择了表现最佳的骨干网络，以便在时间和图像模态中提取特征。在此过程中，我们建立了一个基于Swin-Transformer块来提取颞立方体功能的骨干网络。然后，我们设计了一种分层的跨注意机制，能够从多种方式中稳健融合特征。在训练过程中，我们采用基于微调的10倍交叉融合方法，并使用阈值TOP-K方法进行后处理。消融实验证明了我们提出的解决方案管道带来的模型性能的改善。

### Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks 
[[arxiv](https://arxiv.org/abs/2501.02527)] [[cool](https://papers.cool/arxiv/2501.02527)] [[pdf](https://arxiv.org/pdf/2501.02527)]
> **Authors**: Leo Franklin,Apiradee Boonmee,Kritsada Wongsuwan
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 视觉驱动的迅速优化多模式生成任务中的大型语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉产生仍然是人工智能中的挑战性边界，需要无缝整合视觉理解和生成能力。在本文中，我们提出了一个新颖的框架，即视觉驱动的及时优化（VDPO），该框架利用大型语言模型（LLMS）动态生成来自视觉输入的文本提示，从而指导高保真图像合成。 VDPO结合了视觉嵌入提示调谐器，文本指令生成器和视觉生成模块，以实现各种视觉生成任务的最新性能。在基准（例如可可和粗略）上进行的广泛实验表明，VDPO始终优于现有方法，从而实现了FID，LPIPS和BLEU/CIDER分数的显着改善。其他分析揭示了VDPO的可扩展性，鲁棒性和泛化功能，使其成为内域和室外任务的多功能解决方案。人类评估进一步验证了VDPO在产生视觉吸引力和语义相干输出方面的实际优势。

### Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2501.02523)] [[cool](https://papers.cool/arxiv/2501.02523)] [[pdf](https://arxiv.org/pdf/2501.02523)]
> **Authors**: Dawei Dai,Mingming Jia,Yinxiu Zhou,Hang Xing,Chenghang Li
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 面部效果：多模式面部提示文本到图像生成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 面部图像具有广泛的实用应用。尽管当前的大规模文本图像扩散模型具有强大的生成能力，但仅使用文本提示即可生成所需的面部图像是挑战的。图像提示是一个合乎逻辑的选择。但是，这种类型的当前方法通常集中在通用域上。在本文中，我们旨在优化图像构成技术以生成所需的面部图像。具体来说，（1）我们建立了一个基于Laion-Face的400万高质量脸部图像对（facecaptionhq-4m）的数据集，以训练我们的脸部制作模型； （2）为了与参考面部图像保持一致性，我们提取/学习面部图像的多尺度内容特征和姿势特征，将它们集成到扩散模型中，以增强扩散模型的面部身份特征的保存。两个与面部相关的测试数据集的验证表明，我们的面部效果可以实现最佳的全面性能。所有代码可在以下位置提供：https：//github.com/ddw2aigroup2cqupt/face-makeup

### Facial Attractiveness Prediction in Live Streaming: A New Benchmark and Multi-modal Method 
[[arxiv](https://arxiv.org/abs/2501.02509)] [[cool](https://papers.cool/arxiv/2501.02509)] [[pdf](https://arxiv.org/pdf/2501.02509)]
> **Authors**: Hui Li,Xiaoyu Ren,Hongjiu Yu,Huiyu Duan,Kai Li,Ying Chen,Libo Wang,Xiongkuo Min,Guangtao Zhai,Xu Liu
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 现场流中的面部吸引力预测：一种新的基准和多模式方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 长期以来，面部吸引力预测（FAP）一直是一项重要的计算机视觉任务，可以广泛应用在直播中进行面部修饰，内容建议等。但是，以前的FAP数据集是小的，封闭的，或者缺乏多样性。此外，相应的FAP模型具有有限的概括和适应能力。为了克服这些局限性，在本文中，我们介绍了LiveBeauty，这是第一个大规模的实时FAP数据集，在更具挑战性的应用程序场景中，即实时流媒体。直接从实时流媒体平台收集了10,000张面部图像，其中200,000个相应的吸引力注释从一个良好的主观实验中获得，使LiveBeauty成为具有挑战性的实时场景中最大的开放访问FAP数据集。此外，提出了一种多模式FAP方法来测量现场流中的面部吸引力。具体而言，我们首先通过个性化吸引力先验模块（PAPM）和多模式的吸引力编码器模块（MAEM）提取整体面部先验知识和多模式美学语义特征，然后通过交叉模式融合模块（CMFM）整合提取的特征。对LiveBeauty和其他开源FAP数据集进行的广泛实验表明，我们提出的方法可以实现最新的性能。数据集将很快可用。

### FedRSClip: Federated Learning for Remote Sensing Scene Classification Using Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.02461)] [[cool](https://papers.cool/arxiv/2501.02461)] [[pdf](https://arxiv.org/pdf/2501.02461)]
> **Authors**: Hui Lin,Chao Zhang,Danfeng Hong,Kexin Dong,Congcong Wen
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: FedRsClip：使用视觉模型的联合学习进行遥感场景分类
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 遥感数据通常分布在多个机构中，并且由于隐私问题和数据共享限制，在集中式培训框架中利用大规模数据集很具有挑战性。联邦学习通过在不需要数据集中介绍分布式数据源的协作模型培训中提供了有希望的解决方案。但是，通常包含数十亿个参数的当前视觉模型（VLMS）基于模型参数更新对传统的联邦学习方法构成了巨大的沟通挑战，因为它们会产生大量的沟通成本。在本文中，我们提出了FedRsClip，这是第一个用于基于VLM的遥感图像分类的联合学习框架，特别是剪辑。 FEDRSCLIP通过引入及时学习来解决联合环境中数据异质性和大规模模型传输的挑战，该及时学习仅优化了一小部分可调参数。该框架引入了双重提示机制，包括共享全球知识共享和私人提示的共享提示。为了保持共享和私人提示之间的语义连贯性，我们提出了双重及时对准约束，以平衡各种客户分布之间的全球一致性和局部适应性。此外，为了增强跨模式表示学习，我们将跨模式特征对齐约束介绍给文本和图像提示之间的多模式特征。为了验证我们提出的模型的有效性，我们基于三个现有的遥感图像分类数据集构建了一个FED-RSIC数据集，该数据集专门为模拟各种联合学习配置而设计。实验结果证明了FedRsClip在遥感图像分类中的有效性和优势。

### FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance 
[[arxiv](https://arxiv.org/abs/2501.02430)] [[cool](https://papers.cool/arxiv/2501.02430)] [[pdf](https://arxiv.org/pdf/2501.02430)]
> **Authors**: Haicheng Wang,Zhemeng Yu,Gabriele Spadaro,Chen Ju,Victor Quétu,Enzo Tartaglione
> **First submission**: 2025-01-04
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 文件夹：加速具有增强性能的多模式大语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，多模式大型语言模型（MLLM）由于能力生成和理解跨模式数据，因此对多模式任务显示出了显着的有效性。但是，处理从视觉主链中提取的长序列的视觉令牌序列对实时应用程序中的部署构成了挑战。为了解决此问题，我们介绍了一个简单而有效的插件模块，旨在减少视觉令牌序列的长度，从而减轻训练和推理期间的计算和内存需求。通过对令牌减少过程的全面分析，我们分析了不同还原策略引入的信息损失，并开发文件夹以保留关键信息，同时删除视觉冗余。我们通过将其集成到几个MLLM的视觉主链中来展示其有效性，从而显着加速了推理阶段。此外，我们评估了其作为培训加速器甚至MLLM的性能促进效应的实用性。在这两种情况下，文件夹都比原始模型实现了可比甚至更好的性能，同时通过删除多达70％的视觉令牌来大大降低复杂性。

### MetaNeRV: Meta Neural Representations for Videos with Spatial-Temporal Guidance 
[[arxiv](https://arxiv.org/abs/2501.02427)] [[cool](https://papers.cool/arxiv/2501.02427)] [[pdf](https://arxiv.org/pdf/2501.02427)]
> **Authors**: Jialong Guo,Ke liu,Jiangchao Yao,Zhihua Wang,Jiajun Bu,Haishuai Wang
> **First submission**: 2025-01-04
> **First announcement**: 2025-01-06
> **comment**: Accepted by AAAI2025
- **标题**: Metanerv：具有时空指导的视频的元神经表示
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频（神经）的神经表示已成为视频分析的有希望的隐式神经表示方法（INR）方法，该方法将视频表示为具有框架索引作为输入的神经网络。但是，在适应大量不同的视频时，基于神经的方法正在耗时，因为每个视频都需要从头开始训练单独的神经模型。此外，基于神经的方法在空间上需要从低维时间戳的输入中生成高维信号（即整个图像），并且视频通常由数十帧的时间组成，这些框架在相邻帧之间具有较小的变化。为了提高视频表示的效率，我们提出了视频的元神经表示，名为Metanerv，这是一个新颖的框架，用于快速神经表示看不见的视频。 Metanerv利用元学习框架来学习最佳参数初始化，这是适应新视频的好起点。为了解决视频模式的独特空间和时间特征，我们进一步介绍了时空指导，以提高metanerv的表示能力。具体而言，具有多分辨率损失的空间指导旨在从不同的分辨率阶段捕获信息，并且具有有效渐进式学习策略的时间指导可以逐渐完善元学习过程中拟合框架的数量。在多个数据集上进行的广泛实验证明了Metanerv对视频表示和视频压缩的优越性。

### Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud Embedding 
[[arxiv](https://arxiv.org/abs/2501.02285)] [[cool](https://papers.cool/arxiv/2501.02285)] [[pdf](https://arxiv.org/pdf/2501.02285)]
> **Authors**: Yingjie Liu,Pengyu Zhang,Ziyao He,Mingsong Chen,Xuan Tang,Xian Wei
> **First submission**: 2025-01-04
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 分层3D点云嵌入的双曲线对比度学习
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 双曲线空间允许对复杂的分层结构进行更有效的建模，这在涉及多模式数据的任务中尤其有益。尽管双曲线几何形状已被证明对语言图像预训练有效，但它们统一语言，图像和3D点云模式的能力却没有探索。我们将3D点云模态扩展在双曲线多模式对比预训练中。此外，我们还探讨了学习层次结构3D嵌入的必要条件，模态差距和对齐正规，并促进了从文本和图像方式中转移知识的转移。这些正规化器可以在每种模式和模式间层次结构中学习跨文本，2D图像和3D点云的模式内层次结构。实验结果表明，我们提出的训练策略产生了出色的3D点云编码器，而获得的3D点云层次嵌入可显着提高各种下游任务的性能。

### What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph 
[[arxiv](https://arxiv.org/abs/2501.02268)] [[cool](https://papers.cool/arxiv/2501.02268)] [[pdf](https://arxiv.org/pdf/2501.02268)]
> **Authors**: Yutao Jiang,Qiong Wu,Wenhao Lin,Wei Yu,Yiyi Zhou
> **First submission**: 2025-01-04
> **First announcement**: 2025-01-06
> **comment**: 9 pages, 6 figures
- **标题**: 我们需要什么样的视觉令牌？从图的角度来看
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近的多模式大型语言模型（MLLM）经常使用大量的视觉令牌来补偿其视觉缺陷，从而导致过度的计算和明显的视觉冗余。在本文中，我们研究了MLLM需要哪种视觉令牌，并揭示了前景和背景令牌对于MLLM至关重要的，鉴于示例的各种困难。基于此观察，我们提出了一种基于图的方法，用于训练无视觉令牌修剪，尤其称为g-prune，g-prune将视觉令牌视为节点，并根据其语义相似性构建其连接。之后，通过加权链接传播信息流，并且在迭代迭代后保留最重要的令牌，这可以是前面或背景。要验证G-Prune，我们将其应用于最近的MLLM，称为LLA​​VA-NEXT，并在一组基准的实验上进行了广泛的实验。在一组实验上，Grune可以很好地降低计算量的高度验证，并降低了计算的高高效果，并均可衡量高高的效果。例如，g-prune可以分别减少VQA2.0上的Llava-Next的63.57 \％触发器和仅为0.95 \％和2.34 \％精度下降的TextVQA。

### Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey 
[[arxiv](https://arxiv.org/abs/2501.02189)] [[cool](https://papers.cool/arxiv/2501.02189)] [[pdf](https://arxiv.org/pdf/2501.02189)]
> **Authors**: Zongxia Li,Xiyang Wu,Hongyang Du,Huy Nghiem,Guangyao Shi
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: 35 pages, 3 figures
- **标题**: 大型视觉语言模型的基准评估，应用和挑战：调查
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,机器人技术
- **摘要**: 在计算机视觉和自然语言处理的交集中，多模式视觉语言模型（VLM）已成为一种变革性技术，使机器能够通过视觉和文本方式来感知和理性。例如，诸如剪辑，克劳德（Claude）和GPT-4V之类的模型在视觉和文本数据上表现出强烈的推理和理解能力，并在零摄像分类上击败了经典的单一模态视觉模型。尽管它们在研究方面迅速发展和在应用方面日益普及，但对现有的VLMS研究的全面调查显然是缺乏的，特别是对于旨在利用VLM在特定领域中的研究人员而言。为此，我们在以下方面提供了VLM的系统概述：过去五年中开发的主要VLM的模型信息（2019-2024）；这些VLM的主要体系结构和培训方法； VLMS的流行基准和评估指标的摘要和分类； VLM的应用，包括具体的代理，机器人技术和视频生成；幻觉，公平和安全等当前VLM所面临的挑战和问题。 https://github.com/zli12321/awsome-vlm-papers-and-models.git中列出了包括论文和模型存储库在内的详细集合。

### Generating Multimodal Images with GAN: Integrating Text, Image, and Style 
[[arxiv](https://arxiv.org/abs/2501.02167)] [[cool](https://papers.cool/arxiv/2501.02167)] [[pdf](https://arxiv.org/pdf/2501.02167)]
> **Authors**: Chaoyi Tan,Wenqing Zhang,Zhen Qi,Kowei Shih,Xinshi Li,Ao Xiang
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 用gan生成多模式图像：整合文本，图像和样式
- **领域**: 计算机视觉和模式识别
- **摘要**: 在计算机视觉领域，多模式图像生成已成为研究热点，尤其是整合文本，图像和样式的任务。在这项研究中，我们提出了一种基于生成对抗网络（GAN）的多模式图像生成方法，能够有效地结合文本描述，参考图像和样式信息，以生成满足多模式要求的图像。此方法涉及文本编码器的设计，图像特征提取器和样式集成模块，以确保生成的图像在视觉内容和样式一致性方面保持高质量。我们还引入了多个损失功能，包括对抗性损失，文本图像一致性损失和样式匹配损失，以优化生成过程。实验结果表明，我们的方法在多个公共数据集中产生具有很高清晰度和一致性的图像，与现有方法相比表明性能的显着改善。这项研究的结果为多模式图像产生提供了新的见解，并提供了广泛的应用前景。

### AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs 
[[arxiv](https://arxiv.org/abs/2501.02135)] [[cool](https://papers.cool/arxiv/2501.02135)] [[pdf](https://arxiv.org/pdf/2501.02135)]
> **Authors**: Sanjoy Chowdhury,Sayan Nag,Subhrajyoti Dasgupta,Yaoting Wang,Mohamed Elhoseiny,Ruohan Gao,Dinesh Manocha
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: AvTrustBench：评估和增强视听LLMS的可靠性和鲁棒性
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着多模式大语言模型（MLLM）的快速发展，最近已经开发了几种诊断基准，以评估这些模型的多模式推理水平。但是，这些基准仅限于评估视觉方面，并且不检查整体视听（AV）的理解。此外，目前，没有基准测试能够调查Avllms在出现扰动输入时校准其响应的功能。为此，我们介绍了视听可信度评估基准（Avtrustbench），其中包括600k样品，这些样本涵盖了9个精心制作的任务，评估了三个不同的维度：对抗性攻击，组成推理和模态特定于特定于特定的依赖性。使用我们的基准测试，我们广泛评估了13个最先进的Avllms。研究结果表明，大多数现有模型大大落后于实现人类的理解，为未来的研究方向提供了宝贵的见解。为了减轻现有方法中的局限性，我们进一步提出了一项强大的，模型的校准校准的视听偏好优化优化培训策略CAVPREF，在所有9个任务中获得了高达30.19％的增益。我们将公开发布我们的代码和基准，以促进这一方向的未来研究。

### ArtCrafter: Text-Image Aligning Style Transfer via Embedding Reframing 
[[arxiv](https://arxiv.org/abs/2501.02064)] [[cool](https://papers.cool/arxiv/2501.02064)] [[pdf](https://arxiv.org/pdf/2501.02064)]
> **Authors**: Nisha Huang,Kaer Huang,Yifan Pu,Jiangshan Wang,Jie Guo,Yiqiang Yan,Xiu Li
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: Artcrafter：通过嵌入重新构图的文本图像对准样式转移
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 近年来，文本引导风格转​​移的取得了重大进步，主要归因于扩散模型中的创新。这些模型在条件指导中表现出色，利用文本或图像指导采样过程。然而，尽管具有直接有条件的指导方法，但在平衡文本语义的表现力与输出结果的多样性时，在捕获风格特征的同时，经常面临挑战。为了应对这些挑战，我们介绍了Artcrafter，这是文本对图像风格转移的新型框架。具体来说，我们引入了一个基于注意力的样式提取模块，精心设计，以捕获图像中微妙的风格元素。该模块具有多层体系结构，该体系结构利用感知者注意机制的功能来集成细粒度的信息。此外，我们提出了一种新颖的文本图像对齐增强组件，该组件可以很好地平衡对这两种模式的控制，从而使模型能够有效地将图像和文本嵌入到共享特征空间中。我们通过注意力操作实现了这一目标，从而使模式之间的平滑信息流动。最后，我们结合了一个明确的调制，该调制通过嵌入重新设计设计将多模式增强的嵌入与原始嵌入方式无缝混合在一起，从而增强了模型以生成各种输出。广泛的实验表明，Artcrafter在视觉风格化方面产生了令人印象深刻的结果，表现出特殊水平的风格强度，可控性和多样性。

### CRRG-CLIP: Automatic Generation of Chest Radiology Reports and Classification of Chest Radiographs 
[[arxiv](https://arxiv.org/abs/2501.01989)] [[cool](https://papers.cool/arxiv/2501.01989)] [[pdf](https://arxiv.org/pdf/2501.01989)]
> **Authors**: Jianfei Xu,Thanet Markchom,Huizhi Liang
> **First submission**: 2024-12-30
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: CRRG-CLIP：自动生成胸部放射学报告和胸部X光片分类
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 堆叠成像的复杂性和大量的X光片使写作放射学报告变得复杂且效率低下。即使是经验丰富的放射科医生，也很难在长时间的高强度工作下解释放射线照片时保持准确性和一致性。为了解决这些问题，这项工作提出了CRRG-CLIP模型（胸部放射学报告生成和X光片分类模型），这是一种自动报告生成和X光片分类的端到端模型。该模型由两个模块组成：放射学报告生成模块和X光片分类模块。该生成模块使用更快的R-CNN来识别射线照片中的解剖区域，选择关键区域的二进制分类器和GPT-2来生成语义相干报告。分类模块使用无监督的对比语言图像预处理（剪辑）模型，以应对高成本标记的数据集的挑战和功能不足。结果表明，生成模块的性能与BLEU，流星和Rouge-L指标上的高性能基线模型相当，并且在BLEU-2，BLEU-3，BLEU-4和ROUGE-L MEMICS上的GPT-4O模型的表现优于GPT-4O模型。分类模块在AUC和准确性中显着超过了最新模型。这表明，所提出的模型可以在报告生成中实现高精度，可读性和流利度，而使用未标记的X光片报告对的多模式对比度训练可以增强分类性能。

### INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models 
[[arxiv](https://arxiv.org/abs/2501.01973)] [[cool](https://papers.cool/arxiv/2501.01973)] [[pdf](https://arxiv.org/pdf/2501.01973)]
> **Authors**: Di Jin,Xing Liu,Yu Liu,Jia Qing Yap,Andrea Wong,Adriana Crespo,Qi Lin,Zhiyuan Yin,Qiang Yan,Ryan Ye
> **First submission**: 2024-12-27
> **First announcement**: 2025-01-06
> **comment**: Di Jin and Xing Liu contributed equally to this work
- **标题**: Infelm：大型文本对图像模型的深入公平评估
- **领域**: 计算机视觉和模式识别,人工智能,计算机与社会
- **摘要**: 大语言模型（LLM）和大型视觉模型（LVM）的快速发展推动了多模式AI系统的演变，这些系统通过模拟类似人类的认知来证明了工业应用的显着潜力。但是，它们还构成了重大的道德挑战，包括扩大有害内容和加强社会偏见。例如，一些工业图像生成模型中的偏见强调了迫切需要进行健壮的公平评估。大多数现有的评估框架都集中在模型各个方面的全面性上，但它们表现出关键的局限性，包括对内容产生一致性和社会偏见敏感领域的关注不足。更重要的是，它们对像素检测技术的依赖很容易出现。为了解决这些问题，本文介绍了Infelm，这是对广泛使用的文本模型的深入公平评估。我们的关键贡献是：（1）一种先进的肤色分类器，结合了面部拓扑和精致的皮肤像素表示，以提高至少16.04％的分类精度，（（2）一种偏见敏感的内容一致性测量测量，以理解社会影响，（3）对六个概述的偏见偏见的偏见，多种多样的偏见跨度跨度的偏见 - 分析范围（4）分析性的实验 -  4）分析的实验 -  4）社会偏见敏感领域。我们发现研究中的现有模型通常不符合经验公平标准，并且代表偏见通常比对齐错误更为明显。 Infelm建立了公平评估的强大基准，支持与道德和以人为本原则保持一致的多模式AI系统的发展。

### GAF-FusionNet: Multimodal ECG Analysis via Gramian Angular Fields and Split Attention 
[[arxiv](https://arxiv.org/abs/2501.01960)] [[cool](https://papers.cool/arxiv/2501.01960)] [[pdf](https://arxiv.org/pdf/2501.01960)]
> **Authors**: Jiahao Qin,Feng Liu
> **First submission**: 2024-12-07
> **First announcement**: 2025-01-06
> **comment**: 14 pages, 1 figure, accepted by ICONIP 2024
- **标题**: GAF融合网络：通过Gramian Angular场进行多模式ECG分析，并分散注意力
- **领域**: 计算机视觉和模式识别,人工智能,图形,机器学习
- **摘要**: 心电图（ECG）分析在诊断心血管疾病中起着至关重要的作用，但是对这些复杂信号的准确解释仍然具有挑战性。本文介绍了用于ECG分类的新型多模式框架（GAF-FUSHNET），该框架将时间序列分析与使用Gramian Angular Fields（GAF）的基于图像的表示相结合。我们的方法采用双层跨渠道分开注意模块来适应性融合时间和空间特征，从而使互补信息的细微整合。我们在三个不同的ECG数据集上评估了GAF融合网络：ECG200，ECG5000和MIT-BIH心律失常数据库。结果表明，与最新方法相比，我们的模型达到94.5 \％，96.9 \％和99.6 \％的准确性。我们的代码很快将在https://github.com/cross-innovation-lab/gaf-fusionnet.git上找到。

### VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction 
[[arxiv](https://arxiv.org/abs/2501.01957)] [[cool](https://papers.cool/arxiv/2501.01957)] [[pdf](https://arxiv.org/pdf/2501.01957)]
> **Authors**: Chaoyou Fu,Haojia Lin,Xiong Wang,Yi-Fan Zhang,Yunhang Shen,Xiaoyu Liu,Haoyu Cao,Zuwei Long,Heting Gao,Ke Li,Long Ma,Xiawu Zheng,Rongrong Ji,Xing Sun,Caifeng Shan,Ran He
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: https://github.com/VITA-MLLM/VITA (2K+ Stars by now)
- **标题**: VITA-1.5：迈向GPT-4O级实时视觉和语音互动
- **领域**: 计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 最近的多模式大型语言模型（MLLM）通常专注于整合视觉和文本方式，而较少的重点是语音在增强互动中的作用。但是，语音在多模式对话系统中起着至关重要的作用，并且由于基本方式差异，在视觉和语音任务中实施高性能仍然是一个重大挑战。在本文中，我们提出了一种经过精心设计的多阶段训练方法，该方法逐步训练LLM以了解视觉和语音信息，最终使能够流利的视力和语音相互作用。我们的方法不仅可以保留强大的视力语言能力，而且还可以在没有单独的ASR和TTS模块的情况下实现有效的语音到语音对话能力，从而显着加速了多模式的端到端响应速度。通过将我们的方法与跨基准的最先进的方法进行比较，以进行图像，视频和语音任务，我们证明了我们的模型配备了强大的视觉和语音功能，从而实现了几乎实时的视觉和语音交互。

### Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding 
[[arxiv](https://arxiv.org/abs/2501.01926)] [[cool](https://papers.cool/arxiv/2501.01926)] [[pdf](https://arxiv.org/pdf/2501.01926)]
> **Authors**: Jiaming Li,Jiacheng Zhang,Zequn Jie,Lin Ma,Guanbin Li
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 通过模式间相关校准解码来缓解大视力语言模型的幻觉
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉模型（LVLMS）在视觉语言理解中显示出了显着的能力，用于下游多态任务。尽管他们成功了，但LVLM仍然因复杂的生成任务中的幻觉而遭受损失，从而导致视觉输入和生成内容之间的不一致。为了解决这个问题，一些方法引入了推理时间干预措施，例如对比度解码和注意力纠正，以减少对语言先验的过度依赖。但是，这些方法忽略了幻觉，这是由于虚假的模式相关性。在本文中，我们提出了一种模式间相关校准解码（IMCCD）方法，以无训练的方式减轻LVLM的幻觉。在这种方法中，我们设计了一个跨模式值增强解码（CMVED）模块，以通过一种新型的对比解码机制来减轻幻觉。在估算扭曲分布的估计中，cmask掩盖了与显着的跨模式注意权重相关的价值向量，该值既解决单一模式过高依赖性和误导模式间相关性。此外，内容驱动的注意力完善（CDAR）模块会完善跨模式的注意力重量，并指导LVLM，以专注于重要的视觉内容。各种幻觉基准的实验结果验证了我们方法比现有的最新技术的优越性，以减少LVLM文本生成的幻觉。我们的代码将在https://github.com/lijm48/imccd上找到。

### Virgo: A Preliminary Exploration on Reproducing o1-like MLLM 
[[arxiv](https://arxiv.org/abs/2501.01904)] [[cool](https://papers.cool/arxiv/2501.01904)] [[pdf](https://arxiv.org/pdf/2501.01904)]
> **Authors**: Yifan Du,Zikang Liu,Yifan Li,Wayne Xin Zhao,Yuqi Huo,Bingning Wang,Weipeng Chen,Zheng Liu,Zhongyuan Wang,Ji-Rong Wen
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: Technical Report on Slow Thinking with LLMs: Visual Reasoning
- **标题**: 处女座：关于再现O1样MLLM的初步探索
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近，建立在大型语言模型（LLM）基于大型语言模型（LLM）的缓慢思考的推理系统通过扩展推断期间的思维时间而引起了广泛的关注。将这种能力适应多模式大语言模型（MLLM）也越来越兴趣。鉴于MLLM在不同方式上处理更复杂的数据语义，因此实施多模式慢思维系统在直觉上更具挑战性。为了解决这个问题，在本文中，我们通过对具有少量文本长形式思想数据的有能力的MLLM进行微调来探讨一种直接的方法，从而产生了多模式的缓慢思考系统，即处女座（长时间思考的视觉推理）。我们发现，以自然语言表达的这些长形式的推理过程可以有效地转移到MLLM。此外，这种文本推理数据似乎比视觉推理数据更有效地引起了MLLM的缓慢思考能力。尽管这项工作是初步的，但它表明，缓慢思考的能力与语言模型组件的根本上相关联，该语言模型组件可以跨模态或域转移。可以利用这一发现来指导更强大的缓慢思考的推理系统的发展。我们在https://github.com/rucaibox/virgo上发布资源。

### MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation 
[[arxiv](https://arxiv.org/abs/2501.01808)] [[cool](https://papers.cool/arxiv/2501.01808)] [[pdf](https://arxiv.org/pdf/2501.01808)]
> **Authors**: Huaize Liu,Wenzhang Sun,Donglin Di,Shibo Sun,Jiahui Yang,Changqing Zou,Hujun Bao
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: Moee：情感专家的音频驱动肖像动画的混合
- **领域**: 计算机视觉和模式识别
- **摘要**: 会说话的化身的产生在精确的音频同步方面取得了重大进步。但是，制作栩栩如生的会说话的视频需要捕捉各种各样的情绪和微妙的面部表情。当前的方法面临着根本挑战：a）缺乏建模单个基本情感表达的框架，这限制了复杂情绪等复杂情绪的产生； b）缺乏富含人类情感表达的全面数据集，这限制了模型的潜力。为了应对这些挑战，我们提出了以下创新：1）情感专家（MOEE）模型的混合物，该模型将六种基本情感脱离，以使奇异和复杂情绪状态的精确综合； 2）DH-Faceemovid-150数据集，专门策划，包括六种普遍的人类情感表达以及四种类型的复合情绪，从而扩大了情绪驱动模型的训练潜力。此外，为了增强情绪控制的灵活性，我们提出了一个情感对态度模块，该模块利用多模式输入，使多样化的控制信号像音频，文本和标签一样确保更多多样化的控制输入以及单独使用音频控制情绪的能力。通过广泛的定量和定性评估，我们证明了MOEE框架与DH-Faceemovid-150数据集结合使用，在产生复杂的情感表达和细微差别的面部细节方面表现出色，在该领域设定了新的基准。这些数据集将公开发布。

### LogicAD: Explainable Anomaly Detection via VLM-based Text Feature Extraction 
[[arxiv](https://arxiv.org/abs/2501.01767)] [[cool](https://papers.cool/arxiv/2501.01767)] [[pdf](https://arxiv.org/pdf/2501.01767)]
> **Authors**: Er Jin,Qihui Feng,Yongli Mou,Stefan Decker,Gerhard Lakemeyer,Oliver Simons,Johannes Stegmaier
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: Accepted for publication at aaai25, project page: https://jasonjin34.github.io/logicad.github.io/
- **标题**: LOGICAD：可解释的通过基于VLM的文本功能提取的可解释异常检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 逻辑图像理解涉及解释和推理图像视觉内容中的关系和一致性。这种功能对于诸如工业检查等应用至关重要，在工业检查中，逻辑异常检测对于维持高质量标准和最小化昂贵的召回至关重要。先前在异常检测方面的研究（AD）依赖于设计算法的先验知识，该算法通常需要大量的手动注释，重要的计算能力和大量培训数据。自回归的多模式视觉语言模型（AVLM）提供了一种有希望的替代方法，因为它们在各个领域的视觉推理方面表现出色。尽管如此，它们在逻辑广告中的应用仍未开发。在这项工作中，我们使用AVLM进行逻辑广告进行了调查，并证明它们非常适合任务。将AVLM与格式嵌入和逻辑推理器相结合，我们在公共基准，MVTEC Loco AD上实现了SOTA性能，AUROC为86.0％，F1-MAX为83.7％，以及对异常的解释。这大大超过了现有的SOTA方法的大幅度。

### Multi-modal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds 
[[arxiv](https://arxiv.org/abs/2501.01728)] [[cool](https://papers.cool/arxiv/2501.01728)] [[pdf](https://arxiv.org/pdf/2501.01728)]
> **Authors**: Simon B. Jensen,Stefan Oehmcke,Andreas Møgelmose,Meysam Madadi,Christian Igel,Sergio Escalera,Thomas B. Moeslund
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 从二维正射击和3D机载激光扫描点云的森林生物多样性潜力的多模式分类
- **领域**: 计算机视觉和模式识别
- **摘要**: 森林生物多样性的准确评估对于生态系统管理和保护至关重要。尽管传统的现场调查提供了高质量的评估，但它们是劳动密集型且在空间上有限的。这项研究研究了来自2D正射击（12.5 cm分辨率）和3D空降激光扫描（ALS）点云（8分/M^2）的近距离感应数据的深度融合是否可以增强生物多样性评估。我们介绍了Biovista数据集，其中包括44.378个来自丹麦温带森林的正吞和ALS点云的配对样品，旨在探索用于生物多样性潜在分类的多模式融合方法。使用深神网络（用于ALS点云的正射原和点向量的重新连接），我们研究了每个数据模式评估森林生物多样性潜力的能力，分别达到平均精度为69.4％和72.8％。我们探讨了两种融合方法：一种基于置信的集合方法和特征级的串联策略，后者的平均准确性为75.5％。我们的结果表明，来自正吞原的光谱信息以及来自ALS点云的结构信息在森林生物多样性评估中有效地相互补充。

### Interpretable Face Anti-Spoofing: Enhancing Generalization with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.01720)] [[cool](https://papers.cool/arxiv/2501.01720)] [[pdf](https://arxiv.org/pdf/2501.01720)]
> **Authors**: Guosheng Zhang,Keyao Wang,Haixiao Yue,Ajian Liu,Gang Zhang,Kun Yao,Errui Ding,Jingdong Wang
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: Accepted to AAAI2025(Oral)
- **标题**: 可解释的面部反欺骗：通过多模式大语模型增强概括
- **领域**: 计算机视觉和模式识别
- **摘要**: 面部反欺骗（FAS）对于确保面部识别系统的安全性和可靠性至关重要。大多数现有的FAS方法都是将二进制分类任务提出的，可以提供置信度得分而无需解释。它们在跨域情景中表现出有限的概括，例如新环境或看不见的欺骗类型。在这项工作中，我们引入了用于FAS的多模式大语言模型（MLLM）框架，称为可解释的面部反欺骗（I-FAS），该框架将FAS任务转换为可解释的视觉问题答案（VQA）范式。具体来说，我们提出了一个欺骗意识的字幕和过滤（SCF）策略，以生成FAS图像的高质量字幕，并通过自然语言解释丰富了模型的监督。为了减轻训练期间嘈杂的字幕的影响，我们开发了一个偏斜的语言模型（L-LM）损失函数，该函数将损失计算分开以进行判断和解释，并优先考虑对前者的优化。此外，为了增强模型对全球视觉特征的看法，我们设计了一个全球意识的连接器（GAC），以使多层次的视觉表示与语言模型保持一致。关于标准和新设计的一个跨域基准测试的广泛实验，包括12个公共数据集，表明我们的方法显着优于最先进的方法。

### Aesthetic Matters in Music Perception for Image Stylization: A Emotion-driven Music-to-Visual Manipulation 
[[arxiv](https://arxiv.org/abs/2501.01700)] [[cool](https://papers.cool/arxiv/2501.01700)] [[pdf](https://arxiv.org/pdf/2501.01700)]
> **Authors**: Junjie Xu,Xingjiao Wu,Tanren Yao,Zihao Zhang,Jiayang Bei,Wu Wen,Liang He
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 音乐感知的审美事物图像风格化：情感驱动的音乐与视觉操作
- **领域**: 计算机视觉和模式识别
- **摘要**: 情感信息对于增强人类计算机的互动和加深图像理解至关重要。但是，尽管深度学习具有先进的图像识别，但图像中对情绪表达的直观理解和精确控制仍然具有挑战性。同样，音乐研究主要集中在理论方面，对其情感维度及其与视觉艺术的融合的探索有限。为了解决这些差距，我们介绍了EMOMV，这是一种情感驱动的音乐操纵方法，可根据音乐情感操纵图像。 EMOMV结合了音乐元素的自下而上处理，例如音调和节奏，这些情绪在颜色和照明等视觉方面进行了自上而下的应用。我们使用包括图像质量指标，美学评估和脑电图测量的多尺度框架评估EMOMV，以捕获实时的情感响应。我们的结果表明，EMOMV有效地将音乐的情感内容转化为具有视觉吸引人的图像，推进多模式的情感整合，并为创意产业和交互式技术开辟了新的途径。

### Robust Self-Paced Hashing for Cross-Modal Retrieval with Noisy Labels 
[[arxiv](https://arxiv.org/abs/2501.01699)] [[cool](https://papers.cool/arxiv/2501.01699)] [[pdf](https://arxiv.org/pdf/2501.01699)]
> **Authors**: Ruitao Pu,Yuan Sun,Yang Qin,Zhenwen Ren,Xiaomin Song,Huiming Zheng,Dezhong Peng
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: 9 pages, AAAI 25 conference
- **标题**: 与嘈杂标签的跨模式检索的稳健自节奏的散列
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 跨模式哈希（CMH）由于其低储存成本和大规模数据中的高计算效率，因此已成为跨模式检索的流行技术。大多数现有方法隐含地假设多模式数据已正确标记，这是由于现实世界中的不可避免的不完善的注释（即嘈杂的标签），这是昂贵甚至无法实现的。受到人类认知学习的启发，一些方法引入了自定进度学习（SPL），以逐步训练模型从易于到硬样品，这些样本通常用于减轻特征噪声或异常值的影响。如何利用SPL来减轻哈希模型上嘈杂的标签的误导是一个不太接触的问题。为了解决这个问题，我们提出了一种新的认知跨模式检索方法，称为稳健的自定位散列标签（rshnl），可以模仿人类的认知过程，以识别噪音，同时拥抱对嘈杂标签的稳健性。具体而言，我们首先提出了一种对比的哈希学习（CHL）方案，以提高多模式的一致性，从而减少固有的语义差距。之后，我们建议中心聚集学习（CAL）来减轻类内的变化。最后，我们提出了容忍自进度的哈希（NSH），该哈希（NSH）动态估计每个实例的学习难度，并通过难度级别区分嘈杂的标签。对于所有估计的干净对，我们进一步采用自定进度的正规器来逐渐从易于努力到硬质。广泛的实验表明，所提出的RSHNL在最新的CMH方法上表现出色。

### Dual Mutual Learning Network with Global-local Awareness for RGB-D Salient Object Detection 
[[arxiv](https://arxiv.org/abs/2501.01648)] [[cool](https://papers.cool/arxiv/2501.01648)] [[pdf](https://arxiv.org/pdf/2501.01648)]
> **Authors**: Kang Yi,Haoran Tang,Yumeng Li,Jing Xu,Jun Zhang
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 双重共同学习网络具有RGB-D显着对象检测的全球本地意识
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: RGB-D显着对象检测（SOD）旨在通过共同建模RGB和深度信息来突出给定场景的突出区域，是具有挑战性的像素级预测任务之一。最近，双重注意机制由于能够加强检测过程而投入到该区域。但是，大多数现有方法在手动融合范式下直接融合注意力跨模式特征，而无需考虑RGB和深度之间的固有差异，这可能会导致性能的降低。此外，从全球和本地信息得出的远程依赖性使得很难利用统一的有效融合策略。因此，在本文中，我们提出了GL-DMNET，这是一个具有全球本地意识的新型双重学习网络。具体而言，我们提出了一个位置相互融合模块和一个通道互融合模块，以利用空间和通道维度不同模态之间的相互依赖性。此外，我们采用基于级联变压器注入的重建的有效解码器，共同集成多级融合功能。在六个基准数据集上进行的广泛实验表明，我们提出的GL-DMNET的性能优于24种RGB-D SOD方法，与第二好的模型（S3NET）相比，在四个评估指标中的平均提高约为3％。代码和结果可在https://github.com/kingkung2016/gl-dmnet上找到。

### HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.01645)] [[cool](https://papers.cool/arxiv/2501.01645)] [[pdf](https://arxiv.org/pdf/2501.01645)]
> **Authors**: Heqing Zou,Tianze Luo,Guiyang Xie,Victor,Zhang,Fengmao Lv,Guangcong Wang,Junyang Chen,Zhuochen Wang,Hansheng Zhang,Huaijian Zhang
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: HLV-1K：大规模的一个小时的视频基准测试，用于特定时间的长期视频理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 由于许多有希望的现实世界应用，多模式的大型语言模型已成为深层视觉理解的流行主题。然而，长达一个小时的视频理解超过一小时，包含数以万计的视觉框架，由于1）挑战性长期视频分析，2）效率低下的大型模型方法和3）缺乏大型基准数据集。其中，在本文中，我们专注于构建一个长时间长的视频基准HLV-1K，旨在评估长期的视频理解模型。 HLV-1K包含1009个小时的视频，其中包含14,847个高质量的答案（QA）和多项选择问题，并带有时间感知的查询和多样化的注释，涵盖了框架级别，涵盖了框架级别，内部级别，事实级别，事实级别，跨越级别，跨越级别和长期的理性任务。我们使用现有的最新方法来评估我们的基准测试，并证明其在测试不同级别和各种任务的深层视频理解能力方面的价值。这包括促进未来长期的视频理解任务，例如对长时间现场视频，会议录音和电影的深刻理解。

### SceneBooth: Diffusion-based Framework for Subject-preserved Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2501.03490)] [[cool](https://papers.cool/arxiv/2501.03490)] [[pdf](https://arxiv.org/pdf/2501.03490)]
> **Authors**: Shang Chai,Zihang Lin,Min Zhou,Xubin Li,Liansheng Zhuang,Houqiang Li
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: Scenebooth：主题保存的文本对图像生成的基于扩散的框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于对个性化图像生成的需求，主题驱动的文本对图像生成方法基于文本提示创建了对输入主题的新颖性，因此引起了越来越多的研究兴趣。现有方法通常会学习主题表示，并将其纳入及时嵌入以指导图像生成，但它们在保留主题保真度方面挣扎。为了解决此问题，本文将使用一个名为Scenebooth的新颖框架，用于主题保存的文本对图像生成，该框架消耗了主题图像，对象短语和文本提示的输入。我们的Scenebooth没有学习主题表示并生成主题，而是修复了给定的主题图像，并生成以文本提示为指导的背景图像。为此，我们的Scendbooth介绍了两个关键组件，即多模式布局生成模块和背景绘画模块。前者通过生成与文本字幕，对象短语和主题视觉信息一致的适当场景布局来确定主题的位置和规模。后者将两个适配器（ControlNet和门控自我注意力）集成到潜在扩散模型中，以生成与场景布局和文本描述指导的主题相协调的背景。通过这种方式，我们的场景工作室确保了对受试者在输出中的外观的准确保存。定量和定性的实验结果表明，在主题保存，图像协调和整体质量方面，场景工厂的表现明显优于基线方法。

### CM3T: Framework for Efficient Multimodal Learning for Inhomogeneous Interaction Datasets 
[[arxiv](https://arxiv.org/abs/2501.03332)] [[cool](https://papers.cool/arxiv/2501.03332)] [[pdf](https://arxiv.org/pdf/2501.03332)]
> **Authors**: Tanay Agrawal,Mohammed Guermal,Michal Balazia,Francois Bremond
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: Preprint. Final paper accepted at the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Tucson, February, 2025. 10 pages
- **标题**: CM3T：用于不均匀交互数据集的有效多模式学习框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 交叉学习的挑战涉及不均匀甚至不足的培训数据，并且缺乏重新审计模型的资源。受到NLP，适配器和前缀调整的转移学习技术的启发，本文介绍了一种新的模型无关插件体系结构，用于交叉学习，称为CM3T，该插件称为CM3T，将基于变压器的模型适应了新的或缺少的信息。我们介绍了两个适配器块：用于转移学习的多头视觉适配器，以及用于多模式学习的跨注意适配器。由于主干和其他插件不需要对这些添加，因此训练变得非常有效。对三个数据集的比较和消融研究Epic-kitchens-100，mpiigroupaction和Udiva V0.5在不同的记录设置和任务上显示了此框架的功效。与处理视频输入的骨干相比，只有12.8％的可训练参数，并且仅针对另外两种方式进行了可训练的参数，我们比最先进的结果获得了可比甚至更好的结果。 CM3T对培训或预处理没有具体要求，并且是弥合一般模型和视频分类特定实际应用之间差距的一步。

### MObI: Multimodal Object Inpainting Using Diffusion Models 
[[arxiv](https://arxiv.org/abs/2501.03173)] [[cool](https://papers.cool/arxiv/2501.03173)] [[pdf](https://arxiv.org/pdf/2501.03173)]
> **Authors**: Alexandru Buburuzan,Anuj Sharma,John Redford,Puneet K. Dokania,Romain Mueller
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: 8 pages
- **标题**: MOBI：使用扩散模型的多模式对象插入
- **领域**: 计算机视觉和模式识别
- **摘要**: 安全至关重要的应用（例如自动驾驶）需要广泛的多模式数据才能进行严格的测试。由于收集现实世界数据的成本和复杂性，但需要高度的现实主义和可控性才能有用，因此基于综合数据的方法正在变得突出。本文介绍了MOBI，这是一种用于多模式对象的新型框架，该框架利用扩散模型来创建跨感知模式的真实和可控制的对象，同时为相机和激光镜头展示。使用单个参考RGB图像，MOBI使对象可以在由边界框指定的3D位置处无缝插入现有的多模式场景中，同时保持语义一致性和多模式相干性。与仅依赖编辑面膜的传统涂垫方法不同，我们的3D边界框调节可为对象提供准确的空间定位和逼真的缩放。结果，我们的方法可用于灵活地插入多模式场景中，从而为测试感知模型提供了重要的优势。

### MVP: Multimodal Emotion Recognition based on Video and Physiological Signals 
[[arxiv](https://arxiv.org/abs/2501.03103)] [[cool](https://papers.cool/arxiv/2501.03103)] [[pdf](https://arxiv.org/pdf/2501.03103)]
> **Authors**: Valeriya Strizhkova,Hadi Kachmar,Hava Chaptoukaev,Raphael Kalandadze,Natia Kukhilava,Tatia Tsmindashvili,Nibras Abo-Alzahab,Maria A. Zuluaga,Michal Balazia,Antitza Dantcheva,François Brémond,Laura Ferrari
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: Preprint. Final paper accepted at Affective Behavior Analysis in-the-Wild (ABAW) at IEEE/CVF European Conference on Computer Vision (ECCV), Milan, September, 2024. 17 pages
- **标题**: MVP：基于视频和生理信号的多模式情绪识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类情绪需要一系列复杂的行为，生理和认知变化。当前的最新模型使用经典的机器学习融合了行为和生理组件，而不是最近的深度学习技术。我们建议填补这一空白，设计视频和生理（MVP）体系结构的多模式，以融合视频和生理信号。以不同的方式接近其他方法，MVP利用了注意力的好处，以便能够使用长输入序列（1-2分钟）。我们研究了用于输入长序列的视频和生理骨干，并评估了我们有关最先进的方法。我们的结果表明，根据面部视频，EDA和ECG/PPG，MVP优于以前的情感识别方法。

### Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild 
[[arxiv](https://arxiv.org/abs/2501.02964)] [[cool](https://papers.cool/arxiv/2501.02964)] [[pdf](https://arxiv.org/pdf/2501.02964)]
> **Authors**: Wanpeng Hu,Haodi Liu,Lin Chen,Feng Zhou,Changming Xiao,Qi Yang,Changshui Zhang
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: 苏格拉底式询问：在野外学会自我指导多模式推理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 复杂的视觉推理仍然是当今的关键挑战。通常，挑战是使用诸如思想链（COT）和视觉指导调整之类的方法来应对的。但是，如何有机地结合这两种方法以获得更大的成功，仍然没有探索。此外，仍然需要解决幻觉和高训练成本之类的问题。在这项工作中，我们设计了一个创新的多轮培训和推理框架，适用于轻型多模式大型语言模型（MLLM）。我们的自我询问方法可以启发性地指导MLLM，专注于与目标问题相关的视觉线索，减少幻觉并增强模型描述细粒度图像细节的能力。这最终使该模型能够在复杂的视觉推理和提问任务中表现良好。我们将此框架命名为Socratic询问（SQ）。为了促进未来的研究，我们创建了一个名为CAPQA的多模式迷你数据，其中包括1K精细活动的图像，用于视觉指导调整和评估，我们提出的SQ方法可提高31.2％的幻觉分数。我们对各种基准测试的广泛实验表明，SQ在启发式自我询问，零射击视觉推理和缓解幻觉方面具有出色的功能。我们的模型和代码将公开使用。

### SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild 
[[arxiv](https://arxiv.org/abs/2501.02962)] [[cool](https://papers.cool/arxiv/2501.02962)] [[pdf](https://arxiv.org/pdf/2501.02962)]
> **Authors**: Jiawei Liu,Yuanzhi Zhu,Feiyu Gao,Zhibo Yang,Peng Wang,Junyang Lin,Xinggang Wang,Wenyu Liu
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: scenevtg ++：野外可控的多语言视觉文本生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 在自然场景图像中生成视觉文本是一项具有挑战性的任务，并且有许多未解决的问题。与在人工设计的图像（例如海报，封面，卡通等）上生成文本不同，自然场景图像中的文本需要满足以下四个关键标准：（1）保真度：生成的文本应像照片一样现实，并且在任何笔触中都没有错误。 （2）合理性：文本应在合理的载体区域（例如木板，标志，墙壁等）上生成，并且生成的文本内容也应与场景相关。 （3）实用程序：生成的文本可以促进自然场景OCR（光学特征识别）任务的训练。 （4）可控性：应根据需要控制文本的属性（例如字体和颜色）。在本文中，我们提出了一种两个阶段方法，即SceneVTG ++，同时满足了上述四个方面。 SceneVTG ++由文本布局和内容生成器（TLCG）和可控的本地文本扩散（CLTD）组成。前者利用多模态大语言模型的世界知识来查找合理的文本领域，并根据自然场景背景图像推荐文本内容，而后者则根据扩散模型生成可控的多语言文本。通过广泛的实验，我们分别验证了TLCG和CLTD的有效性，并证明了SceneVTG ++的最新文本生成性能。此外，生成的图像在OCR任务中具有较高的实用性，例如文本检测和文本识别。代码和数据集将可用。

### A Novel Vision Transformer for Camera-LiDAR Fusion based Traffic Object Segmentation 
[[arxiv](https://arxiv.org/abs/2501.02858)] [[cool](https://papers.cool/arxiv/2501.02858)] [[pdf](https://arxiv.org/pdf/2501.02858)]
> **Authors**: Toomas Tahves,Junyi Gu,Mauro Bellone,Raivo Sell
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: International Conference on Agents and Artificial Intelligence 2025
- **标题**: 基于摄像机融合的流量对象细分的新型视觉变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了用于交通对象分割的摄像头融合变压器（CLFT）模型，该模型利用视觉变压器利用相机和激光镜数据的融合。我们基于利用自我发挥机制的视觉变压器的方法，将分割功能扩展到具有其他分类选项的细分能力，以在各种天气条件下包括骑自行车的对象，交通标志和行人在内的各种对象。尽管表现良好，但这些模型在不利条件下仍面临挑战，这突显了需要进一步优化以提高黑暗和降雨的性能。总而言之，CLFT模型为自主驾驶感知提供了令人信服的解决方案，在多模式融合和对象细分方面推进了最新的，并且需要持续的努力来应对现有的局限性并充分利用其在实际部署中的潜力。

### First-place Solution for Streetscape Shop Sign Recognition Competition 
[[arxiv](https://arxiv.org/abs/2501.02811)] [[cool](https://papers.cool/arxiv/2501.02811)] [[pdf](https://arxiv.org/pdf/2501.02811)]
> **Authors**: Bin Wang,Li Jing
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: technical report
- **标题**: 街道景观商店标志识别竞赛的第一名解决方案
- **领域**: 计算机视觉和模式识别
- **摘要**: 应用于街景店面标志的文本识别技术越来越多地在各个实际领域，包括地图导航，智能城市规划分析和商业价值评估。这项技术具有巨大的研究和商业潜力。然而，它面临许多挑战。街景图像通常包含带有复杂设计和多种文本样式的招牌，使文本识别过程变得复杂。我们的团队在最近的一场比赛中引入了这一领域的显着进步。我们开发了一种新型的多阶段方法，该方法将多模式特征融合，广泛的自我监督训练和基于变压器的大型模型整合在一起。此外，采用了依靠强化学习和文本纠正方法的创新技术，例如BoxDQN，从而带来了令人印象深刻的结果。全面的实验验证了这些方法的有效性，展示了我们增强复杂城市环境中文本识别能力的潜力。

### Visual Large Language Models for Generalized and Specialized Applications 
[[arxiv](https://arxiv.org/abs/2501.02765)] [[cool](https://papers.cool/arxiv/2501.02765)] [[pdf](https://arxiv.org/pdf/2501.02765)]
> **Authors**: Yifan Li,Zhixin Lai,Wentao Bao,Zhen Tan,Anh Dao,Kewei Sui,Jiayi Shen,Dong Liu,Huan Liu,Yu Kong
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: 通用和专业应用程序的视觉大语言模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉语言模型（VLM）已成为学习视觉和语言的统一嵌入空间的强大工具。受大型语言模型的启发，这些模型表现出强大的推理和多任务功能，视觉大语模型（VLLM）正在越来越关注构建通用VLMS。尽管VLLM在VLLM中取得了重大进展，但相关文献仍然有限，特别是从全面的应用程序角度来看，跨越视觉（图像，视频，深度），动作和语言方式涵盖了广义和专业应用。在这项调查中，我们专注于VLLM的各种应用，检查其使用方案，确定道德规范的考虑和挑战，并讨论未来的发展方向。通过综合这些内容，我们旨在提供一份全面的指南，为未来的创新和VLLM的更广泛应用铺平道路。可以使用纸列表存储库：https：//github.com/jackyfl/awesome-vllms。

### MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation 
[[arxiv](https://arxiv.org/abs/2501.04155)] [[cool](https://papers.cool/arxiv/2501.04155)] [[pdf](https://arxiv.org/pdf/2501.04155)]
> **Authors**: Siddharth Joshi,Besmira Nushi,Vidhisha Balachandran,Varun Chandrasekaran,Vibhav Vineet,Neel Joshi,Baharan Mirzasoleiman
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: MM-GEN：通过有针对性的多模式数据策划增强任务性能
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 视觉语言模型（VLM）非常有效，但在专业任务上通常表现不佳；例如，由于特定于任务的培训数据，LLAVA-1.5在图表和图表理解方面挣扎。来自通用数据集的现有培训数据未能捕获这些任务所需的细微细节。我们介绍了MM-GEN，这是一种可扩展的方法，该方法通过利用更强大的模型来生成特定于任务的高质量合成文本，用于候选图像。 MM-GEN采用一个三阶段的目标过程：将数据分配到子组中，根据任务描述生成目标文本，并滤除冗余和离群数据。通过MM-GEN生成的数据进行微调VLM可带来显着的性能增长，其中包括空间推理的29％和15％的LLAVA-1.5（7b）图表理解。与人类策划的标题数据相比，MM-GEN可以为原始模型提供高达1.6倍的改进，证明其在提高特定任务的VLM性能方面的有效性，并弥合通用数据集和专业要求之间的差距。可在https://github.com/sjoshi804/mm-gen上找到代码。

### Benchmarking Large and Small MLLMs 
[[arxiv](https://arxiv.org/abs/2501.04150)] [[cool](https://papers.cool/arxiv/2501.04150)] [[pdf](https://arxiv.org/pdf/2501.04150)]
> **Authors**: Xuelu Feng,Yunsheng Li,Dongdong Chen,Mei Gao,Mengchen Liu,Junsong Yuan,Chunming Qiao
> **First submission**: 2025-01-04
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: 基准大小的MLLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模式模型（MLLM），例如GPT-4V和GPT-4O，在理解和生成多模式内容方面取得了显着的进步，展示了各种任务跨越的质量和能力。但是，他们的部署面临着重大挑战，包括缓慢的推理，高计算成本和对设备应用程序的不切实际性。相比之下，小型MLLM的出现以Llava系列模型和PHI-3视频为例，提供了有希望的替代方案，具有更快的推理，降低的部署成本以及处理特定领域的情况的能力。尽管存在日益增长，但大小的MLLM之间的能力边界仍未被逐渐倍增。在这项工作中，我们进行了系统的全面评估，以基准小MLLM和大型MLLM，涵盖一般能力，例如对象识别，时间推理和多模式理解，以及在工业和汽车等领域中的现实应用程序。我们的评估表明，在特定情况下，小型MLLM可以实现与大型模型相当的性能，但在需要更深入理解或细微理解的复杂任务中显着滞后。此外，我们确定了小型和大型MLLM的常见故障案例，突出了即使是最先进的模型都在挣扎的领域。我们希望我们的发现将指导研究界突破MLLM的质量界限，从而在各种应用程序中提高其可用性和有效性。

### Graph-Based Multimodal and Multi-view Alignment for Keystep Recognition 
[[arxiv](https://arxiv.org/abs/2501.04121)] [[cool](https://papers.cool/arxiv/2501.04121)] [[pdf](https://arxiv.org/pdf/2501.04121)]
> **Authors**: Julia Lee Romero,Kyle Min,Subarna Tripathi,Morteza Karimzadeh
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: 9 pages, 6 figures
- **标题**: 基于图形的多模式和多视图对齐，以识别密钥步骤
- **领域**: 计算机视觉和模式识别
- **摘要**: 以eg中心的视频从佩戴者的角度捕获场景，导致动态背景，频繁的运动和遮挡，对准确的KeyStep识别提出了挑战。我们提出了一个灵活的图形学习框架，以实现细粒度的关键识别，该框架能够有效利用以自我为中心的视频中的长期依赖性，并在培训期间利用以自我为中心和以外的式视频之间的一致性来改善自我中心视频的推论。我们的方法包括构造图形，其中每个视频的视频片段与节点相对应。在培训期间，我们将每个以外心视频的每个剪辑（如果有）视为其他节点。我们研究了几种策略来定义这些节点上的连接，并构成键入识别为构造图上的节点分类任务。我们在EGO-EXO4D数据集上执行了广泛的实验，并表明我们提出的基于图形的框架的准确性大大优于现有方法超过12点。此外，构造的图是稀疏且计算有效的。我们还介绍了一项研究，该研究在异质图上利用多种模式特征，包括叙述，深度和对象类标签，并讨论它们对KeyStep识别性能的相应贡献。

### LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2501.04005)] [[cool](https://papers.cool/arxiv/2501.04005)] [[pdf](https://arxiv.org/pdf/2501.04005)]
> **Authors**: Lingdong Kong,Xiang Xu,Youquan Liu,Jun Cen,Runnan Chen,Wenwei Zhang,Liang Pan,Kai Chen,Ziwei Liu
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: Preprint; 16 pages, 7 figures, 8 tables; Project Page at https://ldkong.com/LargeAD
- **标题**: Largead：自动驾驶的大规模跨传感器数据预处理
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 视觉基础模型（VFM）的最新进步已经彻底改变了2D的视觉感知，但是它们具有3D场景理解的潜力，尤其是在自主驾驶应用中，仍然没有得到充实的态度。在本文中，我们介绍了Largead，这是一个多功能且可扩展的框架，旨在在不同的现实世界驾驶数据集中进行大规模3D预处理。我们的框架利用VFM从2D图像中提取语义上富含的超像素，这些图像与LiDar Point Cloud对齐以生成高质量的对比样品。这种一致性有助于跨模式表示学习，从而增强了2D和3D数据之间的语义一致性。我们介绍了几项关键创新：i）VFM驱动的超级像素生成，用于详细的语义表示，ii）一种由VFM辅助的对比度学习策略，以使多模式特征与多模式特征保持一致，iii）SuperPoint Point Pimoint时间一致性，以维持跨时间的稳定表示，IV）多source数据，以跨各种LIDAR配置为总体而言。我们的方法在基于激光雷达的细分和对象检测的线性探测和微调任务中，对最先进的方法进行了显着的性能改进。对11个大规模多模式数据集进行了广泛的实验，突出了我们的出色性能，证明了现实世界自动驾驶场景中的适应性，效率和鲁棒性。

### Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives 
[[arxiv](https://arxiv.org/abs/2501.04003)] [[cool](https://papers.cool/arxiv/2501.04003)] [[pdf](https://arxiv.org/pdf/2501.04003)]
> **Authors**: Shaoyuan Xie,Lingdong Kong,Yuhao Dong,Chonghao Sima,Wenwei Zhang,Qi Alfred Chen,Ziwei Liu,Liang Pan
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: Preprint; 41 pages, 32 figures, 16 tables; Project Page at https://drive-bench.github.io/
- **标题**: VLMS已准备好进行自动驾驶吗？从可靠性，数据和度量角度来看的实证研究
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 视觉模型（VLM）的最新进展引起了对自动驾驶的使用的兴趣，尤其是通过自然语言产生可解释的驾驶决策。但是，VLM固有地为驾驶提供视觉扎根，可靠和可解释的解释的假设在很大程度上尚未进行。为了解决这一差距，我们介绍了DriveBench，这是一个基准数据集，旨在评估17个设置（干净，损坏和仅文本输入）的VLM可靠性，涵盖了19,200帧，20,498个问题 - 答案对，三种问题类型，四种问题类型，四个Mainstream驾驶任务，总计12个流行的VLM。我们的发现表明，VLM通常会产生从常识或文本提示而不是真正的视觉接地的合理响应，尤其是在退化或缺失的视觉输入下。这种行为是由于数据集失衡和评估指标不足所掩盖的，在自动驾驶（例如自动驾驶）等安全性场景中构成了重大风险。我们进一步观察到，VLM与多模式推理斗争，并表现出对输入腐败的敏感性提高，从而导致性能不一致。为了应对这些挑战，我们提出了精致的评估指标，以优先考虑强大的视觉接地和多模式理解。此外，我们强调了利用VLMS对腐败意识提高其可靠性的潜力，为在现实世界自动驾驶环境中开发更可信赖和可解释的决策系统提供了路线图。基准工具包是可以公开访问的。

### Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos 
[[arxiv](https://arxiv.org/abs/2501.04001)] [[cool](https://papers.cool/arxiv/2501.04001)] [[pdf](https://arxiv.org/pdf/2501.04001)]
> **Authors**: Haobo Yuan,Xiangtai Li,Tao Zhang,Zilong Huang,Shilin Xu,Shunping Ji,Yunhai Tong,Lu Qi,Jiashi Feng,Ming-Hsuan Yang
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: Project page: https://lxtgh.github.io/project/sa2va
- **标题**: SA2VA：将SAM2与llava结婚，以深入地理解图像和视频
- **领域**: 计算机视觉和模式识别
- **摘要**: 这项工作介绍了SA2VA，这是对图像和视频的密集理解的第一个统一模型。与通常仅限于特定模式和任务的现有多模式大型语言模型不同，SA2VA支持广泛的图像和视频任务，包括参考细分和对话，并以最少的单次指令调整调整。 SA2VA结合了SAM-2（基础视频细分模型）与Llava（一个先进的视觉语言模型）结合在一起，将文本，图像和视频统一为共享的LLM令牌空间。 SA2VA使用LLM生成指导令牌，该指令令牌指导SAM-2生成精确的掩码，从而对静态和动态视觉内容产生扎根的多模式理解。此外，我们介绍了Ref-SAV，这是一种自动标记的数据集，该数据集包含复杂的视频场景中的72K对象表达式，旨在提高模型性能。我们还手动验证了Ref-SAV数据集中的2K视频对象，以基准在复​​杂环境中引用视频对象分割。实验表明，SA2VA在多个任务中实现了最新的实现，尤其是在引用视频对象细分时，突出了其对复杂现实世界应用的潜力。

### Visual question answering: from early developments to recent advances -- a survey 
[[arxiv](https://arxiv.org/abs/2501.03939)] [[cool](https://papers.cool/arxiv/2501.03939)] [[pdf](https://arxiv.org/pdf/2501.03939)]
> **Authors**: Ngoc Dung Huynh,Mohamed Reda Bouadjenek,Sunil Aryal,Imran Razzak,Hakim Hacid
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: 20 papers
- **标题**: 视觉问题回答：从早期发展到最近的进步 - 一项调查
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 视觉问题回答（VQA）是一个不断发展的研究领域，旨在通过整合图像和语言处理技术，例如特征提取，对象检测，文本嵌入，自然语言理解和语言产生来回答有关视觉内容的问题。随着多模式数据研究的增长，VQA由于其广泛的应用，包括交互式教育工具，医学图像诊断，客户服务，娱乐和社交媒体字幕，引起了人们的关注。此外，VQA通过从图像中产生描述性内容来帮助视力障碍个体起着至关重要的作用。这项调查介绍了VQA体系结构的分类法，根据设计选择和关键组成部分对它们进行分类，以促进比较分析和评估。我们回顾了主要的VQA方法，专注于基于深度学习的方法，并探索大型视觉语言模型（LVLMS）的新兴领域，这些模型在诸如VQA之类的多模式任务中都取得了成功。本文进一步研究了可用的数据集和评估指标，这对于测量VQA系统性能至关重要，然后探索了实际VQA应用程序。最后，我们强调了VQA研究中的持续挑战和未来的方向，提出了开放的问题和潜在的进一步发展领域。这项调查是对最新进步和未来感兴趣的研究人员和从业者的综合资源

### LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token 
[[arxiv](https://arxiv.org/abs/2501.03895)] [[cool](https://papers.cool/arxiv/2501.03895)] [[pdf](https://arxiv.org/pdf/2501.03895)]
> **Authors**: Shaolei Zhang,Qingkai Fang,Zhe Yang,Yang Feng
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b
- **标题**: Llava-Mini：具有一个视觉令牌的高效图像和视频大型多模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 像GPT-4O这样的实时大型多模型（LMM）的出现引起了人们对有效LMM的极大兴趣。 LMM框架通常将视觉输入编码到视觉令牌（连续表示）中，并将其集成并将文本指令集成到大语言模型（LLMS）的上下文中，其中大规模参数和众多上下文令牌（主要是视觉令牌）会导致实质性计算的开销。先前为有效LMM的努力始终专注于用较小的模型代替LLM主链，同时忽略了令牌数量的关键问题。在本文中，我们介绍了Llava-Mini，这是一种有效的LMM，具有最小的视觉令牌。为了在保留视觉信息的同时获得高度的视觉令牌，我们首先分析了LMMS如何理解视觉令牌，并发现大多数视觉令牌仅在LLM主链的早期层中起着至关重要的作用，在那里它们主要将视觉信息融合到文本令牌中。在这一发现的基础上，LLAVA-MINI将融合前融合的方式融合到了文本令牌中，从而促进了将视觉令牌的极端压缩为five fise five five backen for to llm backbone。 Llava-Mini是一个统一的大型多式模型，可以有效地支持对图像，高分辨率图像和视频的理解。跨11个基于图像和7个基于视频的基准的实验表明，Llava-Mini的表现优于Llava-V1.5，只有1个视觉令牌而不是576。效率分析表明，LLAVA-MINI可以将Flops降低77％，在40毫升范围内进行了40毫升响应，并在10,000倍的gpu sone of Brokess of Brokes of Brokess of Onders of Bergame of Bergame of Broke of 40 millisenty响应中。

### CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds Ratio on High-Resolution Point Clouds 
[[arxiv](https://arxiv.org/abs/2501.03879)] [[cool](https://papers.cool/arxiv/2501.03879)] [[pdf](https://arxiv.org/pdf/2501.03879)]
> **Authors**: Keonwoo Kim,Yeongjae Cho,Taebaek Hwang,Minsoo Jo,Sangdo Han
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: CL3DOR：通过高分辨率点云上的优势比3D大型多模型的对比度学习
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近的研究表明，大型语言模型（LLM）不仅限于仅文本任务，还可以在各种模式（包括音频，图像和视频）中充当多模型。特别是，对3D大型多模型模型（3D LMM）的研究正在取得显着的步伐，这是由于处理高维数据（如点云）的潜力。但是，经过仔细检查，我们发现现有培训数据集的每个样本中的视觉和文本内容都缺乏高信息粒度和清晰度，这是一种瓶颈，可以进行精确的跨模式理解。为了解决这些问题，我们通过高分辨率点云上的几率比例提出了3D大型多模型模型的CL3DOR，旨在确保视觉和文本内容的更特异性和清晰度。具体来说，我们增加了每个对象的点云的密度，并在培训数据集中构建信息性的硬性响应，以惩罚不需要的响应。为了利用硬性负面响应，我们将优势比作为对比度学习的辅助术语纳入常规语言建模损失。 CL3DOR在3D场景的理解和推理基准中实现了最先进的表现。此外，我们通过广泛的实验证明了CL3DOR关键成分的有效性。

### LM-Net: A Light-weight and Multi-scale Network for Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2501.03838)] [[cool](https://papers.cool/arxiv/2501.03838)] [[pdf](https://arxiv.org/pdf/2501.03838)]
> **Authors**: Zhenkun Lu,Chaoyin She,Wei Wang,Qinghua Huang
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: LM-NET：一个轻巧和多尺度网络，用于医疗图像分段
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的医疗图像细分方法在深入探索多尺度信息并有效地将本地细节纹理与全球上下文语义信息结合在一起时存在局限性。这导致过度分割，分割不足和分割界限模糊。为了应对这些挑战，我们从不同的角度探讨了多尺度特征表示形式，提出了一种新颖，轻巧和多规模的体系结构（LM-NET），该体系结构（LM-NET）整合了卷积神经网络（CNN）和视觉变形金刚（VITS）的优势，以增强分割的准确性。 LM-NET采用轻巧的多支球模块来捕获同一级别的多尺度功能。此外，我们介绍了两个模块，以同时捕获不同级别的多尺度特征的本地细节纹理和全局语义：本地功能变压器（LFT）和全局功能变压器（GFT）。 LFT整合了本地窗口自我注意，以捕获本地细节纹理，而GFT则利用全球自我注意力来捕获全球上下文语义。通过结合这些模块，我们的模型实现了本地和全球表示之间的互补性，从而减轻了医疗图像分割中分割界限模糊的问题。为了评估LM-NET的可行性，在三个具有不同方式的公开数据集上进行了广泛的实验。我们提出的模型可实现最新的结果，超过了先前的方法，而仅需要4.66克拖鞋和540万参数。这些最先进的结果在三个具有不同模式的数据集中证明了我们所提出的LM-NET对各种医疗图像分割任务的有效性和适应性。

### KAnoCLIP: Zero-Shot Anomaly Detection through Knowledge-Driven Prompt Learning and Enhanced Cross-Modal Integration 
[[arxiv](https://arxiv.org/abs/2501.03786)] [[cool](https://papers.cool/arxiv/2501.03786)] [[pdf](https://arxiv.org/pdf/2501.03786)]
> **Authors**: Chengyuan Li,Suyang Zhou,Jieping Kong,Lei Qi,Hui Xue
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: Accepted by ICASSP 2025
- **标题**: Kanoclip：通过知识驱动的及时学习和增强的跨模式集成，零射击异常检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 零射击异常检测（ZSAD）识别异常，而无需从目标数据集中进行培训样本，这对于具有隐私问题或有限数据的方案必不可少。诸如剪辑之类的视觉模型在ZSAD中显示出潜力，但具有局限性：依靠手动制作的固定文本描述或异常提示是时必的，既耗时又容易出现语义歧义，并且在像素级异常分段中剪辑的斗争更加集中在全球语义上，而不是本地详细信息。为了解决这些局限性，我们引入了Kanoclip，这是一种利用视觉模型的新型ZSAD框架。 Kanoclip通过知识驱动的及时学习（KNPL），将大语言模型（GPT-3.5）和细粒度特定于图像的知识（LLAMA3）结合在一起。 KNPL使用知识驱动（KD）损失功能来创建可学习的异常提示，消除了对固定文本提示的需求并增强概括。 Kanoclip包括带有V-V注意的剪辑视觉编码器（夹子VV），多级交叉模式相互作用（BI-CMCI）的双向交叉注意和Conv-Adapter。这些组件保留局部视觉语义，改善局部跨模式融合，并将全局视觉特征与文本信息保持一致，从而增强像素级异常检测。 Kanoclip在12个工业和医疗数据集中达到了ZSAD的最先进的性能，与现有方法相比，综合性的概括。

### SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning 
[[arxiv](https://arxiv.org/abs/2501.03675)] [[cool](https://papers.cool/arxiv/2501.03675)] [[pdf](https://arxiv.org/pdf/2501.03675)]
> **Authors**: Andrew Li,Rahul Thapa,Rahul Chalamala,Qingyang Wu,Kezhen Chen,James Zou
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: Smir：有效的合成数据管道以改善多图像推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLMS）在理解单个图像方面表现出色，并在高质量的指令数据集的帮助下。但是，由于两个关键挑战，多图像推理在开源社区中仍未得到充实的态度：（1）具有相关图像的扩展数据集和复杂的推理指令是资源密集的，并且（2）缺乏针对多图像任务的可靠评估基准。为了解决这个问题，我们介绍了Smir，Smir是一种用于多图像推理的合成数据生成管道，以及使用此管道生成的高质量数据集。 Smir通过多模式嵌入有效提取相关的图像，集成视觉和描述性信息，并利用开源LLMS生成质量指令。使用这种方法，我们生产160K合成训练样本，为封闭源解决方案提供具有成本效益的替代方案。此外，我们提出了Smir Bench，这是一个多图像推理基准，其中包括七个复杂的推理任务中的200种不同示例。 Smir Bench是多转弯的，并聘请了VLM法官来评估自由形式的响应，从而对模型表达和推理能力进行了全面评估。我们通过微调开源VLM并在Smir Bench上评估SMIR的有效性。

### Action Quality Assessment via Hierarchical Pose-guided Multi-stage Contrastive Regression 
[[arxiv](https://arxiv.org/abs/2501.03674)] [[cool](https://papers.cool/arxiv/2501.03674)] [[pdf](https://arxiv.org/pdf/2501.03674)]
> **Authors**: Mengshi Qi,Hao Ye,Jiaxuan Peng,Huadong Ma
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: 通过分层姿势指导的多阶段对比回归评估行动质量评估
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 旨在自动且公平地评估运动表现的行动质量评估（AQA），近年来引起了人们的关注。但是，运动员经常处于快速运动，相应的视觉外观差异是微妙的，这使得捕获细粒度的姿势差异并导致估计效果不佳。此外，最常见的AQA任务（例如运动中的潜水）通常分为多个子actions，每个子弹都包含不同的持续时间。但是，现有方法着重于将视频分割为固定帧，这破坏了子action的时间连续性，导致不可避免的预测错误。为了应对这些挑战，我们通过分层引导的多阶段对比回归提出了一种新颖的动作质量评估方法。首先，我们引入了一个多尺度的动态视觉 - 骨骼编码器，以捕获细粒的时空视觉和骨骼特征。然后，引入了一个过程分割网络以分离不同的子动作并获得分段特征。之后，分段的视觉和骨骼特征都被送入多模式融合模块作为物理结构先验，以指导模型学习精致的活动相似性和方差。最后，采用多阶段的对比度学习回归方法来学习判别性表示和输出预测结果。此外，我们还引入了新的注释订单数据集，以改善当前的低质量人姿势标签。在实验中，有关罚款和MTL-AQA数据集的结果证明了我们提出的方法的有效性和优势。我们的源代码和数据集可从https://github.com/lumos0507/hp-mcore获得。

### Evaluating Image Caption via Cycle-consistent Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2501.03567)] [[cool](https://papers.cool/arxiv/2501.03567)] [[pdf](https://arxiv.org/pdf/2501.03567)]
> **Authors**: Tianyu Cui,Jinbin Bai,Guo-Hua Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Ye Shi
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: 通过循环一致的文本对图像生成评估图像字幕
- **领域**: 计算机视觉和模式识别
- **摘要**: 评估图像标题通常取决于参考字幕，这些标题是获得和表现出重要的多样性和主观性的代价高昂的。尽管已经提出了无参考评估指标，但大多数关注字幕和图像之间的跨模式评估。最近的研究表明，模态差距通常存在于基于对比度学习的多模式系统的表示，这破坏了诸如ClipsCore之类的跨模式指标的可靠性。在本文中，我们提出了CAMScore，这是图像字幕模型的无环自动评估度量。为了规避上述模态差距，CamScore使用文本对图像模型从字幕中生成图像，然后根据原始图像评估这些生成的图像。此外，为了提供精细的信息以进行更全面的评估，我们为CAMSCORE设计了一个三级评估框架，其中包括像素级，语义级别和目标级别的观点。多个基准数据集的广泛实验结果表明，与现有的基于参考和无参考的指标相比，CAMSCORE与人类判断具有较高的相关性，这证明了框架的有效性。

### Towards Generalizable Trajectory Prediction Using Dual-Level Representation Learning And Adaptive Prompting 
[[arxiv](https://arxiv.org/abs/2501.04815)] [[cool](https://papers.cool/arxiv/2501.04815)] [[pdf](https://arxiv.org/pdf/2501.04815)]
> **Authors**: Kaouther Messaoud,Matthieu Cord,Alexandre Alahi
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 使用双级表示学习和自适应提示来实现可推广的轨迹预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的车辆轨迹预测模型与普遍性，预测不确定性和处理复杂的相互作用的斗争。它通常是由于针对特定数据集定制的复杂体系结构等限制以及效率低下的多模式处理。我们提出了带有寄存器查询（PERREG+）的感知器，这是一个新颖的轨迹预测框架，介绍了：（1）通过自distillation（SD）和蒙版重建（MR）进行双层表示学习，捕获全球上下文和细粒度细节。此外，我们从掩盖的输入中重建分段轨迹和泳道段的方法，可以有效利用上下文信息并改善概括； （2）使用基于寄存器的查询和预处理增强了多模式，从而消除了对聚类和抑制的需求； （3）在微调过程中进行自适应及时调整，冻结主要体系结构并优化少量提示以进行有效的适应性。 perreg+设置了Nuscenes [1]，Argoverse 2 [2]和Waymo Open Motion数据集（WOMD）[3]的新最新性能。引人注目的是，我们审慎的模型在较小的数据集中将误差降低了6.8％，并且多数据集训练增强了概括。在跨域测试中，与非预告片变体相比，PERREG+将B-FDE降低了11.8％。

### DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests 
[[arxiv](https://arxiv.org/abs/2501.04671)] [[cool](https://papers.cool/arxiv/2501.04671)] [[pdf](https://arxiv.org/pdf/2501.04671)]
> **Authors**: Charles Corbière,Simon Roburin,Syrielle Montariol,Antoine Bosselut,Alexandre Alahi
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: driveVQA：通过驾驶理论测试在现实世界中分析视觉链的视觉推理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉模型（LVLMS）增强了具有视觉理解的语言模型，从而实现了多模式推理。但是，由于文本和视觉数据之间的方式差距，它们经常面临重大挑战，例如过度依赖文本先验，幻觉和复杂视觉推理的能力有限。评估LVLM中视觉推理的现有基准通常依赖于示意图或合成图像以及不精确的机器生成的解释。为了弥合模式差距，我们提出了DrivingVQA，这是一种由驱动理论测试得出的新基准测试，以评估复杂的现实世界情景中的视觉链经过思考推理。它提供了3,931个专家制作的多项选择问题，并提供了与推理过程相关的实体的交错解释。我们利用该数据集对LVLMS进行复杂视觉场景的推理能力进行广泛研究。我们的实验表明，开源和专有的LVLM在零击设置下与视觉链中的推理斗争。我们调查利用相关实体来改善视觉推理的培训策略。值得注意的是，当与这些实体相关的裁剪区域的图像令牌上进行推理时，我们会观察到高达7％的性能。

### Are They the Same? Exploring Visual Correspondence Shortcomings of Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2501.04670)] [[cool](https://papers.cool/arxiv/2501.04670)] [[pdf](https://arxiv.org/pdf/2501.04670)]
> **Authors**: Yikang Zhou,Tao Zhang,Shilin Xu,Shihao Chen,Qianyu Zhou,Yunhai Tong,Shunping Ji,Jiangning Zhang,Xiangtai Li,Lu Qi
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: fix typos, figures, tables, and other details; additional results
- **标题**: 他们一样吗？探索多模式LLM的视觉对应关系缺点
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模型模型的最新进展表明，在视觉感知，推理能力和视力语言理解方面具有很强的能力。但是，缺少有关视觉匹配能力的研究，其中找到对象的视觉对应关系在视觉研究中至关重要。我们的研究表明，即使使用当前强大的MLLMS模型GPT-4O，最近多模式LLMS（MLLM）中的匹配功能仍然存在系统的缺点。特别是，我们构建了一个多模式的视觉匹配（MMVM）基准，以在30个不同的MLLM上相当基准。 MMVM基准测试由15个开源数据集和Internet视频，并提供手动注释。我们根据所需的提示和功能将MMVM基准的数据样本分为八个方面，以更全面地评估和分析当前的MLLM。此外，我们设计了一个自动注释管道来生成MMVM SFT数据集，其中包括220k视觉匹配的数据，并带有推理注释。最后，我们提出了Colva，这是一种新颖的对比度，具有两种新颖的技术设计：具有对象级对比度学习和教学增强策略的细粒度视觉专家。 Colva在MMVM基准上达到51.06 \％的总体准确性（OA），分别超过GPT-4O和基线，分别以8.41 \％和23.58 \％OA。结果显示了我们的MMVM SFT数据集和我们新颖的技术设计的有效性。代码，基准，数据集和模型可在https://github.com/zhouyiks/colva上找到。

### Unified Coding for Both Human Perception and Generalized Machine Analytics with CLIP Supervision 
[[arxiv](https://arxiv.org/abs/2501.04579)] [[cool](https://papers.cool/arxiv/2501.04579)] [[pdf](https://arxiv.org/pdf/2501.04579)]
> **Authors**: Kangsheng Yin,Quan Liu,Xuelin Shen,Yulin He,Wenhan Yang,Shiqi Wang
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: 9 pages, 10 figures, publised to AAAI 2025
- **标题**: 通过剪辑监督进行人类感知和广义机器分析的统一编码
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 图像压缩模型长期以来一直在适应性和概括方面挣扎，因为解码的Bitstream通常仅满足人类或机器的需求，并且无法为看不见的视觉任务保留信息。因此，本文创新地介绍了从多模式预训练模型获得的监督，并结合了适应的自适应多目标优化，以同时用单个比特斯流同时支持人类的视觉感知和机器视觉，并将其表示为统一的统一和通用的机器图像编码（UG-ICM）。具体而言，为了摆脱在下游任务监督和下游任务监督之间的压缩模型之间的依赖，我们将对比性语言图像预训练（剪辑）模型引入培训约束，以改善概括。应用全球到现实的剪辑监督来帮助获得层次的语义，从而使模型更易于依赖不同粒度信息的任务。此外，为了支持人类和机器视觉，仅使用统一的bitstream，我们结合了一种有条件的解码策略，该策略以人或机器的偏好为条件，使得对相应偏好的bitstream可以被解码为不同的版本。因此，我们提出的UG-ICM以一种自我监督的方式进行了充分的训练，即没有任何特定的下游模型和任务的意识。广泛的实验表明，所提出的UG-ICM能够在各种看不见的机器分析任务中取得显着改进，同时提供感知上令人满意的图像。

### FGU3R: Fine-Grained Fusion via Unified 3D Representation for Multimodal 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2501.04373)] [[cool](https://papers.cool/arxiv/2501.04373)] [[pdf](https://arxiv.org/pdf/2501.04373)]
> **Authors**: Guoxin Zhang,Ziying Song,Lin Liu,Zhonghong Ou
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: Accepted by ICASSP 2025
- **标题**: FGU3R：通过统一的3D表示，用于多模式3D对象检测的细粒度融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式3D对象检测对自动驾驶引起了极大的兴趣。然而，多模式检测器的尺寸错配中的尺寸不匹配，这些尺寸源于与2D像素的融合，这会导致次优融合性能。在本文中，我们提出了一个多模式框架FGU3R，以通过统一的3D表示和细粒度融合来解决上述问题，该融合由两个重要组成部分组成。首先，我们为原始和伪点提出了一个有效的特征提取器，称为伪-RAW卷积（PRCONV），该卷积（PRCONV）同步调节多模式特征，并根据多模束相互作用将来自关键点的不同类型点的特征汇总。其次，跨注意的自适应融合（CAAF）旨在通过跨注意变体以细粒度的方式适应均匀的3D ROI（感兴趣区域）特征。他们一起在统一的3D表示上进行细粒度的融合。在Kitti和Nuscenes上进行的实验显示了我们提出的方法的有效性。

### DeFusion: An Effective Decoupling Fusion Network for Multi-Modal Pregnancy Prediction 
[[arxiv](https://arxiv.org/abs/2501.04353)] [[cool](https://papers.cool/arxiv/2501.04353)] [[pdf](https://arxiv.org/pdf/2501.04353)]
> **Authors**: Xueqiang Ouyang,Jia Wei,Wenjie Huo,Xiaocong Wang,Rui Li,Jianlong Zhou
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 报道：多模式妊娠预测的有效分离融合网络
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 时间胚胎图像和父母生育表指标对于\ textbf {体外受精胚胎转移}（IVF-ET）的妊娠预测很有价值。但是，当前的机器学习模型无法充分利用两种方式之间的互补信息，以提高怀孕预测的表现。在本文中，我们提出了一个称为融合的去耦融合网络，以有效整合IVF-ET妊娠预测的多模式信息。具体而言，我们提出了一个脱钩融合模块，该模块将信息从不同模式中解耦为相关和无关的信息，从而实现了更精致的融合。然后我们将颞胚图像与空间位置编码融合在一起，并将生育表指示剂信息与表变压器提取。为了评估我们的模型的有效性，我们使用了一个新数据集，其中包括从南科医科大学收集的4046例病例。实验表明，我们的模型优于最先进的方法。同时，眼睛疾病预测数据集的性能反映了该模型的良好概括。我们的代码和数据集可在https://github.com/ouy-young-1999/dfnet上找到。

### Eve: Efficient Multimodal Vision Language Models with Elastic Visual Experts 
[[arxiv](https://arxiv.org/abs/2501.04322)] [[cool](https://papers.cool/arxiv/2501.04322)] [[pdf](https://arxiv.org/pdf/2501.04322)]
> **Authors**: Miao Rang,Zhenni Bi,Chuanjian Liu,Yehui Tang,Kai Han,Yunhe Wang
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 前夕：具有弹性视觉专家的高效多模式视觉语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式视觉语言模型（VLM）在不断增加模型大小和数据量的支持下取得了重大进展。在边缘设备上运行VLM已成为其广泛应用程序的挑战。有几项有效的VLM努力，但他们经常牺牲语言能力来增强多模式能力，或者需要进行广泛的培训。为了解决这个难题，我们通过弹性视觉专家（EVE）介绍了有效的视觉语言模型的创新框架。通过在多个培训的多个阶段策略性地纳入适应性的视觉专业知识，EVE在保持语言能力和增强多模式能力之间取得了平衡。这种平衡的方法导致一种多功能模型，只有1.8B参数可以在多模式和语言任务中得到显着改进。值得注意的是，在低于3B参数的配置中，EVE在语言基准中明显优于语言基准，并在VLM基准中获得最新的结果68.87％。此外，其多模式准确性概念了较大的7B LLAVA-1.5模型的精度。我们的代码可在https://github.com/rangmiao/eve上找到。

### H-MBA: Hierarchical MamBa Adaptation for Multi-Modal Video Understanding in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2501.04302)] [[cool](https://papers.cool/arxiv/2501.04302)] [[pdf](https://arxiv.org/pdf/2501.04302)]
> **Authors**: Siran Chen,Yuxiao Luo,Yue Ma,Yu Qiao,Yali Wang
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: 7 pages, 4 figures
- **标题**: H-MBA：用于自动驾驶中多模式视频理解的分层MAMBA改编
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着多模式大语言模型（MLLM）的流行，自动驾驶遇到了新的机遇和挑战。特别是，多模式视频理解对于交互式分析自主驾驶过程中会发生什么至关重要。但是，在这样一个动态场景中的视频通常包含复杂的时空运动，从而限制了该领域现有MLLM的概括能力。为了弥合差距，我们提出了一种新型的分层Mamba改编（H-MBA）框架，以适合自主驾驶视频中复杂的运动变化。具体而言，我们的H-MBA由两个不同的模块组成，包括上下文Mamba（C-Mamba）和查询Mamba（Q-Mamba）。首先，C-Mamba包含各种类型的结构状态空间模型，这些模型可以有效地捕获不同时间分辨率的多粒度视频上下文。其次，Q-Mamba灵活地将当前框架转换为可学习的查询，并认真地将多粒度视频上下文选择为查询。因此，它可以自适应地整合多尺度时间分辨率的所有视频上下文，以增强视频理解。通过MLLMS中的插件范式，我们的H-MBA显示了自主驾驶中多模式视频任务的出色性能，例如，对于风险对象检测，它的表现优于先前的SOTA方法，其改进为5.5％。

### Enhancing Scene Classification in Cloudy Image Scenarios: A Collaborative Transfer Method with Information Regulation Mechanism using Optical Cloud-Covered and SAR Remote Sensing Images 
[[arxiv](https://arxiv.org/abs/2501.04283)] [[cool](https://papers.cool/arxiv/2501.04283)] [[pdf](https://arxiv.org/pdf/2501.04283)]
> **Authors**: Yuze Wang,Rong Xiao,Haifeng Li,Mariana Belgiu,Chao Tao
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 在云彩图像方案中增强场景分类：使用光学云覆盖和SAR遥感图像的信息调节机制的协作转移方法
- **领域**: 计算机视觉和模式识别,人工智能,图像和视频处理
- **摘要**: 在遥感场景分类中，利用训练有素的光学模型的转移方法是克服标签稀缺的有效方法。但是，云污染会导致光学信息损失和对特征分布的重大影响，从而挑战了转移目标模型的可靠性和稳定性。通用解决方案包括用于光学数据的云删除，或直接使用目标域中的合成孔径雷达（SAR）数据。但是，云的去除需要大量的辅助数据才能进行支持和预训练，而直接使用SAR则无视光学数据的未解决部分。这项研究提出了一种场景分类转移方法，该方法协同结合了多模式数据，该方法旨在将基于云的光学数据训练的源域模型转移到目标域中，以低成本的成本包括云光学和SAR数据。具体而言，该框架结合了两个部分：（1）基于知识蒸馏的协作转移策略，使得跨异质数据具有有效的先验知识转移； （2）提出了信息调节机制（IRM）来解决转移过程中的方式不平衡问题。它采用辅助模型来衡量每种模式的贡献差异，并自动平衡样本级别目标模型学习过程中模态的信息利用。转移实验是在模拟和真实的云数据集上进行的，证明了与云覆盖方案中的其他解决方案相比，该方法的出色性能。我们还验证了IRM的重要性和局限性，并在模型转移过程中进一步讨论和可视化了模式不平衡问题。代码可从https://github.com/wangyuze-csu/esccs获得

### Overcoming Language Priors for Visual Question Answering Based on Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2501.05690)] [[cool](https://papers.cool/arxiv/2501.05690)] [[pdf](https://arxiv.org/pdf/2501.05690)]
> **Authors**: Daowan Peng,Wei Wei
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: Accepted to ICME2024
- **标题**: 克服基于知识蒸馏的视觉问题的语言先验
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 先前的研究指出，视觉问题回答（VQA）模型容易依靠语言先验来进行答案预测。在这种情况下，预测通常取决于语言捷径，而不是对多模式知识的全面掌握，从而降低了它们的概括能力。在本文中，我们提出了一种新颖的方法，即KDAR，利用知识蒸馏来解决VQA任务中的先前依赖性困境。具体而言，训练有素的老师的软标签促进的正规化效果被用来对最常见的答案过度拟合。发挥正规化角色的软标签也提供了语义指导，可缩小候选答案的范围。此外，我们设计了一种自适应样本重新加权学习策略，以动态调整每个样本的重要性来进一步减轻偏见。实验结果表明，我们的方法在OOD和IID设置中都提高了性能。我们的方法在VQA-CPV2分布（OOD）基准测试中实现了最先进的性能，从而大大超过了先前的最新方法。

### Deep Reversible Consistency Learning for Cross-modal Retrieval 
[[arxiv](https://arxiv.org/abs/2501.05686)] [[cool](https://papers.cool/arxiv/2501.05686)] [[pdf](https://arxiv.org/pdf/2501.05686)]
> **Authors**: Ruitao Pu,Yang Qin,Dezhong Peng,Xiaomin Song,Huiming Zheng
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 深模式检索的深度可逆一致性学习
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 跨模式检索（CMR）通常涉及学习共同表示，以直接测量多模式样本之间的相似性。大多数现有的CMR方法通常采用成对的多模式样本，并采用联合培训来学习共同表示，从而限制了CMR的灵活性。尽管某些方法采用了每种模式的独立培训策略来提高CMR的灵活性，但他们利用随机初始化的正交矩阵来指导表示表示学习，这是次优的，因为它们假设级别的样本相互独立，限制了样本表示和接地标签之间的语义比对的潜力。为了解决这些问题，我们提出了一种新颖的方法，称为跨模式检索的深层可逆一致性学习（DRCL）。 DRCL包括两个核心模块，\ ie选择性先验学习（SPL）和可逆的语义一致性学习（RSC）。更具体地说，SPL首先在每种模态上学习一个转换权重矩阵，并根据质量得分作为先验选择最佳的矩阵，这极大地避免了从低质量方式中学到的盲目选择。然后，RSC采用模态不变表示重铸造机制（MRR），通过先前的一般逆基质从样品语义标记中重铸下潜在的模态不变表示。由于标签没有模态特异性信息，因此我们利用重铸功能来指导表示学习，从而在最大程度上保持语义一致性。此外，在RSC中引入了功能增强机制（FA），以鼓励该模型学习多样性的更广泛的数据分布。最后，在五个广泛使用的数据集上进行的广泛实验，并与15个最先进的基线进行了比较，证明了我们DRCL的有效性和优势。

### HFMF: Hierarchical Fusion Meets Multi-Stream Models for Deepfake Detection 
[[arxiv](https://arxiv.org/abs/2501.05631)] [[cool](https://papers.cool/arxiv/2501.05631)] [[pdf](https://arxiv.org/pdf/2501.05631)]
> **Authors**: Anant Mehta,Bryant McArthur,Nagarjuna Kolloju,Zhengzhong Tu
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: This work is accepted to WACV 2025 Workshop on AI for Multimedia Forensics & Disinformation Detection. Code is available at: https://github.com/taco-group/HFMF
- **标题**: HFMF：层次融合符合多流模型以进行深泡沫检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 深层生成模型的快速进步导致创造了令人难以置信的逼真的合成图像，这些综合图像越来越难以区分现实世界数据。各种模型，扩散模型和生成的对抗网络的广泛使用使产生令人信服的假图像和视频变得更加容易，这对检测和减轻错误信息传播的扩散构成了重大挑战。结果，开发用于检测AI生成的假货的有效方法已成为一个紧迫的问题。在我们的研究中，我们提出了HFMF，这是一个全面的两阶段深层检测框架，利用层次跨模式融合和多流式特征提取，以增强对先进生成AI模型产生的图像的检测性能。我们方法的第一个组成部分通过分层特征融合机制整合了视觉变压器和卷积网。我们框架的第二个组成部分结合了对象级信息和微调的卷积净模型。然后，我们通过集合深神经网融合了两个组件的输出，从而实现了强大的分类性能。我们证明，我们的体系结构在保持校准和互操作性的同时，在不同的数据集基准中实现了卓越的性能。

### ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding 
[[arxiv](https://arxiv.org/abs/2501.05452)] [[cool](https://papers.cool/arxiv/2501.05452)] [[pdf](https://arxiv.org/pdf/2501.05452)]
> **Authors**: Xingyu Fu,Minqian Liu,Zhengyuan Yang,John Corring,Yijuan Lu,Jianwei Yang,Dan Roth,Dinei Florencio,Cha Zhang
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: Project link: https://zeyofu.github.io/ReFocus/
- **标题**: 重新关注：视觉编辑作为结构化图像理解的思想链
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 结构化的图像理解，例如解释表和图表，需要从图像中的各种结构和文本进行战略性重新聚焦，从而形成推理顺序以得出最终答案。但是，当前的多模式大语言模型（LLMS）缺乏这种多名选择性的关注能力。在这项工作中，我们介绍了Repocus，这是一个简单而有效的框架，它通过通过代码在输入图像上执行视觉编辑，转移和完善其视觉焦点来使多模式LLMS具有生成“视觉思想”的能力。具体而言，Repous可以使多模式LLMS生成Python代码来调用工具并修改输入映像，顺序绘制绘制框，突出显示部分并掩盖了区域，从而增强视觉推理过程。我们尝试各种结构化图像理解涉及表和图表的任务。重新集中在很大程度上提高了GPT-4O的所有任务的性能而无需直观编辑，从而在表任务上的平均增益为11.0％，图表任务的平均增益为6.8％。我们对不同视觉编辑的效果进行了深入的分析，以及重新关注可以改善性能而无需引入其他信息的原因。此外，我们使用Repous收集了14K训练集，并证明与标准VQA数据相比，使用中间信息进行的视觉链可提供更好的监督，比使用QA Pairs训练的同一型号的平均增长率为8.0％，比COT达到2.6％。

### Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark 
[[arxiv](https://arxiv.org/abs/2501.05444)] [[cool](https://papers.cool/arxiv/2501.05444)] [[pdf](https://arxiv.org/pdf/2501.05444)]
> **Authors**: Yunzhuo Hao,Jiawei Gu,Huichen Will Wang,Linjie Li,Zhengyuan Yang,Lijuan Wang,Yu Cheng
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: MLLM可以推理多模式吗？艾玛：增强的多模式推理基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 有机地推理文本和图像的能力是人类智力的支柱，但是多模式大语言模型（MLLMS）执行此类多模式推理的能力仍然不足。现有的基准通常强调文本主导推理或依靠浅视觉提示，无法充分评估综合的视觉和文本推理。我们介绍了Emma（增强的多模式推理），这是针对数学，物理，化学和编码的有机多模式推理的基准测试。艾玛（Emma）任务需要高级跨模式推理，这些推理无法通过在每种模式中独立推理来解决，并为MLLMS推理功能提供了增强的测试套件。我们对Emma最新的MLLM的评估揭示了处理复杂的多模式和多步推理任务的重大局限性，即使采用了高级技术，例如经过思考的促进链和测试时间的计算缩放量表的表现不佳。这些发现强调了需要改进的多模式架构和训练范例，以弥合人类和模型推理之间多模式的差距。

### Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation 
[[arxiv](https://arxiv.org/abs/2501.05264)] [[cool](https://papers.cool/arxiv/2501.05264)] [[pdf](https://arxiv.org/pdf/2501.05264)]
> **Authors**: Jiaxuan Peng,Mengshi Qi,Dong Zhao,Huadong Ma
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 在人类姿势估计中迈向平衡的持续多模式学习
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 3D人姿势估计（3D HPE）已成为一个著名的研究主题，尤其是在基于RGB的方法领域。但是，RGB图像容易受到限制，例如对照明条件的敏感性和潜在的用户不适。因此，利用非侵入传感器的多模式传感正在引起人们的注意。然而，多模式3D HPE仍然面临挑战，包括模态失衡和持续学习的必要性。在这项工作中，我们为3D HPE介绍了一种新型的平衡持续多模式学习方法，该方法利用了RGB，LIDAR，MMWAVE和WIFI的力量。具体而言，我们提出了一种基于沙普利价值的贡献算法，以量化每种方式的贡献并确定模态不平衡。为了解决这种不平衡，我们采用了重新学习策略。此外，认识到原始数据容易受到噪声污染的影响，我们开发了一种新颖的DeNo，以持续的学习方法。这种方法结合了噪声识别和分离模块，以减轻噪声的不利影响，并与平衡的学习策略合作以增强优化。此外，采用自适应EWC机制来减轻灾难性遗忘。我们对广泛的多模式数据集MM-FI进行了广泛的实验，这证明了我们在增强3D姿势估计和减轻复杂场景中的灾难性遗忘方面的优越性。我们将发布我们的代码。

### FOCUS: Towards Universal Foreground Segmentation 
[[arxiv](https://arxiv.org/abs/2501.05238)] [[cool](https://papers.cool/arxiv/2501.05238)] [[pdf](https://arxiv.org/pdf/2501.05238)]
> **Authors**: Zuyao You,Lingyu Kong,Lingchen Meng,Zuxuan Wu
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 重点：朝向通用前景细分
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 前景细分是计算机视觉中的一项基本任务，包括各种细分任务。以前的研究通常为每个任务设计了特定于任务的架构，导致缺乏统一。此外，他们主要专注于识别前景对象，而没有有效地将它们与背景区分开。在本文中，我们强调了背景的重要性及其与前景的关系。我们介绍了焦点，这是可以处理多个前景任务的前景对象通用细分框架。我们使用对象的边缘信息开发了多尺度语义网络，以增强图像特征。为了实现边界感知的分割，我们提出了一种新颖的蒸馏方法，集成了对比度学习策略，以完善多模式特征空间中的预测掩模。我们对5个任务的总共13个数据集进行了广泛的实验，结果表明，重点在大多数指标上始终优于最先进的任务特定模型。

### Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.05179)] [[cool](https://papers.cool/arxiv/2501.05179)] [[pdf](https://arxiv.org/pdf/2501.05179)]
> **Authors**: Xuyang Liu,Ziming Wang,Yuhang Han,Yingyao Wang,Jiale Yuan,Jun Song,Bo Zheng,Linfeng Zhang,Siteng Huang,Honggang Chen
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 全局压缩指挥官：高分辨率大视觉模型的插件推理加速度
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉模型（LVLM）在视觉理解和推理方面表现出色，但是由于处理较长的多模式环境时的二次复杂性而面临效率挑战。尽管令牌压缩技术可以降低计算成本，但现有方法是为单视图LVLM设计的，并且无法考虑使用动态瓷砖的最近高分辨率LVLM的独特多视图特征。尽管现有方法统一地对待所有令牌，但我们的分析表明，全球缩略图可以自然可以通过为信息评估提供整体背景来指导当地作物的压缩。在本文中，我们首先全面分析了动态平铺策略，揭示了缩略图和作物之间的互补性质以及不同农作物之间的独特特征。根据我们的观察，我们提出了“全球压缩指挥官”（即Globalcom $^2 $），这是一个新颖的HR-LVLMS插件播放令牌压缩框架。 Globalcom $^2 $利用缩略图作为“指挥官”来指导本地作物的压缩过程，从而自适应地保留了信息的细节，同时消除了冗余。广泛的实验表明，GlobalCom $^2 $在压缩90 \％的视觉令牌，将拖鞋和峰值内存降低到9.1 \％和60 \％的同时，保持超过90％\％的性能。我们的代码可在https://github.com/xuyang-liu16/globalcom2上找到。

### Optimizing Multitask Industrial Processes with Predictive Action Guidance 
[[arxiv](https://arxiv.org/abs/2501.05108)] [[cool](https://papers.cool/arxiv/2501.05108)] [[pdf](https://arxiv.org/pdf/2501.05108)]
> **Authors**: Naval Kishore Mehta,Arvind,Shyam Sunder Prasad,Sumeet Saurav,Sanjay Singh
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 通过预测行动指导优化多任务工业流程
- **领域**: 计算机视觉和模式识别
- **摘要**: 监视复杂的组装过程对于维持生产力和确保符合组装标准至关重要。但是，人类行动和主观任务偏好的可变性使准确的任务预期和指导变得复杂。为了应对这些挑战，我们介绍了多模式变压器融合和经常性单元（MMTFRU）网络，以进行自我中心的活动预期，利用多模式融合来提高预测准确性。该系统与操作员操作监控单元（OAMU）集成在一起，提供了主动的操作员指导，从而防止了组装过程中的偏差。 OAMU采用两种策略：（1）前5个MMTF-RU预测以及参考图和动作词典的结合； （2）与参考图集成的TOP-1 MMTF-RU预测，用于检测序列偏差并通过熵信息的置信机制预测异常得分。我们还介绍了时间加权的序列准确性（TWSA），以评估操作员效率并确保及时完成任务。我们的方法在工业Meccano数据集和Larges-Cale Epic-Kitchens-55数据集上得到了验证，这证明了其在动态环境中的有效性。

### Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset 
[[arxiv](https://arxiv.org/abs/2501.05098)] [[cool](https://papers.cool/arxiv/2501.05098)] [[pdf](https://arxiv.org/pdf/2501.05098)]
> **Authors**: Yuhong Zhang,Jing Lin,Ailing Zeng,Guanlin Wu,Shunlin Lu,Yurong Fu,Yuanhao Cai,Ruimao Zhang,Haoqian Wang,Lei Zhang
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: 17 pages, 14 figures, This work extends and enhances the research published in the NeurIPS 2023 paper, "Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset". arXiv admin note: substantial text overlap with arXiv:2307.00818
- **标题**: Motion-X ++：大规模多模式3D全身运动数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们介绍了Motion-X ++，这是一种大规模的多模式3D表达全身运动数据集。现有的运动数据集主要捕获只有身体的姿势，缺乏面部表情，手势和细粒度的姿势描述，通常仅限于具有手动标记文本描述的实验室设置，从而限制了其可扩展性。为了解决此问题，我们开发了可扩展的注释管道，可以自动从RGB视频中捕获3D全身运动和全面的纹理标签，并构建包括81.1k文本 - 文本 - 动作对的Motion-X数据集。此外，我们通过改善注释管道，引入更多数据模式并扩大数据量来扩展运动-X ++。 Motion-X ++提供了19.5m 3D全身姿势注释，涵盖了来自大型场景的120.5k运动序列，80.8K RGB视频，45.3k Audios，19.5m帧级全身姿势描述和120.5K序列序列序列级别的语义标签。全面的实验验证了我们的注释管道的准确性，并通过配对的多模式标签来产生表现力，精确和自然运动的重要好处，并配对多模式标签，支持多个下游任务，包括文本驱动的全身运动产生，包括音频驱动的运动驱动的运动，3D全部身体的人类网格恢复和2D全部恢复，以及2D全部恢复，以及2D全部身体恢复和2D全部Boty keptation estation estection extection estection extection extection eckeption is ectection extection extection-ectection-ectection，3D全部男子恢复和2D全部身体恢复和2D全部效果。

### LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.05067)] [[cool](https://papers.cool/arxiv/2501.05067)] [[pdf](https://arxiv.org/pdf/2501.05067)]
> **Authors**: Jiaxing Zhao,Boyuan Sun,Xiang Chen,Xihan Wei,Qibin Hou
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: llava-octopus：解锁指令驱动的自适应投影仪融合用于视频理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在本文中，我们介绍了Llava-Octopus，这是一种新颖的视频多模式模型。 llava-octopus根据用户说明从不同的视觉投影仪中自适应权重，使我们能够利用每个投影仪的互补优势。我们观察到，在处理特定任务时，不同的视觉投影仪表现出不同的特征。例如，有些投影仪擅长捕获静态细节，而另一些则更有效地处理时间信息，并且有些更适合需要时间连贯性的任务。通过根据用户说明动态调整特征权重，LLAVA-OCTOPUS动态选择并结合了最合适的功能，从而显着增强了该模型在多模式任务中的性能。实验结果表明，Llava-Octopus在多个基准测试中实现了出色的性能，尤其是在多模式理解，视觉问题答案和视频理解等任务中，突出了其广泛的应用潜力。

### ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark 
[[arxiv](https://arxiv.org/abs/2501.05031)] [[cool](https://papers.cool/arxiv/2501.05031)] [[pdf](https://arxiv.org/pdf/2501.05031)]
> **Authors**: Ronghao Dang,Yuqian Yuan,Wenqi Zhang,Yifei Xin,Boqiang Zhang,Long Li,Liuyi Wang,Qinyang Zeng,Xin Li,Lidong Bing
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 迪士座：多模式基础模型可以理解以自我为中心的世界吗？整体体现的认知基准
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 大型视觉模型（LVLM）在机器人中概括的增强越来越明显。因此，基于以自我为中心的视频的LVLM的具体认知能力引起了人们的极大兴趣。但是，当前用于体现视频问题的数据集回答缺乏全面和系统的评估框架。很少解决关键的体现认知问题，例如机器人自我认知，动态场景和幻觉。为了应对这些挑战，我们提出了Ecbench，这是一种高质量的基准测试，旨在系统地评估LVLM的具体认知能力。 e蚀具有各种各样的场景视频源，开放和多样化的问题格式以及30个体现认知的维度。为了确保质量，平衡和高视觉依赖性，Ecbench使用与班级无关的人类注释和多轮问题筛选策略。此外，我们介绍了Eceval，这是一个全面的评估系统，可确保指标的公平性和合理性。利用档案，我们对专有，开源和特定于任务的LVLM进行了广泛的评估。躁郁症在推进LVLM的具体认知能力方面至关重要，为为体现剂开发可靠的核心模型奠定了坚实的基础。所有数据和代码均可在https://github.com/rh-dang/ecbench上找到。

### V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer 
[[arxiv](https://arxiv.org/abs/2501.04975)] [[cool](https://papers.cool/arxiv/2501.04975)] [[pdf](https://arxiv.org/pdf/2501.04975)]
> **Authors**: Hangzhou He,Lei Zhu,Xinliang Zhang,Shuang Zeng,Qian Chen,Yanye Lu
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: Accepted by AAAI2025
- **标题**: V2C-CBM：构建概念瓶颈，具有视力到概念令牌
- **领域**: 计算机视觉和模式识别
- **摘要**: 概念瓶颈模型（CBMS）通过最初将图像转化为可易读的概念，提供了固有的可解释性，然后是这些概念的线性组合进行分类。但是，视觉识别任务的概念注释需要广泛的专家知识和劳动，从而限制了CBM的广泛采用。最近的方法利用了大语言模型的知识来构建概念瓶颈，多模型之类的模型随后将图像特征映射到概念特征空间中进行分类。尽管如此，语言模型产生的概念可能是冗长的，并且可能引入非视觉属性，这会损害准确性和解释性。在这项研究中，我们通过直接从多模型模型构建CBM来研究这些问题。为此，我们采用常用词作为基本概念词汇，并利用辅助图像辅助图像来构建视觉概念（V2C）令牌，可以将图像显式地量化为其最相关的视觉概念，从而创建一个面向视觉的概念瓶颈与多模型模型密切相关。这导致了我们的V2C-CBM，该V2C-CBM可以高效地训练且可解释。我们的V2C-CBM在各种视觉分类基准上匹配或胜过LLM监督的CBM，从而验证了我们方法的疗效。

### LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.06986)] [[cool](https://papers.cool/arxiv/2501.06986)] [[pdf](https://arxiv.org/pdf/2501.06986)]
> **Authors**: Mozhgan Nasr Azadani,James Riddell,Sean Sedwards,Krzysztof Czarnecki
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 狮子座：增强视觉编码器的多模式大语言模型的混合物
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 增强的视觉理解是多模式大语言模型（MLLM）的基石。最近的混合MLLM结合了视觉专家的混合物，以解决使用单一视觉编码器和过长的视觉令牌的局限性。尽管这些MLLM取得了进展，但研究差距仍在有效地整合各种视力编码器。 This work explores fusion strategies of visual tokens for hybrid MLLMs, leading to the design of LEO, a novel MLLM with a dual-branch vision encoder framework that incorporates a post-adaptation fusion strategy and adaptive tiling: for each segmented tile of the input images, LEO sequentially interleaves the visual tokens from its two vision encoders.对13个视觉基准的广泛评估表明，在大多数任务上，LEO优于最先进的开源MLLM和混合MLLM。此外，我们表明LEO可以适应自动驾驶的专业领域，而无需更改模型架构或训练配方，而与现有基线相比，实现了竞争性能。代码和模型将公开可用。

### CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications 
[[arxiv](https://arxiv.org/abs/2501.06927)] [[cool](https://papers.cool/arxiv/2501.06927)] [[pdf](https://arxiv.org/pdf/2501.06927)]
> **Authors**: Xinyi Zheng,Steve Zhang,Weizhe Lin,Aaron Zhang,Walterio W. Mayol-Cuevas,Junxiao Shen
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 文化3D：3D应用的文化地标和地形数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们使用来自全球位置捕获的高分辨率图像提出了一个大规模的细粒数据集。与现有数据集相比，我们的数据集提供了更大的尺寸，并包括更高级别的细节，使其非常适合细粒3D应用程序。值得注意的是，我们的数据集是使用无人机捕获的空中图像构建的，该数据集提供了更准确的观点，用于捕获现实世界的场地布局和建筑结构。通过使用这些详细图像重建环境，我们的数据集支持应用程序，例如用于高斯分裂的Colmap格式和结构 - 胶合（SFM）方法。它与广泛使用的技术兼容，包括SLAM，多视图立体声和神经辐射场（NERF），启用了准确的3D重建和点云。这使其成为重建和细分任务的基准。该数据集可与多模式数据无缝集成，并支持从建筑重建到虚拟旅游的一系列3D应用程序。它的灵活性促进了创新，促进了3D建模和分析中的突破。

### X-LeBench: A Benchmark for Extremely Long Egocentric Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.06835)] [[cool](https://papers.cool/arxiv/2501.06835)] [[pdf](https://arxiv.org/pdf/2501.06835)]
> **Authors**: Wenqi Zhou,Kai Cao,Hao Zheng,Xinyi Zheng,Miao Liu,Per Ola Kristensson,Walterio Mayol-Cuevas,Fan Zhang,Weizhe Lin,Junxiao Shen
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: X-Lebench：非常长的以自我为中心的视频理解的基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 长期以来的自我为中心的视频理解为长期人类行为提供了丰富的上下文信息和独特的见解，在体现智能，长期活动分析和个性化的辅助技术中具有巨大的应用潜力。但是，现有的基准数据集主要关注单个，短期视频或中度长的视频，多达数十分钟，在评估广泛的，超长的中心视频录制时留下了很大的差距。为了解决这个问题，我们介绍了X-Lebench，这是一种专门制作的基准数据集，该数据集专门用于评估以漫长的以自我为中心的视频记录的任务。 X-Lebench利用大型语言模型（LLMS）的高级文本处理功能（LLMS）开发了一条命运的模拟管道，该管道产生了与现实世界视频数据一致的现实，连贯的每日计划。这种方法使综合日常计划与来自EGO4D-A大规模的EgeCentric视频数据集的真实录像可以灵活地集成，其中涵盖了432个模拟视频生活日志中各种各样的日常生活场景，这些日志在上下文丰富的场景中镜像现实的日常活动。视频寿命持续时间从23分钟到16.4小时。对几种基线系统和多模式大型语言模型（MLLM）的评估揭示了它们的整体表现不佳，强调了长期以来的以自我为中心的视频理解的固有挑战，并强调了对更高级模型的需求。

### GeoPix: Multi-Modal Large Language Model for Pixel-level Image Understanding in Remote Sensing 
[[arxiv](https://arxiv.org/abs/2501.06828)] [[cool](https://papers.cool/arxiv/2501.06828)] [[pdf](https://arxiv.org/pdf/2501.06828)]
> **Authors**: Ruizhe Ou,Yuan Hu,Fan Zhang,Jiaxin Chen,Yu Liu
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: Geopix：用于像素级图像的多模式大语模型在遥感中的理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大型语言模型（MLLM）在图像和区域级遥感（RS）图像理解任务（例如图像字幕，视觉问题答录和视觉接地）方面取得了显着成功。但是，现有的RS MLLM缺乏像素级对话能力，这涉及对用户说明响应用户说明，并使用细分掩码进行特定实例。在本文中，我们提出了Geopix，RS MLLM将图像理解功能扩展到像素级别。这是通过将MLLM配备蒙版预测器来实现的，该预测因子将视觉特征从视觉编码器转换为LLM分割令牌嵌入条件上的掩模。为了促进RS图像中多尺度对象的分割，将班级可学习的内存模块集成到了掩码预测器中，以在整个数据集中的实例级别捕获和存储class-geo-context。此外，为了解决用于训练像素级RS MLLM的大型数据集，我们构建了geopixinstruct数据集，其中包括65,463张图像和140,412个实例，每个实例都带有文本描述，边界框和掩码。此外，我们制定了一个两阶段的培训策略，以平衡文本生成的不同要求，并掩盖了多模式多任务优化中的预测。广泛的实验验证了Geopix在像素级分割任务中的有效性和优越性，同时还保持了图像和区域级别基准中的竞争性能。

### RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation Models 
[[arxiv](https://arxiv.org/abs/2501.06809)] [[cool](https://papers.cool/arxiv/2501.06809)] [[pdf](https://arxiv.org/pdf/2501.06809)]
> **Authors**: Keyan Chen,Jiafan Zhang,Chenyang Liu,Zhengxia Zou,Zhenwei Shi
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: RSREFSEG：使用基础模型参考遥感图像分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 参考遥感图像分割对于通过自由格式的文本输入来实现细粒度的视觉理解至关重要，从而在遥感应用程序中实现了增强的场景和对象提取。当前的研究主要利用预训练的语言模型编码文本描述并与视觉方式保持一致，从而促进相关视觉特征的表达。但是，这些方法通常很难在细粒度的语义概念之间建立强大的一致性，从而导致文本和视觉信息之间的表示不一致。为了解决这些局限性，我们引入了引用的遥感图像分割基础模型RSREFSEG。 RSREFSEG利用剪辑进行视觉和文本编码，使用全球和本地文本语义作为过滤器，以在潜在空间中生成与参考相关的视觉激活功能。然后，这些激活的功能是SAM的输入提示，该提示通过其强大的视觉概括功能来完善分割掩模。 RRSIS-D数据集的实验结果表明，RSREFSEG的表现优于现有方法，强调了基础模型在增强多模式任务理解方面的有效性。该代码可在\ url {https://github.com/kyanchen/rsrefseg}中获得。

### Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints 
[[arxiv](https://arxiv.org/abs/2501.06710)] [[cool](https://papers.cool/arxiv/2501.06710)] [[pdf](https://arxiv.org/pdf/2501.06710)]
> **Authors**: Ming Dai,Jian Li,Jiedong Zhuang,Xian Zhang,Wankou Yang
> **First submission**: 2025-01-11
> **First announcement**: 2025-01-13
> **comment**: AAAI2025
- **标题**: 多任务视觉接地具有粗到最高的一致性约束
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多任务视觉接地涉及基于文本表达式在图像中同时执行本地化和分割。大多数高级方法主要集中于基于变压器的多模式融合，旨在提取强大的多模式表示。但是，参考表达理解（REC）与参考图像分割（RIS）之间的歧义是容易出错的，导致多任务预测之间的不一致。此外，多模式理解不足直接导致目标感知。为了克服这些挑战，我们提出了一个粗到最新的一致性约束视觉接地架构（$ \ text {c}^3 \ text {vg} $），该结构在两个阶段的框架内集成了隐式和明确的建模方法。最初，使用查询和像素解码器来生成初步检测和分割输出，该过程称为粗糙的语义感知（RSP）阶段。这些粗糙的预测随后通过所提出的掩模引导的相互作用模块（MIM）和新型的明确双向一致性约束损失来完善，以确保跨任务的一致表示，我们将其称为精制一致性相互作用（RCI）阶段。此外，为了应对不足的多峰理解的挑战，我们利用基于视觉语言融合表示的预训练模型。对reccoco，refcoco+和reccocog数据集的经验评估证明了$ \ text {c}^3 \ text {vg} $的功效和声音性，这显着超过了最先进的rec和ris方法，并以实质性的范围进行了RIS方法。代码和模型将在\ url {https://github.com/dmmm1997/c3vg}上提供。

### CoreNet: Conflict Resolution Network for Point-Pixel Misalignment and Sub-Task Suppression of 3D LiDAR-Camera Object Detection 
[[arxiv](https://arxiv.org/abs/2501.06550)] [[cool](https://papers.cool/arxiv/2501.06550)] [[pdf](https://arxiv.org/pdf/2501.06550)]
> **Authors**: Yiheng Li,Yang Yang,Zhen Lei
> **First submission**: 2025-01-11
> **First announcement**: 2025-01-13
> **comment**: Accepted by Information Fusion 2025
- **标题**: Corenet：用于点像素未对准的冲突解决网络和3D激光镜对象检测的子任务抑制
- **领域**: 计算机视觉和模式识别
- **摘要**: 融合来自不同传感器的多模式输入是提高3D对象检测性能的有效方法。但是，当前的方法忽略了两个重要的冲突：点像素错位和子任务抑制。前者意味着来自不透明对象的像素特征被投影到世界空间中同一射线的多个点特征，而后者表示分类预测和边界框回归可能会导致相互抑制。在本文中，我们提出了一种名为“冲突解决网络”（Corenet）的新方法，以解决上述问题。具体而言，我们首先提出了一个双流变换模块，以解决点像素未对准。它由基于射线和基于点的2d-bev变换组成。他们俩都实现了从图像空间到世界空间的大约独特映射。此外，我们引入了一个特定于任务的预测指标来解决子任务抑制。它使用双分支结构，该结构采用特定于类的查询和Bbox特定查询到相应的子任务。每个特定于任务的查询都是由特定于任务的功能和一般功能构建的，该功能允许头部根据不同的子任务自适应地选择感兴趣的信息。大规模Nuscenes数据集的实验证明了我们提出的Corenet的优越性，通过在没有测试时间增强和模型集合技术的情况下实现75.6 \％NDS和73.3 \％MAP。充足的消融研究还证明了每个组件的有效性。该代码在https://github.com/liyih/corenet上发布。

### Natural Language Supervision for Low-light Image Enhancement 
[[arxiv](https://arxiv.org/abs/2501.06546)] [[cool](https://papers.cool/arxiv/2501.06546)] [[pdf](https://arxiv.org/pdf/2501.06546)]
> **Authors**: Jiahui Tang,Kaihua Zhou,Zhijian Luo,Yueen Hou
> **First submission**: 2025-01-11
> **First announcement**: 2025-01-13
> **comment**: 12 pages, 10 figures
- **标题**: 弱光图像增强的自然语言监督
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着深度学习的发展，众多用于低光图像增强的方法（LLIE）表现出了显着的性能。主流LLIE方法通常会根据一对低光和正常光图像学习端到端映射。但是，在不同的照明条件下的正常光图像作为参考图像，因此很难定义``完美''参考图像，这导致了调和面向公制的和视觉友好的结果的挑战。最近，许多跨模式研究发现，来自其他相关模式的附带信息可以指导视觉表示学习。基于此，我们引入了一种自然语言监督（NLS）策略，该策略从与图像相对应的文本中学习特征地图，提供了一个通用且灵活的界面，用于描述不同照明下的图像。但是，以文本描述为条件的图像分布是高度多模式的，这使训练变得困难。为了解决这个问题，我们设计了一种文本指导调节机制（TCM），该机制结合了图像区域和句子单词之间的连接，增强了捕获图像和文本的细粒跨模式提示的能力。该策略不仅利用了更广泛的监督资源，而且还基于视觉和文本功能对齐为LLIE提供了新的范式。为了有效地识别和合并各个级别的图像和文本信息的特征，我们设计了一个信息融合注意（IFA）模块，以增强不同级别的不同区域。我们将拟议的TCM和IFA集成到名为Nalsuper的Llie的自然语言监督网络中。最后，广泛的实验证明了我们提出的Nalsuper的鲁棒性和卓越有效性。

### Enhancing Multi-Modal Video Sentiment Classification Through Semi-Supervised Clustering 
[[arxiv](https://arxiv.org/abs/2501.06475)] [[cool](https://papers.cool/arxiv/2501.06475)] [[pdf](https://arxiv.org/pdf/2501.06475)]
> **Authors**: Mehrshad Saadatinia,Minoo Ahmadi,Armin Abdollahi
> **First submission**: 2025-01-11
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 通过半监督聚类增强多模式视频情感分类
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 了解视频中的情绪是一项艰巨的任务。但是，视频包含几种方式，使它们成为机器学习和深度学习任务的丰富数据来源。在这项工作中，我们旨在通过关注两个关键方面：视频本身，随附的文本和声学特征来改善视频情感分类。为了解决依赖大型标记数据集的局限性，我们正在开发一种方法，该方法利用基于群集的半监督预训练来从数据中提取有意义的表示形式。此预训练步骤确定了视频和文本数据中的模式，从而使模型可以学习基础结构和关系，而无需一开始就需要广泛的标记信息。一旦建立了这些模式，我们将以监督的方式微调系统，以对视频中表达的情感进行分类。我们认为，这种多模式方法将聚类与监督的微调结合在一起，将导致更准确和有见地的情感分类，尤其是在标记数据有限的情况下。

### Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs 
[[arxiv](https://arxiv.org/abs/2501.06430)] [[cool](https://papers.cool/arxiv/2501.06430)] [[pdf](https://arxiv.org/pdf/2501.06430)]
> **Authors**: Shan Zhang,Aotian Chen,Yanpeng Sun,Jindong Gu,Yi-Yu Zheng,Piotr Koniusz,Kai Zou,Anton van den Hengel,Yuan Xue
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 睁开眼睛，然后原因：MLLM中的细粒度视觉数学理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的多模式大型语言模型（MLLM）通常在需要细粒度的视觉理解的数学解决任务上表现不佳。该限制在很大程度上归因于图像级对比度预训练期间对几何原始物的感知不足（例如，剪辑）。尽管最近改善数学MLLM的努力集中在扩展数学视觉指导数据集并采用更强大的LLM骨架上，但他们经常忽略视觉识别的持续错误。在本文中，我们系统地评估了最先进的MLLM的视觉接地能力，并揭示了视觉接地精度和解决问题的性能之间的显着负相关性，从而强调了细粒度视觉理解的关键作用。值得注意的是，在识别几何实体时，像GPT-4O这样的高级模型在识别几何实体时表现出70％的错误率，强调这仍然是视觉数学推理中的关键瓶颈。为了解决这个问题，我们提出了一种新颖的方法，即SVE-MATH（选择性视觉增强的数学MLLM），其中具有几何感应的视觉编码器和功能路由器，该路由器动态调整了层次视觉特征图的贡献。我们的模型识别准确的视觉原始图，并生成根据语言模型的推理需求量身定制的精确视觉提示。在实验中，SVE-MATH-QWEN2.5-7B在Mathverse上的表现优于其他7B模型，并且与Mathvista上的GPT-4V兼容。尽管在较小的数据集上接受过培训，但SVE-Math-7B在GEOQA上取得了竞争性能，但在更大的数据集中进行了竞争模型。我们的发现强调了将细粒度的视觉理解纳入MLLM的重要性，并为未来的研究提供了有希望的方向。

### Generative AI for Cel-Animation: A Survey 
[[arxiv](https://arxiv.org/abs/2501.06250)] [[cool](https://papers.cool/arxiv/2501.06250)] [[pdf](https://arxiv.org/pdf/2501.06250)]
> **Authors**: Yunlong Tang,Junjia Guo,Pinxin Liu,Zhiyuan Wang,Hang Hua,Jia-Xing Zhong,Yunzhong Xiao,Chao Huang,Luchuan Song,Susan Liang,Yizhi Song,Liu He,Jing Bi,Mingqian Feng,Xinyang Li,Zeliang Zhang,Chenliang Xu
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-13
> **comment**: 20 pages
- **标题**: 生成AI用于Cel-Animation：调查
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **摘要**: 传统的赛璐oid（CEL）动画生产管道包括多个基本步骤，包括故事板，布局设计，密钥帧动画，互联网和着色，它们需要大量的手动工作，技术专业知识和大量的时间投资。这些挑战历史上阻碍了Cel-Animation生产的效率和可扩展性。生成人工智能（Genai）的兴起，包括大型语言模型，多模式模型和扩散模型，通过自动化任务（例如框架的生成，着色和故事板创建）来提供创新的解决方案。这项调查探讨了Genai整合如何通过降低技术障碍，通过Anidoc，Tooncrafter和Anisora等工具来拓宽技术障碍，扩大更广泛的创作者的可访问性，并使艺术家能够更多地专注于创造性的表达和艺术创新。尽管具有潜力，但仍保持视觉一致性，确保风格连贯性以及解决道德考虑的问题继续构成挑战。此外，本文讨论了未来的方向，并探讨了AI辅助动画的潜在进步。有关进一步的探索和资源，请访问我们的GitHub存储库：https：//github.com/yunlong10/awesome-ai4animation

### Detection, Retrieval, and Explanation Unified: A Violence Detection System Based on Knowledge Graphs and GAT 
[[arxiv](https://arxiv.org/abs/2501.06224)] [[cool](https://papers.cool/arxiv/2501.06224)] [[pdf](https://arxiv.org/pdf/2501.06224)]
> **Authors**: Wen-Dong Jiang,Chih-Yung Chang,Diptendu Sinha Roy
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-13
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: 检测，检索和解释统一：基于知识图和GAT的暴力检测系统
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近，使用统一的多峰模型开发的暴力检测系统已取得了巨大的成功，并引起了广泛的关注。但是，这些系统中的大多数面临两个关键挑战：缺乏作为黑框模型和有限功能的可解释性，仅提供分类或检索功能。为了应对这些挑战，本文提出了一种新型的可解释的暴力检测系统，称为三合一（TIO）系统。 TIO系统集成了知识图（KG）和图形注意网络（GAT），以提供三个核心功能：检测，检索和解释。具体而言，系统处理每个视频框架以及大型语言模型（LLM）生成的文本描述，用于包含潜在暴力行为的视频。它采用ImageBind来生成高维嵌入来构建知识图，使用GAT进行推理，并应用轻巧的时间序列模块来提取视频嵌入功能。最后一步连接了用于多功能输出的分类器和检索器。 KG的解释性使系统能够验证每个输出背后的推理过程。此外，本文还引入了几种轻巧的方法，以减少TIO系统的资源消耗并提高其效率。对XD暴力和UCF-Crime数据集进行的广泛实验验证了所提出的系统的有效性。案例研究进一步揭示了一种有趣的现象：随着旁观者的数量增加，暴力行为的发生往往会减少。

### Fitting Different Interactive Information: Joint Classification of Emotion and Intention 
[[arxiv](https://arxiv.org/abs/2501.06215)] [[cool](https://papers.cool/arxiv/2501.06215)] [[pdf](https://arxiv.org/pdf/2501.06215)]
> **Authors**: Xinger Li,Zhiqiang Zhong,Bo Huang,Yang Yang
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 拟合不同的互动信息：情感和意图的联合分类
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习,多媒体,音频和语音处理
- **摘要**: 本文是ICASSP Meiju@2025 Track I的第一名解决方案，该曲目侧重于低资源多模式情感和意图识别。如何有效利用大量未标记的数据，同时确保在交互阶段相互促进不同的难度级别任务，这两个点成为竞争的关键。在本文中，在训练有标记的数据的模型上进行了伪标签标签，并选择了高信心的样品及其标签以减轻资源低的问题。同时，在实验中发现的意图识别能力的特征是在不同的注意力头下与情绪识别相互促进的特征，并且通过融合实现了更高的意图识别表现。最后，在精致的处理数据下，我们在测试集中达到了0.5532的得分，并赢得了赛道的冠军。

### LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs 
[[arxiv](https://arxiv.org/abs/2501.06186)] [[cool](https://papers.cool/arxiv/2501.06186)] [[pdf](https://arxiv.org/pdf/2501.06186)]
> **Authors**: Omkar Thawakar,Dinura Dissanayake,Ketan More,Ritesh Thawkar,Ahmed Heakl,Noor Ahsan,Yuhao Li,Mohammed Zumri,Jean Lahoud,Rao Muhammad Anwer,Hisham Cholakkal,Ivan Laptev,Mubarak Shah,Fahad Shahbaz Khan,Salman Khan
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: 15 pages, 5 Figures
- **标题**: Llamav-O1：在LLMS中重新思考逐步视觉推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 推理是解决复杂的多步骤问题的基本能力，尤其是在顺序逐步理解至关重要的视觉环境中。现有方法缺乏评估视觉推理的全面框架，并且不强调逐步解决问题。为此，我们提出了一个综合框架，用于通过三个关键贡献在大语言模型（LMM）中逐步进行视觉推理。首先，我们介绍了专门设计用于评估多步推理任务的视觉推理基准。该基准提出了一系列挑战，其中八个不同的类别从复杂的视觉感知到科学推理，总共有超过4K的推理步骤，从而对LLMS的能力进行了强有力的评估，可以在多个步骤中执行准确且可解释的视觉推理。其次，我们提出了一个新颖的度量标准，该指标在单个步骤的粒度上评估视觉推理质量，强调正确性和逻辑连贯性。与传统的终执行任务准确度指标相比，拟议的指标为推理性能提供了更深入的见解。第三，我们提出了一种新的多模式视觉推理模型，名为Llamav-O1，该模型使用多步课程学习方法训练，其中逐步组织了任务，以促进逐步的技能获取和解决问题。拟议的Llamav-O1是为多步推理而设计的，并通过结构化的培训范式逐步学习。广泛的实验表明，我们的Llamav-O1胜过现有的开源模型，并且对封闭式专有模型表现出色。与最近的Llava-COT相比，我们的Llamav-O1的平均得分为67.3，绝对增益为3.8％\％，而在推理缩放期间的速度更快5倍。我们的基准，模型和代码公开可用。

### PEACE: Empowering Geologic Map Holistic Understanding with MLLMs 
[[arxiv](https://arxiv.org/abs/2501.06184)] [[cool](https://papers.cool/arxiv/2501.06184)] [[pdf](https://arxiv.org/pdf/2501.06184)]
> **Authors**: Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 和平：授权地质图与MLLM的整体理解
- **领域**: 计算机视觉和模式识别,多代理系统
- **摘要**: 地质图作为地质科学的基本图，为地球地下和表面的结构和组成提供了重要的见解。这些地图在各个领域都是必不可少的，包括灾难探测，资源探索和土木工程。尽管具有重要意义，但当前的多模式大语言模型（MLLM）通常在地质地图理解中缺乏。该差距主要是由于制图概括的挑战性质，涉及处理高分辨率图，管理多个相关组件以及需要特定领域的知识。为了量化这一差距，我们构建了GEOMAP基础，这是评估地质图理解中MLLM的首个基准，该基准评估了提取，参考，接地，推理和分析方面的全尺度能力。为了弥合这一差距，我们介绍了为地质图理解设计的开幕式代理，其中包含三个模块：分层信息提取（HIE），域知识注入（DKI）和及时增强的问题答案（PEQA）。受到人类科学家之间的跨学科合作的启发，AI专家小组充当顾问，利用多样化的工具池来全面分析问题。通过全面的实验，GEOMAP-AGENT在GEOMAP板凳上的总体得分为0.811，在GPT-4O中的表现明显优于0.369。我们的工作，通过MLLM赋予地质图的整体理解（和平），为地质上的高级AI应用铺平了道路，增强了地质研究的效率和准确性。

### BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response 
[[arxiv](https://arxiv.org/abs/2501.06019)] [[cool](https://papers.cool/arxiv/2501.06019)] [[pdf](https://arxiv.org/pdf/2501.06019)]
> **Authors**: Hongruixuan Chen,Jian Song,Olivier Dietrich,Clifford Broni-Bediako,Weihao Xuan,Junjue Wang,Xinlei Shao,Yimin Wei,Junshi Xia,Cuiling Lan,Konrad Schindler,Naoto Yokoya
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: BRIGHT：全球分布式的多模式建筑损害评估数据集，其全天候灾难响应非常高分辨率
- **领域**: 计算机视觉和模式识别,人工智能,图像和视频处理,信号处理
- **摘要**: 灾难事件发生在世界各地，并对人类的生命和财产造成重大破坏。地球观察（EO）数据可实现快速而全面的建筑物损害评估（BDA），这是灾难后减少人身伤亡并为救灾工作提供依据的基本能力。最近的研究重点是开发AI模型，以实现未见灾难事件的准确映射，主要是使用光学EO数据。但是，基于光学数据的解决方案仅限于晴朗的天空和日光时间，从而阻止了对灾难的迅速反应。整合多模式（MM）EO数据，尤其是光学图像的组合，可以提供全天候，晚上的灾难响应。尽管存在这种潜力，但稳健的多模式AI模型的发展受到缺乏合适的基准数据集的限制。在本文中，我们使用非常高分辨率的光学和SAR图像（BRIGHT）提出了一个BDA数据集，以支持基于AI的全天候灾难响应。据我们所知，BRIGHT是第一个开放式访问，全球分布式，事件多样的MM数据集，专门策划了基于AI的灾难响应。它涵盖了全球12个地区的五种自然灾害和两种人造灾难，特别关注最需要外部援助的发展中国家。明亮的光学和SAR图像在0.3-1米之间具有空间分辨率，提供了单个建筑物的详细表示，使其非常适合精确BDA。在我们的实验中，我们测试了七个经过明亮训练的高级AI模型，以验证可传递性和鲁棒性。数据集和代码可在https://github.com/chenhongruixuan/bright上找到。 Bright还作为2025 IEEE GRSS数据融合竞赛的官方数据集。

### Scalable Vision Language Model Training via High Quality Data Curation 
[[arxiv](https://arxiv.org/abs/2501.05952)] [[cool](https://papers.cool/arxiv/2501.05952)] [[pdf](https://arxiv.org/pdf/2501.05952)]
> **Authors**: Hongyuan Dong,Zijian Kang,Weijie Yin,Xiao Liang,Chao Feng,Jiao Ran
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 通过高质量数据策划可扩展视觉语言模型培训
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 在本文中，我们在2B和8B参数中介绍了开源视觉语言模型（VLM）系列的开源视觉语言模型（VLM）系列，介绍了Sail-VL（可扩展视觉语言模型培训）。以下三个关键改进有助于Sail-VL的领先表现：（1）可扩展的高质量视觉理解数据构建：我们实施数据构建管道以实现千万级的高质量恢复数据注释，并且与OpenSource替代方案相比，所得的数据集攻击量被验证为最高数据质量。 （2）具有高质量的视觉理解数据的可扩展预处理：我们将Sail-VL预算的预算扩展到655B代币，并表明即使是2B VLM也从扩展的培训数据尺寸中受益，在绩效后显示出预期的数据尺寸扩展法律，在视觉理解和表现后表现出预期的数据规模。 （3）通过数据数量和复杂性缩放可扩展的SFT：我们策划了高质量的SFT数据集收集，该集合的表现优于数据数量缩放效果的OpenSource替代方案。我们还证明，使用较高复杂性数据的培训超过了基线的一阶段训练。在我们的评估中，Sail-VL系列模型在18种广泛使用的VLM基准测试中获得了最高的平均得分，而2B模型则在OpenCompass 2024（https://rank.opencompass.org.org.cn/leaderboard-multimodal）上，在opencompass 2024上的VLM上排名最高的位置。 Sail-Vl系列模型在HuggingFace（https://huggingface.co/bytedancedouyincontent）上发布。

### A Multimodal Dataset for Enhancing Industrial Task Monitoring and Engagement Prediction 
[[arxiv](https://arxiv.org/abs/2501.05936)] [[cool](https://papers.cool/arxiv/2501.05936)] [[pdf](https://arxiv.org/pdf/2501.05936)]
> **Authors**: Naval Kishore Mehta,Arvind,Himanshu Kumar,Abeer Banerjee,Sumeet Saurav,Sanjay Singh
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: Accepted at the 20th International Conference on Human-Robot Interaction (HRI) 2025
- **标题**: 一个多模式数据集，用于增强工业任务监控和参与预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 在动态工业工作流程中检测和解释操作员的行动，参与度和对象相互作用仍然是人类机器人协作研究中的重大挑战，尤其是在复杂的现实世界环境中。传统的单峰方法通常无法捕获这些非结构化工业环境的复杂性。为了解决这一差距，我们提出了一种新型的多模式工业活动监控（MIAM）数据集，该数据集捕获了现实的组装和拆卸任务，从而促进了关键元任务的评估，例如行动定位，对象互动和参与预测。该数据集包括从22个会话收集的多视图RGB，深度和惯性测量单元（IMU）数据，总计290分钟的未修剪视频，详细注释了任务性能和操作员行为。它的独特性在于多种数据模式的整合及其对现实世界中未经修剪的工业工作流程键的重视，用于推进人类机器人协作和操作员监控的研究。此外，我们提出了一个融合RGB帧，IMU数据和骨架序列的多模式网络，以预测工业任务期间的参与水平。我们的方法提高了识别参与状态的准确性，为在动态工业环境中监测操作员的绩效提供了强大的解决方案。可以从https://github.com/navalkishoremehta95/miam/访问数据集和代码。

### Valley2: Exploring Multimodal Models with Scalable Vision-Language Design 
[[arxiv](https://arxiv.org/abs/2501.05901)] [[cool](https://papers.cool/arxiv/2501.05901)] [[pdf](https://arxiv.org/pdf/2501.05901)]
> **Authors**: Ziheng Wu,Zhenghao Chen,Ruipu Luo,Can Zhang,Yuan Gao,Zhentao He,Xian Wang,Haoran Lin,Minghui Qiu
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: Valley2：探索具有可扩展视觉语言设计的多模式
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，视觉模型取得了显着的进步，展示了在图像字幕和视频理解等各种任务中出色的功能。我们介绍了Valley2，这是一种新型的多模式大型语言模型，旨在提高所有领域的性能，并扩展电子商务和简短视频场景中实用应用的界限。值得注意的是，Valley2在电子商务基准上实现了最先进的（SOTA）性能，超过了相似大小的开源模型的大幅度（79.66 vs. 72.76）。此外，Valley2在少于10b参数的模型中排名第二的型号，平均得分为67.4。代码和型号的权重是https://github.com/bytedance/valley开源的。

### Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2501.05884)] [[cool](https://papers.cool/arxiv/2501.05884)] [[pdf](https://arxiv.org/pdf/2501.05884)]
> **Authors**: Dabing Cheng,Haosen Zhan,Xingchen Zhao,Guisheng Liu,Zemin Li,Jinghui Xie,Zhao Song,Weiguo Feng,Bingyue Peng
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: 16pages conference
- **标题**: 文本对编辑：可控的端到端视频广告通过多模式LLMS创建
- **领域**: 计算机视觉和模式识别
- **摘要**: 短视频内容的指数增长激发了有效，自动化解决方案进行视频编辑的必要性，这是由于需要了解视频并根据用户要求量身定制编辑的挑战。在满足这一需求时，我们提出了一个创新的端到端基础框架，最终实现了对最终视频内容编辑的精确控制。利用多模式大语言模型（MLLM）的灵活性和概括性，我们定义了清晰的输入输出映射以进行有效的视频创建。为了增强该模型在处理和理解视频内容方面的能力，我们介绍了较密集的框架速率和缓慢快速处理技术的战略组合，从而显着增强了对时间和空间视频信息的提取和理解。此外，我们引入了一种文本对编辑机制，该机制使用户可以通过文本输入实现所需的视频结果，从而增强了编辑视频的质量和可控性。通过全面的实验，我们的方法不仅在广告数据集中展示了重要的有效性，而且在公共数据集中得出了普遍适用的结论。

### VideoRAG: Retrieval-Augmented Generation over Video Corpus 
[[arxiv](https://arxiv.org/abs/2501.05874)] [[cool](https://papers.cool/arxiv/2501.05874)] [[pdf](https://arxiv.org/pdf/2501.05874)]
> **Authors**: Soyeong Jeong,Kangsan Kim,Jinheon Baek,Sung Ju Hwang
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: Videorag：通过视频语料库检索启动一代
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,信息检索,机器学习
- **摘要**: 检索增强的生成（RAG）是通过检索与查询相关的外部知识并将其纳入生成过程来提高模型的事实准确性的有力策略。但是，现有方法主要集中在文本上，最近有一些进步正在考虑图像，并且它们在很大程度上忽略了视频，这是一种丰富的多模式知识来源，能够比任何其他模式更有效地表示上下文细节。尽管最近的研究探讨了视频在响应生成中的使用，但它们要么预先定义与查询相关的视频，而无需检索，要么将视频转换为文本描述，而却失去了多模式丰富。为了解决这些问题，我们介绍了Videorag，该框架不仅可以根据查询的相关性动态检索视频，而且还利用了视觉和文字信息。 Videorag的操作由最近的大型视频语言模型（LVLMS）提供动力，该模型可以直接处理视频内容以代表检索和无缝集成与响应生成的查询共同检索的视频。另外，受到LVLMS的上下文大小的启发，可能不足以处理非常长的视频中的所有帧，并且并非所有帧都很重要，我们引入了视频框架选择机制来提取框架的最有信息的子集，以及从视频中提取文本信息的策略（在视频中可以帮助视频内容的理解时）。我们在实验中验证了视频的有效性，并表明它优于相关基线。代码可在https://github.com/starsuzi/videorag上找到。

### Poetry in Pixels: Prompt Tuning for Poem Image Generation via Diffusion Models 
[[arxiv](https://arxiv.org/abs/2501.05839)] [[cool](https://papers.cool/arxiv/2501.05839)] [[pdf](https://arxiv.org/pdf/2501.05839)]
> **Authors**: Sofia Jamil,Bollampalli Areen Reddy,Raghvendra Kumar,Sriparna Saha,K J Joseph,Koustava Goswami
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 像素中的诗歌：通过扩散模型提示为诗图像生成诗
- **领域**: 计算机视觉和模式识别
- **摘要**: 当应用于文学作品，尤其是诗歌时，文本到图像生成的任务遇到了重大挑战。诗是一种独特的文学形式，其含义经常超越字面上的话。为了解决这一缺点，我们提出了一个诗歌框架，旨在生成在视觉上代表诗歌固有含义的图像。我们的方法将迅速调整的概念包含在我们的图像生成框架中，以确保所得图像与诗意内容紧密相符。此外，我们提出了Poekey算法，该算法以情感，视觉元素和主题的形式提取三个关键要素，以形成指令，这些指令随后提供给生成相应图像的扩散模型。此外，为了扩大诗歌数据集的多样性在不同的流派和年龄段中，我们引入了Minipo，这是一种新型的多模式数据集，其中包括1001个儿童诗歌和图像。利用该数据集与诗歌一起，我们使用诗歌框架对图像生成进行了定量和定性评估。本文展示了我们方法的有效性，并为从文学来源产生图像提供了新的视角。

### TB-Bench: Training and Testing Multi-Modal AI for Understanding Spatio-Temporal Traffic Behaviors from Dashcam Images/Videos 
[[arxiv](https://arxiv.org/abs/2501.05733)] [[cool](https://papers.cool/arxiv/2501.05733)] [[pdf](https://arxiv.org/pdf/2501.05733)]
> **Authors**: Korawat Charoenpitaks,Van-Quang Nguyen,Masanori Suganuma,Kentaro Arai,Seiji Totsuka,Hiroshi Ino,Takayuki Okatani
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: Main Paper: 8 pages, Supplementary Materials: 15 pages
- **标题**: TB板凳：培训和测试多模式AI，以了解仪表板图像/视频的时空交通行为
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于对特定于交通的数据的培训有限，并且缺乏专用的基准测试，因此多模式大型语言模型（MLLM）在自主驾驶（AD）中的应用面临重大挑战。这项研究通过提出TB Bench来解决这些问题，TB Bench是一种综合基准测试，旨在评估MLLM从以自我为中心的观点中理解八个感知任务的交通行为。我们还介绍了Vision语言说明调谐数据集TB-100K和TB-250K，以及简单而有效的基线。通过广泛的实验，我们表明在这些任务中现有的MLLM表现不佳，甚至像GPT-4O这样的强大模型平均达到了不到35％的精度。相比之下，当用TB-100K或TB-250K微调时，我们的基线模型的平均准确性高达85％，可显着提高任务的性能。此外，我们通过共同培训TB-100K与另一个流量数据集证明了性能转移，从而改善了后者的性能。总体而言，这项研究代表了向前迈出的一步，它引入了全面的基准，高质量的数据集和基准，从而支持MLLM逐步整合到AD的感知，预测和计划阶段。

### 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene Understanding 
[[arxiv](https://arxiv.org/abs/2501.07819)] [[cool](https://papers.cool/arxiv/2501.07819)] [[pdf](https://arxiv.org/pdf/2501.07819)]
> **Authors**: Haomiao Xiong,Yunzhi Zhuge,Jiawen Zhu,Lu Zhang,Huchuan Lu
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Accepted to IEEE Transactions on Multimedia (TMM)
- **标题**: 3ur-llm：一种用于3D场景的端到端多模式的大型语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在2D任务中表现出令人印象深刻的功能，但在从2D表示到3D表示时，在识别场景中的空间位置，相互关系和因果逻辑方面遇到了挑战。我们发现局限性主要在于：i）高注释成本限制了3D场景数据的规模限制，ii）缺乏一种直接有效的方法来感知3D信息，从而导致较长的培训时间并使流化的框架复杂化。为此，我们开发基于开源2D MLLM和LLM的管道，以生成高质量的3D文本对并构造3DS-160K，以增强预训练过程。利用这种高质量的预训练数据，我们介绍了3UR-LLM模型，这是一种端到端的3D MLLM，设计用于精确解释3D场景，在浏览物理世界的复杂性方面表现出了非凡的能力。 3ur-llm直接接收3D点云作为输入，项目3D功能与文本说明融合到一组可管理的令牌中。考虑到这些混合代币的计算负担，我们设计了一个3D压缩机模块，以凝聚力压缩3D空间提示和文本叙述。 3ur-llm在先前的SOTA方面取得了有希望的表现，例如，3ur-llm在Scanqa上超过了7.1 \％苹果酒，同时利用较少的培训资源。 3UR-LLM和3DS-160K基准的代码和型号权重可在3UR-LLM上获得。

### AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation 
[[arxiv](https://arxiv.org/abs/2501.07810)] [[cool](https://papers.cool/arxiv/2501.07810)] [[pdf](https://arxiv.org/pdf/2501.07810)]
> **Authors**: Sitong Gong,Yunzhi Zhuge,Lu Zhang,Yifan Wang,Pingping Zhang,Lijun Wang,Huchuan Lu
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Accepted to IEEE Transactions on Multimedia (TMM)
- **标题**: AVS-MAMBA：探索视听分段的时间和多模式的曼巴
- **领域**: 计算机视觉和模式识别
- **摘要**: 视听分割（AVS）的本质在于在视频流中定位和描述发声对象。尽管基于变压器的方法已经显示出希望，但由于二次计算成本而导致的远程依赖性斗争的处理，在复杂的场景中呈现瓶颈。为了克服这一限制并促进具有线性复杂性的复杂多模式理解，我们介绍了AVS-Mamba，这是一种选择性的状态空间模型，以解决AVS任务。我们的框架结合了两个用于视频理解和跨模式学习的关键组成部分：用于顺序视频处理的时间mamba块，以及用于高级音频视觉集成的视觉效果融合块。在此基础上，我们开发了多尺度的时间编码器，旨在增强范围内视觉特征的学习，从而促进对框架内和框架间信息的感知。为了执行多模式融合，我们提出了模态聚合解码器，利用愿景来访问融合块将视觉特征整合到框架和时间级别的音频特征中。此外，我们采用上下文集成金字塔来执行音频到视觉时空上下文的环境协作。通过这些创新的贡献，我们的方法在AVSBENCH-OBJECT和AVSBENCH-SEMINANIC数据集上实现了新的最新结果。我们的源代码和模型权重可以在AVS-Mamba提供。

### Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding 
[[arxiv](https://arxiv.org/abs/2501.07783)] [[cool](https://papers.cool/arxiv/2501.07783)] [[pdf](https://arxiv.org/pdf/2501.07783)]
> **Authors**: Zhaokai Wang,Xizhou Zhu,Xue Yang,Gen Luo,Hao Li,Changyao Tian,Wenhan Dou,Junqi Ge,Lewei Lu,Yu Qiao,Jifeng Dai
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: 参数反向图像金字塔网络，用于视觉感知和多模式理解
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 图像金字塔在表现最佳的方法中被广泛采用，以获得多尺度特征，以进行精确的视觉感知和理解。但是，当前的图像金字塔使用相同的大规模模型来处理多个图像分辨率，从而导致了大量的计算成本。为了应对这一挑战，我们提出了一种新颖的网络体系结构，称为参数式图像金字塔网络（PIIP）。具体而言，PIIP使用验证的模型（VIT或CNN）作为分支来处理多尺度图像，其中较小的网络分支机构处理较高分辨率的图像以平衡计算成本和性能。为了整合来自不同空间尺度的信息，我们进一步提出了一种新型的跨分支特征相互作用机制。为了验证PIIP，我们将其应用于各种感知模型和称为LLA​​VA的代表性多模式模型，并对各种任务进行了广泛的实验，例如对象检测，分割，图像分类和多模式理解。与单分支和现有的多分辨率方法相比，PIIP的计算成本较低。当应用于Intervit-6B（大规模视觉基础模型）时，PIIP可以在检测和分割时将其性能提高1％-2％，仅为原始计算的40％-60％，最终在Coco上获得60.0盒AP，在ADE20K上获得59.7 MIOU。为了获得多模式的理解，我们的Piip-llava仅具有280万个培训数据，在TextVQA上的精度为73.0％，MMBench的精度为74.5％。我们的代码在https://github.com/opengvlab/piip上发布。

### MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training 
[[arxiv](https://arxiv.org/abs/2501.07556)] [[cool](https://papers.cool/arxiv/2501.07556)] [[pdf](https://arxiv.org/pdf/2501.07556)]
> **Authors**: Xingyi He,Hao Yu,Sida Peng,Dongli Tan,Zehong Shen,Hujun Bao,Xiaowei Zhou
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Project page: https://zju3dv.github.io/MatchAnything/
- **标题**: MatchAnything：通用跨模式图像与大规模预训练的匹配
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像匹配旨在识别图像之间相应的像素位置，在广泛的科学学科中至关重要，有助于图像注册，融合和分析。近年来，基于深度学习的图像匹配算法在迅速，准确地找到大量对应方面的人类表现显着优于人类。但是，当处理在不同成像方式下捕获的图像会导致出现重大变化时，这些算法的性能通常由于带注释的跨模式训练数据的稀缺而恶化。此限制阻碍了依赖多个图像模式的各个领域的应用程序来获取互补信息。为了应对这一挑战，我们提出了一个大规模的预训练框架，该框架利用合成的跨模式训练信号，结合了来自各种来源的各种数据，以识别和匹配图像跨图像的基本结构。该功能可将其传递到现实世界中，看不见的跨模式匹配任务。我们的主要发现是，使用我们的框架训练的匹配模型可以使用相同的网络权重，在八个以上看不见的跨模式注册任务中实现了显着的普遍性，无论是为概括还是针对特定任务量身定制的，都大大优于现有方法。这一进步显着增强了各种科学学科的图像匹配技术的适用性，并为多模式的人类和人工智能分析及其他方面的新应用铺平了道路。

### Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal Violence Detection Method 
[[arxiv](https://arxiv.org/abs/2501.07496)] [[cool](https://papers.cool/arxiv/2501.07496)] [[pdf](https://arxiv.org/pdf/2501.07496)]
> **Authors**: Wenping Jin,Li Zhu,Jing Sun
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: 首先对齐，然后进行融合：一种新型弱监督的多模式暴力检测方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 弱监督的暴力检测是指培训模型的技术，即仅使用视频级标签识别视频中的暴力细分市场。在这些方法中，整合了音频和光流等方式的多模式暴力检测具有巨大的潜力。该域中的现有方法主要集中于设计多模式融合模型以解决模式差异。相比之下，我们采取了不同的方法。利用暴力事件表示中跨模式之间的固有差异，以提出一种新型的多模式语义特征对准方法。这种方法稀疏地将局部，瞬态和信息较少的方式（例如音频和光流）的语义特征映射到更具信息性的RGB语义特征空间中。通过迭代过程，该方法可以根据此子空间来识别合适的NO-Zero特征匹配子空间，并将特定于模态的事件表示形式对齐，从而在随后的模态融合阶段对所有模式的信息进行完整的利用。在此基础上，我们设计了一个新的弱监督暴力检测框架，该框架由单峰多种现代学习组成，用于提取单峰语义特征，多模式对准，多模式融合和最终检测。基准数据集的实验结果证明了我们方法的有效性，在XD-Violence数据集上达到了平均精度（AP）为86.07％。我们的代码可在https://github.com/xjpp2016/mavd上找到。

### A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal Sensor Fusion 
[[arxiv](https://arxiv.org/abs/2501.07451)] [[cool](https://papers.cool/arxiv/2501.07451)] [[pdf](https://arxiv.org/pdf/2501.07451)]
> **Authors**: Fabio Montello,Ronja Güldenring,Simone Scardapane,Lazaros Nalpantidis
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Under review at International Journal of Computer Vision
- **标题**: 动态神经网络的调查：从计算机视觉到多模式传感器融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 模型压缩对于在嵌入式设备上部署大型计算机视觉模型至关重要。但是，静态优化技术（例如修剪，量化等）忽略了以下事实：不同的输入具有不同的复杂性，因此需要不同量的计算。动态神经网络允许调节计算数量到特定输入。当前有关该主题的文献非常广泛和分散。我们提出了一项全面的调查，该调查在计算机视觉的背景下综合并统一了现有的动态神经网络研究。此外，我们基于网络的哪个组件自适应提供逻辑分类法：输出，计算图或输入。此外，我们认为动态神经网络在传感器融合的背景下特别有益，以更好地适应性，降低降噪和信息优先级。我们朝着这个方向介绍初步作品。

### Introducing 3D Representation for Medical Image Volume-to-Volume Translation via Score Fusion 
[[arxiv](https://arxiv.org/abs/2501.07430)] [[cool](https://papers.cool/arxiv/2501.07430)] [[pdf](https://arxiv.org/pdf/2501.07430)]
> **Authors**: Xiyue Zhu,Dou Hoon Kwark,Ruike Zhu,Kaiwen Hong,Yiqi Tao,Shirui Luo,Yudu Li,Zhi-Pei Liang,Volodymyr Kindratenko
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: 通过得分融合引入医学图像体积到体积翻译的3D表示形式
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在医学图像中的体积到体积翻译中，由于高计算数据集的需求，现有模型通常难以使​​用3D VoxelSpace表示固有的体积分布。我们提出了分数融合，这是一种新型的体积翻译模型，它通过在得分函数空间中结合垂直训练的2D扩散模型来有效地学习3D表示。通过仔细初始化我们的模型，以从TPDM中的平均2D模型开始，我们将3D培训减少到微调过程，从而减轻计算和数据需求。此外，我们明确设计了3D模型的层次结构层，以了解2D功能的合奏，从而进一步提高效率和性能。此外，分数融合自然扩展到多模式设置，通过融合以不同输入为条件的扩散模型，以进行灵活，准确的集成。我们证明，在下游识别任务（例如肿瘤分割）中，3D表示对于更好的性能至关重要，其中大多数分割模型基于3D表示。广泛的实验表明，在3D医学图像超分辨率和模态翻译中，得分融合可以达到较高的精度和体积忠诚度。除了这些改进之外，我们的工作还提供了对基于学习的分数功能融合方法的更广泛的见解。

### TimberVision: A Multi-Task Dataset and Framework for Log-Component Segmentation and Tracking in Autonomous Forestry Operations 
[[arxiv](https://arxiv.org/abs/2501.07360)] [[cool](https://papers.cool/arxiv/2501.07360)] [[pdf](https://arxiv.org/pdf/2501.07360)]
> **Authors**: Daniel Steininger,Julia Simon,Andreas Trondl,Markus Murschitz
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Accepted at Winter Conference on Applications of Computer Vision (WACV) 2025. Code and dataset available at https://github.com/timbervision/timbervision
- **标题**: Timbervision：自动林业操作中的对数组件分割和跟踪的多任务数据集和框架
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 木材代表着越来越有价值且通用的资源。但是，诸如收获，处理和测量日志之类的林业行动仍需要大量的人工劳动，在带来重大安全风险的偏远环境中。逐步自动化这些任务具有提高其效率和安全性的潜力，但需要准确检测单个日志以及活树及其背景。尽管已经针对这个具有挑战性的应用领域提出了初始方法，但专业数据和算法仍然太稀缺了，无法开发可靠的解决方案。为了减轻这一差距，我们介绍了Timbervision数据集，由超过2K注释的RGB图像组成，其中包含总计51K中继组件，包括剪切和侧面表面，从而超过了该域中的任何现有数据集，以大量和细节的范围通过大量的范围。基于这些数据，我们进行了一系列的消融实验，以实现对象检测和实例分割，并评估多个场景参数对模型性能的影响。我们引入了一个通用框架，将两个任务检测到的组件融合到统一的中继表示中。此外，我们会自动得出几何特性并应用多对象跟踪以进一步增强鲁棒性。我们的检测和跟踪方法即使在充满挑战的环境条件下，也仅来自RGB图像数据的高度描述性和准确的中继表示。我们的解决方案适用于各种应用程序场景，并且可以与其他传感器模式结合使用。

### Code and Pixels: Multi-Modal Contrastive Pre-training for Enhanced Tabular Data Analysis 
[[arxiv](https://arxiv.org/abs/2501.07304)] [[cool](https://papers.cool/arxiv/2501.07304)] [[pdf](https://arxiv.org/pdf/2501.07304)]
> **Authors**: Kankana Roy,Lars Krämer,Sebastian Domaschke,Malik Haris,Roland Aydin,Fabian Isensee,Martin Held
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: 代码和像素：用于增强表格数据分析的多模式对比预训练
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 从表格数据中学习至关重要，因为它通过提供丰富的结构化信息来源来补充图像和视频数据的常规分析，这通常对于全面的理解和决策过程至关重要。我们提出了多任务对比度屏蔽表格建模（MT-CMTM），这是一种新型方法，旨在通过利用表格数据和相应图像之间的相关性来增强表格模型。 MT-CMTM采用双重策略，将对比度学习与掩盖的表格建模相结合，从而优化了这些数据模式之间的协同作用。我们方法的核心是具有剩余连接和注意机制（1D-Resnet-CBAM）的1D卷积神经网络，旨在有效地处理表格数据而不依赖图像。这使MT-CMTM能够处理纯粹的表格数据以进行下游任务，从而消除了对图像采集和处理的潜在昂贵的需求。我们在DVM CAR数据集上评估了MT-CMTM，该数据集非常适合这种特定情况，以及新开发的HIPMP数据集，该数据集将膜制造参数与图像数据连接起来。我们的MT-CMTM模型的表现优于提出的表格1D Resnet-CBAM，该表格从头开始训练，在HIPMP上相对MSE的相对1.48％提高了1.48％，并且DVM的绝对准确性提高了2.38％。这些结果证明了MT-CMTM的鲁棒性及其推进多模式学习领域的潜力。

### Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion 
[[arxiv](https://arxiv.org/abs/2501.07260)] [[cool](https://papers.cool/arxiv/2501.07260)] [[pdf](https://arxiv.org/pdf/2501.07260)]
> **Authors**: Li Liang,Naveed Akhtar,Jordan Vice,Xiangrui Kong,Ajmal Saeed Mian
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Accepted by AAAI 2025
- **标题**: 跳过Mamba传播单程3D语义场景完成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 3D语义场景完成对于自主系统中的多个下游任务至关重要。它估计了获得的场景数据中缺少几何和语义信息。由于现实世界中的具有挑战性，此任务通常需要复杂的模型来处理多模式数据以实现可接受的性能。我们提出了一个独特的神经模型，利用了状态空间和扩散生成建模的进步，以通过单眼图像输入实现出色的3D语义场景完成性能。我们的技术在各种自动编码器的条件潜在空间中处理数据，其中通过创新的状态空间技术进行扩散建模。我们的神经网络的一个关键组成部分是拟议的Skimba（Skip Mamba）Denoiser，它擅长有效地处理长期序列数据。 Skimba扩散模型是我们的3D场景完成网络不可或缺的一部分，它结合了三个方向的三重MAMBA结构，尺寸分解残差和变化的扩张。我们还采用了该网络的一种变体，用于我们方法的后续语义分割阶段。对标准Semantickitti和SSCBENCH-KITTI360数据集的广泛评估表明，我们的方法不仅要超过其他单眼技术，而且还可以在立体声方法上实现竞争性能。该代码可从https://github.com/xrkong/skimba获得

### Exploring the Use of Contrastive Language-Image Pre-Training for Human Posture Classification: Insights from Yoga Pose Analysis 
[[arxiv](https://arxiv.org/abs/2501.07221)] [[cool](https://papers.cool/arxiv/2501.07221)] [[pdf](https://arxiv.org/pdf/2501.07221)]
> **Authors**: Andrzej D. Dobrzycki,Ana M. Bernardos,Luca Bergesio,Andrzej Pomirski,Daniel Sáez-Trigueros
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: ef:Mathematics 2024, 12(1), 76
- **标题**: 探索使用对比的语言图像预训练对人类姿势分类的使用：瑜伽姿势分析的见解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 图像和视频中的准确人类姿势分类对于各个领域的自动应用至关重要，包括工作安全，体育康复，体育培训或日常辅助生活。最近，多模式学习方法（例如对比性语言图像预处理（剪辑））在共同理解图像和文本方面已显着提高。这项研究旨在评估剪辑在对人类姿势分类的分类中的有效性，重点是其在瑜伽中的应用。尽管零射击方法的初始局限性，但在15,301张图像（真实和合成）上使用82个类别的转移学习显示出令人鼓舞的结果。本文介绍了用于微调的完整过程，包括图像描述语法，模型和超参数调整的选择。对3826张图像进行了测试的微型剪辑模型的精度超过85％，超过了同一数据集上先前作品的当前最新作品，大约6％，其训练时间比微调Yolov8基于Yolov8的模型所需的训练时间低3.5倍。对于更面向应用程序的方案，每个六个姿势的较小数据集，包含1301和401个培训图像，微型模型的精度分别为98.8％和99.1％。此外，我们的实验表明，在六级数据集中，每个姿势的训练少于20张图像的精度约为90％。这项研究表明，这种多模式技术可以有效地用于瑜伽姿势分类，并且可能用于人类姿势分类。此外，剪辑推理时间（约7毫秒左右）支持该模型可以集成到自动化系统中以进行姿势评估，例如，用于开发实时个人瑜伽助理进行绩效评估。

### BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature 
[[arxiv](https://arxiv.org/abs/2501.07171)] [[cool](https://papers.cool/arxiv/2501.07171)] [[pdf](https://arxiv.org/pdf/2501.07171)]
> **Authors**: Alejandro Lozano,Min Woo Sun,James Burgess,Liangyu Chen,Jeffrey J Nirschl,Jeffrey Gu,Ivan Lopez,Josiah Aklilu,Austin Wolfgang Katzer,Collin Chiu,Anita Rau,Xiaohan Wang,Yuhui Zhang,Alfred Seunghoon Song,Robert Tibshirani,Serena Yeung-Levy
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: BioMedica：开放的生物医学捕获档案，数据集和视觉模型，该模型来自科学文献
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉模型（VLM）的开发是由大规模和多样化的多模式数据集驱动的。然而，朝着通才生物医学VLM的进展受到了在生物学和医学范围内缺乏注释的，公共访问的数据集的限制。现有的努力仅限于狭窄的领域，缺少在科学文献中编码的生物医学知识的全部多样性。为了解决此差距，我们介绍了BioMedica，这是一个可扩展的开源框架，用于提取，注释和序列化整个PubMed Central Open Access子集中，以易于使用，可公开访问的数据集。我们的框架生产了一个全面的档案，拥有超过600万篇文章的超过2400万个独特的图像文本对。还提供了元数据和专家指导的注释。我们通过释放BMCA-CLIP（通过流媒体数据集在BioMedica数据集中连续预训练的夹子式模型套件来证明我们的资源的实用性和可访问性，从而消除了在本地下载27个数据的需求。 On average, our models achieve state-of-the-art performance across 40 tasks - spanning pathology, radiology, ophthalmology, dermatology, surgery, molecular biology, parasitology, and cell biology - excelling in zero-shot classification with a 6.56% average improvement (as high as 29.8% and 17.5% in dermatology and ophthalmology, respectively), and stronger image-text retrieval, all while使用10倍的计算。为了促进可重复性和协作，我们为更广泛的研究社区发布了代码库和数据集。

### Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video Recommendation 
[[arxiv](https://arxiv.org/abs/2501.07110)] [[cool](https://papers.cool/arxiv/2501.07110)] [[pdf](https://arxiv.org/pdf/2501.07110)]
> **Authors**: Han Liu,Yinwei Wei,Fan Liu,Wenjie Wang,Liqiang Nie,Tat-Seng Chua
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: This paper has been accepted by ACM Transactions on Information Systems
- **标题**: 通过元学习的动态多模式融合到微视频推荐
- **领域**: 计算机视觉和模式识别,信息检索,多媒体
- **摘要**: 多模式信息（例如，视觉，声学和文本）已被广泛用于增强微视频建议的表示。为了将多模式信息整合到微观视频的联合表示中，多模式融合在现有的Micro-Video建议方法中起着至关重要的作用。但是，先前研究中使用的静态多模式融合不足以模拟不同微视频的多模式信息之间的各种关系。在本文中，我们开发了一种新型的基于元学习的多模式融合框架，称为Meta多模式融合（MetAMMF），该框架在其表示学习过程中，将参数动态分配给每个微观视频的多模式融合函数。具体而言，metAMMF将每个微观视频的多模式融合视为一项独立的任务。基于从输入任务的多模式特征中提取的元信息，metAMMF通过元学习者将神经网络参数化为项目特定的融合函数。我们在三个基准数据集上进行了广泛的实验，证明了对几种最先进的多模式推荐模型的显着改进，例如MMGCN，Lattice和Invrl。此外，我们通过采用规范的多层分解来提高训练效率，并通过实验结果验证其有效性，从而减轻了模型。代码可在https://github.com/hanliu95/metammf上找到。

### The Quest for Visual Understanding: A Journey Through the Evolution of Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2501.07109)] [[cool](https://papers.cool/arxiv/2501.07109)] [[pdf](https://arxiv.org/pdf/2501.07109)]
> **Authors**: Anupam Pandey,Deepjyoti Bodo,Arpan Phukan,Asif Ekbal
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: :53A45; 53-02ACM Class:I.2.7; I.2.10; I.4.8; I.5.4; A.1
- **标题**: 对视觉理解的追求：通过视觉问题的演变回答的旅程
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉问题回答（VQA）是一个跨学科的领域，它弥合了计算机视觉（CV）和自然语言处理（NLP）之间的差距，使人工智能（AI）系统能够回答有关图像的问题。自2015年成立以来，VQA一直在迅速发展，这是在深度学习，注意机制和基于变压器的模型方面的进步驱动的。这项调查可以追溯到VQA从早期开始，通过重大突破，例如注意机制，组成推理和视觉语言预训练方法的兴起。我们重点介绍了塑造VQA系统开发的关键模型，数据集和技术，强调了变压器体系结构和多模式预训练在推动最新进度方面的关键作用。此外，我们探讨了VQA在医疗保健等领域中的专业应用，并讨论正在进行的挑战，例如数据集偏见，模型可解释性以及对常识性推理的需求。最后，我们讨论了大型多模式模型的新兴趋势以及外部知识的整合，从而提供了对VQA未来方向的见解。本文旨在对VQA的演变进行全面的概述，并强调其当前状态和潜在的进步。

### The Devil is in Temporal Token: High Quality Video Reasoning Segmentation 
[[arxiv](https://arxiv.org/abs/2501.08549)] [[cool](https://papers.cool/arxiv/2501.08549)] [[pdf](https://arxiv.org/pdf/2501.08549)]
> **Authors**: Sitong Gong,Yunzhi Zhuge,Lu Zhang,Zongxin Yang,Pingping Zhang,Huchuan Lu
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: ef:CVPR 2025
- **标题**: 魔鬼在暂时令牌中：高质量的视频推理细分
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视频推理细分的现有方法在很大程度上依赖于单个特殊令牌来表示密钥帧或整个视频中的对象，从而不充分捕获空间复杂性和框架间运动。为了克服这些挑战，我们建议VRS-HQ是一种端到端的视频推理细分方法，利用多模式大语言模型（MLLM）将丰富的时空特征注入层次代币。具体来说，我们设计了框架级<seg>和时间级<tak>令牌，这些<tak> <tak>标记利用MLLM的自动回归学习有效地捕获本地和全局信息。随后，我们采用基于相似性的加权融合和框架选择策略，然后利用SAM2执行密钥帧分割和传播。为了提高密钥帧本地化精度，TKS在推理过程中根据SAM2的闭塞得分过滤了密钥帧。 VRS-HQ在Revos上实现了最先进的表现，在这三个子集的J＆F分数中，签证超过了5.9％/12.5％/9.1％。这些结果突出了我们方法的强烈时间推理和分割能力。代码和模型权重将在VRS-HQ上发布。

### Multimodal Fake News Video Explanation Generation: Dataset, Model, and Evaluation 
[[arxiv](https://arxiv.org/abs/2501.08514)] [[cool](https://papers.cool/arxiv/2501.08514)] [[pdf](https://arxiv.org/pdf/2501.08514)]
> **Authors**: Lizhi Chen,Zhong Qian,Peifeng Li,Qiaoming Zhu
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: 多模式假新闻视频说明生成：数据集，模型和评估
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 尽管现有方法已将虚假新闻视频检测作为分类问题，但尚不清楚为什么某些新闻内容被确定为伪造。没有适当的解释，最终用户可能无法理解虚假新闻的潜在含义。因此，我们提出了一项新颖的任务，即假新闻视频说明（FNVE），以生成自然语言解释，以揭示新闻视频的虚假性。为此，我们首先开发了OnVE和VTSE，这是两个新数据集来解释假新闻视频帖子。然后，我们提出了一个多模式关系图形变压器（MRGT）模型，以基准OnVE和VTSE。 MRGT引入了一个多模式关系图，以全面表示多模式关系，然后引入基于BART的解码器来解释世代。实验结果表明，提出的MRGT表现优于强基础。此外，对注释的OnVE和VTSE的人类评估在充分等级方面也得分很高。

### FLAVARS: A Multimodal Foundational Language and Vision Alignment Model for Remote Sensing 
[[arxiv](https://arxiv.org/abs/2501.08490)] [[cool](https://papers.cool/arxiv/2501.08490)] [[pdf](https://arxiv.org/pdf/2501.08490)]
> **Authors**: Isaac Corley,Simone Fobi Nsutezo,Anthony Ortiz,Caleb Robinson,Rahul Dodhia,Juan M. Lavista Ferres,Peyman Najafirad
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: 类型：用于遥感的多模式基础语言和视觉对齐模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 遥感图像是密集的，具有对象和上下文视觉信息。最近有一个趋势将配对的卫星图像和文本字幕结合起来，用于预处理表演者编码，以进行下游任务。然而，尽管剪辑诸如剪辑的对比图形方法可以实现视觉和零射击分类能力，但与仅图像预处理（例如MAE）相比，仅视觉下游性能往往会降低。在本文中，我们提出了一种训练方法，一种预处理的方法，结合了对比度学习和掩盖建模，以及通过对比度位置编码的地理空间对齐。我们发现，针对仅视觉分类和语义分割的唯一视觉任务，spacenet1上的skyclip的基线显着超过了SkyClip的基线，同时保留了执行零发射分类的能力，与MAE预处理的方法不同。

### Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models 
[[arxiv](https://arxiv.org/abs/2501.08453)] [[cool](https://papers.cool/arxiv/2501.08453)] [[pdf](https://arxiv.org/pdf/2501.08453)]
> **Authors**: Weichen Fan,Chenyang Si,Junhao Song,Zhenyu Yang,Yinan He,Long Zhuo,Ziqi Huang,Ziyue Dong,Jingwen He,Dongwei Pan,Yi Wang,Yuming Jiang,Yaohui Wang,Peng Gao,Xinyuan Chen,Hengjie Li,Dahua Lin,Yu Qiao,Ziwei Liu
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: VCHITECT-2.0：用于扩展视频扩散模型的并行变压器
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 我们提出了VCHITECT-2.0，这是一种平行的变压器体系结构，旨在扩展大型文本到视频生成的视频扩散模型。总体VCHITECT-2.0系统具有多种关键设计。 （1）通过引入一种新型的多模式扩散块，我们的方法实现了文本描述和生成的视频帧之间的一致比对，同时保持了跨序列的时间连贯性。 （2）为了克服内存和计算瓶颈，我们提出了一个记忆效率的训练框架，该培训框架结合了混合并行性和其他记忆减少技术，从而有效地训练了分布式系统上的长视频序列。 （3）此外，我们增强的数据处理管道可确保通过严格的注释和审美评估创建VCHITECT T2V DATAVERSE，这是高质量的数百万尺度培训数据集。广泛的基准测试表明，VCHITECT-2.0在视频质量，培训效率和可扩展性方面的现有方法优于现有方法，这是适合高保真视频生成的基础。

### Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.08443)] [[cool](https://papers.cool/arxiv/2501.08443)] [[pdf](https://arxiv.org/pdf/2501.08443)]
> **Authors**: Xu Li,Yi Zheng,Haotian Chen,Xiaolei Chen,Yuxuan Liang,Chenghang Lai,Bin Li,Xiangyang Xue
> **First submission**: 2024-12-26
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: 多层视觉特征在大型视觉模型中的指导引导的融合
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 大型视觉模型（LVLM）通过整合预训练的视觉编码器和大型语言模型，在各种多模式任务中取得了巨大的成功。但是，当前的LVLM主要依赖于从视觉编码器的最终层中提取的视觉特征，从而俯瞰较浅层中可用的互补信息。尽管最近的方法探索了LVLM中多层视觉特征的使用，但它们倾向于进行任务敏捷，并且无法检查层次视觉特征对特定任务的依赖性。为了解决这些差距，我们使用跨越6个任务类别的18个基准测试基准系统地研究了来自不同编码层的视觉特征的贡献。我们的发现表明，多层功能提供了具有不同任务依赖性的互补优势，均匀的融合会导致次优性能。在这些见解的基础上，我们提出了指导引导的视觉聚合器，该模块基于文本说明动态整合了多层视觉特征，而无需增加视觉令牌的数量。广泛的评估证明了我们方法的出色性能。此外，对聚合器行为的深入分析突出了中高级别特征在语义丰富的任务中的主导地位，以及低级特征在细粒度感知中的关键作用。

### SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval 
[[arxiv](https://arxiv.org/abs/2501.08347)] [[cool](https://papers.cool/arxiv/2501.08347)] [[pdf](https://arxiv.org/pdf/2501.08347)]
> **Authors**: Bhavin Jawade,Joao V. B. Soares,Kapil Thadani,Deen Dayal Mohan,Amir Erfan Eshratifar,Benjamin Culpepper,Paloma de Juan,Srirangaraj Setlur,Venu Govindaraju
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-15
> **comment**: Paper accepted at WACV 2025 in round 1
- **标题**: 苏格兰人：零拍的构图检索预处理预处理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 构图图像检索（CIR）是一项多模式学习任务，其中模型将查询图像与用户提供的文本修改结合在一起以检索目标图像。 CIR在包括产品检索（电子商务）和Web搜索在内的各个领域中找到了应用程序。现有的方法主要集中于完全监督的学习，其中在诸如FashionIQ和CIRR等标记的三胞胎数据集上培训了其中的模型。这提出了两个重大挑战：（i）策划此类三重态数据集是劳动密集型的； （ii）模型缺乏对看不见的对象和域的概括。在这项工作中，我们提出了SCOT（自我监督的构图训练），这是一种新型的零弹性组成预处理策略，将现有大型图像文本对数据集与大语言模型的生成能力相结合，以对比训练嵌入式组成网络。具体而言，我们表明，可以在组成预处理过程中将大规模构图的嵌入文本嵌入为代理目标监督，以取代目标图像嵌入。在零拍设置中，该策略超过了SOTA零摄像的组成检索方法以及许多对标准基准（例如FashionIQ和CIRR）的全面监督方法。

### Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks 
[[arxiv](https://arxiv.org/abs/2501.08326)] [[cool](https://papers.cool/arxiv/2501.08326)] [[pdf](https://arxiv.org/pdf/2501.08326)]
> **Authors**: Miran Heo,Min-Hung Chen,De-An Huang,Sifei Liu,Subhashree Radhakrishnan,Seon Joo Kim,Yu-Chiang Frank Wang,Ryo Hachiuma
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: Project page: https://miranheo.github.io/omni-rgpt/
- **标题**: OMNI-RGPT：通过令牌标记统一图像和视频区域级别的理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了Omni-Rgpt，这是一种多式模式的大型语言模型，旨在促进图像和视频的区域级别的理解。为了在时空维度上实现一致的区域表示，我们引入了令牌标记，这是一组令牌，突出了视觉特征空间内的目标区域。这些令牌使用区域提示（例如，框或掩码）直接嵌入空间区域中，并同时将其集成到文本提示中以指定目标，并在视觉令牌和文本令牌之间建立直接连接。为了进一步支持强大的视频理解而无需曲目，我们引入了一个辅助任务，该任务通过利用令牌的一致性来指导令牌标记，从而在整个视频中实现稳定的区域解释。此外，我们介绍了一个大型区域级视频指令数据集（Regvid-300k）。 Omni-RGPT在图像和基于视频的常识性推理基准上取得了最新的结果，同时在字幕和引用表达理解任务中显示出强大的性能。

### Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers 
[[arxiv](https://arxiv.org/abs/2501.08303)] [[cool](https://papers.cool/arxiv/2501.08303)] [[pdf](https://arxiv.org/pdf/2501.08303)]
> **Authors**: Efstathios Karypidis,Ioannis Kakogeorgiou,Spyros Gidaris,Nikos Komodakis
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: 通过多模式的视觉序列变压器来推进语义未来预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 语义未来预测对于自主系统导航动态环境很重要。本文介绍了未来主义者，这是一种使用统一，有效的视觉序列变压器体系结构的多模式未来语义预测的方法。我们的方法结合了多模式掩盖的视觉建模目标和设计用于多模式训练的新型掩蔽机制。这使该模型可以有效地整合来自各种方式的可见信息，从而提高预测准确性。此外，我们提出了一个无VAE的层次令牌化过程，该过程降低了计算复杂性，简化训练管道，并通过高分辨率，多模态输入来实现端到端训练。我们验证了CityScapes数据集中的未来主义者，这表明了短期和中期预测的未来语义细分中最先进的表现。我们在https://github.com/sta8is/futurist上提供实现代码。

### LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding 
[[arxiv](https://arxiv.org/abs/2501.08282)] [[cool](https://papers.cool/arxiv/2501.08282)] [[pdf](https://arxiv.org/pdf/2501.08282)]
> **Authors**: Hongyu Li,Jinyu Chen,Ziyu Wei,Shaofei Huang,Tianrui Hui,Jialin Gao,Xiaoming Wei,Si Liu
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: LLAVA-ST：一种多模式的大语言模型，用于细粒度的时空理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的最新进展显示出令人鼓舞的结果，但是现有的方法很难同时有效地处理时间和空间定位。这一挑战源于两个关键问题：首先，结合时空定位引入了大量的坐标组合，使语言和视觉坐标表示的对齐变得复杂；其次，在视频功能压缩过程中编码细粒度的时间和空间信息本质上是困难的。为了解决这些问题，我们提出了LLAVA-ST，这是一种用于良好的时空多模式理解的MLLM。在LLAVA-ST中，我们提出了与语言一致的位置嵌入，该位置嵌入将文本坐标特殊令牌嵌入到视觉空间中，从而简化了细粒空间时空的对应关系。此外，我们设计了时空包装器，将时间和空间分辨率的特征压缩分解为两个不同的点对区域注意处理流。此外，我们提出了具有430万培训样本的ST-Align数据集，以进行细粒度的空间多模式理解。借助ST-Align，我们提出了一条渐进式培训管道，该管道通过顺序的粗到细节阶段与视觉和文本特征保持一致。在此方面，我们介绍了ST-Align基准测试，以评估时空相互交叉的细粒度良好的理解任务，其中包括时空视频接地（STVG），事件本地化和Captirg（Spatial）（Spatial（Spatial）（SPAT）（SPAT）（SPAT）（SPAT）（SPAT）（SPAT）（SPAT）。 LLAVA-ST在11个基准测试基准上取得了出色的性能，需要细粒度的时间，空间或时空的交织多模式理解。我们的代码，数据和基准将通过我们的代码，数据和基准发布，将在https://github.com/appletea233/llava-st上发布。

### AI Driven Water Segmentation with deep learning models for Enhanced Flood Monitoring 
[[arxiv](https://arxiv.org/abs/2501.08266)] [[cool](https://papers.cool/arxiv/2501.08266)] [[pdf](https://arxiv.org/pdf/2501.08266)]
> **Authors**: Sanjida Afrin Mou,Tasfia Noor Chowdhury,Adib Ibn Mannan,Sadia Nourin Mim,Lubana Tarannum,Tasrin Noman,Jamal Uddin Ahamed
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: 8 pages, 6 figures
- **标题**: AI驱动的水分割具有深度学习模型，以增强洪水监测
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,图像和视频处理
- **摘要**: 洪水是一种主要的自然危害，每年造成严重的死亡和经济损失，由于气候变化而频率增加。快速准确的洪水检测和监测对于缓解这些影响至关重要。这项研究比较了三种深度学习模型UNET，RESNET和DEEPLABV3的性能，用于Pixelwise水分分割，以帮助洪水检测，利用无人机，现场观测和社交媒体中的图像。这项研究涉及创建一个新的数据集，该数据集通过特定于洪水的图像来增强知名的基准数据集，从而增强了模型的鲁棒性。测试了UNET，Resnet和DeepLab V3架构，以确定它们在各种环境条件和地理位置的有效性，并且在此还讨论了每个模型的优势和局限性，从而通过预测图像分割掩码来预测其在不同情况下的适用性。这种完全自动化的方法使这些模型可以隔离图像中的洪水泛滥区域，与传统的半自动方法相比，处理时间大大减少了。这项研究的结果是预测由洪水灾难影响的每个图像和这些模型的验证精度。这种方法促进了及时，持续的洪水监测，为应急小组提供了重要的数据，以减少生命损失和经济损失。它可以大大减少产生洪水图所需的时间，从而减少手动处理时间。此外，我们提出了未来研究的途径，包括多模式数据源的整合以及专门针对洪水检测任务量身定制的强大深度学习体系结构的发展。总体而言，我们的工作通过创新的深度学习技术有助于洪水管理策略的发展。

### Benchmarking Multimodal Models for Fine-Grained Image Analysis: A Comparative Study Across Diverse Visual Features 
[[arxiv](https://arxiv.org/abs/2501.08170)] [[cool](https://papers.cool/arxiv/2501.08170)] [[pdf](https://arxiv.org/pdf/2501.08170)]
> **Authors**: Evgenii Evstafev
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: 6 pages, 2 tables, 2 charts
- **标题**: 基准测试用于细粒图像分析的多模式模型：跨不同视觉特征的比较研究
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了一个基准，旨在评估多模型分析和解释图像的功能。该基准重点关注七个关键的视觉方面：主要对象，其他对象，背景，细节，主导颜色，样式和观点。由不同文本提示产生的14,580张图像的数据集用于评估七个领先的多模式模型的性能。评估了这些模型的准确识别和描述每个视觉方面的能力，从而提供了对其优势和缺点的见解，以了解全面的图像理解。该基准的发现对各种图像分析任务的多模型模型的开发和选择具有重要意义。

### GAC-Net_Geometric and attention-based Network for Depth Completion 
[[arxiv](https://arxiv.org/abs/2501.07988)] [[cool](https://papers.cool/arxiv/2501.07988)] [[pdf](https://arxiv.org/pdf/2501.07988)]
> **Authors**: Kuang Zhu,Xingli Gan,Min Sun
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: 13pages,4 figures, 2 tables
- **标题**: GAC-NET_地理和基于注意力的网络，以完成深度完成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 深度完成是自主驾驶的关键任务，旨在通过图像指导将稀疏的LiDAR深度测量完成为高质量的密集深度图。但是，现有方法通常将深度图视为颜色图像的附加渠道，或直接在稀疏数据上执行卷积，未能在深度图中充分利用3D几何信息，尤其是在复杂边界和稀疏区域的性能有限的情况下。为了解决这些问题，本文提出了一个深度完成网络，结合了渠道注意机制和3D全球特征感知（CGA-NET）。主要创新包括：1）利用PointNet ++从稀疏深度图中提取全局3D几何特征，从而增强了低线LIDAR数据的场景感知能力； 2）设计基于渠道注意的多模式融合模块，以有效整合稀疏深度，RGB图像和3D几何特征； 3）将残留学习与CSPN ++相结合，以优化深度细化阶段，从而进一步提高边缘区域和复杂场景的完成质量。 KITTI深度完成数据集的实验表明，CGA-NET可以显着提高密集深度图的预测准确性，实现新的最新技术（SOTA），并表现出对稀疏和复杂场景的强大鲁棒性。

### Facial Dynamics in Video: Instruction Tuning for Improved Facial Expression Perception and Contextual Awareness 
[[arxiv](https://arxiv.org/abs/2501.07978)] [[cool](https://papers.cool/arxiv/2501.07978)] [[pdf](https://arxiv.org/pdf/2501.07978)]
> **Authors**: Jiaxing Zhao,Boyuan Sun,Xiang Chen,Xihan Wei
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: 视频中的面部动态：改善面部表达感知和上下文意识的指导调整
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 面部表达字幕已在各个领域发现了广泛的应用。最近，视频多模式大型语言模型（MLLM）的出现在一般视频理解任务中显示出了希望。但是，描述视频中的面部表情对这些模型提出了两个主要挑战：（1）缺乏足够的数据集和基准测试，以及（2）视频MLLM的视觉令牌能力有限。为了解决这些问题，本文介绍了一个针对动态面部表达标题的新的指令关注数据集。该数据集由手动注释的5,033个高质量的视频剪辑，包含700,000个令牌。其目的是提高视频MLLM识别微妙面部细微差别的能力。此外，我们提出了FaceTrack-MM，该Fooketrack-MM利用有限的令牌来编码主角的脸部。该模型在跟踪面上表现出了卓越的性能，并关注主要角色的面部表情，即使在复杂的多人场景中也是如此。此外，我们介绍了一种新的评估度量指标，该指标结合事件提取，关系分类和最长的常见子序列（LCS）算法，以评估生成文本的内容一致性和时间顺序一致性。此外，我们提出了FEC-Bench，这是一种旨在评估此特定任务中现有视频MLLM的性能的基准。所有数据和源代码将公开可用。

### Cloud Removal With PolSAR-Optical Data Fusion Using A Two-Flow Residual Network 
[[arxiv](https://arxiv.org/abs/2501.07901)] [[cool](https://papers.cool/arxiv/2501.07901)] [[pdf](https://arxiv.org/pdf/2501.07901)]
> **Authors**: Yuxi Wang,Wenjuan Zhang,Bing Zhang
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: No comments
- **标题**: 使用两流剩余网络使用Polsar-Optical数据融合去除云
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 光学遥感图像在观察地球表面中起着至关重要的作用。但是，由于云覆盖率，获得完整的光学遥感图像是具有挑战性的。近年来，重建无云的光学图像已成为一项主要任务。本文提出了两流偏振仪（POLSAR） - 光学数据融合云去除算法（PODF-CR），该算法实现了缺失的光学图像的重建。 PODF-CR由编码模块和一个解码模块组成。编码模块包括两个平行分支，它们提取Polsar图像特征和光学图像特征。为了解决Polsar图像中的Speckle噪声，我们在Polsar分支中引入动态过滤器以进行图像DeNoisising。为了更好地促进多模式光学图像和Polsar图像之间的融合，我们建议基于交叉连接的融合块，以启用多模式数据信息的相互作用。通过注意机制可以完善获得的融合特征，以提供更好的条件，以便随后的融合图像解码。在解码模块中，引入了多尺度卷积以获取多尺度信息。此外，为了更好地利用全面的散射信息和极化特性来帮助恢复光学图像，我们将数据集用于云恢复，称为opt-bcfsar-pfsar，其中包括反向散射系数特征图像和极化图像，从POLSAR数据和光学图像获得。实验结果表明，该方法在定性和定量评估中都优于现有方法。

### OpticFusion: Multi-Modal Neural Implicit 3D Reconstruction of Microstructures by Fusing White Light Interferometry and Optical Microscopy 
[[arxiv](https://arxiv.org/abs/2501.09259)] [[cool](https://papers.cool/arxiv/2501.09259)] [[pdf](https://arxiv.org/pdf/2501.09259)]
> **Authors**: Shuo Chen,Yijin Li,Guofeng Zhang
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: 3DV 2025
- **标题**: 视频：通过融合白光干涉法和光学显微镜，多模式神经隐式3D重建微观结构
- **领域**: 计算机视觉和模式识别,应用物理,仪器仪表和探测器,光学
- **摘要**: 白光干涉法（WLI）是用于测量微观结构3D地形的精确光学工具。但是，常规WLI无法捕获样品表面的自然色，这对于许多需要3D几何和颜色信息的微观研究应用程序至关重要。以前的方法试图通过修改WLI硬件和分析软件来克服此限制，但是这些解决方案通常是昂贵的。在这项工作中，我们首次从计算机视觉多模式重建的角度解决了这一挑战。我们介绍了OpticFusion，这是一种新型方法，它使用额外的数字光学显微镜（OM）使用多视图WLI和OM图像实现3D重建。我们的方法采用两步数据关联过程来获得WLI和OM数据的姿势。通过利用神经隐式表示，我们融合了多模式数据并应用颜色分解技术来提取样品的自然色。 OpticFusion在我们的各种微观样品的多模式数据集上进行了测试，可实现带有颜色纹理的详细3D重建。我们的方法为跨微观研究领域的实际应用提供了有效的工具。源代码和我们的现实数据集可从https://github.com/zju3dv/opticfusion获得。

### Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures 
[[arxiv](https://arxiv.org/abs/2501.09203)] [[cool](https://papers.cool/arxiv/2501.09203)] [[pdf](https://arxiv.org/pdf/2501.09203)]
> **Authors**: Pengru Deng,Jiapeng Yao,Chun Li,Su Wang,Xinrun Li,Varun Ojha,Xuhui He,Takashi Matsumoto
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 统一的少量裂纹分割及其精确的3D自动测量在混凝土结构中
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 视觉空间系统在混凝土裂纹检查中变得越来越重要。但是，现有方法通常缺乏对各种情况的适应性，在基于图像的方法中表现出有限的鲁棒性，以及在弯曲或复杂的几何形状方面的挣扎。为了解决这些局限性，提出了针对二维（2D）裂纹检测，三维（3D）重建以及3D自动裂纹测量测量的创新框架，该框架是通过整合计算机觉得技术和多模式同时定位和映射（SLAM）的3D自动裂纹测量。首先，在基础DeepLabv3+分割模型上构建，并通过基础模型分段任何模型（SAM）结合了特定的修补，我们开发了一种裂纹分割方法，在不熟悉的场景中具有强烈的概括，从而能够生成精确的2D裂纹掩模。为了提高3D重建的准确性和鲁棒性，将光检测和范围（LIDAR）点云与图像数据和分割掩码一起使用。通过利用图像和激光雷达 - 萨克，我们开发了一个多框和多模式融合框架，该框架会产生密集的，有色的点云，有效地以3D现实世界的规模捕获了裂纹语义。此外，裂纹几何归因是自动测量的，直接在3D密集的点云空间内，超过了传统的基于2D图像的测量的局限性。这种进步使该方法适用于具有弯曲和复杂3D几何形状的结构成分。各种混凝土结构的实验结果突出了所提出方法的显着改善和独特的优势，证明了其在现实世界应用中的有效性，准确性和鲁棒性。

### CookingDiffusion: Cooking Procedural Image Generation with Stable Diffusion 
[[arxiv](https://arxiv.org/abs/2501.09042)] [[cool](https://papers.cool/arxiv/2501.09042)] [[pdf](https://arxiv.org/pdf/2501.09042)]
> **Authors**: Yuan Wang,Bin Zhu,Yanbin Hao,Chong-Wah Ngo,Yi Tan,Xiang Wang
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 烹饪扩散：烹饪程序图像生成稳定的扩散
- **领域**: 计算机视觉和模式识别,图形,机器学习
- **摘要**: 文本到图像生成模型的最新进步在创建多样化和逼真的图像方面表现出色。这种成功扩展到了食品图像，其中使用了各种条件输入，例如烹饪样式，食材和食谱。但是，尚未尚未探索的挑战是基于食谱中的烹饪步骤生成一系列程序图像。这可以通过视觉指导增强烹饪体验，并可能导致智能烹饪模拟系统。为了填补这一空白，我们介绍了一个名为\ textbf {烹饪过程图像生成}的新颖任务。此任务本质上是要求的，因为它努力创建与烹饪步骤一致的照片真实的图像，同时保持顺序一致性。为了共同解决这些挑战，我们提出\ textbf {cookeDiffusion}，这是一种利用稳定扩散和三个创新的记忆网的新颖方法来对程序提示进行建模。这些提示包括文本提示（表示烹饪步骤），图像提示（对应于烹饪图像）和多模式提示（混合烹饪步骤和图像），以确保始终如一地生成烹饪过程图像。为了验证我们的方法的有效性，我们预处理YouCookii数据集，建立了新的基准。我们的实验结果表明，我们的模型在通过FID和提出的平均程序一致性指标衡量的跨顺序烹饪步骤的高质量烹饪程序图像中具有显着的一致性。此外，烹饪排水证明了在食谱中操纵成分和烹饪方法的能力。我们将公开访问代码，模型和数据集。

### Multimodal LLMs Can Reason about Aesthetics in Zero-Shot 
[[arxiv](https://arxiv.org/abs/2501.09012)] [[cool](https://papers.cool/arxiv/2501.09012)] [[pdf](https://arxiv.org/pdf/2501.09012)]
> **Authors**: Ruixiang Jiang,Changwen Chen
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: WIP, Homepage https://github.com/songrise/MLLM4Art
- **标题**: 多模式LLM可以在零拍摄中理解美学
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体
- **摘要**: 我们介绍了有关如何将多模式LLM（MLLM）推理能力评估以评估艺术品美学的第一个研究。为了促进这项调查，我们构建了MM-Stylebench，这是一种用于基准艺术风格化的新型高质量数据集。然后，我们开发了一种原则性的方法，用于人类偏好建模，并在MLLM的反应和人类偏好之间进行系统的相关分析。我们的实验揭示了与响应主观性有关的艺术评估中MLLM的固有幻觉问题。提出了Artcot，表明特定于艺术的任务分解以及使用具体语言增强MLLM的美学推理能力。我们的发现为艺术的MLLM提供了宝贵的见解，可以使广泛的下游应用程序受益，例如样式转移和艺术形象的生成。可在https://github.com/songrise/mllm4art上找到代码。

### Enhanced Multi-Scale Cross-Attention for Person Image Generation 
[[arxiv](https://arxiv.org/abs/2501.08900)] [[cool](https://papers.cool/arxiv/2501.08900)] [[pdf](https://arxiv.org/pdf/2501.08900)]
> **Authors**: Hao Tang,Ling Shao,Nicu Sebe,Luc Van Gool
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: Accepted to TPAMI, an extended version of a paper published in ECCV2020. arXiv admin note: substantial text overlap with arXiv:2007.09278
- **标题**: 增强人形象的多尺度跨注意
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们提出了一个基于跨注意的新型生成对抗网络（GAN），以实现具有挑战性的人形象生成任务。跨注意是一种新颖而直观的多模式融合方法，其中在两个不同模态的两个特征图之间计算了注意/相关矩阵。具体而言，我们提出了新颖的Xinggan（或Crossinggan），该Xinggan（或Crossinggan）分别由两个一代分支组成，分别捕获了该人的外观和形状。此外，我们提出了两个新型的跨注意区块，以有效地转移和更新人的形状和外观嵌入，以相互改进。任何其他基于GAN的图像生成工作尚未考虑这一点。为了进一步了解不同人在不同尺度和子区域的远程相关性，我们提出了两个新型的多尺度跨注意区块。为了解决导致嘈杂和模棱两可的注意力的跨注意机制中独立相关计算的问题，这阻碍了性能的改善，我们提出了一个称为增强注意力的模块（EA）。最后，我们引入了一个新型的密集连接的共同发项模块，以有效地在不同阶段融合外观和形状特征。在两个公共数据集上进行的广泛实验表明，所提出的方法的表现优于当前基于GAN的方法，并且以基于扩散的方法进行标准。但是，在训练和推理中，我们的方法明显快于基于扩散的方法。

### IDEA: Image Description Enhanced CLIP-Adapter 
[[arxiv](https://arxiv.org/abs/2501.08816)] [[cool](https://papers.cool/arxiv/2501.08816)] [[pdf](https://arxiv.org/pdf/2501.08816)]
> **Authors**: Zhipeng Ye,Feng Jiang,Qiufeng Wang,Kaizhu Huang,Jiaqi Huang
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 想法：图像描述增强的剪辑适配器
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 剪辑（对比性语言图像预训练）在模式识别和计算机视觉方面取得了巨大的成功。将剪辑转移到下游任务（例如零或几次分类）是多模式学习的热门话题。但是，当前的研究主要集中于迅速学习文本或适配器调整以进行视觉，而无需完全利用图像文本对之间的互补信息和相关性。在本文中，我们提出了一个图像描述增强剪辑适配器（Idea）方法，以适应剪辑，以适应几个弹片的图像分类任务。该方法通过利用图像的视觉特征和文本描述来捕获细粒度的特征。想法是一种无训练的剪辑方法，它可以与多个任务上的最新模型相提并论。此外，我们引入了可训练的IDEA（T-IDEA），通过添加两个轻巧的可学习组件（即投影仪和可学习的潜在空间）来扩展想法，从而进一步增强了模型的性能并在11个数据集中实现SOTA结果。作为一项重要贡献，我们采用Llama模型并设计一条全面的管道来生成11个数据集图像的文本描述，从而总共1,637,795个图像文本对，名为“ IMD-11”。我们的代码和数据在https://github.com/fourierai/idea上发布。

### Multi-visual modality micro drone-based structural damage detection 
[[arxiv](https://arxiv.org/abs/2501.08807)] [[cool](https://papers.cool/arxiv/2501.08807)] [[pdf](https://arxiv.org/pdf/2501.08807)]
> **Authors**: Isaac Osei Agyemanga,Liaoyuan Zeng,Jianwen Chena,Isaac Adjei-Mensah,Daniel Acheampong
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 多视态方式基于微无人机的结构损伤检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 对象探测器在结构损伤检测中的准确检测和弹性对于确保连续使用民用基础设施很重要。但是，在对象探测器中实现鲁棒性仍然是一个持续的挑战，从而影响了他们有效概括的能力。这项研究提出了detectorx，这是结构性损伤检测的强大框架，加上微型无人机。通过合并两个创新的模块：一个茎块和螺旋池技术来解决对象探测器鲁棒性的挑战。茎块通过利用两个深卷积神经网络（DCNN）模型的输出来引入动态视觉模态。该框架采用拟议的基于事件的奖励加强学习来限制父母和子女DCNN模型的行动，从而获得奖励。这导致在红色，绿色和蓝色（RGB）数据的同时，引起了两种动态视觉方式。这种增强大大增强了Detectorx在各种环境情况下的看法和适应性。此外，一种螺旋池技术是一种在线图像增强方法，通过通过串联螺旋和平均/最大汇总功能来增加特征表示来增强框架。 In three extensive experiments: (1) comparative and (2) robustness, which use the Pacific Earthquake Engineering Research Hub ImageNet dataset, and (3) field-experiment, DetectorX performed satisfactorily across varying metrics, including precision (0.88), recall (0.84), average precision (0.91), mean average precision (0.76), and mean average recall (0.73), compared to the competing包括您在内的探测器只能看一次X-Medium（Yolox-M）等。该研究的发现表明，DetectorX可以提供令人满意的结果，并在具有挑战性的环境中证明弹性。

### BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with Multi-modality Refinement Module 
[[arxiv](https://arxiv.org/abs/2501.08659)] [[cool](https://papers.cool/arxiv/2501.08659)] [[pdf](https://arxiv.org/pdf/2501.08659)]
> **Authors**: Dongzhihan Wang,Yang Yang,Liang Xu
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: We have identified significant issues in the methodology and data analysis that impact the validity of our conclusions
- **标题**: Bright-VO：带有多模式细化模块的视觉进程的亮度引导的混合变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉进程（VO）通过根据视觉输入估算相机的位置和方向，在自主驾驶，机器人导航和其他相关任务中起着至关重要的作用。数据驱动的VO方法已经取得了重大进展，尤其是那些利用深度学习技术来提取图像特征并估算摄像头姿势的方法。但是，由于特征的可见性降低以及匹配关键点的难度增加，这些方法通常在弱光条件下挣扎。为了解决此限制，我们介绍了Brightvo，Brightvo是一种基于变压器体系结构的新型VO模型，它不仅执行前端视觉特征提取，而且还结合了后端的多模式细化模块，该模块集成了惯性测量单元（IMU）数据。使用姿势图优化，该模块迭代地完善了姿势估计，以减少误差并提高准确性和鲁棒性。此外，我们创建了一个合成的低光数据集KIC4R，其中包括各种照明条件，以促进在挑战性环境中对VO框架进行培训和评估。实验结果表明，Brightvo在KIC4R数据集和KITTI基准测试中都达到了最先进的性能。具体而言，它在正常室外环境中的姿势估计准确性中的平均提高为20％，在弱光条件下的平均估计准确性可提供超过现有方法的平均提高。为了广泛使用和进一步的开发，研究工作是https://github.com/anastasiawd/brightvo完全开源的。

### Densely Connected Parameter-Efficient Tuning for Referring Image Segmentation 
[[arxiv](https://arxiv.org/abs/2501.08580)] [[cool](https://papers.cool/arxiv/2501.08580)] [[pdf](https://arxiv.org/pdf/2501.08580)]
> **Authors**: Jiaqi Huang,Zunnan Xu,Ting Liu,Yong Liu,Haonan Han,Kehong Yuan,Xiu Li
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: Accepted by AAAI2025
- **标题**: 用于参考图像分割的密集连接参数效率调谐
- **领域**: 计算机视觉和模式识别
- **摘要**: 在计算机视觉领域，参数有效的调整（PET）越来越多地替换传统的预训练范式，然后进行全面调整。宠物在大型基础模型中的有效性特别受到青睐，因为它简化了学习成本并优化了硬件利用率。但是，当前的PET方法主要是为单模优化设计的。尽管一些开创性的研究已经进行了初步探索，但它们仍然保持在对齐编码器（例如剪辑）的水平上，并且缺乏对编码错误的编码者的探索。这些方法与未对齐的编码器显示出亚最佳性能，因为它们在微调过程中无法有效地对齐多模式特征。在本文中，我们介绍了DEDRIS，这是一个旨在通过在每个层和所有前面层之间建立密集的互连来增强低级视觉特征传播的参数调谐框架，从而实现有效的跨模式特征交互并适应未对齐的编码器。我们还建议使用文本适配器改善文本功能。我们简单而有效的方法极大地超过了最先进的方法，该方法具有0.9％至1.8％的骨干参数更新，对具有挑战性的基准进行了评估。我们的项目可在\ url {https://github.com/jiaqihuang01/detris}中找到。

### IE-Bench: Advancing the Measurement of Text-Driven Image Editing for Human Perception Alignment 
[[arxiv](https://arxiv.org/abs/2501.09927)] [[cool](https://papers.cool/arxiv/2501.09927)] [[pdf](https://arxiv.org/pdf/2501.09927)]
> **Authors**: Shangkun Sun,Bowen Qu,Xiaoyu Liang,Songlin Fan,Wei Gao
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: No comments
- **标题**: IE基础：推进文本驱动图像编辑的人类感知对齐方式的测量
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 文本驱动图像编辑的最新进展非常重要，但是准确评估这些编辑图像的任务继续构成了巨大的挑战。与对文本驱动图像生成的评估不同，文本驱动的图像编辑的特征是同时对文本和源图像进行调节。编辑的图像通常保留与原始图像的内在连接，该图像通过文本的语义动态变化。但是，以前的方法倾向于仅专注于文本图像对齐方式，或者与人类的看法不符。在这项工作中，我们介绍了文本驱动的图像编辑基准套件（IE Bench），以增强对文本驱动的编辑图像的评估。 IE台式包含一个数据库包含不同的源图像，各种编辑提示和相应的结果不同的编辑方法，以及25个受试者提供的总计3,010个平均意见分数（MOS）。此外，我们介绍了IE-QA，这是一种用于文本驱动图像编辑的多模式源感知质量评估方法。据我们所知，IE Bench提供了第一个用于文本驱动图像编辑的IQA数据集和模型。与以前的指标相比，广泛的实验证明了IE-QA在文本驱动的图像编辑任务上的出色主观对齐。我们将使所有相关的数据和代码可向公众使用。

### CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified Intermediate Representation 
[[arxiv](https://arxiv.org/abs/2501.09838)] [[cool](https://papers.cool/arxiv/2501.09838)] [[pdf](https://arxiv.org/pdf/2501.09838)]
> **Authors**: Alex Berian,Daniel Brignac,JhihYang Wu,Natnael Daba,Abhijit Mahalanobis
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: Accepted in the 2025 WACV workshop GeoCV
- **标题**: Crossodationalditality-Diffusion：多模式的新型视图合成与统一中间表示
- **领域**: 计算机视觉和模式识别,人工智能,图像和视频处理
- **摘要**: 地理空间成像利用了来自各种传感方式的数据，例如EO，SAR和LIDAR，从地面无人机到卫星视图。这些异质输入为场景理解提供了重要的机会，但在准确地解释几何形状时提出了挑战，尤其是在没有精确的地面真相数据的情况下。为了解决这个问题，我们提出了CrossModality -Diffusion，这是一个模块化框架，旨在在不同模式和观点上生成图像，而无需先前了解场景几何形状。 CrossModality-Diffusion采用特定于模态的编码器，这些编码器采用多个输入图像并产生与输入摄像机位置相对于其输入摄像头位置编码场景结构的几何特征量。放置特征量的空间是统一输入方式的共同点。这些特征量通过体积渲染技术从新颖的角度重叠并渲染到特征图像中。渲染的特征图像用作模式特异性扩散模型的调节输入，从而使新图像合成所需的输出方式。在本文中，我们表明，共同训练不同的模块可确保框架内所有模式的几何理解一致。我们验证了在合成造型剂汽车数据集上的CrossModalityDiffusion的功能，证明了其在跨多个成像方式和观点跨产生准确，一致的新型视图方面的有效性。

### PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery 
[[arxiv](https://arxiv.org/abs/2501.09826)] [[cool](https://papers.cool/arxiv/2501.09826)] [[pdf](https://arxiv.org/pdf/2501.09826)]
> **Authors**: Shristi Das Biswas,Matthew Shreve,Xuelu Li,Prateek Singhal,Kaushik Roy
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: No comments
- **标题**: 像素：基于潜在手术的基于Xemplar的渐进图像
- **领域**: 计算机视觉和模式识别
- **摘要**: 语言引导扩散模型的最新进展通常是由繁琐的及时工程瓶颈部颈部颈部塞入，以精确表达所需的更改。直观的替代呼吁从野外图像示例中呼吁指导，以帮助用户将他们想象中的编辑栩栩如生。基于当代示例的编辑方法回避利用通过预先存在的大型文本图像（TTI）模型所学到的丰富潜在空间，并以精心策划的目标功能进行培训以完成任务。尽管有些有效，但这需要大量的计算资源，并且缺乏与不同的基本模型和任意示例计数的兼容性。在进一步调查中，我们还发现这些技术限制了用户控制，仅在整个编辑的区域中应用统一的全球变化。在本文中，我们介绍了一个新颖的框架，该框架是针对以现成的扩散模型（称为Pixels）的渐进式驱动编辑的，以通过对编辑进行粒度控制，从而在像素或区域级别进行调整，从而实现自定义。我们的方法仅在推理期间运行，以促进模仿性编辑，使用户能够从动态数量的参考图像或多模式提示中汲取灵感，并逐渐合并所有所需的更改，而无需重新调整或微调现有的TTI模型。这种精细控制控制的能力开辟了一系列新的可能性，包括选择性修改单个对象并指定逐渐的空间变化。我们证明，像素可以有效地提供高质量的编辑，从而显着改善了定量指标和人类评估。通过使高质量的图像编辑更易于访问，Pixels有可能在使用任何开源图像生成模型的情况下向更广泛的受众提供专业级的编辑。

### Distilling Multi-modal Large Language Models for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2501.09757)] [[cool](https://papers.cool/arxiv/2501.09757)] [[pdf](https://arxiv.org/pdf/2501.09757)]
> **Authors**: Deepti Hegde,Rajeev Yasarla,Hong Cai,Shizhong Han,Apratim Bhattacharyya,Shweta Mahajan,Litian Liu,Risheek Garrepalli,Vishal M. Patel,Fatih Porikli
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: No comments
- **标题**: 蒸馏用于自动驾驶的多模式大型语言模型
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 自动驾驶需要安全的运动计划，尤其是在关键的“长尾”场景中。最近的端到端自动驾驶系统利用大型语言模型（LLM）作为计划者，以提高对罕见事件的普遍性。但是，在测试时间使用LLM会引入高计算成本。为了解决这个问题，我们提出了DiMa，这是一种端到端的自动驾驶系统，在利用LLM的世界知识的同时，保持了无LLM（或基于视觉的）计划者的效率。 DIMA通过一组专门设计的替代任务将信息从多模式LLM提炼为基于视觉的端到端计划者。在联合培训策略下，两个网络共有的场景编码器产生的结构化表示形式是基于语义的，并且与最终的计划目标保持一致。值得注意的是，LLM在推理时是可选的，可以在不损害效率的情况下进行健壮的计划。 DIMA训练导致L2轨迹误差降低了37％，基于视力的计划者的碰撞率降低了80％，在长尾方案中降低了44％的轨迹误差。 Dima还在Nuscenes规划基准中实现了最先进的表现。

### A Simple Aerial Detection Baseline of Multimodal Language Models 
[[arxiv](https://arxiv.org/abs/2501.09720)] [[cool](https://papers.cool/arxiv/2501.09720)] [[pdf](https://arxiv.org/pdf/2501.09720)]
> **Authors**: Qingyun Li,Yushi Chen,Xinya Shu,Dong Chen,Xin He,Yi Yu,Xue Yang
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: 4 pages, 1 table, 4 figures
- **标题**: 多模式模型的简单空中检测基线
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 基于生成预训练的变压器的多模式语言模型（MLM）被认为是统一各种领域和任务的强大候选者。用于遥感（RS）开发的MLM在多个任务中表现出了出色的性能，例如视觉问题答案和视觉接地。除了检测到与给定指令相对应的特定对象的视觉接地外，空中检测检测多个类别的所有对象，也是RS基础模型的一项宝贵且具有挑战性的任务。但是，现有RS MLM尚未探索空中检测，因为MLMS的自回归预测机制与检测输出明显不同。在本文中，我们提出了一个简单的基线，用于首次将MLMS应用于空中检测，名为Lmmrotate。具体而言，我们首先引入了一种归一化方法，将检测输出转换为文本输出，以与MLM框架兼容。然后，我们提出了一种评估方法，该方法可确保MLM和常规对象检测模型之间的公平比较。我们通过微调开源通用传销MLM来构建基线，并实现与常规检测器相当的令人印象深刻的检测性能。我们希望该基线将成为未来MLM开发的参考，从而为理解RS图像提供更全面的功能。代码可在https://github.com/li-qingyun/mllm-mmrotate上找到。

### Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images 
[[arxiv](https://arxiv.org/abs/2501.09552)] [[cool](https://papers.cool/arxiv/2501.09552)] [[pdf](https://arxiv.org/pdf/2501.09552)]
> **Authors**: Tuan Truong,Ivo M. Baltruschat,Mark Klemens,Grit Werner,Matthias Lenga
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: In progress
- **标题**: 在医学图像中探索基于AI的系统设计，以进行像素级保护的健康信息检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 目的：本研究旨在评估基于AI的解决方案的不同设置，以检测医学图像中受保护的健康信息（PHI）。材料和方法：模拟了八个PHI和八个非PHI类别的文本，并将其整合到一个策划的数据集中，该数据集构成了四种模式的1,000张医学图像：CT，X射线，骨扫描和MRI。提出的PHI检测管道包括三个关键组成部分：文本定位，提取和分析。三种视觉和语言模型，即Yolov11，EasyORC和GPT-4O，在与三个关键组件相对应的不同设置中进行了标准。通过分类指标评估性能，包括精度，召回，F1分数和准确性。结果：所有四个设置在检测PHI烙印方面均表现出强烈的性能，所有指标均超过0.9。利用Yolov11进行文本本地化和GPT-4O进行文本提取和分析的设置在PHI检测中的性能最高。但是，由于与GPT-4O模型相关的产生代币数量增加，这种设置的成本最高。相反，仅使用GPT-4O进行端到端管道的设置表现出最低的性能，但展示了多模式模型在解决复杂任务方面的可行性。结论：对于最佳的文本定位和提取，建议微调对象检测模型并使用内置的光学字符识别（OCR）软件。可以有效利用诸如GPT-4O之类的大型语言模型来推理和语义分析PHI含量。尽管GPT-4O的视觉能力对于阅读图像作物具有希望，但对于具有整个图像的端到端管道应用程序仍然有限。

### Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis 
[[arxiv](https://arxiv.org/abs/2501.09502)] [[cool](https://papers.cool/arxiv/2501.09502)] [[pdf](https://arxiv.org/pdf/2501.09502)]
> **Authors**: Qize Yang,Detao Bai,Yi-Xing Peng,Xihan Wei
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: No comments
- **标题**: Omni-Enotion：扩展视频MLLM，具有详细的面部和音频建模，用于多模式情绪分析
- **领域**: 计算机视觉和模式识别
- **摘要**: 准确地理解情绪对于人类计算机互动等领域至关重要。由于情绪的复杂性及其多模式的性质（例如，情绪受到面部表情和音频的影响），研究人员已转向使用多模型来理解人类的情绪，而不是单模式。但是，当前的视频多模式大型语言模型（MLLM）在有效整合音频并识别微妙的面部微表达时会遇到困难。此外，缺乏详细的情感分析数据集还限制了多模式情绪分析的发展。为了解决这些问题，我们介绍了一个自我审查的数据集和经过人工评论的数据集，分别包含24,137个粗粒样本和3,500个手动注释的样本，并带有详细的情感注释。这些数据集允许模型从不同的方案中学习，并更好地推广到现实世界的应用程序。此外，除了音频建模外，我们还建议将面部编码模型明确整合到现有的高级视频MLLM中，从而使MLLM有效地统一音频和微妙的面部提示，以了解情感。通过在统一的空间中调整这些功能并在我们提出的数据集中使用指令调整，我们的Omni-Ontion在情感识别和推理任务中都实现了最先进的表现。

### VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization 
[[arxiv](https://arxiv.org/abs/2501.09499)] [[cool](https://papers.cool/arxiv/2501.09499)] [[pdf](https://arxiv.org/pdf/2501.09499)]
> **Authors**: Zixun Fang,Zhiheng Liu,Kai Zhu,Yu Liu,Ka Leong Cheng,Wei Zhai,Yang Cao,Zheng-Jun Zha
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: No comments
- **标题**: Vangogh：统一的视频着色基于多模式扩散的框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频着色旨在将灰度视频转变为生动的色彩表示，同时保持时间一致性和结构完整性。现有的视频着色方法通常会遭受颜色出血和缺乏全面的控制，尤其是在复杂的运动或多样化的语义提示下。为此，我们介绍了Vangogh，这是一个统一的基于多模式扩散的框架，用于视频着色。 Vangogh使用双Qformer解决了这些挑战，以使多种模态的特征对齐和融合功能，并以深度引导的生成过程和光流损失进行补充，这有助于减少颜色溢出。此外，还实施了颜色注入策略和Luma通道更换，以改善概括并减轻闪烁的工件。借助此设计，用户可以对生成过程进行全球和本地控制，从而产生更高质量的色彩视频。广泛的定性和定量评估以及用户研究表明，Vangogh实现了较高的时间一致性和颜色保真度。

### Vision-Language Models Do Not Understand Negation 
[[arxiv](https://arxiv.org/abs/2501.09425)] [[cool](https://papers.cool/arxiv/2501.09425)] [[pdf](https://arxiv.org/pdf/2501.09425)]
> **Authors**: Kumail Alhamoud,Shaden Alshammari,Yonglong Tian,Guohao Li,Philip Torr,Yoon Kim,Marzyeh Ghassemi
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: Project page: https://negbench.github.io
- **标题**: 视觉语言模型不了解否定
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 许多实用的视觉语言应用都需要了解否定的模型，例如，当使用自然语言检索包含某些对象而不是其他对象的图像时。尽管通过大规模培训在视觉模型（VLM）方面取得了进步，但他们理解否定的能力仍未得到充实。这项研究解决了以下问题：当前VLM的否定如何？我们介绍了Negbench，这是一种新的基准测试，旨在评估跨越图像，视频和医疗数据集的18个任务变化和79K示例的否定理解。该基准由两个核心任务组成，旨在评估各种多模式设置中的否定理解：取回否定和带有否定字幕的多项选择问题。我们的评估表明，现代VLM在否定方面存在重大斗争，通常在偶然的水平上表现。为了解决这些缺点，我们探讨了一种以数据为中心的方法，其中我们在包含数百万个否定字幕的大规模合成数据集中的芬特剪辑模型。我们表明，这种方法可以导致否定查询的召回率增加了10％，并且具有否定字幕的多项选择问题的准确性提高了40％。

### KPL: Training-Free Medical Knowledge Mining of Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.11231)] [[cool](https://papers.cool/arxiv/2501.11231)] [[pdf](https://arxiv.org/pdf/2501.11231)]
> **Authors**: Jiaxiang Liu,Tianxiang Hu,Jiawei Du,Ruiyuan Zhang,Joey Tianyi Zhou,Zuozhu Liu
> **First submission**: 2025-01-19
> **First announcement**: 2025-01-20
> **comment**: AAAI(Oral)
- **标题**: KPL：视觉模型的无培训医学知识挖掘
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型，例如由于广泛的图像文本预训练而引起的图像识别中的剪辑模型。但是，在零摄像分类中应用剪辑推断，尤其是对于医学图像诊断，面临以下挑战：1）仅代表单个类别名称的图像类别不足； 2）剪辑编码生成的视觉空间和文本空间之间的模态差距。尽管试图通过大语言模型来丰富疾病描述，但缺乏特定阶级知识通常会导致表现不佳。此外，经验证据表明，当应用于医疗数据集时，自然图像数据集上的零照片图像分类的现有代理学习方法表现出不稳定。为了应对这些挑战，我们将知识代理学习（KPL）介绍给剪辑中的知识。 KPL旨在通过文本代理优化和多模式代理学习来利用Clip的多模式理解对医学图像进行分类。具体而言，KPL从构造的知识增强基础中检索了与图像相关的知识描述，以丰富语义文本代理。然后，它利用了通过剪辑编码的输入图像和这些描述，以稳定地生成多模式的代理，从而提高零摄像分类性能。在医学和自然图像数据集上进行的广泛实验表明，KPL可以实现有效的零照片分类，表现优于所有基准。这些发现突出了从剪辑中的矿业知识范式进行医学图像分类和更广泛的领域的巨大潜力。

### Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding 
[[arxiv](https://arxiv.org/abs/2501.10967)] [[cool](https://papers.cool/arxiv/2501.10967)] [[pdf](https://arxiv.org/pdf/2501.10967)]
> **Authors**: Zhanpeng Chen,Mingxiao Li,Ziyang Chen,Nan Du,Xiaolong Li,Yuexian Zou
> **First submission**: 2025-01-19
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: 促进具有金字塔散发视觉位置的视觉模型的一般多模式的能力编码
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 视觉语言模型（VLM）在推进通用人工智能方面表现出了显着的功能，但是，视觉位置的非理性编码持续抑制模型在不同级别的粒度上的全面感知表现。在这项工作中，我们提出了一种新颖的方法编码（Pype），这是一种新型方法，旨在增强VLM中视觉令牌的感知。通过将视觉位置索引从外围分配到中心，并逐步扩展中央接受场，Pype解决了传统的栅格扫描方法的局限性，并减轻了由旋转位置嵌入（Rope）引起的长期衰减效应。我们的方法减少了相互关联的视觉元素和指令令牌之间的相对距离，促进了更合理的注意力权重分配，并允许对视觉元素的多粒度感知，并对锚定令牌的过度依赖。广泛的实验评估表明，Pype始终提高VLM在各种尺寸的一般能力。代码可在https://github.com/sakuratroychen/pype上找到。

### Rethinking Early-Fusion Strategies for Improved Multimodal Image Segmentation 
[[arxiv](https://arxiv.org/abs/2501.10958)] [[cool](https://papers.cool/arxiv/2501.10958)] [[pdf](https://arxiv.org/pdf/2501.10958)]
> **Authors**: Zhengwen Shen,Yulian Li,Han Zhang,Yuchen Weng,Jun Wang
> **First submission**: 2025-01-19
> **First announcement**: 2025-01-20
> **comment**: Accepted by ICASSP 2025
- **标题**: 重新思考改进多模式图像分割的早期融合策略
- **领域**: 计算机视觉和模式识别
- **摘要**: RGB和热图像融合具有在低截至条件下表现出改善语义分割的巨大潜力。现有方法通常采用多模式特征提取和设计复杂的特征融合策略来实现特征提取和融合以进行多模式语义分段的多模式特征提取和设计复杂的特征融合策略。但是，这些方法需要在特征提取和融合过程中进行大量参数更新和计算工作。为了解决这个问题，我们提出了一个新型的多模式融合网络（EFNET），基于早期的融合策略以及一种简单但有效的功能聚类，用于训练有效的RGB-T语义分段。此外，我们还提出了一个基于欧几里得距离的轻质和高效的多尺度特征聚合解码器。我们验证方法在不同数据集上的有效性，并且胜过较低参数和计算的先前最先前的方法。

### Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition 
[[arxiv](https://arxiv.org/abs/2501.10917)] [[cool](https://papers.cool/arxiv/2501.10917)] [[pdf](https://arxiv.org/pdf/2501.10917)]
> **Authors**: Haoyu Xie,Haoxuan Li,Chunyuan Zheng,Haonan Yuan,Guorui Liao,Jun Liao,Li Liu
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: 分解和融合多传感器可穿戴的人类活动识别的传感器间和传感器间时空信号
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **摘要**: 可穿戴的人类活动识别（WHAR）是无处不在的计算中的重要研究领域。事实证明，多传感器同步测量比使用单个传感器更有效。但是，现有的壁方法使用共享的卷积内核来对每个传感器变量进行不加区分的时间特征提取，这无法有效捕获传感器内和传感器间变量的时空关系。我们提出了由分解阶段和融合阶段组成的分解模型，以更好地模拟模态变量之间的关系。分解通过改进的深度可分离卷积创造了每个传感器内变量的高维表示，以捕获局部时间特征，同时保留其独特的特征。融合阶段首先捕获传感器内变量之间的关系，并在通道和变量级别融合其特征。使用状态空间模型（SSM）对远程时间依赖性进行建模，然后通过自我发挥的机制动态捕获跨传感器相互作用，突出了传感器间的空间相关性。我们的模型在三个广泛使用的码头数据集上表现出了卓越的性能，在保持可接受的计算效率的同时，大大优于最先进的模型。我们的代码和补充材料可在https://github.com/anakin2555/decomposewhar上找到。

### Know "No" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP 
[[arxiv](https://arxiv.org/abs/2501.10913)] [[cool](https://papers.cool/arxiv/2501.10913)] [[pdf](https://arxiv.org/pdf/2501.10913)]
> **Authors**: Junsung Park,Jungbeom Lee,Jongyoon Song,Sangwon Yu,Dahuin Jung,Sungroh Yoon
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: 知道“否”更好：一种以数据为基础的方法来增强剪辑中的否定意识
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 尽管剪辑通过桥接视觉和语言具有显着高级的多模式理解，但无法掌握否定 - 例如未能区分诸如“停车”与“无停车”之类的概念，却带来了重大挑战。通过分析公共剪辑模型的预训练中使用的数据，我们提出了这种限制，这是由于缺乏否定的数据。为了解决这个问题，我们介绍了采用大型语言模型（LLM）和多模式LLM产生否定字幕的数据生成管道。通过管道产生的数据进行微调剪辑，我们开发了NegationClip，从而在保留一般性的同时增强了否定意识。此外，为了对否定理解进行全面的评估，我们提出了NegreFcocog-A量身定制的基准测试，以测试VLMS在句子中解释各种表达式和位置的否定能力。在各种夹子体系结构上进行的实验验证了我们数据生成管道在增强夹子准确感知否定能力方面的有效性。此外，NegationClip的增强否定意识在各种多模式任务中具有实际应用，这是通过文本到图像生成和参考图像细分的性能提高所证明的。

### Visual RAG: Expanding MLLM visual knowledge without fine-tuning 
[[arxiv](https://arxiv.org/abs/2501.10834)] [[cool](https://papers.cool/arxiv/2501.10834)] [[pdf](https://arxiv.org/pdf/2501.10834)]
> **Authors**: Mirco Bonomo,Simone Bianco
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: 视觉抹布：扩展MLLM视觉知识而无需微调
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 多模式的大语言模型（MLLM）在需要跨视觉和文本方式推理的计算机视觉任务中取得了显着的性能，但是它们的功能仅限于其预训练的数据，需要进行大量的微调以进行更新。最近的研究探索了使用文章学习（ICL）来克服这些挑战，通过提供一组演示示例作为在多个任务中增强MLLMS绩效的背景，这表明与几乎没有射击ICL相比，许多射击ICL可以实质性改进。但是，依赖众多示例和有限的MLLM上下文Windows的依赖存在很大的障碍。本文旨在通过引入一种新颖的方法，即视觉抹布来解决这些挑战，该方法可以协同结合MLLMS从上下文中学习的能力和检索机制。这种方法的症结在于仅选择最相关的查询示例来确保增强MLLM知识，从而促使其类比学习。通过这种方式，依靠推理时间动态提供的新信息，结果系统不限于从培训数据中提取的知识，而是可以在不进行微调的情况下快速而轻松地更新。此外，这大大降低了改善模型图像分类性能的计算成本，并将模型知识增强到未经训练的新的视觉域和任务中。在跨越几个领域和图像分类任务的八个不同数据集上进行的广泛实验表明，与最新的最新技术相比，所提出的视觉抹布（即，许多弹药ICL）能够获得非常接近甚至更高的准确性（平均 +2％），同时使用了大约较小的示例（大约平均示例）。

### LD-DETR: Loop Decoder DEtection TRansformer for Video Moment Retrieval and Highlight Detection 
[[arxiv](https://arxiv.org/abs/2501.10787)] [[cool](https://papers.cool/arxiv/2501.10787)] [[pdf](https://arxiv.org/pdf/2501.10787)]
> **Authors**: Pengcheng Zhao,Zhixian He,Fuwei Zhang,Shujin Lin,Fan Zhou
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: LD-DETR：循环解码器检测变压器用于视频瞬间检索并突出显示检测
- **领域**: 计算机视觉和模式识别,信息检索,机器学习
- **摘要**: 视频力矩检索并突出显示检测目的是根据文本查询在视频中找到相应的内容。现有模型通常首先使用对比度学习方法来对齐视频和文本功能，然后将其融合和提取多模式信息，最后使用变压器解码器来解码多模式信息。但是，现有方法面临几个问题：（1）数据集中不同样本之间的语义信息重叠，阻碍了模型的多模式对齐性能； （2）现有模型无法有效提取视频的本地功能； （3）现有模型使用的变压器解码器无法充分解码多模式特征。为了解决上述问题，我们提出了视频时刻检索的LD-DETR模型，并突出显示检测任务。具体而言，我们首先将相似性矩阵提炼到身份矩阵中，以减轻重叠语义信息的影响。然后，我们设计了一种方法，该方法使卷积层可以更有效地提取多模式局部特征。最后，我们将变压器解码器的输出返回到自身中，以充分解码多模式信息。我们在四个公共基准上评估了LD-detr，并进行了广泛的实验，以证明我们方法的优势和有效性。我们的模型优于QVHighlight，Charades-STA和Tacos数据集上的最新模型。我们的代码可在https://github.com/qingchen239/ld-detr上找到。

### Multi-modal Fusion and Query Refinement Network for Video Moment Retrieval and Highlight Detection 
[[arxiv](https://arxiv.org/abs/2501.10692)] [[cool](https://papers.cool/arxiv/2501.10692)] [[pdf](https://arxiv.org/pdf/2501.10692)]
> **Authors**: Yifang Xu,Yunzhuo Sun,Benxiang Zhai,Zien Xie,Youyao Jia,Sidan Du
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: Accepted by ICME 2024
- **标题**: 视频时刻检索的多模式融合和查询精炼网络并突出显示检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 鉴于视频和语言查询，视频瞬间检索并突出显示检测（MR＆HD）旨在定位所有相关的跨度，同时预测显着性得分。大多数现有方法都使用RGB图像作为输入，俯瞰固有的多模式视觉信号，例如光流和深度。在本文中，我们提出了一个多模式融合和查询细化网络（MRNET），以从多模式提示中学习互补信息。具体而言，我们设计了一个多模式融合模块，以动态组合RGB，光流和深度图。此外，为了模拟人类对句子的理解，我们介绍了一个查询细化模块，该模块将文本融合在不同的粒度上，其中包含单词，短语和句子级别。关于QVHighlights和Charades数据集的全面实验表明，MRNET优于当前最新方法，在MR-MAP@AVG（+3.41）和HD-HIT@1（+3.46）上取得了显着改善。

### Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No! 
[[arxiv](https://arxiv.org/abs/2501.10674)] [[cool](https://papers.cool/arxiv/2501.10674)] [[pdf](https://arxiv.org/pdf/2501.10674)]
> **Authors**: Mohamed Fazli Imam,Chenyang Lyu,Alham Fikri Aji
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: Our dataset can be found at \url{https://huggingface.co/datasets/fazliimam/temporal-vqa}
- **标题**: 多模式LLM可以进行视觉时间理解和推理吗？答案是否！
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式大型语言模型（MLLM）通过利用基础大语言模型（LLM）在视觉问题回答（VQA）等任务中取得了重大进步。但是，它们在特定领域（例如视觉时间理解）的能力，这对于理解现实世界动态至关重要，仍然没有被逐渐解散。为了解决这个问题，我们提出了一个具有挑战性的评估基准，称为tuer​​malvqa，由两个部分组成：1）时间顺序理解和2）延时估计。第一部分要求MLLM通过分析时间连续的视频帧来确定事件的顺序。第二部分呈现图像对，具有不同的时间差异，将其构成多项选择问题，要求MLLM估计图像之间的延时，选项范围从秒到几年。我们对高级MLLM的评估，包括诸如GPT-4O和Gemini-1.5-Pro之类的模型，揭示了重大挑战：GPT-4O在时间顺序任务中仅达到49.1％的平均一致准确性，而在延时估计中仅实现了70％，开源模型的表现较差。这些发现强调了当前MLLM在视觉时空理解和推理中的局限性，强调了需要进一步改进其时间能力的局限性。可以在https://huggingface.co/datasets/fazliimam/temporal-vqa上找到我们的数据集。

### When language and vision meet road safety: leveraging multimodal large language models for video-based traffic accident analysis 
[[arxiv](https://arxiv.org/abs/2501.10604)] [[cool](https://papers.cool/arxiv/2501.10604)] [[pdf](https://arxiv.org/pdf/2501.10604)]
> **Authors**: Ruixuan Zhang,Beichen Wang,Juexiao Zhang,Zilin Bian,Chen Feng,Kaan Ozbay
> **First submission**: 2025-01-17
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: 当语言和视觉符合道路安全时：利用多模式的大语模型进行基于视频的交通事故分析
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 在24/7/365时间范围内运行的流量视频的可用性增加具有增加交通事故的时空覆盖范围的巨大潜力，这将有助于提高交通安全性。但是，在24/7/365的工作协议中分析数百个（即使不是数千个）的交通摄像机的录像仍然是一项极具挑战性的任务，因为基于当前的基于视觉的方法主要集中在提取原始信息（例如车辆轨迹或个人对象检测）上，但需要经费的后处理后处理以获得可行的洞察力。我们提出了Seeunsafe，这是一个新的框架，该框架集成了多模式大语言模型（MLLM）代理，以将基于视频的交通事故分析从传统的提取 - 转换工作流程转换为更具交互性的，更具交互性的对话方法。这种转变可以通过自动化视频分类和视觉接地等复杂任务来显着增强处理吞吐量，同时通过对各种交通情况和用户定义的查询进行无缝调整来提高适应性。我们的框架采用了基于严重性的聚合策略来处理各种长度的视频，并采用了新颖的多模式提示来生成结构化响应，以进行审查和评估，并实现细粒度的视觉接地。我们介绍了IMS（信息匹配分数），这是一种基于MLLM的新指标，用于将结构化响应与地面真相保持一致。我们在丰田编织的交通安全数据集上进行了广泛的实验，证明Seeunsafe通过利用现成的MLLM来有效地执行事故感知的视频分类和视觉接地。源代码将在\ url {https://github.com/ai4ce/seeunsafe}上获得。

### FaceXBench: Evaluating Multimodal LLMs on Face Understanding 
[[arxiv](https://arxiv.org/abs/2501.10360)] [[cool](https://papers.cool/arxiv/2501.10360)] [[pdf](https://arxiv.org/pdf/2501.10360)]
> **Authors**: Kartik Narayan,Vibashan VS,Vishal M. Patel
> **First submission**: 2025-01-17
> **First announcement**: 2025-01-20
> **comment**: Project Page: https://kartik-3004.github.io/facexbench/
- **标题**: facexbench：评估脸部理解的多模式LLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）表现出在各种任务和域中的令人印象深刻的解决问题的能力。但是，他们的面部理解能力尚未系统地研究。为了解决这一差距，我们介绍了facexbench，这是一个综合基准，旨在评估复杂面部理解任务的MLLM。 Facexbench包括5,000个多模式多项选择问题，这些问题来自25个公共数据集和一个新创建的数据集facexapi。这些问题涵盖了6个广泛类别的14个任务，评估了MLLM在偏见和公平，面对身份验证，识别，分析，本地化和工具检索方面的面部理解能力。使用FaceXbench，我们与2个专有模型一起对26个开源MLLM进行了广泛的评估，从而揭示了复杂的面部理解任务所面临的独特挑战。我们在三个评估设置中分析了模型：零射击，内部文化任务描述和经过思考链的提示。我们的详细分析表明，当前的MLLM，包括诸如GPT-4O和Geminipro 1.5的高级模型，都显示了很大的改进空间。我们认为，Facexbench将是开发配备有精致面部理解的MLLM的关键资源。代码：https：//github.com/kartik-3004/facexbench

### Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and MModalCC Framework 
[[arxiv](https://arxiv.org/abs/2501.10075)] [[cool](https://papers.cool/arxiv/2501.10075)] [[pdf](https://arxiv.org/pdf/2501.10075)]
> **Authors**: Ali Can Karaca,M. Enes Ozelbas,Saadettin Berber,Orkhan Karimli,Turabi Yildirim,M. Fatih Amasyali
> **First submission**: 2025-01-17
> **First announcement**: 2025-01-20
> **comment**: This work has been submitted to the IEEE Transactions on Geoscience and Remote Sensing journal for possible publication
- **标题**: 遥感中强大的更改字幕：第二cc数据集和mmodalcc框架
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体
- **摘要**: 遥感变更字幕（RSICC）旨在描述自然语言中的零花态图像之间的变化。现有方法通常在挑战之类的挑战中失败，例如差异，观点变化，模糊效果，导致不准确，尤其是在不变地区。此外，以不同的空间分辨率获取的图像并具有注册错误往往会影响字幕。为了解决这些问题，我们介绍了Second-CC，这是一个具有高分辨率RGB图像对，语义分割图和各种真实世界情景的新型RSICC数据集。第二cc包含6,041对Bitemoral RS图像和30,205个句子，描述了图像之间的差异。此外，我们提出了MMODALCC，这是一种多模式框架，该框架使用高级注意机制（包括跨模式交叉注意（CMCA）和多模式门控交叉注意（MGCA））整合语义和视觉数据。详细的消融研究和注意力可视化进一步证明了其有效性和应对RSICC挑战的能力。综合实验表明，MMODALCC的表现优于最先进的RSICC方法，包括RSICCFORNER，CHG2CAP和PSNET，BLEU4分数提高了4.6％，苹果酒评分提高了9.6％。我们将在https://github.com/changecapsinrs/secondcc上公开提供数据集和代码库，以促进未来的研究

### FiLo++: Zero-/Few-Shot Anomaly Detection by Fused Fine-Grained Descriptions and Deformable Localization 
[[arxiv](https://arxiv.org/abs/2501.10067)] [[cool](https://papers.cool/arxiv/2501.10067)] [[pdf](https://arxiv.org/pdf/2501.10067)]
> **Authors**: Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Ming Tang,Jinqiao Wang
> **First submission**: 2025-01-17
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: FILO ++：通过融合的细粒描述和可变形的定位零 - /少数射击异常检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 异常检测方法通常需要来自目标类别的广泛正常样本进行训练，从而限制了它们在需要快速适应（例如冷启动）的情况下的适用性。零射击和少量射击异常检测不需要事先从目标类标记的样品，这使它们成为有前途的研究方向。现有的零射击和少量射击方法通常利用强大的多模型模型来通过比较图像文本相似性来检测和定位异常。但是，他们手工制作的通用描述未能捕获可能在不同对象中出现的各种异常范围，而简单的贴片级图像文本匹配通常会努力努力地定位各种形状和大小的异常区域。为了解决这些问题，本文提出了Filo ++方法，该方法由两个关键组成部分组成。第一个组件，融合的细粒描述（FUSDES），利用大型语言模型为每个对象类别生成异常描述，结合了固定和可学习的提示模板并应用运行时提示方法，生成了更准确和更精确的特定于任务的文本描述。第二个组件，可变形的定位（DEFLOC），将视觉基础模型接地Dino与位置增强的文本描述和多尺度可变形的跨模式相互作用（MDCI）模块集成在一起，从而使异常定位具有各种形状和尺寸。此外，我们设计了一种具有位置增强的补丁匹配方法，以改善几射击异常检测性能。多个数据集的实验表明，与现有方法相比，FILO ++可以实现显着的性能改进。代码将在https://github.com/casia-iva-lab/filo上找到。

### Multi-Modal Attention Networks for Enhanced Segmentation and Depth Estimation of Subsurface Defects in Pulse Thermography 
[[arxiv](https://arxiv.org/abs/2501.09994)] [[cool](https://papers.cool/arxiv/2501.09994)] [[pdf](https://arxiv.org/pdf/2501.09994)]
> **Authors**: Mohammed Salah,Naoufel Werghi,Davor Svetinovic,Yusra Abdulrahman
> **First submission**: 2025-01-17
> **First announcement**: 2025-01-20
> **comment**: Pulse thermography, infrared thermography, defect segmentation,multi-modalnetworks, attention mechanism
- **标题**: 多模式注意网络，用于增强脉冲热成像中地下缺陷的分割和深度估计的多模式注意网络
- **领域**: 计算机视觉和模式识别,人工智能,图像和视频处理
- **摘要**: AI驱动的脉冲热力计（PT）已成为非破坏性测试（NDT）的关键工具，从而可以自动检测各种工业组件中的隐藏异常。当前的最新技术使用主成分分析（PCA）或热力计信号重建（TSR）压缩PT序列，进给分割和深度估计网络。但是，由于这些表示具有互补的语义特征，因此对这两种方式进行独立限制了PT检查模型的性能。为了解决这一限制，这项工作提出了PT-Fusion，这是一种基于多模式的融合网络，它可以融合PCA和TSR模式，以用于缺陷分割和PT设置中地下缺陷的深度估计。 PT融合引入了新型特征融合模块，编码器注意融合门（EAFG）和注意力增强的解码块（AEDB），以融合PCA和TSR特征，以增强地下缺陷的分割和深度估计。此外，提出了一种新型的数据增强技术，它是基于从热力计序列的随机数据采样提出的，以减轻PT数据集​​的稀缺性。提出的方法针对最新的PT检查模型进行了基准测试，包括U-NET，注意力NET和3D-CNN，UniversitéLavalirt-PVC数据集。结果表明，PT融合在缺陷分段和深度估计准确度中的表现优于上述模型，边缘为10％。

### EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents 
[[arxiv](https://arxiv.org/abs/2501.11858)] [[cool](https://papers.cool/arxiv/2501.11858)] [[pdf](https://arxiv.org/pdf/2501.11858)]
> **Authors**: Zhili Cheng,Yuge Tu,Ran Li,Shiqi Dai,Jinyi Hu,Shengding Hu,Jiahao Li,Yang Shi,Tianyu Yu,Weize Chen,Lei Shi,Maosong Sun
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 体现：评估多模式LLM作为具体剂
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式的大语言模型（MLLM）显示出很大的进步，为具体的代理提供了有希望的未来。用于评估MLLM的现有基准主要利用静态图像或视频，将评估限制为非相互作用的情况。同时，现有的体现的AI基准是特定于任务的，并且不够多样化，这些基准不能充分评估MLLM的具体功能。为了解决这个问题，我们提出了BebsodiedeVal，这是具有具体任务的MLLM的全面和交互式评估基准。体现expodiedeval具有328个不同的3D场景中的不同任务，每个场景都经过严格选择和注释。它涵盖了具有显着增强多样性的现有体现的AI任务的广泛范围，这些任务均在针对MLLM量身定制的统一模拟和评估框架内。这些任务分为五个类别：导航，对象互动，社交互动，属性问答和空间问题回答以评估代理的不同功能。我们在体现上评估了最新的MLLM，并发现它们与体现的任务相比具有明显的缺口。我们的分析证明了现有的MLLM在具体能力中的局限性，为其未来发展提供了见解。我们在https://github.com/thunlp/embodiedeval上开放所有评估数据和仿真框架。

### A Review Paper of the Effects of Distinct Modalities and ML Techniques to Distracted Driving Detection 
[[arxiv](https://arxiv.org/abs/2501.11758)] [[cool](https://papers.cool/arxiv/2501.11758)] [[pdf](https://arxiv.org/pdf/2501.11758)]
> **Authors**: Anthony. Dontoh,Stephanie. Ivey,Logan. Sirbaugh,Armstrong. Aboah
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 评论论文，介绍了分散注意力驾驶检测的不同方式和ML技术的影响
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 分散注意力的驾驶仍然是严重的人类和经济影响，要求改进检测和干预策略。尽管以前的研究已经广泛探讨了单模式方法，但最近的研究表明，这些系统通常在识别复杂的分心模式（尤其是认知分心）方面缺乏。这项系统的审查通过提供对机器学习（ML）和深度学习（DL）技术的全面分析（在各种数据模式）（视觉，感觉，听觉和多模式）上应用的全面分析来解决关键差距。通过基于模式，数据可访问性和方法的研究进行分类和评估，本综述阐明了方法的准确性最高，并且最适合特定分心的驾驶检测目标。这些发现提供了有关多模式与单模式系统优势的明确指导，并捕获了该领域的最新进步。最终，这篇综述为开发强大的分散注意力驾驶检测框架提供了宝贵的见解，支持了增强的道路安全和缓解策略。

### Teaching Large Language Models to Regress Accurate Image Quality Scores using Score Distribution 
[[arxiv](https://arxiv.org/abs/2501.11561)] [[cool](https://papers.cool/arxiv/2501.11561)] [[pdf](https://arxiv.org/pdf/2501.11561)]
> **Authors**: Zhiyuan You,Xin Cai,Jinjin Gu,Tianfan Xue,Chao Dong
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 教授大型语言模型，使用分数分布回归准确的图像质量得分
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着多模式大语言模型（MLLM）的快速发展，基于MLLM的图像质量评估（IQA）方法在语言质量描述中表现出了有希望的表现。但是，当前方法仍然无法准确评分图像质量。在这项工作中，我们旨在利用MLLM来回归准确的质量分数。一个关键的挑战是，质量得分本质上是连续的，通常以高斯分布为模型，而MLLMS会产生离散令牌输出。这种不匹配需要得分离散化。以前的方法将平均分数离散为单热标签，从而导致信息丢失和未能捕获图像间的关系。我们提出了一种基于分布的方法，该方法将分数分布分配到软标签中。该方法保留了分数分布的特征，可实现高精度并保持图像间关系。此外，要解决数据集变化，不同的IQA数据集展示了各种分布，我们引入了基于Thurstone模型的忠实损失。这种损失捕获了数据集的关系，从而促进了多个IQA数据集的共同培训。通过这些设计，我们为分数回归（DEQA得分）开发了基于分布的图像质量评估模型。跨多个基准测试的实验表明，DEQA得分在得分回归中均优于基准。同样，DEQA得分可以预测与人类注释紧密一致的分数分布。代码和模型权重已在https://depictqa.github.io/deqa-score/中发布。

### A baseline for machine-learning-based hepatocellular carcinoma diagnosis using multi-modal clinical data 
[[arxiv](https://arxiv.org/abs/2501.11535)] [[cool](https://papers.cool/arxiv/2501.11535)] [[pdf](https://arxiv.org/pdf/2501.11535)]
> **Authors**: Binwu Wang,Isaac Rodriguez,Leon Breitinger,Fabian Tollens,Timo Itzel,Dennis Grimm,Andrei Sirazitdinov,Matthias Frölich,Stefan Schönberg,Andreas Teufel,Jürgen Hesser,Wenzhao Zhao
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 使用多模式临床数据的基于机器学习的基线肝癌诊断
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文的目的是为在新型的肝细胞癌（HCC）的开放多模式数据集上进行多模式数据分类提供基线，其中包括图像数据（对比增强的CT和MRI图像）和表格数据（临床实验室测试数据以及病例报告）。 TNM分期是分类任务。收集了来自矢量化预处理的表格数据和对比度增强CT和MRI图像的放射线特征。基于共同信息执行特征选择。 XGBoost分类器预测了TNM登台，它显示的预测准确性为$ 0.89 \ pm 0.05 $，AUC为$ 0.93 \ pm 0.03 $。分类器表明，只有通过组合图像和临床实验室数据才能获得高水平的预测准确性，因此是一个很好的示例案例，其中必须将多模型分类用于实现准确的结果。

### MASS: Overcoming Language Bias in Image-Text Matching 
[[arxiv](https://arxiv.org/abs/2501.11469)] [[cool](https://papers.cool/arxiv/2501.11469)] [[pdf](https://arxiv.org/pdf/2501.11469)]
> **Authors**: Jiwan Chung,Seungwon Lim,Sangkyu Lee,Youngjae Yu
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: AAAI 2025
- **标题**: 质量：克服图像文本匹配中的语言偏见
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 预处理的视觉语言模型已在多模式任务（包括图像文本检索）方面取得了重大进步。但是，图像文本匹配的主要挑战在于语言偏见，在该语言偏见中，模型主要依靠语言先验，而忽略了充分考虑视觉内容。因此，我们提出了多模式关联得分（质量），该框架降低了对语言先验的依赖，以在图像文本匹配问题中更好地视觉准确性。可以将其无缝地纳入现有的视觉语言模型中，而无需进行其他培训。我们的实验表明，质量可以有效地减少语言偏见而不会失去对语言组成的理解。总体而言，Mass提供了一种有希望的解决方案，可在视觉语言模型中增强图像文本匹配性能。

### EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery 
[[arxiv](https://arxiv.org/abs/2501.11347)] [[cool](https://papers.cool/arxiv/2501.11347)] [[pdf](https://arxiv.org/pdf/2501.11347)]
> **Authors**: Guankun Wang,Long Bai,Junyi Wang,Kun Yuan,Zhen Li,Tianxu Jiang,Xiting He,Jinlin Wu,Zhen Chen,Zhen Lei,Hongbin Liu,Jiazheng Wang,Fan Zhang,Nicolas Padoy,Nassir Navab,Hongliang Ren
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: Endochat：内窥镜手术的接地多模式大语模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，多模式的大语言模型（MLLM）证明了它们在计算机辅助诊断和决策中的巨大潜力。在机器人辅助手术的背景下，MLLM可以作为手术训练和指导的有效工具。但是，在临床应用中，仍然缺乏专门用于手术场景的MLLM。在这项工作中，我们介绍了Endochat，以解决外科医生遇到的外科手术场景中的各种对话范例和子任务。为了训练我们的内牙，我们通过一条新型管道来构建Surg-396K数据集，该数据集系统地提取手术信息并基于收集的大规模内窥镜外科手术数据集生成结构化注释。此外，我们引入了多尺度的视觉令牌交互机制和基于视觉对比度的推理机制，以增强模型的表示和推理能力。我们的模型在五个对话范式和八个手术场景中理解任务中实现了最先进的表现。此外，我们与专业外科医生进行评估，其中大多数人都在与Endochat合作提供积极的反馈。总体而言，这些结果表明，我们的内牙具有很大的潜力，可以显着提高机器人辅助手术的训练和自动化。

### Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation 
[[arxiv](https://arxiv.org/abs/2501.11309)] [[cool](https://papers.cool/arxiv/2501.11309)] [[pdf](https://arxiv.org/pdf/2501.11309)]
> **Authors**: Ziheng Zhang,Jianyang Gu,Arpita Chowdhury,Zheda Mai,David Carlyn,Tanya Berger-Wolf,Yu Su,Wei-Lun Chao
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 细摄像头：发现差异揭示了更细节的视觉说明
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 类激活图（CAM）已被广泛用于突出有助于班级预测的图像区域。尽管具有简单性和计算效率，但CAM经常努力识别区分视觉上相似细粒类别的区分区域。先前的努力通过引入更复杂的解释过程来解决这一限制，但要付出额外的复杂性。在本文中，我们提出了较细的摄像头，这种方法可以保留CAM的效率，同时实现歧视区域的精确定位。我们的关键见解是，CAM的不足不在它解释的“方式”中，而是在“什么”解释中}。具体而言，以前的方法试图识别有助于目标类的logit值的所有提示，这些线索也无意中激活了可预测视觉上相似类的区域。通过将目标类别与类似的类别进行比较，并发现其差异，更精细的摄像头抑制了与其他类共享的功能，并强调了目标类的独特，歧视性细节。 Finer-Cam易于实现，与各种CAM方法兼容，并且可以扩展到多模式模型，以准确地定位特定概念。此外，Finer-Cam允许可调节的比较强度，从而使用户能够选择性地突出显示粗略的对象轮廓或精细的判别细节。在数量上，我们表明，通过精细胶卷掩盖了激活像素的前5％会导致与基础线相比，相对置信度更高。源代码和演示可在https://github.com/imageomics/finer-cam上获得。

### MIFNet: Learning Modality-Invariant Features for Generalizable Multimodal Image Matching 
[[arxiv](https://arxiv.org/abs/2501.11299)] [[cool](https://papers.cool/arxiv/2501.11299)] [[pdf](https://arxiv.org/pdf/2501.11299)]
> **Authors**: Yepeng Liu,Zhichao Sun,Baosheng Yu,Yitian Zhao,Bo Du,Yongchao Xu,Jun Cheng
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: Mifnet：可概括的多模式图像匹配的学习模态不变特征
- **领域**: 计算机视觉和模式识别
- **摘要**: 已经提出了许多关键点检测和描述方法用于图像匹配或注册。尽管这些方法证明了单模性图像匹配的有希望的性能，但它们通常在多模式数据上挣扎，因为对单模式数据训练的描述符往往缺乏针对多模式数据中存在的非线性变化的鲁棒性。将这种方法扩展到多模式图像匹配通常需要良好的多模式数据来学习模态不变的描述符。但是，在许多实际情况下，获取此类数据通常是昂贵且不切实际的。为了应对这一挑战，我们提出了一个模态不变的特征学习网络（MIFNET），以计算仅使用单模式训练数据匹配多模式图像中关键点描述的模态不变特征。具体而言，我们提出了一个新型的潜在特征聚合模块和一个累积混合聚合模块，以通过利用稳定扩散模型中的预训练的预训练特征来增强对单个模式数据训练的基本关键点描述符。我们使用三个多模式视网膜图像数据集（CF-FA，CF-OCT，EMA-OCTA）和两个遥感数据集（Optical-SAR和Optical-NIR）中的三个多模式图像数据集（CF-FA，CF-OCT，EMA-OCTA）中验证我们的方法。广泛的实验表明，所提出的mifnet能够学习多模式图像匹配的模态不变特征，而无需访问目标模态，并且具有良好的零拍概括能力。源代码将公开可用。

### ImageRef-VL: Enabling Contextual Image Referencing in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.12418)] [[cool](https://papers.cool/arxiv/2501.12418)] [[pdf](https://arxiv.org/pdf/2501.12418)]
> **Authors**: Jingwei Yi,Junhao Yin,Ju Xu,Peng Bao,Yongliang Wang,Wei Fan,Hao Wang
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: ImagereF-VL：在视觉模型中启用上下文图像引用
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉语言模型（VLM）在理解多模式输入方面表现出了显着的功能，并已被广泛整合到基于检索的演讲（RAG）对话系统中。虽然当前的VLM驱动聊天机器人可以在其响应中提供文本源参考，但它们在对话期间参考上下文相关的图像时表现出重大限制。在本文中，我们介绍了上下文图像参考 - 能够根据对话上下文适当参考检索文档中相关图像的能力 - 并在这方面系统地研究VLMS的功能。我们对上下文图像引用进行了首次评估，包括专用的测试数据集和评估指标。此外，我们提出了ImagereF-VL，该方法可通过在大型，手动策划的多模式对话数据集中进行微调来显着增强开源VLMS的图像引用功能。实验结果表明，ImagereF-VL不仅胜过专有模型，而且在上下文图像引用任务中，对最先进的开源VLM的性能提高了88％。我们的代码可在https://github.com/bytedance/imageref-vl上找到。

### InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling 
[[arxiv](https://arxiv.org/abs/2501.12386)] [[cool](https://papers.cool/arxiv/2501.12386)] [[pdf](https://arxiv.org/pdf/2501.12386)]
> **Authors**: Yi Wang,Xinhao Li,Ziang Yan,Yinan He,Jiashuo Yu,Xiangyu Zeng,Chenting Wang,Changlian Ma,Haian Huang,Jianfei Gao,Min Dou,Kai Chen,Wenhai Wang,Yu Qiao,Yali Wang,Limin Wang
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: technical report
- **标题**: InternVideo2.5：通过长而丰富的上下文建模授权视频MLLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文旨在通过长而丰富的环境（LRC）建模来提高视频多模式大语模型（MLLM）的性能。结果，我们开发了一种新版本的InternVideo2.5，重点是增强原始MLLM的感知细节细节并捕获视频中的长期时间结构的能力。具体而言，我们的方法将密集的视力任务注释纳入了MLLM，并使用直接的偏好优化，并通过自适应层次的令牌压缩来开发紧凑的时空表示。实验结果表明，这种独特的LRC设计极大地改善了主流视频理解基准（短和长）中视频MLLM的结果，使MLLM能够记住明显更长的视频输入（至少比原始长度长6倍），以及众多的专业视觉功能，例如对象跟踪和分段。我们的工作强调了多模式上下文丰富度（长度和细度）在赋予MLLM先天性Abilites（Focus和Memory）的能力中的重要性，为视频MLLM的未来研究提供了新的见解。代码和型号可从https://github.com/opengvlab/internvideo/tree/main/main/internvideo2.5获得。

### MMVU: Measuring Expert-Level Multi-Discipline Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.12380)] [[cool](https://papers.cool/arxiv/2501.12380)] [[pdf](https://arxiv.org/pdf/2501.12380)]
> **Authors**: Yilun Zhao,Lujing Xie,Haowei Zhang,Guo Gan,Yitao Long,Zhiyuan Hu,Tongyan Hu,Weiyuan Chen,Chuhan Li,Junyang Song,Zhijian Xu,Chengye Wang,Weifeng Pan,Ziyao Shangguan,Xiangru Tang,Zhenwen Liang,Yixin Liu,Chen Zhao,Arman Cohan
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: MMVU：测量专家级的多学科视频理解
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 我们介绍了MMVU，这是一个全面的专家级，多学科基准，用于评估视频理解中的基础模型。 MMVU包括3,000个跨越四个核心学科的科学，医疗保健，人文和社会科学和工程学的专家通知的问题。与先前的基准测试相比，MMVU具有三个关键进步。首先，它挑战模型，以应用特定领域的知识并执行专家级别的推理来分析专门的域视频，超越了通常在当前视频基准中评估的基本视觉感知。其次，每个示例都由从头开始的人类专家注释。我们实施严格的数据质量控制，以确保数据集的高质量。最后，每个示例都充满了专家注册的推理理性和相关领域知识，从而促进了深入的分析。我们对MMVU上的32个Frontier多模式基础模型进行了广泛的评估。最新的具有System-2功能的模型O1和Gemini 2.0 Flash Thinking在测试模型中实现了最高的性能。但是，他们仍然无法匹配人类专业知识。通过深入的错误分析和案例研究，我们为专家级，知识密集型视频理解的未来进步提供了可行的见解。

### InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model 
[[arxiv](https://arxiv.org/abs/2501.12368)] [[cool](https://papers.cool/arxiv/2501.12368)] [[pdf](https://arxiv.org/pdf/2501.12368)]
> **Authors**: Yuhang Zang,Xiaoyi Dong,Pan Zhang,Yuhang Cao,Ziyu Liu,Shengyuan Ding,Shenxi Wu,Yubo Ma,Haodong Duan,Wenwei Zhang,Kai Chen,Dahua Lin,Jiaqi Wang
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: Tech Report
- **标题**: internlm-xcomposer2.5-奖励：一种简单而有效的多模式奖励模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 尽管大型视觉语言模型（LVLM）在视觉理解中表现出色，但它们偶尔会产生不正确的输出。尽管具有加强学习或测试时间缩放的奖励模型（RMS）为提高发电质量提供了潜力，但仍然存在一个关键的差距：公开可用的LVLM的多模式RMS稀缺，并且常见模型的实施细节通常不清楚。我们使用interlm-xcomposer2.5-reward（IXC-2.5-奖励）弥合这一差距，这是一个简单而有效的多模式奖励模型，将LVLM与人类偏好保持一致。为了确保IXC-2.5奖励的鲁棒性和多功能性，我们设置了跨不同领域的文本，图像和视频输入的高质量多模式偏好语料库，例如以下教学，一般理解，文本丰富的文档，数学推理和视频理解。 IXC-2.5-奖励在最新的多模式奖励模型基准上取得了出色的成绩，并在仅文本奖励模型基准中显示出竞争性能。我们进一步证明了IXC-2.5-奖励的三个关键应用：（1）为RL培训提供监督信号。我们将IXC-2.5-奖励与近端策略优化（PPO）整合在一起，得出IXC-2.5-Chat，该键盘在以下教学和多模式开放式对话中显示出一致的改进； （2）从候选响应中选择最佳响应以进行测试时间缩放； （3）从现有图像和视频说明调谐培训数据中过滤异常值或嘈杂的样本。为了确保可重复性并促进进一步的研究，我们在https://github.com/internlm/internlm/internlm-xcomposer上开源了所有模型权重和培训食谱

### Vision-Language Models for Automated Chest X-ray Interpretation: Leveraging ViT and GPT-2 
[[arxiv](https://arxiv.org/abs/2501.12356)] [[cool](https://papers.cool/arxiv/2501.12356)] [[pdf](https://arxiv.org/pdf/2501.12356)]
> **Authors**: Md. Rakibul Islam,Md. Zahid Hossain,Mustofa Ahmed,Most. Sharmin Sultana Samu
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: Preprint, manuscript under-review
- **标题**: 自动胸部X射线解释的视觉语言模型：利用VIT和GPT-2
- **领域**: 计算机视觉和模式识别
- **摘要**: 放射学由于其非侵入性诊断能力而在现代医学中起关键作用。但是，非结构化医疗报告的手动生成很耗时，容易出现错误。它在临床工作流程中产生了重要的瓶颈。尽管在AI生成的放射学报告中取得了进步，但在实现详细而准确的报告产生方面仍存在挑战。在这项研究中，我们评估了多模型模型的不同组合，这些模型将计算机视觉和自然语言处理整合以生成全面的放射学报告。我们采用了预验证的视觉变压器（VIT-B16）和一个SWIN变压器作为图像编码。 BART和GPT-2模型是文本解码器。我们使用了IU-XRAY数据集的胸部X射线图像和报告来评估Swin Transformer-Bart，Swin Transformer-GPT-2，VIT-B16-BART和VIT-B16-GPT-2模型的可用性，以生成报告生成。我们旨在在模型之间找到最佳组合。 SWIN-BART模型在几乎所有评估指标（例如Rouge，Bleu和Bertscore）中，在四个模型中表现出色。

### VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2501.12327)] [[cool](https://papers.cool/arxiv/2501.12327)] [[pdf](https://arxiv.org/pdf/2501.12327)]
> **Authors**: Xianwei Zhuang,Yuxin Xie,Yufan Deng,Liming Liang,Jinghan Ru,Yuguo Yin,Yuexian Zou
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: VARGPT：视觉自动回归多模式模型中的统一理解和产生
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了Vargpt，这是一种新型的多式模式模型（MLLM），该模型（MLLM）在单个自动回归框架内统一了视觉理解和产生。 Vargpt采用了下一句话的预测范式来视觉理解，以及用于视觉自回归产生的临时预测范例。 Vargpt创新地扩展了LLAVA体系结构，在MLLM中实现了有效的规模自学视觉生成，同时无缝地包含单个模型框架中的混合模式输入和输出。我们的VARGPT在特殊策划的数据集上经历了三阶段的统一训练过程，其中包括训练阶段和两个混合视觉指导阶段。统一的培训策略旨在在视觉和文本特征之间达到一致性，增强跟随理解和生成的指导，并提高视觉生成质量。尽管具有基于LLAVA的多模型理解的架构，但Vargpt在各种以视觉为中心的基准（例如视觉提问和推理任务）上的表现明显优于Llava-1.5。值得注意的是，Vargpt自然支持自回归视觉生成和指导 - 图像合成的能力，展示其在视觉理解和发电任务中的多功能性。项目页面在：\ url {https://vargpt-1.github.io/}

### InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.12231)] [[cool](https://papers.cool/arxiv/2501.12231)] [[pdf](https://arxiv.org/pdf/2501.12231)]
> **Authors**: Pha Nguyen,Sailik Sengupta,Girik Malik,Arshit Gupta,Bonan Min
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: 安装：具有多模式大语言模型的上下文感知的教学任务帮助
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 生成模型的提高能力可以帮助建立利用语言超出语言方式的多模式虚拟助手。通过观察人类执行多步骤任务，可以建立对执行行动和任务的情境意识的助手，从而使他们能够根据这种理解来满足援助。在本文中，我们使用多模式的大型语言模型（安装）开发了一个上下文感知的教学助手，该任务助理利用在线视觉流（例如用户的屏幕共享或视频录制），并实时响应与手头任务相关的用户查询。要启用有用的帮助，请安装1）在任务视频和配对的文本数据上训练多模式模型，以及2）自动从视频数据中提取任务图，并在培训和推理时间中利用它。我们显示安装在考虑多模式活动理解的所提出的子任务中实现了最新的性能 - 任务识别（TR），动作识别（AR），下一个动作预测（AP）和计划预测（PP），并且在两个与自动误差识别有关的新型子任务上的现有基准均超过了现有的基准。

### High-dimensional multimodal uncertainty estimation by manifold alignment:Application to 3D right ventricular strain computations 
[[arxiv](https://arxiv.org/abs/2501.12178)] [[cool](https://papers.cool/arxiv/2501.12178)] [[pdf](https://arxiv.org/pdf/2501.12178)]
> **Authors**: Maxime Di Folco,Gabriel Bernardino,Patrick Clarysse,Nicolas Duchateau
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: 高维多模式不确定性通过流动比对估计：应用于3D右心室应变计算
- **领域**: 计算机视觉和模式识别
- **摘要**: 对结果的信心是提高临床医生对机器学习方法采用的关键要素。文献中已经考虑了对结果的不确定性，但主要是源自学习和处理方法的结果。数据上的不确定性几乎没有挑战，因为单个样本通常被认为是分析中包括的每个主题的足够代表性。在本文中，我们提出了一种代表性学习策略，以估计先前从医学图像中通过不同定义或计算从医学图像获得的生理描述符（此处，心肌变形）的局部不确定性。我们首先使用歧管对准来匹配与不同高维输入描述符相关的潜在表示。然后，我们制定了潜在不确定性的合理分布，并最终利用它们以重建输入高维描述符上的不确定性。我们证明了它与右心室超声心动图图像序列的心肌变形（应变）的相关性，为此，其定义缺乏共识以及要使用的方向成分。我们使用了一个具有右心室过载的100个控制受试者的数据库，在右心室心内膜表面网格的每个点都可以使用不同类型的应变。我们的方法量化了定义这种生理概念的不同描述符对心肌变形的局部不确定性。这种不确定性不能由局部统计数据直接估计此类描述符，可能是异质类型的。除了这种受控的说明性应用外，我们的方法还可以将许多其他人群分析推广到考虑异质高维描述符的许多其他人群分析。

### ComposeAnyone: Controllable Layout-to-Human Generation with Decoupled Multimodal Conditions 
[[arxiv](https://arxiv.org/abs/2501.12173)] [[cool](https://papers.cool/arxiv/2501.12173)] [[pdf](https://arxiv.org/pdf/2501.12173)]
> **Authors**: Shiyue Zhang,Zheng Chong,Xi Lu,Wenqing Zhang,Haoxiang Li,Xujie Zhang,Jiehui Huang,Xiao Dong,Xiaodan Liang
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: Composeanyone：可控的布局到人类，具有脱钩的多模式条件
- **领域**: 计算机视觉和模式识别
- **摘要**: 在扩散模型的成功基础上，在多模式图像生成任务中已取得了重大进步。其中，人类形象的产生已成为一种有前途的技术，提供了彻底改变时装设计过程的潜力。但是，现有方法通常仅关注基于文本图像或图像参考的人类一代，而这些人的人类一代无法满足日益复杂的需求。为了解决人类发电中灵活性和精确度的局限性，我们引入了Composeanyone，这是一种可控制的布局至人类生成方法，具有脱钩的多模式条件。具体而言，我们的方法允许使用文本或参考图像对任何零件进行解耦控制任何部分，并在生成过程中无缝整合它们。可以轻松地绘制手绘布局，该布局利用了椭圆形和矩形等颜色块的几何形状，例如椭圆形和矩形，提供了一种更灵活，更容易访问的方法来定义空间布局。此外，我们介绍了ComposeHuman数据集，该数据集为每个人类图像的不同组成部分提供分离的文本和参考图像注释，从而在人类图像生成任务中更广泛地应用。多个数据集上的广泛实验表明，Composeanyone生成具有更好对齐方式的人类图像，以提供给定的布局，文本描述和参考图像，从而展示其多任务功能和可控性。

### A Multi-annotated and Multi-modal Dataset for Wide-angle Video Quality Assessment 
[[arxiv](https://arxiv.org/abs/2501.12082)] [[cool](https://papers.cool/arxiv/2501.12082)] [[pdf](https://arxiv.org/pdf/2501.12082)]
> **Authors**: Bo Hu,Wei Wang,Chunyi Li,Lihuo He,Leida Li,Xinbo Gao
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: 用于广角视频质量评估的多通道和多模式数据集
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 广角视频因其广阔的视角和捕捉大片风景的能力而受到青睐，这是体育和冒险录制的理想选择。但是，广角视频易于变形，暴露和其他扭曲，导致视频质量差并影响感知和经验，这可能会严重阻碍其在竞争性运动等领域的应用。到目前为止，很少有探索关注广角视频的质量评估问题。这种缺陷主要是由于缺乏用于广角视频的专门数据集。为了弥合这一差距，我们构建了第一个多通道和多模式广角视频质量评估（MWV）数据集。然后，通过数据集测试和数据集测试研究了MWV数据集上最先进的视频质量方法的性能。实验结果表明，这些方法对其适用性施加了重大限制。

### Multi-aspect Knowledge Distillation with Large Language Model 
[[arxiv](https://arxiv.org/abs/2501.13341)] [[cool](https://papers.cool/arxiv/2501.13341)] [[pdf](https://arxiv.org/pdf/2501.13341)]
> **Authors**: Taegyeong Lee,Jinsik Bang,Soyeong Kwon,Taehwan Kim
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: Preprint
- **标题**: 多种语言模型的多光值知识蒸馏
- **领域**: 计算机视觉和模式识别
- **摘要**: 深度学习的最新进展已大大提高了计算机视觉任务的性能。先前的图像分类方法主要修改模型体系结构或添加功能，并使用类逻辑上的跨凝结损失优化模型。由于他们专注于将图像与考虑类标签进行分类，因此这些方法可能难以学习类的各种\ emph {expects}（例如，自然位置和形状变化）。从一种新颖的观点重新考虑了先前的方法，我们建议使用多模式大语言模型（MLLM）提出一种多光谱知识蒸馏方法。我们的方法涉及：1）与我们要转移到模型的知识相关的多光值问题查询大语言模型，2）从MLLM中提取相应的logits； 3）扩展模型的输出维度，以蒸馏这些多主体逻辑。然后，我们将跨凝结损失应用于类ligits和二进制横向渗透损失对多方面的逻辑。通过我们的方法，该模型不仅可以学习有关视觉方面的知识，还可以学习需要更深入理解的抽象和复杂方面。我们主要将我们的方法应用于图像分类，并探索扩展模型的潜力，将其扩展到其他任务，例如对象检测。在所有实验结果中，我们的方法都提高了基准的性能。此外，我们分析了多光值知识蒸馏的效果。这些结果表明，我们的方法可以将有关各个方面的知识转移到模型，而知识的方面可以增强计算机视觉任务中的模型性能。本文展示了多个知识蒸馏的巨大潜力，我们认为它为未来在计算机视觉及其他方面进行研究提供了有希望的方向。

### Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on Retrieval-Augmented Diffusion Models 
[[arxiv](https://arxiv.org/abs/2501.13340)] [[cool](https://papers.cool/arxiv/2501.13340)] [[pdf](https://arxiv.org/pdf/2501.13340)]
> **Authors**: Hao Fang,Xiaohang Sui,Hongyao Yu,Kuofeng Gao,Jiawei Kong,Sijin Yu,Bin Chen,Hao Wu,Shu-Tao Xia
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: No comments
- **标题**: 检索可能是有害的：对检索式扩散模型的对比后门攻击范式
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散模型（DMS）最近表现出显着的产生能力。但是，他们的培训通常需要大量的计算资源和大规模数据集。为了解决这些最近的研究，使用先进的检索效果（RAG）技术增强了DMS的能力，并提出了检索增强的扩散模型（RDMS）。通过从辅助数据库中纳入丰富的知识，RAG增强了扩散模型的产生和泛化能力，同时显着降低了模型参数。尽管取得了巨大的成功，但RAG可能会引入新的安全问题，需要进一步调查。在本文中，我们揭示了RDM通过提出一种名为BadRDM的多模式对比攻击方法来容易受到后门攻击。我们的框架充分考虑了RAG的特征，并设计用于操纵给定文本触发器检索的项目，从而进一步控制生成的内容。具体而言，我们首先将一小部分图像插入检索数据库中，作为目标毒性代理。随后，采用了对比度学习的恶意变异，以将后部注入猎犬，从而建立了从触发器到毒性替代物的快捷方式。此外，我们通过基于新颖的熵选择和生成增强策略来增强攻击，从而获得更好的毒性替代物。对两个主流任务进行的广泛实验表明，在保留模型的良性实用程序的同时，提出的BADRDM实现了出色的攻击效果。

### MEDFORM: A Foundation Model for Contrastive Learning of CT Imaging and Clinical Numeric Data in Multi-Cancer Analysis 
[[arxiv](https://arxiv.org/abs/2501.13277)] [[cool](https://papers.cool/arxiv/2501.13277)] [[pdf](https://arxiv.org/pdf/2501.13277)]
> **Authors**: Daeun Jung,Jaehyeok Jang,Sooyoung Jang,Yu Rang Park
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: 8 pages, 1 figure
- **标题**: Medform：多癌分析中CT成像和临床数字数据的对比度学习的基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 计算机断层扫描（CT）和临床数字数据是癌症评估的重要方式，但是由于多层CT数据的结构复杂性和高度专家注释成本的结构复杂性，建立用于开发医学基础模型的大规模多模式训练数据集仍然具有挑战性。在这项研究中，我们提出了MedForm，这是一种多式联运预训练策略，该策略使用临床数据中的互补信息来指导CT图像表示学习，以进行医学基础模型开发。 MEDFORM通过多个实例学习（MIL）有效地处理CT切片，并采用双重预训练策略：首先使用基于SIMCLR的自我监督学习的CT切片提取器进行预处理，然后通过交叉模式对比学习来对齐CT和临床方式。我们的模型已在三种不同的癌症类型上进行了预培训：肺癌（141,171片），乳腺癌（8,100片），结直肠癌（10,393片）。实验结果表明，这种双重训练策略可改善癌症分类的性能，并在几次学习方案中保持稳健的表现。可在https://github.com/digitalhealthcarelab/25multimodalfoundation.git上获得代码

### VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.13106)] [[cool](https://papers.cool/arxiv/2501.13106)] [[pdf](https://arxiv.org/pdf/2501.13106)]
> **Authors**: Boqiang Zhang,Kehan Li,Zesen Cheng,Zhiqiang Hu,Yuqian Yuan,Guanzheng Chen,Sicong Leng,Yuming Jiang,Hang Zhang,Xin Li,Peng Jin,Wenqi Zhang,Fan Wang,Lidong Bing,Deli Zhao
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3
- **标题**: Videollama 3：用于图像和视频理解的边界多模式基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们提出了Videollama3，这是一个更高级的多模式基础模型，用于图像和视频理解。 Videollama3的核心设计理念以视觉为中心。 “以视觉为中心”的含义是两倍：以视觉为中心的训练范式和以视觉为中心的框架设计。我们以视觉为中心的训练范式的关键见解是，高质量的图像培训数据对于图像和视频理解至关重要。我们专注于构建大规模和高质量的图像文本数据集，而不是准备大量的视频文本数据集。 Videollama3有四个训练阶段：1）视觉编码器改编，这使视觉编码器能够接受可变分辨率的图像作为输入； 2）视觉语言对齐，它可以通过大规模的图像文本数据涵盖多种类型（包括场景图像，文档，图表）以及仅文本数据，从而共同调整了视觉编码器，投影仪和LLM。 3）多任务微型调整，其中包含用于下游任务和视频文本数据的图像text SFT数据，以建立视频理解的基础。 4）以视频为中心的微调，这进一步提高了模型在视频理解中的能力。至于框架设计，为了更好地捕获图像中的细节细节，验证的视觉编码器适用于用相应数字而不是固定数量的令牌编码各种大小的图像中的图像。对于视频输入，我们根据其相似性减少视力令牌的数量，以使视频的表示将更加精确和紧凑。以视觉为中心的设计受益，Videollama3在图像和视频理解基准中都取得了令人信服的表演。

### AMM-Diff: Adaptive Multi-Modality Diffusion Network for Missing Modality Imputation 
[[arxiv](https://arxiv.org/abs/2501.12840)] [[cool](https://papers.cool/arxiv/2501.12840)] [[pdf](https://arxiv.org/pdf/2501.12840)]
> **Authors**: Aghiles Kebaili,Jérôme Lapuyade-Lahorgue,Pierre Vera,Su Ruan
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: No comments
- **标题**: AMM-DIFF：自适应多模式扩散网络缺失模态插补
- **领域**: 计算机视觉和模式识别
- **摘要**: 在临床实践中，完全是由于复杂的获取方案，严格的隐私法规或特定的临床需求，全部成像并不总是可行的。但是，缺少MR模式对诸如脑肿瘤细分之类的任务构成了重大挑战，尤其是在基于深度学习的细分中，因为每种模式为提高准确性提供了至关重要的互补信息。一个有希望的解决方案是缺少数据插补，其中缺乏可用的方法。尽管生成模型已被广泛用于此目的，但大多数最先进的方法仅限于单个或双重目标翻译，因此缺乏基于不同输入配置生成缺失模态的适应性。为了解决这个问题，我们提出了一个自适应多模式扩散网络（AMM-DIFF），这是一种基于新颖的基于扩散的生成模型，能够处理任何数量的输入方式并生成缺失的输入模式。我们设计了一个图像频率融合网络（IFFN），该网络通过在完整的输入方式及其选定的高频傅立叶组件中的自我监督借口任务来学习统一的特征表示。提出的扩散模型利用此表示，封装了完整模式的先验知识，并将其与自适应重建策略结合在一起，以实现缺失的模态完成。 Brats 2021数据集的实验结果证明了我们方法的有效性。

### Modality Unified Attack for Omni-Modality Person Re-Identification 
[[arxiv](https://arxiv.org/abs/2501.12761)] [[cool](https://papers.cool/arxiv/2501.12761)] [[pdf](https://arxiv.org/pdf/2501.12761)]
> **Authors**: Yuan Bian,Min Liu,Yunqi Yi,Xueping Wang,Yunfeng Ma,Yaonan Wang
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: 9 pages,3 figures
- **标题**: Omni-Moditaly人的模态统一攻击重新识别
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 基于深度学习的人重新识别（RE-ID）模型已广泛用于监视系统中。最近的研究表明，黑盒的单模式和跨模式重新ID模型容易受到对抗性示例（AES）的影响，从而使多模式重新ID模型的稳健性未经探索。由于缺乏对目标黑框监视系统中部署的特定模型类型的知识，因此我们旨在生成模态统一AES OMNI模式（单，交叉和多模式）RE-ID模型。具体而言，我们提出了一种新型的模态统一攻击方法来训练模式特异性的对抗发电机，以生成有效攻击不同OMNI模型模型的AE。多模式模型被用作替代模型，其中每种模态的特征在融合前受到指标破坏损失的干扰。为了折叠Omni模式模型的共同特征，引入了跨模态模拟的破坏方法，以模仿跨模式特征嵌入，通过将图像有意地馈送到替代模型的非相应模态特异性子网络中。此外，通过利用多形式功能协作指标损失损失来促进攻击者的多种方式协作策略，以促进攻击者对人图像的信息内容进行全面损害。广泛的实验表明，我们的MUA方法可以有效地攻击Omni模式重新ID模型，分别达到55.9％，24.4％，49.0％和62.7％的平均地图下降率。

### Dynamic Token Reduction during Generation for Vision Language Models 
[[arxiv](https://arxiv.org/abs/2501.14204)] [[cool](https://papers.cool/arxiv/2501.14204)] [[pdf](https://arxiv.org/pdf/2501.14204)]
> **Authors**: Xiaoyu Liang,Chaofeng Guan,Jiaying Lu,Huiyao Chen,Huan Wang,Haoji Hu
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 视觉语言模型的生成过程中的动态令牌减少
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉模型（VLM）在多模式任务中取得了显着的成功，但由于解码器注意机制的二次复杂性和自回归产生的二次复杂性而面临实际局限性。诸如FASTV和VTW之类的现有方法在减少冗余视觉令牌方面取得了显着的结果，但是这些方法集中在单个前向通行中，而无需系统地分析整个世代的视觉令牌的冗余。在本文中，我们介绍了针对VLMS（Dyrate）量身定制的动态修剪策略（Dyrate），该策略逐渐调整了生成期间的压缩率。我们对注意力分布的分析表明，在整个生成过程中，视觉令牌的重要性降低，激发了我们采用更具侵略性的压缩率。通过基于注意力分布的重量预测变量，我们的方法可以根据注意力分布的灵活调整修剪率。我们的实验结果表明，我们的方法不仅减少了计算需求，而且还保持了响应质量。

### Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation 
[[arxiv](https://arxiv.org/abs/2501.14166)] [[cool](https://papers.cool/arxiv/2501.14166)] [[pdf](https://arxiv.org/pdf/2501.14166)]
> **Authors**: Cong-Duy Nguyen,Xiaobao Wu,Thong Nguyen,Shuai Zhao,Khoi Le,Viet-Anh Nguyen,Feng Yichao,Anh Tuan Luu
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 增强与基于JACCARD距离的条件对比学习和上下文视觉增强的多模式实体
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 先前关于多模式实体联系（MEL）的研究主要采用对比度学习为主要目标。但是，将其余的批次用作负面样本而无需仔细考虑，这些研究可能会利用简单的功能，并可能忽略使实体独特的基本细节。在这项工作中，我们提出了JD-CCL（基于Jaccard距离的条件对比学习），这是一种新型方法，旨在增强匹配多模式实体链接模型的能力。 JD-CCL利用元信息选择具有相似属性的负样本，从而使链接任务更具挑战性和鲁棒性。此外，为了解决由提及和实体之间视觉模态内变化引起的局限性，我们引入了一种新颖的方法CVACPT（上下文视觉AID可控制的补丁变换）。它通过将多视图合成图像和上下文文本表示形式结合到缩放和移动补丁表示来增强视觉表示。基准MEL数据集的实验结果证明了我们方法的有效性。

### StreamingRAG: Real-time Contextual Retrieval and Generation Framework 
[[arxiv](https://arxiv.org/abs/2501.14101)] [[cool](https://papers.cool/arxiv/2501.14101)] [[pdf](https://arxiv.org/pdf/2501.14101)]
> **Authors**: Murugan Sankaradas,Ravi K. Rajendran,Srimat T. Chakradhar
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: Accepted and Presented at AI4Sys, HPDC 2024
- **标题**: StreamingRag：实时上下文检索和生成框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 从医疗保健，智能运输和卫星遥感等各个领域的多模式数据流中提取实时见解仍然是一个挑战。高计算需求和有限的知识范围限制了这些数据流中多模式大型语言模型（MM-LLM）的适用性。传统的检索生成一代（RAG）系统解决了这些模型的知识局限性，但预处理缓慢，使其不适合实时分析。我们提出了StreamingRag，这是一种新颖的抹布框架，旨在流媒体数据。 Streamrag构造不断发展的知识图实时捕获场景 - 对象 - 实体关系。知识图使用MM-LLM实现了时间感知的场景表示，并可以及时响应特定事件或用户查询。 StreamingRag解决了现有方法中的局限性，在实时分析（快5-6倍），上下文准确性（通过时间知识图）和资源消耗降低（使用轻量级模型使用2-3x）方面实现了重大改进。

### Revisiting CLIP: Efficient Alignment of 3D MRI and Tabular Data using Domain-Specific Foundation Models 
[[arxiv](https://arxiv.org/abs/2501.14051)] [[cool](https://papers.cool/arxiv/2501.14051)] [[pdf](https://arxiv.org/pdf/2501.14051)]
> **Authors**: Jakob Krogh Petersen,Valdemar Licht,Mads Nielsen,Asbjørn Munk
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: 10 pages, 2 figures. To be published in ISBI 2025
- **标题**: 重新审视剪辑：使用域特异性基础模型的3D MRI和表格数据有效排列
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 多模式模型需要对齐的共享嵌入空间。但是，基于夹的常见方法需要大量样本，并且不本地支持3D或表格数据，这两者在医疗领域至关重要。为了解决这些问题，我们通过训练特定于域的3D基础模型作为图像编码器来重新访问夹式的对准，并证明仅使用62次MRI扫描可行，模态对齐是可行的。在3D中训练所需的简单嵌入积累策略可以实现我们的方法，该培训范围跨越批量的负面对以稳定训练。我们对各种设计选择进行了彻底的评估，包括骨干和损失功能的选择，并评估有关零射击分类和图像回归任务的拟议方法。虽然零射击图像 - 取回仍然具有挑战性，但零击分类结果表明，所提出的方法可以有意义地将3D MRI的表示与表格数据对齐。

### DEFEND: A Large-scale 1M Dataset and Foundation Model for Tobacco Addiction Prevention 
[[arxiv](https://arxiv.org/abs/2501.13950)] [[cool](https://papers.cool/arxiv/2501.13950)] [[pdf](https://arxiv.org/pdf/2501.13950)]
> **Authors**: Naga VS Raviteja Chappa,Matthew Shepard,Connor McCurtain,Charlotte McCormick,Page Daniel Dobbs,Khoa Luu
> **First submission**: 2025-01-19
> **First announcement**: 2025-01-24
> **comment**: 11 pages, 5 figures, 5 tables
- **标题**: 防御：大规模的1M数据集和预防烟草成瘾的基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 虽然烟草广告以空前的速度创新，但传统的监视方法仍及时冻结，尤其是在社交媒体的背景下。缺乏大规模，全面的数据集和复杂的监控系统，从而在行业进步和公共卫生监督之间存在着扩大的差距。本文通过引入烟草1M来解决这一关键挑战，这是一个综合数据集，其中包括具有75个产品类别的层次标签的一百万个烟草产品图像，并为烟草产品理解的新颖基础模型Deffess提供了辩护。我们的方法集成了一个用于丰富多模式表示学习的功能增强模块，用于详细特征歧视的局部全球视觉连贯机制以及用于精确产品表征的增强图像培训策略。实验结果表明，防御能力的出色表现，在产品分类方面达到了83.1％的精度，在视觉提问任务中达到了73.8％，从而超过了大量利润率的现有方法。此外，该模型在新型产品类别上具有45.6％的精度，具有强劲的零拍学习能力。这项工作为监管机构和公共卫生研究人员提供了强大的工具，可监视新兴的烟草产品和营销策略，可能彻底改变烟草控制和公共卫生监视的方法。

### GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing 
[[arxiv](https://arxiv.org/abs/2501.13925)] [[cool](https://papers.cool/arxiv/2501.13925)] [[pdf](https://arxiv.org/pdf/2501.13925)]
> **Authors**: Akashah Shabbir,Mohammed Zumri,Mohammed Bennamoun,Fahad S. Khan,Salman Khan
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 地理素：遥感中的像素接地大型多模式模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）的最新进展已将细粒度的基础视为视觉理解和对话的命令性因素。但是，在LMM中，这种表示的好处仅限于自然图像域，并且这些模型在遥感（RS）方面的表现较差。高分辨率RS图像中的独特的高架观点，比例变化以及小物体的存在提出了区域级别理解的独特挑战。此外，由于缺乏颗粒状的RS域特异性接地数据，LMM在RS中的接地对话能力的发展受到阻碍。在解决这些局限性时，我们提出了地理素 - 第一个支持像素级接地的端到端高分辨率RMM。这种功能可以通过在对话中产生交织的面具来允许细粒度的视觉感知。 Geopixel以任何纵横比最多支持4K HD分辨率，非常适合高精度RS图像分析。为了支持RS图像中的扎根对话生成（GCG），我们通过半自动化的管道来策划一个视觉接地的数据集地理固定，该管道利用了促使RS数据量身定制的标记和空间先验，以有条不紊地控制数据生成数据。 Geopixel在像素级理解中表现出卓越的性能，超过了单目标和多目标分段任务中的现有LMM。我们的方法学消融研究验证了每个组件在整体体系结构中的有效性。我们的代码和数据将公开发布。

### Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization 
[[arxiv](https://arxiv.org/abs/2501.13924)] [[cool](https://papers.cool/arxiv/2501.13924)] [[pdf](https://arxiv.org/pdf/2501.13924)]
> **Authors**: Hao Dong,Eleni Chatzi,Olga Fink
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: Accepted by ICLR 2025
- **标题**: 通过自适应熵 - 感知优化朝着强大的多模式开放式测试时间适应
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 测试时间适应（TTA）在解决训练和测试数据之间的分布变化方面表现出了巨大的潜力。开放式测试时间适应（OSTTA）旨在在线适应源预先训练的模型，以适应包含未知类别的未标记的目标域。当涉及多种方式时，此任务变得更具挑战性。现有方法主要集中在单峰ostta上，通常在不解决多模式数据的复杂性的情况下滤除低信心样本。在这项工作中，我们提出了自适应熵感知优化（AEO），这是一个专门设计用于首次解决多模式开放式测试时间适应（MM-OSTTA）的新型框架。我们的分析表明，目标域中已知样品和未知样品之间的熵差与MM-OSTTA性能密切相关。为了利用这一点，我们提出了两个关键组成部分：未知的自适应熵优化（UAE）和自适应模态预测差异优化（AMP）。这些组件通过扩增已知样本和未知样本之间的熵差来增强模型在在线适应过程中区分未知类样品的能力。为了在MM-Ostta设置中彻底评估我们所提出的方法，我们建立了一个从现有数据集中得出的新基准。该基准包括两个下游任务，并结合了五个模式。各种域转移情况的广泛实验证明了AEO框架的功效和多功能性。此外，我们强调了AEO在长期和持续的MM-OSTTA环境中的强劲表现，这两者都具有挑战性，并且与现实世界应用高度相关。我们的源代码可在https://github.com/donghao51/aeo上找到。

### Temporal Preference Optimization for Long-Form Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.13919)] [[cool](https://papers.cool/arxiv/2501.13919)] [[pdf](https://arxiv.org/pdf/2501.13919)]
> **Authors**: Rui Li,Xiaohan Wang,Yuhui Zhang,Zeyu Wang,Serena Yeung-Levy
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 时间偏好优化，以了解长型视频理解
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,机器人技术
- **摘要**: 尽管视频大型多模型模型（视频LMM）取得了重大进步，但在长期视频中实现有效的时间基础仍然是现有模型的挑战。为了解决这一限制，我们提出了时间偏好优化（TPO），这是一个新型的训练后框架，旨在通过偏好学习来增强视频LMM的时间接地能力。 TPO采用了一种自我训练的方法，该方法可以通过利用两个粒度的策划偏好数据集（局部时间基础）来区分良好的时间和较少准确的时间响应，该数据集介绍了特定的视频片段和全面的时间基础，从而捕捉了整个视频序列的扩展时间依赖性。通过优化这些偏好数据集，TPO可以显着增强时间理解，同时减少对手动注释数据的依赖。在三个长形式视频理解基准测试的基准（LongvideObench，MLVU和视频MME）上进行了广泛的实验，可以证明TPO在两个最先进的视频LMM中的有效性。值得注意的是，LLAVA-VIDEO-TPO将自己确立为视频MME基准测试的领先7B模型，强调了TPO作为在长期视频理解中推进时间推理的可扩展和高效解决方案的潜力。项目页面：https：//ruili33.github.io/tpo_website。

### Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning 
[[arxiv](https://arxiv.org/abs/2501.13893)] [[cool](https://papers.cool/arxiv/2501.13893)] [[pdf](https://arxiv.org/pdf/2501.13893)]
> **Authors**: Zuyao You,Junke Wang,Lingyu Kong,Bo He,Zuxuan Wu
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: PIX2CAP-COCO：通过像素级字幕推进视觉理解
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 我们提出PIX2CAP-COCO，这是第一个泛型像素级标题数据集，旨在提高细粒度的视觉理解。为了实现这一目标，我们仔细设计了一个自动注释管道，该管道提示GPT-4V为图像中的单个对象生成与像素的实例特定字幕，使模型能够学习对象及其上下文之间的更多粒度关系。这种方法产生167,254个详细的字幕，平均每个字幕22.94个字。在Pix2Cap-Coco的基础上，我们介绍了一项新颖的任务，Pastic Sementation Captioning，该任务挑战模型以识别图像中的实例并同时为每个图像提供详细的描述。为了基准这项任务，我们设计了一个基于X-Decoder的强大基线。实验结果表明，Pix2Cap-Coco是一个特别具有挑战性的数据集，因为它要求模型在细粒度的视觉理解和详细的语言生成中都表现出色。此外，我们利用Pix2Cap-Coco在大型多模型（LMMS）上进行监督微调（SFT）来提高其性能。 For example, training with Pix2Cap-COCO significantly improves the performance of GPT4RoI, yielding gains in CIDEr +1.4%, ROUGE +0.4%, and SPICE +0.5% on Visual Genome dataset, and strengthens its region understanding ability on the ViP-BENCH, with an overall improvement of +5.1%, including notable increases in recognition accuracy +11.2% and language generation quality +22.2%.

### MV-GMN: State Space Model for Multi-View Action Recognition 
[[arxiv](https://arxiv.org/abs/2501.13829)] [[cool](https://papers.cool/arxiv/2501.13829)] [[pdf](https://arxiv.org/pdf/2501.13829)]
> **Authors**: Yuhui Lin,Jiaxuan Lu,Yue Yong,Jiahao Zhang
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: MV-GMN：多视图动作识别的状态空间模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多视图动作识别的最新进展主要取决于基于变压器的模型。这些模型虽然有效且适应能力，但通常需要大量的计算资源，尤其是在具有多个视图和多个时间序列的情况下。在解决此限制时，本文介绍了MV-GMN模型，MV-GMN模型是一种专门设计的，该模型是专门针对有效汇总多模式数据（RGB和骨骼），多视图观点和多个暂时性信息的动作识别的，用于降低计算复杂性。 MV-GMN模型采用创新的多视图图MAMBA网络，其中包括一系列MV-GMN块。每个块包括建议的双向状态空间块和一个GCN模块。双向状态空间块引入了四种扫描策略，包括视图优先和时间优先的方法。 GCN模块利用基于规则的方法和基于KNN的方法来构建图形网络，从不同的角度和时间实例中有效地集成了特征。 MV-GMN证明了其功效，在几个数据集上的最先进，在NTU RGB+D 120数据集中分别以交叉对象和交叉观察现场的ntu RGB+D 120数据集上的97.3 \％和96.7 \％的明显准确性。 MV-GMN还超过了基于变压器的基线，同时仅需要线性推理复杂性，强调了模型减少计算负载并增强多视图动作识别技术的可扩展性和适用性的能力。

### Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos 
[[arxiv](https://arxiv.org/abs/2501.13826)] [[cool](https://papers.cool/arxiv/2501.13826)] [[pdf](https://arxiv.org/pdf/2501.13826)]
> **Authors**: Kairui Hu,Penghao Wu,Fanyi Pu,Wang Xiao,Yuanhan Zhang,Xiang Yue,Bo Li,Ziwei Liu
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 视频 - 毫米：从多学科专业视频中评估知识获取
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 人类通过三个认知阶段获得知识：感知信息，理解知识并调整知识以解决新的问题。视频是该学习过程的有效媒介，从而促进了通过这些认知阶段的发展。但是，现有的视频基准无法系统地评估大型多模型（LMMS）中的知识获取功能。为了解决这一差距，我们介绍了Video-MMMU，这是一种多模式，多学科的基准测试，旨在评估LMMS从视频中获取和利用知识的能力。 Video-MMMU包含了300个专家级视频和900个人类宣传的问题的策划集合，这些问题在六个学科中进行了策划，通过阶段一致的问题解答对评估知识获取：感知，理解和适应。拟议的知识获得度量，δ知识，可以量化视频观看后的性能改善。对LMM的评估表明，随着认知需求的增加，性能急剧下降，并突出了人类和模型知识获取之间的显着差距，强调了对增强LMMS学习和适应视频的方法的需求。

### EventVL: Understand Event Streams via Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2501.13707)] [[cool](https://papers.cool/arxiv/2501.13707)] [[pdf](https://arxiv.org/pdf/2501.13707)]
> **Authors**: Pengteng Li,Yunfan Lu,Pinghao Song,Wuyang Li,Huizai Yao,Hui Xiong
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: EventVl：通过多模式大语言模型了解事件流
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 基于事件的视觉模型（VLM）最近在实践视觉任务方面取得了良好的进步。但是，这些作品中的大多数只是利用剪辑来专注于传统的感知任务，这些任务妨碍了模型理解事件流的足够语义和上下文。为了解决该缺陷，我们提出了EventVl，这是第一个基于生成事件的MLLM（多模式大语言模型）框架，以实现明确的语义理解。具体而言，为了弥合与不同模式语义连接的数据差距，我们首先注释一个大型事件图像/视频图形数据集，其中包含近140万个高质量的数据，这可以在各种场景中有效学习，例如驱动器场景或人类运动。之后，我们设计事件时空表示形式，以通过多样化和分割事件流来充分探索综合信息。为了进一步促进紧凑的语义空间，引入了动态语义对齐，以改善事件的稀疏语义空间。广泛的实验表明，在事件字幕和场景描述生成任务中，我们的EventVL可以显着超过现有的MLLM基线。我们希望我们的研究能够为事件视觉社区的发展做出贡献。

### MPG-SAM 2: Adapting SAM 2 with Mask Priors and Global Context for Referring Video Object Segmentation 
[[arxiv](https://arxiv.org/abs/2501.13667)] [[cool](https://papers.cool/arxiv/2501.13667)] [[pdf](https://arxiv.org/pdf/2501.13667)]
> **Authors**: Fu Rong,Meng Lan,Qian Zhang,Lefei Zhang
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: MPG-SAM 2：将SAM 2与面具先验和全局上下文调整，用于引用视频对象分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 引用视频对象细分（RVO）的目的是根据文本描述在视频中分割对象，该描述需要集成多模式信息和时间动力学感知。任何模型2（SAM 2）的细分市场在各种视频分割任务中显示出很大的有效性。但是，将文本转换为有效的提示和缺乏全球环境意识的挑战，其在离线RVO上的应用受到了挑战。在本文中，我们提出了一个新颖的RVOS框架，称为MPG-SAM 2，以应对这些挑战。具体而言，MPG-SAM 2采用统一的多模式编码器来共同编码视频和文本功能，生成语义对齐的视频和文本嵌入以及多模式类代币。掩码先验生成器利用视频嵌入和类令牌来创建目标对象和全局上下文的伪面罩。这些蒙版作为密集的提示以及多模式类代币作为稀疏提示，以生成SAM 2的准确提示。为在线SAM 2提供全球视图，我们介绍了层级全球 - 历史汇总聚合器，SAM 2允许SAM 2允许在Pixel和对象级别的目标对象构成目标对象，并介绍目标对象的全球和历史信息。在几个RVOS基准上进行的广泛实验证明了MPG-SAM 2的优势以及我们提出的模块的有效性。

### Cognitive Paradigms for Evaluating VLMs on Visual Reasoning Task 
[[arxiv](https://arxiv.org/abs/2501.13620)] [[cool](https://papers.cool/arxiv/2501.13620)] [[pdf](https://arxiv.org/pdf/2501.13620)]
> **Authors**: Mohit Vaishnav,Tanel Tammet
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 评估视觉推理任务中VLM的认知范例
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 推进机器的视觉推理需要更深入地了解视觉模型（VLM）过程并解释复杂的视觉模式。这项工作介绍了一个新颖的，认知启发的评估框架，以系统地分析基于自然图像的邦加德问题的VLM推理。我们提出了三个结构化范式 - 直接的视觉规则学习，演绎规则学习和构成分析 - 旨在逐步执行逐步推理并解散感知和推理之间的相互作用。我们的评估表明，先进的，封闭的VLM（GPT-4O和Gemini 2.0）实现了近乎超人的性能，尤其是在提供高质量的图像描述时，而开源模型由于感知的不足而表现出明显的性能瓶颈。一项消融研究进一步证实，感知而不是推理是主要的限制因素，因为开源模型在给出准确的描述时有效地应用了提取的规则。这些发现强调了强大的多模式感知在增强可概括的视觉推理方面的关键作用，并强调了结构化的，逐步推理范式在推进机器智能方面的重要性。

### ReasVQA: Advancing VideoQA with Imperfect Reasoning Process 
[[arxiv](https://arxiv.org/abs/2501.13536)] [[cool](https://papers.cool/arxiv/2501.13536)] [[pdf](https://arxiv.org/pdf/2501.13536)]
> **Authors**: Jianxin Liang,Xiaojun Meng,Huishuai Zhang,Yueqian Wang,Jiansheng Wei,Dongyan Zhao
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: Accepted to main conference at NAACL 2025; 8 pages;
- **标题**: REASVQA：通过不完美的推理过程推进VideoQA
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视频问题回答（VideoQA）是一项具有挑战性的任务，需要了解视频中复杂的视觉和时间关系才能准确回答问题。在这项工作中，我们介绍了\ textbf {reAsvqa}（推理增强视频询问答案），这是一种新的方法，利用多模式大语言模型（MLLMS）生成的推理过程来提高视频QA模型的性能。我们的方法包括三个阶段：推理产生，推理的改进和从推理中学习。首先，我们使用其他MLLM生成详细的推理过程，然后通过过滤步骤将它们改进，以确保数据质量。最后，我们使用可能是不完美形式的推理数据，通过多任务学习来指导视频QA模型，以根据给定的视频来解释和回答问题。我们在三个流行的基准上评估了REASVQA，我们的结果建立了新的最新性能，在Next-QA上的显着改善为+2.9，在Star上+7.3，IntentQA上的+5.9。我们的发现证明了将推理过程集成到VideoQA中的监督好处。进一步的研究验证了我们方法的每个组件，也可以使用不同的骨干和MLLM，并再次强调了这种简单但有效方法的优势。我们通过利用先进的推理技术，在该研究领域设定新的基准，为增强VideoQA性能增强性能提供了新的观点。

### Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge 
[[arxiv](https://arxiv.org/abs/2501.13468)] [[cool](https://papers.cool/arxiv/2501.13468)] [[pdf](https://arxiv.org/pdf/2501.13468)]
> **Authors**: Haomiao Xiong,Zongxin Yang,Jiazuo Yu,Yunzhi Zhuge,Lu Zhang,Jiawen Zhu,Huchuan Lu
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: Accepted to ICLR 2025. Code is available at https://github.com/hmxiong/StreamChat
- **标题**: 流式视频理解和与内存增强知识的多轮互动
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型语言模型（LLM）的最新进展使视频插件的开发能够开发，从而通过将视频数据与语言任务桥接在一起，从而推进了多模式学习。但是，当前的视频理解模型与处理长期视频序列，支持多转话对话并适应现实世界动态方案。为了解决这些问题，我们提出了StreamChat，这是一个用于流视频推理和对话互动的无培训框架。 $ \ streamchat $利用新颖的层次内存系统有效地处理和压缩视频功能，超过扩展序列，从而实现实时，多转向对话。我们的框架结合了一个并行系统调度策略，该策略可提高处理速度并降低延迟，从而确保在现实世界应用中的稳健性能。此外，我们推出了Streambench，这是一种多功能基准测试，该基准评估了各种媒体类型和交互式场景的流视频理解，包括多转交互和复杂的推理任务。对Streambench和其他公共基准的广泛评估表明，在准确性和响应时间方面，Streamchat显着超过了现有的最新模型，证实了其对流视频理解的有效性。代码可在StreamChat：https：//github.com/hmxiong/streamchat中找到。

### EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion 
[[arxiv](https://arxiv.org/abs/2501.13452)] [[cool](https://papers.cool/arxiv/2501.13452)] [[pdf](https://arxiv.org/pdf/2501.13452)]
> **Authors**: Jiangchuan Wei,Shiyue Yan,Wenfeng Lin,Boyuan Liu,Renjie Chen,Mingyu Guo
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: Echovideo：通过多模式特征融合提供身份的人类视频发电
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频发电的最新进展极大地影响了各种下游应用程序，特别是在具有身份的视频生成（IPT2V）中。但是，现有的方法与“复制式”工件和低相似性问题遇到了困难，这主要是由于它们依赖低级面部图像信息。这种依赖性可能导致僵硬的面部外观和伪像，反映出无关紧要的细节。为了应对这些挑战，我们提出了Echovideo，它采用了两种关键策略：（1）身份图像文本融合模块（IITF），该模块（IITF）整合了文本中的高级语义特征，从文本中捕获清洁的面部身份表示，同时丢弃遮挡，姿势和照明差异，以避免摄取Artifacts的引入； （2）一种两阶段的训练策略，在第二阶段中纳入了随机方法，以随机利用浅面信息。目的是平衡浅层特征提供的忠诚度的增强，同时减轻对它们的过度依赖。该策略鼓励模型在训练过程中利用高级功能，最终促进面部身份的更强大的表现。 Echovideo有效地保留了面部身份并保持全身完整性。广泛的实验表明，它在产生高质量，可控性和保真度视频方面取得了出色的成果。

### A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks 
[[arxiv](https://arxiv.org/abs/2501.15724)] [[cool](https://papers.cool/arxiv/2501.15724)] [[pdf](https://arxiv.org/pdf/2501.15724)]
> **Authors**: Dong Li,Guihong Wan,Xintao Wu,Xinyu Wu,Ajit J. Nirmal,Christine G. Lian,Peter K. Sorger,Yevgeniy R. Semenov,Chen Zhao
> **First submission**: 2025-01-26
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 计算病理基础模型的调查：数据集，适应策略和评估任务
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 计算病理基础模型（CPATHFM）已成为分析组织病理学数据的强大方法，利用自我监督的学习来从未标记的全片图像中提取强大的特征表示。这些模型归类为单模式和多模式框架，在自动化复杂的病理任务（例如分割，分类和生物标志物发现）方面已经证明了有望。但是，CPATHFM的开发提出了重大挑战，例如有限的数据可访问性，跨数据集的高可变性，特定于领域的适应性的必要性以及缺乏标准化的评估基准。这项调查对计算病理学中的CPathFM进行了全面综述，重点是数据集，适应策略和评估任务。我们分析了关键技术，例如对比度学习和多模式集成，并突出了当前研究中的现有差距。最后，我们从四个角度探索未来的方向来推进CPathFM。这项调查是研究人员，临床医生和AI从业人员的宝贵资源，指导CPATHFMS促进稳健和临床适用的AI驱动病理解决方案。

### GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2501.15619)] [[cool](https://papers.cool/arxiv/2501.15619)] [[pdf](https://arxiv.org/pdf/2501.15619)]
> **Authors**: Jiajun Dong,Chengkun Wang,Wenzhao Zheng,Lei Chen,Jiwen Lu,Yansong Tang
> **First submission**: 2025-01-26
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: Gaussiantoken：有效的图像令牌，带有2D高斯分裂
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 有效的图像令牌化对于多模式理解和生成任务至关重要，因为与离散文本数据保持一致性。为此，现有方法利用向量量化（VQ）将像素投影到离散代码书上，并从离散表示形式重建图像。但是，与连续的潜在空间相比，有限的离散代码书空间显着限制了这些图像令牌的代表性。在本文中，我们提出了Gaussiantoken：一种有效的图像令牌，并以2D高斯分裂作为解决方案。我们首先将编码的样品表示为多个柔性的2D高斯人，其特征是位置，旋转角度，缩放因子和特征系数。我们采用高斯特征的标准量化，然后将量化结果与其他固有的高斯参数进行连接，然后再进行相应的分裂操作和随后的解码模块。通常，高斯语将2D高斯分布的局部影响整合到离散空间中，从而增强了图像令牌的表示能力。 CIFAR，MINI-IMAGENET和IMAGENET-1K上的竞争性重建性能证明了我们框架的有效性。我们的代码可在以下网址提供：https：//github.com/chrisdong-thu/gaussiantoken。

### Ocean-OCR: Towards General OCR Application via a Vision-Language Model 
[[arxiv](https://arxiv.org/abs/2501.15558)] [[cool](https://papers.cool/arxiv/2501.15558)] [[pdf](https://arxiv.org/pdf/2501.15558)]
> **Authors**: Song Chen,Xinyu Guo,Yadong Li,Tao Zhang,Mingan Lin,Dongdong Kuang,Youwei Zhang,Lingfeng Ming,Fengyu Zhang,Yuran Wang,Jianhua Xu,Zenan Zhou,Weipeng Chen
> **First submission**: 2025-01-26
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: Ocean-Or：通过视觉语言模型朝着一般的OCR应用
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在各个领域表现出了令人印象深刻的功能，在处理和理解多种方式的信息方面表现出色。尽管以前取得了迅速的进展，但OCR的能力不足会阻碍MLLM在与文本相关的任务方面出色。在本文中，我们提出\ textbf {Ocean-ocr}，这是一个在各种OCR方案上具有最先进性能的3B MLLM，并且对一般任务的理解能力可比。我们采用本地分辨率VIT来启用可变分辨率输入，并利用大量高质量的OCR数据集来增强模型性能。我们通过对开源OCR基准和各种OCR场景进行的全面实验来证明Ocean-Or的优势。这些场景涵盖了文档的理解，场景文本识别和手写识别，突出了Ocear-Ocr的强大OCR功能。请注意，Ocean-Ocr是第一个超越专业OCR模型（例如Textin和Paddleocr）的MLLM。

### TinyLLaVA-Video: A Simple Framework of Small-scale Large Multimodal Models for Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.15513)] [[cool](https://papers.cool/arxiv/2501.15513)] [[pdf](https://arxiv.org/pdf/2501.15513)]
> **Authors**: Xingjian Zhang,Xi Weng,Yihao Yue,Zhaoxin Fan,Wenjun Wu,Lei Huang
> **First submission**: 2025-01-26
> **First announcement**: 2025-01-27
> **comment**: code and training recipes are available at https://github.com/ZhangXJ199/TinyLLaVA-Video
- **标题**: Tinyllava-Video：小型大型多模式的简单框架，用于视频理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了Tinyllava-Video，这是一个视频理解模型，该模型的参数不超过4B，以简单的方式处理视频序列，而无需复杂的体系结构，支持FPS采样和统一的帧采样。我们的模型以模块化和可扩展性为特征，允许使用有限的计算资源进行培训和推断，并使用户能够根据其需求替换组件。我们通过实验验证了该框架的有效性，这是在多个视频理解基准测试中与某些现有7B模型相当的最佳模型。代码和培训食谱是完全开源的，所有组件和培训数据公开可用。我们希望这项工作可以作为探索小规模多模型的从业者的基准，以了解视频。它可在\ url {https://github.com/zhangxj199/tinyllava-video}上找到。

### Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception 
[[arxiv](https://arxiv.org/abs/2501.15394)] [[cool](https://papers.cool/arxiv/2501.15394)] [[pdf](https://arxiv.org/pdf/2501.15394)]
> **Authors**: Lianqing Zheng,Jianan Liu,Runwei Guan,Long Yang,Shouyi Lu,Yuanzhe Li,Xiaokai Bai,Jie Bai,Zhixiong Ma,Hui-Liang Shen,Xichan Zhu
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: Doracamom：连接3D检测和占用预测与多视雷达和相机的全向感知
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D对象检测和占用预测是自动驾驶中的关键任务，引起了重大关注。尽管最近基于视觉的方法有潜力，但它们在不利条件下遇到挑战。因此，将相机与下一代4D成像雷达相结合以实现统一的多任务感知是非常重要的，尽管该领域的研究仍然有限。在本文中，我们提出了Doracamom，这是第一个融合了多视图摄像机的框架和连接3D对象检测和语义占用预测的4D雷达，从而实现了全面的环境知觉。具体而言，我们引入了一种新型的粗素查询生成器，该生成器将4D雷达的几何先验与来自图像的语义特征集成到初始化体素查询，为后续的基于变压器的细化建立了强大的基础。为了利用时间信息，我们设计了一个双分支时间编码器，该编码器在BEV和Voxel空间中并行处理多模式的时间特征，从而实现了全面的时空表示学习。此外，我们提出了一个跨模式的BEV-Voxel融合模块，该模块通过注意机制适应互补的特征，同时采用辅助任务来增强功能质量。在OmniHd-Scenes，Dive-of-Delft（VOD）和TJ4Dradset数据集上进行了广泛的实验，这表明Doracamom在这两个任务中都能达到最先进的性能，从而为多模式3D感知建立了新的基准。代码和模型将公开可用。

### MetaOcc: Surround-View 4D Radar and Camera Fusion Framework for 3D Occupancy Prediction with Dual Training Strategies 
[[arxiv](https://arxiv.org/abs/2501.15384)] [[cool](https://papers.cool/arxiv/2501.15384)] [[pdf](https://arxiv.org/pdf/2501.15384)]
> **Authors**: Long Yang,Lianqing Zheng,Wenjin Ai,Minghao Liu,Sen Li,Qunshu Lin,Shengyu Yan,Jie Bai,Zhixiong Ma,Xichan Zhu
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: METAOCC：通过双重培训策略的3D入住预测的环绕范围4D雷达和相机融合框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 3D占用预测对于自主驾驶感知至关重要。 4D雷达和相机的融合为服务天气提供了强大的占用预测的潜在解决方案，成本最低。如何实现有效的多模式特征融合并降低注释成本仍然是巨大的挑战。在这项工作中，我们提出了一种新型的多模式占用预测框架MetaoCC，它可以融合环绕摄像机和4D雷达，以进行全面的环境感知。我们首先设计一个高度自我发场模块，以从稀疏雷达点提取有效的3D特征。然后，提出了一种局部全球融合机制，以自适应地捕获形式贡献，同时处理时空未对准。时间对齐和融合模块用于进一步汇总历史特征。此外，我们开发了一个半监督的训练程序，该程序利用了伪标签的开放式分段和几何约束，从而实现了有限的注释。 OmniHd-Scenes数据集的广泛实验表明，MetaoCC实现了最先进的性能，超过了大量边缘的先前方法。值得注意的是，作为第一个半监督的4D雷达和基于摄像机融合的占用预测方法，MetaOCC维持了92.5％的全面监督性能，同时仅使用50％的地面真相注释，为多模式3D占用率预测建立了新的基准。代码和数据可从https://github.com/lucasyang567/metaocc获得。

### Scaling Large Vision-Language Models for Enhanced Multimodal Comprehension In Biomedical Image Analysis 
[[arxiv](https://arxiv.org/abs/2501.15370)] [[cool](https://papers.cool/arxiv/2501.15370)] [[pdf](https://arxiv.org/pdf/2501.15370)]
> **Authors**: Robinson Umeike,Neil Getty,Fangfang Xia,Rick Stevens
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: 4 Pages, 4 Figures, 1 Table
- **标题**: 在生物医学图像分析中缩放大型视觉模型，以增强多模式理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型语言模型（LLM）在理解文本数据方面表现出了巨大的能力，并且越来越多地通过知识提取（信息检索），知识蒸馏（总结关键发现和方法论中的关键发现和将其转化为简洁的形式）和知识综合（将信息从多个科学来源汇总为复杂的实验，并形成复杂的实验，并形成了综合实验，并形成了综合实验，从而帮助研究人员加快科学发现的能力，并形成了知识综合。但是，科学数据通常以视觉和文本方式存在。视觉语言模型（VLM）通过合并预处理的视觉主链来处理图像和跨模式投影仪，该跨模式投影仪将图像令牌适应LLM维度空间，从而提供更丰富的多模式理解。但是，现成的VLMS在处理域特异性数据方面显示出有限的功能，并且容易出现幻觉。我们开发了从LLAVA模型进行训练的智能助手，以增强低剂量放射治疗（LDRT）的多模式理解 - 一种用于治疗癌症相关疾病的良性方法。使用来自42,673篇文章的多语言数据，我们设计了复杂的推理和详细描述视觉问题答案（VQA）基准的任务。在50,882个图像文本对上训练的我们的助手表现出了使用LLM-AS-A-A-Gudge方法评估的基本模型的优越性能，尤其是在减少幻觉和改善域特异性理解方面。

### Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.15140)] [[cool](https://papers.cool/arxiv/2501.15140)] [[pdf](https://arxiv.org/pdf/2501.15140)]
> **Authors**: Hulingxiao He,Geng Li,Zijun Geng,Jinglin Xu,Yuxin Peng
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: Published as a conference paper at ICLR 2025
- **标题**: 分析和增强对多模式大语言模型的细粒视觉识别的力量
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 多模式大型语言模型（MLLM）在各种视觉理解任务中表现出了显着的能力。但是，MLLM仍在精细粒度的视觉识别（FGVR）中挣扎，该识别旨在从图像中识别下属级别类别。这可能会对MLLM的更先进的功能产生负面影响，例如以对象为中心的视觉问题回答和推理。在我们的研究中，我们重新访问MLLM的三个典型功能用于FGVR，包括对象信息提取，类别知识储备，对象类别对准以及根本原因的位置作为未对准问题。为了解决这个问题，我们提出了FineDeafics，该MLLM通过将对象的信息描述纳入训练阶段，从而增强了模型的FGVR功能。我们同时使用对象属性对和属性类别对进行对比学习，并使用类似但不正确的类别的示例作为艰难的否定性，自然会带来视觉对象的表示形式，并且类别名称更紧密。对多个流行的FGVR数据集进行的广泛评估表明，罚款人的表现优于现有的可比参数尺寸的MLLM，展示了其出色的功效。该代码可从https://github.com/pku-icst-mipl/finedefics_iclr2025获得。

### HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.15111)] [[cool](https://papers.cool/arxiv/2501.15111)] [[pdf](https://arxiv.org/pdf/2501.15111)]
> **Authors**: Jiaxing Zhao,Qize Yang,Yixing Peng,Detao Bai,Shimin Yao,Boyuan Sun,Xiang Chen,Shenghao Fu,Weixuan chen,Xihan Wei,Liefeng Bo
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: Hanumomni：一种大型视觉语言语言模型，用于以人为中心的视频理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 在以人为本的场景中，同时了解视觉和听觉信息的能力至关重要。尽管最近的Omni模型可以处理多种模式，但由于缺乏大规模，专业的数据集和非目标体系结构，它们通常在以人为本的场景中缺乏效率。在这项工作中，我们开发了Handomni，这是该行业的第一个以人为中心的Omni-Multimodal大语模型。我们构建了一个数据集，其中包含超过240万个以人为中心的视频剪辑，并带有详细的标题和超过1400万个说明，从而促进了对以人为中心的场景的理解。 Hanumomni包括三个专门分支，用于了解不同类型的场景。它根据用户说明适应这些分支机构的功能，从而显着增强了以个人为中心的场景中的视觉理解。此外，Humanomni集成了音频功能，以确保对环境和个人的全面了解。我们的实验验证了Hymanomni在处理以人为中心的各种任务处理中的高级能力，包括情绪识别，面部表情描述和动作理解。我们的模型将被开源，以促进学术界和行业的进一步发展和协作。

### Bringing RGB and IR Together: Hierarchical Multi-Modal Enhancement for Robust Transmission Line Detection 
[[arxiv](https://arxiv.org/abs/2501.15099)] [[cool](https://papers.cool/arxiv/2501.15099)] [[pdf](https://arxiv.org/pdf/2501.15099)]
> **Authors**: Shengdong Zhang,Xiaoqin Zhang,Wenqi Ren,Linlin Shen,Shaohua Wan,Jun Zhang,Yujing M Jiang
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 将RGB和IR融合在一起：用于稳健传输线检测的分层多模式增强
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 确保农村地区的稳定电源在很大程度上取决于有效检查电力设备，尤其是输电线路（TLS）。但是，在处理可见光（RGB）和红外（IR）图像之间的未对准以及卷积网络中的高级和低级特征之间的未对准时，检测到航空影像中的TLS可能具有挑战性。为了解决这些局限性，我们提出了一个新型的层次多模式增强网络（HMMEN），该网络（HMMEN）集成了RGB和IR数据以进行稳健和准确的TL检测。我们的方法介绍了两个关键组成部分：（1）相互的多模式增强块（MMEB），该块以粗到1的方式融合和增强层次结构的RGB和IR特征图，以及（2）特征对齐块（FAB），通过利用可解码和可变形的IR特征映射贴图，可纠正不一致。我们使用基于Mobilenet的RGB和IR输入的编码器来适应边缘计算的约束并减少计算开销。与最先进的方法相比，对各种天气和照明条件的实验结果，夜间，雪和daytimedexdimend示意了我们方法的优越性和鲁棒性，从而较少误报，增强的边界描述以及更好的总体检测性能。因此，该框架显示出对无人驾驶汽车进行实际大规模电源线检查的希望。

### PatentLMM: Large Multimodal Model for Generating Descriptions for Patent Figures 
[[arxiv](https://arxiv.org/abs/2501.15074)] [[cool](https://papers.cool/arxiv/2501.15074)] [[pdf](https://arxiv.org/pdf/2501.15074)]
> **Authors**: Shreya Shukla,Nakul Sharma,Manish Gupta,Anand Mishra
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: Accepted at AAAI 2025 (Main Track). Project page: https://vl2g.github.io/projects/PatentLMM/
- **标题**: Attentlmm：用于生成专利数字描述的大型多模式模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在专利文档中撰写全面，准确的技术图纸描述对于有效的知识共享和实现知识产权的复制和保护至关重要。但是，研究界在很大程度上忽略了此任务的自动化。为此，我们介绍了Paterdesc-355k，这是一个新颖的大规模数据集，其中包含〜355K专利数字，以及从超过60k的美国专利文档中提取的简短而详细的文本描述。此外，我们提出了Paterlmm-一种新型的多式模式大型语言模型，专门针对生成专利数字的高质量描述。我们提出的PateNTLMM包括两个关键组成部分：（i）PathentMme，一种专门的多模式视觉编码器，捕获了专利数字的独特结构元素，以及（ii）Paterentllama，Patentllama，patentllama，一种适应大量专利的Llama filesain paterlama版本。广泛的实验表明，专门针对专利数字设计的视觉编码器显着提高了性能，与微调相似大小的现成的多模型模型相比，产生了连贯的描述。 Patentdesc-355k和Paterlmm为自动理解专利数字的理解铺平了道路，从而实现了有效的知识共享和更快的专利文件起草。我们将代码和数据公开可用。

### Graph-Based Cross-Domain Knowledge Distillation for Cross-Dataset Text-to-Image Person Retrieval 
[[arxiv](https://arxiv.org/abs/2501.15052)] [[cool](https://papers.cool/arxiv/2501.15052)] [[pdf](https://arxiv.org/pdf/2501.15052)]
> **Authors**: Bingjun Luo,Jinpeng Wang,Wang Zewen,Junjie Zhu,Xibin Zhao
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: Accepted by AAAI 2025
- **标题**: 基于图的跨域知识蒸馏，用于跨数据库文本对图像人检索
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **摘要**: 视频监视系统是确保智慧城市公共安全和管理的关键组件。作为视频监视的一项基本任务，文本到图像人的检索旨在从最能匹配给定文本描述的图像库中检索目标人员。大多数现有的文本对象的人检索方法都是以有监督的方式培训的，该方法需要在目标域中进行足够的标记数据。但是，在实践中通常，由于数据注释的难度和成本，目标域中仅可用数据，这限制了现有方法在实际应用程序方案中的概括。为了解决这个问题，我们提出了一种新颖的无监督域适应方法，称为基于图的跨域知识蒸馏（GCKD），以学习在跨数据库场景中的文本对图像人检索的跨模式特征表示。提出的GCKD方法由两个主要组成部分组成。首先，基于图的多模式传播模块旨在弥合视觉和文本样本之间的跨域相关性。其次，提出了一种对比动量知识蒸馏模块，以使用在线知识蒸馏策略学习跨模式特征表示。通过共同优化两个模块，所提出的方法能够实现跨数据库文本对图像人检索的有效性能。对三个公开可用的文本对象人员检索数据集进行了杂文实验，证明了所提出的GCKD方法的有效性，这始终优于最先进的基准。

### Evaluating Hallucination in Large Vision-Language Models based on Context-Aware Object Similarities 
[[arxiv](https://arxiv.org/abs/2501.15046)] [[cool](https://papers.cool/arxiv/2501.15046)] [[pdf](https://arxiv.org/pdf/2501.15046)]
> **Authors**: Shounak Datta,Dhanasekar Sundararaman
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 基于上下文感知的对象相似性评估大视觉模型中的幻觉
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 尽管在多模式任务上表现出色，但大型视觉语言模型（LVLM）倾向于遭受幻觉的影响。一种重要的类型是对象幻觉，其中LVLM会生成与模型显示的图像不一致的对象。现有作品通常试图通过检测和测量生成字幕中幻觉对象的分数来量化对象幻觉。此外，最近的工作还通过直接查询LVLM的二进制问题来衡量对象幻觉，以了解可能存在基于对象统计的可能幻觉对象（例如TOP-K频繁对象和Top-K共同出现的对象）。在本文中，我们提出了上下文感知的对象相似性（CAO），这是一种使用对象统计信息以及生成的字幕评估LVLM中对象幻觉的新方法。 CAO唯一地将对象统计信息与字幕和地面数据中对象之间的语义关系集成在一起。此外，现有方法通常仅检测和测量属于预定的一组内域对象集（通常是训练数据集的所有地面真实对象的集合），而忽略该集合的一部分的生成对象，从而导致不高度评价。为了解决这个问题，我们进一步采用基于语言模型的对象识别来检测潜在的幻觉外幻觉对象，并使用LVLMS集合来验证查询图像中此类对象的存在。 Caos还研究了对象产生的顺序动态，阐明了对象外观如何影响幻觉，并采用单词嵌入模型来分析幻觉背后的语义原因。 CAO的目的是通过提供一个系统的框架来识别和解释对象幻觉，从而对LVLM的幻觉倾向提供细微的理解。

### Measuring and Mitigating Hallucinations in Vision-Language Dataset Generation for Remote Sensing 
[[arxiv](https://arxiv.org/abs/2501.14905)] [[cool](https://papers.cool/arxiv/2501.14905)] [[pdf](https://arxiv.org/pdf/2501.14905)]
> **Authors**: Madeline Anderson,Miriam Cha,William T. Freeman,J. Taylor Perron,Nathaniel Maidel,Kerri Cahoy
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 遥感的视觉数据集生成中的测量和减轻幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型在各个领域都取得了令人印象深刻的结果。但是，遥感中的采用仍然有限，这在很大程度上是由于配对的图像文本数据的稀缺性。为了弥合这一差距，综合标题的产生引起了人们的兴趣，传统上依赖于使用元数据或边界框的基于规则的方法。尽管这些方法提供了一些描述，但它们通常缺乏捕捉复杂区域场景所需的深度。大型语言模型（LLMS）提供了一种有希望的替代方案来产生更多的描述性标题，但它们可以产生通用的输出，并且容易产生幻觉。在本文中，我们提出了一种新方法来增强视觉语言数据集，以通过将地图集成为外部数据源，从而可以生成详细的，上下文丰富的字幕。此外，我们提出了在LLM生成的文本中测量和减轻幻觉的方法。我们介绍了FMOW-MM，这是一个包含卫星图像，地图，元数据和文本注释的多模式数据集。我们证明了其在几次射击设置中自动识别的有效性，与其他视觉遥感数据集相比，实现了卓越的性能。

### Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.14818)] [[cool](https://papers.cool/arxiv/2501.14818)] [[pdf](https://arxiv.org/pdf/2501.14818)]
> **Authors**: Zhiqi Li,Guo Chen,Shilong Liu,Shihao Wang,Vibashan VS,Yishen Ji,Shiyi Lan,Hao Zhang,Yilin Zhao,Subhashree Radhakrishnan,Nadine Chang,Karan Sapra,Amala Sanjay Deshmukh,Tuomas Rintamaki,Matthieu Le,Ilia Karmanov,Lukas Voegtle,Philipp Fischer,De-An Huang,Timo Roman,Tong Lu,Jose M. Alvarez,Bryan Catanzaro,Jan Kautz,Andrew Tao, et al. (2 additional authors not shown)
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: Eagle 2：从头开始建立训练后数据策略
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 最近，开源视觉语言模型（VLM）取得了有希望的进展，使其能力更接近专有边界模型。但是，大多数开源模型仅发布他们的最终模型权重，留下数据策略和实施的关键细节，在很大程度上不透明。在这项工作中，我们从以数据为中心的角度解决了VLM训练后培训，显示了数据策略在开发Frontier VLM中的关键作用。通过从头开始研究和构建我们的培训后数据策略，我们分享了对开发过程的详细见解，旨在使开源社区的竞争模型开发。我们引入的数据策略以及培训食谱和模型设计将导致一个名为Eagle2的表演者VLM家族。具体而言，EAGLE2-9B在各种多模式基准中实现了最新的结果，与某些具有多达70B参数的竞争模型匹配。

### Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite Images by Poverty Level: Advancing Tools for Social Science Research 
[[arxiv](https://arxiv.org/abs/2501.14546)] [[cool](https://papers.cool/arxiv/2501.14546)] [[pdf](https://arxiv.org/pdf/2501.14546)]
> **Authors**: Hamid Sarmadi,Ola Hall,Thorsteinn Rögnvaldsson,Mattias Ohlsson
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 利用Chatgpt的多模式视觉功能来按贫困级别对卫星图像进行排名：社会科学研究的推进工具
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 本文研究了具有视觉功能的大型语言模型（LLM）的新应用，以分析卫星图像以进行乡村水平的贫困预测。尽管LLM最初是为自然语言理解而设计的，但它们对包括地理空间分析在内的多模式任务的适应性为数据驱动的研究开辟了新的边界。通过利用启用视觉的LLM的进步，我们评估了它们从卫星图像中提供对人类贫困的可解释，可扩展和可靠的见解的能力。使用成对比较方法，我们证明Chatgpt可以根据贫困水平对卫星图像进行排名，准确性与域专家相当。这些发现凸显了LLM在社会经济研究中的承诺和局限性，为它们融入贫困评估工作流程提供了基础。这项研究有助于对福利分析的非常规数据源进行持续的探索，并为具有成本效益的大规模贫困监测打开了途径。

### Triple Path Enhanced Neural Architecture Search for Multimodal Fake News Detection 
[[arxiv](https://arxiv.org/abs/2501.14455)] [[cool](https://papers.cool/arxiv/2501.14455)] [[pdf](https://arxiv.org/pdf/2501.14455)]
> **Authors**: Bo Xu,Qiujie Xie,Jiahui Zhou,Linlin Zong
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: IEEE International Conference on Acoustics, Speech, and Signal Processing(ICASSP 2025)
- **标题**: 三重路径增强神经架构搜索多模式假新闻检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式假新闻检测已成为社交媒体平台上最关键的问题之一。尽管现有方法已经达到了高级绩效，但两个主要挑战仍然存在：（1）由于模型架构固化而导致的多模式新闻信息融合，以及（2）部分模式的概括能力较弱，其中包含假新闻。为了应对这些挑战，我们提出了一种新颖而灵活的三路径，增强了神经建筑搜索模型缪斯。 Muse包括两条动态途径，用于检测部分模式，其中包含虚假新闻和利用潜在多模式相关性的静态途径。实验结果表明，缪斯在基线方面取得了稳定的性能提高。

### Low-rank Prompt Interaction for Continual Vision-Language Retrieval 
[[arxiv](https://arxiv.org/abs/2501.14369)] [[cool](https://papers.cool/arxiv/2501.14369)] [[pdf](https://arxiv.org/pdf/2501.14369)]
> **Authors**: Weicai Yan,Ye Wang,Wang Lin,Zirun Guo,Zhou Zhao,Tao Jin
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 持续视力语言检索的低排名及时互动
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式任务中持续学习的研究一直在受到越来越多的关注。但是，大多数现有的工作都忽略了明确的交叉模式和交叉任务交互。在本文中，我们对低级及时相互作用（LPI）进行了创新，以解决这个多模式理解的一般问题，该问题考虑了交叉模式和交叉任务相互作用。具体而言，至于前者，我们对相应的变压器层采用多模式相关模块。考虑到训练参数的规模缩小到层和任务的数量，我们提出了低级交互作用的分解，以避免记忆爆炸，同时通过共享和分离普通特异性的低级别因素来增强跨模式关联。此外，由于低级初始化带来了多模式的语义差异，我们采用了层次的低级对比度学习以确保训练鲁棒性。至于后者，我们最初采用了视觉分析，并确定不同的任务在接近性方面具有明显的区别。因此，我们根据任务语义距离在及时的学习过程中介绍了明确的任务对比约束。对两个检索任务进行的实验显示了最小数量的参数，这表明了我们方法的有效性。代码可在https://github.com/kelvin-ywc/lpi上找到。

### Global Semantic-Guided Sub-image Feature Weight Allocation in High-Resolution Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2501.14276)] [[cool](https://papers.cool/arxiv/2501.14276)] [[pdf](https://arxiv.org/pdf/2501.14276)]
> **Authors**: Yuxuan Liang,Xu Li,Xiaolei Chen,Haotian Chen,Yi Zheng,Chenghang Lai,Bin Li,Xiangyang Xue
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: 10 pages, 10 figures and tables
- **标题**: 全球语义引导的子图像特征重量分配在高分辨率大型视力模型中
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着对大视觉语言模型（LVLM）中高分辨率图像处理的需求的增长，子图像分区已成为减轻与固定分辨率处理相关的视觉信息损失的流行方法。但是，现有的分区方法均匀地处理子图像，从而产生了次优图像的理解。在这项工作中，我们揭示了与整个图像具有较高语义相关的子图像封装了更丰富的视觉信息，以保持模型的视觉理解能力。因此，我们提出了全球语义指导的权重分配器（GSWA）模块，该模块根据其相对信息密度动态分配重量为子图像，从而模仿人类的视觉注意机制。这种方法使该模型能够专注于更有信息的区域，从而克服了统一治疗的局限性。我们将GSWA集成到Intervl2-2b框架中，以创建SleighVl，这是一种轻巧但高性能的模型。广泛的实验表明，SleighVl优于具有可比参数的模型，并且与较大的模型保持竞争力。我们的工作为LVLMS中更有效且更有效的高分辨率图像处理提供了一个有希望的方向，从而推进了多模式系统的开发。

### Molecular-driven Foundation Model for Oncologic Pathology 
[[arxiv](https://arxiv.org/abs/2501.16652)] [[cool](https://papers.cool/arxiv/2501.16652)] [[pdf](https://arxiv.org/pdf/2501.16652)]
> **Authors**: Anurag Vaidya,Andrew Zhang,Guillaume Jaume,Andrew H. Song,Tong Ding,Sophia J. Wagner,Ming Y. Lu,Paul Doucet,Harry Robertson,Cristina Almagro-Perez,Richard J. Chen,Dina ElHarouni,Georges Ayoub,Connor Bossi,Keith L. Ligon,Georg Gerber,Long Phi Le,Faisal Mahmood
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 分子驱动的肿瘤病理基础模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 基础模型通过启用转移学习来重塑计算病理，在该学习中，可以在大量数据集中预先培训的模型适应下游诊断，预后和治疗性响应任务。尽管有这些进展，但基础模型仍然限制其编码整个Gigapixel全盘图像的能力，而无需额外的培训，并且通常缺乏互补的多模式数据。在这里，我们介绍了Threads，这是一个幻灯片级的基础模型，该模型能够生成任何大小的全片图像的通用表示。使用多模式学习方法预先训练螺纹，以47,171个苏木精和曙红（H＆E）塑成的组织切片的各种队列，并与相应的基因组和转录谱 - 最大的此类配对数据集配对，可用于基础模型开发至今。这种独特的训练范式使线程能够捕获组织的基本分子组成，从而产生适用于各种下游任务的强大表示。在54项肿瘤学任务中进行了广泛的基准测试，包括临床亚型，评分，突变预测，免疫组织化学状态确定，治疗反应预测和生存预测，线程的表现优于所有基准，同时表现出显着的推广性和标签效率。它特别适合预测罕见事件，进一步强调其临床实用性。我们打算使该模型公开可用于更广泛的社区。

### Large Models in Dialogue for Active Perception and Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2501.16300)] [[cool](https://papers.cool/arxiv/2501.16300)] [[pdf](https://arxiv.org/pdf/2501.16300)]
> **Authors**: Tzoulio Chamiti,Nikolaos Passalis,Anastasios Tefas
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: Accepted to International Conference of Pattern Recognition (ICPR 2024)
- **标题**: 主动感知和异常检测的对话中的大型模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 自主天线监测是一项重要的任务，旨在从人类可能不容易获得的领域收集信息。同时，这项任务通常需要从过去的距离处识别异常，或者以前曾经遇到过异常。在本文中，我们提出了一个新型框架，该框架利用大型语言模型（LLMS）提供的高级功能积极收集信息并在新型场景中进行异常检测。为此，我们提出了一种基于LLM的模型对话方法，其中两个深度学习模型进行了对话，以积极控制无人机以提高感知和异常检测准确性。我们在高保真模拟环境中进行实验，其中为LLM提供了一组预定的自然语言移动命令集，该命令映射到可执行的代码功能中。此外，我们部署了一个多模式的视觉问题回答（VQA）模型，该模型负责视觉问题的任务回答和字幕。通过与这两个模型参与对话，LLM提出了探索性问题，同时将无人机飞向场景的不同部分，提供了一种新颖的方式来实现主动感知。通过利用LLMS推理能力，我们输出了超越现有静态感知方法的改进的场景详细说明。除了信息收集外，我们的方法还用于异常检测，我们的结果证明了提出的方法在告知和警告潜在危害方面的有效性。

### FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers 
[[arxiv](https://arxiv.org/abs/2501.16297)] [[cool](https://papers.cool/arxiv/2501.16297)] [[pdf](https://arxiv.org/pdf/2501.16297)]
> **Authors**: Renshan Zhang,Rui Shao,Gongwei Chen,Kaiwen Zhou,Weili Guan,Liqiang Nie
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 猎鹰：通过视觉寄存器在高分辨率多模型中解决视觉冗余和碎片化
- **领域**: 计算机视觉和模式识别
- **摘要**: 高分辨率视觉输入的结合使多模式大型语言模型（MLLM）具有增强的视觉感知功能，可实现现实世界任务。但是，大多数现有的高分辨率MLLM都依赖于基于种植的方法来处理图像，从而导致视觉编码碎片和冗余令牌的急剧增加。为了解决这些问题，我们提出了猎鹰模型。 Falcon引入了一种新颖的视觉寄存器技术，以同时：1）在视觉编码阶段消除冗余令牌。为了直接解决视觉编码器输出中存在的视觉冗余，我们提出了一种基于寄存器的表示压实（重新组合）机制。这种机制引入了一组可学习的视觉寄存器，旨在在丢弃冗余的同时适应基本信息。它使编码器能够产生更紧凑的视觉表示，并具有最少数量的输出令牌，从而消除了对其他压缩模块的需求。 2）确保视觉编码的连续性。为了解决由零散的视觉输入引起的潜在编码错误，我们开发了寄存器交互式注意（Reatten）模块。该模块通过启用视觉寄存器之间的相互作用来促进跨子图像的有效信息交流。它确保整个编码过程中视觉语义的连续性。我们在各种场景的高分辨率基准上对猎鹰进行全面的实验。猎鹰表现出卓越的性能，视觉令牌的降低了9倍和16倍。

### Can Multimodal Large Language Models be Guided to Improve Industrial Anomaly Detection? 
[[arxiv](https://arxiv.org/abs/2501.15795)] [[cool](https://papers.cool/arxiv/2501.15795)] [[pdf](https://arxiv.org/pdf/2501.15795)]
> **Authors**: Zhiling Chen,Hanning Chen,Mohsen Imani,Farhad Imani
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: 16 pages, 11 figures
- **标题**: 可以指导多模式大型语言模型以改善工业异常检测吗？
- **领域**: 计算机视觉和模式识别
- **摘要**: 在工业环境中，准确检测异常对于维持产品质量和确保运营安全至关重要。传统的工业异常检测（IAD）模型通常会在灵活性和适应性方面挣扎，尤其是在经常出现新的缺陷类型和操作变化的动态生产环境中。多模式大语言模型（MLLM）的最新进展有望通过结合视觉和文本信息处理能力来克服这些局限性。 Mllms在大型，多样化的数据集上进行了培训，因此在一般的视觉理解中表现出色，但缺乏特定领域的知识，例如特定于行业的缺陷公差水平，这限制了它们在IAD任务中的有效性。为了应对这些挑战，我们提出了Echo，这是一种新型的多专家框架，旨在提高IAD的MLLM性能。 ECHO整合了四个专家模块：参考提取器，通过检索类似的正常图像，提供特定于领域的见解的知识指南，提供特定于领域的洞察力，可以使结构化的，逐步推理复杂的查询以及决策者，从所有模块中综合所有模块的信息来提供精确的响应，上下文响应。 ECHO在MMAD基准测试中进行了评估，显示出适应性，精度和鲁棒性的显着改善，更接近满足现实世界工业异常检测的需求。

### Learning Free Token Reduction for Multi-Modal LLM 
[[arxiv](https://arxiv.org/abs/2501.17391)] [[cool](https://papers.cool/arxiv/2501.17391)] [[pdf](https://arxiv.org/pdf/2501.17391)]
> **Authors**: Zihui Zhao,Yingxin Li,Yang Li
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: No comments
- **标题**: 学习自由令牌的多模式LLM
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 视觉语言模型（VLM）在一系列多模式任务中取得了巨大的成功。但是，他们的实际部署通常受到高计算成本和延长推理时间的限制。由于视觉模式通常比文本模式更多，因此压缩视觉提示提供了一种有希望的解决方案来减轻这些挑战。现有方法主要集中在精炼模型架构或直接减少视觉令牌的数量上。但是，由于缺乏对视觉数据的独特空间和时间特征的考虑，这些方法通常会损害推理性能。在这项工作中，我们提出了一个在空间和时间维度上运行的令牌压缩范式。我们的方法包括一个无学习的插件压缩管道，可以将其无缝集成到大多数多模式大型语言模型（MLLM）框架中。通过利用此方法，我们可以增强模型推理能力，同时降低其计算成本。视频QA任务的实验结果证明了拟议方法的有效性，展示了效率的显着提高而不牺牲绩效。

### Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding 
[[arxiv](https://arxiv.org/abs/2501.17053)] [[cool](https://papers.cool/arxiv/2501.17053)] [[pdf](https://arxiv.org/pdf/2501.17053)]
> **Authors**: Akash Kumar,Zsolt Kira,Yogesh Singh Rawat
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: ICLR'25 Main Conference. Project Page: https://akash2907.github.io/cospal_webpage
- **标题**: 上下文的自定进度学习，用于弱监督时空视频接地
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项工作中，我们专注于弱监督的时空视频接地（WSTVG）。这是一项多模式任务，旨在根据文本查询本地化特定主题，而无需限制框监管。由多模式基础模型的最新进步进行，我们首先探讨了WSTVG最先进的对象检测模型的潜力。尽管具有强大的零射击功能，但我们的适应性揭示了重大局限性，包括时间预测不一致，对复杂查询的理解不足以及适应困难场景的挑战。我们提出了Cospal（上下文自定进度学习），这是一种新颖的方法，旨在克服这些局限性。 COSPAL整合了三个核心组成部分：（1）小管短语接地（TPG），它通过将文本查询与小管链接起来引入时空预测； （2）上下文转介接地（CRG），通过提取上下文信息来改进对象识别，从而提高对复杂查询的理解； （3）自定进度的场景理解（SPS），这是一种训练范式，逐渐增加了任务难度，从而使模型能够通过从粗糙的理解过渡到细粒度的理解来适应复杂的方案。

### Exploring the Role of Explicit Temporal Modeling in Multimodal Large Language Models for Video Understanding 
[[arxiv](https://arxiv.org/abs/2501.16786)] [[cool](https://papers.cool/arxiv/2501.16786)] [[pdf](https://arxiv.org/pdf/2501.16786)]
> **Authors**: Yun Li,Zhe Liu,Yajing Kong,Guangrui Li,Jiyuan Zhang,Chao Bian,Feng Liu,Lina Yao,Zhenbang Sun
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: No comments
- **标题**: 探索在多模式大语言模型中明确的时间建模的作用以进行视频理解
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 应用多模式的大语言模型（MLLM）来视频理解，由于需要对跨框架建模时间关系，提出了重大挑战。现有方法采用隐式时间建模，仅依靠LLM解码器或使用辅助时间编码器的明确时间建模。为了调查两个范式之间的辩论，我们提出了可堆叠的时间编码器（Ste）。 Ste可以通过可调节的时间接收场和令牌压缩比进行柔性明确的时间建模。使用Ste，我们会系统地比较跨维度的隐式和明确的时间建模，例如总体性能，令牌压缩效果和时间特定的理解。我们还探讨了Ste的设计考虑因素和更广泛的影响，作为插件模块和图像模式。我们的发现强调了明确的时间建模的关键作用，提供了可行的见解来推动视频MLLM。

### FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation 
[[arxiv](https://arxiv.org/abs/2501.16778)] [[cool](https://papers.cool/arxiv/2501.16778)] [[pdf](https://arxiv.org/pdf/2501.16778)]
> **Authors**: Arvin Tashakori,Arash Tashakori,Gongbo Yang,Z. Jane Wang,Peyman Servati
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: No comments
- **标题**: 挠性：轻巧，物理学和可控的人类运动产生
- **领域**: 计算机视觉和模式识别,人工智能,图形,机器学习
- **摘要**: 轻巧，可控且在物理上合理的人类运动合成对于动画，虚拟现实，机器人和人类计算机的相互作用应用至关重要。现有方法通常会在计算效率，物理现实主义或空间可控性之间妥协。我们提出了FlexMotion，这是一个新型框架，利用在潜在空间中运行的计算轻量级扩散模型，从而消除了对物理模拟器的需求并实现快速有效的训练。 Flexmotion采用了多模式预训练的变压器编码器编码器，整合了关节位置，接触力，关节驱动和肌肉激活，以确保生成的运动的物理合理性。 FlexMotion还引入了插件模块，该模块在一系列运动参数（例如，关节位置，关节驱动，接触力和肌肉激活）上增加了空间可控性。我们的框架通过提高效率和控制，实现了现实的运动产生，为人类运动合成树立了新的基准。我们在扩展数据集上评估了弹性，并在现实主义，身体合理性和可控性方面证明了其出色的性能。

### REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning 
[[arxiv](https://arxiv.org/abs/2501.18124)] [[cool](https://papers.cool/arxiv/2501.18124)] [[pdf](https://arxiv.org/pdf/2501.18124)]
> **Authors**: Liangjing Shao,Benshuang Chen,Shuting Zhao,Xinrong Chen
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: Accepted by ICRA 2025
- **标题**: 遥控器：通过多模式视觉功能学习的各种内窥镜的实时自我运动跟踪
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 内窥镜的实时自我运动跟踪是有效导航和机器人自动化的重要任务。在本文中，提出了一个新颖的框架来对内窥镜进行实时自我运动跟踪。首先，提出了一个多模式的视觉特征学习网络来执行相对姿势预测，其中从光流，场景特征，两个相邻观测值的接头特征和关节特征都提取以进行预测以进行预测。由于串联图像的通道维度中的更多相关信息，因此根据注意机制设计了一种新型特征提取器，以从两个连续帧的串联中整合多维信息。为了从融合功能中提取更完整的特征表示形式，提出了一种新型的姿势解码器，以预测框架末尾的串联特征图的姿势转换。最后，基于相对姿势计算内窥镜的绝对姿势。该实验是在各种内窥镜场景的三个数据集上进行的，结果表明所提出的方法的表现优于最先进的方法。此外，所提出方法的推理速度每秒超过30帧，满足实时要求。项目页面在这里：remote-bmxs.netlify.app

### LLMs can see and hear without any training 
[[arxiv](https://arxiv.org/abs/2501.18096)] [[cool](https://papers.cool/arxiv/2501.18096)] [[pdf](https://arxiv.org/pdf/2501.18096)]
> **Authors**: Kumar Ashutosh,Yossi Gandelsman,Xinlei Chen,Ishan Misra,Rohit Girdhar
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: Code: https://github.com/facebookresearch/MILS
- **标题**: LLM可以在没有任何培训的情况下看到和听到
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 我们提出MILS：多模式迭代LLM Solver，这是一种令人惊讶的简单，无训练的方法，可将多模式功能浸入您喜欢的LLM中。 MILS利用其先天的能力执行多步推理，提示LLM生成候选输出，每种输出都会对其进行评分和返回，最终为任务生成了解决方案。这实现了各种通常需要在特定于任务数据上培训专业模型的应用程序。特别是，我们在紧急零摄像，视频和音频字幕上建立了一个新的最新技术。 MILS也无缝适用于媒体发电，发现及时重写以改善文本到图像的生成，甚至编辑提示提示样式转移！最后，作为一种无梯度的优化方法，MIL可以将多模式嵌入文本倒入文本中，从而使应用程序诸如跨模式算术之类的应用程序。

### Generative AI for Vision: A Comprehensive Study of Frameworks and Applications 
[[arxiv](https://arxiv.org/abs/2501.18033)] [[cool](https://papers.cool/arxiv/2501.18033)] [[pdf](https://arxiv.org/pdf/2501.18033)]
> **Authors**: Fouad Bousetouane
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: 53 pages, 18 figures
- **标题**: 视力的生成AI：框架和应用的全面研究
- **领域**: 计算机视觉和模式识别
- **摘要**: 生成的AI正在改变图像的综合，从而在设计，媒体，医疗保健和自主系统等行业之间创建高质量，多样化和相思的视觉效果。图像到图像翻译，文本到图像生成，域传输和多模式对齐等技术的进步扩大了自动化视觉内容创建的范围，支持了广泛的应用程序。这些进步是由生成对抗网络（GAN），条件框架和基于扩散的方法（例如稳定扩散）驱动的。这项工作基于输入的性质，通过输入模态（例如嘈杂的向量，潜在表示和条件输入）来组织图像生成技术的结构化分类。我们探索这些模型背后的原理，突出显示包括DALL-E，ControlNet和DeepSeek Janus-Pro在内的关键框架，并应对诸如计算成本，数据偏见以及与用户意图的输出一致性等挑战。通过提供这种以输入为中心的观点，这项研究将技术深度与实用的见解融为一体，从而为研究人员和从业人员提供了用于利用现实世界应用程序生成的AI的全面资源。

### VidSole: A Multimodal Dataset for Joint Kinetics Quantification and Disease Detection with Deep Learning 
[[arxiv](https://arxiv.org/abs/2501.17890)] [[cool](https://papers.cool/arxiv/2501.17890)] [[pdf](https://arxiv.org/pdf/2501.17890)]
> **Authors**: Archit Kambhamettu,Samantha Snyder,Maliheh Fakhar,Samuel Audia,Ross Miller,Jae Kun Shim,Aniket Bera
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-30
> **comment**: Accepted by AAAI 2025 Special Track on AI for Social Impact
- **标题**: vidsole：一种用于深度学习的联合动力学定量和疾病检测的多模式数据集
- **领域**: 计算机视觉和模式识别,信号处理
- **摘要**: 了解内部关节负荷对于诊断与步态相关疾病（如膝关节骨关节炎）至关重要。但是，当前测量关节风险因素的方法是耗时，昂贵且限于实验室设置。在本文中，我们可以通过三个关键贡献对关节负荷进行大规模的，具有成本效益的生物力学分析：新型仪器鞋垫的开发和部署，创建大型多模式生物力学数据集（Vidsole）（Vidsole）以及基线深度学习管道以预测内部关节负载因子。我们的新型仪器鞋垫测量了脚下五个高压点的三轴力和力矩。 VIDSOLE由这些鞋垫测量的力和力矩以及从两个观点的相应RGB视频组成，3D身体运动捕获以及对52个不同参与者进行的超过2,600个试验的强制板数据，这些试验进行了四种日常生活的基本活动（坐姿到姿势，站立，站立，站立，步行，步行和跑步）。我们将可从视频（即姿势，膝盖角度）提取的鞋垫数据和动力学参数馈送到一个深度学习管道中，该管道由一个合奏的门控复发单元（GRU）活性分类器组成，然后是活动特异性长期记忆（LSTM），然后进行估计的膝盖累加力矩（KAM），生物机械风险因素，以供kneey kneee soste soste ost。以99.02％的精度和KAM估计的成功分类，平均绝对误差（MAE）小于0.5％*体重*身高*身高*，目前用KAM准确检测膝关节骨关节炎的阈值说明了我们数据集用于未来的研究和临床设置的有用性。

### Robust Multimodal Learning via Cross-Modal Proxy Tokens 
[[arxiv](https://arxiv.org/abs/2501.17823)] [[cool](https://papers.cool/arxiv/2501.17823)] [[pdf](https://arxiv.org/pdf/2501.17823)]
> **Authors**: Md Kaykobad Reza,Ameya Patil,Mashhour Solh,M. Salman Asif
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: 17 Pages, 10 Figures, 6 Tables
- **标题**: 通过跨模式代理代币进行强大的多模式学习
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 当推断过程中缺少一种或多种方式时，多模型模型通常会出现显着的性能下降。为了应对这一挑战，我们提出了一种简单而有效的方法，可以增强缺失方式的鲁棒性，同时在所有模式都可以保持强劲的表现。我们的方法引入了跨模式代理令牌（CMPTS），该代币仅通过参加可用模式的令牌来近似丢失模式的类令牌。为了有效地通过最小的计算开销的CMPT来学习缺失模式的近似值，我们在冷冻的单峰编码器中采用了低级适配器，并共同优化了与特定于任务的损失的对齐损失。在五个多模式数据集上进行的广泛实验表明，我们的方法在各种缺失率上优于最先进的基线，同时在完全模式设置中获得竞争性结果。总体而言，我们的方法为强大的多模式学习提供了灵活，有效的解决方案。代码和验证的模型将在GitHub上发布。

### VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies in Generated Report Without Human Feedback 
[[arxiv](https://arxiv.org/abs/2501.17726)] [[cool](https://papers.cool/arxiv/2501.17726)] [[pdf](https://arxiv.org/pdf/2501.17726)]
> **Authors**: Sayeh Gholipour Picha,Dawood Al Chanti,Alice Caplier
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: No comments
- **标题**: 维卡：在没有人类反馈的报告中，胸部X射线异常的视觉解释和理解
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 随着人工智能（AI）对医疗保健的核心越来越重要，对可解释和值得信赖的模型的需求至关重要。胸部X射线（CXR）的当前报告生成系统通常缺乏验证输出而无需专家监督的机制，从而引起了对可靠性和可解释性的担忧。为了应对这些挑战，我们提出了一个新型的多模式框架，旨在增强AI生成的医学报告的语义一致性和定位精度。我们的框架集成了两个关键模块：一个短语接地模型，该模型基于文本提示来识别和定位CXR图像中的病理，而文本对图像扩散模块，该模块从提示中生成综合CXR图像，同时保留解剖学忠诚度。通过比较原始图像和生成的图像之间的功能，我们引入了双分数系统：一个分数量化了本地化精度，而另一个分数则评估了语义一致性。这种方法极大地胜过现有的方法，实现最新的方法会导致病理定位和文本对象对齐。短语接地与扩散模型的整合，再加上双评分评估系统，为验证报告质量提供了强大的机制，为在医学成像中更值得信赖和透明的AI铺平了道路。

### Action Recognition Using Temporal Shift Module and Ensemble Learning 
[[arxiv](https://arxiv.org/abs/2501.17550)] [[cool](https://papers.cool/arxiv/2501.17550)] [[pdf](https://arxiv.org/pdf/2501.17550)]
> **Authors**: Anh-Kiet Duong,Petra Gomez-Krämer
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: ICPR2024, MMVPR, 12 pages
- **标题**: 使用时间偏移模块和合奏学习的行动识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了多模式动作识别挑战的首先解决方案，这是\ acl {ICPR} 2024的多模式视觉模式识别研讨会的一部分。竞争旨在使用20个动作类别的人类行动来识别从多模式来源收集的20个动作类别的人类行动。所提出的方法建立在\ acl {TSM}上，该技术旨在有效地捕获视频数据中的时间动态，并结合了多种数据输入类型。我们的策略包括转移学习以利用预训练的模型，然后在挑战的特定数据集上进行细致的微调，以优化20个动作类别的性能。我们仔细选择了一个骨干网络，以平衡计算效率和识别精度，并使用集成技术进一步完善了模型，从而整合了来自不同方式的输出。这种合奏方法证明对提高整体性能至关重要。我们的解决方案在测试集上实现了完美的TOP-1准确性，证明了拟议方法在识别20个类别的人类行为方面的有效性。我们的代码可在线获得https://github.com/ffyyytt/tsm-mmvpr。

### UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent 
[[arxiv](https://arxiv.org/abs/2501.18867)] [[cool](https://papers.cool/arxiv/2501.18867)] [[pdf](https://arxiv.org/pdf/2501.18867)]
> **Authors**: Jianke Zhang,Yanjiang Guo,Yucheng Hu,Xiaoyu Chen,Xiang Zhu,Jianyu Chen
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: UP-VLA：体现代理的统一理解和预测模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉动作（VLA）模型的最新进展已利用预先训练的视力语言模型（VLM）来提高概括能力。 VLM通常在视觉理解任务上进行培训，提供丰富的语义知识和推理能力。但是，先前的研究表明，VLM通常专注于高级语义内容和忽略低级功能，从而限制了它们捕获详细的空间信息并了解物理动态的能力。这些方面对于体现的控制任务至关重要，在现有的预训练范式中仍然没有被忽视。在本文中，我们调查了VLA的培训范式，并介绍了\ textbf {up-vla}，一个\ textbf {u}带有多模式\ textbf {u} u} nderstanding和未来\ textbf {p textbf {p}降低的对象，增强了较高的精确效果的nifif vla模型培训，并提高了高低的理解。实验结果表明，与先前的最新方法相比，UP-VLA在Calvin ABC-D基准测试方面提高了33％。此外，UP-VLA表明了实际操纵任务的成功率提高了，尤其是那些需要精确空间信息的任务。

### Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion 
[[arxiv](https://arxiv.org/abs/2501.18804)] [[cool](https://papers.cool/arxiv/2501.18804)] [[pdf](https://arxiv.org/pdf/2501.18804)]
> **Authors**: Vitor Guizilini,Muhammad Zubair Irshad,Dian Chen,Greg Shakhnarovich,Rares Ambrus
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: Project page: https://mvgd.github.io
- **标题**: 零拍的新型视图和深度合成，具有多视图几何扩散
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 从稀疏摆姿势的图像进行3D场景重建的当前方法采用了中间3D表示，例如神经场，素网格或3D高斯人，以实现多视图一致的场景外观和几何形状。在本文中，我们介绍了MVGD，这是一种基于扩散的架构，能够从新颖的观点从新观点进行直接像素级生成图像和深度图。我们的方法使用Raymap调节来增强视觉特征，并从不同的角度使用空间信息，并指导新视图中的图像和深度图的产生。我们方法的一个关键方面是使用可学习的任务嵌入图像和深度图的多任务生成，以指导扩散过程朝着特定的方式。我们将该模型训练在公开可用数据集的超过6000万个多视图样本的集合中，并提出技术以在这种不同的条件下实现有效，一致的学习。我们还提出了一种新颖的策略，该策略可以通过逐步调整较小的模型进行有效的较小模型，并具有有希望的缩放行为。通过广泛的实验，我们报告最先进的结果可导致多种新型视图综合基准，以及多视图立体声和视频深度估计。

### Every Image Listens, Every Image Dances: Music-Driven Image Animation 
[[arxiv](https://arxiv.org/abs/2501.18801)] [[cool](https://papers.cool/arxiv/2501.18801)] [[pdf](https://arxiv.org/pdf/2501.18801)]
> **Authors**: Zhikang Dong,Weituo Hao,Ju-Chiang Wang,Peng Zhang,Pawel Polak
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 每个图像都会听，每个图像跳舞：音乐驱动的图像动画
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 图像动画已成为多模式研究的一个有前途的领域，重点是从参考图像中生成视频。虽然先前的工作在很大程度上强调了以文本为指导的通用视频，但音乐驱动的舞蹈视频发电尚未被逐渐发展。在本文中，我们介绍了一种创新的端到端模型Musedance，可以使用音乐和文本输入来对参考图像进行动画化。这种双重输入使沉思能够生成个性化视频，以遵循文本描述并将角色动作与音乐同步。与现有的方法不同，《沉思》消除了对复杂运动指导输入的需求，例如姿势或深度序列，使所有专业水平的用户都可以访问灵活和创造性的视频。为了推进该领域的研究，我们提出了一个新的多模式数据集，其中包括2,904个舞蹈视频，其中包含相应的背景音乐和文本说明。我们的方法利用基于扩散的方法来实现强大的概括，精确控制和时间一致性，为音乐驱动的图像动画任务设定了新的基线。

### Multispectral 3D mapping on a Roman sculpture to study ancient polychromy 
[[arxiv](https://arxiv.org/abs/2501.18786)] [[cool](https://papers.cool/arxiv/2501.18786)] [[pdf](https://arxiv.org/pdf/2501.18786)]
> **Authors**: Francesca Uccheddu,Umair Shafqat Malik,Emanuela Massa,Anna Pelagotti,Maria Emilia Masci,Gabriele Guidi
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: 14 pages, 5 figures, to be published in the proceedings of "Heri-Tech - The Future of Heritage Science And Technologies" Conference by Springer, 29-30 April 2024, Florence, Italy (https://www.florenceheritech.com/)
- **标题**: 多光谱3D映射在罗马雕塑上研究古代多chromy
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 对希腊语和罗马雕塑的多chromy术的研究刺激了以下假设：古老的雕塑最初不是原始的白色，而是饰有颜色。多光谱和多模式成像技术对于研究彩绘表面至关重要，即使在痕迹中也揭示了多色。实际上，成像技术（例如反射率和荧光）可以识别不同的材料和映射不均匀性，指导进一步的研究，例如拉曼，X夏列荧光和傅立叶变换红外光谱光谱（FTIR），以研究残留颜色。但是，这种方法可能低估了原始多色在雕塑表面的复杂表面上的范围。这项研究提出了一种方法，以使用基于现实的3D模型，其纹理不限于肉眼可见的方法来分析古代雕塑的原始外观。我们采用可见的反射成像（VIS）和紫外线诱导的荧光成像（UVF）。从UVF和VIS数据集中，基础3D模型是通过摄影测量法构建的。通过原始数据处理，用不同照明源拍摄的图像成功地对齐和处理，创建了一个单个3D模型，其多个纹理映射到相同的双维空间。不同纹理的像素到像素的对应关系允许实现分类算法，该算法可以将其结果直接映射到3D模型表面。这使保护者能够加深他们对伪影保存的理解，详细观察伴侣 - 摩尔分布，并将其与3D几何数据相关联。在这项研究中，我们在意大利格罗塞托（Grosseto）的Maremma（Maam）考古和艺术博物馆（Maremma）的考古和艺术博物馆中进行了这种方法，以这种方法进行了这种方法。

### Human Re-ID Meets LVLMs: What can we expect? 
[[arxiv](https://arxiv.org/abs/2501.18698)] [[cool](https://papers.cool/arxiv/2501.18698)] [[pdf](https://arxiv.org/pdf/2501.18698)]
> **Authors**: Kailash Hambarde,Pranita Samale,Hugo Proença
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 人类的重新ID遇到LVLM：我们可以期待什么？
- **领域**: 计算机视觉和模式识别
- **摘要**: 从内容生成到虚拟助手到多模式搜索或检索，大型视觉模型（LVLM）被视为突破性的进步。但是，对于许多这些应用程序，这些方法的性能受到了广泛的批评，尤其是与每个特定领域中的最新方法和技术相比。在这项工作中，我们比较了人类重新识别任务中领先的大型视力模型的性能，该模型使用基线，由最新的AI模型实现的性能，专门针对此问题设计。我们使用著名的Market1501数据集将结果与Chatgpt-4O，Gemini-2.0-Flash，Claude 3.5十四行诗和QWEN-VL-MAX与基线REID PersonVit模型进行比较。我们的评估管道包括数据集策划，及时工程和度量选择，以评估模型的性能。从许多不同的角度分析结果：相似性得分，分类准确性和分类指标，包括精度，召回，F1分数和曲线下的面积（AUC）。我们的结果证实了LVLM的优势，但它们的严重局限性通常会导致灾难性的答案，应该是进一步研究的范围。总之，我们推测一些进一步的研究应该融合传统和LVLM，以结合两种技术家族的优势，并在绩效方面取得良好的改进。

### High-Accuracy ECG Image Interpretation using Parameter-Efficient LoRA Fine-Tuning with Multimodal LLaMA 3.2 
[[arxiv](https://arxiv.org/abs/2501.18670)] [[cool](https://papers.cool/arxiv/2501.18670)] [[pdf](https://arxiv.org/pdf/2501.18670)]
> **Authors**: Nandakishor M,Anjali M
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 使用参数有效的Lora微调的高准确性ECG图像解释与多模式骆驼3.2
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 心电图（ECG）解释是心脏诊断的基石。本文探讨了一种使用多模式骆驼3.2模型增强ECG图像解释的实用方法。我们使用了一种专门设计的参数效率微调策略，低级适应性（LORA），以提高模型理解ECG图像并在各种心脏条件下取得更好的结果。我们的方法是针对心电图分析而定制的，并利用Edcinstruct，这是一个具有100万个样本的大规模指令数据集。该数据集是由合成的ECG图像组成的丰富集合，该图像是从可信赖的开源存储库（如MIMIC-IV ECG和PTB-XL）中生成的。 Ecginstruct中的每个ECG图像都有专家编写的问题和详细的答案，涵盖了各种ECG解释方案，包括心肌梗塞和传导干扰等复杂的心脏状况。我们的微调方法有效地适应了Llama 3.2模型（基于Llama 3），通过集成低级适应技术，通过仅更新一小部分参数来集中于效率，专门忽略了`lm_head'和`embed_tokens''层。本文详细介绍了模型设置，我们有效的微调方法和实施细节。我们通过广泛的实验提供了彻底的评估，证明了我们在各种心电图解释任务中的方法的有效性。结果令人信服地表明，我们的参数效率洛拉微调在ECG图像解释中取得了出色的性能，显着超过了基线模型，并且达到与基于CNN的传统方法相当或超过基于CNN的准确性，以识别多种心脏异常，包括PTB-XL数据集中的70多种条件。

### Image, Text, and Speech Data Augmentation using Multimodal LLMs for Deep Learning: A Survey 
[[arxiv](https://arxiv.org/abs/2501.18648)] [[cool](https://papers.cool/arxiv/2501.18648)] [[pdf](https://arxiv.org/pdf/2501.18648)]
> **Authors**: Ranjan Sapkota,Shaina Raza,Maged Shoman,Achyut Paudel,Manoj Karkee
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 使用多模式LLM进行深度学习的图像，文本和语音数据扩展：调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 在过去的五年中，研究已从传统的机器学习（ML）和深度学习（DL）方法转变为利用大型语言模型（LLMS），包括多模式，以增强数据增强，以增强概括，并在培训深层卷积神经网络中对过度拟合。但是，尽管现有的调查主要集中在ML和DL技术或有限的模式（文本或图像）上，但差距仍在解决基于LLM的方法的最新进步和多模式应用方面。这项调查通过探索最近的文献利用多模式LLM来增强图像，文本和音频数据，从而填补了这一空白，从而对这些过程有了全面的了解。我们概述了基于LLM的图像，文本和语音增强中使用的各种方法，并讨论了当前方法中确定的局限性。此外，我们确定了对文献中这些局限性的潜在解决方案，以增强使用多模式LLM的数据增强实践的功效。这项调查是未来研究的基础，旨在完善和扩展多模式LLMS的使用，以增强数据集质量和多样性，以实现深度学习应用程序。 (Surveyed Paper GitHub Repo: https://github.com/WSUAgRobotics/data-aug-multi-modal-llm. Keywords: LLM data augmentation, LLM text data augmentation, LLM image data augmentation, LLM speech data augmentation, audio augmentation, voice augmentation, chatGPT for data augmentation, DeepSeek R1 text data augmentation, DeepSeek R1图像增强，使用LLM的图像增强，使用LLM，LLM数据增强的文本增强，用于深度学习应用程序）

### Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models 
[[arxiv](https://arxiv.org/abs/2501.18592)] [[cool](https://papers.cool/arxiv/2501.18592)] [[pdf](https://arxiv.org/pdf/2501.18592)]
> **Authors**: Hao Dong,Moru Liu,Kaiyang Zhou,Eleni Chatzi,Juho Kannala,Cyrill Stachniss,Olga Fink
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: Project page: https://github.com/donghao51/Awesome-Multimodal-Adaptation
- **标题**: 多模式适应和概括的进步：从传统方法到基础模型
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术
- **摘要**: 在实际情况下，实现域的适应性和泛化会带来重大挑战，因为模型必须适应或推广到未知的目标分布中。将这些功能扩展到看不见的多模式分布，即多模式域的适应和概括，由于不同方式的独特特征，更具挑战性。这些年来，已经取得了重大进展，应用程序从行动识别到语义细分范围不等。此外，最近大规模训练的多模式模型（例如剪辑）启发了这些模型来增强适应性和泛化性能或使其适应下游任务的工作。这项调查提供了对从传统方法到基础模型的最新进展的首次全面综述，涵盖：（1）多模式域的适应； （2）多模式测试时间适应； （3）多模式域的概括； （4）域的适应性和概括在多模式基础模型的帮助下； （5）改编多模式基础模型。对于每个主题，我们正式定义问题并彻底查看现有方法。此外，我们分析了相关的数据集和应用程序，突出了开放挑战和潜在的未来研究方向。我们维护一个活跃的存储库，该存储库包含https://github.com/donghao51/awesome-multimodal-apaptation上的最新文献。

### Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models 
[[arxiv](https://arxiv.org/abs/2501.18533)] [[cool](https://papers.cool/arxiv/2501.18533)] [[pdf](https://arxiv.org/pdf/2501.18533)]
> **Authors**: Yi Ding,Lijun Li,Bing Cao,Jing Shao
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 重新考虑视觉语言模型的安全微调中的瓶颈
- **领域**: 计算机视觉和模式识别,计算语言学,密码学和安全
- **摘要**: 大型视觉模型（VLM）在各种任务中都取得了出色的性能。但是，它们在安全关键领域中的部署构成了重大挑战。专注于文本或多模式内容的现有安全微调方法在解决挑战案件或破坏帮助和无害性之间的平衡方面缺乏。我们的评估强调了安全推理差距：这些方法缺乏安全视觉推理能力，从而导致了这种瓶颈。为了解决这一限制并在安全至关重要的环境中增强视觉感知和推理，我们提出了一个新颖的数据集，将多图像输入与安全链（COT）标签集成为细粒度的推理逻辑，以提高模型性能。具体来说，我们介绍了多图像安全（MIS）数据集，这是一个针对多图像安全方案的指令遵循的数据集，包括培训和测试拆分。我们的实验表明，在挑战需要与安全相关的视觉推理的多图像任务中，对实习生VL2.5-8B的微调显着优于强大的开源模型和基于API的模型。这种方法不仅提供了出色的安全性能，而且还可以保留没有任何权衡的一般能力。具体而言，通过MIS进行微调将平均准确度提高了0.83％，并在多个安全基准上降低了攻击成功率（ASR）的幅度很大。数据和模型在以下内容下发布：\ href {https://dripnowhy.github.io/mis/} {\ texttt {https://dripnowhy.githhy.github.io/mis/mis/}}}

### MAMS: Model-Agnostic Module Selection Framework for Video Captioning 
[[arxiv](https://arxiv.org/abs/2501.18269)] [[cool](https://papers.cool/arxiv/2501.18269)] [[pdf](https://arxiv.org/pdf/2501.18269)]
> **Authors**: Sangho Lee,Il Yong Chun,Hogun Park
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: Accepted to the AAAI 2025 Main Technical Track. This is an extended version of the original submission
- **标题**: 妈妈：视频字幕的模型 - 敏锐的模块选择框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式变压器在视频字幕任务中迅速引起关注。现有的多模式视频字幕方法通常提取固定数量的帧，这引起了关键的挑战。当提取有限数量的帧时，可能会错过带有字幕生成基本信息的重要帧。相反，提取过多数量的帧包括连续帧，可能会导致从连续的视频帧中提取的视觉令牌中的冗余。为了为每个视频提取适当数量的框架，本文提出了视频字幕中的第一个模型不合Snostic模块选择框架，该框架具有两个主要功能：（1）根据从视频框架中提取的可视化令牌选择标题的生成模块，（2）为所选的字幕生成模块构建图表子集。此外，我们提出了一种新的自适应注意掩盖方案，以增强对重要视觉令牌的关注。我们在三个不同基准数据集上的实验表明，所提出的框架显着提高了三个最近的视频字幕模型的性能。

### Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers 
[[arxiv](https://arxiv.org/abs/2501.18237)] [[cool](https://papers.cool/arxiv/2501.18237)] [[pdf](https://arxiv.org/pdf/2501.18237)]
> **Authors**: Malte Tölle,Mohamad Scharaf,Samantha Fischer,Christoph Reich,Silav Zeid,Christoph Dieterich,Benjamin Meder,Norbert Frey,Philipp Wild,Sandy Engelhardt
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 作为图像的任意数据：跨模态和视觉变压器不规则间隔的患者数据融合
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 患者在每次住院期间进行多次检查，每个医院都提供了健康状况的不同方面。这些评估包括具有不同采样率的时间数据，离散的单点测量，治疗干预措施，例如药物给药和图像。尽管医生能够直观地处理和整合多种方式，但神经网络需要针对每种模态的特定建模，使训练程序变得复杂。我们证明，通过将所有信息视为图像以及非结构化的文本，然后训练传统的视觉 - 文本变压器，可以显着降低这种复杂性。我们的方法，视觉变压器用于不规则采样多模式测量（VITIMM），不仅简化了数据预处理和建模，而且在预测院内死亡率和表型方面的当前最新方法，可从模拟数据集对6,175名患者进行评估。这种方式包括患者的临床测量，药物，X射线图像和心电图扫描。我们希望我们的工作通过将培训复杂性降低到（视觉）促进工程，从而降低入口障碍并实现无代码解决方案进行培训，从而激发了多模式医学AI的进步。源代码将公开可用。

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

该领域共有 3 篇论文

### Efficiently Serving Large Multimodal Models Using EPD Disaggregation 
[[arxiv](https://arxiv.org/abs/2501.05460)] [[cool](https://papers.cool/arxiv/2501.05460)] [[pdf](https://arxiv.org/pdf/2501.05460)]
> **Authors**: Gursimran Singh,Xinglu Wang,Yifan Hu,Timothy Yu,Linzi Xing,Wei Jiang,Zhefeng Wang,Xiaolong Bai,Yi Li,Ying Xiong,Yong Zhang,Zhenan Fan
> **First submission**: 2024-12-25
> **First announcement**: 2025-01-10
> **comment**: 16 pages, 11 figures
- **标题**: 使用EPD分解有效地提供大型多模型模型
- **领域**: 分布式、并行和集群计算,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 大型多模型模型（LMM）通过处理图像，音频和视频等各种输入来扩展大语言模型（LLMS），但以添加多模式编码阶段的成本来增加计算和内存开销。此步骤会对关键服务级别目标（SLO）产生负面影响，例如第一个令牌（TTFT）和端到端吞吐量（E2ETP）。我们介绍了Encode-Prefill-Decode（EPD）分类，这是一个新颖的框架，将编码，预填充和解码阶段分开到专用资源上。与当前的系统（将编码和预填充捆绑在一起的当前系统）不同，我们的方法将这些步骤解开了新的机会和优化。其中包括一种新的机制来缓存多媒体代币以进行有效的转移，这是一种并行化对请求中编码负载并行编码负载的新方法，一个模块以查找分类服务的最佳资源分配以及一种新的角色切换方法来处理变化的工作负载特征。受欢迎的LMM的实验评估显示出记忆效率的可观提高（最多15美元$ \ times $少于利用率），批量尺寸（最高22美元$ \ times $较大），10 $ \ times $ $ $ $ $ $ \ times $ \ times $ $ \ times $ $ kv kv缓存。此外，与不分解分类的系统相比，它导致延迟指标（TTFT高达71 \％\％降低）和端到端吞吐量（最高57 \％降低）的显着改善。

### Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for Foundation Models 
[[arxiv](https://arxiv.org/abs/2501.14755)] [[cool](https://papers.cool/arxiv/2501.14755)] [[pdf](https://arxiv.org/pdf/2501.14755)]
> **Authors**: Daoyuan Chen,Yilun Huang,Xuchen Pan,Nana Jiang,Haibin Wang,Ce Ge,Yushuo Chen,Wenhao Zhang,Zhijian Ma,Yilei Zhang,Jun Huang,Wei Lin,Yaliang Li,Bolin Ding,Jingren Zhou
> **First submission**: 2024-12-23
> **First announcement**: 2025-01-27
> **comment**: 16 pages, 9 figures, 3 tables
- **标题**: Data-Juicer 2.0：基础模型的云规模自适应数据处理
- **领域**: 分布式、并行和集群计算,人工智能
- **摘要**: 基础模型的新兴领域需要能够利用这些模型使用的各种类型的大量有价值数据的高级数据处理机制。然而，当前的景观提出了传统数据处理框架无法有效处理的独特挑战，尤其是在多模式复杂性的情况下。作为回应，我们提出了数据Juicer 2.0，这是一个新系统，提供了一百多个运营商支持的富有成果的数据处理功能，这些操作员涵盖了文本，图像，音频和视频。凭借无缝的兼容性和专用于流行的数据集枢纽，例如拥抱面孔和诸如Ray之类的计算引擎，Data-Juicer 2.0在可用性，效率和可编程性方面都增强了其前身。它具有易于访问的用户界面层，该层支持分离的Python交互，静止API和对话命令。除此之外，它包含一个优化的核心运行时层，可在不同的数据集量表，处理需求和计算环境中进行自适应执行和管理，同时屏蔽不必要的系统详细信息。广泛的经验评估证明了数据Juicer 2.0的出色性能和可伸缩性，强调了其有效处理数以万计具有数以万计的数据样本的能力。该系统可公开可用，积极维护并广泛采用，并在多样化的研究努力，实际应用和现实世界中（例如阿里巴巴云PAI）公开使用。

### AI-Driven Health Monitoring of Distributed Computing Architecture: Insights from XGBoost and SHAP 
[[arxiv](https://arxiv.org/abs/2501.14745)] [[cool](https://papers.cool/arxiv/2501.14745)] [[pdf](https://arxiv.org/pdf/2501.14745)]
> **Authors**: Xiaoxuan Sun,Yue Yao,Xiaoye Wang,Pochun Li,Xuan Li
> **First submission**: 2024-12-16
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: AI驱动的分布式计算体系结构的健康监测：XGBoost和Shap的见解
- **领域**: 分布式、并行和集群计算,机器学习
- **摘要**: 随着人工智能技术的快速发展，其在复杂计算机系统的优化中的应用变得越来越广泛。边缘计算是一种有效的分布式计算体系结构，其节点的健康状况直接影响整个系统的性能和可靠性。鉴于在节点健康状况判断中缺乏传统方法的准确性和可解释性，本文提出了基于XGBoost的健康状况判断方法，并结合了Shap方法来分析模型的可解释性。通过实验，可以验证XGBoost在处理复杂功能和边缘计算节点的非线性数据方面具有较高的性能，尤其是在捕获关键特征（例如响应时间和功耗）对节点状态的影响时。 Shap值分析进一步揭示了特征的全球和局部重要性，因此该模型不仅具有高精度歧视能力，而且可以提供直观的解释，从而为系统优化提供数据支持。研究表明，AI技术和计算机系统优化的组合不仅可以实现对边缘计算节点健康状况的智能监控，而且还为动态优化计划，资源管理和异常检测提供了科学基础。将来，随着AI技术的深入开发，模型动态，跨节点协作优化和多模式数据融合将成为研究的重点，为边缘计算系统的智能演变提供重要的支持。

## 人机交互(cs.HC:Human-Computer Interaction)

该领域共有 7 篇论文

### A Metasemantic-Metapragmatic Framework for Taxonomizing Multimodal Communicative Alignment 
[[arxiv](https://arxiv.org/abs/2501.01535)] [[cool](https://papers.cool/arxiv/2501.01535)] [[pdf](https://arxiv.org/pdf/2501.01535)]
> **Authors**: Eugene Yu Ji
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: 34 pages, 1 figure, 3 tables. Draft presented at 2023 ZJU Logic and AI Summit EAI Workshop
- **标题**: 分类学多模式交流对准分类法的元语义 - 词法框架
- **领域**: 人机交互,人工智能,计算语言学,计算机与社会
- **摘要**: 本文利用当代实用主义哲学和语言理论，关于认知，含义和交流，提出了一种动态的，元语言的超声分类法，用于接地，并概念化类似人类的多模式交流一致性。该框架源于美国逻辑学家和实用主义哲学家查尔斯·桑德斯·皮尔斯（Charles Sanders Peirce）最初确定的三个基本交流能力的当代发展：标志性（感觉和感知质量），索引（上下文和社会文化关联），以及统一的（象征性和直觉的理由）。为了扩展这些发展，我介绍了索引上下文化的概念，并提出了“上下文化方向性”的原则，以表征维持，导航或在多模式通信的语义和务实模式之间维持，导航或过渡的关键元元素能力。我认为，当前的认知社会计算和工程方法学不成比例地强调语义/元语言领域，从而忽略了元磁索引在遍历通信语义 - 词法频谱中的关键作用。还讨论了该框架对模式内和跨模式人机对齐方式中的意图，身份，影响和道德的更广泛含义。

### LlaMADRS: Prompting Large Language Models for Interview-Based Depression Assessment 
[[arxiv](https://arxiv.org/abs/2501.03624)] [[cool](https://papers.cool/arxiv/2501.03624)] [[pdf](https://arxiv.org/pdf/2501.03624)]
> **Authors**: Gaoussou Youssouf Kebe,Jeffrey M. Girard,Einat Liebenthal,Justin Baker,Fernando De la Torre,Louis-Philippe Morency
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: :I.2.7
- **标题**: Llamadrs：促使大型语言模型用于基于面试的抑郁评估
- **领域**: 人机交互,计算语言学
- **摘要**: 这项研究介绍了Llamadrs，这是一个利用开源大语模型（LLMS）的新型框架，以使用Montgomery-Asberg抑郁量表（MADRS）自动化抑郁症的严重程度评估。我们采用零拍的促使策略和精心设计的提示来指导模型解释和评分转录的临床访谈。我们的方法对来自上下文自适应多模式信息学（CAMI）数据集的236次现实世界访谈进行了测试，证明了与临床医生评估的密切相关性。 QWEN 2.5--72B模型在大多数MADR项目中达到了接近人类的一致性，具有内部相关系数（ICC）与人类评估者之间的QWEN级别达成了相关系数（ICC）。我们对不同MADR项目的模型性能进行了全面的分析，突出了优势和当前的局限性。我们的发现表明，通过适当的提示，LLM可以作为心理健康评估的有效工具，可能会增加资源有限的设置的可访问性。但是，挑战仍然存在，特别是在评估依赖非语言提示的症状时，强调了未来工作中多模式方法的需求。

### Eye Gaze as a Signal for Conveying User Attention in Contextual AI Systems 
[[arxiv](https://arxiv.org/abs/2501.13878)] [[cool](https://papers.cool/arxiv/2501.13878)] [[pdf](https://arxiv.org/pdf/2501.13878)]
> **Authors**: Ethan Wilson,Naveen Sendhilnathan,Charlie S. Burlingham,Yusuf Mansour,Robert Cavin,Sai Deep Tetali,Ajoy Savio Fernandes,Michael J. Proulx
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 眼目光作为在上下文AI系统中传达用户关注的信号
- **领域**: 人机交互,计算机视觉和模式识别
- **摘要**: 高级多模式AI代理现在可以与用户合作解决世界上的挑战。我们探索眼睛跟踪在这种互动中的作用，以传达用户相对于物理环境的注意力。我们假设这些知识可以提高对AI代理的上下文理解。通过观察人类对象相互作用的小时，我们首先测量了眼睛跟踪器的信号质量与可靠地将目光注视在附近物理物体上的能力之间的关系。然后，我们进行实验，将用户的扫描历史记录作为其他上下文查询多模式代理。我们的结果表明，眼睛跟踪作为用户注意力信号提供了很高的价值，并且可以向代理传达有关用户当前任务和兴趣的信息。

### Explainable XR: Understanding User Behaviors of XR Environments using LLM-assisted Analytics Framework 
[[arxiv](https://arxiv.org/abs/2501.13778)] [[cool](https://papers.cool/arxiv/2501.13778)] [[pdf](https://arxiv.org/pdf/2501.13778)]
> **Authors**: Yoonsang Kim,Zainab Aamir,Mithilesh Singh,Saeed Boorboor,Klaus Mueller,Arie E. Kaufman
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: 11 pages, 8 figures. This is the author's version of the article that has been accepted for publication in IEEE Transactions on Visualization and Computer Graphics
- **标题**: 可解释的XR：使用LLM辅助分析框架了解XR环境的用户行为
- **领域**: 人机交互,计算语言学
- **摘要**: 我们提出了可解释的XR，这是一个端到端框架，用于通过利用大型语言模型（LLM）来分析不同扩展现实（XR）环境中的用户行为。现有的XR用户分析框架在处理跨虚拟性方面面临挑战-AR，VR，MR-过渡，多用户协作应用程序方案以及多模式数据的复杂性。可解释的XR通过为沉浸式会议的收集，分析和可视化提供虚拟性 - 敏捷的解决方案来解决这些挑战。我们在框架中提出了三个主要组件：（1）一种新颖的用户数据记录架构，称为用户操作描述符（UAD），可以捕获用户的多模式操作以及其意图和上下文； （2）平台不可静止的XR会话记录器和（3）视觉分析接口，可为分析师的观点量身定制LLM辅助见解，从而促进对录制的XR会话数据的探索和分析。我们通过在虚拟性跨个体和协作XR应用中演示五个用例场景来证明可解释的XR的多功能性。我们的技术评估和用户研究表明，可解释的XR提供了一种高度可用的分析解决方案，用于了解用户行动并在沉浸式环境中对用户行为提供多方面的可行见解。

### Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning 
[[arxiv](https://arxiv.org/abs/2501.15727)] [[cool](https://papers.cool/arxiv/2501.15727)] [[pdf](https://arxiv.org/pdf/2501.15727)]
> **Authors**: Michael Xieyang Liu,Savvas Petridis,Vivian Tsai,Alexander J. Fiannaca,Alex Olwal,Michael Terry,Carrie J. Cai
> **First submission**: 2025-01-26
> **First announcement**: 2025-01-27
> **comment**: ef:30th International Conference on Intelligent User Interfaces (IUI'25), March 24-27, 2025, Cagliari, Italy. ACM, New York, NY, USA, 16 pages
- **标题**: Gensors：使用多模式基础模型和推理创作个性化的视觉传感器
- **领域**: 人机交互,人工智能
- **摘要**: 多模式的大型语言模型（MLLMS）及其广泛的世界知识和推理能力为最终用户提供了一个独特的机会，可以创建个性化的AI传感器，能够推理复杂情况。用户可以用自然语言描述所需的感应任务（例如，“如果我的幼儿陷入恶作剧”），MLLM在几秒钟内分析了相机提要并做出响应。在一项形成性的研究中，我们发现用户在定义自己的传感器方面看到了巨大的价值，但却努力阐明其独特的个人要求，并通过单独提示来调试传感器。为了应对这些挑战，我们开发了Gensors，该系统使用户能够定义由MLLM的推理功能支持的定制传感器。 Gensors 1）通过自动生成和手动创建传感器标准来协助用户提出需求，2）通过允许用户隔离和测试并行测试个体标准来促进调试，3）提出基于用户提供的图像的其他标准，4）提出了测试案例，以帮助用户对用户“压力测试”“潜在的“潜在的” Sensors“潜在的感官” FormeSeal foreSeal foreSealsememementeriose Seneariose。在一项用户研究中，参与者报告了使用Gensor定义传感器时的控制感，理解和通信性的更大意识。除了解决模型限制外，Gensors还支持用户通过基于标准的推理对传感器表达独特的个人要求，以支持用户。它还通过公开被忽视的标准并揭示意外故障模式来帮助揭示用户的“盲点”。最后，我们讨论了MLLMS的独特特征（例如幻觉和响应不一致）如何影响传感器创造过程。这些发现有助于日常用户直观且可以定制的未来智能传感系统的设计。

### MetaDecorator: Generating Immersive Virtual Tours through Multimodality 
[[arxiv](https://arxiv.org/abs/2501.16164)] [[cool](https://papers.cool/arxiv/2501.16164)] [[pdf](https://arxiv.org/pdf/2501.16164)]
> **Authors**: Shuang Xie,Yang Liu,Jeannie S. A. Lee,Haiwei Dong
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 元代理：通过多模式生成沉浸式虚拟旅行
- **领域**: 人机交互,人工智能,新兴技术,多媒体
- **摘要**: MetAdecorator是一个框架，授权用户个性化虚拟空间。通过利用文本驱动的提示和图像合成技术，元代理装饰器装饰了由360°成像设备捕获的静态全景，将它们转变为独特的风格和视觉上吸引人的环境。与传统产品相比，这显着增强了虚拟旅行的现实主义和参与度。除了核心框架之外，我们还讨论了VR应用中大型语言模型（LLM）和触觉的整合，以提供更身临其境的体验。

### Leveraging Multimodal LLM for Inspirational User Interface Search 
[[arxiv](https://arxiv.org/abs/2501.17799)] [[cool](https://papers.cool/arxiv/2501.17799)] [[pdf](https://arxiv.org/pdf/2501.17799)]
> **Authors**: Seokhyeon Park,Yumin Song,Soohyun Lee,Jaeyoung Kim,Jinwook Seo
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '25)
- **标题**: 利用多模式LLM进行励志用户界面搜索
- **领域**: 人机交互,信息检索
- **摘要**: 鼓舞人心的搜索是探索设计以告知和激发新创意作品的过程，在移动用户界面（UI）设计中都是关键的。但是，探索UI参考的广阔空间仍然是一个挑战。现有的基于AI的UI搜索方法通常会错过关键语义，例如目标用户或应用程序的心情。此外，这些模型通常需要元数据，例如视图层次结构，从而限制其实际使用。我们使用多模式的大语言模型（MLLM）从移动UI图像中提取和解释语义。我们通过形成性研究确定了关键的UI语义，并开发了基于语义的UI搜索系统。通过计算和人类评估，我们证明了我们的方法极大地胜过现有的UI检索方法，为UI设计师提供了更丰富且相关的搜索体验。我们增强了对移动UI设计语义的理解，并突出了MLLM在鼓舞人心的搜索中的潜力，为未来的研究提供了丰富的UI语义数据集。

## 信息检索(cs.IR:Information Retrieval)

该领域共有 15 篇论文

### Quantum Cognition-Inspired EEG-based Recommendation via Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2501.02671)] [[cool](https://papers.cool/arxiv/2501.02671)] [[pdf](https://arxiv.org/pdf/2501.02671)]
> **Authors**: Jinkun Han,Wei Li,Yingshu Li,Zhipeng Cai
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: ef:CIKM '24: Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 2024
- **标题**: 基于图形神经网络的基于量子认知启发的EEG建议
- **领域**: 信息检索
- **摘要**: 当前的建议系统通过考虑用户的历史行为，社会关系，评级和其他多模式来推荐商品。尽管过时的用户信息介绍了用户兴趣的趋势，但是没有建议系统能够知道用户的实时思想。随着脑部计算机界面的开发，现在是时候探索下一代推荐人，以显示用户的实时思想毫不延迟。脑电图（EEG）是一种有前途的方法，可以收集大脑信号，因为它的便利性和活动能力。当前，由于学习人脑活动的复杂性，对基于脑电图的建议的研究只有很少的研究。为了探索基于EEG的建议的实用性，我们提出了一种新型的神经网络模型，Quark，将量子认知理论和图形卷积网络结合在一起，以进行准确的项目建议。与最先进的建议模型相比，通过广泛的实验确认了夸克的优势。

### Foundations of GenIR 
[[arxiv](https://arxiv.org/abs/2501.02842)] [[cool](https://papers.cool/arxiv/2501.02842)] [[pdf](https://arxiv.org/pdf/2501.02842)]
> **Authors**: Qingyao Ai,Jingtao Zhan,Yiqun Liu
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: Chapter 2 of the book on Information Access in the Era of Generative AI
- **标题**: 精灵的基础
- **领域**: 信息检索,机器学习
- **摘要**: 本章讨论了现代生成AI模型对信息访问（IA）系统的基本影响。与传统的AI相反，生成AI模型的大规模培训和出色的数据建模使它们能够产生高质量的类似人类的响应，从而为IA范式的发展带来了全新的机会。在本章中，我们详细介绍并介绍了其中两个，即信息生成和信息综合。信息生成允许AI直接创建量身定制的内容，以直接满足用户需求，从而通过立即，相关的输出来增强用户体验。信息综合利用生成AI整合和重组现有信息的能力，提供基础的响应并减轻模型幻觉等问题，这在需要精确和外部知识的情况下特别有价值。本章深入研究了生成模型的基本方面，包括体系结构，缩放和培训，并在多模式场景中讨论了它们的应用。此外，它还研究了检索增强的生成范式和其他用于语料库建模和理解的方法，并证明了生成AI如何增强信息访问系统。它还总结了未来研究的潜在挑战和富有成果的方向。

### Comparison of Feature Learning Methods for Metadata Extraction from PDF Scholarly Documents 
[[arxiv](https://arxiv.org/abs/2501.05082)] [[cool](https://papers.cool/arxiv/2501.05082)] [[pdf](https://arxiv.org/pdf/2501.05082)]
> **Authors**: Zeyd Boukhers,Cong Yang
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 从PDF学术文档中提取元数据的特征学习方法的比较
- **领域**: 信息检索,计算语言学,数字图书馆,机器学习
- **摘要**: 元数据在科学文档中的可用性对于推动科学知识向前并遵守研究发现的公平原则（即可发现性，可访问性，互操作性和可重复性）至关重要。但是，已发表的文件中缺乏足够的元数据，尤其是来自较小和中型出版商的文档，这阻碍了他们的可访问性。这个问题在某些学科中很普遍，例如德国社会科学，出版物经常采用不同的模板。为了应对这一挑战，我们的研究评估了各种特征学习和预测方法，包括自然语言处理（NLP），计算机视觉（CV）和多模式方法，用于从具有高模板差异的文档中提取元数据。我们旨在提高科学文档的可访问性并促进其更广泛的使用。为了支持我们对这些方法的比较，我们提供了全面的实验结果，分析了它们在提取元数据方面的准确性和效率。此外，我们还提供了有关各种特征学习和预测方法的优势和缺点的宝贵见解，这些方法可以指导该领域的未来研究。

### Multimodal semantic retrieval for product search 
[[arxiv](https://arxiv.org/abs/2501.07365)] [[cool](https://papers.cool/arxiv/2501.07365)] [[pdf](https://arxiv.org/pdf/2501.07365)]
> **Authors**: Dong Liu,Esther Lopez Ramos
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Accepted at EReL@MIR WWW 2025
- **标题**: 用于产品搜索的多模式语义检索
- **领域**: 信息检索,机器学习
- **摘要**: 基于文本数据的语义检索（也称为密集检索）已针对Web搜索和产品搜索应用程序字段进行了广泛的研究，其中查询和潜在目标文档的相关性是由其密集的向量表示比较计算的。产品形象对于电子商务搜索交互至关重要，并且是产品探索客户的关键因素。但是，它对语义检索的影响尚未得到很好的研究。在这项研究中，我们在电子商务搜索中为产品项目构建了一个多模式表示，与产品的纯文本表示形成对比，并研究此类表示的影响。这些模型是在电子商务数据集上开发和评估的。我们证明，产品的多模式表示方案可以在购买召回时表现出改进或语义检索的相关性准确性。此外，我们为通过多模式语义检索模型与仅文本语义检索模型检索的独家匹配提供了数值分析，以证明对多模式解决方案的验证。

### TriMod Fusion for Multimodal Named Entity Recognition in Social Media 
[[arxiv](https://arxiv.org/abs/2501.08267)] [[cool](https://papers.cool/arxiv/2501.08267)] [[pdf](https://arxiv.org/pdf/2501.08267)]
> **Authors**: Mosab Alfaqeeh
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: Accepted at CASCON
- **标题**: 在社交媒体中，多模式命名实体识别的Trimod Fusion
- **领域**: 信息检索,社交和信息网络
- **摘要**: 社交媒体平台是用户生成的内容的宝贵来源，为人类行为的各个方面提供了见解。命名实体识别（NER）通过识别和将命名实体分为预定义的类而在分析此类内容中起着至关重要的作用。但是，传统的NER模型经常在社交媒体语言的非正式，上下文稀疏和模棱两可的本质上挣扎。为了应对这些挑战，最近的研究集中在多模式的方法上，这些方法利用文本和视觉提示来增强实体识别。尽管有进步，但现有方法在捕获视觉对象和文本实体之间细微的映射方面面临限制，并解决模式之间的分布差异。在本文中，我们提出了一种新颖的方法，该方法将文本，视觉和主题标签特征（Trimod）整合在一起，利用变压器注意来进行有效的模态融合。我们的模型所展示的改进表明，命名实体可以从多种方式提供的辅助上下文中受益匪浅，从而更准确地识别。通过在多模式社交媒体数据集上的实验，我们证明了我们的方法优于现有的最新方法，从而在精度，回忆和F1分数方面取得了重大改善。

### MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents 
[[arxiv](https://arxiv.org/abs/2501.08828)] [[cool](https://papers.cool/arxiv/2501.08828)] [[pdf](https://arxiv.org/pdf/2501.08828)]
> **Authors**: Kuicai Dong,Yujing Chang,Xin Deik Goh,Dexun Li,Ruiming Tang,Yong Liu
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: https://huggingface.co/MMDocIR
- **标题**: MMDocir：长文档的基准测试多模式检索
- **领域**: 信息检索,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式文档检索旨在识别和检索各种形式的多模式内容，例如图，表，表，图表和布局信息。尽管具有重要意义，但显然缺乏强大的基准，可以有效评估多模式文档检索中系统的性能。为了解决这一差距，这项工作介绍了一个新的基准，称为MMDocir，涵盖了两个不同的任务：页面级别和布局级检索。前者着重于在长文档中定位最相关的页面，而后者则针对特定布局的检测，比整页分析提供了更细粒度的粒度。布局可以指各种元素，例如文本段落，方程，图形，表格或图表。 MMDocir基准包括一个丰富的数据集，其中包含1,685个问题的专业注释标签和173,843个问题的引导标签，这使其成为用于推进培训和评估的多模式文档检索的关键资源。通过严格的实验，我们揭示（i）视觉检索器的表现明显优于其文本对应物，（ii）MMDocir Train set可以有效地使多模式文档检索的训练过程有效，并且（iii）在VLM-Text上利用VLM-TEXT的文本检索，比使用OCR-Text的训练更好。这些发现强调了整合多模式文档检索的视觉元素的潜在优势。

### PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents 
[[arxiv](https://arxiv.org/abs/2501.11233)] [[cool](https://papers.cool/arxiv/2501.11233)] [[pdf](https://arxiv.org/pdf/2501.11233)]
> **Authors**: Kanika Goswami,Puneet Mathur,Ryan Rossi,Franck Dernoncourt
> **First submission**: 2025-01-19
> **First announcement**: 2025-01-20
> **comment**: Accepted at ECIR 2025
- **标题**: PLOTEDIT：通过多模式LLM代理在PDF中进行自然语言驱动的可访问图表编辑
- **领域**: 信息检索,计算语言学,多代理系统
- **摘要**: 图表可视化虽然对于数据解释和通信必不可少，但主要仅作为PDF中的图像可以访问，缺少源数据表和风格信息。为了实现PDFS或数字扫描中图表的有效编辑，我们介绍了Plotedit，这是一种通过自我反射LLM代理编辑自然语言驱动的端到端图表图像的新型多代理框架。绘图编排五个LLM代理：（1）可用于数据表提取的图表2，（2）样式属性识别的图表2VISION，（3）用于检索渲染代码的Chart2Code，（4）指令分解代理，用于将用户请求解析为可执行的访问量表，以及（5）通过实施型号的组件组合的组合构图 - 忠诚。 PlotEdit跨样式，布局，格式和以数据为中心的编辑在Chartcraft数据集上的现有基线优于现有基线，增强了视觉挑战用户的可访问性并提高了新手生产率。

### Generating with Fairness: A Modality-Diffused Counterfactual Framework for Incomplete Multimodal Recommendations 
[[arxiv](https://arxiv.org/abs/2501.11916)] [[cool](https://papers.cool/arxiv/2501.11916)] [[pdf](https://arxiv.org/pdf/2501.11916)]
> **Authors**: Jin Li,Shoujin Wang,Qi Zhang,Shui Yu,Fang Chen
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: Accepted by WWW 2025
- **标题**: 公平生成：一个不完整的多模式建议的模态 - 模态反事实框架
- **领域**: 信息检索
- **摘要**: 不完整的情况是多模式建议（MMREC）中普遍，实用但具有挑战性的环境，由于各种因素，某些项目方式缺失了。最近，通过探索不完整数据的通用结构来提高建议准确性的一些努力。但是，两个显着的差距持续存在：1）由于捕获模态分布的能力有限，难以准确生成丢失的数据； 2）关键但被忽视的可见性偏差，由于项目的多模式数据优先于用户偏好对齐，因此更可能忽略具有丢失模式的项目。这种偏见引起了人们对项目公平处理的严重关注。为了弥合这两个差距，我们提出了一个新型的模态 - 局部反事实（MODICF）框架，以解决不完整的多模式建议。 MODICF具有两个关键模块：一种新型的模态扩散数据完成模块和一个新的反事实多模式推荐模块。前者配备了一个特别设计的多模式生成框架，可以准确生成和迭代地完善来自学习方式特定模式的分布空间中缺少的数据。后者以因果的角度为基础，有效地减轻了可见性偏见的负因果作用，因此可以确保建议中的公平性。这两个模块都合作解决了上述两个显着差距，以产生更准确，更公平的结果。在三个现实世界数据集上进行的广泛实验证明了MODICF在建议准确性和公平性方面的出色性能。代码和处理的数据集将在https://github.com/jinli-i/modicf上发布。

### Exploring GPT's Ability as a Judge in Music Understanding 
[[arxiv](https://arxiv.org/abs/2501.13261)] [[cool](https://papers.cool/arxiv/2501.13261)] [[pdf](https://arxiv.org/pdf/2501.13261)]
> **Authors**: Kun Fang,Ziyu Wang,Gus Xia,Ichiro Fujinaga
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: No comments
- **标题**: 探索GPT作为音乐理解的法官的能力
- **领域**: 信息检索,声音,音频和语音处理
- **摘要**: 基于文本的大语言模型（LLM）的最新进展及其处理多模式感觉数据的扩展能力使我们探索了他们在解决音乐信息检索（MIR）挑战方面的适用性。在本文中，我们使用系统的及时工程方法来解决MIR问题。我们将音乐数据转换为符号输入，并评估LLM在三个关键MIR任务中检测注释错误的能力：BEAT跟踪，和弦提取和关键估计。提出了一种概念增强方法，以评估LLMS的音乐推理与提示中提供的音乐概念的一致性。我们的实验测试了生成预训练的变压器（GPT）的miR能力。结果表明，GPT的错误检测准确性分别为65.20％，64.80％和59.72％的Beat跟踪，和弦提取和关键估计任务，均超过随机基线。此外，我们观察到GPT的错误找到准确性与所提供的概念信息的量之间存在正相关。基于符号音乐输入的当前发现为将来的基于LLM的MIR研究提供了坚实的基础。

### Unveiling the Potential of Multimodal Retrieval Augmented Generation with Planning 
[[arxiv](https://arxiv.org/abs/2501.15470)] [[cool](https://papers.cool/arxiv/2501.15470)] [[pdf](https://arxiv.org/pdf/2501.15470)]
> **Authors**: Xiaohan Yu,Zhihan Yang,Chong Chen
> **First submission**: 2025-01-26
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 通过计划揭示了多模式检索增强产生的潜力
- **领域**: 信息检索,多代理系统
- **摘要**: 多模式检索增强生成（MRAG）系统，同时有望增强多模式大型语言模型（MLLM），但通常依赖于刚性的单步检索方法。这种限制阻碍了他们有效解决需要自适应信息获取和查询改进的现实情况的能力。为了克服这一点，我们介绍了多模式检索增强生成计划（MRAG计划）的新任务，重点是优化MLLM性能，同时最大程度地减少计算开销。我们提出了Cogplanner，这是一个灵感来自人类认知过程的多功能框架。 Cogplanner迭代地完善查询并选择检索策略，从而实现并行和顺序建模方法。为了严格评估MRAG计划，我们介绍了Cogbench，这是一种专门为此任务设计的新基准。 Cogbench促进了将轻量级齿轮与资源有效的MLLM的集成。我们的实验发现表明，Cogplanner超过了现有的MRAG基准，通过最小的计算开销，可以显着提高准确性和效率。

### Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented Representations 
[[arxiv](https://arxiv.org/abs/2501.15379)] [[cool](https://papers.cool/arxiv/2501.15379)] [[pdf](https://arxiv.org/pdf/2501.15379)]
> **Authors**: Zijun Long,Kangheng Liang,Gerardo Aragon-Camarasa,Richard Mccreadie,Paul Henderson
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 通过扩散表述零拍摄的交互式文本对图像检索
- **领域**: 信息检索,人工智能,计算机视觉和模式识别
- **摘要**: 交互式文本对图像检索（I-TIR）已成为一种用于电子商务和教育等领域应用程序的可变性用户交互工具。然而，当前的方法论主要取决于鉴定的多模式大型语言模型（MLLMS），该模型面临两个关键局限性：（1）填充施加了过度的计算开销和长期维护成本。 （2）填充范围缩小了MLLM的知识分子的知识分布，从而降低了它们对新型情景的适应性。这些问题因现实世界I-TIR系统的固有动态性质而加剧，在该系统中，查询和图像数据库的复杂性和多样性会发展，通常偏离静态培训分布。为了克服这些约束，我们建议扩散增强检索（DAR），这是一个范式移动框架，完全绕开了MLLM Finetuning。 DAR协同化大型语言模型（LLM）指导的查询细化，基于扩散模型（DM）的视觉合成，以创建上下文丰富的中间表示。这种双模式方法可以更整体地解释了用户的细微差别，从而在文本查询和视觉相关图像之间进行了精确的对齐。在四个基准测试中进行严格的评估揭示了DAR的双重优势：（1）在直接查询的情况下，在没有特定于任务的培训的情况下与最先进的I-TIR模型匹配。 （2）可扩展的概括：在多转交谈的复杂性下，命中率@10（前十大精度）超过了7.61％的命名基线，这表明对复杂的，分布偏移的相互作用的鲁棒性。通过消除鉴定依赖性并利用生成增强的表示形式，DAR为有效，自适应和可扩展的跨模式检索系统建立了新的轨迹。

### Generating Negative Samples for Multi-Modal Recommendation 
[[arxiv](https://arxiv.org/abs/2501.15183)] [[cool](https://papers.cool/arxiv/2501.15183)] [[pdf](https://arxiv.org/pdf/2501.15183)]
> **Authors**: Yanbiao Ji,Yue Ding,Dan Luo,Chang Liu,Jing Tong,Shaokai Wu,Hongtao Lu
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 为多模式推荐生成负样品
- **领域**: 信息检索
- **摘要**: 多模式推荐系统（MMR）由于能够利用各种模式的信息来增强建议质量的能力，因此受到了极大的关注。但是，现有的负面抽样技术通常难以有效利用多模式数据，从而导致次优性能。在本文中，我们确定了MMR负面采样的两个关键挑战：（1）产生与正样本对比的凝聚力负样品，以及（2）在不同方式之间保持平衡影响。为了应对这些挑战，我们提出了Neggen，这是一个新颖的框架，利用多模式的大语言模型（MLLM）来生成平衡和对比的负面样本。我们设计了三个不同的提示模板，以使Neggen能够在多种模态上分析和操纵项目属性，然后生成负面样本，以引入更好的监督信号并确保模态平衡。此外，Neggen采用因果学习模块来解开介入的关键特征和无关的项目属性的效果，从而可以对用户偏好进行细粒度学习。对现实世界数据集的广泛实验表明，与最先进的方法相比，在负抽样和多模式建议中，neggen的表现出色。

### Multi-Modality Transformer for E-Commerce: Inferring User Purchase Intention to Bridge the Query-Product Gap 
[[arxiv](https://arxiv.org/abs/2501.14826)] [[cool](https://papers.cool/arxiv/2501.14826)] [[pdf](https://arxiv.org/pdf/2501.14826)]
> **Authors**: Srivatsa Mallapragada,Ying Xie,Varsha Rani Chawan,Zeyad Hailat,Yuanbo Wang
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-27
> **comment**: Published in IEEE Big Data Conference 2024, Washington DC
- **标题**: 电子商务的多模式变压器：推断用户购买意图以弥合查询产品差距
- **领域**: 信息检索,人工智能,机器学习
- **摘要**: 电子商务点击流数据和产品目录提供关键的用户行为洞察力和产品知识。本文提出了一种称为Pincer的多模式变压器，它利用上述数据源将初始用户查询转换为伪产品表示。通过利用这些外部数据源，我们的模型可以从有限的查询中推断出用户的潜在购买意图，并捕获查询相关的产品功能。我们在受控实验和现实世界中的电子商务在线检索上展示了模型优于最先进的替代方案。我们的消融研究证实，拟议的变压器体系结构和集成学习策略使挖掘关键数据源可以推断购买意图，提取产品功能，并增强从查询到更准确的伪产品表示的转换管道。

### Handling Heterophily in Recommender Systems with Wavelet Hypergraph Diffusion 
[[arxiv](https://arxiv.org/abs/2501.14399)] [[cool](https://papers.cool/arxiv/2501.14399)] [[pdf](https://arxiv.org/pdf/2501.14399)]
> **Authors**: Darnbi Sakong,Thanh Tam Nguyen
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 在具有小波超图扩散的推荐系统中处理异质
- **领域**: 信息检索,人工智能,数据库,机器学习,社交和信息网络
- **摘要**: 推荐系统在提供各个域的个性化用户体验方面至关重要。但是，捕获异性模式和用户项目相互作用的多维性质带来了重大挑战。为了解决这个问题，我们介绍了FWHDNN（基于Fusion的小波HyperGraph扩散神经网络），这是一个创新的框架，旨在在基于HyperGraph的推荐任务中推进表示学习。 The model incorporates three key components: (1) a cross-difference relation encoder leveraging heterophily-aware hypergraph diffusion to adapt message-passing for diverse class labels, (2) a multi-level cluster-wise encoder employing wavelet transform-b​​ased hypergraph neural network layers to capture multi-scale topological relationships, and (3) an integrated multi-modal fusion mechanism that combines structural and textual通过中间和后期融合策略信息。在现实世界数据集上进行的广泛实验表明，FWHDNN在捕获用户和项目之间的高阶互连方面的准确性，鲁棒性和可扩展性都超过了最先进的方法。

### Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2501.14269)] [[cool](https://papers.cool/arxiv/2501.14269)] [[pdf](https://arxiv.org/pdf/2501.14269)]
> **Authors**: Shengzhe Zhang,Liyi Chen,Dazhong Shen,Chao Wang,Hui Xiong
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: Accepted to WWW 2025
- **标题**: 多模式顺序推荐专家的分层时间感知混合物
- **领域**: 信息检索,人工智能
- **摘要**: 多模式顺序推荐（SR）比传统的SR方法利用多模式数据来学习更多的全面项目功能和用户偏好，这已成为学术界和行业的关键话题。现有方法通常着重于通过自适应模态融合来增强多模式信息实用程序，以捕获用户互动序列从用户偏好演变的发展。但是，其中大多数忽略了由丰富的多模式数据中包含的冗余利益 - 息肉信息引起的干扰。此外，它们主要依赖于仅基于按时间顺序排列的隐性时间信息，而忽略了可以更有效地代表动态用户兴趣的明确时间信号。为了解决这些局限性，我们提出了专家的分层时间感知混合物，用于多模式顺序推荐（HM4S​​R），以及两级专家（MOE）和多任务学习策略的混合物。具体而言，第一个媒体（命名为Interactive Moe）从每个项目的多模式数据中提取了与用户兴趣相关的基本信息。然后，第二届MUE称为“暂时教育部”，通过在模态编码中引入时间戳的显式时间嵌入来捕获用户动态兴趣。为了进一步解决数据稀疏性，我们提出了三个辅助监督任务：序列级别类别预测（CP），用于项目特征理解，ID上的对比度学习（IDCL），以使序列上下文与用户兴趣和占位符对比度学习（PCL）相结合，以将时间信息与动态兴趣模型的模态整合到时间上。与几种最新方法相比，在四个公共数据集上进行了广泛的实验验证了HM4SR的有效性。

## 机器学习(cs.LG:Machine Learning)

该领域共有 61 篇论文

### DiagrammaticLearning: A Graphical Language for Compositional Training Regimes 
[[arxiv](https://arxiv.org/abs/2501.01515)] [[cool](https://papers.cool/arxiv/2501.01515)] [[pdf](https://arxiv.org/pdf/2501.01515)]
> **Authors**: Mason Lary,Richard Samuelson,Alexander Wilentz,Alina Zare,Matthew Klawonn,James P. Fairbanks
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 图表学习：一种用于组成训练制度的图形语言
- **领域**: 机器学习,人工智能,编程语言,范畴论
- **摘要**: 由具有多个相互作用但不同模型组件的深度学习制度的动机，我们介绍了学习图，图形描绘培训设置，这些设置将参数化学习作为数据而不是代码。学习图将训练组件模型的独特损耗函数编译为唯一的损失函数。 The result of training on this loss is a collection of models whose predictions ``agree" with one another. We show that a number of popular learning setups such as few-shot multi-task learning, knowledge distillation, and multi-modal learning can be depicted as learning diagrams. We further implement learning diagrams in a library that allows users to build diagrams of PyTorch and Flux.jl models. By implementing some classic machine learning use cases, we demonstrate how learning图表使从业者可以将复杂的模型作为较小组件的组成，确定工作流程之间的关系，并在培训期间或之后操纵模型。

### Balance-aware Sequence Sampling Makes Multi-modal Learning Better 
[[arxiv](https://arxiv.org/abs/2501.01470)] [[cool](https://papers.cool/arxiv/2501.01470)] [[pdf](https://arxiv.org/pdf/2501.01470)]
> **Authors**: Zhi-Hao Guan
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 平衡感知序列采样使多模式学习更好
- **领域**: 机器学习,人工智能
- **摘要**: 为了解决数据异质性引起的模式失衡，现有的多模式学习（MML）方法主要侧重于从优化目标的角度平衡这种差异。但是，几乎所有现有的方法都忽略了样本序列的影响，即，不适当的训练顺序倾向于触发模型中的学习偏见，从而进一步加剧了模式失衡。在本文中，我们提出了平衡感知序列采样（BSS），以增强MML的鲁棒性。具体而言，我们首先定义一个多角度测量器来评估每个样本的平衡度。通过评估，我们采用了基于课程学习（CL）的启发式调度程序，该调度程序逐渐提供培训子集，从平衡到不平衡的样本到重新平衡MML。此外，考虑到样本平衡可能会随着模型能力的增加而发展，我们提出了一种基于学习的概率抽样方法，以动态更新时期的训练序列，从而进一步提高MML性能。与最新的MML方法相比，广泛使用数据集的广泛实验证明了我们方法的优势。

### TabTreeFormer: Tabular Data Generation Using Hybrid Tree-Transformer 
[[arxiv](https://arxiv.org/abs/2501.01216)] [[cool](https://papers.cool/arxiv/2501.01216)] [[pdf](https://arxiv.org/pdf/2501.01216)]
> **Authors**: Jiayu Li,Bingyin Zhao,Zilong Zhao,Kevin Yee,Uzair Javaid,Biplab Sikdar
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: TabtreeFormer：使用混合树转化器的表格数据生成
- **领域**: 机器学习
- **摘要**: 变形金刚在表格数据生成中取得了巨大的成功。但是，它们缺乏特异性的归纳偏见，这对于保留表格数据的内在特征至关重要。同时，由于二次计算复杂性，它们的可扩展性和效率差。在本文中，我们提出了TabtreeFormer，这是一种混合变压器架构，结合了基于树的模型，该模型保留了表格特异性的电感偏见，其非平滑和潜在低相关的模式与离散性和非转移不变性引起，因此增强了合成数据的保真度和实用性。此外，我们设计了一个双量化令牌仪来捕获多模式连续分布并进一步促进数值分布的学习。此外，由于表格数据的复杂性有限（例如，尺寸的语义含义），我们提出的令牌降低了词汇大小和序列长度，从而使显着的模型尺寸缩小而不牺牲变压器模型的能力。我们对10个数据集的TabtreeFormer评估了各种指标上的多个生成模型；我们的实验结果表明，TabtreeFormer实现了卓越的保真度，实用性，隐私和效率。我们的最佳模型可在基线型号的1/16中获得40％的公用事业改进。

### A Novel Diffusion Model for Pairwise Geoscience Data Generation with Unbalanced Training Dataset 
[[arxiv](https://arxiv.org/abs/2501.00941)] [[cool](https://papers.cool/arxiv/2501.00941)] [[pdf](https://arxiv.org/pdf/2501.00941)]
> **Authors**: Junhuan Yang,Yuzhou Zhang,Yi Sheng,Youzuo Lin,Lei Yang
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: Accepted at AAAI 2025. This is the preprint version. Keywords:Multi-modalgeneration, diffuison models, scientific data generation, unbalanced modalities
- **标题**: 与培训数据集不平衡的成对地球科学数据生成的新型扩散模型
- **领域**: 机器学习,计算机视觉和模式识别,地球物理学
- **摘要**: 最近，生成AI技术的出现对我们的日常生活产生了变革的影响，但其在科学应用中的应用仍处于早期阶段。数据稀缺性是数据驱动的科学计算中的主要，众所周知的障碍，因此物理引导的生成AI具有巨大的希望。在科学计算中，大多数任务研究了多种数据模式的转化，以描述物理现象，例如，信号处理中的地震成像，时间和频率在气候建模中的时间和频谱中的空间和波形；因此，高度需要多模式的成对数据生成，而不是单模式数据生成，该数据通常用于自然图像（例如面部，风景）。此外，在现实世界应用中，通常存在的方式不平衡可用数据的不平衡；例如，可以很容易地模拟地震成像中的空间数据（即速度图），但是实际缺乏现实世界地震波形。尽管最新的努力使强大的扩散模型能够生成多模式数据，但如何利用不平衡的可用数据仍不清楚。在这项工作中，我们在地下地球物理学中使用地震成像作为介绍``ub-diff''的工具，``ub-diff''，这是一种用于多模式配对科学数据生成的新型扩散模型。一个主要的创新是一个二合一的编码器网络结构，它可以确保从共同表示形式获得成对数据。然后，分散过程将使用共贴表示来生成成对数据。 OpenFWI数据集的实验结果表明，UB-DIFF在Fréchet成立距离（FID）分数和成对评估方面显着优于现有技术，表明生成可靠且有用的多模式成对数据。

### Dementia Detection using Multi-modal Methods on Audio Data 
[[arxiv](https://arxiv.org/abs/2501.00465)] [[cool](https://papers.cool/arxiv/2501.00465)] [[pdf](https://arxiv.org/pdf/2501.00465)]
> **Authors**: Saugat Kannojia,Anirudh Praveen,Danish Vasdev,Saket Nandedkar,Divyansh Mittal,Sarthak Kalankar,Shaurya Johari,Vipul Arora
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: 4 pages
- **标题**: 在音频数据上使用多模式方法检测痴呆症检测
- **领域**: 机器学习
- **摘要**: 痴呆症是一种神经退行性疾病，会导致逐渐认知障碍，在世界上非常普遍，每年都会进行大量研究以预防和治愈它。这严重影响了患者记住事件并清晰沟通的能力，其中大多数变异都没有已知的治愈方法，但是早期发现可以帮助减轻症状恶化。痴呆症的主要症状之一是难以通过语音表达思想。本文试图谈论开发的模型，以通过患者的录音来预测疾病的发作。开发了一种基于ASR的模型，该模型使用耳语模型从音频文件中生成转录本，然后应用Roberta回归模型来为患者生成MMSE评分。该分数可用于预测患者的认知能力受到影响的程度。我们将Process_V1数据集用于此任务，这是通过流程Grand Challenge 2025引入的。该模型的RMSE得分为2.6911，比所述基线低约10％。

### RealDiffFusionNet: Neural Controlled Differential Equation Informed Multi-Head Attention Fusion Networks for Disease Progression Modeling Using Real-World Data 
[[arxiv](https://arxiv.org/abs/2501.02025)] [[cool](https://papers.cool/arxiv/2501.02025)] [[pdf](https://arxiv.org/pdf/2501.02025)]
> **Authors**: Aashish Cheruvu,Nathaniel Rigoni
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: RealDiffusionNet：使用现实世界数据的神经控制微分方程知情的多头注意融合网络，用于疾病进展建模
- **领域**: 机器学习,计算机视觉和模式识别,定量方法
- **摘要**: 本文提出了一种新型的基于深度学习的方法，名为ReaLdiffusionNet，其中包含神经控制的微分方程（神经CDE） - 时间序列模型，可在每个时间点处理不规则采样的数据 - 对不规则采样的数据（图像数据，时间不变数据等）的多头注意。长短期内存（LSTM）模型也被用作基线。使用了两种不同的数据集：来自开源成像联盟（OSIC）的数据，其中包含人口统计学和肺部功能的结构化时间序列数据，其基线CT的肺CT扫描，第二个来自阿尔茨海默氏病神经影像学计划（ADNI）的数据，其中包含一系列MRI SCANS，其中包括一系列MRI SCANS，并进行了人口统计学评估，并进行了物理评估，并进行了物理检查，并进行了研究。进行了一项消融研究，以了解CDE，多模式数据，注意力融合和插值策略对模型性能的作用。评估基线模型时，多模式数据的使用会改善神经CDE性能，并具有较低的测试RMSE。另外，多模式神经CDE的性能也优于多模式LSTM。在基于注意力的架构中，发现通过串联和直线插值融合可提高模型性能。发现所提出的RealDiffusionNet的性能比所有模型都优于（0.2570）。对于ADNI数据集，仅在结构化数据上训练的神经CDE和LSTM模型之间，测试RMSE是可比的（LSTM对0.471 vs. 0.4581 Neural-CDE）。此外，患者MRI系列的图像特征的添加导致了性能的提高，较低的RMSE（0.4372具有多模式与0.4581具有结构化数据）。 RealDiffusionNet在利用CDE和多模式数据方面表现出了有望准确预测疾病进展。

### Social Processes: Probabilistic Meta-learning for Adaptive Multiparty Interaction Forecasting 
[[arxiv](https://arxiv.org/abs/2501.01915)] [[cool](https://papers.cool/arxiv/2501.01915)] [[pdf](https://arxiv.org/pdf/2501.01915)]
> **Authors**: Augustinas Jučas,Chirag Raman
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-06
> **comment**: This is an extension paper to "Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues", by Raman et al. (arXiv:2107.13576)
- **标题**: 社会过程：自适应多方互动预测的概率元学习
- **领域**: 机器学习
- **摘要**: 在社会环境中自适应预测的人类行为是迈向实现人工通用情报的重要一步。社会预测中的大多数现有研究都集中在不重点的互动上，例如行人轨迹预测，或者是涉及单核和二元行为预测。相反，社会心理学强​​调了群体互动对理解复杂社会动态的重要性。这产生了我们在本文中解决的差距：预测小组（对话）级别的社交互动。此外，对于一个预测模型来说，能够在火车时适应看不见的群体很重要，因为即使是相同的个体在不同群体中的行为也有所不同。这突出了预测模型的必要性，以明确说明每个组的独特动态。为了实现这一目标，我们对人类行为的预测采用了元学习方法，将每个群体视为单独的元学习任务。结果，我们的方法可以预测该组内的特定行为，从而导致对看不见的群体的概括。具体而言，我们介绍了社会过程（SP）模型，该模型根据其先前的低级多模式提示，预测所有小组成员共同为未来的多模式提示进行分配，同时结合了同一小组交互的其他过去序列。在这项工作中，我们还通过使用逼真的合成数据集分析了SP模型在其输出和潜在空间中的概括能力。

### Multi-Source Urban Traffic Flow Forecasting with Drone and Loop Detector Data 
[[arxiv](https://arxiv.org/abs/2501.03492)] [[cool](https://papers.cool/arxiv/2501.03492)] [[pdf](https://arxiv.org/pdf/2501.03492)]
> **Authors**: Weijiang Xiong,Robert Fonod,Alexandre Alahi,Nikolas Geroliminis
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: 使用无人机和环探测器数据进行多源城市交通流量预测
- **领域**: 机器学习
- **摘要**: 交通预测是运输研究中的一项基本任务，但是当前研究的范围主要集中在环形检测器的单个数据模式上。最近，人工智能和无人机技术的进步使新的解决方案成为了新颖的解决方案，以实现对城市交通的有效，准确和灵活的空中观察。作为一种有希望的交通监控方法，当与现有基础架构结合使用时，无人机捕获数据可以为大型城市网络创建精确的多传感器移动性观测值。因此，本文使用无人机和循环检测器数据同时研究了多源交通速度预测的问题。提出了一种简单但有效的基于图的模型HIMSNET，以整合多种数据模式并学习时空相关性。详细的分析表明，预测准确的细分级速度比区域速度更具挑战性，尤其是在高需求的场景下，充血和交通动态的变化。利用无人机和环路检测器数据，与单模式案例相比，当传感器的覆盖范围较低并受到噪声时，预测精度可以提高。我们基于真正的城市道路网络中车辆轨迹的模拟研究突出了将无人机在交通预测和监视中集成的附加价值。

### Multi-Modal One-Shot Federated Ensemble Learning for Medical Data with Vision Large Language Model 
[[arxiv](https://arxiv.org/abs/2501.03292)] [[cool](https://papers.cool/arxiv/2501.03292)] [[pdf](https://arxiv.org/pdf/2501.03292)]
> **Authors**: Naibo Wang,Yuchen Deng,Shichen Fan,Jianwei Yin,See-Kiong Ng
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: 多模式的一击联合合奏学习医疗数据与视觉大语言模型
- **领域**: 机器学习,人工智能
- **摘要**: 联邦学习（FL）由于其促进协作模型培训的能力而在维护数据隐私的同时，引起了对医疗领域的极大兴趣。但是，常规的FL方法通常需要进行多个通信，从而导致大量的沟通开销和延误，尤其是在带宽有限的环境中。联合学习的一杆通过在单个通讯中进行模型培训和聚合来解决这些问题，从而在保留隐私的同时降低了沟通成本。其中，使用集合技术（例如投票）结合了一个独立训练的客户模型，在非IID数据方案中进一步提高了性能。另一方面，医疗保健中现有的机器学习方法主要使用单峰数据（例如医学图像或文本报告），这限制了其诊断准确性和全面性。因此，提出了多模式数据的集成来解决这些缺点。在本文中，我们介绍了FedMme，这是一种创新的单次多模式联合合奏学习框架，该框架利用多模式数据进行医学图像分析。具体而言，FedMme利用了视觉大型语言模型来从医学图像中产生文本报告，采用BERT模型从这些报告中提取文本功能，并将这些功能与视觉功能合并以提高诊断精度。实验结果表明，与四个具有各种数据分布的数据集中的医疗保健方案中现有的单次联合学习方法相比，我们的方法表现出了卓越的性能。例如，在使用（$α$ = 0.3）的dirichlet分布时，它超过了现有联合学习方法的现有单发方法的准确性超过17.5％。

### Multimodal Machine Learning Can Predict Videoconference Fluidity and Enjoyment 
[[arxiv](https://arxiv.org/abs/2501.03190)] [[cool](https://papers.cool/arxiv/2501.03190)] [[pdf](https://arxiv.org/pdf/2501.03190)]
> **Authors**: Andrew Chang,Viswadruth Akkaraju,Ray McFadden Cogliano,David Poeppel,Dustin Freeman
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: ICASSP 2025
- **标题**: 多模式的机器学习可以预测视频会议的流动性和享受
- **领域**: 机器学习,人机交互,音频和语音处理,图像和视频处理
- **摘要**: 视频会议现在是专业和非正式设置中的常见沟通方式，但通常缺乏交流的流动性和享受。这项研究利用多模式的机器学习来预测视频会议中负面经验的时刻。我们从Roomreader语料库中抽样了数千个短剪辑，从而提取音频嵌入，面部动作和身体运动功能，以训练模型，以识别低对话流动性，低享受和分类对话事件（回信，中断，中断或GAP）。我们最好的模型在举行视频会议上达到了高达0.87的ROC-AUC，域总音频功能证明最关键。这项工作表明，多模式音频视频信号可以有效预测高级主观对话结果。此外，这是对视频会议用户体验的研究的贡献，它表明可以使用多模式的机器学习来确定罕见的负面用户体验的时刻，以进行进一步的学习或缓解。

### Stable Derivative Free Gaussian Mixture Variational Inference for Bayesian Inverse Problems 
[[arxiv](https://arxiv.org/abs/2501.04259)] [[cool](https://papers.cool/arxiv/2501.04259)] [[pdf](https://arxiv.org/pdf/2501.04259)]
> **Authors**: Baojun Che,Yifan Chen,Zhenghao Huan,Daniel Zhengyu Huang,Weijie Wang
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: 25 pages, 10 figures
- **标题**: 稳定的衍生衍生物自由高斯混合物的变化推断贝叶斯反问题
- **领域**: 机器学习,数值分析
- **摘要**: 本文涉及到已知的概率分布的近似值，以期为标准化常数，重点是贝叶斯对科学计算中大规模反问题的推断。在这种情况下，关键挑战包括对远期模型的昂贵重复评估，以及对远期模型的无法访问的梯度。为了解决这些问题，我们开发了一个变分推理框架，将Fisher-Rao天然梯度与专门的正交规则相结合，以实现高斯混合物变化家庭的衍生性免费更新。所得的方法称为衍生物游离高斯混合物变化推理（DF-GMVI），保证了协方差阳性和仿射不变性，为近似复杂的后验分布提供了稳定有效的框架。 DF-GMVI的有效性通过数值实验在具有挑战性的情况下进行，包括具有多种模式的分布，无限多种模式和弯曲模式，并具有多达数百个维度的空间。该方法的实用性在大规模应用中进一步证明，在该应用程序中，它成功地从溶液数据正以正时成功恢复了Navier-Stokes方程的初始条件。

### RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance 
[[arxiv](https://arxiv.org/abs/2501.03995)] [[cool](https://papers.cool/arxiv/2501.03995)] [[pdf](https://arxiv.org/pdf/2501.03995)]
> **Authors**: Matin Mortaheb,Mohammad A. Amir Khojastepour,Srimat T. Chakradhar,Sennur Ulukus
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: 抹布检查：评估多模式检索增强发电性能
- **领域**: 机器学习,计算机视觉和模式识别,信息检索,信息论
- **摘要**: 通过外部知识指导响应产生，减少幻觉，检索增强的生成（RAG）改善了大语言模型（LLM）。但是，RAG，尤其是多模式的RAG，可以引入新的幻​​觉来源：（i）检索过程可以从数据库中选择无关的作品（例如，文档，图像）作为原始上下文，（ii）检索图像通过视觉语言模型（VLMS）或直接使用的是基于文本的上下文，或直接使用多型模型（Miay-modals）gpt-n ands whermms gpt-4。为了解决这个问题，我们提出了一个新型框架，以使用两种绩效措施评估多模式抹布的可靠性：（i）相关得分（RS），评估检索到的条目与查询的相关性，以及（ii）正确性得分（CS），评估生成响应的准确性。我们使用CHATGPT衍生的数据库和人类评估器样本训练RS和CS模型。结果表明，这两种模型在测试数据上的精度约为88％。此外，我们构建了一个5000个样本的人类通知数据库，该数据库评估了检索到的零件的相关性和响应陈述的正确性。我们的RS模型与人类偏好的一致性比检索中的夹子高20％，而我们的CS模型与人类的偏好相匹配约91％。最后，我们使用RS和CS评估各种抹布系统的选择和发电性能。

### Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series 
[[arxiv](https://arxiv.org/abs/2501.03747)] [[cool](https://papers.cool/arxiv/2501.03747)] [[pdf](https://arxiv.org/pdf/2501.03747)]
> **Authors**: Yuxiao Hu,Qian Li,Dongxiao Zhang,Jinyue Yan,Yuntian Chen
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: no comment
- **标题**: 上下文对准：激活和增强时间序列的LLM功能
- **领域**: 机器学习,计算语言学,应用领域
- **摘要**: 最近，利用预训练的大语言模型（LLMS）进行时间序列（TS）任务已引起了越来越多的关注，这涉及激活和增强LLMS的功能。许多方法旨在激活LLMS基于令牌级别对齐的功能，但忽略LLMS对自然语言处理的固有力量 - 他们对语言逻辑和结构的深刻理解，而不是浅表嵌入处理。我们提出了上下文对齐，这是一种新的范式，它与LLMS熟悉的语言环境中的TS与语言组件保持一致，以使LLMS能够上下文化和理解TS数据，从而激活其功能。具体而言，这种上下文级的对齐包括结构对齐和逻辑对齐，这是通过应用于TS语言多模态输入的双尺度上下文对准GNN（DSCA-GNNS）来实现的。结构比对利用双尺度节点来描述TS语言中的层次结构，使LLMS可以将长TS数据视为整个语言组件，同时保留了固有的令牌特征。逻辑对齐使用有向边来指导逻辑关系，从而确保上下文语义中的连贯性。 DSCA-GNNS框架之后，使用了示例示例提示来构建基于演示示例的基于示范示例的上下文对齐（DECA）。 DECA可以灵活地并反复集成到预训练的LLM的各个层中，以提高对逻辑和结构的认识，从而提高性能。广泛的实验表明了DECA的有效性以及跨任务的上下文一致性的重要性，尤其是在几次射击和零镜头的预测中，证实上下文对齐提供了关于上下文的有力的先验知识。

### A Multimodal Lightweight Approach to Fault Diagnosis of Induction Motors in High-Dimensional Dataset 
[[arxiv](https://arxiv.org/abs/2501.03746)] [[cool](https://papers.cool/arxiv/2501.03746)] [[pdf](https://arxiv.org/pdf/2501.03746)]
> **Authors**: Usman Ali
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: 高维数据集中感应电动机的故障诊断的多模式轻量级方法
- **领域**: 机器学习,信号处理,系统与控制
- **摘要**: 基于AI的准确诊断系统（IMS）具有增强主动维护，减轻计划外停机时间并在工业环境中遏制总体维护成本的潜力。值得注意的是，在IMS中普遍的故障中，经常遇到一个破裂的转子杆（BRB）故障。研究人员提出了使用信号处理（SP），机器学习（ML），深度学习（DL）和BRB断层混合体系结构的各种故障诊断方法。现有文献中的一个局限性是在相对较小的数据集上对这些架构进行培训，在工业环境中实施此类系统时有可能过度拟合。本文通过使用基于转移学习的轻巧DL模型来诊断一个，二，三和四个BRB故障，使用当前和振动信号数据来解决BRB故障的大规模数据来解决此限制。使用短时傅立叶变换（STFT）生成用于训练和测试的光谱图像。该数据集包含57,500张图像，其中47,500张用于培训和10,000张测试。值得注意的是，ShufflenETV2模型以较低的计算成本表现出卓越的性能，并且准确地分类了98.856％的光谱图像。为了进一步增强折线破裂导致的谐波边带的可视化，将快速傅立叶变换（FFT）应用于电流和振动数据。本文还提供了有关每个模型的培训和测试时间的见解，有助于对拟议的故障诊断方法进行全面的了解。我们的研究结果为不同的ML和DL模型的性能和效率提供了宝贵的见解，为在工业环境中的感应电动机开发了强大的故障诊断系统提供了基础。

### Online Continual Learning: A Systematic Literature Review of Approaches, Challenges, and Benchmarks 
[[arxiv](https://arxiv.org/abs/2501.04897)] [[cool](https://papers.cool/arxiv/2501.04897)] [[pdf](https://arxiv.org/pdf/2501.04897)]
> **Authors**: Seyed Amir Bidaki,Amir Mohammadkhah,Kiyan Rezaee,Faeze Hassani,Sadegh Eskandari,Maziar Salahi,Mohammad M. Ghassemi
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 在线持续学习：对方法，挑战和基准测试的系统文献综述
- **领域**: 机器学习
- **摘要**: 在线持续学习（OCL）是机器学习中的关键领域，重点是使模型能够实时适应数据流，同时解决诸如灾难性遗忘和稳定性 - 塑性权衡等挑战。这项研究对OCL进行了首次全面的系统文献综述（SLR），分析81种方法，提取1,000多个功能（这些方法解决的特定任务），并确定了500多个组件（包括算法和工具在内的方法中的子模型）。我们还审查了83个数据集，这些数据集涉及图像分类，对象检测和多模式视觉语言任务。我们的发现重点介绍了关键挑战，包括减少计算开销，开发域 - 不合SNOSTIC解决方案以及改善资源约束环境中的可扩展性。此外，我们确定了未来研究的有前途的方向，例如利用自我监督的学习来获取多模式和顺序数据，设计自适应记忆机制，以整合稀疏的检索和生成性重播，并为现实世界应用与嘈杂或发展的任务边界创建有效的框架。通过提供对OCL当前状态的严格和结构化的综合，本综述为推进这一领域并应对其关键挑战和机遇提供了宝贵的资源。完整的SLR方法论步骤和提取的数据可通过提供的链接公开获取：https：//github.com/kiyan-rezaee/ systematic-literature-review-review-on-online-continual-learning

### Re-ranking the Context for Multimodal Retrieval Augmented Generation 
[[arxiv](https://arxiv.org/abs/2501.04695)] [[cool](https://papers.cool/arxiv/2501.04695)] [[pdf](https://arxiv.org/pdf/2501.04695)]
> **Authors**: Matin Mortaheb,Mohammad A. Amir Khojastepour,Srimat T. Chakradhar,Sennur Ulukus
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 重新列入多模式检索增强发电的上下文
- **领域**: 机器学习,计算机视觉和模式识别,信息检索,信息论
- **摘要**: 检索增强的生成（RAG）通过合并外部知识以在上下文中以提高准确性和减少幻觉的方式产生响应来增强大语模型（LLMS）。但是，多模式的抹布系统面临着独特的挑战：（i）检索过程可能会选择无关紧要的入口（例如，图像，文档），以及（ii）视觉模型或诸如GPT-4O（例如GPT-4O）的视觉模型或多模式模型可能在处理这些入口时幻觉以生成RAG输出。在本文中，我们旨在应对第一个挑战，即，在多模式抹布的检索阶段，从知识库中改善相关上下文的选择。具体来说，我们利用了我们先前工作中设计的相关评分（RS）度量，以评估抹布性能，以在检索过程中选择更多相关条目。基于嵌入的检索，例如基于夹子的嵌入，余弦相似性通常表现不佳，尤其是在多模式数据方面。我们表明，通过使用更高级的相关性度量，可以通过从知识库中选择更相关的作品来增强检索过程，并通过自适应地选择最新的$ K $条目而不是固定数量的条目来消除上下文中的无关。我们使用可可数据集的评估表明，在选择相关的上下文和生成响应的准确性方面有显着的增强。

### A Statistical Theory of Contrastive Pre-training and Multimodal Generative AI 
[[arxiv](https://arxiv.org/abs/2501.04641)] [[cool](https://papers.cool/arxiv/2501.04641)] [[pdf](https://arxiv.org/pdf/2501.04641)]
> **Authors**: Kazusato Oko,Licong Lin,Yuhang Cai,Song Mei
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: 108 pages
- **标题**: 对比前训练和多模式生成AI的统计理论
- **领域**: 机器学习,统计理论,机器学习
- **摘要**: 多模式生成的AI系统，例如那些结合视觉和语言的系统，都依赖于对比的预训练来学习跨不同方式的表示形式。尽管他们的实际好处得到了广泛的认可，但对对比前训练框架的严格理论理解仍然有限。本文开发了一个理论框架，以解释下游任务中对比前训练的成功，例如零射击分类，条件扩散模型和视觉模型。我们介绍了近似足够统计的概念，经典的足够统计数据的概括，并表明对比的训练前损失的近距离群体大致足够，使它们适应了多样化的下游任务。我们进一步提出了图像和文本联合分布的联合生成层次模型，表明变压器可以通过信仰传播在本模型中有效地近似相关功能。在此框架的基础上，我们基于对比的预训练表示，为多模式学习提供了样本复杂性。数值模拟验证了这些理论发现，证明了在各种多模式任务中对比预训练的变压器的强烈概括性能。

### Mechanistic understanding and validation of large AI models with SemanticLens 
[[arxiv](https://arxiv.org/abs/2501.05398)] [[cool](https://papers.cool/arxiv/2501.05398)] [[pdf](https://arxiv.org/pdf/2501.05398)]
> **Authors**: Maximilian Dreyer,Jim Berend,Tobias Labarta,Johanna Vielhaben,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: 74 pages (18 pages manuscript, 7 pages references, 49 pages appendix)
- **标题**: 具有senanticlens的大型AI模型的机械理解和验证
- **领域**: 机器学习,人工智能
- **摘要**: 与人工设计的系统（例如飞机）不同，每个组件的作用和依赖关系都已充分了解，AI模型的内部运作在很大程度上是不透明的，阻碍了可验证性并破坏了信任。本文介绍了Semanticlens，这是一种神经网络的通用解释方法，该方法将组件（例如单个神经元）编码的隐藏知识映射到基础模型（例如剪辑）的语义结构的多模式空间中。在这个空间中，独特的操作变得可能是可能的，包括（i）识别编码特定概念的神经元的文本搜索，（ii）模型表示的系统分析和比较，（iii）神经元的自动标签以及对其功能角色的解释，以及（iv）审核以根据要求验证决策。完全可扩展和运行，没有人类输入，semanticlens被证明可有效调试和验证，汇总模型知识，将推理与期望保持一致（例如，在黑色素瘤分类中遵守ABCDE-RULE）以及检测与扭曲的相关性及其相关培训数据相关的组件。通过启用组件级别的理解和验证，提出的方法有助于弥合AI模型和传统工程系统之间的“信任差距”。我们在https://github.com/jim-berend/semanticlens上提供semanticlens的代码，并在https://semanticlens.hhhi-research-insights.eu上提供演示。

### DriVLM: Domain Adaptation of Vision-Language Models in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2501.05081)] [[cool](https://papers.cool/arxiv/2501.05081)] [[pdf](https://arxiv.org/pdf/2501.05081)]
> **Authors**: Xuran Zheng,Chang D. Yoo
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: DRIVLM：自动驾驶中视觉模型的域适应
- **领域**: 机器学习
- **摘要**: 近年来，大型语言模型的表现非常令人印象深刻，这在很大程度上促进了人工智能的发展和应用，并且模型的参数和性能仍在迅速增长。特别是，多模式的大语言模型（MLLM）可以结合多种模式，例如图片，视频，声音，文本等，并且在各种任务中具有巨大的潜力。但是，大多数MLLM都需要非常高的计算资源，这对大多数研究人员和开发人员来说是一个主要挑战。在本文中，我们探讨了小规模MLLM的实用性，并在自主驾驶领域应用了小规模的MLLM。我们希望这将推动MLLM在实际情况下的应用。

### Compact Bayesian Neural Networks via pruned MCMC sampling 
[[arxiv](https://arxiv.org/abs/2501.06962)] [[cool](https://papers.cool/arxiv/2501.06962)] [[pdf](https://arxiv.org/pdf/2501.06962)]
> **Authors**: Ratneel Deo,Scott Sisson,Jody M. Webster,Rohitash Chandra
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: 22 pages, 11 figures
- **标题**: 紧凑的贝叶斯神经网络通过修剪的MCMC采样
- **领域**: 机器学习,人工智能
- **摘要**: 贝叶斯神经网络（BNN）在模型预测中提供了鲁棒的不确定性量化，但是训练它们提出了重大的计算挑战。这主要是由于使用马尔可夫链蒙特卡洛（MCMC）采样和变异推理算法对多模式后分布进行采样的问题。此外，模型参数的数量呈指数尺度，并在数据集中使用其他隐藏层，神经元和功能。通常，这些密集连接的参数中很大一部分是冗余的，修剪神经网络不仅可以提高可移植性，而且还具有更好的概括能力。在这项研究中，我们通过利用网络修剪来利用MCMC采样来解决一些挑战，以获得具有冗余参数的紧凑型概率模型。我们采样了模型参数（权重和偏见）的后验分布和较低重要性的修剪权重，从而产生了紧凑的模型。我们确保紧凑的BNN保留其通过后验分布估算不确定性的能力，同时通过调整固定后的重新采样来保留模型训练和概括性能精度。我们通过经验结果分析评估了MCMC修剪策略对选定基准数据集的回归和分类问题的有效性。我们还考虑两个珊瑚礁钻核岩性分类数据集，以测试复杂的现实世界数据集中修剪模型的鲁棒性。我们进一步研究了精炼的紧凑型BNN是否可以保留任何绩效损失。我们的结果表明，使用MCMC训练和修剪BNN的可行性，同时保留泛化性能，网络大小降低了75％。这为开发紧凑型BNN模型的方式铺平了道路，该模型为现实世界应用提供了不确定性估计。

### Comparison of Autoencoders for tokenization of ASL datasets 
[[arxiv](https://arxiv.org/abs/2501.06942)] [[cool](https://papers.cool/arxiv/2501.06942)] [[pdf](https://arxiv.org/pdf/2501.06942)]
> **Authors**: Vouk Praun-Petrovic,Aadhvika Koundinya,Lavanya Prahallad
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: 9 pages, 2 tables, 4 figures
- **标题**: 比较ASL数据集的自动编码器
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 由大型语言模型（LLM）提供支持的生成AI已彻底改变了文本，音频，图像和视频的应用。这项研究重点是开发和评估美国手语（ASL）图像数据集的编码器架构，由29个手签名类别组成87,000张图像。比较了三种方法：前馈自动编码器，卷积自动编码器和扩散自动编码器。扩散自动编码器的表现优于其他自动编码器，由于其概率噪声建模和迭代性降解能力，达到了最低的平方误差（MSE）和最高的平均意见评分（MOS）。卷积自动编码器表现出有效的空间特征提取，但缺乏扩散过程的鲁棒性，而前馈自身启动自动编码器则用作基线，并在处理复杂图像数据中局限性。客观和主观评估证实了扩散自动编码器对高保真图像重建的优越性，强调了其在多模式AI应用中的潜力，例如手语识别和产生。这项工作为设计强大的编码器数据系统设计以提高多模式AI功能提供了关键的见解。

### MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome Prediction 
[[arxiv](https://arxiv.org/abs/2501.06823)] [[cool](https://papers.cool/arxiv/2501.06823)] [[pdf](https://arxiv.org/pdf/2501.06823)]
> **Authors**: Yiqing Zhang,Xiaozhong Liu,Fabricio Murai
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: Accepted and to be published in SDM2025
- **标题**: MEXA-CTP：模式专家交叉注意临床试验结果预测
- **领域**: 机器学习,人工智能,定量方法
- **摘要**: 临床试验是评估药物治疗疾病的有效性和安全性的黄金标准。鉴于药物分子的庞大设计空间，财务成本的提高以及这些试验的多年时间表，对临床试验结果预测的研究已获得了巨大的吸引力。准确的预测必须利用各种模式的数据，例如药物分子，靶疾病和资格标准来推断成功和失败。以前的深度学习方法（例如提示）通常需要合成分子的湿实验室数据和/或依靠先验知识来编码相互作用作为模型体系结构的一部分。为了解决这些局限性，我们提出了一个基于重视注意力的模型MEXA-CTP，以整合易于获取的多模式数据，并通过称为“模式专家”的专业模块来生成有效表示，同时避免在模型设计中人类的偏见。我们通过凯奇（Cauchy）损失优化了MEXA-CTP，以捕获跨模式的相关相互作用。我们对试验结果预测（TOP）基准的实验表明，与HINT相比，MEXA-CTP分别在F1得分中分别改善了现有方法，在F1分数中分别提高了11.3％，PR-AUC为12.2％和ROC-AUC的2.5％。提供消融研究以量化我们提出的方法中每个组件的有效性。

### MTPareto: A MultiModal Targeted Pareto Framework for Fake News Detection 
[[arxiv](https://arxiv.org/abs/2501.06764)] [[cool](https://papers.cool/arxiv/2501.06764)] [[pdf](https://arxiv.org/pdf/2501.06764)]
> **Authors**: Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu,Guanjun Li
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: MTPARETO：假新闻检测的多模式目标帕累托框架
- **领域**: 机器学习
- **摘要**: 多模式假新闻检测对于维持互联网多媒体信息的真实性至关重要。多模式信息的形式和内容的显着差异导致了加剧的优化冲突，阻碍了有效的模型训练，并降低了现有的融合方法对双峰的有效性。为了解决这个问题，我们建议使用目标帕累托（TPARETO）优化算法来优化多模式融合，以优化融合级别特定的客观学习算法，并以一定的重点进行了焦点。基于设计的层次融合网络，该算法定义了三个融合水平，具有相应的损失，并为每个融合量定义了所有面向模式的帕累托梯度集成。这种方法通过利用从中间融合获得的信息来为整个过程提供积极效果，从而实现了高级多模式融合。 FAKESV和FVC数据集的实验结果表明，所提出的框架的表现优于基准，而TPARETO优化算法分别提高了2.40％和1.89％的精度。

### TempoGPT: Enhancing Time Series Reasoning via Quantizing Embedding 
[[arxiv](https://arxiv.org/abs/2501.07335)] [[cool](https://papers.cool/arxiv/2501.07335)] [[pdf](https://arxiv.org/pdf/2501.07335)]
> **Authors**: Haochuan Zhang,Chunhua Yang,Jie Han,Liyang Qin,Xiaoli Wang
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: tempogpt：通过量化嵌入来增强时间序列推理
- **领域**: 机器学习,人工智能
- **摘要**: 多模式语言模型在视觉和音频方面取得了进步，但在处理时代序列域中复杂的推理任务方面仍然面临重大挑战。原因是双重的。首先，多模式时间序列数据的标签是粗糙的，没有分析或推理过程。使用这些数据培训无法提高模型的推理能力。其次，由于处理时间序列中缺乏精确的令牌化，时间和文本信息的表示模式不一致，这阻碍了多模式对齐的有效性。为了应对这些挑战，我们提出了一种多模式时间序列数据构建方法和tempogpt的多模式时间序列语言模型（TLM）。特别是，我们通过分析白框系统中的可变系统关系来构建用于复杂推理任务的多模式数据。此外，提出的tempogpt通过量化时间嵌入来实现时间和文本信息之间的一致表示，其中将时间嵌入使用预定义的代码簿中量化为一系列离散令牌；随后，共享的嵌入层处理时间和文本令牌。广泛的实验表明，Tempogpt准确地感知时间信息，从逻辑上讲结论并在构建的复杂时间序列推理任务中实现最新的结论。此外，我们定量证明了定量时间嵌入在增强多模式比对和TLM的推理能力方面的有效性。代码和数据可从https://github.com/zhanghaochuan20/tempogpt获得。

### A data-driven approach to discover and quantify systemic lupus erythematosus etiological heterogeneity from electronic health records 
[[arxiv](https://arxiv.org/abs/2501.07206)] [[cool](https://papers.cool/arxiv/2501.07206)] [[pdf](https://arxiv.org/pdf/2501.07206)]
> **Authors**: Marco Barbero Mota,John M. Still,Jorge L. Gamboa,Eric V. Strobl,Charles M. Stein,Vivian K. Kawai,Thomas A. Lasko
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: Received Runner-up Knowledge Discovery and Data Mining Innovation Award at the American Medical Informatics Association Annual Symposium 2024
- **标题**: 从电子健康记录中发现和量化系统性红斑狼疮病因异质性的数据驱动方法
- **领域**: 机器学习,应用领域
- **摘要**: 全身性红斑狼疮（SLE）是一种复杂的异质性疾病，具有许多表现方面。我们提出了一种数据驱动的方法，以从多模式不完美的EHR数据中发现概率独立来源。这些来源代表了数据生成过程中的外源变量因果图，该图估计了健康记录中SLE的潜在根本原因。我们通过训练监督模型从中发现的原始变量进行了客观评估这些源，从而使用降低的标记实例将SLE与负面的健康记录区分开来。我们发现了19个具有较高临床有效性的预测来源，其EHR签名定义了SLE异质性的独立因素。使用这些源作为输入患者数据表示，使模型能够提供丰富的解释，从而更好地捕获特定记录为SLE案例的临床原因。提供者可能愿意将患者级别的解释性交易以歧视，尤其是在具有挑战性的情况下。

### BiDepth Multimodal Neural Network: Bidirectional Depth Deep Learning Architecture for Spatial-Temporal Prediction 
[[arxiv](https://arxiv.org/abs/2501.08411)] [[cool](https://papers.cool/arxiv/2501.08411)] [[pdf](https://arxiv.org/pdf/2501.08411)]
> **Authors**: Sina Ehsani,Fenglian Pan,Qingpei Hu,Jian Liu
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: This paper has been submitted to Applied Intelligence for review
- **标题**: Bidepth多模式神经网络：双向深度深度学习架构的空间预测
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,应用领域
- **摘要**: 在动态系统（例如城市流动性和天气模式）中，准确预测空间 - 周期性（ST）信息是一个至关重要但充满挑战的问题。复杂性源于空间近端和时间相关性之间的复杂相互作用，在这种情况下，长期趋势和短期波动都以复杂的模式存在。现有方法，包括传统的统计方法和常规神经网络，可能会由于缺乏有效的机制而提供不准确的结果，该机制在保持空间环境的同时同时纳入了信息，从而在保持空间上下文的同时纳入了信息，从而在全面的长期历史分析与短期新信息中的全面长期历史分析之间进行了权衡。为了弥合这一差距，本文提出了具有双向深度调制的BIDEPTH多模式神经网络（BDMNN），以使人们可以全面了解长期的季节性和短期波动，适应复杂的ST环境。现实世界中公共数据的案例研究表明，预测准确性的显着提高，与最先进的基准测试相比，城市交通预测的平均误差的平均误差减少了12％，而雨水降水预测的提高了15％，而无需额外的计算资源。

### Free-Knots Kolmogorov-Arnold Network: On the Analysis of Spline Knots and Advancing Stability 
[[arxiv](https://arxiv.org/abs/2501.09283)] [[cool](https://papers.cool/arxiv/2501.09283)] [[pdf](https://arxiv.org/pdf/2501.09283)]
> **Authors**: Liangwewi Nathan Zheng,Wei Emma Zhang,Lin Yue,Miao Xu,Olaf Maennel,Weitong Chen
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 自由节Kolmogorov-Arnold网络：用于分析样条结和提高稳定性
- **领域**: 机器学习
- **摘要**: Kolmogorov-Arnold神经网络（KANS）在机器学习社区中引起了极大的关注。但是，他们的实施通常遭受训练稳定性差和繁重的训练参数的影响。此外，人们对从B型序列中得出的学习激活函数的行为的理解有限。在这项工作中，我们通过样条结的镜头分析了KAN的行为，并为基于B的基于B-Spline的KAN的结数提供了下层和上限。为了解决现有的限制，我们提出了一个新颖的自由结kan，可以增强原始KAN的性能，同时减少可训练参数的数量，以匹配标准多层Perceptrons（MLP）的可训练参数量表。此外，我们引入了新的A培训策略，以确保可学习的样条的$ C^2 $连续性，从而与原始KAN相比，使激活更顺畅，并通过扩展范围扩展提高了训练稳定性。该方法在涵盖各个域的8个数据集上进行了全面评估，包括图像，文本，时间序列，多模式和功能近似任务。有希望的结果表明，基于KAN的网络的可行性以及提出的方法的有效性。

### U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer Depression Detection 
[[arxiv](https://arxiv.org/abs/2501.09687)] [[cool](https://papers.cool/arxiv/2501.09687)] [[pdf](https://arxiv.org/pdf/2501.09687)]
> **Authors**: Jiaee Cheong,Aditya Bangar,Sinan Kalkan,Hatice Gunes
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: To appear at the Proceedings of Machine Learning Research 259, 1-14, 2024 as part of the Machine Learning for Health (ML4H) Symposium 2024
- **标题**: U-FAIR：基于不确定性的多模式多任务学习，以实现更公平的抑郁检测
- **领域**: 机器学习
- **摘要**: 精神健康中的机器学习偏见正成为越来越多的挑战。尽管有前途的努力表明多任务方法通常比单位施加方法更好，但仍有最少的工作调查了多任务学习对抑郁症检测中表现和公平性的影响，也没有利用它来实现更公平的预测结果。在这项工作中，我们对使用多任务方法进行系统研究来改善抑郁症检测的绩效和公平性。我们提出了一种基于性别的任务培训方法，该方法采用了基于PHQ-8问卷的结构方式的不确定性。我们的结果表明，尽管多任务方法可以提高性能和公平性，但与单位施加方法相比，结果并不总是一致的，我们看到了帕累托前沿的负转移和减少的证据，鉴于高为医疗保健设置，这与帕累托前沿相比。我们提出的基于性别的重新享用不确定性的方法可以提高绩效和公平性，并在一定程度上减轻这两种挑战。我们对每个PHQ-8次级任务难度的发现也与对PHQ-8子项目歧视能力进行的最大研究一致，因此提供了将ML发现与对PHQ-8进行的大规模经验人群研究联系起来的第一个有形证据。

### LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading 
[[arxiv](https://arxiv.org/abs/2501.09636)] [[cool](https://papers.cool/arxiv/2501.09636)] [[pdf](https://arxiv.org/pdf/2501.09636)]
> **Authors**: Kuan-Ming Liu,Ming-Chih Lo
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging Innovations in Finance, Social Media, and Crime Prevention
- **标题**: 专家混合的基于LLM的路由：交易的新型框架
- **领域**: 机器学习,交易和市场微观结构
- **摘要**: 深度学习和大型语言模型（LLM）的最新进展促进了股票投资领域中的混合物（MOE）机制的部署。尽管这些模型表现出了有希望的交易绩效，但它们通常是单峰的，忽略了其他模式中可用的信息，例如文本数据。此外，传统的基于神经网络的路由器选择机制未能考虑上下文和现实世界的细微差别，从而导致了次优的专家选择。为了解决这些局限性，我们提出了LLMOE，这是一个新颖的框架，该框架采用LLMS作为MOE架构中的路由器。具体来说，我们用LLM替换了传统的基于神经网络的路由器，利用其广泛的世界知识和推理能力来根据历史价格数据和股票新闻来选择专家。这种方法提供了一种更有效和可解释的选择机制。我们对多模式现实世界库存数据集的实验表明，LLMOE胜过最先进的MOE模型和其他深神经网络方法。此外，LLMOE的灵活体系结构可轻松适应各种下游任务。

### Multimodal Marvels of Deep Learning in Medical Diagnosis: A Comprehensive Review of COVID-19 Detection 
[[arxiv](https://arxiv.org/abs/2501.09506)] [[cool](https://papers.cool/arxiv/2501.09506)] [[pdf](https://arxiv.org/pdf/2501.09506)]
> **Authors**: Md Shofiqul Islam,Khondokar Fida Hasan,Hasibul Hossain Shajeeb,Humayan Kabir Rana,Md Saifur Rahmand,Md Munirul Hasan,AKM Azad,Ibrahim Abdullah,Mohammad Ali Moni
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: 43 pages
- **标题**: 多模式的医学诊断中深度学习的奇迹：COVID-19检测的全面评论
- **领域**: 机器学习,声音,音频和语音处理,图像和视频处理
- **摘要**: 这项研究以Covid-19的案例为例，对多模式深度学习（DL）在医学诊断中的潜力（DL）的潜力进行了全面综述。这项研究旨在揭示DL在疾病筛查，预测和分类中的能力，并获得增强抗原能力，可持续性以及科学，技术和创新系统的韧性，可持续性以及包容性。采用系统的方法，我们研究了各种研究和实施中遇到的基本方法，数据源，预处理步骤以及挑战。我们探索深度学习模型的架构，强调其数据特定的结构和基础算法。随后，我们比较了COVID-19分析中使用的不同深度学习策略，并根据方法，数据，绩效和未来研究的先决条件进行评估。通过检查多种数据类型和诊断方式，这项研究有助于科学理解和了解DL的多模式应用及其在诊断中的有效性。我们已经使用Covid-19的图像，文本和语音（即，咳嗽）数据实施并分析了11个深度学习模型。我们的分析表明，Mobilenet模型的COVID-19图像数据达到了99.97％的最高精度，对于语音数据（即咳嗽），获得了99.97％的精度。但是，BigRU模型在Covid-19文本分类中表现出卓越的性能，精度为99.89％。这项研究的更广泛的含义表明，可能利用深度学习技术来进行图像，文本和语音分析的其他领域和学科的潜在好处。

### PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal Class-Incremental Learning 
[[arxiv](https://arxiv.org/abs/2501.09352)] [[cool](https://papers.cool/arxiv/2501.09352)] [[pdf](https://arxiv.org/pdf/2501.09352)]
> **Authors**: Xianghu Yue,Yiming Chen,Xueyi Zhang,Xiaoxue Gao,Mengling Feng,Mingrui Lao,Huiping Zhuang,Haizhou Li
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: No comments
- **标题**: PAL：促使分析学习以缺失的方式用于多模式课程学习
- **领域**: 机器学习,多媒体,图像和视频处理
- **摘要**: 多模式的课程学习（MMCIL）试图利用多模式数据，例如视听和图像文本对，从而使模型能够在一系列任务中连续学习，同时减轻遗忘。尽管现有研究主要集中于MMCIL多模式信息的整合和利用，但仍然存在一个关键挑战：在增量学习阶段中缺失模式的问题。这种疏忽会加剧严重的遗忘，并严重损害模型的表现。为了弥合这一差距，我们提出了PAL，这是一个针对MMCIL量身定制的新颖的无典范框架。具体而言，我们设计了特定于模式的提示，以补偿缺失的信息，从而促进该模型维护数据的整体表示。在这个基础上，我们将MMCIL问题重新制定为递归最小二乘任务，并提供分析线性解决方案。在这些基础上，PAL不仅减轻了分析学习中固有的不合格限制，而且还保留了缺失模式数据的整体表示，从而实现了卓越的性能，而在各种多模式增量方案中却少了。广泛的实验表明，PAL在包括UPMC-Food101和N24News在内的各种数据集的竞争方法显着优于竞争方法，从而展示了其对模态缺失的稳健性及其抗遗忘能力，可维持高增量准确性。

### Diffusion-Based Imitation Learning for Social Pose Generation 
[[arxiv](https://arxiv.org/abs/2501.10869)] [[cool](https://papers.cool/arxiv/2501.10869)] [[pdf](https://arxiv.org/pdf/2501.10869)]
> **Authors**: Antonio Lech Martin-Ozimek,Isuru Jayarathne,Su Larb Mon,Jouh Yeong Chew
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: This paper was submitted as an LBR to HRI2025
- **标题**: 基于扩散的模仿学习社会姿势产生
- **领域**: 机器学习,机器人技术
- **摘要**: 智能代理人，例如机器人和虚拟代理，必须了解与人类互动的复杂社会互动的动态。有效地代表社会动态是具有挑战性的，因为我们需要多模式，同步的观测来理解场景。我们探讨了如何在社交互动中使用多个个体的单一模态，姿势行为，用于为这种互动的促进者生成非语言社交线索。主持人的行动是使社会互动顺利进行，这是智能代理在人类机器人互动中复制的重要作用。在本文中，我们调整了现有的扩散行为克隆模型来学习和复制促进者行为。此外，我们评估了一个场景中姿势观察的两种表示形式，一种表示已应用了预处理，而一个表示没有。本文的目的是引入一种新的用途，用于扩散行为，以克隆社交互动中的姿势产生。第二个是了解绩效和计算负载之间的关系，用于使用两种不同的技术来收集场景观察，从而产生社交姿势行为。因此，我们基本上正在测试扩散模型两种不同类型的调节的有效性。然后，我们使用定量度量（例如平均每关节位置误差（MPJPE），训练时间和推理时间）从每种技术中评估所得的生成行为。此外，我们将培训和推断时间与MPJPE进行了研究，以检查效率和绩效之间的权衡。我们的结果表明，进一步的预处理数据可以成功地调节扩散模型以产生现实的社会行为，并在准确性和处理时间上进行合理的权衡。

### Fake Advertisements Detection Using Automated Multimodal Learning: A Case Study for Vietnamese Real Estate Data 
[[arxiv](https://arxiv.org/abs/2501.10848)] [[cool](https://papers.cool/arxiv/2501.10848)] [[pdf](https://arxiv.org/pdf/2501.10848)]
> **Authors**: Duy Nguyen,Trung T. Nguyen,Cuong V. Nguyen
> **First submission**: 2025-01-18
> **First announcement**: 2025-01-20
> **comment**: No comments
- **标题**: 使用自动多模式学习的伪造广告检测：越南房地产数据的案例研究
- **领域**: 机器学习,人工智能
- **摘要**: 电子商务的普及引起了虚假的广告，这些广告可能会使用户面临财务和数据风险，同时损害这些电子商务平台的声誉。由于这些原因，检测和删除此类虚假广告对于电子商务网站的成功至关重要。在本文中，我们提出了一种新颖的端到端机器学习系统Fadaml，以检测和过滤伪造的在线广告。我们的系统结合了多模式机器学习和自动化机器学习中的技术，以达到高检测率。作为案例研究，我们应用FADAML来检测流行的越南房地产网站上的伪造广告。我们的实验表明，我们可以达到91.5％的检测准确性，这极大地超过了三个不同的最先进的假新闻检测系统。

### New Fashion Products Performance Forecasting: A Survey on Evolutions, Models and Emerging Trends 
[[arxiv](https://arxiv.org/abs/2501.10324)] [[cool](https://papers.cool/arxiv/2501.10324)] [[pdf](https://arxiv.org/pdf/2501.10324)]
> **Authors**: Andrea Avogaro,Luigi Capogrosso,Andrea Toaiari,Franco Fummi,Marco Cristani
> **First submission**: 2025-01-17
> **First announcement**: 2025-01-20
> **comment**: Accepted at the Springer Nature Computer Science journal
- **标题**: 新的时尚产品性能预测：一项有关演变，模型和新兴趋势的调查
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 快速时装行业对新风格和快速生产周期的需求无限的需求导致了巨大的环境负担。生产过多，浪费过多和有害化学物质导致了该行业的负面影响。为了减轻这些问题，迫切需要迫切需要将可持续性和效率优先考虑的范式转变。将基于学习的预测分析纳入时装业是应对环境挑战并推动可持续实践的重要机会。通过预测时尚趋势和优化生产，品牌可以减少其生态足迹，同时在快速变化的市场中保持竞争力。但是，预测时尚销售的主要挑战之一是消费者偏好的动态性质。时尚是周期性的，趋势不断发展和浮出水面。此外，文化变化和意外事件可能会破坏已建立的模式。这个问题也被称为新的时尚产品性能预测（NFPPF），最近它对全球研究景观引起了越来越多的兴趣。鉴于其多学科性质，NFPPF领域已从许多不同的角度接近。这项全面的调查希望提供最新的概述，重点是基于学习的NFPPF策略。该调查基于用于系统评价和荟萃分析（PRISMA）方法流的首选报告项目，从而允许进行系统和完整的文献综述。特别是，我们提出了涵盖NFPPF学习全景的第一个分类学，详细检查了用于增加多模式信息量的不同方法以及可用的最先进的可用数据集。最后，我们讨论挑战和未来的方向。

### DenoMAE: A Multimodal Autoencoder for Denoising Modulation Signals 
[[arxiv](https://arxiv.org/abs/2501.11538)] [[cool](https://papers.cool/arxiv/2501.11538)] [[pdf](https://arxiv.org/pdf/2501.11538)]
> **Authors**: Atik Faysal,Taha Boushine,Mohammad Rostami,Reihaneh Gh. Roshan,Huaxia Wang,Nikhil Muralidhar,Avimanyu Sahoo,Yu-Dong Yao
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: DENOMAE：用于降级调制信号的多模式自动编码器
- **领域**: 机器学习
- **摘要**: 我们提出了剥落的蒙面自动编码器（DENO-MAE），这是一种新型的多模式自动编码器框架，用于在训练过程中降低调制信号。 Denomae通过合并多种输入方式（包括噪声作为明确的方式）来扩展蒙版自动编码器的概念，以增强跨模式学习并改善剥离性能。该网络是使用未标记的嘈杂调制信号和星座图进行预训练的，有效地学习重建其等效的无噪声信号和图表。 Deno-Mae在自动调制分类任务中实现了最先进的精度，其培训样本明显较少，与现有方法相比，未标记的预处理数据降低了10％，标记的微调数据降低了3％。此外，我们的模型在不同的信噪比（SNR）中表现出强大的性能，并支持在看不见的下SNR上推断。结果表明，Denomee是一种有效，灵活且具有数据效率的解决方案，用于在挑战性的噪声密集度环境中降解和分类调制信号。

### A Survey on Diffusion Models for Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2501.11430)] [[cool](https://papers.cool/arxiv/2501.11430)] [[pdf](https://arxiv.org/pdf/2501.11430)]
> **Authors**: Jing Liu,Zhenchao Ma,Zepu Wang,Chenxuanyin Zou,Jiayang Ren,Zehua Wang,Liang Song,Bo Hu,Yang Liu,Victor C. M. Leung
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 一项关于分散检测扩散模型的调查
- **领域**: 机器学习,人工智能
- **摘要**: 扩散模型（DMS）已成为强大的生成AI模型类别，在各个领域的异常检测（AD）任务中显示出巨大的潜力，例如网络安全，欺诈检测，医疗保健和制造。这两个字段的相交称为异常检测（DMAD）的扩散模型，提供了有希望的解决方案，用于识别日益复杂且高维数据中的偏差。在这项调查中，我们回顾了DMAD研究的最新进展。我们首先介绍AD和DMS的基本概念，然后对包括DDPM，DDIMS和SCORE SDE在内的经典DM体系结构进行全面分析。我们将现有的DMAD方法进一步分为基于重建，基于密度和混合方法的方法，从而提供了其方法论创新的详细检查。我们还探索了不同数据模式的各种任务，包括图像，时间序列，视频和多模式数据分析。此外，我们讨论了关键的挑战和新兴的研究方向，包括计算效率，模型解释性，鲁棒性增强，边缘云协作以及与大语言模型的集成。 DMAD研究论文和资源的收集可在https://github.com/fdjingliu/dmad上获得。

### RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems? 
[[arxiv](https://arxiv.org/abs/2501.11284)] [[cool](https://papers.cool/arxiv/2501.11284)] [[pdf](https://arxiv.org/pdf/2501.11284)]
> **Authors**: Haotian Xu,Xing Wu,Weinong Wang,Zhongzhi Li,Da Zheng,Boyuan Chen,Yi Hu,Shijia Kang,Jiaming Ji,Yingying Zhang,Zhijiang Guo,Yaodong Yang,Muhan Zhang,Debing Zhang
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: technique-report, https://huggingface.co/RedStar-Reasoning
- **标题**: REDSTAR：扩展长密度数据是否解锁更好的慢速系统？
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 扩展可以变换推理吗？在这项工作中，我们探讨了将长期考虑（长期记录）数据扩展到1000k样本的尚未开发的潜力，从而开发了一个慢速思维模型Redstar的开发。通过对各种LLM和不同尺寸的广泛实验，我们发现了用于长期训练的专业化和规模的成分。令人惊讶的是，即使是较小的模型也显示出有限的数据表现出显着的性能增长，从而揭示了长期计算的样本效率以及样本难度在学习过程中的关键作用。我们的发现表明，仅几千个示例可以有效地触发长期推理，而较大的模型可以实现无与伦比的改进。我们还介绍了加强学习（RL） - 规模训练，这是推进缓慢思考系统的有希望的方向。 Redstar在跨域中闪耀：在数学基准测试中，Redstar-Code-Math将性能从66.2 \％提高到％至81.6 \％，在美国数学奥林匹克（AIME）上，仅使用21k混合代码模式数据集群解决了46.7％的问题。在诸如GEOQA和MATHVISTA-GEO之类的多模式任务中，Redstar-Geo通过最小的长期数据数据实现了竞争成果，表现优于其他缓慢思考的系统，例如QVQ-Preview。与QWQ相比，Redstar在推理和概括性之间取得了完美的平衡。我们的工作强调，通过仔细的调整，长期缩放可以解锁非凡的推理能力 - 即使是有限的数据集，并为各种挑战的慢速模型设定了新的标准。我们的数据和模型在https://huggingface.co/redstar-reasoning上发布。

### Modality Interactive Mixture-of-Experts for Fake News Detection 
[[arxiv](https://arxiv.org/abs/2501.12431)] [[cool](https://papers.cool/arxiv/2501.12431)] [[pdf](https://arxiv.org/pdf/2501.12431)]
> **Authors**: Yifan Liu,Yaokun Liu,Zelin Li,Ruichen Yao,Yang Zhang,Dong Wang
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: Accepted by the Proceedings of the ACM Web Conference 2025
- **标题**: 用于假新闻检测的互动混合物的互动混合物
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 在社交媒体平台上的假新闻的扩散会影响脆弱的人群，侵蚀信任，加剧不平等和扩大有害叙事。在多模式上下文中检测假新闻（欺骗性内容都结合了文本和图像），由于模式之间的细微相互作用，尤其具有挑战性。现有的多模式假新闻检测方法通常强调跨模式的一致性，但忽略了文本和视觉元素之间的复杂相互作用，这些元素可能会补充，矛盾或独立影响帖子的预测真实性。为了应对这些挑战，我们提出了伪造新闻检测的互动混合物（Mimoe-fnd），这是一种新型的层次结构混合物框架，旨在通过通过交互互动的互动式机制来显式地对多模式的假新闻检测来增强多模式的假新闻检测。我们的方法通过评估模式相互作用的两个关键方面来模拟模式相互作用：单峰预测一致和语义一致性。 Mimoe-FND的层次结构允许针对不同融合场景量身定制的不同学习途径，适应每种模态相互作用的独特特征。通过将融合策略定制为各种方式互动场景，Mimoe-FND为多模式假新闻检测提供了一种更强大和细微的方法。我们在三种跨越两种语言的现实基准测试的方法上评估了我们的方法，这表明了其与最新方法相比的表现出色。通过提高虚假新闻检测的准确性和解释性，Mimoe-FND提供了一种有希望的工具来减轻错误信息的传播，并有可能更好地保护弱势群体，以防止其有害影响。

### Multi-Modality Collaborative Learning for Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2501.12424)] [[cool](https://papers.cool/arxiv/2501.12424)] [[pdf](https://arxiv.org/pdf/2501.12424)]
> **Authors**: Shanmin Wang,Chengguang Liu,Qingshan Liu
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: 多模式的协作学习用于情感分析
- **领域**: 机器学习,人工智能,信息检索
- **摘要**: 多模式情感分析（MSA）通过整合视觉，音频和文本方式来识别视频中个人的情感状态。尽管在现有方法方面取得了进展，但固有的模态异质性限制了跨模态互动情感特征的有效捕获。在本文中，通过引入多模式协作学习（MMCL）框架，我们分别促进了交叉模式的交互，并分别捕获了来自模态 - 统一和模态特异性表示的增强和互补功能。具体而言，我们通过跨模式元素的语义评估，将无参数的解耦模块和单独模式分开为模态和模态特异性组件。对于特定于模式的表示，受到强化学习的行动回报机制的启发，我们设计了政策模型，以在共同奖励的指导下适应地挖掘互补情绪特征。对于模态符号表示，使用模式内关注来突出关键组成部分，在模态之间发挥了增强的作用。实验结果，包括对四个数据库的优越性评估，每个模块的有效性验证以及互补特征的评估，表明MMCL成功地学习了跨模式的协作特征，并显着提高了性能。该代码可以在https://github.com/smwanghhh/mmcl上找到。

### CroMe: Multimodal Fake News Detection using Cross-Modal Tri-Transformer and Metric Learning 
[[arxiv](https://arxiv.org/abs/2501.12422)] [[cool](https://papers.cool/arxiv/2501.12422)] [[pdf](https://arxiv.org/pdf/2501.12422)]
> **Authors**: Eunjee Choi,Junhyun Ahn,XinYu Piao,Jong-Kook Kim
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: Crome：使用跨模式的Tri-Transform和公制学习的多模式假新闻检测
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 多模式的假新闻检测最近受到了越来越多的关注。现有方法依赖于独立编码的单峰数据，并忽略了使用高级技术捕获模式内关系关系和集成模式间相似性的优势。为了解决这些问题，提出了多模式假新闻检测（Crome）的跨模式三变形和度量学习。 Crome使用冷冻图像编码器和大型语言模型（BLIP2）用作编码的引导语言图像预训练，以捕获详细的文本，图像和组合的图像文本表示。公制学习模块采用代理锚方法来捕获模式内关系关系，而特征融合模块则使用跨模式和三型转换器进行有效集成。最终的假新闻探测器通过分类器处理融合功能，以预测内容的真实性。数据集上的实验表明，Crome在多模式假新闻检测中表现出色。

### Multimodal AI on Wound Images and Clinical Notes for Home Patient Referral 
[[arxiv](https://arxiv.org/abs/2501.13247)] [[cool](https://papers.cool/arxiv/2501.13247)] [[pdf](https://arxiv.org/pdf/2501.13247)]
> **Authors**: Reza Saadati Fard,Emmanuel Agu,Palawat Busaranuvong,Deepak Kumar,Shefalika Gautam,Bengisu Tulu,Diane Strong
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: arXiv admin note: text overlap with arXiv:2208.05051 by other authors
- **标题**: 在家庭患者转诊的伤口图像和临床笔记上的多模式AI
- **领域**: 机器学习,计算机视觉和模式识别,图像和视频处理
- **摘要**: 慢性伤口影响850万美国人，尤其是老年人和糖尿病患者。这些伤口可能需要长达九个月的时间才能治愈，这是确保愈合并防止肢体截肢等严重结果的定期护理。许多患者从探视具有不同伤口专业知识水平的护士来接受护理，从而导致不一致的护理。有问题的非治疗伤口应转交给伤口专家，但是在非临床环境中的转诊决策通常是错误的，延迟或不必要的。本文介绍了深层多模式伤口评估工具（DM-WAT），该工具是一个机器学习框架，旨在协助来访护士决定是否参考慢性伤口患者。 DM-WAT分析了电子健康记录（EHRS）的智能手机捕获的伤口图像和临床笔记。它使用视觉变压器（VIT）从图像和Deberta基础中提取视觉特征来从临床注释中提取文本特征。 DM-WAT使用中间融合方法结合了视觉和文本特征。为了解决小型且不平衡的数据集提出的挑战，它将图像和文本增强与转移学习集成在一起，以实现高性能。在评估中，DM-WAT以STD的精度为77％，而STD 2％F1得分的77％均超过了先验方法。 Score-Cam和Captum解释算法提供了对影响建议的特定部分和文本输入的洞察力，从而增强了解释性和信任。

### TFG-Flow: Training-free Guidance in Multimodal Generative Flow 
[[arxiv](https://arxiv.org/abs/2501.14216)] [[cool](https://papers.cool/arxiv/2501.14216)] [[pdf](https://arxiv.org/pdf/2501.14216)]
> **Authors**: Haowei Lin,Shanda Li,Haotian Ye,Yiming Yang,Stefano Ermon,Yitao Liang,Jianzhu Ma
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: ef:ICLR 2025
- **标题**: TFG流：多模式生成流的无训练指导
- **领域**: 机器学习,人工智能,计算工程、金融和科学
- **摘要**: 鉴于无条件的生成模型和目标属性的预测因子（例如，分类器），无培训指导的目标是生成具有理想目标属性的样品，而无需其他培训。作为转向生成模型的高效技术，无训练的指导在扩散模型中引起了人们的关注。但是，现有方法仅在连续空间中处理数据，而许多科学应用程序涉及连续和离散数据（称为多模态）。另一个新出现的趋势是，在建立生成基础模型中，简单和一般的流量匹配框架的使用日益增长，在这种模型中，引导生成仍然不足。为了解决这个问题，我们介绍了TFG-Flow，这是一种新型的多模式生成流量的无培训指导方法。 TFG-Flow在指导离散变量中保持无偏采样的属性，以解决差异的诅咒。我们在四个分子设计任务上验证了TFG-Flow，并表明TFG-Flow通过产生具有所需特性的分子在药物设计中具有巨大的潜力。

### Coordinating Ride-Pooling with Public Transit using Reward-Guided Conservative Q-Learning: An Offline Training and Online Fine-Tuning Reinforcement Learning Framework 
[[arxiv](https://arxiv.org/abs/2501.14199)] [[cool](https://papers.cool/arxiv/2501.14199)] [[pdf](https://arxiv.org/pdf/2501.14199)]
> **Authors**: Yulong Hu,Tingting Dong,Sen Li
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 使用奖励指导的保守Q学习与公共交通协调乘车交通：离线培训和在线微调强化学习框架
- **领域**: 机器学习,人工智能,新兴技术
- **摘要**: 本文介绍了一种新颖的增强学习（RL）框架，称为奖励指导的保守Q学习（RG-CQL），以增强多模式运输网络中乘车通行与公共交通之间的协调。我们将每辆乘车车辆建模为受马尔可夫决策过程（MDP）管辖的代理商，并提出了离线培训和在线微调RL框架，以了解多模式运输系统的最佳操作决策，包括骑手 - 车辆匹配，选择乘客的下降地点以及具有提高数据效率的车辆路线决策，并提高了汽车路线决策。在离线培训阶段，我们开发了一个保守的双重Q网络（CDDQN），作为行动执行者和基于学习的基于学习的奖励估计器称为指导网络，以从数据批次中提取对行动 - 奖励关系的宝贵见解。在在线微调阶段，指导网络是探索指南，有效且保守地探索未知的国家行动对。通过使用曼哈顿的现实世界数据进行现实的案例研究，证明了我们算法的功效。我们表明，将乘车通行与公共交通的整合优于两个基准案例，与运输和乘车协调的独奏骑行分别在实现的系统奖励中分别在没有交通协调的情况下分别为17％和22％。此外，与传统的在线RL方法相比，我们的创新离线培训和在线微调框架的数据效率显着提高了81.3％，具有足够的勘探预算，总奖励增长了4.3％，降低了5.6％的高估错误。实验结果进一步表明，RG-CQL有效地解决了与运输集成的大规模乘车系统中从离线RL过渡到在线RL的挑战。

### Multimodal Prescriptive Deep Learning 
[[arxiv](https://arxiv.org/abs/2501.14152)] [[cool](https://papers.cool/arxiv/2501.14152)] [[pdf](https://arxiv.org/pdf/2501.14152)]
> **Authors**: Dimitris Bertsimas,Lisa Everest,Vasiliki Stoumpou
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 多模式的规定深度学习
- **领域**: 机器学习,机器学习
- **摘要**: 我们介绍了一个多模式深度学习框架，规定性神经网络（PNN），该框架结合了从优化和机器学习中的想法，据我们所知，这是处理多模式数据的第一种规定方法。 PNN是一种在嵌入式上训练的前馈神经网络，以输出结果优化的处方。在两个现实世界中的多模式数据集中，我们证明了PNN处方治疗方法，能够通过将估计的术后并发症率降低32％，并通过降低估计的估计估计的估计估计的估计率，从而通过降低40％以上的估计估计损伤来显着提高经导管主动脉瓣置换术（TAVR）程序的估计结果。在四个现实世界中的单峰表格数据集中，我们证明了PNNS优于或与其他知名，最先进的规范模型相当的表现。重要的是，在表格数据集上，我们还通过知识蒸馏恢复了可解释性，将可解释的最佳分类树模型拟合到PNN处方作为分类目标，这对于许多现实世界应用至关重要。最后，我们证明了我们的多模式PNN模型在随机数据拆分中达到稳定性，可与其他规定方法相媲美，并在不同数据集中产生现实的处方。

### Pilot: Building the Federated Multimodal Instruction Tuning Framework 
[[arxiv](https://arxiv.org/abs/2501.13985)] [[cool](https://papers.cool/arxiv/2501.13985)] [[pdf](https://arxiv.org/pdf/2501.13985)]
> **Authors**: Baochen Xiong,Xiaoshan Yang,Yaguang Song,Yaowei Wang,Changsheng Xu
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 飞行员：构建联合的多模式指令调谐框架
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 在本文中，我们探讨了一项新颖的联合多模式指令调整任务（FedMit），这对于在分布式设备上不同类型的多模式指令数据上进行协作微调MLLM非常重要。为了解决新任务，我们提出了一个联合的多模式指令调谐框架（PILOT）。我们的框架将“适配器上的适配器”的两个阶段集成到视觉编码器和LLM的连接器中。在第1阶段，我们从视觉信息中提取特定于任务的功能和特定于客户端的功能。在第2阶段，我们构建了调整后的交叉任务混合物（CT-MOA）模块以执行交叉任务相互作用。每个客户不仅可以捕获本地数据的个性化信息并学习与任务相关的多模式信息，而且还可以从其他任务中学习一般知识。此外，我们引入了文本训练参数的自适应参数聚合策略，该策略通过根据参数之间的欧几里得距离来计算权重来优化参数聚集，以便参数聚集可以在最大程度上从正面效应中受益，同时有效地降低负面影响。我们的框架可以从不同本地客户端进行协作利用分布式数据，以学习交叉任务知识，而不会在教学调整过程中受到任务异质性的影响。我们方法的有效性在两个不同的交叉任务场景中得到了验证。

### Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.13904)] [[cool](https://papers.cool/arxiv/2501.13904)] [[pdf](https://arxiv.org/pdf/2501.13904)]
> **Authors**: Linh Tran,Wei Sun,Stacy Patterson,Ana Milanova
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multimodal Large Language Models (LLMs) are pivotal in revolutionizing customer support and operations by integrating multiple modalities such as text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed approach that combines pre-trained multimodal LLMs such as vision-language models with federated learning to create personalized, privacy-preserving AI systems. However, balancing the competing goals of personalization, generalization, and privacy remains a significant challenge. Over-personalization can lead to overfitting, reducing generalizability, while stringent privacy measures, such as differential privacy, can hinder both personalization and generalization. In this paper, we propose a Differentially Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by leveraging a low-rank factorization scheme to capture generalization while maintaining a residual term that preserves expressiveness for personalization. To ensure privacy, we introduce a novel method where we apply local differential privacy to the two low-rank components of the local prompt, and global differential privacy to the global prompt. Our approach mitigates the impact of privacy noise on the model performance while balancing the tradeoff between personalization and generalization. Extensive experiments demonstrate the effectiveness of our approach over other benchmarks.

### Multimodal Sensor Dataset for Monitoring Older Adults Post Lower-Limb Fractures in Community Settings 
[[arxiv](https://arxiv.org/abs/2501.13888)] [[cool](https://papers.cool/arxiv/2501.13888)] [[pdf](https://arxiv.org/pdf/2501.13888)]
> **Authors**: Ali Abedi,Charlene H. Chu,Shehroz S. Khan
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 多模式传感器数据集，用于监测老年人在社区环境中下limb骨折的骨折
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 下limb骨折（LLF）是老年人的主要健康问题，通常会导致流动性和恢复延长，可能会损害日常活动和独立性。在康复过程中，老年人经常面临社会隔离和功能下降，康复复杂化并对身心健康产生不利影响。连续收集数据并使用机器学习算法分析数据的多模式传感器平台可以远程监测该人群并推断健康结果。他们还可以提醒临床医生对有孤立和衰落风险的个人。本文介绍了一种新的公开多模式传感器数据集Maison-LLF，该数据集是从社区环境中从LLF恢复的老年人收集的。该数据集包括来自智能手机和智能手表传感器的数据，运动探测器，睡眠跟踪床垫以及有关隔离和衰落的临床问卷。该数据集是从十名单独生活在家中的十名老年人中收集的八周，总计560天24小时传感器数据。为了进行技术验证，使用传感器和临床问卷数据开发了监督的机器学习和深度学习模型，为研究界提供了基础比较。

### Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility 
[[arxiv](https://arxiv.org/abs/2501.13479)] [[cool](https://papers.cool/arxiv/2501.13479)] [[pdf](https://arxiv.org/pdf/2501.13479)]
> **Authors**: Rishabh Agrawal
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 自适应少学习（AFSL）：以稳定性，鲁棒性和多功能性解决数据稀缺性
- **领域**: 机器学习,人工智能
- **摘要**: 很少有射击学习（FSL）使机器学习模型可以用最小的标记数据有效地概括，这对于医疗保健，机器人技术和自然语言处理等数据制品领域至关重要。尽管具有潜力，但FSL仍面临挑战，包括对初始化的敏感性，适应不同领域的困难以及对嘈杂数据集的脆弱性。为了解决这些问题，本文介绍了自适应少量学习（AFSL），该框架将元学习，域对齐，噪声弹性和多模式集成的进步集成在一起。 AFSL由四个关键模块组成：一个用于性能一致性的动态稳定模块，用于域适应的上下文域对齐模块，用于处理噪声数据的噪声自适应弹性模块以及用于集成多种模态的多模式融合模块。这项工作还探讨了诸如任务感知数据增强，半监督学习和可解释的AI技术等策略，以增强FSL的适用性和鲁棒性。 AFSL为现实世界中的高风险域提供了可扩展，可靠和有影响力的解决方案。

### M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction with Person-aware Blockwise Attention 
[[arxiv](https://arxiv.org/abs/2501.13416)] [[cool](https://papers.cool/arxiv/2501.13416)] [[pdf](https://arxiv.org/pdf/2501.13416)]
> **Authors**: Yiming Tang,Abrar Anwar,Jesse Thomason
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: M3PT：多模式，多方社交信号预测的变压器，以人的意识关注
- **领域**: 机器学习,人工智能,机器人技术
- **摘要**: 在多方对话中了解社会信号对于人类机器人的互动和人工社会智能很重要。社会信号包括身体姿势，头部姿势，言语以及特定于上下文的活动，例如在就餐时获得和咬食物。多方互动中的过去工作倾向于建立特定于任务的模型来预测社会信号。在这项工作中，我们应对单个模型中多方面的多模式社会信号进行预测的挑战。我们介绍了M3PT，这是一种具有模态和暂时性关注掩盖的因果变压器架构，以同时处理多个参与者及其时间互动的多个社交线索。我们在人类人类尊贵数据集（HHCD）上训练和评估M3PT，并证明使用多种方式可以改善咬合的时机和口语状态预测。源代码：https：//github.com/abraranwar/masked-social-signals/。

### Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink 
[[arxiv](https://arxiv.org/abs/2501.15269)] [[cool](https://papers.cool/arxiv/2501.15269)] [[pdf](https://arxiv.org/pdf/2501.15269)]
> **Authors**: Yining Wang,Mi Zhang,Junjie Sun,Chenyue Wang,Min Yang,Hui Xue,Jialing Tao,Ranjie Duan,Jiexi Liu
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-27
> **comment**: USENIX Security 2025
- **标题**: 眼中的幻影：对多模式大语言模型的幻觉攻击只有注意力下沉
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别
- **摘要**: 将视觉理解融合到语言生成中，多模式大型语言模型（MLLM）正在彻底改变视觉语言应用。然而，这些模型通常受到幻觉问题的困扰，幻觉问题涉及产生不准确的对象，属性和不符合视觉内容的关系。在这项工作中，我们深入研究了MLLM的内部注意力机制，以揭示幻觉的根本原因，从而揭示了指导过程中固有的脆弱性。我们提出了针对MLLM的一种新颖的幻觉攻击，该攻击利用了关注点的行为，以最小的图像文本相关性触发了幻觉的内容，从而对关键的下游应用构成了重大威胁。与以前依赖固定模式的对抗方法区别，我们的方法会产生动态，有效且高度可转移的视觉对抗输入，而无需牺牲模型响应的质量。在6个突出的MLLM上进行的全面实验证明了我们的攻击在损害黑盒MLLM中的功效，即使具有广泛的缓解机制，以及针对较高的商业API（例如GPT-4O和Gemini 1.5）的有希望的结果。我们的代码可在https://huggingface.co/rachelhgf/mirage-in-the-yeyes上找到。

### Humanity's Last Exam 
[[arxiv](https://arxiv.org/abs/2501.14249)] [[cool](https://papers.cool/arxiv/2501.14249)] [[pdf](https://arxiv.org/pdf/2501.14249)]
> **Authors**: Long Phan,Alice Gatti,Ziwen Han,Nathaniel Li,Josephina Hu,Hugh Zhang,Chen Bo Calvin Zhang,Mohamed Shaaban,John Ling,Sean Shi,Michael Choi,Anish Agrawal,Arnav Chopra,Adam Khoja,Ryan Kim,Richard Ren,Jason Hausenloy,Oliver Zhang,Mantas Mazeika,Tung Nguyen,Daron Anderson,Imad Ali Shah,Mikhail Doroshenko,Alun Cennyth Stokes,Mobeen Mahmood, et al. (709 additional authors not shown)
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: 27 pages, 6 figures
- **标题**: 人类的最后考试
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 基准是跟踪大语言模型（LLM）功能快速进步的重要工具。但是，基准测试并没有保持困难：LLMS现在在MMLU等流行的基准测试中实现超过90％的准确性，从而限制了最先进的LLM功能的知情测量。作为回应，我们介绍了人类的最后考试（HLE），这是人类知识前沿的多模式基准，旨在成为具有广泛主题覆盖的最终封闭式学术基准。 HLE包括数十个学科，包括数学，人文科学和自然科学的2700个问题。 HLE是由主题专家在全球开发的，包括适合自动化分级的多项选择和短答案问题。每个问题都有一个已知的解决方案，该解决方案是明确且易于验证的，但不能通过互联网检索快速回答。最先进的LLMS在HLE上表现出较低的精度和校准，这突出了当前LLM功能与封闭式学术问题的专业人类边界之间的显着差距。为了使研究和决策对模型能力有清晰的了解，我们将在https://lastexam.ai公开发布HLE。

### Chinese Stock Prediction Based on a Multi-Modal Transformer Framework: Macro-Micro Information Fusion 
[[arxiv](https://arxiv.org/abs/2501.16621)] [[cool](https://papers.cool/arxiv/2501.16621)] [[pdf](https://arxiv.org/pdf/2501.16621)]
> **Authors**: Lumen AI,Tengzhou No. 1 Middle School,Shihao Ji,Zihui Song,Fucheng Zhong,Jisen Jia,Zhaobo Wu,Zheyi Cao,Xu Tianhao
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 基于多模式变压器框架的中国库存预测：宏观信息信息融合
- **领域**: 机器学习,人工智能
- **摘要**: 本文提出了一个创新的多模式变压器框架（MMF-Trans），旨在通过整合包括宏观经济，微型市场，财务文本和事件知识在内的多源异质信息，从而显着提高中国股票市场的预测准确性。该框架由四个核心模块组成：（1）一个四通道并行编码器，该编码器分别处理技术指标，财务文本，宏观数据和事件知识图，以独立于多模式数据的独立特征提取； （2）一种动态的封闭式跨模式融合机制，该机制可自适应地学习通过可微分的权重分配以有效信息整合的不同模态的重要性； （3）使用创新的位置编码方法有效融合不同时间频率的数据并解决异质数据的时间对齐问题； （4）基于图的事件影响量化模块，该模块通过事件知识图捕获事件对市场的动态影响，并量化事件影响系数。我们引入了混合频率变压器和Event2VEC算法，以有效地融合不同频率的数据并量化事件影响。实验结果表明，在CSI 300组成股的预测任务中，与基线模型相比，MMF-TRANS框架的根平方误差（RMSE）降低了23.7％，事件响应预测的准确性提高了41.2％，而SHARPE比率提高了32.6％。

### SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments 
[[arxiv](https://arxiv.org/abs/2501.16471)] [[cool](https://papers.cool/arxiv/2501.16471)] [[pdf](https://arxiv.org/pdf/2501.16471)]
> **Authors**: Simon Dahan,Gabriel Bénédict,Logan Z. J. Williams,Yourong Guo,Daniel Rueckert,Robert Leech,Emma C. Robinson
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: 27 pages, accepted to ICLR 2025
- **标题**: SIM：基于表面的fMRI分析，用于从电影观看实验的受试者间多模式解码
- **领域**: 机器学习,人工智能,音频和语音处理,图像和视频处理,神经元和认知
- **摘要**: 当前用于大脑解码和编码的AI框架，通常在同一数据集中训练和测试模型。这限制了他们用于大脑计算机界面（BCI）或神经反馈的效用，对于跨个体汇集体验以更好地模拟训练期间未采样的刺激将是有用的。模型概括的关键障碍是受试者间皮质组织的可变性程度，这使得很难对齐或比较参与者的皮质信号。在本文中，我们通过使用表面视觉变压器来解决这一问题，表面视觉变形金刚通过编码皮质网络的地形及其作为跨表面的运动图像来构建可通用的皮质功能动力学模型。然后将其与三模式的自我监督对比度（剪辑）对齐音频，视频和fMRI模态，以使视觉和听觉刺激从皮质活动的模式（反之亦然）中获取。我们验证了来自人类Connectome Project（HCP）的174名健康参与者的74名健康参与者的7T Task-FMRI数据的方法。结果表明，即使是在训练过程中看不到的个人和电影，也可以检测一个人纯粹是从大脑活动中观看的电影剪辑。对注意图的进一步分析表明，我们的模型捕获了反映语义和视觉系统的大脑活动模式。这为未来对大脑功能的个性化模拟打开了大门。代码和预训练的模型将在https://github.com/metrics-lab/sim上提供，可以根据要求在https://gin.g-node.org/sdahan30/sim上获得培训的处理数据。

### Internal Activation Revision: Safeguarding Vision Language Models Without Parameter Update 
[[arxiv](https://arxiv.org/abs/2501.16378)] [[cool](https://papers.cool/arxiv/2501.16378)] [[pdf](https://arxiv.org/pdf/2501.16378)]
> **Authors**: Qing Li,Jiahui Geng,Zongxiong Chen,Kun Song,Lei Ma,Fakhri Karray
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 内部激活修订：保护视觉语言模型没有参数更新
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLMS）表现出强大的多模式能力，但与其骨干大型语言模型（LLMS）相比，发现更容易产生有害内容。我们的调查表明，图像的整合在前通行证期间显着改变了模型的内部激活，这与文本输入触发的模型不同。此外，在VLM中嵌入的LLM的安全对齐不足以应对激活差异，从而使模型甚至最容易受到最简单的越狱攻击。为了解决此问题，我们提出了一个\ textbf {内部激活修订}方法，该方法有效地修改了生成过程中的激活，将模型转向更安全的输出。我们的框架结合了层和头部水平的修订，从而控制了模型在不同层次的粒度上的控制。此外，我们探索了构建正面和负样本的三种策略，以及提取修订矢量的两种方法，从而导致我们方法的不同变体。 Comprehensive experiments demonstrate that the internal activation revision method significantly improves the safety of widely used VLMs, reducing attack success rates by an average of 48.94\%, 34.34\%, 43.92\%, and 52.98\% on SafeBench, Safe-Unsafe, Unsafe, and MM-SafetyBench, respectively, while minimally impacting model helpfulness.

### Foundation Models for CPS-IoT: Opportunities and Challenges 
[[arxiv](https://arxiv.org/abs/2501.16368)] [[cool](https://papers.cool/arxiv/2501.16368)] [[pdf](https://arxiv.org/pdf/2501.16368)]
> **Authors**: Ozan Baris,Yizhuo Chen,Gaofeng Dong,Liying Han,Tomoyoshi Kimura,Pengrui Quan,Ruijie Wang,Tianchen Wang,Tarek Abdelzaher,Mario Bergés,Paul Pu Liang,Mani Srivastava
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: CPS-iot的基础模型：机遇和挑战
- **领域**: 机器学习,人工智能,系统与控制
- **摘要**: 机器学习（ML）的方法已将网络物理系统（CPS）和物联网（IOT）中的感知认知 - 通信循环的实现转换，用数据衍生的方法替换了机械和基本统计​​模型。但是，第一代ML方法依赖于带有注释的数据以创建特定于任务模型的监督学习，在扩展到不同的传感器模式，部署配置，应用程序任务和操作动态方面面临着重大限制，这些动态表征了现实世界中CPS-CPS-IOT系统。任务无关基础模型（FMS）的成功，包括多模式的大语言模型（LLMS），在应对自然语言，计算机视觉和人类语音跨越类似挑战方面，对FMS和LLMS作为FMS和LLMS作为CPS-Iot型固定基础的热情以及探索CPS-Iot分析管道中的灵活构建障碍物，可减少对成本上的任务性发动机的需求。尽管如此，在CPS-iot域中FMS和LLM的当前功能之间存在很大的差距，以及它们必须满足CPS-IOT应用必须满足的要求。在本文中，我们通过对艺术状态和我们的研究的彻底研究来分析和表征这一差距，这在各个方面都超越了它。基于我们的分析和研究结果，我们确定了CPS-iot域特异性FMS和LLM必须满足以弥合这一差距的必需必需品。我们还建议CPS-IOT研究人员的行动，以合作开发建立FMS和LLM作为下一代CPS-IOT系统的基础工具所需的关键社区资源。

### sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging 
[[arxiv](https://arxiv.org/abs/2501.16329)] [[cool](https://papers.cool/arxiv/2501.16329)] [[pdf](https://arxiv.org/pdf/2501.16329)]
> **Authors**: Jingyuan Chen,Yuan Yao,Mie Anderson,Natalie Hauglund,Celia Kjaerby,Verena Untiet,Maiken Nedergaard,Jiebo Luo
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: SDREAMER：自动式Experts Transferter用于自动睡眠阶段
- **领域**: 机器学习,人工智能
- **摘要**: 基于脑电图（EEG）和肌电图（EMG）信号的自动睡眠分期是与睡眠有关的研究的重要方面。当前的睡眠分期方法遭受了两个主要缺点。首先，现有方法中模式之间的信息相互作用有限。其次，当前方法不会开发可以处理不同输入来源的统一模型。为了解决这些问题，我们提出了一个新型的睡眠阶段评分模型SDREAMER，该模型强调了交叉模式的相互作用和每通道性能。具体而言，我们开发了一种模型的混合物（MOME）模型，该模型具有三种用于EEG，EMG和具有部分共享权重的混合信号的途径。我们进一步提出了一种自我验证培训计划，以跨模态进行进一步的信息互动。我们的模型经过多通道输入训练，可以对单渠道或多通道输入进行分类。实验表明，我们的模型优于多通道推理的现有基于变压器的睡眠评分方法。对于单通道推断，我们的模型还胜过基于单通道信号训练的基于变压器的模型。

### Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity 
[[arxiv](https://arxiv.org/abs/2501.16295)] [[cool](https://papers.cool/arxiv/2501.16295)] [[pdf](https://arxiv.org/pdf/2501.16295)]
> **Authors**: Weixin Liang,Junhong Shen,Genghan Zhang,Ning Dong,Luke Zettlemoyer,Lili Yu
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: Mamba的混合物：增强具有模态感知稀疏性的多模式状态空间模型
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 状态空间模型（SSM）已成为序列建模的变压器的有效替代方案，但是它们无法利用特定于模态特异性的特征限制了它们在多模式预审进方面的性能。在这里，我们提出了Mamba的混合物，Mamba是一种新颖的SSM体系结构，通过Mamba块的模态参数化引入了模态感知的稀疏性。基于转变器的混合物（W. Liang等人Arxiv：2411.04996; 2024），在保持其计算效率的同时，将模态感知稀疏性的好处扩展到了SSM。我们在三个多模式预处理环境中评估了山的混合物：输血（交错文本和连续的图像令牌，具有扩散损失），变色龙（交织的文本和离散图像令牌），以及纳入语音的扩展三模式框架。 Mamba的混合物在早期的训练步骤中始终达到相同的损失值，并大大降低了计算成本。在输血设置中，曼巴的混合物仅在1.4B尺度上使用训练拖失lop的34.76％实现同等图像损失。在变色龙的环境中，曼巴（Mamba）的混合物达到了相似的图像损失，只有42.50％的拖鞋以1.4B尺度拖曳，并且仅有65.40％的拖鞋丢失。在三种模式的环境中，妈妈以1.4B量表的速度匹配了拖鞋的24.80％的语音损失。我们的消融研究强调了去耦投影成分的协同作用，其中关节解耦比单个修改所获得的增长更大。这些结果确立了一种多功能和有效的设计原理，将其偏移性建立在变压器到SSM上，并在多模式预处理中设定新的基准测试。我们的代码可以在https://github.com/weixin-liang/mixture of-mamba访问

### Upside Down Reinforcement Learning with Policy Generators 
[[arxiv](https://arxiv.org/abs/2501.16288)] [[cool](https://papers.cool/arxiv/2501.16288)] [[pdf](https://arxiv.org/pdf/2501.16288)]
> **Authors**: Jacopo Di Ventura,Dylan R. Ashley,Vincent Herrmann,Francesco Faccio,Jürgen Schmidhuber
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: 4 pages in main text, 4 figures in main text; source code available at https://github.com/JacopoD/udrlpg_
- **标题**: 通过政策生成器颠倒增强学习
- **领域**: 机器学习,人工智能
- **摘要**: 颠倒的强化学习（UDRL）是解决重点是学习命令条件政策的强化学习问题的有前途的框架。在这项工作中，我们将UDRL扩展到学习深神经网络政策的命令条件发生器的任务。我们使用HyperNetworks（快速权重程序员的变体）来解码输入命令，该命令代表所需的预期返回到特定命令特定的权重矩阵。我们的方法将其称为颠倒的加固学习，以策略生成器（UDRLPG）（UDRLPG），通过消除评估者或评论家来更新发电机的权重来简化可比较的技术。为了抵消由于没有评估者而引起的最后回报的差异的增加，我们将缓冲液的采样概率与其中的绝对策略数量解脱出来，后者与简单的加权策略一起改善了算法的经验收敛。与现有算法相比，UDRLPG可以实现竞争性能和高回报，有时表现优于更复杂的体系结构。我们的实验表明，训练有素的发电机可以概括以创建实现未见回报零拍摄的策略。所提出的方法似乎有效地减轻与学习高度多模式功能相关的一些挑战。总而言之，我们认为UDRLPG代表了在RL中实现更高的经验样本效率方面迈出的前进一步。 https://github.com/jacopod/udrlpg_可以公开获得UDRLPG的完整实现。

### Topological Signatures of Adversaries in Multimodal Alignments 
[[arxiv](https://arxiv.org/abs/2501.18006)] [[cool](https://papers.cool/arxiv/2501.18006)] [[pdf](https://arxiv.org/pdf/2501.18006)]
> **Authors**: Minh Vu,Geigh Zollicoffer,Huy Mai,Ben Nebgen,Boian Alexandrov,Manish Bhattarai
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: No comments
- **标题**: 多模式比对中对手的拓扑特征
- **领域**: 机器学习,人工智能,密码学和安全
- **摘要**: 多模式的机器学习系统，尤其是那些对齐文本和图像数据（如剪辑/BLIP模型）的越来越普遍，但仍然容易受到对抗性攻击的影响。尽管实质性的研究已经解决了在单峰环境中的对抗性鲁棒性，但多模式系统的防御策略却没有驱动。这项工作调查了图像和文本嵌入之间出现的拓扑特征，并显示了对抗性攻击如何破坏其对齐方式，并引入了独特的特征。我们特别利用持久的同源性，并根据总持久性和多尺度内核方法引入了两种新型的拓扑对焦损失，以分析由对抗性扰动引入的拓扑特征。我们观察到在图像文本比对的广泛攻击中出现的拟议拓扑损失的单调变化模式，因为数据中引入了更多的对抗样本。通过设计一种算法以将这些签名重新传播到输入样本中，我们能够将这些签名集成到最大的平均差异测试中，从而创建了一类新的测试，以利用拓扑特征来提供更好的对抗性检测。

### Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition 
[[arxiv](https://arxiv.org/abs/2501.18268)] [[cool](https://papers.cool/arxiv/2501.18268)] [[pdf](https://arxiv.org/pdf/2501.18268)]
> **Authors**: Arthur Hoarau,Benjamin Quost,Sébastien Destercke,Willem Waegeman
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 通过多模式数据获取来减少核心和认知不确定性
- **领域**: 机器学习
- **摘要**: 为了产生准确可靠的预测，现代AI系统需要将来自多种模式的数据（例如文本，图像，音频，电子表格和时间序列）组合在一起。多模式数据引入了新的机会和挑战，以解决不确定性：在机器学习社区中通常假定，可以通过收集更多数据来减少认知不确定性，而且不确定的不确定性是不可约束的。但是，当从不同的方式获得信息时，在现代AI系统中挑战了这一假设。本文介绍了一个创新的数据采集框架，其中不确定性分解会导致可行的决策，从而允许在两个方向上进行采样：样本量和数据模式。主要的假设是，随着模态数量的增加，差异不确定性会降低，而认知不确定性通过收集更多的观察结果降低。我们在两个多模式数据集上提供了概念验证的实现，以展示我们的数据采集框架，该框架结合了主动学习，主动功能获取和不确定性量化的想法。

### Continually Evolved Multimodal Foundation Models for Cancer Prognosis 
[[arxiv](https://arxiv.org/abs/2501.18170)] [[cool](https://papers.cool/arxiv/2501.18170)] [[pdf](https://arxiv.org/pdf/2501.18170)]
> **Authors**: Jie Peng,Shuang Zhou,Longwei Yang,Yiran Song,Mohan Zhang,Kaixiong Zhou,Feng Xie,Mingquan Lin,Rui Zhang,Tianlong Chen
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: 9 pages, 1 figure
- **标题**: 不断进化的癌症预后多模式基础模型
- **领域**: 机器学习
- **摘要**: 癌症预后是一项关键任务，涉及预测患者的结局和存活率。为了提高预测准确性，以前的研究已经整合了各种数据模式，例如临床注释，医学图像和基因组数据，利用其互补信息。但是，现有方法面临两个主要局限性。首先，他们很难将新近到达的数据纳入不同的分配中，例如来自不同医院的患者记录，从而在现实世界中提供了次优的概括性和有限的效用。其次，大多数多模式集成方法都依赖于简单的串联或特定于任务的管道，这些串联无法捕获跨模态的复杂相互依赖性。为了解决这些问题，我们提出了一个不断发展的多模式基础模型。在TCGA数据集上进行的广泛实验证明了我们方法的有效性，突出了其通过实现强大和适应性的多模式整合来提高癌症预后的潜力。

## 多媒体(cs.MM:Multimedia)

该领域共有 5 篇论文

### On the Robustness of Cover Version Identification Models: A Study Using Cover Versions from YouTube 
[[arxiv](https://arxiv.org/abs/2501.01333)] [[cool](https://papers.cool/arxiv/2501.01333)] [[pdf](https://arxiv.org/pdf/2501.01333)]
> **Authors**: Simon Hachmeier,Robert Jäschke
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: accepted for presentation at iConference 2025
- **标题**: 关于封面版本识别模型的鲁棒性：使用YouTube的封面版本的研究
- **领域**: 多媒体,信息检索,社交和信息网络
- **摘要**: 封面歌曲标识的最新进展已取得了巨大的成功。但是，通常会在依赖在线封面歌曲数据库二手证的固定数据集上测试模型。目前尚不清楚模型在在线视频平台上的封面歌曲中的表现如何，这可能会显示出预期的变化。在本文中，我们注释了YouTube中的一部分歌曲，该歌曲是通过多模式不确定性采样方法采样的，并评估了最新模型。我们发现，与社区数据集相比，现有模型在我们的数据集上的排名绩效明显降低。我们还测量了不同类型版本的性能（例如工具版本），并找到几种特殊排名的类型。最后，我们提供了网络上封面版本更改的分类法。

### Listening and Seeing Again: Generative Error Correction for Audio-Visual Speech Recognition 
[[arxiv](https://arxiv.org/abs/2501.04038)] [[cool](https://papers.cool/arxiv/2501.04038)] [[pdf](https://arxiv.org/pdf/2501.04038)]
> **Authors**: Rui Liu,Hongyu Yuan,Haizhou Li
> **First submission**: 2025-01-03
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: 聆听和再次查看：视听语音识别的生成错误校正
- **领域**: 多媒体,人工智能,声音,音频和语音处理
- **摘要**: 与传统的自动语音识别（ASR）不同，视听语音识别（AVSR）同时采用音频和视觉信号来推断转录。最近的研究表明，通过预测来自ASR生成的N-最佳假设的最佳转录，可以有效地使用大型语言模型（LLM）进行ASR中的生成误差校正（GER）。但是，这些LLM缺乏同时了解音频和视觉的能力，这使得GER方法在AVSR中充满挑战。在这项工作中，我们为Avger提出了一个新颖的GER范式，该范式遵循``聆听和再次看到''的概念。具体而言，我们首先使用功能强大的AVSR系统读取音频和视觉信号以获取N-最佳假设，然后使用基于Q的多模式同步编码器再次读取音频和视觉信息，然后将它们分别转换为音频和视频压缩表示，可以由LLM理解。之后，视听压缩表示和N最佳假设共同构成了一个跨模式提示，可以指导LLM产生最佳转录。此外，我们还提出了一个多级一致性约束训练标准，包括逻辑级别，话语级和表示级别，以提高校正精度，同时增强音频和视觉压缩表示的解释性。 LRS3数据集上的实验结果表明，我们的方法的表现优于当前主流AVSR系统。与之相比，提出的AVGER可以将单词错误率（WER）降低24％。代码和模型可以在以下网址找到：https：//github.com/circleredrain/avger。

### Zero-shot Video Moment Retrieval via Off-the-shelf Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.07972)] [[cool](https://papers.cool/arxiv/2501.07972)] [[pdf](https://arxiv.org/pdf/2501.07972)]
> **Authors**: Yifang Xu,Yunzhuo Sun,Benxiang Zhai,Ming Li,Wenxin Liang,Yang Li,Sidan Du
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: Accepted by AAAI 2025
- **标题**: 通过现成的多模式大型语言模型的零拍摄视频瞬间检索
- **领域**: 多媒体,计算机视觉和模式识别
- **摘要**: 视频时刻检索（VMR）的目标是预测在语义上与给定语言查询匹配的视频中的时间跨度。基于多模式大语言模型（MLLM）的现有VMR方法过于依赖昂贵的高质量数据集和耗时的微调。尽管最近的一些研究引入了零拍设置以避免进行微调，但它们忽略了查询中固有的语言偏见，导致错误的本地化。为了应对上述挑战，本文提出了Moment-gpt，这是使用冷冻MLLM的零击VMR的无调管道。具体来说，我们首先采用Llama-3来纠正和重新调整查询以减轻语言偏见。随后，我们设计了一个与迷你V2结合的跨度发生器，以使候选跨度适应性。最后，为了利用MLLM的视频理解能力，我们应用VideoChatgpt和Span得分手来选择最合适的跨度。我们提出的方法在几个公共数据集上大大优于基于MLLM的最先进的MLLM和零摄像模型，包括QVHighlights，ActivityNet-Captions和Charades-STA。

### Mitigating GenAI-powered Evidence Pollution for Out-of-Context Multimodal Misinformation Detection 
[[arxiv](https://arxiv.org/abs/2501.14728)] [[cool](https://papers.cool/arxiv/2501.14728)] [[pdf](https://arxiv.org/pdf/2501.14728)]
> **Authors**: Zehong Yan,Peng Qi,Wynne Hsu,Mong Li Lee
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: 12 pages, 11 figures
- **标题**: 缓解Genai驱动的证据污染污染了多种模式的错误信息检测
- **领域**: 多媒体,计算语言学,计算机视觉和模式识别,计算机与社会
- **摘要**: 尽管大型生成人工智能（Genai）模型取得了巨大的成功，但由于潜在的滥用欺骗性内容，它们也引起了人们对在线信息安全性的日益关注。在错误上下文中经常检索网络证据以确定在错误上下文中的图像重新利用的多模式错误信息检测，面临着与Genai污染的证据相比，以得出准确的预测。现有作品模拟了索赔水平的Genai驱动的污染，并以风格重写来掩盖语言提示，而忽略了此类信息寻求信息的证据级污染。在这项工作中，我们研究了污染的证据如何影响现有的OOC探测器的性能，揭示了超过9个百分点的性能降解。我们提出了两种策略，即跨模式证据重读和跨模式索赔 - 证据推理，以应对污染证据所带来的挑战。在两个基准数据集上进行的广泛实验表明，这些策略可以有效地增强现有的官方外检测器的鲁棒性，并在污染的证据中。

### AGAV-Rater: Adapting Large Multimodal Model for AI-Generated Audio-Visual Quality Assessment 
[[arxiv](https://arxiv.org/abs/2501.18314)] [[cool](https://papers.cool/arxiv/2501.18314)] [[pdf](https://arxiv.org/pdf/2501.18314)]
> **Authors**: Yuqin Cao,Xiongkuo Min,Yixuan Gao,Wei Sun,Guangtao Zhai
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: AGAV评估者：调整大型多模型模型，以进行AI生成的视听质量评估
- **领域**: 多媒体,计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 已经提出了许多视频对ADIO（VTA）方法来配音无声AI生成的视频。 AI生成的视听内容（AGAV）的有效质量评估方法对于确保视听质量至关重要。现有的视听质量评估方法与AGAV中的独特扭曲（例如不现实且不一致的元素）中的独特扭曲作用。为了解决这个问题，我们介绍了第一个大型AGAV质量评估数据集Agavqa，其中包含来自16种VTA方法的3,382个AGAV。 Agavqa包括两个子集：Agavqa-Mos，该子集为音频质量，内容一致性和整体质量提供多维分数，以及旨在最佳AGAV对选择的Agavqa-Pair。我们进一步提出了AGAV-Rater，这是一个基于LMM的模型，可以在多个维度上为AGAV以及从文本产生的音频和音乐评分，并选择VTA方法生成的最佳AGAV以呈现给用户。 AGAV-RATER在Agavqa，文本到原告和文本到音乐数据集方面实现了最先进的性能。主观测试还证实，AGAV-Rater可以增强VTA性能和用户体验。该项目页面可在https://agav-rater.github.io上找到。

## 机器人技术(cs.RO:Robotics)

该领域共有 13 篇论文

### MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception 
[[arxiv](https://arxiv.org/abs/2501.01037)] [[cool](https://papers.cool/arxiv/2501.01037)] [[pdf](https://arxiv.org/pdf/2501.01037)]
> **Authors**: Xiaoshuai Hao,Guanqun Liu,Yuting Zhao,Yuheng Ji,Mengchuan Wei,Haimei Zhao,Lingdong Kong,Rong Yin,Yu Liu
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: MSC基础：基准测试和分析多传感器损坏以驾驶感知
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **摘要**: 多传感器融合模型在自主驾驶感知中起着至关重要的作用，尤其是在3D对象检测和HD MAP构造等任务中。这些模型为自动驾驶系统提供了必不可少的静态环境信息。尽管相机范围的融合方法通过整合了两种模式的数据表现出令人鼓舞的结果，但它们通常取决于完整的传感器输入。当传感器损坏或缺失时，这种依赖会导致鲁棒性和潜在失败，从而引起严重的安全问题。为了应对这一挑战，我们介绍了多传感器腐败基准（MSC Bench），这是第一个旨在评估各种传感器损坏的多传感器自动驾驶感知模型的鲁棒性的全面基准。我们的基准包括单独或同时破坏相机和激光镜输入的16种损坏类型组合。对六个3D对象检测模型和四个HD MAP构造模型的广泛评估显示，在不利的天气条件和传感器故障下，大量的性能退化，强调了关键的安全问题。基准工具包和附属代码和模型检查点已公开访问。

### UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility 
[[arxiv](https://arxiv.org/abs/2501.02341)] [[cool](https://papers.cool/arxiv/2501.02341)] [[pdf](https://arxiv.org/pdf/2501.02341)]
> **Authors**: Yonglin Tian,Fei Lin,Yiduo Li,Tengchao Zhang,Qiyao Zhang,Xuan Fu,Jun Huang,Xingyuan Dai,Yutong Wang,Chunwei Tian,Bai Li,Yisheng Lv,Levente Kovács,Fei-Yue Wang
> **First submission**: 2025-01-04
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 无人机遇到LLM：概述和观点朝着代理低空机动性
- **领域**: 机器人技术,人工智能
- **摘要**: 无人机（UAVS）举例说明的低空迁移率已经引入了各个领域的变革性进步，例如运输，物流和农业。利用灵活的观点和快速的机动性，无人机扩展了传统系统的感知和动作能力，从而引起了学术界和行业的广泛关注。但是，当前的无人机操作主要取决于人类控制，在简单的情况下仅具有有限的自主权，并且缺乏更复杂的环境和任务所需的智能和适应性。大型语言模型（LLM）的出现表现出了显着的问题解决和概括能力，为推进无人机智能提供了有希望的途径。本文探讨了LLM和无人机的集成，首先是对无人机系统的基本组件和功能的概述，然后概述了LLM技术最先进的。随后，它系统地突出了无人机可用的多模式数据资源，这些数据资源为培训和评估提供了重要的支持。此外，它对无人机和LLM收敛的关键任务和应用程序方案进行了分类和分析。最后，提出了针对代理无人机的参考路线图，旨在通过自主感知，记忆，推理和工具利用来实现无人机来实现代理智能。相关资源可在https://github.com/hub-tian/uavs_meet_llms上获得。

### VTAO-BiManip: Masked Visual-Tactile-Action Pre-training with Object Understanding for Bimanual Dexterous Manipulation 
[[arxiv](https://arxiv.org/abs/2501.03606)] [[cool](https://papers.cool/arxiv/2501.03606)] [[pdf](https://arxiv.org/pdf/2501.03606)]
> **Authors**: Zhengnan Sun,Zhaotai Shi,Jiayin Chen,Qingtao Liu,Yu Cui,Qi Ye,Jiming Chen
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: No comments
- **标题**: vtao-bimanip：掩盖的视觉操作训练和对象理解双歧性灵巧操纵的预训练
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 由于每只手的高DOF及其协调性，双重灵巧的操纵仍然是机器人技术的重大挑战。现有的单手操纵技术通常会利用人类的演示来指导RL方法，但未能推广到涉及多个子技能的复杂双人任务。在本文中，我们介绍了VTAO-BIMANIP，这是一个新型框架，将视觉效果预处理与对象理解结合在一起，以促进课程RL以实现人类的双层操作。我们通过合并手运动数据来改善先前的学习，提供比二进制触觉反馈更有效的双手协调指导。我们的训练训练模型可预测未来的动作以及使用掩盖的多模式输入的对象姿势和大小，从而促进了跨模式正则化。为了应对多技能学习挑战，我们引入了两阶段的RL方法来稳定培训。我们在瓶盖拧紧的任务上评估了我们的方法，并在模拟环境和现实环境中都证明了其有效性。我们的方法达到的成功率超过了20％以上的现有视觉训练预处理方法。

### Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding 
[[arxiv](https://arxiv.org/abs/2501.04693)] [[cool](https://papers.cool/arxiv/2501.04693)] [[pdf](https://arxiv.org/pdf/2501.04693)]
> **Authors**: Joshua Jones,Oier Mees,Carmelo Sferrazza,Kyle Stachowicz,Pieter Abbeel,Sergey Levine
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 超越视力：通过语言接地的鉴定通才机器人政策具有异质传感器
- **领域**: 机器人技术,人工智能
- **摘要**: 与世界互动是一种多感官体验：实现有效的通用互动需要利用所有可用的方式（包括视觉，触摸和音频）来填补部分观察的空白。例如，当视力被遮住地伸入袋子时，机器人应依靠其触摸和声音的感觉。但是，最先进的通才机器人策略通常在大型数据集上进行培训，以预测仅从视觉和本体感受观察中的机器人动作。在这项工作中，我们提出了Fuse，这是一种新颖的方法，可以实现对异质传感器模式的视觉运动通才政策，通过利用自然语言作为常见的跨模式基础，不容易获得大型数据集。我们将多模式的对比损失与感觉的语言产生损失结合在一起，以编码高级语义。在机器人操纵的背景下，我们表明保险丝能够执行具有挑战性的任务，这些任务需要在零拍设置中共同进行诸如视觉，触摸和声音之类的模式，例如多模式提示，构图交叉模式提示，并与对象相互作用。我们表明，同一配方适用于广泛不同的通才政策，包括基于扩散的通才政策和大型视觉语言行动（VLA）模型。现实世界中的广泛实验表明，与所有考虑的基线相比，Fuseis能够将成功率提高超过20％。

### Cyber-Physical Steganography in Robotic Motion Control 
[[arxiv](https://arxiv.org/abs/2501.04541)] [[cool](https://papers.cool/arxiv/2501.04541)] [[pdf](https://arxiv.org/pdf/2501.04541)]
> **Authors**: Ching-Chun Chang,Yijie Lin,Isao Echizen
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 机器人运动控制中的网络物理隐肌
- **领域**: 机器人技术,人工智能,密码学和安全
- **摘要**: 隐藏术，即信息隐藏的艺术，一直在视觉，听觉和语言领域不断演变，适应了隐身隐藏和脱氧化启示之间的不断相互作用。这项研究旨在通过在机器人运动控制中引入造影范围来扩展构成可行的地理介质的视野。基于对机器人对环境变化的固有敏感性的观察，我们提出了一种方法，将消息编码为环境刺激影响机器人剂的运动，并从结果运动轨迹中解释消息。最大机器人完整性和最小运动偏差的限制被确定为秘密基础的基本原理。作为概念证明，我们在各种操纵任务的模拟环境中进行实验，并结合了配备通用多模式策略的机器人实施例。

### Enhanced Quantile Regression with Spiking Neural Networks for Long-Term System Health Prognostics 
[[arxiv](https://arxiv.org/abs/2501.05087)] [[cool](https://papers.cool/arxiv/2501.05087)] [[pdf](https://arxiv.org/pdf/2501.05087)]
> **Authors**: David J Poland
> **First submission**: 2025-01-09
> **First announcement**: 2025-01-10
> **comment**: No comments
- **标题**: 增强的分位数回归，具有长期系统健康预后的尖峰神经网络
- **领域**: 机器人技术,机器学习
- **摘要**: 本文介绍了一个新颖的预测维护框架，该框架以增强的分位数回归神经网络eqrnns为中心，用于预测工业机器人技术中的系统故障。我们通过结合高级神经体系结构的混合方法来应对早期失败检测的挑战。该系统利用双重计算阶段：首先实施用于处理多传感器数据流的EQRNN，包括振动，热和功率签名，然后是集成的Spiking神经网络SNN，层，该层启用微秒级响应时间。该体系结构在组件故障预测中实现了92.3 \％的明显准确率，并具有90小时的预警窗口。以50个机器人系统进行的工业规模进行的现场测试表明，操作的显着改进，意外系统故障的降低94 \％，而与维护相关的下降时间减少了76 \％。该框架在处理复杂的多模式传感器数据中的有效性，同时维持计算效率，验证了其对行业4.0制造环境的适用性。

### Enhancing Path Planning Performance through Image Representation Learning of High-Dimensional Configuration Spaces 
[[arxiv](https://arxiv.org/abs/2501.06639)] [[cool](https://papers.cool/arxiv/2501.06639)] [[pdf](https://arxiv.org/pdf/2501.06639)]
> **Authors**: Jorge Ocampo Jimenez,Wael Suleiman
> **First submission**: 2025-01-11
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 通过图像表示高维配置空间的图像表示绩效增强路径计划的性能
- **领域**: 机器人技术,人工智能
- **摘要**: 本文提出了一种新的方法，用于在未知场景中加速路径规划任务，并利用Wasserstein生成的对抗网络（WGAN）（WGANS）具有梯度惩罚（GP），以近似使用快速探索随机树算法的无碰撞路径的路线分布。我们的方法涉及在连续的潜在空间中使用正向扩散过程调节wgan-GP，以有效处理多模式数据集。我们还建议将无碰撞路径的路点编码为矩阵，其中自然保留了航路点的多维顺序。这种方法不仅可以改善模型学习，还可以增强培训融合。此外，我们提出了一种评估训练模型是否无法准确捕获真实路点的方法。在这种情况下，我们恢复为统一的采样，以确保算法的概率完整性；传统上涉及手动确定其他基于机器学习的方法中每种情况的最佳比率的过程。我们的实验证明了在关键时间限制下加速路径规划任务的有希望的结果。源代码可在https://bitbucket.org/joro3001/imagewgangpplanning/src/master/上公开获得。

### CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion Planning for Future Autonomous Mobility on Demand Systems 
[[arxiv](https://arxiv.org/abs/2501.06132)] [[cool](https://papers.cool/arxiv/2501.06132)] [[pdf](https://arxiv.org/pdf/2501.06132)]
> **Authors**: Haichao Liu,Ruoyu Yao,Wenru Liu,Zhenmin Huang,Shaojie Shen,Jun Ma
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: CODRIVEVLM：VLM增强的城市合作派遣和运动计划未来自动移动性按需系统系统
- **领域**: 机器人技术,人工智能,多代理系统
- **摘要**: 对灵活和高效的城市运输解决方案的需求不断增长，引起了传统需求响应式运输（DRT）系统的局限性，尤其是在满足各种乘客需求和动态的城市环境方面。在需求的自动移动性（AMOD）系统已成为有前途的替代方案，利用了连接和自动驾驶汽车（CAVS），以提供响应迅速且适应能力的服务。但是，现有方法主要集中于车辆调度或路径计划，这些方法通常简化了复杂的城市布局，并忽略了同时协调和互惠骑士之间的必要性。这种过度简化为在现实世界中的AMOD系统部署带来了重大挑战。为了解决这些差距，我们提出了CodriveVLM，这是一个新颖的框架，该框架集成了对未来AMOD系统的高保真性同时调度和合作运动计划。我们的方法利用视觉模型（VLM）来增强多模式信息处理，这可以进行全面的调度和碰撞风险评估。引入了VLM增强的CAV调度协调员，以有效地管理复杂和不可预见的AMOD条件，从而支持有效的计划决策。此外，我们通过共识交流方向方法（ADMM）提出了一种可扩展的分散合作运动计划方法，重点是碰撞风险评估和分散轨迹优化。仿真结果表明，在各种交通条件下，CodriveVLM的可行性和鲁棒性，展示了其潜力，以显着提高AMOD系统在未来的城市运输网络中的忠诚度和有效性。该代码可在https://github.com/henryhcliu/codrivevlm.git上找到。

### A Survey of World Models for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2501.11260)] [[cool](https://papers.cool/arxiv/2501.11260)] [[pdf](https://arxiv.org/pdf/2501.11260)]
> **Authors**: Tuo Feng,Wenguan Wang,Yi Yang
> **First submission**: 2025-01-19
> **First announcement**: 2025-01-20
> **comment**: Ongoing project
- **标题**: 对自动驾驶世界模型的调查
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 强大的世界建模方面的进步，从根本上改变了车辆如何解释动态场景并执行安全的决策。特别是，世界模型已成为一项Linchpin技术，提供了整合多传感器数据，语义提示和时间动态的驱动环境的高保真表示。本文系统地回顾了世界模型的自动驾驶模型的最新进展，提出了三层分类学：1）生成未来的物理世界，涵盖图像，BEV-，OG-，OG-和PC基于PC的生成方法，从而通过扩散模型和4D占用率预测来增强场景演变建模； 2）针对智能代理的行为规划，将基于规则驱动的范式和基于学习的范例与成本图优化和增强学习的轨迹学习相结合； 3）预测与计划之间的互动，通过潜在的空间扩散和内存增强体系结构实现多代理协作决策。该研究进一步分析了培训范式，包括自我监督学习，多模式预处理和生成数据增强，同时评估世界模型在场景理解和运动预测任务中的表现。未来的研究必须应对自我监督的表示学习，长尾场景产生和多模式融合的关键挑战，以推动复杂的城市环境中世界模型的实际部署。总体而言，我们的全面分析提供了一个理论框架和技术路线图，用于利用世界模型在推进安全可靠的自动驾驶解决方案方面的变革潜力。

### MCRL4OR: Multimodal Contrastive Representation Learning for Off-Road Environmental Perception 
[[arxiv](https://arxiv.org/abs/2501.13988)] [[cool](https://papers.cool/arxiv/2501.13988)] [[pdf](https://arxiv.org/pdf/2501.13988)]
> **Authors**: Yi Yang,Zhang Zhang,Liang Wang
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: Github repository: https://github.com/1uciusy/MCRL4OR
- **标题**: MCRL4OR：越野环境感知的多模式对比表示学习
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **摘要**: 大多数关于自动驾驶汽车（AV）环境感知的研究集中在城市交通环境上，其中要感知的物体/物品主要来自人造场景和具有密集注释的可扩展数据集，可用于培训监督的学习模型。相比之下，由于越野环境的固有非结构化性质，很难手动地注释大规模越野驾驶数据集。在本文中，我们提出了一种多模式的对比表示学习方法，即MCRL4OR。这种方法旨在通过将运动状态与视觉图像的融合和对比性学习框架内的视觉特征对齐，共同学习三个用于处理视觉图像，运动状态和控制动作的编码器。这种比对策略背后的因果关系是惯性的运动状态是在视觉传感器所感知的当前地形/地形条件下采取一定的控制措施的结果。在实验中，我们使用大规模越野驾驶数据集预先培训MCRL4OR，并在越野驾驶场景中采用学识渊博的多模式表示。下游任务的出色性能证明了预训练的多模式表示的优势。这些代码可以在\ url {https://github.com/1uciusy/mcrl4or}中找到。

### Diffusion-Based Planning for Autonomous Driving with Flexible Guidance 
[[arxiv](https://arxiv.org/abs/2501.15564)] [[cool](https://papers.cool/arxiv/2501.15564)] [[pdf](https://arxiv.org/pdf/2501.15564)]
> **Authors**: Yinan Zheng,Ruiming Liang,Kexin Zheng,Jinliang Zheng,Liyuan Mao,Jianxiong Li,Weihao Gu,Rui Ai,Shengbo Eben Li,Xianyuan Zhan,Jingjing Liu
> **First submission**: 2025-01-26
> **First announcement**: 2025-01-27
> **comment**: No comments
- **标题**: 通过灵活的指导，基于扩散的自动驾驶计划
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 在复杂的开放世界环境中实现类似人类的驾驶行为是自主驾驶的关键挑战。诸如模仿学习方法之类的现代学习计划方法通常很难平衡竞争目标和缺乏安全保证，这是因为适应性有限和学习在人类计划中通常表现出的复杂多模式行为的不足，而不是提及他们对倒退策略的强烈依赖。我们为闭环计划提出了一个新型的基于变压器的扩散计划者，该计划可以有效地对多模式驾驶行为进行建模，并确保轨迹质量而无需任何基于规则的改进。我们的模型支持在相同体系结构下对预测和计划任务的联合建模，从而实现了车辆之间的合作行为。此外，通过学习轨迹得分功能的梯度并采用灵活的分类器指导机制，扩散计划者有效地实现了安全且适应性的计划行为。对大型现实世界自主计划基准NUPLAN和我们新收集的200小时交付车辆驾驶数据集的评估表明，扩散计划者可以实现最先进的闭环性能，并在多样化的驾驶方式中具有可靠的可转移性。

### RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception 
[[arxiv](https://arxiv.org/abs/2501.16803)] [[cool](https://papers.cool/arxiv/2501.16803)] [[pdf](https://arxiv.org/pdf/2501.16803)]
> **Authors**: Lantao Li,Kang Yang,Wenqi Zhang,Xiaoxue Wang,Chen Sun
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: No comments
- **标题**: RG-ATTN：多模式多代理合作感的Radian Glue注意力
- **领域**: 机器人技术,计算机视觉和模式识别,网络和互联网架构,图像和视频处理
- **摘要**: 合作感知提供了一种最佳解决方案，可以通过利用车辆到所有（V2X）通信来克服单个系统系统的感知限制，以在多个代理之间进行数据共享和融合。但是，大多数现有的方法都集中在单模式数据交换上，从而限制了代理之间同质和异质融合的潜力。这忽略了每个代理使用多模式数据的机会，从而限制了系统的性能。在汽车行业中，制造商采用了不同的传感器配置，从而导致代理之间传感器方式的异质组合。为了利用每个可能数据源的最佳性能的潜力，我们设计了可靠的雷达和摄像头跨模式融合模块，辐射 - 胶合 - 注意事项（RG-attn），适用于内部跨模式融合和跨模式间交叉模式融合的场景，这是由于方便的互联性对转换的互联式转换和独立性的Samplied Samplied Samplied Samplied Samepried Samepried Samplied Samefient Same我们还提出了两种不同的架构，称为油漆对插头（PTP）和共同勾选-Co色（COS-Coco），以进行合作感知。 PTP的目的是通过将跨代理融合限制为单个实例，但要求所有参与者配备LIDAR，从而实现了较小的数据包大小。相比之下，COS-Coco用任何仅配置 -  lidar，仅摄像头或激光摄像机both支持代理，具有更多的泛化能力。我们的方法在真实和模拟的合作感知数据集上都实现了最新的（SOTA）性能。该代码将于2025年初在Github发布。

### Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation 
[[arxiv](https://arxiv.org/abs/2501.18733)] [[cool](https://papers.cool/arxiv/2501.18733)] [[pdf](https://arxiv.org/pdf/2501.18733)]
> **Authors**: Yuelei Li,Ge Yan,Annabella Macaluso,Mazeyu Ji,Xueyan Zou,Xiaolong Wang
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 集成LMM计划者和3D技能政策，以进行可推广的操纵
- **领域**: 机器人技术,人工智能
- **摘要**: 大型多模型（LMM）的视觉推理能力以及3D特征场的语义富集的最新进步已扩大了机器人能力的视野。这些发展具有巨大的潜力，可以利用3D特征领域的LMM和低级控制策略之间的高级推理之间的差距。在这项工作中，我们介绍了LMM-3DP，该框架可以集成LMM计划者和3D技能策略。我们的方法包括三个关键观点：高级计划，低级控制和有效的整合。对于高级计划，LMM-3DP支持对环境干扰的动态场景理解，具有自回馈，历史策略记忆的批评家，以及失败后的重新计算。对于低级控制，LMM-3DP使用语义感知的3D功能字段来进行准确的操作。在对机器人动作的高级和低水平控制状态时，代表高级策略的语言嵌入方式与3D变压器中的3D特征字段共同参加，以进行无缝集成。我们在现实世界中的厨房环境中广泛评估跨多种技能和长胜压任务的方法。我们的结果表明，与基于LLM的基线相比，低级控制的1.45倍成功率提高了高水平控制的高度提高，高级计划准确性的提高了约1.5倍。演示视频和LMM-3DP的概述可在https://lmm-3dp-release.github.io上找到。

## 声音(cs.SD:Sound)

该领域共有 11 篇论文

### MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music, and Musical Captions 
[[arxiv](https://arxiv.org/abs/2501.01094)] [[cool](https://papers.cool/arxiv/2501.01094)] [[pdf](https://arxiv.org/pdf/2501.01094)]
> **Authors**: Suhwan Choi,Kyu Won Kim,Myungjoo Kang
> **First submission**: 2025-01-02
> **First announcement**: 2025-01-03
> **comment**: Paper accepted in Artificial Intelligence for Music workshop at AAAI 2025
- **标题**: MMVA：基于价值和唤醒的多模式匹配，音乐和音乐标题
- **领域**: 声音,人工智能,多媒体,音频和语音处理
- **摘要**: 我们介绍了基于Valence和Ausal（MMVA）的多模式匹配，这是一个三模式编码器框架，旨在捕获跨图像，音乐和音乐标题的情感内容。为了支持此框架，我们扩展了图像 - 音乐 - 匹配网络（IMENKENT）数据集，创建Imemnet-C，其中包括24,756张图像和25,944个带有相应音乐字幕的音乐剪辑。我们基于连续价（情绪积极）和唤醒（情绪强度）值采用多模式匹配分数。这种连续的匹配分数可以通过计算来自不同模态的价值相似值的相似性得分来在训练过程中随机抽样图像 - 音乐对。因此，所提出的方法在价值预测任务中实现了最先进的表现。此外，该框架在各种Zeroshot任务中证明了其功效，突出了下游应用程序中价和唤醒预测的潜力。

### FaceSpeak: Expressive and High-Quality Speech Synthesis from Human Portraits of Different Styles 
[[arxiv](https://arxiv.org/abs/2501.03181)] [[cool](https://papers.cool/arxiv/2501.03181)] [[pdf](https://arxiv.org/pdf/2501.03181)]
> **Authors**: Tian-Hao Zhang,Jiawei Zhang,Jun Wang,Xinyuan Qian,Xu-Cheng Yin
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: 面对面：人类肖像的富有表现力和高质量的语音综合
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 人类可以通过外表来感知说话者的特征（例如身份，性别，个性和情感），这通常与他们的声音风格保持一致。最近，视觉驱动的文本到语音（TTS）学者将他们的调查扎根于真实的面孔，从而限制了有效的语音综合，将其应用于具有不同角色和图像样式的巨大潜在用法场景。为了解决这个问题，我们介绍了一种新颖的面孔方法。它从多种图像样式中提取出显着的身份特征和情感表征。同时，它减轻了无关信息（例如背景，衣服和发色等），从而导致综合语音与角色的角色紧密相符。此外，为了克服多模式TTS数据的稀缺性，我们设计了一个创新的数据集，即表达性的多模式TT，该数据集经过精心策划并注释以促进该域中的研究。实验结果表明，我们提出的面孔可以以令人满意的自然性和质量产生与肖像的声音。

### Sanidha: A Studio Quality Multi-Modal Dataset for Carnatic Music 
[[arxiv](https://arxiv.org/abs/2501.06959)] [[cool](https://papers.cool/arxiv/2501.06959)] [[pdf](https://arxiv.org/pdf/2501.06959)]
> **Authors**: Venkatakrishnan Vaidyanathapuram Krishnan,Noel Alben,Anish Nair,Nathaniel Condit-Schultz
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: Accepted to the 25th International Society for Music Information Retrieval Conference (ISMIR 2024)
- **标题**: Sanidha：摄影棚质量的carnatic音乐多模式数据集
- **领域**: 声音,数字图书馆,机器学习,音频和语音处理
- **摘要**: 音乐源分离将一段音乐插入其单个声音源（人声，打击乐，旋律乐器等），这是一个没有简单数学解决方案的任务。它需要深入学习方法，涉及在孤立音乐茎的大型数据集上进行培训。最常见的数据集是由商业西方音乐制成的，将模型的应用程序限制在诸如Carnatic Music之类的非西方类型上。 Carnatic Music是一种现场传统，具有可用的多轨录音，其中包含重叠的声音和来源之间的流血。这对Spleeter和Hybrid Demucs（例如Spleeter和Hybrid Demucs）构成了挑战。在这项工作中，我们介绍了“ Sanidha”，这是第一本用于Carnatic音乐的开源小说数据集，提供工作室质量，多轨录音，最小化至没有重叠或流血。除了音频文件外，我们还提供了艺术家表演的高清视频。此外，与先前存在的Carnatic多轨数据集中的微调相比，我们在数据集中微调了Spleeter，这是最常用的源分离模型之一，并观察到了SDR性能的改善。通过聆听研究评估了使用“ Sanidha”的微调模型的输出。

### Unispeaker: A Unified Approach for Multimodality-driven Speaker Generation 
[[arxiv](https://arxiv.org/abs/2501.06394)] [[cool](https://papers.cool/arxiv/2501.06394)] [[pdf](https://arxiv.org/pdf/2501.06394)]
> **Authors**: Zhengyan Sheng,Zhihao Du,Heng Lu,Shiliang Zhang,Zhen-Hua Ling
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: Unispeaker：多模式驱动扬声器的统一方法
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 个性化语音产生的最新进展使综合言语越来越接近目标说话者唱片的现实主义，但多模式的演讲者的生成仍在上升。本文介绍了Unispeaker，这是一种统一的多模式驱动扬声器生成的方法。具体来说，我们提出了一个基于KV形式的统一语音聚合器，将软性对比损失应用于映射多样的语音描述模式中，以确保生成的语音与输入描述更紧密地对齐。为了评估多模式驱动的语音控制，我们构建了第一个基于多模式的语音控制（MVC）基准，重点是语音适用性，语音多样性和语音质量。使用MVC基准对Unispeaker进行了五个任务评估，实验结果表明，Unispeaker的表现优于先前的模式特异性模型。语音样本可在\ url {https://unispeaker.github.io}中找到。

### CognoSpeak: an automatic, remote assessment of early cognitive decline in real-world conversational speech 
[[arxiv](https://arxiv.org/abs/2501.05755)] [[cool](https://papers.cool/arxiv/2501.05755)] [[pdf](https://arxiv.org/pdf/2501.05755)]
> **Authors**: Madhurananda Pahar,Fuxiang Tao,Bahman Mirheidari,Nathan Pevy,Rebecca Bright,Swapnil Gadgil,Lise Sproson,Dorota Braun,Caitlin Illingworth,Daniel Blackburn,Heidi Christensen
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: This paper has been accepted for publication in IEEE SSCI 2025. Copyright belongs to IEEE
- **标题**: CognoSpeak：自动远程评估现实世界中的早期认知能力下降
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 认知能力下降的早期迹象通常在对话言语中很明显，并且确定这些迹象对于处理后来和更严重的神经退行性疾病阶段至关重要。临床检测是昂贵且耗时的，尽管在自动检测基于语音的提示方面的最新进展，但这些系统在相对较小的数据库上进行了培训，缺乏详细的元数据和人口统计信息。本文介绍了CognoSpeak及其相关的数据收集工作。 CognoSpeak询问内存的长期和短期问题，并管理标准的认知任务，例如使用移动或网络平台上虚拟代理的语言和语义流利度以及图片描述。此外，它还收集了多模式数据，例如音频和视频，以及来自初级和二级护理，记忆诊所以及人们（如人们之家）的丰富元数据。在这里，我们介绍了126位主题的结果，他们的音频是手动转录的。已经对不同类型的提示进行了研究和评估，已经对几种经典分类器以及基于语言模型的大型分类器进行了研究。我们表现​​出高度的性能；特别是，我们使用Distilbert模型实现了0.873的F1得分，以区分患有认知障碍的人（痴呆症和轻度认知障碍者（MCI）），使用健康志愿者使用记忆力，流利的任务和Cookie Teaft Teaft图片描述。 CognoSpeak是一种自动，遥远，低成本，可重复，无创和压力较小的替代品，可替代现有的临床认知评估。

### A Non-autoregressive Model for Joint STT and TTS 
[[arxiv](https://arxiv.org/abs/2501.09104)] [[cool](https://papers.cool/arxiv/2501.09104)] [[pdf](https://arxiv.org/pdf/2501.09104)]
> **Authors**: Vishal Sunder,Brian Kingsbury,George Saon,Samuel Thomas,Slava Shechtman,Hagai Aronowitz,Eric Fosler-Lussier,Luis Lastras
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: 5 pages, 3 figures, 3 tables
- **标题**: 连接STT和TTS的非自动入学模型
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 在本文中，我们朝着以完全非自动性的方式迈出了共同建模自动语音识别（STT）和语音合成（TTS）。我们开发了一个新颖的多模式框架，能够单独或一起处理语音和文本模式。提出的模型也可以通过不成对的语音或文本数据进行培训，这是由于其多模式性质。我们进一步提出了一种迭代改进策略，以改善模型的STT和TTS性能，以便可以将输出的部分假设馈回我们的模型的输入，从而迭代地改善STT和TTS预测。我们表明，我们的联合模型可以有效执行STT和TTS任务，在所有任务中都优于特定于STT的基线，并在广泛的评估指标中与TTS特定基线进行竞争性执行。

### XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework 
[[arxiv](https://arxiv.org/abs/2501.08809)] [[cool](https://papers.cool/arxiv/2501.08809)] [[pdf](https://arxiv.org/pdf/2501.08809)]
> **Authors**: Sida Tian,Can Zhang,Wei Yuan,Wei Tan,Wenjie Zhu
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: accepted by TMM
- **标题**: Xmusic：迈向广义和可控的符号音乐生成框架
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 近年来，在图像综合和文本生成的领域已经实现了人工智能生成的内容（AIGC）的显着进步，从而产生与人类产生的内容相当的内容。但是，AI生成的音乐的质量尚未达到这一标准，这主要是由于有效控制音乐情绪和确保高质量输出的挑战。本文介绍了一个广义的符号音乐生成框架Xmusic，该框架支持灵活的提示（即图像，视频，文本，标签和嗡嗡声），以产生情感控制和高质量的符号音乐。 Xmusic由两个核心组件组成，分别是Xprojector和Xcomposer。 Xprojector在投影空间内将各种模式的提示解析为符号音乐元素（即情绪，流派，节奏和音符），以产生匹配的音乐。 Xcomposer包含一个发电机和一个选择器。发电机基于我们的创新符号音乐表示产生情感控制和悠扬的音乐，而选择者通过构建涉及质量评估，情感识别和流派识别任务的多任务学习方案来识别高质量的符号音乐。此外，我们构建了Xmidi，这是一个大规模的符号音乐数据集，其中包含108,023个MIDI文件，以精确的情感和流派标签注释。客观和主观评估表明，Xmusic显着优于具有令人印象深刻的音乐质量的当前最新方法。我们的Xmusic被授予2023年WAIC的九个收藏品之一。

### Bridging The Multi-Modality Gaps of Audio, Visual and Linguistic for Speech Enhancement 
[[arxiv](https://arxiv.org/abs/2501.13375)] [[cool](https://papers.cool/arxiv/2501.13375)] [[pdf](https://arxiv.org/pdf/2501.13375)]
> **Authors**: Meng-Ping Lin,Jen-Cheng Hou,Chia-Wei Chen,Shao-Yi Chien,Jun-Cheng Chen,Xugang Lu,Yu Tsao
> **First submission**: 2025-01-22
> **First announcement**: 2025-01-23
> **comment**: No comments
- **标题**: 弥合音频，视觉和语言的多模式差距，以增强语音
- **领域**: 声音,机器学习,多媒体,音频和语音处理
- **摘要**: 语音增强（SE）旨在提高嘈杂的语音质量。已经表明，其他视觉提示可以进一步提高性能。鉴于语音交流涉及音频，视觉和语言方式，因此可以通过合并语言信息来期望另一种表现提高是很自然的。但是，桥接模式差距以有效地结合语言信息，以及在知识传递期间的音频和视觉方式，这是一项艰巨的任务。在本文中，我们为SE提出了一个新型的多模式学习框架。在模型框架中，将最新的扩散模型主链用于视听语音增强（AVSE）建模，其中音频和视觉信息都是由麦克风和摄像机直接捕获的。基于此AVSE，语言模式采用PLM通过在AVSE模型训练期间通过称为跨模式知识传递（CMKT）的过程将语言知识传递到视觉声学方式。训练模型后，认为语言知识是由CMKT在AVSE模型的特征处理中编码的，并且在推理阶段将不参与PLM。我们进行SE实验以评估所提出的模型框架。实验结果表明，我们提出的AVSE系统显着提高了语音质量并减少了与最新艺术相比的语音混乱。此外，我们的可视化结果表明，我们的跨模式知识转移方法进一步提高了我们的AVSE系统的生成语音质量。这些发现不仅表明，基于扩散模型的技术有望推进AVSE中最新的技术，而且还证明了合并语言信息以改善基于扩散的AVSE系统的性能的有效性。

### Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak 
[[arxiv](https://arxiv.org/abs/2501.13772)] [[cool](https://papers.cool/arxiv/2501.13772)] [[pdf](https://arxiv.org/pdf/2501.13772)]
> **Authors**: Erjia Xiao,Hao Cheng,Jing Shao,Jinhao Duan,Kaidi Xu,Le Yang,Jindong Gu,Renjing Xu
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: No comments
- **标题**: 调整，采取行动：探索特定于音频方式的编辑对越狱中大型音频语言模型的影响
- **领域**: 声音,人工智能,机器学习,多媒体,音频和语音处理
- **摘要**: 大型语言模型（LLMS）在各种自然语言处理任务中表现出了显着的零拍摄性能。多模式编码器的集成扩展了它们的功能，从而可以开发多模式的大型语言模型，以处理视觉，音频和文本。但是，这些功能也引起了重大的安全问题，因为可以操纵这些模型以通过越狱产生有害或不适当的内容。虽然广泛的研究探讨了特定于模式的输入编辑对基于文本的LLM和越狱中大型视觉模型的影响，但音频特定编辑对大型音频语言模型（LALMS）的影响仍未得到充实。因此，本文通过调查了特定于音频的编辑如何影响LALMS关于越狱的推论，从而解决了这一差距。我们介绍了音频编辑工具箱（AET），该工具箱可以使音频模式编辑，例如调整音调，词强调和噪声喷射，以及编辑的音频数据集（EADS），这是一种全面的音频越狱基准。我们还对最先进的LALM进行了广泛的评估，以评估其在不同音频编辑下的鲁棒性。这项工作为LALMS安全中的音频模式相互作用的未来探索奠定了基础。

### Audio Large Language Models Can Be Descriptive Speech Quality Evaluators 
[[arxiv](https://arxiv.org/abs/2501.17202)] [[cool](https://papers.cool/arxiv/2501.17202)] [[pdf](https://arxiv.org/pdf/2501.17202)]
> **Authors**: Chen Chen,Yuchen Hu,Siyin Wang,Helin Wang,Zhehuai Chen,Chao Zhang,Chao-Han Huck Yang,Eng Siong Chng
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-29
> **comment**: ICLR 2025
- **标题**: 音频大语模型可以是描述性语音质量评估者
- **领域**: 声音,计算语言学,音频和语音处理
- **摘要**: 理想的多模式代理应了解其输入方式的质量。最近的进步使大型语言模型（LLMS）能够合并用于处理各种语音相关任务的听觉系统。但是，大多数音频LLM仍未意识到它们正在处理的演讲的质量。由于缺乏合适的数据集而，这种限制之所以出现，是因为语音质量评估通常被排除在多任务培训之外。为了解决这个问题，我们介绍了由真实人类评级产生的第一个基于自然语言的语音评估语料库。除了整体平均意见分数（MOS）外，该语料库还提供了跨多个维度的详细分析，并确定了质量退化的原因。它还可以在两个语音样本（A/B测试）和类似人类的判断的两个语音样本（A/B测试）之间进行描述性比较。利用该语料库，我们提出了一种与LLM蒸馏（ALLD）的对齐方法，以指导音频LLM从原始语音中提取相关信息并产生有意义的响应。实验结果表明，ALLD在MOS预测中的表现优于先前的最新回归模型，均为0.17，A/B测试精度为98.6％。此外，生成的响应在两个任务上达到了25.8和30.2的BLEU得分，超过了特定于任务模型的功能。这项工作推动了音频LLM对语音信号的全面感知，这有助于实际听觉和感官智能代理的发展。

### Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training and Unimodal Deployment 
[[arxiv](https://arxiv.org/abs/2501.18157)] [[cool](https://papers.cool/arxiv/2501.18157)] [[pdf](https://arxiv.org/pdf/2501.18157)]
> **Authors**: Joanna Hong,Sanjeel Parekh,Honglie Chen,Jacob Donley,Ke Tan,Buye Xu,Anurag Kumar
> **First submission**: 2025-01-30
> **First announcement**: 2025-01-31
> **comment**: No comments
- **标题**: 通过Mutud有效的有效语音处理：多模式训练和单峰部署
- **领域**: 声音,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 构建可靠的语音系统通常需要结合多种方式，例如音频和视觉提示。尽管这种多模式解决方案经常导致性能的提高，甚至在某些情况下可能至关重要，但它们具有多种限制，例如增加的感觉要求，计算成本和模态同步，可以提及一些限制。这些挑战限制了这些多模式解决方案在实际应用中的直接使用。在这项工作中，我们开发了学习方法以所有可用方式发生的方法，但是部署或推理仅以一种或减少的方式完成。为此，我们提出了一个多模式训练和单峰部署（MUTUD）框架，其中包括一个时间对齐的模态特征估计（TAME）模块，该模块可以使用推理过程中存在的模态来估算缺失模态的信息。这种创新的方法促进了跨不同方式的信息的整合，从而通过利用每种方式的优势来补偿推断期间缺乏某些方式来增强整体推理过程。我们将mutud应用于各种视听语音任务，并表明它可以在相当大的程度上减少多模式和相应的单峰模型之间的性能差距。与多模型相比，Mutud可以实现这一目标，同时减少模型和计算，在某些情况下，将几乎80％。

## 软件工程(cs.SE:Software Engineering)

该领域共有 2 篇论文

### Are GNNs Actually Effective for Multimodal Fault Diagnosis in Microservice Systems? 
[[arxiv](https://arxiv.org/abs/2501.02766)] [[cool](https://papers.cool/arxiv/2501.02766)] [[pdf](https://arxiv.org/pdf/2501.02766)]
> **Authors**: Fei Gao,Ruyue Xin,Xiaocui Li,Yaqiang Zhang
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: 6 pages, 5 figures, submitted to conference
- **标题**: GNN实际上对微服务系统中的多模式断层诊断有效吗？
- **领域**: 软件工程,人工智能
- **摘要**: 图形神经网络（GNN）被广泛用于微服务系统中的故障诊断，其前提是其对服务依赖性建模的能力。但是，随着现有评估将预处理与建筑贡献相结合，显式图结构的必要性仍然不受欢迎。为了隔离GNN的真实价值，我们提出了DiagMLP，这是一种故意最小的，拓扑 - 不合骨的基线，在不包括图形建模的同时保留了多模式融合功能。通过五个数据集的消融实验，DIAGMLP通过基于最新的GNN方法在故障检测，定位和分类中实现了绩效奇偶校验。这些发现挑战了图形结构是必不可少的普遍假设，表明：（i）预处理管道已经编码关键的依赖性信息，并且（ii）GNN模块超出了多模构融合的差距。我们的工作提倡系统地重新评估建筑复杂性，并强调了对标准化基线协议的需求，以验证模型创新。

### Enhancing Web Service Anomaly Detection via Fine-grained Multi-modal Association and Frequency Domain Analysis 
[[arxiv](https://arxiv.org/abs/2501.16875)] [[cool](https://papers.cool/arxiv/2501.16875)] [[pdf](https://arxiv.org/pdf/2501.16875)]
> **Authors**: Xixuan Yang,Xin Huang,Chiming Duan,Tong Jia,Shandong Dong,Ying Li,Gang Huang
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: Accepted by WWW' 25
- **标题**: 通过细粒的多模式关联和频域分析增强Web服务异常检测
- **领域**: 软件工程,机器学习
- **摘要**: 异常检测对于确保Web服务系统的稳定性和可靠性至关重要。日志和指标包含多个可以反映系统的操作状态和潜在异常的信息。因此，现有的异常检测方法使用日志和指标通过数据融合方法来检测Web服务系统的异常。他们使用粗粒度的时间窗口对齐将日志和指标关联，并通过重建捕获系统操作的正常模式。但是，这些方法有两个问题，这些问题限制了它们在异常检测中的性能。首先，由于对数和指标之间的异步，粗粒的时间窗口对齐无法达到两种方式之间的精确关联。其次，基于重建的方法遭受严重的过度概括性问题，导致异常重建。在本文中，我们提出了一种名为FFAD的新型异常检测方法来解决这两个问题。一方面，FFAD采用基于图形的对齐方式来挖掘和提取从构造的对数 - 金属关系图的模态之间的关联，从而实现了日志和指标之间的精确关联。另一方面，我们通过傅立叶频率焦点提高了模型与正常数据分布的拟合度，从而提高了异常检测的有效性。我们验证了模型对两个实际工业数据集和一个开源数据集的有效性。结果表明，我们的方法达到的平均异常检测F1评分为93.6％，比以前的最新方法提高了8.8％。

## 社交和信息网络(cs.SI:Social and Information Networks)

该领域共有 1 篇论文

### A Heterogeneous Multimodal Graph Learning Framework for Recognizing User Emotions in Social Networks 
[[arxiv](https://arxiv.org/abs/2501.07746)] [[cool](https://papers.cool/arxiv/2501.07746)] [[pdf](https://arxiv.org/pdf/2501.07746)]
> **Authors**: Sree Bhattacharyya,Shuhua Yang,James Z. Wang
> **First submission**: 2025-01-13
> **First announcement**: 2025-01-14
> **comment**: No comments
- **标题**: 一个识别社交网络中用户情绪的异质多模式图学习框架
- **领域**: 社交和信息网络,计算语言学,计算机视觉和模式识别
- **摘要**: 社交媒体平台的迅速扩展为空前访问了大量的多模式用户生成的内容。理解用户情绪可以提供有价值的见解，以改善对人类行为的沟通和理解。尽管情感计算方面取得了重大进步，但影响社交网络中用户情绪的各种因素仍然相对研究。此外，明显缺乏基于深度学习的方法来预测社交网络中的用户情绪，可以通过利用可用的广泛的多模式数据来解决这些方法。这项工作为基于异质图学习的社交网络中个性化情绪预测提供了一种新颖的表述。在此公式的基础上，我们设计了HMG-Emo，这是一个异质的多模式学习框架，它利用基于深度学习的功能来实现用户情感识别。此外，我们在HMG-EMO中包括一个动态上下文融合模块，该模块能够自适应地整合社交媒体数据中的不同模式。通过广泛的实验，我们证明了HMG-EMO的有效性，并验证采用基于图神经网络的方法的优越性，该方法的表现优于使用丰富的手工制作功能的现有基准。据我们所知，HMG-EMO是第一种基于多模式和深度学习的方法，可以预测在线社交网络中的个性化情绪。我们的工作强调了利用先进的深度学习技术来解决情感计算中较少探索的问题的重要性。

## 普通经济学(econ.GN:General Economics)

该领域共有 1 篇论文

### Adventures in Demand Analysis Using AI 
[[arxiv](https://arxiv.org/abs/2501.00382)] [[cool](https://papers.cool/arxiv/2501.00382)] [[pdf](https://arxiv.org/pdf/2501.00382)]
> **Authors**: Philipp Bach,Victor Chernozhukov,Sven Klaassen,Martin Spindler,Jan Teichert-Kluge,Suhas Vijaykumar
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-03
> **comment**: 42 pages, 9 figures
- **标题**: 使用AI的冒险分析中的冒险
- **领域**: 普通经济学,人工智能,应用领域,机器学习
- **摘要**: 本文通过整合从人工智能（AI）的多模式产品表示来推进经验需求分析。使用\ textit {Amazon.com}上的玩具车的详细数据集，我们将文本描述，图像和表格协变量组合在一起，使用基于变压器的嵌入式模型来表示每种产品。这些嵌入捕获了细微的属性，例如质量，品牌和视觉特征，传统方法通常很难总结。此外，我们将这些嵌入为因果推理任务。我们表明，由此产生的嵌入会大大提高销售排名和价格的预测准确性，并提高了价格弹性的更可靠的因果关系估计。值得注意的是，我们发现了这些特定产品功能驱动的价格弹性的强大异质性。我们的发现表明，AI驱动的表示可以丰富和现代化经验需求分析。产生的见解也可能证明对应用的因果推断有价值。

## 音频和语音处理(eess.AS:Audio and Speech Processing)

该领域共有 1 篇论文

### Developing Enhanced Conversational Agents for Social Virtual Worlds 
[[arxiv](https://arxiv.org/abs/2501.16341)] [[cool](https://papers.cool/arxiv/2501.16341)] [[pdf](https://arxiv.org/pdf/2501.16341)]
> **Authors**: D. Griol,A. Sanchis,J. M. Molina,Z. Callejas
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-28
> **comment**: Neurocomputing 2019
- **标题**: 为社会虚拟世界开发增强的对话代理
- **领域**: 音频和语音处理,计算语言学,声音
- **摘要**: 在本文中，我们提出了一种开发社会虚拟世界体现对话代理的方法。代理商与其用户提供多模式通信，其中包括语音交互。我们的建议结合了与人工智能，自然语言处理，情感计算和用户建模有关的不同技术。首先，开发的对话代理。已经开发了一种统计方法来对系统对话行为进行建模，该方法是从初始语料库中学到的，并通过从连续的相互作用中获得的知识进行了改进。此外，考虑存储在用户配置文件中的信息以及用户话语中检测到的情感内容，对下一个系统响应的选择进行了调整。我们的提议已通过成功地发展在第二人生社会虚拟世界中的体现对话剂的成功开发中进行了评估。头像包括不同的模型，并与居住在虚拟世界中的用户进行交互，以提供学术信息。实验结果表明，代理商对话行为成功地适应了在这种环境中交互的用户的特定特征。

## 图像和视频处理(eess.IV:Image and Video Processing)

该领域共有 17 篇论文

### Ultrasound-QBench: Can LLMs Aid in Quality Assessment of Ultrasound Imaging? 
[[arxiv](https://arxiv.org/abs/2501.02751)] [[cool](https://papers.cool/arxiv/2501.02751)] [[pdf](https://arxiv.org/pdf/2501.02751)]
> **Authors**: Hongyi Miao,Jun Jia,Yankun Cao,Yingjie Zhou,Yanwei Jiang,Zhi Liu,Guangtao Zhai
> **First submission**: 2025-01-05
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 超声QBench：LLM可以帮助对超声成像的质量评估？
- **领域**: 图像和视频处理,计算机视觉和模式识别,多媒体
- **摘要**: 随着超声检查量的急剧增长，由于操作员的熟练程度和成像情况的变化，低质量的超声成像逐渐增加，从而严重负担诊断准确性，甚至可能在关键病例中重新诊断。为了帮助临床医生选择高质量的超声图像并确保准确的诊断，我们引入了超声QBench，这是一种全面的基准，该基准系统地评估了超声图像的质量评估任务多模式大语言模型（MLLMS）。 Ultrasound-Qbench建立了从不同来源收集的两个数据集：IVUSQA，由7,709张图像和心脏硫酸心脏组成，包含3,863张图像。这些图像包含常见的超声成像伪像，由专业超声专家注释，分为三个质量：高，中和低。为了更好地评估MLLM，我们将质量评估任务分解为三个维度：定性分类，定量评分和比较评估。对7个开源MLLM和1个专有MLLM的评估表明，在超声图像质量分类中，MLLM具有低级视觉任务的初步功能。我们希望该基准将激发研究界更深入地研究和增强MLLM在医学成像任务中的未开发潜力。

### Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI 
[[arxiv](https://arxiv.org/abs/2501.02287)] [[cool](https://papers.cool/arxiv/2501.02287)] [[pdf](https://arxiv.org/pdf/2501.02287)]
> **Authors**: Ashiqur Rahman,Muhammad E. H. Chowdhury,Md Sharjis Ibne Wadud,Rusab Sarmun,Adam Mushtak,Sohaib Bassam Zoghoul,Israa Al-Hashimi
> **First submission**: 2025-01-04
> **First announcement**: 2025-01-06
> **comment**: No comments
- **标题**: 使用多通道MRI对缺血性中风病变进行深度学习驱动的分割
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 由脑血管阻塞引起的缺血性中风，由于中风病变的可变性和微妙，在医学成像中提出了重大挑战。磁共振成像（MRI）在诊断和管理缺血性中风中起着至关重要的作用，但是现有的分割技术通常无法准确描述病变。这项研究介绍了一种基于深度学习的新方法，用于使用多通道MRI模态分割缺血性中风病变，包括扩散加权成像（DWI），明显的扩散系数（ADC）和增强的扩散加权成像（EDWI）。所提出的体系结构将Densenet121作为编码器与解码器中的自组织操作神经网络（SEXONN）集成在一起，通过通道和空间复合注意（CSCA）和双重挤压和激发（DSE）块增强。此外，引入了将骰子损失和JACCARD损失与加权平均值结合起来的自定义损失函数，以提高模型性能。在Isles 2022数据集上进行了训练和评估，单独使用DWI实现了骰子相似性系数（DSC）为83.88％，DWI和ADC的骰子相似性系数（DSC）为85.86％，而DWI，ADC和EDWI的集成为87.49％。这种方法不仅胜过现有方法，而且还解决了当前细分实践中的关键局限性。这些进步大大提高了缺血性中风的诊断精度和治疗计划，为临床决策提供了宝贵的支持。

### FgC2F-UDiff: Frequency-guided and Coarse-to-fine Unified Diffusion Model for Multi-modality Missing MRI Synthesis 
[[arxiv](https://arxiv.org/abs/2501.03526)] [[cool](https://papers.cool/arxiv/2501.03526)] [[pdf](https://arxiv.org/pdf/2501.03526)]
> **Authors**: Xiaojiao Xiao,Qinmin Vivian Hu,Guanghui Wang
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: ef:IEEE Transactions on Computational Imaging, 2024
- **标题**: FGC2F-UDIFF：用于多模式的频率引导和粗到1的统一扩散模型缺失MRI合成
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 多模式磁共振成像（MRI）对于诊断和治疗脑肿瘤至关重要。但是，由于扫描时间的限制，扫描腐败，伪影，运动和对比剂不耐受，通常观察到缺失的方式。缺失MRI的综合是解决临床实践和研究中形态不足的局限性的一种手段。但是，仍然存在一些挑战，例如不良的概括，非线性映射不准确以及处理速度缓慢。为了解决上述问题，我们提出了一种新型的统一合成模型，即频率引导和粗到1的统一扩散模型（FGC2F-UDIFF），该模型旨在多种输入和输出。具体而言，通过将非全局的扩散模型的迭代授予模型的迭代性化性能完全利用了通过将剥离过程划分为两个阶段，以提高合成图像的保真度，从而充分利用了扩散模型的迭代deno授予性能。其次，频率引导的协作策略（FCS）将适当的频率信息作为先验知识来指导学习统一的，高度非线性的映射。第三，特异性加速杂种机制（SHM）整合了特定机制，以加速扩散模型并增强多到许多合成的可行性。广泛的实验评估表明，我们提出的FGC2F-UDIFF模型在两个数据集上实现了出色的性能，通过全面评估验证，包括定性观察和定量指标，例如PSNR SSIM，LPIPS和FID。

### Salient Region Matching for Fully Automated MR-TRUS Registration 
[[arxiv](https://arxiv.org/abs/2501.03510)] [[cool](https://papers.cool/arxiv/2501.03510)] [[pdf](https://arxiv.org/pdf/2501.03510)]
> **Authors**: Zetian Feng,Dong Ni,Yi Wang
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: 全自动MR-TRUS注册的显着区域匹配
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 前列腺癌是男性与癌症相关死亡率的主要原因。磁共振（MR）和直肠超声（TRU）的注册可以为前列腺癌的靶向活检提供指导。在这项研究中，我们提出了一个明显的区域匹配框架，以进行全自动的MR-TRUS登记。该框架包括前列腺细分，刚性对准和可变形的注册。使用MR和TRU上的两个分割网络进行前列腺分割，并将预测的显着区域用于刚性比对。刚性的MR和TRUS图像是可变形登记的初始化。可变形的注册网络具有带有跨模式空间注意模块的双流编码器，可促进多模式特征学习，并具有显着的区域匹配损失，以考虑前列腺区域内的结构和强度相似性。公共MR-TRUS数据集进行的实验表明，我们的方法可实现令人满意的注册结果，表现优于几种尖端方法。该代码可在https://github.com/mock1ngbrd/salient-gregion-matching上公开获取。

### ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction 
[[arxiv](https://arxiv.org/abs/2501.02778)] [[cool](https://papers.cool/arxiv/2501.02778)] [[pdf](https://arxiv.org/pdf/2501.02778)]
> **Authors**: Binyu Zhang,Zhu Meng,Junhao Dong,Fei Su,Zhicheng Zhao
> **First submission**: 2025-01-06
> **First announcement**: 2025-01-07
> **comment**: No comments
- **标题**: ICFNET：用于生存预测的综合跨模式融合网络
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 生存预测是医学领域的至关重要的任务，对于优化治疗方案和资源分配至关重要。但是，当前方法通常依赖于有限的数据模式，从而导致次优性能。在本文中，我们提出了一个综合的跨模式融合网络（ICFNET），该网络整合了组织病理学全幻灯片图像，基因组表达谱，患者人口统计和治疗方案。具体而言，使用三种类型的编码器，一个残留的正交分解模块和统一融合模块合并多模式特征，以增强预测准确性。此外，平衡的负模样损失函数旨在确保对不同患者进行公平培训。广泛的实验表明，我们的ICFNET在五个公共TCGA数据集上的最先进算法（包括BLCA，BRCA，GBMLGG，LUAD和UCEC）优于最先进的算法，并显示了其支持临床决策和预先精确药物的潜力。这些代码可在以下网址提供：https：//github.com/binging512/icfnet。

### Deep Learning for Ophthalmology: The State-of-the-Art and Future Trends 
[[arxiv](https://arxiv.org/abs/2501.04073)] [[cool](https://papers.cool/arxiv/2501.04073)] [[pdf](https://arxiv.org/pdf/2501.04073)]
> **Authors**: Duy M. H. Nguyen,Hasan Md Tusfiqur Alam,Tai Nguyen,Devansh Srivastav,Hans-Juergen Profitlich,Ngan Le,Daniel Sonntag
> **First submission**: 2025-01-07
> **First announcement**: 2025-01-08
> **comment**: First version
- **标题**: 眼科深度学习：最新的和未来的趋势
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 人工智能（AI）的出现，尤其是深度学习（DL），标志着眼科领域的一个新时代，为诊断和治疗后段眼部疾病提供了变革性的潜力。这篇综述探讨了DL在各种眼部疾病中的尖端应用，包括糖尿病性视网膜病，青光眼，与年龄相关的黄斑变性和视网膜血管分割。我们提供了基础ML技术和高级DL体系结构的全面概述，例如CNN，注意机制和基于变压器的模型，突出了AI在增强诊断准确性，优化治疗策略和改善整体患者护理方面不断发展的作用。此外，我们提出了将AI解决方案集成到临床实践中的关键挑战，包括确保数据多样性，提高算法透明度以及有效利用多模式数据。这篇综述强调了AI可以改善疾病诊断并改善患者护理的潜力，同时强调协作努力以克服这些障碍和完全利用AI在促进眼保健方面的影响的重要性。

### A Multi-Modal Deep Learning Framework for Pan-Cancer Prognosis 
[[arxiv](https://arxiv.org/abs/2501.07016)] [[cool](https://papers.cool/arxiv/2501.07016)] [[pdf](https://arxiv.org/pdf/2501.07016)]
> **Authors**: Binyu Zhang,Shichao Li,Junpeng Jian,Zhu Meng,Limei Guo,Zhicheng Zhao
> **First submission**: 2025-01-12
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 一个多模式的深度学习框架，用于泛伴奏预后
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 预后任务非常重要，因为它与患者的生存分析，治疗计划的优化和资源分配密切相关。现有的预后模型在特定数据集上显示出令人鼓舞的结果，但是有两个方面存在局限性。一方面，他们仅探索某些类型的模态数据，例如患者组织病理学WSI和基因表达分析。另一方面，他们采用了每种型号的范式，这意味着训练有素的模型只能预测单一类型的癌症的预后效应，从而导致概括能力较弱。在本文中，提出了一个名为Umpsnet的深度学习模型。具体而言，除了分别为组织病理学图像和基因组表达谱的编码器构建编码器外，UMPSNet还将四种类型的重要元数据（人口统计信息，癌症类型信息，治疗方案和诊断结果）整合到文本模板中，然后将文本编码用于提取文本特征。此外，基于OT的最佳转运注意机制可用于对齐不同模态的特征。此外，引入了指导的专家（GMOE）机制的软混合物，以有效解决多个癌症数据集之间的分布差异问题。通过纳入患者数据和联合培训的多模式，UMPSNET的表现优于所有SOTA方法，此外，它证明了单个模型对多种癌症类型的拟议学习范式的有效性和概括能力。 UMPSNET的代码可在https://github.com/binging512/umpsnet上获得。

### Joint Transmission and Deblurring: A Semantic Communication Approach Using Events 
[[arxiv](https://arxiv.org/abs/2501.09396)] [[cool](https://papers.cool/arxiv/2501.09396)] [[pdf](https://arxiv.org/pdf/2501.09396)]
> **Authors**: Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: No comments
- **标题**: 联合传输和脱毛：使用事件的语义通信方法
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 基于深度学习的联合源通道编码（JSCC）正在作为有效图像传输的有前途的技术。但是，大多数现有的方法都集中在传输清晰的图像上，忽略了现实世界中的挑战，例如摄像机摇动或快速移动对象引起的运动模糊。运动模糊通常会降低图像质量，从而使传输和重建更具挑战性。事件摄像机异步记录像素强度随着极低的延迟而变化，它显示出运动去膨胀任务的巨大潜力。但是，事件摄像机生成的丰富数据的有效传输仍然是一个重大挑战。在这项工作中，我们提出了一个新型的JSCC框架，用于模糊图像和事件的联合传输，旨在在有限的通道带宽下实现高质量的重建。该方法被设计为脱蓝色的任务导向的JSCC系统。由于RGB摄像机和事件摄像头通过不同的方式捕获相同的场景，因此它们的输出包含共享和域特异性信息。为了避免反复传输共享信息，我们分别提取和传输其共享信息和特定于域的信息。在接收器上，接收的信号由Deblurring解码器处理以生成清晰的图像。此外，我们引入了多阶段培训策略来培训所提出的模型。仿真结果表明，我们的方法显着胜过现有的基于JSCC的图像传输方案，从而有效地解决了运动模糊。

### WaveNet-SF: A Hybrid Network for Retinal Disease Detection Based on Wavelet Transform in the Spatial-Frequency Domain 
[[arxiv](https://arxiv.org/abs/2501.11854)] [[cool](https://papers.cool/arxiv/2501.11854)] [[pdf](https://arxiv.org/pdf/2501.11854)]
> **Authors**: Jilan Cheng,Guoli Long,Zeyu Zhang,Zhenjia Qi,Hanyu Wang,Libin Lu,Shuihua Wang,Yudong Zhang,Jin Hong
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: WaveNet-SF：基于空间频率域小波变换的视网膜疾病检测的混合网络
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 视网膜疾病是视力障碍和失明的主要原因，及时诊断对于有效治疗至关重要。光学相干断层扫描（OCT）已成为视网膜疾病诊断的标准成像方式，但是OCT图像通常遭受诸如斑点噪声，复杂的病变形状和变化的病变大小等问题，从而使解释具有挑战性。在本文中，我们提出了一个新颖的框架WaveNet-SF，以通过整合空间域和频域学习来增强视网膜疾病的检测。该框架利用小波转换将OCT图像分解为低频和高频组件，从而使模型能够提取全球结构特征和细粒细节。为了改善病变检测，我们引入了多尺度的小波空间注意力（MSW-SA）模块，该模块增强了模型对多个尺度感兴趣区域的关注。此外，还合并了高频特征补偿块（HFFC），以恢复小波分解过程中丢失的边缘信息，抑制噪声并保留对病变检测至关重要的细节。我们的方法分别达到了10月C8和2017年10月的数据集的最新分类精度（SOTA）分类精度为99。58％，超过了现有方法。这些结果表明，波诺特-SF在解决OCT图像分析的挑战及其作为视网膜疾病诊断的有力工具方面的挑战。

### A generalizable 3D framework and model for self-supervised learning in medical imaging 
[[arxiv](https://arxiv.org/abs/2501.11755)] [[cool](https://papers.cool/arxiv/2501.11755)] [[pdf](https://arxiv.org/pdf/2501.11755)]
> **Authors**: Tony Xu,Sepehr Hosseini,Chris Anderson,Anthony Rinaldi,Rahul G. Krishnan,Anne L. Martel,Maged Goubran
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: No comments
- **标题**: 在医学成像中进行自我监督学习的可推广的3D框架和模型
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 当前用于3D医学成像的自我监督学习方法依赖于简单的借口公式和特定于模式的数据集，从而限制了它们的普遍性和可扩展性。我们提出了3Dino，这是一种适合3D数据集的尖端SSL方法，并将其用于预处理3Dino-Vit：一种通用医学成像模型，在一个非常大的，多模式和多层数据集上的〜100,000 3D医学成像扫描中，来自10个Organs。我们使用大量医学成像分割和分类任务进行大量实验来验证3Dino-Vit。我们的结果表明，3dino-Vit跨越模态和器官（包括分布式任务和数据集）的概括，对大多数评估指标和标记的数据集大小的最先进方法优于最先进的方法。我们的3DINO框架和3Dino-Vit将可以使用3D基础模型进行研究，或为广泛的医学成像应用提供进一步的填充。

### ITCFN: Incomplete Triple-Modal Co-Attention Fusion Network for Mild Cognitive Impairment Conversion Prediction 
[[arxiv](https://arxiv.org/abs/2501.11276)] [[cool](https://papers.cool/arxiv/2501.11276)] [[pdf](https://arxiv.org/pdf/2501.11276)]
> **Authors**: Xiangyang Hu,Xiangyu Shen,Yifei Sun,Xuhao Shan,Wenwen Min,Liyilei Su,Xiaomao Fan,Ahmed Elazab,Ruiquan Ge,Changmiao Wang,Xiaopeng Fan
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: 5 pages, 1 figure, accepted by IEEE ISBI 2025
- **标题**: ITCFN：用于轻度认知障碍转换预测的不完整的三模式共同注意融合网络
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 阿尔茨海默氏病（AD）是老年人中常见的神经退行性疾病。早期预测和及时干预其前驱阶段，轻度认知障碍（MCI）可以降低前进的AD风险。结合来自各种方式的信息可以显着提高预测精度。但是，诸如跨模式的缺少数据和异质性之类的挑战使多模式学习方法复杂化，因为增加了更多的方式会使这些问题恶化。当前的多模式融合技术通常无法适应医学数据的复杂性，从而阻碍了识别模式之间关系的能力。为了应对这些挑战，我们提出了一种创新的多模式方法来预测MCI转换，专门针对缺失正电子发射断层扫描（PET）数据并整合多样化的医学信息的问题。为此，提出的不完整的三模式MCI转换预测网络是为此量身定制的。通过缺失的模态生成模块，我们使用专门设计的编码器合成了磁共振成像中缺少的PET数据，并提取特征。我们还开发了一个通道聚集模块和三模式共发融合模块，以降低特征冗余并实现有效的多模态数据融合。此外，我们设计了一个损失功能，以处理缺失的模态问题和对齐跨模式特征。这些组件共同利用多模式数据来提高网络性能。 ADNI1和ADNI2数据集的实验结果表明，我们的方法显着超过了现有的单峰和其他多模式模型。我们的代码可在https://github.com/justinhxy/itfc上找到。

### Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET 
[[arxiv](https://arxiv.org/abs/2501.12425)] [[cool](https://papers.cool/arxiv/2501.12425)] [[pdf](https://arxiv.org/pdf/2501.12425)]
> **Authors**: Fatih Aksu,Fabrizia Gelardi,Arturo Chiti,Paolo Soda
> **First submission**: 2025-01-21
> **First announcement**: 2025-01-22
> **comment**: No comments
- **标题**: 多模式学习的多阶段中间融合，以对CT和PET的非小细胞肺癌亚型分类
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,定量方法
- **摘要**: 在精密医学时代，非小细胞肺癌（NSCLC）的组织学亚型的准确分类至关重要，但是当前的侵入性技术并不总是可行的，可能导致临床并发症。这项研究提出了一种多阶段的中间融合方法，可从CT和PET图像分类NSCLC亚型。我们的方法在特征提取的不同阶段集成了这两种方式，使用体素融合来利用各种抽象水平的互补信息，同时保留空间相关性。我们将我们的方法与仅使用CT或PET图像的单峰方法进行比较，以证明模态融合的好处，并进一步基准与早期和晚期融合技术相对，以突出特征提取过程中中间融合的优势。此外，我们将模型与使用PET/CT图像进行组织学亚型分类的唯一现有中间融合方法进行了比较。我们的结果表明，所提出的方法的表现优于关键指标的所有替代方案，其精度和AUC分别等于0.724和0.681。这种非侵入性方法有可能显着提高诊断准确性，促进更明智的治疗决策，并在肺癌管理中提高个性化护理。

### Fully Guided Neural Schrödinger bridge for Brain MR image synthesis 
[[arxiv](https://arxiv.org/abs/2501.14171)] [[cool](https://papers.cool/arxiv/2501.14171)] [[pdf](https://arxiv.org/pdf/2501.14171)]
> **Authors**: Hanyeol Yang,Sunggyu Kim,Yongseon Yoo,Jong-min Lee
> **First submission**: 2025-01-23
> **First announcement**: 2025-01-24
> **comment**: 9 pages,4 figures
- **标题**: 全引导神经Schrödinger桥的大脑MR图像合成
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 多模式大脑MRI提供了用于临床诊断的必要补充信息。但是，由于时间和成本限制，获得所有方式通常是具有挑战性的。为了解决这个问题，已经提出了各种方法来产生可用的模式。传统方法可以大致分为两种主要类型：配对和未配对的方法。虽然配对方法提供了出色的性能，但在现实情况下，获得大规模的配对数据集挑战。相反，未配对的方法有助于大规模数据收集，但努力保留关键图像特征，例如肿瘤。在本文中，我们提出了完全引导的Schrödinger桥梁（FGSB），这是一个基于神经Schrödinger桥的新型框架，以克服这些局限性。 FGSB使用最小的配对数据实现了稳定的高质量生成缺失模式。此外，当提供特定区域的地面真相或细分网络时，FGSB可以产生缺失的模态，同时保留具有减少数据要求的这些关键领域。我们提出的模型由两个连续阶段组成。 1）生成阶段：融合生成的图像，配对的参考图像和高斯噪声，采用迭代精炼来减轻问题，例如模式崩溃和提高生成质量2）训练阶段：学习从生成的图像到目标方式的映射。实验表明，FGSB与在大型数据集上训练的方法相当，而仅使用两个受试者的数据。此外，使用FGSB的病变信息的利用可显着增强其保持关键病变特征的能力。

### Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2501.16282)] [[cool](https://papers.cool/arxiv/2501.16282)] [[pdf](https://arxiv.org/pdf/2501.16282)]
> **Authors**: Jing Zhang,Xiaowei Yu,Yanjun Lyu,Lu Zhang,Tong Chen,Chao Cao,Yan Zhuang,Minheng Chen,Tianming Liu,Dajiang Zhu
> **First submission**: 2025-01-27
> **First announcement**: 2025-01-28
> **comment**: No comments
- **标题**: 大脑适应器：使用适配器调节多模型模型增强神经系统障碍分析
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 了解脑疾病对于准确的临床诊断和治疗至关重要。多模式大语言模型（MLLM）的最新进展为解释文本描述的支持提供了一种有希望的解释医学图像的方法。但是，以前的研究主要集中在2D医学图像上，留下了未经探索的3D图像的更丰富的空间信息，并且基于单模式的方法受到忽略其他模式中包含的关键临床信息的限制。为了解决这个问题，本文提出了一种新颖的方法，该方法融合了一个额外的瓶颈层，以学习新知识并将其灌输到原始的预训练知识中。主要思想是将轻质的瓶颈层纳入较少的参数，同时捕获基本信息并利用对比度的语言图像预训练（剪辑）策略，以在统一表示空间内对齐多模式数据。广泛的实验证明了我们方法在整合多模式数据方面的有效性，以显着提高诊断准确性而没有高计算成本，从而强调了增强现实世界诊断工作流的潜力。

### Ultra-high resolution multimodal MRI dense labelled holistic brain atlas 
[[arxiv](https://arxiv.org/abs/2501.16879)] [[cool](https://papers.cool/arxiv/2501.16879)] [[pdf](https://arxiv.org/pdf/2501.16879)]
> **Authors**: José V. Manjón,Sergio Morell-Ortega,Marina Ruiz-Perez,Boris Mansencal,Edern Le Bot,Marien Gadea,Enrique Lanuza,Gwenaelle Catheline,Thomas Tourdias,Vincent Planche,Rémi Giraud,Denis Rivière,Jean-François Mangin,Nicole Labra-Avila,Roberto Vivo-Hernando,Gregorio Rubio,Fernando Aparici,Maria de la Iglesia-Vaya,Pierrick Coupé
> **First submission**: 2025-01-28
> **First announcement**: 2025-01-29
> **comment**: 22 pages
- **标题**: 超高分辨率多模式MRI密集标记为整体大脑图集
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 在本文中，我们介绍了整体，多模式和高分辨率人脑图集的Holiatlas。该地图集涵盖了从器官到子结构水平的人脑解剖结构的不同水平，使用了一种新的密集标记的协议，该方案是根据不同尺度上多个局部方案的融合而产生的。该地图集已被构造，平均来自人类Connectome项目数据库的75名健康受试者的图像和分割。具体而言，T1，T2和WMN的MR图像在0.125 $ mm^{3} $分辨率上以非线性注册并使用对称组的标准化平均以构建Atlas进行平均。在最佳级别，Holiatlas协议具有从10种不同的描述协议得出的350个不同的标签。将这些标签分组为不同的尺度，以以连贯且一致的方式在不同水平上提供整体视野。这种多尺度和多模式地图集可用于开发新的超高分辨率分割方法，这些方法可能有可能利用神经系统疾病的早期检测。

### Glioma Multimodal MRI Analysis System for Tumor Layered Diagnosis via Multi-task Semi-supervised Learning 
[[arxiv](https://arxiv.org/abs/2501.17758)] [[cool](https://papers.cool/arxiv/2501.17758)] [[pdf](https://arxiv.org/pdf/2501.17758)]
> **Authors**: Yihao Liu,Zhihao Cui,Liming Li,Junjie You,Xinle Feng,Jianxin Wang,Xiangyu Wang,Qing Liu,Minghua Wu
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: 23 pages, 13 figures
- **标题**: 胶质瘤多模式MRI分析系统，用于通过多任务半监督学习的肿瘤分层诊断
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 神经胶质瘤是中枢神经系统中最常见的原发性肿瘤。多模式MRI被广泛用于对神经胶质瘤的初步筛查，并在辅助诊断，治疗功效和预后评估中起着至关重要的作用。当前，使用MRI对神经胶质瘤进行的计算机辅助诊断研究集中在独立的分析事件上，例如肿瘤分割，分级和放射基因组学分类，而无需研究这些事件之间的依赖性。在这项研究中，我们提出了一种胶质瘤多模式MRI分析系统（GMMA），该系统利用深度学习网络同时处理多个事件，通过基于不确定性的多任务多任务学习架构并同步输出肿瘤区域分割，Gliomoma Histogical subtype，IDH突变基因类型和1P/19Q Chromosome Chromosome Chrolocome Chromosome Chromosome Chromosome Chromosome Chromosome Chromosome Chromosome Chromosome Chromosome conloMosoys和1P/19Q。与报道的单任务分析模型相比，GMMA提高了肿瘤分层诊断任务的精度。此外，我们采用了两阶段的半监督学习方法，通过完全利用标记和未标记的MRI样本来增强模型性能。此外，通过利用基于知识自我验证和跨模式特征提取的对比度学习的适应模块，转基因在模态缺失的情况下表现出鲁棒性，并揭示了每种MRI模态的重要性。最后，基于转角的分析输出，我们为医生和患者创建了一个视觉和用户友好的平台，引入了GMMAS-GPT，以产生个性化的预后评估和建议。

### PulmoFusion: Advancing Pulmonary Health with Efficient Multi-Modal Fusion 
[[arxiv](https://arxiv.org/abs/2501.17699)] [[cool](https://papers.cool/arxiv/2501.17699)] [[pdf](https://arxiv.org/pdf/2501.17699)]
> **Authors**: Ahmed Sharshar,Yasser Attia,Mohammad Yaqub,Mohsen Guizani
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: ef:(ISBI 2025) 2025 IEEE International Symposium on Biomedical Imaging
- **标题**: PulMofusion：通过有效的多模式融合来提高肺部健康
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 传统的远程肺活量测定法缺乏有效肺部监测所需的精度。我们使用多模式预测模型提出了一种新型的非侵入性方法，该模型将RGB或热视频数据与患者元数据相结合。我们的方法利用轻量级的CNN利用强制呼气量（FEV1）和强迫呼气量（FEV1）和强迫呼气量（FEV1）的分类来利用节能尖峰神经网络（SNN），并使用轻量级CNN来克服回归任务中的SNN限制。通过多头注意力层改善了多模式数据集成，我们采用K折叠验证和集合学习来增强鲁棒性。使用热数据，我们的SNN型号在呼吸周期基础上达到92％的精度，而患者则达到99.5％。 PEF回归模型的相对RMSS为0.11（热）和0.26（RGB），FEV1/FVC预测的MAE为4.52％，建立了最先进的性能。可以在https://github.com/ahmed-sharshar/respirodynamics.git上找到代码和数据集

## 信号处理(eess.SP:Signal Processing)

该领域共有 3 篇论文

### A Systematic Review of Machine Learning Methods for Multimodal EEG Data in Clinical Application 
[[arxiv](https://arxiv.org/abs/2501.08585)] [[cool](https://papers.cool/arxiv/2501.08585)] [[pdf](https://arxiv.org/pdf/2501.08585)]
> **Authors**: Siqi Zhao,Wangyang Li,Xiru Wang,Stevie Foglia,Hongzhao Tan,Bohan Zhang,Ameer Hamoodi,Aimee Nelson,Zhen Gao
> **First submission**: 2024-12-31
> **First announcement**: 2025-01-16
> **comment**: This paper includes 4 figures, 6 tables, and totals 18 pages
- **标题**: 对临床应用中多模式脑电图数据的机器学习方法的系统评价
- **领域**: 信号处理,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 机器学习（ML）和深度学习（DL）技术已被广泛应用于分析用于疾病诊断和脑部计算机界面（BCI）的脑电图（EEG）信号。已显示多模式数据的集成可以增强ML和DL模型的准确性。通过解决临床人群的复杂任务，将脑电图与其他方式相结合可以改善临床决策。该系统文献综述探讨了ML和DL模型中多模式EEG数据在临床应用中的使用。在PubMed，Web of Science和Google Scholar进行了全面的搜索，经过三轮过滤后，进行了16项相关研究。这些研究表明，多模式EEG数据在应对临床挑战中的应用，包括神经精神疾病，神经系统疾病（例如，癫痫发作检测），神经发育障碍（例如自闭症谱系障碍）和睡眠阶段分类。数据融合发生在三个级别：信号，功能和决策水平。最常用的ML模型是支持向量机（SVM）和决策树。值得注意的是，在16项研究中，有11项报告了多模式EEG数据的模型准确性提高。这篇评论强调了基于多模式的ML模型在增强临床诊断和解决问题的潜力。

### MambaTron: Efficient Cross-Modal Point Cloud Enhancement using Aggregate Selective State Space Modeling 
[[arxiv](https://arxiv.org/abs/2501.16384)] [[cool](https://papers.cool/arxiv/2501.16384)] [[pdf](https://arxiv.org/pdf/2501.16384)]
> **Authors**: Sai Tarun Inaganti,Gennady Petrenko
> **First submission**: 2025-01-25
> **First announcement**: 2025-01-28
> **comment**: Accepted to the Workshop on Image Quality in Computer Vision and Generative AI, WACV 2025
- **标题**: 肥大：使用聚合选择性状态空间建模的有效跨模式云增强
- **领域**: 信号处理,机器学习
- **摘要**: 点云增强是从不完整输入中生成高质量点云的过程。例如，通过回归填写诸如地面真相之类的参考的缺失细节来完成。除了单峰图像和点云重建外，我们还专注于观看引导点云完成的任务，在该任务中，我们从图像中收集丢失的信息，该信息代表点云的视图并使用它来生成输出点云。随着最初围绕国家空间模型的最新研究工作，最初是在自然语言处理中，现在是2D和3D视觉的研究，Mamba表现出了令人鼓舞的结果，这是自我发场机制的有效替代方法。但是，对于使用MAMBA进行图像和输入点云之间的交叉注意力的研究有限，这对于多模式问题至关重要。在本文中，我们介绍了Mambatron，这是一种amba-transformer单元，它是我们网络的构建基础，它具有单型和跨模式的重建，其中包括View引导点云完成。我们探索了Mamba的长期效率的好处。这种方法是实施基于曼巴的跨注意的类似物，尤其是在计算机视觉中的尝试之一。我们的模型展示了与当前最新技术相当的性能，同时使用了一小部分计算资源。

### Assessment of the January 2025 Los Angeles County wildfires: A multi-modal analysis of impact, response, and population exposure 
[[arxiv](https://arxiv.org/abs/2501.17880)] [[cool](https://papers.cool/arxiv/2501.17880)] [[pdf](https://arxiv.org/pdf/2501.17880)]
> **Authors**: Seyd Teymoor Seydi
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-30
> **comment**: No comments
- **标题**: 评估2025年1月洛杉矶县野火：影响，反应和人口的多模式分析
- **领域**: 信号处理,人工智能,机器学习,数值分析
- **摘要**: 这项研究对四个重要的加利福尼亚野火进行了全面分析：帕利萨德，伊顿，肯尼斯和赫斯特，通过多个维度检查其影响，包括土地覆盖变化，管辖权管理，结构性损害和人口统计脆弱性。使用应用于Sentinel-2图像的Chebyshev-Kolmogorov-Arnold网络模型，绘制了燃烧区域的范围，范围为315.36至10,960.98公顷。我们的分析表明，灌木丛生态系统始终是受影响最大的生态系统，其中占所有事件中燃烧区域的57.4-75.8％。司法管辖区评估表明，从单一权威（帕利塞德大火中的98.7％）到在多个机构分发管理的各种复杂性。结构性影响分析表明，城市界面火之间的差异很大（伊顿：9,869个结构；帕利塞德：8,436个结构）和乡村事件（肯尼斯：24个结构； Hurst：17结构）。人口统计学分析表明性别分布一致，其中50.9％的人口被确定为女性，为男性49.1％。占受影响人群的大多数人群的人口范围从53.7％到54.1％，在火灾后期的时间变化显着。该研究确定了城市界面接近性，结构损害和人口暴露之间的密切相关性。帕利塞德（Palisades）和伊顿（Eaton）的大火影响了20,000多人，而农村活动中的枪口则不到500人。这些发现为制定有针对性的野火管理策略（尤其是在Wildland Urban界面区域）提供了宝贵的见解，并强调需要在紧急响应计划中对年龄和性别意识的方法。

## 优化与控制(math.OC:Optimization and Control)

该领域共有 1 篇论文

### A Multi-agent System for Hybrid Optimization 
[[arxiv](https://arxiv.org/abs/2501.09563)] [[cool](https://papers.cool/arxiv/2501.09563)] [[pdf](https://arxiv.org/pdf/2501.09563)]
> **Authors**: Eric S. Fraga,Veerawat Udomvorakulchai,Miguel Pineda,Lazaros G. Papageorgiou
> **First submission**: 2025-01-16
> **First announcement**: 2025-01-17
> **comment**: 22 pages, 6 figures
- **标题**: 用于混合优化的多代理系统
- **领域**: 优化与控制,多代理系统
- **摘要**: 过程工程中的优化问题（包括设计和操作）通常会对许多求解器构成挑战：多模式，非平滑和不连续的模型通常具有较大的计算要求。在这种情况下，优化问题通常被视为一个黑匣子，其中仅需要目标函数的值，有时表明措施违反了约束。传统上，通过使用直接搜索和元元素方法来解决此类问题。因此，面临的挑战是确定应考虑使用哪种方法或方法组合，以最有效地利用有限的计算资源。本文提出了一个用于优化的多代理系统，该系统使一组求解器可以同时应用于优化问题，包括任何求解器的不同实例。优化问题模型的评估由调度剂控制，该计划促进优化方法之间的合作和竞争。详细描述了代理系统的体系结构和实现，包括求解器，模型评估和调度程序代理。已经开发了一套直接搜索和元元素方法，以与该系统一起使用。提出了过程系统工程应用程序的案例研究，结果表明，不同优化求解器之间自动合作的潜在好处，并激发了求解器之间的竞争实施。

## 物理教育(physics.ed-ph:Physics Education)

该领域共有 1 篇论文

### Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories 
[[arxiv](https://arxiv.org/abs/2501.06143)] [[cool](https://papers.cool/arxiv/2501.06143)] [[pdf](https://arxiv.org/pdf/2501.06143)]
> **Authors**: Gerd Kortemeyer,Marina Babayeva,Giulia Polverini,Bor Gregorcic,Ralf Widenhorn
> **First submission**: 2025-01-10
> **First announcement**: 2025-01-13
> **comment**: No comments
- **标题**: 多模式人工智能系统在多主体物理概念清单上的多语言性能
- **领域**: 物理教育,人工智能
- **摘要**: 我们在涵盖多种语言和主题领域的各种物理概念清单上，研究了大型基于语言模型的人工智能（AI）系统的多语言和多模式性能。从Physport网站上获得的库存涵盖了机械，电磁，光学和热力学的经典物理主题，以及相对论，量子力学，天文学，数学和实验室技能。与以前的仅文本研究不同，我们将库存上传为映像，以反映学生在纸上看到的内容，从而评估了系统的多模式功能。 AI是用英语提示的，并自主选择其响应语言 - 要么以测试的名义语言保留，完全切换到英语或混合语言 - 揭示了根据语言复杂性和数据可用性的适应性行为。我们的结果表明，各个学科领域的性能有所不同，实验室技能是最差的表现领域。此外，与纯粹基于文本的问题相比，AI在需要视觉解释的问题上的表现要差。 AI困难的问题往往是库存语言的这种方式。我们还发现跨语言的性能差异很大，其中一些人似乎从语言切换中受益匪浅，语言切换是一种类似于人类扬声器的代码转换的现象。总体而言，将获得的AI结果与现有文献进行比较，我们发现AI系统的表现优于除实验室技能以外的所有学科领域的平均本科生。

## 光学(physics.optics:Optics)

该领域共有 1 篇论文

### Training Hybrid Neural Networks with Multimode Optical Nonlinearities Using Digital Twins 
[[arxiv](https://arxiv.org/abs/2501.07991)] [[cool](https://papers.cool/arxiv/2501.07991)] [[pdf](https://arxiv.org/pdf/2501.07991)]
> **Authors**: Ilker Oguz,Louis J. E. Suter,Jih-Liang Hsieh,Mustafa Yildirim,Niyazi Ulas Dinc,Christophe Moser,Demetri Psaltis
> **First submission**: 2025-01-14
> **First announcement**: 2025-01-15
> **comment**: 17 pages, 6 figures
- **标题**: 使用Digital Twins培训具有多模光学非线性的培训混合神经网络
- **领域**: 光学,人工智能
- **摘要**: 训练越来越多的神经网络的能力将人工智能带到了科学和技术发现的最前沿。但是，它们的尺寸呈指数级的增加，会形成对能源和计算硬件的比例更大的需求。将复杂的物理事件纳入固定，有效的计算模块中，可以通过降低可训练层的复杂性来满足这一需求。在这里，为此，我们在多模纤维中利用了多模纤维中的Ultrashort脉冲传播。训练混合体系结构是通过一个神经模型来实现的，该神经模型可以分化近似光学系统。训练算法更新神经模拟器，并将误差信号反向该代理上的误差信号，以优化光学前的图层。我们的实验结果达到了最新的图像分类精度和模拟保真度。此外，该框架表明了对实验漂移的特殊韧性。通过将低能物理系统集成到神经网络中，该方法可以实现可扩展的，节能的AI模型，其计算需求大大减少。

## 生物分子(q-bio.BM:Biomolecules)

该领域共有 1 篇论文

### Molecular Fingerprints Are Strong Models for Peptide Function Prediction 
[[arxiv](https://arxiv.org/abs/2501.17901)] [[cool](https://papers.cool/arxiv/2501.17901)] [[pdf](https://arxiv.org/pdf/2501.17901)]
> **Authors**: Jakub Adamczyk,Piotr Ludynia,Wojciech Czech
> **First submission**: 2025-01-29
> **First announcement**: 2025-01-30
> **comment**: No comments
- **标题**: 分子指纹是肽功能预测的强模型
- **领域**: 生物分子,机器学习
- **摘要**: 我们研究了分子指纹在肽性能预测中的有效性，并证明从分子图中域特异性特异性特征提取也可以胜过复合物和计算昂贵的模型，例如GNN，预审预周序的基于序列的变压器和多模束乐团，即使没有超参数调谐。为此，我们对126个数据集进行了彻底的评估，从而在LRGB和5个其他肽功能预测基准上实现了最新结果。我们表明，基于ECFP的计数变体，拓扑扭转和RDKIT分子指纹和LightGBM作为分类头的模型非常健壮。本质上非常短的特征编码器的分子指纹的强烈性能挑战了肽中远距离相互作用的重要性。我们的结论是，将分子指纹用于较大分子（例如肽）可能是一种计算可行的，低参数和多功能的替代品，可用于复杂的深度学习模型。

## 神经元和认知(q-bio.NC:Neurons and Cognition)

该领域共有 1 篇论文

### Single-neuron deep generative model uncovers underlying physics of neuronal activity in Ca imaging data 
[[arxiv](https://arxiv.org/abs/2501.14615)] [[cool](https://papers.cool/arxiv/2501.14615)] [[pdf](https://arxiv.org/pdf/2501.14615)]
> **Authors**: Jordi Abante,Angelo Piga,Berta Ros,Clara F López-León,Josep M Canals,Jordi Soriano
> **First submission**: 2025-01-24
> **First announcement**: 2025-01-27
> **comment**: 12 pages, 5 figures, ECCB 2025
- **标题**: 单神经深层生成模型在Ca成像数据中发现神经元活性的基本物理
- **领域**: 神经元和认知,机器学习
- **摘要**: 钙成像已成为研究神经元活性，提供空间分辨率和以微创方式测量大量神经元种群的电生理学的有力替代方法。该技术在神经科学，神经工程和医学中具有广泛的应用，使研究人员能够探索神经元的位置与活动之间的关系。深层生成模型（DGM）的最新进展促进了神经元种群动力学的建模，发现了潜在表示，这些表示提供了对行为预测和神经元方差的见解。但是，这些模型通常依赖于SPIKE推理算法，主要集中于种群级动力学，从而限制了它们对单神经分析的适用性。为了解决这一差距，我们为使用自回归变分自动编码器（AVAE）的单神经元表示学习提供了一个新颖的框架。我们的方法将单个神经元的时空信号嵌入了降低的空间中，而无需尖峰推理算法。 Avae通过产生更有信息和歧视性的潜在表示，改善了可视化，聚类和对神经元活动的理解等任务，从而在传统的线性方法上脱颖而出。此外，AVAE的重建性能胜过最新的状态，证明了其能够准确地从学习的表示中恢复原始荧光信号的能力。使用逼真的模拟，我们表明我们的模型捕获了潜在的物理属性和连接模式，从而使其能够区分不同的触发和连接类型。这些发现将AVAE定位为一种多功能且强大的工具，用于推进单神经元分析，并为将多模式单细胞数据集在Neuroscience中奠定了基础。

## 定量方法(q-bio.QM:Quantitative Methods)

该领域共有 1 篇论文

### Interpretable Droplet Digital PCR Assay for Trustworthy Molecular Diagnostics 
[[arxiv](https://arxiv.org/abs/2501.09218)] [[cool](https://papers.cool/arxiv/2501.09218)] [[pdf](https://arxiv.org/pdf/2501.09218)]
> **Authors**: Yuanyuan Wei,Yucheng Wu,Fuyang Qu,Yao Mu,Yi-Ping Ho,Ho-Pui Ho,Wu Yuan,Mingkun Xu
> **First submission**: 2025-01-15
> **First announcement**: 2025-01-16
> **comment**: No comments
- **标题**: 可解释的液滴数字PCR测定可信赖的分子诊断
- **领域**: 定量方法,人工智能
- **摘要**: 准确的分子定量对于在传染病，癌症生物学和遗传疾病等领域的研究和诊断方面至关重要。液滴数字PCR（DDPCR）已成为实现绝对定量的金标准。尽管计算DDPCR技术已经取得了长足的发展，但在各种操作环境中实现自动解释和一致的适应性仍然是一个挑战。为了解决这些局限性，我们介绍了智能解释的液滴数字PCR（I2DDPCR）测定，这是一个综合框架，将前端预测模型（用于液滴细分和分类）与GPT-4O多模式大型语言模型（MLLM，MLLM，用于上下文意识到的解释和建议和建议），以自动化和增强DDDPCR图像分析。这种方法超过了最先进的模型，在处理复杂的DDPCR图像中，每个图像中包含300多滴超过300滴的信号 - 噪声比（SNRS）的精度为99.05％。通过将专门的神经网络和大语言模型相结合，I2DDPCR测定法提供了一种可靠且适应性的解决方案，用于绝对分子定量，达到了能够检测低含量靶标低至90.32副本/μl的灵敏度。此外，它通过详细的解释和故障排除指导提高了模型的透明度，从而使用户有能力做出明智的决策。这种创新的框架有可能使分子诊断，疾病研究和临床应用有益，尤其是在资源受限的环境中。

## 交易和市场微观结构(q-fin.TR:Trading and Market Microstructure)

该领域共有 1 篇论文

### LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management 
[[arxiv](https://arxiv.org/abs/2501.00826)] [[cool](https://papers.cool/arxiv/2501.00826)] [[pdf](https://arxiv.org/pdf/2501.00826)]
> **Authors**: Yichen Luo,Yebo Feng,Jiahua Xu,Paolo Tasca,Yang Liu
> **First submission**: 2025-01-01
> **First announcement**: 2025-01-03
> **comment**: No comments
- **标题**: 自动加密投资组合管理LLM驱动的多代理系统
- **领域**: 交易和市场微观结构,人工智能
- **摘要**: 与传统资产相比，加密货币投资本质上很困难，需要整合来自各种方式的大量数据以及对复杂推理的需求。尽管已经采用了深度学习方法来应对这些挑战，但它们的黑盒本质引起了人们对信任和解释性的关注。最近，大型语言模型（LLMS）由于能够理解多模式数据并产生可解释的决策，因此在财务应用中显示了希望。但是，单个LLM面临着复杂，全面的任务（例如资产投资）的限制。这些限制在加密货币投资中更为明显，在该公司的培训语料库中，LLM的域特定知识较少。为了克服这些挑战，我们提出了一个可解释的，多模式的，多代理的框架，用于加密货币投资。我们的框架使用专门的代理商在团队内部和跨团队进行协作，以通过市值来处理数据分析，文献整合和投资决策等子任务，并通过市值进行了30个加密货币。专家培训模块使用多模式的历史数据和专业投资文献进行微调代理，而多代理投资模块则采用实时数据来做出知情的加密货币投资决策。独特的Intrateam和Interteam协作机制通过根据代理团队内的置信度调整最终预测并促进团队之间的信息共享来提高预测准确性。从2023年11月至2024年9月开始使用数据的经验评估表明，我们的框架在分类，资产定价，投资组合和解释性绩效方面优于单一代理模型和市场基准。

## 机器学习(stat.ML:Machine Learning)

该领域共有 2 篇论文

### Natural Variational Annealing for Multimodal Optimization 
[[arxiv](https://arxiv.org/abs/2501.04667)] [[cool](https://papers.cool/arxiv/2501.04667)] [[pdf](https://arxiv.org/pdf/2501.04667)]
> **Authors**: Tâm Le Minh,Julyan Arbel,Thomas Möllenhoff,Mohammad Emtiyaz Khan,Florence Forbes
> **First submission**: 2025-01-08
> **First announcement**: 2025-01-09
> **comment**: No comments
- **标题**: 自然变异退火以进行多模式优化
- **领域**: 机器学习,机器学习,计算
- **摘要**: 我们介绍了一种称为自然变异退火（NVA）的新的多模式优化方法，该方法结合了三个基础概念的优势，以同时搜索Black-Box NonConvex目标的多种全局和局部模式。首先，它通过使用变异后代（例如高斯的混合物）来实现同时搜索。其次，它适用于退火，逐渐将探索探索进行剥削。最后，它使用自然梯度学习来学习变异搜索分布，其中更新类似于知名和易于实现的算法。这三个概念在NVA中汇集在一起​​，从而产生了新的算法，还允许我们融合“健身塑形”，这是进化算法的核心概念。我们评估模拟的搜索质量，并使用梯度下降和进化策略将其与方法进行比较。我们还为行星科学中现实世界中的逆问题提供了应用。

### Can Bayesian Neural Networks Make Confident Predictions? 
[[arxiv](https://arxiv.org/abs/2501.11773)] [[cool](https://papers.cool/arxiv/2501.11773)] [[pdf](https://arxiv.org/pdf/2501.11773)]
> **Authors**: Katharine Fisher,Youssef Marzouk
> **First submission**: 2025-01-20
> **First announcement**: 2025-01-21
> **comment**: Mathematics of Modern Machine Learning Workshop at NeurIPS 2024
- **标题**: 贝叶斯神经网络可以做出自信的预测吗？
- **领域**: 机器学习,机器学习,统计理论
- **摘要**: 贝叶斯推断有望将神经网络预测的原则不确定性量化框架。采用的障碍包括在网络参数上充分表征后验分布的困难以及后验预测分布的解释性。我们证明，在内层重量的离散先验下，我们可以精确地将后验预测分布描述为高斯混合物。此设置使我们能够定义网络参数值的等价类别，这些类别值产生相同的可能性（训练错误），并将这些类的元素与网络的缩放制度相关联 - 通过训练样本大小的比率定义，每一层的大小和最终层参数的数量定义。特别感兴趣的是映射到低训练误差的不同参数实现，但对应于后验预测分布中的不同模式。我们确定具有这种预测性多模式的设置，从而洞悉单峰后近似值的准确性。我们还表征了模型通过在不同尺度制度中评估后验预测的收缩来“从数据中学习”的能力。

## 其他论文

共有 42 篇其他论文

- [Reading to Listen at the Cocktail Party: Multi-Modal Speech Separation](https://arxiv.org/abs/2501.01518)
  - **标题**: 阅读要在鸡尾酒会上听：多模式​​演讲分离
  - **Filtered Reason**: none of eess.AS,eess.SP,cs.SD in whitelist
- [DiffCL: A Diffusion-Based Contrastive Learning Framework with Semantic Alignment for Multimodal Recommendations](https://arxiv.org/abs/2501.01066)
  - **标题**: DIFFCL：基于扩散的对比学习框架，具有语义对齐方式，用于多模式建议
  - **Filtered Reason**: none of cs.MM in whitelist
- [An Immersive Virtual Reality Bimanual Telerobotic System With Haptic Feedback](https://arxiv.org/abs/2501.00822)
  - **标题**: 带有触觉反馈的身临其境的虚拟现实双人望室系统
  - **Filtered Reason**: none of cs.HC in whitelist
- [NMM-HRI: Natural Multi-modal Human-Robot Interaction with Voice and Deictic Posture via Large Language Model](https://arxiv.org/abs/2501.00785)
  - **标题**: NMM-HRI：通过大语言模型与声音和Deictic姿势的自然多模式人类机器人互动
  - **Filtered Reason**: none of cs.RO in whitelist
- [VinT-6D: A Large-Scale Object-in-hand Dataset from Vision, Touch and Proprioception](https://arxiv.org/abs/2501.00510)
  - **标题**: VINT-6D：视觉，触摸和本体感受的大规模对象数据集
  - **Filtered Reason**: none of cs.RO in whitelist
- [MSM-BD: Multimodal Social Media Bot Detection Using Heterogeneous Information](https://arxiv.org/abs/2501.00204)
  - **标题**: MSM-BD：使用异质信息的多模式社交媒体机器人检测
  - **Filtered Reason**: none of cs.SI,cs.MM in whitelist
- [KD-MSLRT: Lightweight Sign Language Recognition Model Based on Mediapipe and 3D to 1D Knowledge Distillation](https://arxiv.org/abs/2501.02321)
  - **标题**: KD-MSLRT：基于Mediapipe和3D至1D知识蒸馏的轻巧手语识别模型
  - **Filtered Reason**: none of cs.CY in whitelist
- [ARTHUR: Authoring Human-Robot Collaboration Processes with Augmented Reality using Hybrid User Interfaces](https://arxiv.org/abs/2501.02304)
  - **标题**: Arthur：使用混合用户界面来创作人机协作过程，并使用增强现实
  - **Filtered Reason**: none of cs.HC in whitelist
- [Design and Benchmarking of A Multi-Modality Sensor for Robotic Manipulation with GAN-Based Cross-Modality Interpretation](https://arxiv.org/abs/2501.02303)
  - **标题**: 使用基于GAN的跨模式解释的多模式传感器的设计和基准测试
  - **Filtered Reason**: none of cs.RO,eess.SP in whitelist
- [Guitar-TECHS: An Electric Guitar Dataset Covering Techniques, Musical Excerpts, Chords and Scales Using a Diverse Array of Hardware](https://arxiv.org/abs/2501.03720)
  - **标题**: 吉他技术：使用各种硬件的电动吉他数据集涵盖技术，音乐摘录，和弦和秤
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Exploring EEG and Eye Movement Fusion for Multi-Class Target RSVP-BCI](https://arxiv.org/abs/2501.03596)
  - **标题**: 探索多级目标RSVP-BCI的EEG和眼动融合
  - **Filtered Reason**: none of cs.HC in whitelist
- [Metric Criticality Identification for Cloud Microservices](https://arxiv.org/abs/2501.03547)
  - **标题**: 云微服务的度量临界识别
  - **Filtered Reason**: none of cs.DC in whitelist
- [FleSpeech: Flexibly Controllable Speech Generation with Various Prompts](https://arxiv.org/abs/2501.04644)
  - **标题**: Flaspeech：通过各种提示灵活地控制语音生成
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Towards accurate and reliable ICU outcome prediction: a multimodal learning framework based on belief function theory using structured EHRs and free-text notes](https://arxiv.org/abs/2501.04389)
  - **标题**: 迈向准确可靠的ICU结果预测：使用结构化EHR和自由文本注释基于信念功能理论的多模式学习框架
  - **Filtered Reason**: none of cs.IT in whitelist
- [MECASA: Motor Execution Classification using Additive Self-Attention for Hybrid EEG-fNIRS Data](https://arxiv.org/abs/2501.05525)
  - **标题**: MECASA：使用添加剂自我注意的电动机执行分类用于混合EEG-FNIRS数据
  - **Filtered Reason**: none of cs.HC in whitelist
- [Video-Conferencing Beyond Screen-Sharing and Thumbnail Webcam Videos: Gesture-Aware Augmented Reality Video for Data-Rich Remote Presentations](https://arxiv.org/abs/2501.05345)
  - **标题**: 视频会议超出屏幕共享和缩略图网络摄像头视频：富含数据远程演示的手势增强现实视频
  - **Filtered Reason**: none of cs.HC in whitelist
- ["What's Happening"- A Human-centered Multimodal Interpreter Explaining the Actions of Autonomous Vehicles](https://arxiv.org/abs/2501.05322)
  - **标题**: “正在发生的事情”  - 以人为本的多式联运解释器解释了自动驾驶汽车的行为
  - **Filtered Reason**: none of cs.HC in whitelist
- [ROSAnnotator: A Web Application for ROSBag Data Analysis in Human-Robot Interaction](https://arxiv.org/abs/2501.07051)
  - **标题**: Rosannotator：人类机器人交互中的ROSBAG数据分析的Web应用程序
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Multi-modal Speech Enhancement with Limited Electromyography Channels](https://arxiv.org/abs/2501.06530)
  - **标题**: 具有有限的肌电图通道的多模式语音增强
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Temperature Driven Multi-modal/Single-actuated Soft Finger](https://arxiv.org/abs/2501.07216)
  - **标题**: 温度驱动的多模式/单驱动软手指
  - **Filtered Reason**: none of cs.RO in whitelist
- [Cognitive Assessment and Training in Extended Reality: Multimodal Systems, Clinical Utility, and Current Challenges](https://arxiv.org/abs/2501.08237)
  - **标题**: 扩展现实中的认知评估和培训：多模式系统，临床公用事业和当前挑战
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [TeamVision: An AI-powered Learning Analytics System for Supporting Reflection in Team-based Healthcare Simulation](https://arxiv.org/abs/2501.09930)
  - **标题**: TeamVision：AI驱动的学习分析系统，用于支持基于团队的医疗保健模拟的反思
  - **Filtered Reason**: none of cs.HC in whitelist
- [Position: Open and Closed Large Language Models in Healthcare](https://arxiv.org/abs/2501.09906)
  - **标题**: 职位：医疗保健中开放和关闭的大型语言模型
  - **Filtered Reason**: none of cs.CY in whitelist
- [Multi-LiCa: A Motion and Targetless Multi LiDAR-to-LiDAR Calibration Framework](https://arxiv.org/abs/2501.11088)
  - **标题**: 多律：一个运动和无目标的多激光到劳动校准框架
  - **Filtered Reason**: none of cs.RO in whitelist
- [Open FinLLM Leaderboard: Towards Financial AI Readiness](https://arxiv.org/abs/2501.10963)
  - **标题**: Open Finllm排行榜：迈向财务AI准备就绪
  - **Filtered Reason**: none of cs.CE in whitelist
- [The Generative AI Ethics Playbook](https://arxiv.org/abs/2501.10383)
  - **标题**: 生成的AI伦理剧本
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Integrating Mediumband with Emerging Technologies: Unified Vision for 6G and Beyond Physical Layer](https://arxiv.org/abs/2501.10122)
  - **标题**: 与新兴技术相结合：6G和超越物理层的统一愿景
  - **Filtered Reason**: none of cs.IT in whitelist
- [Light My Way: Developing and Exploring a Multimodal Interface to Assist People With Visual Impairments to Exit Highly Automated Vehicles](https://arxiv.org/abs/2501.11801)
  - **标题**: 点亮我的方式：开发和探索多模式界面，以帮助有视觉障碍的人退出高度自动化的车辆
  - **Filtered Reason**: none of cs.HC in whitelist
- [LLM supervised Pre-training for Multimodal Emotion Recognition in Conversations](https://arxiv.org/abs/2501.11468)
  - **标题**: LLM监督对话中多模式情绪识别的预训练
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Multi-Modal Variable-Rate CSI Reconstruction for FDD Massive MIMO Systems](https://arxiv.org/abs/2501.11926)
  - **标题**: None
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [Multi-source Multi-level Multi-token Ethereum Dataset and Benchmark Platform](https://arxiv.org/abs/2501.11906)
  - **标题**: 多源多级多级以太坊数据集和基准平台
  - **Filtered Reason**: none of cs.CE in whitelist
- [50 Shades of Deceptive Patterns: A Unified Taxonomy, Multimodal Detection, and Security Implications](https://arxiv.org/abs/2501.13351)
  - **标题**: 欺骗性模式的50种阴影：统一的分类法，多模式检测和安全含义
  - **Filtered Reason**: none of cs.CR in whitelist
- [Real-Time Multi-Modal Subcomponent-Level Measurements for Trustworthy System Monitoring and Malware Detection](https://arxiv.org/abs/2501.13081)
  - **标题**: 实时多模式子组件级测量值可信赖的系统监视和恶意软件检测
  - **Filtered Reason**: none of cs.CR in whitelist
- [Int2Planner: An Intention-based Multi-modal Motion Planner for Integrated Prediction and Planning](https://arxiv.org/abs/2501.12799)
  - **标题**: INT2Planner：一个基于意图的多模式运动计划者，用于集成预测和计划
  - **Filtered Reason**: none of cs.RO in whitelist
- [EmoTech: A Multi-modal Speech Emotion Recognition Using Multi-source Low-level Information with Hybrid Recurrent Network](https://arxiv.org/abs/2501.12674)
  - **标题**: Emotech：使用混合复发网络的多源低级信息使用多模式的语音情感识别
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Vision-Based Multimodal Interfaces: A Survey and Taxonomy for Enhanced Context-Aware System Design](https://arxiv.org/abs/2501.13443)
  - **标题**: 基于视觉的多模式接口：增强上下文感知系统设计的调查和分类学
  - **Filtered Reason**: none of cs.HC in whitelist
- [Learning Complex Heterogeneous Multimodal Fake News via Social Latent Network Inference](https://arxiv.org/abs/2501.15508)
  - **标题**: 通过社交潜在网络推断学习复杂的异质多模式假新闻
  - **Filtered Reason**: none of cs.MM in whitelist
- [Resource Allocation Driven by Large Models in Future Semantic-Aware Networks](https://arxiv.org/abs/2501.14832)
  - **标题**: 在未来的语义感知网络中，由大型模型驱动的资源分配
  - **Filtered Reason**: none of cs.DC,cs.NI in whitelist
- [AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2501.16566)
  - **标题**: 情感：一种新的数据集，模型和基准，用于使用多模式模型的情感理解
  - **Filtered Reason**: none of cs.HC in whitelist
- [Mobile Manipulation Instruction Generation from Multiple Images with Automatic Metric Enhancement](https://arxiv.org/abs/2501.17022)
  - **标题**: 自动公制增强功能的多个图像的移动操作指令生成
  - **Filtered Reason**: none of cs.RO in whitelist
- [Adapting Network Information to Semantics for Generalizable and Plug-and-Play Multi-Scenario Network Diagnosis](https://arxiv.org/abs/2501.16842)
  - **标题**: 将网络信息调整到语义上，以进行概括和插件的多幕科网络诊断
  - **Filtered Reason**: none of cs.NI in whitelist
- [AVE Speech Dataset: A Comprehensive Benchmark for Multi-Modal Speech Recognition Integrating Audio, Visual, and Electromyographic Signals](https://arxiv.org/abs/2501.16780)
  - **标题**: AVE语音数据集：用于整合音频，视觉和肌电图信号的多模式语音识别的综合基准
  - **Filtered Reason**: none of cs.HC,eess.AS,cs.SD,cs.MM in whitelist
