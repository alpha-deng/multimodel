# 2024-02 月度论文分类汇总

共有42篇相关领域论文, 另有25篇其他

## 太阳和恒星天体物理学(astro-ph.SR:Solar and Stellar Astrophysics)

该领域共有 1 篇论文

### Predicting the Emergence of Solar Active Regions Using Machine Learning 
[[arxiv](https://arxiv.org/abs/2402.08890)] [[cool](https://papers.cool/arxiv/2402.08890)] [[pdf](https://arxiv.org/pdf/2402.08890)]
> **Authors**: Spiridon Kasapis,Irina N. Kitiashvili,Alexander G. Kosovichev,John T. Stefan,Bhairavi Apte
> **First submission**: 2024-02-13
> **First announcement**: 2024-02-14
> **comment**: 9 pages, 4 figures, IAU Symposium 365 Proceedings
- **标题**: 使用机器学习预测太阳能活动区域的出现
- **领域**: 太阳和恒星天体物理学,机器学习
- **摘要**: 为了为即将到来的太空天气干扰创建预警功能，我们选择了一个有61个新兴活动区域的数据集，这使我们能够在声学功率密度的演变中识别特征特征，以预测连续强度的出现。在我们的研究中，我们利用了来自Helioseissic和磁成像仪（HMI）在太阳能动力学天文台（SDO）的Helioseiscic和Magemin Imager（HMI）的多普勒移位和连续强度观察。在活动区域​​附近的30.66 x 30.66度斑块的局部跟踪使我们能够追踪从前出现状态开始的活动区域的演变。我们已经开发了一种机器学习模型，以捕获与即将发生的磁通量出现相关的声学通量密度变化。训练有素的长期记忆（LSTM）模型能够预测提前5个小时，在太阳表面的给定区域中，连续强度值是否会降低。进行的研究使我们能够研究机器学习方法的潜力，以使用声学图作为输入来预测活动区域的出现。

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 1 篇论文

### An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection 
[[arxiv](https://arxiv.org/abs/2402.11403)] [[cool](https://papers.cool/arxiv/2402.11403)] [[pdf](https://arxiv.org/pdf/2402.11403)]
> **Authors**: Liying Han,Mani B. Srivastava
> **First submission**: 2024-02-17
> **First announcement**: 2024-02-19
> **comment**: No comments
- **标题**: 对实时多模式事件检测的神经和神经符号方法的经验评估
- **领域**: 人工智能
- **摘要**: 机器人和自主系统需要了解传感器数据的复杂事件（CES），以有效与其环境和人类相互作用。尽管有限的上下文大小和推理能力，传统的端到端神经体系结构，尽管有效地处理传感器数据，但仍与长期事件斗争。神经符号方法的最新进展，整合了利用人类知识的神经和符号模型，他们有望通过更少的数据提高性能。这项研究解决了理解这些方法在复杂事件检测中的有效性（CED）的差距，尤其是在时间推理中。我们在多模式CED任务中研究了神经和神经符号体系结构的性能，分析了IMU和声学数据流以识别CE模式。我们的方法包括（i）（i）端到端神经体系结构，用于从传感器嵌入中直接检测的直接CE检测，（ii）基于两阶段的基于概念的神经模型在CE检测前将嵌入到原子事件（AES）中，以及（iii）使用象征性的有限态机器检测AES的神经符号方法。从经验上讲，神经符号结构显着超过了神经模型，即使有广泛的训练数据和神经方法的大量时间环境，也证明了CE识别的卓越性能。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 7 篇论文

### Prosody in Cascade and Direct Speech-to-Text Translation: a case study on Korean Wh-Phrases 
[[arxiv](https://arxiv.org/abs/2402.00632)] [[cool](https://papers.cool/arxiv/2402.00632)] [[pdf](https://arxiv.org/pdf/2402.00632)]
> **Authors**: Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Accepted at Findings of EACL 2024
- **标题**: 级联和直接语音到文本翻译中的韵律：关于韩语WH-Sprass的案例研究
- **领域**: 计算语言学
- **摘要**: 语音到文本翻译（S2TT）通常是通过级联系统来解决的，其中语音识别系统会生成转录，随后传递给翻译模型。尽管人们对开发直接语音翻译系统的兴趣越来越多，以避免传播错误和失去非语言内容，但直接S2TT中的先前工作一直在努力确定将声学信号直接整合到翻译过程中的优势。这项工作提出使用对比度评估来定量测量直接S2TT系统消除双体奏发言的能力。具体而言，我们在包含WH-Sprass的测试集上评估了韩语 - 英语翻译系统，对于以正确的意图产生翻译的必要特征，无论是陈述，是/否问题，wh-询问等等。我们的结果清楚地表明了直接翻译系统比级联翻译模型的价值，模棱两可的情况下的总体准确性显着提高了12.9％，而主要目的类别之一的F1得分提高了15.6％。据我们所知，这项工作是第一个提供定量证据，表明直接S2TT模型可以有效利用韵律。我们的评估代码可公开访问，并可以自由地进行审查和利用。

### Systematic Literature Review: Computational Approaches for Humour Style Classification 
[[arxiv](https://arxiv.org/abs/2402.01759)] [[cool](https://papers.cool/arxiv/2402.01759)] [[pdf](https://arxiv.org/pdf/2402.01759)]
> **Authors**: Mary Ogbuka Kenneth,Foaad Khosmood,Abbas Edalat
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: 系统文献评论：幽默风格分类的计算方法
- **领域**: 计算语言学,人工智能,机器学习
- **摘要**: 了解各种幽默风格对于理解幽默的多方面性质及其对心理学和人工智能等领域的影响至关重要。这种理解表明，幽默取决于所采用的样式，可以对个人的健康和关系产生治疗或有害影响。尽管专门针对基于计算的幽默风格分析的研究仍然有些罕见，但相关任务的大量研究蓬勃发展，尤其是二元幽默和讽刺识别。在这项系统的文献综述（SLR）中，我们调查了应用于这些相关任务的计算技术的格局，并发现它们与幽默风格分析的基本相关性。通过这项研究，我们推出了通用方法，阐明了各种数据集和评估指标，并有效地浏览了幽默研究的复杂地形。我们的努力决定了潜在的研究差距，并概述了有希望的方向。此外，SLR还标识了一系列功能和计算模型，这些功能和计算模型可以从相关任务中无缝过渡，例如二进制幽默和讽刺检测，以激发幽默风格的分类。这些特征包括不一致，情感和极性分析，歧义检测，声学细微差别，视觉提示，上下文见解等。出现的计算模型包含传统的机器学习范例，神经网络体系结构，基于变压器的模型以及对幽默细微差别的专业模型。最后，SLR提供了对与幽默和讽刺有关的现有数据集的访问，从而促进了未来研究人员的工作。

### Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration 
[[arxiv](https://arxiv.org/abs/2402.03519)] [[cool](https://papers.cool/arxiv/2402.03519)] [[pdf](https://arxiv.org/pdf/2402.03519)]
> **Authors**: Xiliang Zhu,Chia-Tien Chang,Shayna Gardiner,David Rossouw,Jonas Robertson
> **First submission**: 2024-02-05
> **First announcement**: 2024-02-06
> **comment**: Accepted to UnImplicit workshop at EACL 2024
- **标题**: 西班牙语的解决转录歧义：一种用于标点符号修复的混合声学系统
- **领域**: 计算语言学,人工智能
- **摘要**: 标点符号修复是自动语音识别（ASR）系统后的关键步骤，可增强成绩单的可读性并促进随后的NLP任务。然而，传统的基于词汇的方法不足以解决西班牙语中的标点符号修复任务，在这种情况下，在未符合的宣告和问题之间经常发现歧义。在这项研究中，我们提出了一种新型的杂交声学 - 声学标点符号修复系统，用于西班牙转录，该系统通过模块化过程巩固了声学和词汇信号。我们的实验结果表明，所提出的系统可以有效地提高问号的F1评分，并在公共和西班牙对话数据集中恢复整体标点符号。此外，与LLMS（大语言模型）的基准比较表明了我们在准确性，可靠性和延迟方面的优越性。此外，我们证明了ASR模块的单词错误率（WER）也从我们提出的系统中受益。

### Phonetically rich corpus construction for a low-resourced language 
[[arxiv](https://arxiv.org/abs/2402.05794)] [[cool](https://papers.cool/arxiv/2402.05794)] [[pdf](https://arxiv.org/pdf/2402.05794)]
> **Authors**: Marcellus Amadeus,William Alberto Cruz Castañeda,Wilmer Lobato,Niasche Aquino
> **First submission**: 2024-02-08
> **First announcement**: 2024-02-09
> **comment**: No comments
- **标题**: 低资源语言的语音丰富的语料库结构
- **领域**: 计算语言学,人工智能
- **摘要**: 语音技术依靠捕获说话者的语音变异性，同时获得全面的语言信息。文献中已经提出了文本提示和句子选择方法，以包含这种适当的语音数据，称为语音丰富的\ textit {colpus}。但是，它们仍然不足以进行声学建模，对于资源有限的语言尤其重要。因此，本文提出了一种新颖的方法，并概述了创建\ textit {copus}所需的方法论方面，并为低资源的语言巴西葡萄牙语提供了广泛的语音覆盖范围。我们的方法包括基于Triphone分布的句子选择算法的文本数据集收集。此外，我们根据声学发言性的语音特征提出了一种新的音素分类，因为不同的Triphone或低概率的Triphones的绝对数量不能保证对每种可能组合的足够代表。使用我们的算法，我们获得了55.8％的不同三键百分比（对于相似大小的样品），而当前可用的富含语音的语料库，CETUC和TTS-葡萄干，12.6 \％\％和12.3 \％与非语音丰富的数据集相比。

### Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models 
[[arxiv](https://arxiv.org/abs/2402.05581)] [[cool](https://papers.cool/arxiv/2402.05581)] [[pdf](https://arxiv.org/pdf/2402.05581)]
> **Authors**: Maxime Fily,Guillaume Wisniewski,Severine Guillaume,Gilles Adda,Alexis Michaud
> **First submission**: 2024-02-08
> **First announcement**: 2024-02-09
> **comment**: Published in Findings of the EACL2024
- **标题**: 使用大规模的跨语义模型，建立沿不同维度的音频记录之间的亲密程度
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 在低资源语言研究的高度约束背景中，我们探索了审慎模型的语音矢量表示，以确定其在音频信号方面的抽象水平。我们提出了一种新的无监督方法，该方法使用ABX测试在带有精心策划的元数据的音频记录上，以阐明表示表示中存在的信息类型。 ABX测试确定了由多语言语音模型计算的表示形式是否编码给定特征。设计了三个实验：一个实验在房间的声学方面，一个在语言流派上，另一个在语音方面。结果证实，从具有不同语言/语言特征的不同记录中提取的表示形式在相同的线上有所不同。将更多音频信号嵌入一个矢量中，更好地区分语言外特征，而较短的片段最好区分分段信息。该方法是完全无监督的，有可能开放的新研究途径，用于对文献不足的语言的比较工作。

### It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition 
[[arxiv](https://arxiv.org/abs/2402.05457)] [[cool](https://papers.cool/arxiv/2402.05457)] [[pdf](https://arxiv.org/pdf/2402.05457)]
> **Authors**: Chen Chen,Ruizhe Li,Yuchen Hu,Sabato Marco Siniscalchi,Pin-Yu Chen,Ensiong Chng,Chao-Han Huck Yang
> **First submission**: 2024-02-08
> **First announcement**: 2024-02-09
> **comment**: Accepted to ICLR 2024, 17 pages. This work will be open sourced under MIT license
- **标题**: 永远不会太晚：将声学信息融合到大语言模型中以自动语音识别
- **领域**: 计算语言学,人工智能,多媒体,声音,音频和语音处理
- **摘要**: 最近的研究成功表明，大型语言模型（LLM）可以成功用于自动语音识别（ASR）输出之上的生成误差校正（GER）。具体而言，LLM用于从ASR系统生成的N-最佳假设列表到预测的输出转录。但是，尽管具有有效性，但GER还是引入了额外的数据不确定性，因为LLM经过训练而未考虑语音信号中可用的声学信息。在这项工作中，我们的目标是通过在新的晚期融合溶液产生预测的转录之前，通过注入声学信息来克服这种限制，称为不确定性感知动态融合（UADF）。 UADF是一种多模式融合方法，该方法在自动回归解码过程中实施，并在两个阶段进行工作：（i）首先分析和校准令牌级别的LLM决策，然后（ii）它从声学模式中动态地吸收了信息。从各种ASR任务中收集的实验证据表明，UADF以多种方式超过了现有的融合机制。它在融合过程中唯一依赖于唯一的方式来解决数据错误问题，并缓解数据不确定性问题并解决较差的概括。我们还证明，UADF无缝地适应了视听语音识别。

### Autism Detection in Speech -- A Survey 
[[arxiv](https://arxiv.org/abs/2402.12880)] [[cool](https://papers.cool/arxiv/2402.12880)] [[pdf](https://arxiv.org/pdf/2402.12880)]
> **Authors**: Nadine Probol,Margot Mieskes
> **First submission**: 2024-02-20
> **First announcement**: 2024-02-21
> **comment**: Accepted to EACL 2024 Findings
- **标题**: 语音中的自闭症检测 - 调查
- **领域**: 计算语言学
- **摘要**: 关于语音，语音和语言如何显示自闭症的研究已经进行了一系列研究。我们分析了来自生物医学以及心理领域的研究，也分析了NLP领域的研究，以找到可能表明自闭症的语言，韵律和声学线索。我们的调查着眼于所有三个领域。我们定义自闭症，哪些合并症可能会影响对疾病的正确检测。我们尤其要研究诸如言语和语义流利，韵律特征等观察结果，以及反思和说话率。我们还展示了基于单词的方法，并描述了在音频数据和成绩单上的基于机器学习和基于变压器的方法。最后，我们得出结论，尽管已经进行了大量研究，但女性患者似乎已经严重研究了。同样，大多数NLP研究都集中在传统的机器学习方法上，而不是在这种情况下可能有益的变压器。此外，我们找不到结合音频和笔录的功能的研究。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 13 篇论文

### Binding Touch to Everything: Learning Unified Multimodal Tactile Representations 
[[arxiv](https://arxiv.org/abs/2401.18084)] [[cool](https://papers.cool/arxiv/2401.18084)] [[pdf](https://arxiv.org/pdf/2401.18084)]
> **Authors**: Fengyu Yang,Chao Feng,Ziyang Chen,Hyoungseob Park,Daniel Wang,Yiming Dou,Ziyao Zeng,Xien Chen,Rit Gangopadhyay,Andrew Owens,Alex Wong
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: No comments
- **标题**: 对所有事物的约束触摸：学习统一的多模式触觉表示
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 将触摸与其他方式关联的能力对人类和计算系统具有巨大的影响。但是，由于昂贵的数据收集过程和非标准化的传感器输出，具有触摸的多模式学习仍然具有挑战性。我们介绍了Unitouch，这是一种统一的触觉模型，用于基于视觉的触摸传感器，该模型连接到多种方式，包括视觉，语言和声音。我们通过将单元嵌入与已经与其他各种模式相关的预处理的图像嵌入来对齐来实现这一目标。我们进一步提出了可学习的传感器特异性令牌，使该模型可以同时从一组异质触觉传感器中学习。 UniTouch能够在零击设置中执行各种触摸感应任务，从机器人抓住预测到触摸图像问题的回答。据我们所知，Unitouch是第一个展示此类功能的人。项目页面：https：//cfeng16.github.io/unitouch/

### From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information 
[[arxiv](https://arxiv.org/abs/2401.17981)] [[cool](https://papers.cool/arxiv/2401.17981)] [[pdf](https://arxiv.org/pdf/2401.17981)]
> **Authors**: Qirui Jiao,Daoyuan Chen,Yilun Huang,Yaliang Li,Ying Shen
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: 32 pages, 22 tables, 7 figures
- **标题**: 从无培训到自适应：对MLLM对检测信息的理解的经验见解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 尽管多模式大语言模型（MLLM）在整合文本和图像模式方面具有令人印象深刻的功能，但仍在准确解释详细的视觉元素方面仍然存在挑战。视觉检测模型在识别细粒度细节的范围内表现出色，促使研究人员使用它们来增强MLLM。一种有效的策略是以文本格式注入检测信息，这已被证明简单有效。但是，大多数研究都在没有培训的情况下使用这种方法，从而使自适应训练的潜力在很大程度上没有探索。自适应训练可以显着增强MLLM对独特输入的理解，同时滤除无关的信息。本文解决了一个关键问题：培训如何影响MLLM对注入文本检测信息的理解？我们系统地尝试各种代表性模型，以评估无训练，再培训和微调策略的影响。我们还研究了训练对MLLM的原始能力的影响以及检测模型的互换性。我们的发现表明，与无训练和再培训方法相比，对预训练的MLLM进行微调以融合了文本检测信息，从而在10个众所周知的基准中提高了6.71％的效果。此外，微型调整使MLLM即使交换了检测模型，也可以保留性能增强功能，这表明对格式化文本数据的理解有所提高。我们发布我们的代码，以支持对视觉检测模型的融合策略的进一步探索，并增强MLLM的细粒多模式能力。

### ControlCap: Controllable Region-level Captioning 
[[arxiv](https://arxiv.org/abs/2401.17910)] [[cool](https://papers.cool/arxiv/2401.17910)] [[pdf](https://arxiv.org/pdf/2401.17910)]
> **Authors**: Yuzhong Zhao,Yue Liu,Zonghao Guo,Weijia Wu,Chen Gong,Fang Wan,Qixiang Ye
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: https://github.com/callsys/ControlCap
- **标题**: ControlCAP：可控区域级字幕
- **领域**: 计算机视觉和模式识别
- **摘要**: 标题变性问题挑战了区域级的字幕，该问题是指预先训练的多模式模型倾向于预测最频繁的标题，但错过了较不频繁的标题。在这项研究中，我们提出了可控的区域级字幕（ControlCAP）方法，该方法将控制词引入多模型模型以解决标题变性问题。在具体而言，ControlCAP利用一个区分模块在标题空间内生成控制单词，以将其划分为多个子空间。多模型模型受到限制，以在包含控制词的几个子空间内生成字幕，这增加了命中频率较低的字幕的机会，从而减轻了标题变性问题。此外，交互式控制词可以由人类或专家模型给出，该模型可以超越训练标题空间的字幕，从而增强了模型的概括能力。关于视觉基因组和Refcocog数据集的广泛实验表明，控制范围分别提高了苹果酒评分的21.6和2.2，表现优于最先进的边缘。代码可在https://github.com/callsys/controlcap上找到。

### PVLR: Prompt-driven Visual-Linguistic Representation Learning for Multi-Label Image Recognition 
[[arxiv](https://arxiv.org/abs/2401.17881)] [[cool](https://papers.cool/arxiv/2401.17881)] [[pdf](https://arxiv.org/pdf/2401.17881)]
> **Authors**: Hao Tan,Zichang Tan,Jun Li,Jun Wan,Zhen Lei
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: 15 pages, 8 figures
- **标题**: PVLR：多标签图像识别的及时驱动的视觉语言表示学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 多标签图像识别是计算机视觉中的基本任务。最近，视觉模型在该领域取得了显着的进步。但是，以前的方法通常无法有效利用语言模型中的丰富知识，而是以单向方式将标签语义纳入视觉特征。在本文中，我们提出了一个迅速驱动的视觉语言表示学习（PVLR）框架，以更好地利用语言方式的能力。在PVLR中，我们首先引入了包括知识意识提示（KAP）和上下文感知提示（CAP）的双重宣传策略。 KAP利用固定的提示来捕获所有标签上的内在语义知识和关系，而CAP则使用可学习的提示来捕获上下文感知的标签语义和关系。后来，我们提出了一个相互作用和融合模块（IFM），以相互作用并融合从KAP和CAP获得的表示。与以前的作品中的单向融合相反，我们引入了双模式的关注（DMA），该注意力可以在文本和视觉特征之间进行双向相互作用，从而产生上下文感知的标签表示和与语义相关的视觉表示，随后用于计算所有标签的相似性并为所有标签生成最终预测。在包括MS-Coco，Pascal VOC 2007和NUS范围的三个流行数据集上进行了广泛的实验，证明了PVLR的优势。

### Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis 
[[arxiv](https://arxiv.org/abs/2401.17862)] [[cool](https://papers.cool/arxiv/2401.17862)] [[pdf](https://arxiv.org/pdf/2401.17862)]
> **Authors**: Jianing Li,Xi Nan,Ming Lu,Li Du,Shanghang Zhang
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: 15 pages,version 1
- **标题**: 接近质量检查：释放多模式大语言模型的力量以进行空间接近分析
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大型语言模型（MLLM）表现出了显着的视觉语言功能，这主要是由于大型语言模型（LLMS）的特殊内在理解和多任务学习强度。视觉教学调整的出现进一步增强了MLLM在视觉理解中的表现。但是，虽然现有的mllms巧妙地识别\ textit {what}对象在图像中，但它们在有效辨别\ textit {where}中仍然面临着挑战，尤其是沿距离（场景深度）轴。为了克服MLLM中的这一限制，我们引入了接近问题答案（接近QA），这是一个新颖的框架，旨在使MLLMS推断图像中对象之间的接近性关系。该框架分为两个阶段：第一阶段着重于指导模型以了解对象的相对深度，第二阶段进一步鼓励模型根据对象的深度感知来推断对象之间的接近性关系。我们还提出了一个称为proximity-1110k的VQA数据集，其中包含包含深度信息和对象的接近关系的其他指令。我们已经进行了广泛的实验，以验证邻近质量质量质量质量检查的深度感知和接近性分析，表现优于其他最先进的MLLM。代码和数据集将在\ textColor {magenta} {https://github.com/northsummer/proximityqa.git}上发布。

### Instruction-Guided Scene Text Recognition 
[[arxiv](https://arxiv.org/abs/2401.17851)] [[cool](https://papers.cool/arxiv/2401.17851)] [[pdf](https://arxiv.org/pdf/2401.17851)]
> **Authors**: Yongkun Du,Zhineng Chen,Yuchen Su,Caiyan Jia,Yu-Gang Jiang
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: Accepted by TPAMI
- **标题**: 指导引导的场景文本识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 自由形式的文本指导训练唤起了了解细粒度的视觉内容的能力，多模型模型在视觉识别任务中表现出了吸引人的表现。但是，由于自然图像和文本图像之间的组成差异，目前的模型不能微不足道地应用于场景文本识别（STR）。 We propose a novel instruction-guided scene text recognition (IGTR) paradigm that formulates STR as an instruction learning problem and understands text images by predicting character attributes, e.g., character frequency, position, etc. IGTR first devises $\left \langle condition,question,answer\right \rangle$ instruction triplets, providing rich and diverse descriptions of character attributes.为了通过提问有效地学习这些属性，IGTR开发了轻巧的指令编码器，一个跨模式特征融合模块和多任务答案头，它指导细微的文本图像理解。此外，IGTR仅通过使用不同的指令就实现了不同的识别管道，从而实现了基于字符的文本推理范式，与当前方法有很大不同。英语和中文基准的实验表明，IGTR的表现优于现有模型，同时保持较小的模型大小和快速推理速度。此外，通过调整说明的采样，IGTR提供了一种优雅的方式来解决对很少出现和形态上相似角色的认识，这是以前的挑战。代码：https：//github.com/topdu/openocr。

### M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval 
[[arxiv](https://arxiv.org/abs/2401.17797)] [[cool](https://papers.cool/arxiv/2401.17797)] [[pdf](https://arxiv.org/pdf/2401.17797)]
> **Authors**: Xingning Dong,Zipeng Feng,Chunluan Zhou,Xuzheng Yu,Ming Yang,Qingpei Guo
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: No comments
- **标题**: M2-RAAP：一种多模式的配方，用于推进基于适应的预培训，以培训有效，有效地零摄影视频检索
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一种多模式配方，用于推进基于适应性的预训练，以进行有效，有效的零拍摄视频检索，称为M2-RAAP。在流行的图像文本模型（如剪辑）中，大多数当前基于适应性的视频文本预训练方法都面临三个主要问题，即嘈杂的数据语料库，耗时的预训练和有限的性能增长。为此，我们进行了一项全面的研究，其中包括视频文本预训练的四个关键步骤。具体来说，我们研究1）数据过滤和改进，2）视频输入类型选择，3）时间建模和4）视频功能增强。 We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the replacement of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features.我们通过对来自不同语言的两个精制视频文本数据集进行调整三个图像文本基础模型来进行广泛的实验，从而验证M2-RAAP对基于适应的预训练的鲁棒性和可重复性。结果表明，M2-RAAP可以通过大幅降低数据（-90％）和时间消耗（-95％），在四个英式零摄像数据集和两个中文数据集上产生卓越的性能。我们正在准备精致的双语数据注释和代码库，可在https://github.com/alipay/ant-multi-modal-framework/tree/main/main/main/main/prj/m2_raap上找到。

### SNP-S3: Shared Network Pre-training and Significant Semantic Strengthening for Various Video-Text Tasks 
[[arxiv](https://arxiv.org/abs/2401.17773)] [[cool](https://papers.cool/arxiv/2401.17773)] [[pdf](https://arxiv.org/pdf/2401.17773)]
> **Authors**: Xingning Dong,Qingpei Guo,Tian Gan,Qing Wang,Jianlong Wu,Xiangyuan Ren,Yuan Cheng,Wei Chu
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: Accepted by TCSVT (IEEE Transactions on Circuits and Systems for Video Technology)
- **标题**: SNP-S3：共享网络预训练和各种视频文本任务的大量语义加强
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 我们通过直接对原始数据进行预训练，以促进各种下游视频文本任务来学习跨模式视频表示的框架。我们的主要贡献在于培训前框架和代理任务。首先，基于两个主流像素级预训练架构（有限的应用程序或效率较低）的缺点，我们建议共享网络预培训（SNP）。通过使用一个共享的BERT型网络同时完善文本和跨模式功能，SNP轻量轻巧，可以支持各种下游应用程序。其次，基于人们在理解句子时始终关注几个“重要单词”的直觉，我们提出了重要的语义加强（S3）策略，其中包括一项新颖的掩盖和匹配代理任务，以促进培​​训前训练。在三个下游视频文本任务和六个数据集上进行的实验表明，我们在Pixel级视频文本预训练中建立了新的最新技术；我们还在训练效率和微调性能之间取得了令人满意的平衡。该代码库可从https://github.com/alipay/ant-multi-modal-framework/tree/main/main/prj/snps3_vtp获得。

### Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation 
[[arxiv](https://arxiv.org/abs/2401.17664)] [[cool](https://papers.cool/arxiv/2401.17664)] [[pdf](https://arxiv.org/pdf/2401.17664)]
> **Authors**: Yuanhuiyi Lyu,Xu Zheng,Lin Wang
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: No comments
- **标题**: 图像任何东西：朝着推理和无训练的多模式图像生成
- **领域**: 计算机视觉和模式识别,图形
- **摘要**: 人类感知和理解的多方面性质表明，当我们认为我们的身体自然可以采取任何感官的组合，又称模式，并在我们的大脑中形成美丽的画面。例如，当我们看到一个猫会和同时感知猫的声音时，我们的大脑就可以在帕特里（Cattery）构造猫的照片。在直觉上，生成的AI模型应保持人类的多功能性，并能够有效，合作地产生图像。本文介绍了Imgany，这是一种新型的端到端多模式生成模型，可以模仿人类的推理并产生高质量的图像。我们的方法是其有效，灵活地采用七种方式的能力的首次尝试，从语言，音频到视觉方式，包括图像，点云，热，深度和事件数据。我们的关键思想是受到人类水平的认知过程的启发，涉及在实体和属性级别上的多种输入方式的整合和协调，而无需跨模态进行特定调整。因此，我们的方法带来了两个新颖的无培训技术分支：1）实体融合分支确保输入和输出之间的连贯性。它从由我们特殊构造的实体知识图提供支持的多模式表示中提取实体特征； 2）属性融合分支熟练地保留并处理属性。它通过我们提出的属性知识图有效地将不同的输入方式与不同的输入方式合并。最后，将实体和属性特征自适应地融合为图像生成预先训练的稳定扩散模型的条件输入。在各种模态组合下进行的广泛实验证明了其出色的视觉内容创建能力。

### Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition 
[[arxiv](https://arxiv.org/abs/2401.17604)] [[cool](https://papers.cool/arxiv/2401.17604)] [[pdf](https://arxiv.org/pdf/2401.17604)]
> **Authors**: Lei Liu,Li Liu,Haizhou Li
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: Accepted by TASLP
- **标题**: 用于提示语音识别的计算和参数有效的多模式融合变压器
- **领域**: 计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 提示语音（CS）是一种纯粹的视觉编码方法，由听力受损的人使用，将唇部阅读与几种特定的手工形状结合在一起，使口语可见。自动CS识别（ACSR）试图将语音的视觉提示转录到文本中，这可以帮助听力受损的人有效沟通。 CS的视觉信息包含唇部阅读和手动提示，因此它们的融合在ACSR中起着重要作用。但是，大多数以前的融合方法都难以捕获多模式CS数据的长序列输入中存在的全局依赖性。结果，这些方法通常无法学习有助于融合的有效跨模式关系。最近，基于注意力的变压器是一个普遍的想法，可以在多模式融合中捕获长序列的全球依赖性，但是现有的多模式融合变压器既遇到了识别精度差，又遭受了ACSR任务的效率低下计算。为了解决这些问题，我们通过提出一种新颖的令牌 - 物种感知注意机制（TIAA）来开发一种新颖的计算和参数有效的多模式融合变压器，其中将令牌利用率（TUR）配制为从多模式流中选择重要的令牌。更确切地说，TIAA首先在每种模式的所有令牌上对模式特异性的细粒度依赖性进行建模，然后了解与不同模态的重要令牌相对于模态共享的粗粒度的时间依赖性的有效跨模式相互作用。此外，轻巧的门控隐藏投影旨在控制TIAA的特征流。与现有的基于变压器的融合方法和ACSR融合方法相比，所得模型称为经济提示的语音融合变压器（Ecococued）（Ecococued），在所有现有CS数据集上实现了最先进的性能。

### AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion 
[[arxiv](https://arxiv.org/abs/2402.03309)] [[cool](https://papers.cool/arxiv/2402.03309)] [[pdf](https://arxiv.org/pdf/2402.03309)]
> **Authors**: Mohamad Qadri,Kevin Zhang,Akshay Hinduja,Michael Kaess,Adithya Pediredla,Christopher A. Metzler
> **First submission**: 2024-02-05
> **First announcement**: 2024-02-06
> **comment**: SIGGRAPH 2024 (conference track full paper). First two authors contributed equally. Paper website: https://aoneus.github.io/
- **标题**: Aoneus：声学传感器融合的神经渲染框架
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 水下感知和3D表面重建是在建筑，安全，海洋考古和环境监测中广泛应用的挑战性问题。危险的操作条件，脆弱的环境和有限的导航控制通常决定了沉带限制其运动范围，从而限制了他们可以捕获测量值的基线。在3D场景重建的背景下，众所周知，较小的基线使重建更具挑战性。我们的工作开发了一个基于物理学的多模式声学神经表面重建框架（AONEUS），能够有效地将高分辨率的RGB测量与低分辨率深度分辨成像声纳测量结果相结合。通过融合这些互补方式，我们的框架可以从在受限制性的基线上捕获的测量值重建准确的高分辨率3D表面。通过大量的模拟和LAB实验，我们证明了Aoneus极大地超过了最新的仅RGB，而仅通过声纳的逆差异差异且基于呈现的基于渲染的表面重建方法。一个可视化论文结果的网站位于此地址：https：//aoneus.github.io/

### SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.04178)] [[cool](https://papers.cool/arxiv/2402.04178)] [[pdf](https://arxiv.org/pdf/2402.04178)]
> **Authors**: Yichen Shi,Yuhao Gao,Yingxin Lai,Hongyang Wang,Jun Feng,Lei He,Jun Wan,Changsheng Chen,Zitong Yu,Xiaochun Cao
> **First submission**: 2024-02-06
> **First announcement**: 2024-02-07
> **comment**: No comments
- **标题**: 盾：通过多模式模型的面部欺骗和伪造检测的评估基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大型语言模型（MLLM）基于强大的视觉语义表示和语言推理能力，在各种视觉领域（例如，通用对象识别和接地）中表现出了显着的解决问题的能力。但是，MLLM是否对微妙的视觉欺骗/锻造线索敏感，以及它们在面部攻击检​​测领域的表现（例如，面部欺骗和伪造检测）仍然没有探索。在本文中，我们介绍了一种新的基准，即屏蔽，以评估MLLM在面部欺骗和伪造检测中的能力。具体来说，我们设计了True/fals和多项选择问题，以评估这两个面部安全任务中的多模式面部数据。对于面部反欺骗任务，我们在四种类型的演示攻击（即打印攻击，重播攻击，刚性面具，纸面罩）下评估了三种不同的方式（即RGB，红外，深度）。对于面对伪造的检测任务，我们以视觉和声学方式评估了基于GAN的基于GAN和基于扩散的数据。每个问题都在标准和思想链（COT）设置下进行零射击和少量测试。结果表明，MLLM在面部安全域中具有巨大的潜力，就可解释性，多模式柔性推理以及联合面部欺骗和伪造检测提供了比传统特定模型的优势。此外，我们开发了一种新颖的多属性思想链（MA-COT）范式，用于描述和判断面部图像的各种特定任务和任务 - 近乎范围的属性，该属性为您提供了丰富的任务相关知识，以实现微妙的欺骗/伪造线索挖掘。在单独的面部抗散热，单独的面部伪造检测和联合检测任务中进行的广泛实验证明了所提出的MA-COT的有效性。该项目可从https $：$ // github.com/laiyingxin2/shield获得

### AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies 
[[arxiv](https://arxiv.org/abs/2402.13152)] [[cool](https://papers.cool/arxiv/2402.13152)] [[pdf](https://arxiv.org/pdf/2402.13152)]
> **Authors**: José-M. Acosta-Triana,David Gimeno-Gómez,Carlos-D. Martínez-Hinarejos
> **First submission**: 2024-02-20
> **First announcement**: 2024-02-21
> **comment**: Accepted at the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)
- **标题**: Annotheia：用于视听语音技术的半自动注释工具包
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 世界各地都有7,000多种已知语言。但是，由于缺乏带注释的资源，语音技术目前只有一小部分。尽管自我监督的语音表现形式，最近的大规模言语语料库集以及挑战的组织已经减轻了这种不平等，但大多数研究主要以英语为基准。当解决了涉及声学和视觉语音方式的任务时，这种情况就会加剧。为了促进对视听语音技术的低资源语言的研究，我们提出了Annotheia，Annotheia是一种半自动注释工具包，该工具包检测到一个人何时在现场讲话和相应的转录。此外，为了展示为一种感兴趣的语言准备安托斯（Annotheia）的完整过程，我们还描述了预先训练的模型用于主动扬声器检测到西班牙语的模型，并使用最初未针对此类任务构想的数据库。 Annotheia工具包，教程和预培训模型可在GitHub上找到。

## 信息检索(cs.IR:Information Retrieval)

该领域共有 3 篇论文

### Towards Semantic Consistency: Dirichlet Energy Driven Robust Multi-Modal Entity Alignment 
[[arxiv](https://arxiv.org/abs/2401.17859)] [[cool](https://papers.cool/arxiv/2401.17859)] [[pdf](https://arxiv.org/pdf/2401.17859)]
> **Authors**: Yuanyi Wang,Haifeng Sun,Jiabo Wang,Jingyu Wang,Wei Tang,Qi Qi,Shaoling Sun,Jianxin Liao
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: arXiv admin note: text overlap with arXiv:2307.16210 by other authors
- **标题**: 迈向语义一致性：Dirichlet Energy驱动强大的多模式实体对齐
- **领域**: 信息检索
- **摘要**: 在多模式知识图（MMKG）中，多模式实体对齐（MMEA）对于识别各种模态属性的相同实体至关重要。但是，语义上的不一致性主要是由于缺失的模态属性构成了重大挑战。传统方法取决于属性插值，但这通常会引入模态噪声，从而扭曲了原始语义。此外，缺乏普遍的理论框架限制了实现语义一致性方面的进步。这项研究介绍了一种新颖的方法，即DeSalign，该方法通过应用基于Dirichlet Energy的理论框架来确保语义一致性来解决这些问题。我们发现，语义不一致会导致模型过度适应模态噪声，从而导致性能波动，尤其是在缺失方式时。使用现有模态，脱水与过度平滑的语义作战，并在缺乏语义上进行了插值。我们的方法包括一种多模式知识图学习策略和传播技术，该技术采用现有的语义特征来补偿缺失的语义特征，从而提供明确的Euler解决方案。在包括单语和双语场景在内的60个基准分割的全面评估表明，DeSalign超过了现有方法，为性能设定了新的标准。高度缺失率的进一步测试证实了其稳健性，为现实世界中MMKG的语义不一致提供了有效的解决方案。

### PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning 
[[arxiv](https://arxiv.org/abs/2402.17188)] [[cool](https://papers.cool/arxiv/2402.17188)] [[pdf](https://arxiv.org/pdf/2402.17188)]
> **Authors**: Wei Wei,Jiabin Tang,Yangqin Jiang,Lianghao Xia,Chao Huang
> **First submission**: 2024-02-26
> **First announcement**: 2024-02-27
> **comment**: WWW 2024
- **标题**: 提示：及时调整推荐的多模式知识蒸馏
- **领域**: 信息检索
- **摘要**: 多媒体在线平台（例如，亚马逊，Tiktok）大大受益于将多媒体（例如视觉，文本和声学）内容纳入其个人推荐系统。这些方式提供了直观的语义，可促进模式感知的用户偏好建模。但是，多模式推荐器中的两个关键挑战仍未解决：i）引入多模式编码器，具有大量其他参数原因过于拟合，鉴于提取器提供的高维多模式特征（例如VIT，BERT）。 ii）侧面信息不可避免地引入了不准确和冗余，这使模式相互作用的依赖性偏向反映真实的用户偏好。为了解决这些问题，我们建议通过多模式知识蒸馏（提示）简化和授权推荐人，并迅速调整能够自适应质量蒸馏。具体而言，提示性通过蒸馏u-i边缘关系和繁琐的教师的多模式节点内容进行模型压缩，以使学生免于其他功能降低参数。为了弥合多模式上下文和协作信号之间的语义差距，以赋予过度拟合的教师，引入了软及时调整以执行学生任务自适应。此外，为了调整不准确性在多媒体数据中的影响，通过模态感知到的重新加权机制开发了分离的多模式列表蒸馏。关于现实世界数据的实验证明了提示与现有技术的优势。消融测试证实了关键组件的有效性。其他测试表明效率和有效性。

### Effect of utterance duration and phonetic content on speaker identification using second-order statistical methods 
[[arxiv](https://arxiv.org/abs/2402.16429)] [[cool](https://papers.cool/arxiv/2402.16429)] [[pdf](https://arxiv.org/pdf/2402.16429)]
> **Authors**: Ivan Magrin-Chagnolleau,Jean François Bonastre,Frédéric Bimbot
> **First submission**: 2024-02-26
> **First announcement**: 2024-02-27
> **comment**: ef:Eurospeech 1995, Sep 1995, Madrid, Spain. pp.337-340
- **标题**: 使用二阶统计方法的话语持续时间和语音内容对说话者识别的影响
- **领域**: 信息检索,信号处理
- **摘要**: 二阶统计方法在受控记录条件下对自动扬声器识别显示出非常好的结果。这些方法通常用于可用的整个语音材料。在本文中，我们研究了测试语音材料含量对此类方法的性能的影响，即在更分析的方法下。目的是研究这些方法使用的信息类型，以及它位于语音信号中的位置。发现液体和滑动液，元音，尤其是鼻音元音和鼻音辅音，特别是说话者：1秒钟的测试话语，这些班级中的大多数声学材料都可以提供比语音平衡的测试说法更好的扬声器识别结果，即使在两种情况下都进行了15秒的训练，并且在15秒的情况下都完成了训练。然而，与其他音素类别的结果永远不会很差。这些结果倾向于表明，长期二阶统计捕获的说话者依赖性信息始终是所有语音类别的共同点，并且测试材料的同质性可以提高估计值的质量。

## 机器学习(cs.LG:Machine Learning)

该领域共有 2 篇论文

### Graph Multi-Similarity Learning for Molecular Property Prediction 
[[arxiv](https://arxiv.org/abs/2401.17615)] [[cool](https://papers.cool/arxiv/2401.17615)] [[pdf](https://arxiv.org/pdf/2401.17615)]
> **Authors**: Hao Xu,Zhengyang Zhou,Pengyu Hong
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-01
> **comment**: No comments
- **标题**: 用于分子属性预测的多相似学习图
- **领域**: 机器学习,计算工程、金融和科学
- **摘要**: 增强准确的分子属性预测取决于有效和熟练的表示学习。至关重要的是结合以分子之间多相似（自相似性和相对相似性）为特征的各种分子关系。但是，当前的分子表示方法在探索多相似性方面缺乏缺乏，并且常常低估了分子之间关系的复杂性。此外，以前的多相似性方法需要将正面和负对的规范归因于不同的相对相似性，这可能会引入潜在的偏见。在这项工作中，我们介绍了分子性质预测（GraphMSL）框架的图形多相似性学习，以及一种新的方法来制定广义的多相似度度量，而无需定义正面和负对。在正在考虑的每个化学模态空间（例如分子描绘图像，指纹，NMR和笑容）中，我们首先定义一个自相似度指标（即，锚固分子和另一个分子之间的相似性），然后将其转换为通过配对量的锚定锚定为锚定的一般性多相似度计量。 GraphMSL验证了跨分子数据集的多相似度度量的功效。此外，这些模式的这些指标都集成到多模式的多模式指标中，从而展示了提高性能的潜力。此外，可以通过更改融合功能来重定向或定制模型的焦点。最后但并非最不重要的一点是，GraphMSL通过对学到的表示形式进行事后分析证明在药物发现评估中有效。

### On Designing Features for Condition Monitoring of Rotating Machines 
[[arxiv](https://arxiv.org/abs/2402.09957)] [[cool](https://papers.cool/arxiv/2402.09957)] [[pdf](https://arxiv.org/pdf/2402.09957)]
> **Authors**: Seetaram Maurya,Nishchal K. Verma
> **First submission**: 2024-02-15
> **First announcement**: 2024-02-16
> **comment**: No comments
- **标题**: 在设计旋转机的条件监视的功能上
- **领域**: 机器学习,信号处理
- **摘要**: 已经提出了使用一维原始传感器数据在旋转机中设计输入功能的各种方法。可用的方法是复杂的，依赖于经验方法，并且可能取决于使用的条件监视数据。因此，本文提出了一种新型算法来设计输入特征，该特征统一了不同时间序列传感器数据的特征提取过程。设计/提取输入特征的新见解是通过直方图理论的镜头获得的。所提出的算法提取了歧视性输入特征，该特征适用于简单的分类器与深神网络分类器。设计的输入功能作为分类器的输入，并在单个框架中以用于机器条件识别的端到端培训。提出的方案已通过三个实时数据集进行了验证：a）声学数据集，b）CWRU振动数据集和c）IMS振动数据集。实时结果和比较研究表明，拟议方案对机器健康状况的预测有效性。

## 机器人技术(cs.RO:Robotics)

该领域共有 1 篇论文

### Active propulsion noise shaping for multi-rotor aircraft localization 
[[arxiv](https://arxiv.org/abs/2402.17289)] [[cool](https://papers.cool/arxiv/2402.17289)] [[pdf](https://arxiv.org/pdf/2402.17289)]
> **Authors**: Gabriele Serussi,Tamir Shor,Tom Hirshberg,Chaim Baskin,Alex Bronstein
> **First submission**: 2024-02-27
> **First announcement**: 2024-02-28
> **comment**: No comments
- **标题**: 多旋翼飞机本地化的主动推进噪声塑形
- **领域**: 机器人技术,人工智能
- **摘要**: 多旋翼航空自动驾驶汽车（MAV）主要依赖于导航目的的愿景。但是，在低阳​​光或直射的阳光下，视觉定位和进程技术的性能不佳，有限的视野以及易受阻塞的脆弱性。在许多情况下，声传感可以作为视觉的互补甚至替代方式，并且还具有较低的系统成本和能源足迹的额外好处，这对于微型飞机尤为重要。本文提议积极控制和塑造转子产生的飞机推进噪声，以使本地化任务受益，而不是认为这是有害的滋扰。我们为在已知的环境中提供了一种基于自我的本地化的神经网络体系结构。我们表明，与学习时变的转子相调制同时训练它可以实现准确且健壮的定位。使用在2D声学环境中对MAV转子噪声的计算负担得起的模拟来评估所提出的方法，该方法适合转子压力场的真实记录。

## 声音(cs.SD:Sound)

该领域共有 7 篇论文

### Identification of Cognitive Decline from Spoken Language through Feature Selection and the Bag of Acoustic Words Model 
[[arxiv](https://arxiv.org/abs/2402.01824)] [[cool](https://papers.cool/arxiv/2402.01824)] [[pdf](https://arxiv.org/pdf/2402.01824)]
> **Authors**: Marko Niemelä,Mikaela von Bonsdorff,Sami Äyrämö,Tommi Kärkkäinen
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: 从口语选择到特征选择和声学单词模型的认知能力下降的识别
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 记忆障碍是老年人功能下降和日常活动下降的核心因素。确认疾病，启动药物以减缓其进展以及旨在维持和修复认知能力的职业治疗的开始需要医学诊断。早期鉴定记忆障碍症状，尤其是认知能力的下降，在确保人口福祉方面起着重要作用。已知与语音生产有关的功能与说话者的认知能力和变化有关。在临床环境中缺乏标准化的语音测试，导致人们越来越强调开发自动机器学习技术来分析自然口语。在快速，具有成本效益和可扩展的解决方案的快速诊断时，口语的非静态但声学特性已被证明是有用的。这项工作提出了一种与特征选择相关的方法，可以自动选择日内瓦简约声学参数集和相对语音暂停所需的基本特征，该特征旨在自动副语言和临床语音分析。这些功能被完善为单词直方图特征，在该特征中，对机器学习分类器进行了培训，可以从痴呆症银行的PITT音频数据库中对控制对象和痴呆症患者进行分类。结果表明，只有25个功能，具有独立的Adress 2020竞赛测试数据，达到75％的平均分类准确性，并且可以对整个竞争数据进行一项剩余的跨标准交叉验证。与国际研究相比，结果排名最高，在国际研究中，仅使用相同的数据集和声学特征来诊断患者。

### A Data-Driven Analysis of Robust Automatic Piano Transcription 
[[arxiv](https://arxiv.org/abs/2402.01424)] [[cool](https://papers.cool/arxiv/2402.01424)] [[pdf](https://arxiv.org/pdf/2402.01424)]
> **Authors**: Drew Edwards,Simon Dixon,Emmanouil Benetos,Akira Maezawa,Yuta Kusaka
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted for publication in IEEESignalProcessing Letters on 31 Janurary, 2024
- **标题**: 强大的自动钢琴转录的数据驱动分析
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 由于新的数据集和建模技术，近年来，自动钢琴转录算法已大大改善。最近的发展主要集中在适应新的神经网络体系结构，例如变压器和感知器，以产生更准确的系统。在这项工作中，我们从培训数据的角度研究转录系统。通过衡量其在分发带注释的钢琴数据上的性能，我们展示了这些模型如何严重过度拟合训练数据的声学特性。我们为Maestro数据集创建了一组新的音频，并通过Yamaha Disklavier播放自动捕获在专业的工作室录音环境中。在Maestro数据集的原始版本和重新执行版本的培训时，使用各种数据增强技术，我们在地图数据集上实现了88.4 F1得分的最新注释精度，而没有看到任何培训数据。随后，我们在一系列消融研究中分析了这些数据增强技术，以更好地了解它们对所得模型的影响。

### On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification 
[[arxiv](https://arxiv.org/abs/2402.01274)] [[cool](https://papers.cool/arxiv/2402.01274)] [[pdf](https://arxiv.org/pdf/2402.01274)]
> **Authors**: Calum Heggan,Sam Budgett,Timothy Hospedales,Mehrdad Yaghoobi
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Camera Ready version as submitted to ICASSP SASB Workshop 2024. 5 pages, 2 figures, 3 tables
- **标题**: 关于将大规模自我选择转移到几声音频分类的性
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 近年来，自我监管的学习能力非常出色，可以从未标记的数据中学习强大的功能表示形式。通过自我实验预测的网络是下游任务的有效提取器，包括很少的学习。虽然对几次学习的无监督方法的评估在图像中已经建立了良好，但声学中显然不存在。这项研究通过评估大规模的自我监督模型的表现来解决这一差距。此外，我们探讨了模型的几个学习能力与其他下游任务基准之间的关系。我们的发现揭示了一些诸如SpeechCommandsv2之类的一些问题，以及基于语音的几局问题与各种下游音频任务之间的强烈相关性。

### Dual Knowledge Distillation for Efficient Sound Event Detection 
[[arxiv](https://arxiv.org/abs/2402.02781)] [[cool](https://papers.cool/arxiv/2402.02781)] [[pdf](https://arxiv.org/pdf/2402.02781)]
> **Authors**: Yang Xiao,Rohan Kumar Das
> **First submission**: 2024-02-05
> **First announcement**: 2024-02-06
> **comment**: Accepted to ICASSP 2024 (Deep Neural Network Model Compression Workshop)
- **标题**: 双重知识蒸馏以进行有效的声音事件检测
- **领域**: 声音,人工智能,计算语言学,机器学习,音频和语音处理
- **摘要**: 声音事件检测（SED）对于识别声学信号内的特定声音及其时间位置至关重要。这变得具有挑战性，尤其是对于计算资源受到限制的设备应用程序。为了解决这个问题，我们引入了一个新颖的框架，称为双重知识蒸馏，以在这项工作中开发有效的SED系统。我们提出的双重知识蒸馏始于时间平均知识蒸馏（TAKD），利用从学生模型参数的时间平均得出的平均学生模型。这使学生模型可以间接从预先训练的教师模型中学习，从而确保稳定的知识蒸馏。随后，我们引入了嵌入增强的特征蒸馏（EEFD），其中涉及将嵌入式蒸馏层纳入学生模型中以增强上下文学习。在DCASE 2023任务4A公共评估数据集上，我们提出的具有双重知识蒸馏的SED系统仅占基线模型参数的三分之一，在PSDS1和PSDS2方面表现出了出色的性能。这突出了建议的双重知识蒸馏对紧凑型SED系统的重要性，这对于边缘设备来说是理想的选择。

### Speech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI 
[[arxiv](https://arxiv.org/abs/2402.06984)] [[cool](https://papers.cool/arxiv/2402.06984)] [[pdf](https://arxiv.org/pdf/2402.06984)]
> **Authors**: Xiaofeng Liu,Fangxu Xing,Jiachen Zhuo,Maureen Stone,Jerry L. Prince,Georges El Fakhri,Jonghye Woo
> **First submission**: 2024-02-10
> **First announcement**: 2024-02-12
> **comment**: SPIE Medical Imaging 2024: Image Processing
- **标题**: 语音运动异常检测通过标记MRI的4D运动场的跨模式翻译检测
- **领域**: 声音,计算机视觉和模式识别,多媒体,音频和语音处理,图像和视频处理
- **摘要**: 了解语音期间的舌头运动模式及其由此产生的言语声学结果（即发音 - 声学关系）在评估语音质量以及制定创新的治疗和康复策略方面非常重要。在评估和检测与​​语音有关的疾病患者中评估和检测异常的关节特征时，这一点尤其重要。在这项工作中，我们旨在开发一个框架，以结合其相应的语音声学来检测语音运动异常。这是通过仅使用来自健康个体的数据训练的深层跨模式翻译器来实现的，该数据仅弥合了从标记的MRI获得的4D运动场与从语音声学数据得出的2D频谱图之间的差距。通过测量健康个体或患者的频谱重建质量，训练有素的翻译器被用作异常检测器。特别是，与健康个体相比，跨模式翻译器可能在患者数据上产生有限的概括能力，其中包括看不见的分发模式并证明表现不佳。为了验证我们的框架，我们总共收集了39个配对标签的MRI和语音波形，由36个健康个体和3个舌癌患者的数据组成。我们同时使用了3D卷积和基于变压器的深层翻译模型，在健康训练集中训练它们，然后将其应用于健康和患者的测试集中。我们的框架证明了检测异常患者数据的能力，从而说明了其增强对健康个体和患者的关节声音关系的理解的潜力。

### ML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal Processing Analysis for Sounds, & Strains Emerging Technology 
[[arxiv](https://arxiv.org/abs/2402.10005)] [[cool](https://papers.cool/arxiv/2402.10005)] [[pdf](https://arxiv.org/pdf/2402.10005)]
> **Authors**: Ratul Ali,Aktarul Islam,Md. Shohel Rana,Saila Nasrin,Sohel Afzal Shajol,A. H. M. Saifullah Sadi
> **First submission**: 2023-12-17
> **First announcement**: 2024-02-16
> **comment**: 7 pages, 5 figures, Article
- **标题**: ML-ASPA：对声音的基于机器学习的声学信号处理分析的沉思和新兴技术的压力
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **摘要**: 声学数据是跨不同学科，跨越生物学，通信以及海洋和地球科学的科学和工程理解的基本基石。这项探究精心探讨了声学领域内的最新进步和变革潜力，特别关注机器学习（ML）和深度学习。 ML包括广泛的统计技术，证明是在数据中自主辨别和利用模式必不可少的。与传统的声学和信号处理相反，ML采用了数据驱动的方法，揭示了特征和所需标签或动作之间的复杂关系，以及在功能本身之间，给出了足够的培训数据。 ML在广泛的训练数据集中的应用促进了阐明复杂声学现象（如人类语音和混响）的模型的发现。声学中ML的动态演化产生了令人信服的结果，并对未来具有实质性的希望。电子听诊器以及类似的记录和数据记录设备的出现，扩大了声学信号处理概念的应用到肠声分析中。本文批判性地回顾了有关肠道声音分析的声学信号处理的现有文献，概述了基本方法和适用的机器学习原理。它记录了信号处理技术中的历史进步，这些进步促进了从肠声中提取有价值信息的，强调了降噪，分割，信号增强，特征提取，声音定位和机器学习技术方面的进步...

### Phonetic and Lexical Discovery of a Canine Language using HuBERT 
[[arxiv](https://arxiv.org/abs/2402.15985)] [[cool](https://papers.cool/arxiv/2402.15985)] [[pdf](https://arxiv.org/pdf/2402.15985)]
> **Authors**: Xingyuan Li,Sinong Wang,Zeyu Xie,Mengyue Wu,Kenny Q. Zhu
> **First submission**: 2024-02-24
> **First announcement**: 2024-02-26
> **comment**: No comments
- **标题**: 使用休伯特的语音和词汇发现犬类语言
- **领域**: 声音,计算语言学,机器学习,音频和语音处理
- **摘要**: 本文深入研究了狗发声中潜在的沟通模式的开创性探索，并超越了传统的语言分析障碍，这些障碍在很大程度上依赖于人类对有限数据集的先验知识，以找到狗发声中的声音单位。我们通过休伯特（Hubert）提出了一种自我监督的方法，使音素标签的准确分类以及识别嗓音模式的识别，暗示了狗嗓音中的基本词汇。我们的发现表明，在这些已确定的犬词汇中具有显着的声学一致性，涵盖了整个观察到的狗嗓音序列。我们进一步开发了基于网络的狗发声标签系统。该系统可以突出显示用户上传的狗音频中存在的音素n-grams。

## 音频和语音处理(eess.AS:Audio and Speech Processing)

该领域共有 6 篇论文

### Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift 
[[arxiv](https://arxiv.org/abs/2402.02694)] [[cool](https://papers.cool/arxiv/2402.02694)] [[pdf](https://arxiv.org/pdf/2402.02694)]
> **Authors**: Jisheng Bai,Mou Wang,Haohe Liu,Han Yin,Yafei Jia,Siwei Huang,Yutong Du,Dongzhe Zhang,Dongyuan Shi,Woon-Seng Gan,Mark D. Plumbley,Susanto Rahardja,Bin Xiang,Jianfeng Chen
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: IEEE ICME 2024大挑战的描述：半监督的声学现场分类
- **领域**: 音频和语音处理,机器学习,声音
- **摘要**: 声学场景分类（ASC）是计算听觉场景分析中的关键研究问题，它旨在认识环境的独特声学特征。 ASC任务的挑战之一是训练和测试数据之间的域移动。自2018年以来，ASC挑战一直集中在ASC模型跨不同记录设备上的概括。尽管近年来，这项任务在设备概括方面取得了重大进展，但域之间的域挑战在不同的地理区域之间转移，涉及时间，空间，文化和语言等差异，目前仍未得到充分的探索。此外，考虑到现实世界中的大量未标记的声学场景数据，研究使用这些未标记数据的可能方法很重要。因此，我们在ICME 2024 Grand Challenge中介绍了在域转移下的半监督声学场景分类。我们鼓励参与者通过半监督的学习技术进行创新，旨在在域转移下开发更健壮的ASC模型。

### BAT: Learning to Reason about Spatial Sounds with Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01591)] [[cool](https://papers.cool/arxiv/2402.01591)] [[pdf](https://arxiv.org/pdf/2402.01591)]
> **Authors**: Zhisheng Zheng,Puyuan Peng,Ziyang Ma,Xie Chen,Eunsol Choi,David Harwath
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted to ICML 2024. Our demo, dataset, code and model weights are available at: https://zhishengzheng.com/BAT
- **标题**: 蝙蝠：学习用大语言模型来推理空间声音
- **领域**: 音频和语音处理,人工智能,计算语言学,声音
- **摘要**: 空间声音推理是一种基本的人类技能，使我们能够根据声音来浏览和解释周围环境。在本文中，我们介绍了BAT，它结合了双耳声学场景分析模型的空间声音感知能力与大语言模型（LLM）的自然语言推理能力（以复制这种先天能力）。为了解决缺乏现有的野外空间声音数据集，我们使用音频集和Soundspaces 2.0合成了双耳音频数据集。接下来，我们开发了SpatialSoundQA，这是一种基于空间声音的提问数据集，提供了一系列质量检查任务，这些任务在空间声音感知和推理的各个方面训练BAT。 BAT的声学前端编码器是一种新型的空间音频编码器，称为空间音频谱图变压器或空间 -  ast，它本身在声音事件检测，空间定位和距离估计中就可以实现强大的性能。通过将空间播种与Llama-2 7b模型集成，BAT超越了标准声音事件的本地化和检测（SELD）任务，使该模型能够推理其环境中声音之间的关系。我们的实验表明，BAT在空间声音感知和推理上的出色表现，展示了LLM在导航和解释复杂的空间音频环境中的巨大潜力。

### On combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification 
[[arxiv](https://arxiv.org/abs/2402.02865)] [[cool](https://papers.cool/arxiv/2402.02865)] [[pdf](https://arxiv.org/pdf/2402.02865)]
> **Authors**: Ascensión Gallardo-Antolín,Juan M. Montero
> **First submission**: 2024-02-05
> **First announcement**: 2024-02-06
> **comment**: ef:Ascension Gallardo-Antolin and Juan M. Montero Neurocomputing 456 (2021) 49-60
- **标题**: 关于在基于注意力LSTM的系统中，用于语音清晰度分类的系统中的声学和调制频谱图
- **领域**: 音频和语音处理,机器学习
- **摘要**: 语音清晰度可能会受到多种因素的影响，例如嘈杂的环境，渠道扭曲或生理问题。在这项工作中，我们处理在后一种情况下对语音可理解性水平的自动预测问题。从我们以前的工作开始，这是一个基于LSTM网络的非侵入性系统，其注意机制为此任务设计，我们提出了两个主要贡献。在第一个中，提出了使用人均调制频谱图作为输入特征的使用，而不是从它们中得出的紧凑表示，从而丢弃了重要的时间信息。在第二种中，探索了每种框架原子log-mel和调制频谱图中的两种不同策略：在决策水平或晚期融合，在话语级别或加权级别（WP）融合处。使用UA语音数据库评估所提出的模型，该数据库包含不同程度的严重程度的质心语音。一方面，结果表明，注意力LSTM网络能够充分对调制频谱序列进行充分建模，从而产生与日志频谱图相似的分类速率。另一方面，两种组合策略，晚期和WP融合都胜过单功能系统，这表明人均日志和调制频谱图具有互补的信息，以实现语音清晰度预测的任务，而不是由LSTM基于LSTM的体系结构来有效利用，这是WP融合策略的系统，可以使您的注意力和注意力融合了一个可以达到一个成果。

### UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL Models 
[[arxiv](https://arxiv.org/abs/2402.08898)] [[cool](https://papers.cool/arxiv/2402.08898)] [[pdf](https://arxiv.org/pdf/2402.08898)]
> **Authors**: Ruchao Fan,Natarajan Balaji Shanka,Abeer Alwan
> **First submission**: 2024-02-13
> **First announcement**: 2024-02-14
> **comment**: Published in IEEESignalProcessing Letters
- **标题**: Unienc-cassnat：语音SSL模型的仅编码非自动回旋ASR
- **领域**: 音频和语音处理,计算语言学,声音
- **摘要**: 非自动性自动语音识别（NASR）模型由于其并行性和快速推断而引起了人们的关注。基于编码器的NASR，例如连接派时间分类（CTC）可以从语音基础模型（SFM）初始化，但不能考虑中间令牌之间的任何依赖关系。基于编码器的NASR，例如基于CTC的单步非解放式变压器（CASS-NAT），可以减轻依赖关系问题，但无法有效地集成SFM。受到语音文本联合与共享变压器编码器的最新作品的成功启发，我们提出了一个新的基于编码器的NASR UNIENC-CASSNAT，以结合CTC和Cass-Nat的优势。 Unienc-cassnat仅由一个编码器作为主要模块组成，可以是SFM。编码器通过两个正向传球扮演Cass-Nat编码器和解码器的角色。编码器的第一个通过接受语音信号作为输入，而语音信号的串联和令牌级别的声学嵌入被用作第二次通过的输入。在Librispeech 100h，myst和aishell1数据集上进行了检查，所提出的Unienc-Cassnat取得了最新的NASR结果，并且与Cass-NAT更好或可比，仅使用编码器，因此，模型参数较少。我们的代码公开可用。

### Diffusion Models for Audio Restoration 
[[arxiv](https://arxiv.org/abs/2402.09821)] [[cool](https://papers.cool/arxiv/2402.09821)] [[pdf](https://arxiv.org/pdf/2402.09821)]
> **Authors**: Jean-Marie Lemercier,Julius Richter,Simon Welker,Eloi Moliner,Vesa Välimäki,Timo Gerkmann
> **First submission**: 2024-02-15
> **First announcement**: 2024-02-16
> **comment**: Currently in revision for IEEESignalProcessing Magazine Special Issue "Model-based and Data-Driven AudioSignalProcessing"
- **标题**: 音频恢复的扩散模型
- **领域**: 音频和语音处理,机器学习,声音
- **摘要**: 随着音频播放设备和快速数据传输的开发，对娱乐和通信的高音质需求正在上升。为了寻求更好的声音质量，挑战源自录制侧或由不完美的传输管道引起的扭曲和干扰。为了解决这个问题，音频恢复方法旨在从损坏的输入数据中恢复清洁的声音信号。我们在这里介绍基于扩散模型的音频恢复算法，重点是语音增强和音乐恢复任务。通常以手工规则和统计启发式为基础的传统方法已经塑造了我们对音频信号的理解。在过去的几十年中，朝着利用DNNS建模功能的数据驱动方法进行了显着转变。深层生成模型以及其中的扩散模型已成为学习复杂数据分布的强大技术。但是，仅依靠基于DNN的学习方法具有降低可解释性的风险，尤其是在采用端到端模型时。尽管如此，与基于统计模型的框架相比，数据驱动的方法可以更灵活，这些框架的性能取决于可能难以保证的分布和统计假设。在这里，我们的目的是表明，扩散模型可以结合两全其美，并提供机会设计音频恢复算法，并具有良好的解释性和在声音质量方面的出色表现。我们解释了扩散形式主义及其在有条件的清洁音频信号中的应用。我们认为，扩散模型开辟了一个令人兴奋的研究领域，并有可能在艰难的声学情况下产生新的音频恢复算法，并且在艰难的声学情况下保持强劲。

### When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection 
[[arxiv](https://arxiv.org/abs/2402.13276)] [[cool](https://papers.cool/arxiv/2402.13276)] [[pdf](https://arxiv.org/pdf/2402.13276)]
> **Authors**: Xiangyu Zhang,Hexin Liu,Kaishuai Xu,Qiquan Zhang,Daijiao Liu,Beena Ahmed,Julien Epps
> **First submission**: 2024-02-17
> **First announcement**: 2024-02-21
> **comment**: No comments
- **标题**: 当LLM遇到声学地标时：将语音整合到大语言模型中以进行抑郁症检测的有效方法
- **领域**: 音频和语音处理,人工智能,声音
- **摘要**: 抑郁症是全球心理健康的关键问题，促使人们对基于AI的检测方法进行了广泛的研究。在各种AI技术中，大型语言模型（LLMS）在心理保健应用中的多功能性脱颖而出。但是，它们的主要局限性源于其对文本输入的独家依赖，这限制了它们的整体功能。此外，LLM在识别和分析抑郁状态的利用仍然相对尚未开发。在本文中，我们提出了一种创新的方法，将声音语音信息整合到LLMS框架中以进行多模式抑郁症检测。我们通过将语音信号整合到利用声学标志的LLM中来研究一种有效的抑郁症检测方法。通过合并具有特定于口语的发音的声学地标，我们的方法为文本转录本增加了关键的维度。这种整合还为个人的独特语音模式提供了见解，从而揭示了个人的潜在心理状态。与现有音频文本基线相比，对DAIC-WOZ数据集的拟议方法的评估揭示了最先进的结果。此外，这种方法不仅对于抑郁症的检测很有价值，而且代表着增强LLMS理解和处理语音信号能力的新观点。

## 定量方法(q-bio.QM:Quantitative Methods)

该领域共有 1 篇论文

### All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data 
[[arxiv](https://arxiv.org/abs/2402.15360)] [[cool](https://papers.cool/arxiv/2402.15360)] [[pdf](https://arxiv.org/pdf/2402.15360)]
> **Authors**: Amanda K. Navine,Tom Denton,Matthew J. Weldy,Patrick J. Hart
> **First submission**: 2024-02-23
> **First announcement**: 2024-02-26
> **comment**: 14 pages, 6 figures, 3 tables; submitted to Frontiers in Bird Science; Our Hawaiian PAM dataset and classifier scores, as well as annotation information for the three study species, can be found on Zenodo at https://doi.org/10.5281/zenodo.10581530. The fully annotated Powdermill dataset assembled by Chronister et al. that was used in this study is available at https://doi.org/10.1002/ecy.3329
- **标题**: 所有禁止的阈值：直接估计生物声数据中的呼叫密度
- **领域**: 定量方法,机器学习,声音,音频和语音处理
- **摘要**: 被动声监测（PAM）研究产生数千个小时的音频，可用于监测特定的动物种群，进行广泛的生物多样性调查，检测诸如偷猎者之类的威胁等。用于物种识别的机器学习分类器越来越多地用于处理生物声学调查产生的大量音频，加快分析并增加了PAM作为管理工具的效用。在通常的实践中，将阈值应用于分类器输出评分，并将高于阈值的分数汇总为检测计数。阈值的选择会产生有偏见的发声计数，这些声音可能会受到数据集的子集的误报/负率的影响。在这项工作中，我们主张直接估计呼叫密度：包含目标发声的检测窗口的比例，无论分类器得分如何。我们的方法针对理想的生态估计器，并为确定由分布变化引起的核心问题提供了更严格的基础 - 当数据分配变化的定义特征以及设计策略以减轻它们时。我们提出了一个验证方案，用于估计数据体中的呼叫密度，并通过贝叶斯推理获得正面和负面类别的置信分数的概率分布。我们使用这些分布来预测站点级别的密度，这可能会发生分配变化。我们在对夏威夷鸟类的现实研究中测试了我们提出的方法，并提供了利用现有完全注释的数据集的模拟结果，这证明了对呼叫密度和分类器模型质量变化的稳健性。

## 其他论文

共有 25 篇其他论文

- [SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization](https://arxiv.org/abs/2401.17907)
  - **标题**: 子管：用于分割和视觉持续定位的海底管道检查数据集
  - **Filtered Reason**: none of cs.RO in whitelist
- [Exploiting Audio-Visual Features with Pretrained AV-HuBERT for Multi-Modal Dysarthric Speech Reconstruction](https://arxiv.org/abs/2401.17796)
  - **标题**: 使用预审预测的AV-HUBERT利用视听功能进行多模式违规语音重建
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Learning Which Side to Scan: Multi-View Informed Active Perception with Side Scan Sonar for Autonomous Underwater Vehicles](https://arxiv.org/abs/2402.01106)
  - **标题**: 学习要扫描的一面：多视图明智的主动感知和侧扫描声纳用于自动水下车辆
  - **Filtered Reason**: none of cs.RO in whitelist
- [Acoustic Local Positioning With Encoded Emission Beacons](https://arxiv.org/abs/2402.02384)
  - **标题**: 带有编码发射信标的声学局部定位
  - **Filtered Reason**: none of eess.SP,cs.AR,eess.AS,cs.SD in whitelist
- [ToMoBrush: Exploring Dental Health Sensing using a Sonic Toothbrush](https://arxiv.org/abs/2402.01933)
  - **标题**: Tomobrush：使用声音牙刷探索牙齿健康感
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Sensor Misalignment-tolerant AUV Navigation with Passive DoA and Doppler Measurements](https://arxiv.org/abs/2402.07218)
  - **标题**: 传感器耐耐受耐受的AUV导航，具有被动DOA和多普勒测量值
  - **Filtered Reason**: none of cs.RO in whitelist
- [Analytical model for the relation between signal bandwidth and spatial resolution in Steered-Response Power Phase Transform (SRP-PHAT) maps](https://arxiv.org/abs/2402.06586)
  - **标题**: 分析模型，用于转向响应功率相变（SRP-PHAT）图中信号带宽和空间分辨率之间的关系
  - **Filtered Reason**: none of eess.SP,eess.AS,cs.SD in whitelist
- [Exploiting spatial diversity for increasing the robustness of sound source localization systems against reverberation](https://arxiv.org/abs/2402.06411)
  - **标题**: 利用空间多样性，以提高声音源定位系统的鲁棒性，以防止混响
  - **Filtered Reason**: none of eess.SP,eess.AS,cs.SD in whitelist
- [Data-driven Joint Detection and Localization of Acoustic Reflectors](https://arxiv.org/abs/2402.06246)
  - **标题**: 数据驱动的联合检测和声学反射器的定位
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Evaluation of a Smart Mobile Robotic System for Industrial Plant Inspection and Supervision](https://arxiv.org/abs/2402.07691)
  - **标题**: 评估工业工厂检查和监督的智能移动机器人系统
  - **Filtered Reason**: none of cs.RO in whitelist
- [Safe Distributed Control of Multi-Robot Systems with Communication Delays](https://arxiv.org/abs/2402.09382)
  - **标题**: 与通信延迟的多机器人系统的安全分布式控制
  - **Filtered Reason**: none of cs.RO in whitelist
- [A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings](https://arxiv.org/abs/2402.09797)
  - **标题**: 多党剂相互作用的跨词鲁棒多通道VAD模型，该模型使用合成重新录制训练
  - **Filtered Reason**: none of eess.AS,cs.HC,cs.SD in whitelist
- [Guiding the underwater acoustic target recognition with interpretable contrastive learning](https://arxiv.org/abs/2402.12658)
  - **标题**: 通过可解释的对比度学习指导水下声学目标识别
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [EyeEcho: Continuous and Low-power Facial Expression Tracking on Glasses](https://arxiv.org/abs/2402.12388)
  - **标题**: Eyeecho：玻璃上的连续和低功率面部表情跟踪
  - **Filtered Reason**: none of cs.HC in whitelist
- [Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models](https://arxiv.org/abs/2402.12208)
  - **标题**: None
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Multimodal Emotion Recognition from Raw Audio with Sinc-convolution](https://arxiv.org/abs/2402.11954)
  - **标题**: sinc-convolution的原始音频识别的多模式情绪识别
  - **Filtered Reason**: none of eess.AS,cs.SD,cs.MM in whitelist
- [Unraveling Complex Data Diversity in Underwater Acoustic Target Recognition through Convolution-based Mixture of Experts](https://arxiv.org/abs/2402.11919)
  - **标题**: 通过基于卷积的专家的混合物来阐明水下声学目标识别中的复杂数据多样性
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for In-Air Acoustic Imaging](https://arxiv.org/abs/2402.13110)
  - **标题**: HIRIS：带有1024频道麦克风阵列的空气传播声纳传感器用于空气成像
  - **Filtered Reason**: none of eess.SP,eess.AS,cs.SD in whitelist
- [GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame](https://arxiv.org/abs/2402.14634)
  - **标题**: Gazetrak：在玻璃框架上探索基于声学的眼睛跟踪
  - **Filtered Reason**: none of cs.HC in whitelist
- [A circular microphone array with virtual microphones based on acoustics-informed neural networks](https://arxiv.org/abs/2402.15735)
  - **标题**: 基于声学信息的神经网络的圆形麦克风阵列，带有虚拟麦克风
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Batch Estimation of a Steady, Uniform, Flow-Field from Ground Velocity and Heading Measurements](https://arxiv.org/abs/2402.17078)
  - **标题**: 从地面速度和标题测量值的稳定，均匀，流场的批量估计
  - **Filtered Reason**: none of eess.SY,cs.RO in whitelist
- [Towards Environmental Preference Based Speech Enhancement For Individualised Multi-Modal Hearing Aids](https://arxiv.org/abs/2402.16757)
  - **标题**: 为个性化多模式助听器提供基于环境偏好的语音增强
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Underwater Acoustic Source Seeking Using Time-Difference-of-Arrival Measurements](https://arxiv.org/abs/2402.17405)
  - **标题**: 水下声源寻求使用时间差异测量的时间差异
  - **Filtered Reason**: none of cs.RO in whitelist
- [Acoustic tactile sensing for mobile robot wheels](https://arxiv.org/abs/2402.18682)
  - **标题**: 移动机器人车轮的声学感测
  - **Filtered Reason**: none of cs.RO in whitelist
- [ConvDTW-ACS: Audio Segmentation for Track Type Detection During Car Manufacturing](https://arxiv.org/abs/2402.18204)
  - **标题**: Convdtw-ACS：用于汽车制造过程中轨道类型检测的音频细分
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
