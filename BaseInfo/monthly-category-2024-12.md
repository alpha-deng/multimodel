# 2024-12 月度论文分类汇总

共有788篇相关领域论文, 另有62篇其他

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 59 篇论文

### Construction and optimization of health behavior prediction model for the elderly in smart elderly care 
[[arxiv](https://arxiv.org/abs/2412.02062)] [[cool](https://papers.cool/arxiv/2412.02062)] [[pdf](https://arxiv.org/pdf/2412.02062)]
> **Authors**: Qian Guo,Peiyuan Chen
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: 23 pages
- **标题**: 智能老年护理中老年人的健康行为预测模型的建设和优化
- **领域**: 人工智能,计算机与社会
- **摘要**: 随着全球衰老的加剧，老年人的健康管理已成为社会关注的重点。这项研究设计并实施了智能的老年护理服务模型，以解决数据多样性，健康状况复杂性，长期依赖性和数据丢失，行为突然变化以及对老年人健康行为的预测中的数据隐私。该模型通过多模式数据融合，数据丢失处理，非线性预测，紧急检测和隐私保护来实现老年人健康行为的准确预测和动态管理。在实验设计中，基于多源数据集和市场研究结果，该模型在健康行为预测，紧急检测和个性化服务方面表现出色。实验结果表明，该模型可以有效地提高健康行为预测的准确性和鲁棒性，并满足智能老年护理领域的实际应用需求。将来，随着更多数据的整合和技术的进一步优化，该模型将为智能老年护理服务提供更强大的技术支持。

### FastRM: An efficient and automatic explainability framework for multimodal generative models 
[[arxiv](https://arxiv.org/abs/2412.01487)] [[cool](https://papers.cool/arxiv/2412.01487)] [[pdf](https://arxiv.org/pdf/2412.01487)]
> **Authors**: Gabriela Ben-Melech Stan,Estelle Aflalo,Man Luo,Shachar Rosenman,Tiep Le,Sayak Paul,Shao-Yen Tseng,Vasudev Lal
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Fastrm：多模式模型的有效自动解释性框架
- **领域**: 人工智能
- **摘要**: 大型视觉语言模型（LVLM）表现出了与文本和视觉输入相对于文本和视觉输入的显着推理能力。但是，这些模型仍然容易产生错误信息。识别和缓解未接地的反应对于发展值得信赖的AI至关重要。传统的解释性方法，例如基于梯度的相关图，可深入了解模型的决策过程，但通常在计算上昂贵且不适合实时输出验证。在这项工作中，我们介绍了Fastrm，这是一种预测LVLM的可解释相关性图的有效方法。此外，Fastrm提供了模型置信度的定量和定性评估。实验结果表明，与传统相关性图的生成相比，FasTRM的计算时间减少了99.8％，记忆足迹降低了44.4％。 Fastrm允许可解释的AI更实用和可扩展，从而促进其在现实世界应用程序中的部署，并使用户能够更有效地评估模型输出的可靠性。

### LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations 
[[arxiv](https://arxiv.org/abs/2412.01441)] [[cool](https://papers.cool/arxiv/2412.01441)] [[pdf](https://arxiv.org/pdf/2412.01441)]
> **Authors**: Anian Ruoss,Fabio Pardo,Harris Chan,Bonnie Li,Volodymyr Mnih,Tim Genewein
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: LMACT：使用长多模式演示的中下文模仿学习的基准
- **领域**: 人工智能,机器学习
- **摘要**: 在本文中，我们为当今的Frontier Models在非常长篇文本的制度（最多一百万个令牌）中进行了压力测试的基准，并研究了这些模型是否可以在其上下文中从大量的专家演示中学习。 We evaluate the performance of Claude 3.5 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 2.0 Flash Experimental, GPT-4o, o1-mini, o1-preview, and o1 as policies across a battery of simple interactive decision-making tasks: playing tic-tac-toe, chess, and Atari, navigating grid worlds, solving crosswords, and controlling a simulated cheetah.我们研究了从无演示到512个完整剧集的上下文中越来越多的专家演示$ \ Unicode {x2013} $。在我们的任务中，模型很少设法完全达到专家的绩效，并且通常，展示更多的演示几乎没有影响。一些模型随着一些任务的更多演示而稳步改善。我们研究了编码观测值作为文本或图像的效果以及经过思考链的影响的影响。为了帮助量化其他方法和未来创新的影响，我们为统一评估中涵盖零，少数和许多拍摄制度的基准开源。

### Multimodal Medical Disease Classification with LLaMA II 
[[arxiv](https://arxiv.org/abs/2412.01306)] [[cool](https://papers.cool/arxiv/2412.01306)] [[pdf](https://arxiv.org/pdf/2412.01306)]
> **Authors**: Christian Gapp,Elias Tappeiner,Martin Welk,Rainer Schubert
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: 9 pages, 6 figures, conference: AIRoV -- The First Austrian Symposium on AI, Robotics, and Vision 25.-27.3.2024, Innsbruck
- **标题**: 与美洲驼II的多模式医学疾病分类
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 医疗患者数据始终是多模式的。在这种情况下，图像，文本，年龄，性别，组织病理学数据只是不同方式的示例。加工和整合这些多模式数据与基于深度学习的方法是最大的兴趣，因为它具有较大的医疗程序（例如诊断和患者治疗计划）的巨大潜力。在这项工作中，我们重新训练了基于多模式变压器的疾病分类模型。为此，我们使用来自OpenI的文本图像对数据集，该数据集由与临床报告相关的2D胸部X射线组成。我们的重点是融合从医疗数据集中提取的文本和视觉信息的融合方法。测试了具有Llama II骨干模型的不同建筑结构。从更深层次的体系结构中，最佳模型达到97.10％的最佳模型，最佳模型的最佳模型比晚期融合（最佳模型：最佳模型：96.67％的平均AUC），可以创造出更好的结果。两者的表现都超过了在同一多模式数据集上测试的以前的分类模型。新引入的多模式体系结构可以不用努力应用于其他多模式数据集，并且可以轻松地适应进一步的研究，尤其是但不限于医疗AI领域。

### BIGCity: A Universal Spatiotemporal Model for Unified Trajectory and Traffic State Data Analysis 
[[arxiv](https://arxiv.org/abs/2412.00953)] [[cool](https://papers.cool/arxiv/2412.00953)] [[pdf](https://arxiv.org/pdf/2412.00953)]
> **Authors**: Xie Yu,Jingyuan Wang,Yifan Yang,Qian Huang,Ke Qu
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: BigCity：用于统一轨迹和交通状态数据分析的通用时空模型
- **领域**: 人工智能
- **摘要**: 典型的动态ST数据包括轨迹数据（代表个人级别的移动性）和交通状态数据（代表人口级的移动性）。传统研究通常将轨迹和交通状态数据视为独立的独立方式，每个模式都针对单个模式中的特定任务量身定制。但是，实际应用程序（例如导航应用程序）需要对轨迹和交通状态数据进行联合分析。将这些数据类型视为两个单独的域可能会导致次优模型性能。尽管ST数据预训练和ST基础模型的最新进展旨在开发用于ST数据分析的通用模型，但大多数现有模型是“多任务，独奏模式”（MTSM），这意味着它们可以在轨迹数据或交通状态数据中处理多个任务，但同时却不同时处理。为了解决这一差距，本文介绍了BigCity，这是第一个多任务，多数据模式（MTMD）模型，用于ST数据分析。该模型针对设计MTMD ST模型的两个关键挑战：（1）统一不同ST数据模式的表示形式，以及（2）统一异构ST分析任务。为了克服第一个挑战，BigCity引入了一种新颖的ST-UNIT，以统一的格式代表轨迹和交通状态。此外，对于第二个挑战，BigCity采用了具有ST任务的提示的可调大型模型，使其能够执行一系列异质任务，而无需进行微调。对现实世界数据集的广泛实验表明，BigCity在8个任务中实现最先进的性能，表现优于18个基线。据我们所知，BigCity是第一个能够处理轨迹和交通状态的模型，以完成各种异质任务。我们的代码可从https://github.com/bigscity/bigcity获得

### Playable Game Generation 
[[arxiv](https://arxiv.org/abs/2412.00887)] [[cool](https://papers.cool/arxiv/2412.00887)] [[pdf](https://arxiv.org/pdf/2412.00887)]
> **Authors**: Mingyu Yang,Junyou Li,Zhongbin Fang,Sheng Chen,Yangbin Yu,Qiang Fu,Wei Yang,Deheng Ye
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 可玩的游戏生成
- **领域**: 人工智能
- **摘要**: 近年来，人工智能生成的内容（AIGC）已从文本到图像生成到文本到视频和多模式视频综合。但是，由于对实时互动，高视觉质量以及对游戏力学的准确模拟的严格要求，生成可玩游戏带来了重大挑战。现有方法通常缺乏，要么缺乏实时功能，要么无法准确模拟交互式力学。为了解决可玩性问题，我们提出了一种名为\ emph {playgen}的新方法，该方法包括游戏数据生成，基于自动回归DIT的扩散模型以及一个基于可玩性的评估框架。 Playgen在著名的2D和3D游戏中得到验证，可实现实时互动，确保足够的视觉质量，并提供准确的交互式力学模拟。值得注意的是，即使在NVIDIA RTX 2060 GPU上进行了1000多个游戏玩法之后，这些结果也得到了维持。我们的代码公开可用：https：//github.com/greatx3/playable-game-generation。我们AI生成的可播放演示是：http：//124.156.151.207。

### Improving Multimodal LLMs Ability In Geometry Problem Solving, Reasoning, And Multistep Scoring 
[[arxiv](https://arxiv.org/abs/2412.00846)] [[cool](https://papers.cool/arxiv/2412.00846)] [[pdf](https://arxiv.org/pdf/2412.00846)]
> **Authors**: Avinash Anand,Raj Jaiswal,Abhishek Dharmadhikari,Atharva Marathe,Harsh Parimal Popat,Harshil Mital,Kritarth Prasad,Rajiv Ratn Shah,Roger Zimmermann
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: 15 pages
- **标题**: 提高几何问题解决，推理和多步得分的多模式LLMS能力
- **领域**: 人工智能
- **摘要**: 本文介绍了GPSM4K，这是一个综合几何多模式数据集，该数据集量身定制，旨在增强大型视觉语言模型（LVLMS）的问题解决功能。 GPSM4K包括2157多模式问答对，从数学教科书中手动提取，跨越了7  -  12年级，并进一步增加了5340个问题，包括数值和定理问题。与PGPS9K，几何3K和GEO170K相比，GPSM4K以一致的格式提供了详细的逐步解决方案，以促进解决问题方法解决方法的全面评估。该数据集是评估LVLM的几何推理能力的绝佳基准。对我们的测试集的评估表明，在几何问题解决问题中，开源语言模型中需要改进的范围。我们的训练集中的填充增加了模型的几何解决能力。此外，我们还评估了诸如图像字幕和检索增强生成（RAG）等技术的有效性。我们利用LLM通过提供基础真理和预测解决方案来自动化最终答案评估的任务。这项研究将有助于评估和提高LVLM的几何推理能力。

### Fairness at Every Intersection: Uncovering and Mitigating Intersectional Biases in Multimodal Clinical Predictions 
[[arxiv](https://arxiv.org/abs/2412.00606)] [[cool](https://papers.cool/arxiv/2412.00606)] [[pdf](https://arxiv.org/pdf/2412.00606)]
> **Authors**: Resmi Ramachandranpillai,Kishore Sampath,Ayaazuddin Mohammad,Malihe Alikhani
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 每个交叉路口公平：在多模式临床预测中发现和缓解交叉偏见
- **领域**: 人工智能
- **摘要**: 使用电子医疗保健记录（EHR）自动临床决策的偏见在患者护理和治疗结果上造成了显着差异。常规方法主要集中在源自单个属性的偏置缓解策略上，俯瞰着截面亚组 - 跨各种人口相交（例如种族，性别，种族等）形成的群体。由于这些亚组的分布和偏差模式的变化，将单属性缓解策略渲染到交叉亚组的策略在统计学上无关紧要。 EHR的多模式性质 - 来自各种来源的数据，例如文本，时间序列，表格，事件和图像的组合 - 增加了另一层复杂性，因为对少数群体的影响可能会跨模态波动。在本文中，我们采取了初步步骤来发现预测中潜在的相交偏差，通过采购广泛的多模式数据集，Mimic-eye1和Mimic-IV ed，并在交叉子组水平上提出缓解。我们通过从多模式来源学习统一的文本表示，在数据集中执行和基准评估，从而利用了预训练的临床语言模型（LM），梅德伯特，临床BERT和临床生物Biobert的巨大功能。我们的发现表明，在不同的数据集，亚组和嵌入中，提出的亚组特异性偏置缓解措施是可靠的，这表明在解决多模式设置中的截面偏见方面有效性。

### LAMBDA: Covering the Multimodal Critical Scenarios for Automated Driving Systems by Search Space Quantization 
[[arxiv](https://arxiv.org/abs/2412.00517)] [[cool](https://papers.cool/arxiv/2412.00517)] [[pdf](https://arxiv.org/pdf/2412.00517)]
> **Authors**: Xinzheng Wu,Junyi Chen,Xingyu Xing,Jian Sun,Ye Tian,Lihao Liu,Yong Shen
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: 17pages, 21figures
- **标题**: lambda：通过搜索空间量化涵盖自动驾驶系统的多模式关键方案
- **领域**: 人工智能,新兴技术,机器人技术
- **摘要**: 基于方案的虚拟测试是测试和评估自动驾驶系统（ADSS）安全性的最重要方法之一。但是，在逻辑场景空间中枚举所有具体场景并进行详尽的测试是不切实际的。最近，引入了黑盒优化（BBO），以通过利用历史测试信息生成新的测试用例来加速基于方案的ADSS测试。但是，在逻辑情况下，BBO算法发现的单个最佳算法不足以对ADSS进行全面的安全评估。实际上，在逻辑场景空间中代表危险的所有子空间，而不仅仅是最关键的具体场景，在安全评估中起着更重要的作用。通过有限数量的测试涵盖在逻辑场景空间中涵盖许多关键具体方案的定义是本文中的黑盒覆盖范围（BBC）问题。我们在基于样本的搜索范式中正式化了这个问题，并通过混乱矩阵分析构建了覆盖标准。此外，我们提出了Lambda（具有密度适应的潜在蒙特卡洛束搜索）来解决BBC问题。 Lambda可以通过将逻辑场景空间递归将逻辑场景分配为被公认和拒绝的部分来快速关注关键子空间。与其前身的lamcts相比，Lambda引入了采样密度，以克服优化和梁搜索的采样偏置以获得更加并行的性能。实验结果表明，Lambda在所有基线中都能达到最先进的性能，并且比随机搜索最多可以达到33倍和6000倍，以分别获得2和5维合成功能的关键区域的95％覆盖率。实验还表明，Lambda在虚拟测试中对ADS的安全评估中具有有希望的未来。

### Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions 
[[arxiv](https://arxiv.org/abs/2412.02621)] [[cool](https://papers.cool/arxiv/2412.02621)] [[pdf](https://arxiv.org/pdf/2412.02621)]
> **Authors**: Kai Sun,Siyan Xue,Fuchun Sun,Haoran Sun,Yu Luo,Ling Wang,Siyuan Wang,Na Guo,Lei Liu,Tian Zhao,Xinzhou Wang,Lei Yang,Shuo Jin,Jun Yan,Jiahong Dong
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: 临床诊断和治疗中的医学多模式基础模型：应用，挑战和未来方向
- **领域**: 人工智能,机器学习
- **摘要**: 深度学习的最新进展极大地彻底改变了临床诊断和治疗领域，提供了新的方法来提高各种临床领域的诊断精度和治疗功效，从而推动了对精度医学的追求。多轨和多模式数据集的可用性日益增长，加速了大规模医学多模式模型（MMFMS）的开发。这些模型以其强大的概括能力和丰富的代表力而闻名，越来越适应从早期诊断到个性化治疗策略的广泛临床任务。这篇评论对MMFM的最新发展进行了全面分析，重点介绍了三个关键方面：数据集，模型架构和临床应用。我们还探讨了优化多模式表示形式的挑战和机会，并讨论了这些进步如何通过实现改善患者结果和更有效的临床工作流程来塑造医疗保健的未来。

### ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation? 
[[arxiv](https://arxiv.org/abs/2412.02368)] [[cool](https://papers.cool/arxiv/2412.02368)] [[pdf](https://arxiv.org/pdf/2412.02368)]
> **Authors**: Leixin Zhang,Steffen Eger,Yinjie Cheng,Weihe Zhai,Jonas Belouadi,Christoph Leiter,Simone Paolo Ponzetto,Fahimeh Moafian,Zhixue Zhao
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: Scimage：科学文本到图像生成的多模式大型语言模型有多好？
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（LLMS）在从文本说明中生成高质量图像方面表现出了令人印象深刻的功能。但是，它们在生成科学图像中的表现（这是加速科学进步的关键应用），这些应用程序不受欢迎。在这项工作中，我们通过引入Scimage来解决这一差距，Scimage是一种基准测试，旨在评估LLMS在文本描述中生成科学图像时的多模式能力。 Scimage评估了理解的三个关键维度：空间，数字和属性理解及其组合，重点关注科学对象之间的关系（例如，正方形，圆圈）。我们使用两种输出生成模式：基于代码的输出（Python，tikz）和直接的光栅图像生成，我们评估了五个模型，GPT-4O，Llama，Llama，Automatikz，dall-E和Stablediffusion。此外，我们研究了四种不同的输入语言：英语，德语，波尔西和中文。我们的评估是在三个标准（正确性，相关性和科学准确性）的11位科学家进行的，虽然GPT-4O产生了不错的质量输出，以提供更简单的提示，涉及单个维度，例如空间，数字或属性，例如孤立的理解，但所有模型都在此任务中面临挑战，尤其是更复杂的提示。

### Large Multimodal Agents for Accurate Phishing Detection with Enhanced Token Optimization and Cost Reduction 
[[arxiv](https://arxiv.org/abs/2412.02301)] [[cool](https://papers.cool/arxiv/2412.02301)] [[pdf](https://arxiv.org/pdf/2412.02301)]
> **Authors**: Fouad Trad,Ali Chehab
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: Accepted in the 2nd International Conference on Foundation and Large Language Models (FLLM2024)
- **标题**: 通过增强的令牌优化和降低成本的大型多模式剂，用于准确的网络钓鱼检测
- **领域**: 人工智能,计算语言学,密码学和安全
- **摘要**: 随着复杂的网络钓鱼攻击的兴起，对有效和经济的检测解决方案的需求日益增长。本文探讨了大型多模式代理，特别是Gemini 1.5 Flash和GPT-4O Mini，通过API分析URL和网页屏幕截图，从而避免了训练和维护AI系统的复杂性。我们的发现表明，整合这两种数据类型可大大提高检测性能，而不是仅使用任何一种类型。但是，API使用会导致每个查询的成本取决于输入和输出令牌的数量。为了解决这个问题，我们提出了一种两层的代理方法：最初，一个代理评估URL，如果不确定，第二代理会评估URL和屏幕截图。该方法不仅可以保持强大的检测性能，而且通过最大程度地减少不必要的多输入查询来大大降低API成本。成本分析表明，使用代理方法，与多模式方法相比，GPT-4O Mini可以处理每100美元的网站的4.2倍（107,440 vs.25,626），而Gemini 1.5 Flash可以处理约2.6倍的网站（2,232,142 vs. 862,068）。这些发现强调了代理方法对多模式方法的重大经济利益，这为旨在利用先进的AI在控制费用的同时利用高级AI进行网络钓鱼的组织提供了可行的解决方案。

### Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries 
[[arxiv](https://arxiv.org/abs/2412.03837)] [[cool](https://papers.cool/arxiv/2412.03837)] [[pdf](https://arxiv.org/pdf/2412.03837)]
> **Authors**: Abul Ehtesham,Saket Kumar,Aditi Singh,Tala Talaei Khoei
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 电影gen：META生成AI基础模型的SWOT分析，用于改变媒体，广告和娱乐行业
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 生成的AI正在重塑媒体景观，从而在视频创建，个性化和可扩展性中实现了前所未有的功能。本文介绍了Metas电影Gen的全面分析，Metas电影Gen是一种尖端的生成AI粉底模型，旨在生产带有简单文本提示的同步音频的1080p HD视频。我们探讨了它的优势，包括高分辨率的视频生成，精确的编辑和无缝的音频集成，这使其成为整个电影制作，广告和教育等行业的变革性工具。但是，该分析还解决了局限性，例如对视频长度的限制和生成内容的潜在偏见，这对更广泛的采用构成了挑战。此外，我们研究了围绕生成AI的不断发展的监管和道德考虑因素，重点是内容真实性，文化代表和负责任的问题。通过与DALL-E和Google Imagen（例如Dall-E和Google Imagen）等领先模型的比较见解，本文突出了电影Gens独特的功能，例如视频个性化和多模式合成，同时确定了创新的机会和需要进一步研究的领域。我们的发现为利益相关者提供了可行的见解，强调了在媒体生产中部署生成AI的机遇和挑战。这项工作旨在指导生成AI的未来进步，确保在这个快速发展的领域中的可伸缩性，质量和道德完整性。

### Enhancing CLIP Conceptual Embedding through Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2412.03513)] [[cool](https://papers.cool/arxiv/2412.03513)] [[pdf](https://arxiv.org/pdf/2412.03513)]
> **Authors**: Kuei-Chun Kao
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 通过知识蒸馏增强剪辑概念嵌入
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 最近，剪辑已成为在多模式上下文中对齐图像和文本的重要模型。但是，研究人员已经确定了剪辑的文本和图像编码者从字幕和图像对提取详细知识的能力的局限性。作为回应，本文提出了一种创新的方法，旨在通过整合基于Llama 2的新知识蒸馏（KD）方法来提高剪辑的性能。我们的方法侧重于三个关键目标：文本嵌入蒸馏，概念学习和对比度学习。首先，插入蒸馏的文本涉及训练知识卷 - 卷文本编码器以镜像教师模型，Llama 2。接下来，概念学习通过使用离线k-Means聚类从Llama 2中的文本数据进行了分类，从而为每个字幕图像对分配了一个软概念标签，从而使知识clip从这些软概念标签中学习。最后，对比学习使文本和图像嵌入对齐。我们的实验发现表明，所提出的模型可以改善文本和图像编码器的性能。

### ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning 
[[arxiv](https://arxiv.org/abs/2412.03104)] [[cool](https://papers.cool/arxiv/2412.03104)] [[pdf](https://arxiv.org/pdf/2412.03104)]
> **Authors**: Zhe Xie,Zeyan Li,Xiao He,Longlong Xu,Xidao Wen,Tieying Zhang,Jianjun Chen,Rui Shi,Dan Pei
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: CHATTS：通过合成数据与LLMS对齐时间序列，以增强理解和推理
- **领域**: 人工智能
- **摘要**: 了解时间序列对于在现实世界中的应用至关重要。最近，大型语言模型（LLM）越来越多地应用于时间序列任务，利用其强大的语言功能来增强各种应用程序。但是，对时间序列理解和推理的多模式LLM（MLLM）的研究仍然有限，这主要是由于高质量数据集与文本信息相结合的高质量数据集的稀缺性。本文介绍了Chatts，这是一种用于时间序列分析的新型MLLM。 Chatts将时间序列视为一种方式，类似于Vision Mllms处理图像的方式，使其能够通过时间序列进行理解和推理。为了解决培训数据的稀缺性，我们提出了一种基于属性的方法，用于生成具有详细属性描述的合成时间序列。我们进一步介绍了时间序列Evol-Instruct，这是一种新型的方法，可以生成各种时间序列的问答，从而增强了模型的推理能力。据我们所知，Chatts是将多元时间序列作为理解和推理的输入的第一个TS-MLLM，仅在合成数据集上进行了微调。我们使用带有实际数据的基准数据集评估其性能，包括六个对齐任务和四个推理任务。我们的结果表明，CHATTS明显优于现有的基于视觉的MLLM（例如GPT-4O）和基于文本/代理的LLMS，在一致性任务方面提高了46.0％，推理任务提高了25.8％。

### Question Answering for Decisionmaking in Green Building Design: A Multimodal Data Reasoning Method Driven by Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.04741)] [[cool](https://papers.cool/arxiv/2412.04741)] [[pdf](https://arxiv.org/pdf/2412.04741)]
> **Authors**: Yihui Li,Xiaoyue Yan,Hao Zhou,Borong Lin
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: Published at Association for Computer Aided Design in Architecture (ACADIA) 2024
- **标题**: 绿色建筑设计中决策的问题回答：一种由大语言模型驱动的多模式数据推理方法
- **领域**: 人工智能,计算语言学,人机交互
- **摘要**: 近年来，绿色建筑在解决能源消耗和环境问题方面的关键作用已被广泛认可。研究表明，在早期设计阶段可以实现超过40％的势能节省。因此，基于建模和性能模拟的绿色建筑设计（DGBD）的决策对于降低建筑能源成本至关重要。但是，绿色建筑领域涵盖了广泛的专业知识，这涉及大量的学习成本，并导致决策效率低下。许多研究已经将人工智能（AI）方法应用于该领域。基于先前的研究，本研究将大型语言模型与DGBD整合在一起，创建了GreenQA，这是一个问题回答多模式数据推理的问题。 GreenQA利用检索增强生成，思想链和功能呼叫方法，可实现多模式问答，包括天气数据分析和可视化，检索绿色建筑案例和知识查询。此外，这项研究还使用GreenQA Web平台进行了用户调查。结果表明，有96％的用户认为该平台有助于提高设计效率。这项研究不仅有效地支持DGBD，而且为AI辅助设计提供了灵感。

### Parametric-ControlNet: Multimodal Control in Foundation Models for Precise Engineering Design Synthesis 
[[arxiv](https://arxiv.org/abs/2412.04707)] [[cool](https://papers.cool/arxiv/2412.04707)] [[pdf](https://arxiv.org/pdf/2412.04707)]
> **Authors**: Rui Zhou,Yanxia Zhang,Chenyang Yuan,Frank Permenter,Nikos Arechiga,Matt Klenk,Faez Ahmed
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 参数控制：精确工程设计综合基础模型中的多模式控制
- **领域**: 人工智能,计算工程、金融和科学,计算机视觉和模式识别,人机交互
- **摘要**: 本文介绍了一种生成模型，旨在对文本形象至图像基础生成的AI模型（例如稳定扩散）进行多模式控制，该模型是专门针对工程设计综合量身定制的。我们的模型提出了参数，图像和文本控制方式，以增强设计精度和多样性。首先，它使用扩散模型处理部分和完整的参数输入，该模型充当设计自动完成的副驾驶，并与参数编码器结合以处理信息。其次，该模型利用汇编图来系统地组装输入组件图像，然后通过组件编码器对其进行处理以捕获基本的视觉数据。第三，文本描述是通过剪辑编码集成的，以确保对设计意图的全面解释。这些不同的输入是通过多模式融合技术合成的，创建了一个关节嵌入，该嵌入充当了受控制网络启发的模块的输入。这种集成使该模型可以将强大的多模式控制应用于基础模型，从而促进复杂而精确的工程设计的产生。这种方法扩大了AI驱动的设计工具的功能，并基于各种数据模式来证明精确控制的重大进步，以增强设计生成。

### EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios 
[[arxiv](https://arxiv.org/abs/2412.04447)] [[cool](https://papers.cool/arxiv/2412.04447)] [[pdf](https://arxiv.org/pdf/2412.04447)]
> **Authors**: Lu Qiu,Yuying Ge,Yi Chen,Yixiao Ge,Ying Shan,Xihui Liu
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: Code & data are available at: https://qiulu66.github.io/egoplanbench2/
- **标题**: EGOPLAN-BENCH2：在现实世界中的多模式大语言模型计划的基准
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 多模式的大型语言模型的出现，利用大型语言模型的力量，最近表现出了卓越的多模式理解和推理能力，预示着人工通用智能的新时代。但是，实现AGI不仅需要理解和推理。所需的关键能力是在各种情况下有效计划，这涉及基于复杂环境来解决现实世界问题的合理决策。尽管它很重要，但在各种情况下，当前MLLM的计划能力仍未得到充实。在本文中，我们介绍了Egoplan-Bench2，这是一种严格而全面的基准测试，旨在评估在各种现实世界中的MLLM的计划能力。 Egoplan-Bench2涵盖了涵盖4个主要领域和24个详细场景的日常任务，与人类日常生活紧密相符。 Egoplan-Bench2是通过使用自动验证的以自动为中心视频的半自动过程来构建的。它以第一人称的视角为基础，反映了人类在日常生活中解决问题的方式。我们评估了21种竞争性MLLM，并对其局限性进行了深入的分析，表明他们在现实世界中面临重大挑战。为了进一步提高当前MLLM的计划水平，我们提出了一种使用多模式链（COT）的无培训方法，通过研究各种多模式提示在复杂计划中的有效性来提示。我们的方法在没有其他培训的情况下，在Egoplan-Bench2上，在Egoplan-Bench2上，GPT-4V的性能提高了10.24。我们的工作不仅阐明了MLLM在计划中的当前局限性，而且还为这一关键领域的未来增强提供了见解。我们已经在https://qiulu66.github.io/egoplanbench2/上提供了数据和代码。

### Bench-CoE: a Framework for Collaboration of Experts from Benchmark 
[[arxiv](https://arxiv.org/abs/2412.04167)] [[cool](https://papers.cool/arxiv/2412.04167)] [[pdf](https://arxiv.org/pdf/2412.04167)]
> **Authors**: Yuanshuai Wang,Xingjian Zhang,Jinkun Zhao,Siwei Wen,Peilin Feng,Shuhao Liao,Lei Huang,Wenjun Wu
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: The code is available at \url{https://github.com/ZhangXJ199/Bench-CoE}
- **标题**: 台式：基准专家合作的框架
- **领域**: 人工智能
- **摘要**: 大型语言模型（LLMS）是驱动智能系统来处理多个任务的关键技术。为了满足各种任务的需求，已经开发了越来越多的LLMS驱动的专家，并伴随着相应的基准来评估其性能。本文提出了台式框架框架，该框架可以通过有效利用基准评估来实现各种任务的最佳性能来实现专家（COE）的协作。台式COE包括一组专家模型，一个用于将任务分配给相应专家的路由器以及用于训练路由器的基准数据集。此外，我们根据我们的框架制定查询级别和主题级别的方法，并分析这两种方法的优点和缺点。最后，我们对语言和多模式任务进行了不同的数据分布进行了一系列实验，以验证我们所提出的台式台式在整体绩效方面都优于任何单个模型。我们希望这种方法是该领域进一步研究的基准。该代码可在\ url {https://github.com/zhangxj199/bench-coe}中获得。

### ProtDAT: A Unified Framework for Protein Sequence Design from Any Protein Text Description 
[[arxiv](https://arxiv.org/abs/2412.04069)] [[cool](https://papers.cool/arxiv/2412.04069)] [[pdf](https://arxiv.org/pdf/2412.04069)]
> **Authors**: Xiao-Yu Guo,Yi-Fan Li,Yuan Liu,Xiaoyong Pan,Hong-Bin Shen
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: Protdat：来自任何蛋白质文本描述的统一蛋白质序列设计框架
- **领域**: 人工智能
- **摘要**: 蛋白质设计已成为促进各种应用（例如药物开发和酶工程）的重要潜力的关键方法。然而，利用大型语言模型的蛋白质设计方法仅通过预处理和微调努力来捕获多模式蛋白质数据中的关系。为了解决这个问题，我们提出了Protdat，这是一个从头细粒度的框架，能够从任何描述性蛋白质文本输入中设计蛋白质。 Protdat建立在蛋白质数据的固有特征上，以将序列和文本统一为具有内聚的整体而不是单独的实体。它利用创新的多模式跨注意事项，将蛋白质序列和文本信息整合到基础水平和无缝集成中。实验结果表明，Protdat在蛋白质序列的产生中达到了最新性能，在理性，功能，结构相似性和有效性方面表现出色。在瑞士 - 普罗特（Swiss-Prot）的20,000个文本序列对中，它将PLDDT提高了6％，TM得分提高了0.26，并将RMSD降低了1.2Å，突出了其推进蛋白质设计的潜力。

### SocialMind: LLM-based Proactive AR Social Assistive System with Human-like Perception for In-situ Live Interactions 
[[arxiv](https://arxiv.org/abs/2412.04036)] [[cool](https://papers.cool/arxiv/2412.04036)] [[pdf](https://arxiv.org/pdf/2412.04036)]
> **Authors**: Bufang Yang,Yunqi Guo,Lilin Xu,Zhenyu Yan,Hongkai Chen,Guoliang Xing,Xiaofan Jiang
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **摘要**: 社会互动是人类生活的基础。大型语言模型（LLMS）的最新出现的虚拟助手表明了它们有可能彻底改变人类互动和生活方式的潜力。但是，现有的辅助系统主要为单个用户提供反应性服务，而不是在与会话伙伴的实时社交互动中提供原位援助。在这项研究中，我们介绍了社交媒介，这是第一个基于LLM的积极AR社会辅助系统，为用户提供现场社会援助。 SocialMind采用类似人类的感知来利用多模式传感器来提取口头和非语言提示，社会因素和隐性角色，将这些社会线索纳入LLM推理，以创造社会建议的产生。此外，SocialMind采用多层协作生成策略和主动更新机制来展示有关增强现实（AR）眼镜的社交建议，以确保及时向用户提供建议，而不会破坏自然的对话流动。对三个公共数据集的评估和20名参与者的用户研究表明，与基准相比，社交界的参与度增加了38.3％，而95％的参与者愿意在实时社交互动中使用社交媒介。

### TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft 
[[arxiv](https://arxiv.org/abs/2412.05255)] [[cool](https://papers.cool/arxiv/2412.05255)] [[pdf](https://arxiv.org/pdf/2412.05255)]
> **Authors**: Qian Long,Zhi Li,Ran Gong,Ying Nian Wu,Demetri Terzopoulos,Xiaofeng Gao
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: Teamcraft：Minecraft多模式多代理系统的基准
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,多代理系统
- **摘要**: 合作是社会的基石。在现实世界中，人类队友利用多感官数据来解决不断变化的环境中具有挑战性的任务。对于在视觉上富裕环境中合作的体现代理至关重要，充满动态交互，以了解多模式观察和任务规格。为了评估可概括的多模式协作代理商的性能，我们提出了Teamcraft，这是一种在开放世界视频游戏Minecraft之上构建的多模式多代理基准。该基准具有55,000个由多模式提示指定的任务变体，用于模仿学习的程序生成的专家演示以及精心设计的协议，以评估模型概括功能。我们还进行了广泛的分析，以更好地了解现有方法的局限性和优势。我们的结果表明，现有模型在推广新的目标，场景和看不见的代理数量时继续面临重大挑战。这些发现强调了在这一领域进行进一步研究的必要性。 Teamcraft平台和数据集可在https://github.com/teamcraft-bench/teamcraft上公开获得。

### Agents for self-driving laboratories applied to quantum computing 
[[arxiv](https://arxiv.org/abs/2412.07978)] [[cool](https://papers.cool/arxiv/2412.07978)] [[pdf](https://arxiv.org/pdf/2412.07978)]
> **Authors**: Shuxiang Cao,Zijian Zhang,Mohammed Alghadeer,Simone D Fasciati,Michele Piscitelli,Mustafa Bakr,Peter Leek,Alán Aspuru-Guzik
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 用于量子计算的自动驾驶实验室的代理
- **领域**: 人工智能,量子物理学
- **摘要**: 完全自动化的自动驾驶实验室有望通过减少重复性劳动来实现高通量和大规模的科学发现。但是，有效的自动化需要深入整合实验室知识，这通常是非结构化的，多模式的，并且难以融入当前的AI系统。本文介绍了K-Agent框架，该框架旨在支持实验者在组织实验室知识和与代理的实验中自动化实验。我们的框架采用大型基于语言模型的代理来封装实验室知识，包括可用的实验室操作和用于分析实验结果的方法。为了使实验自动化，我们将执行代理引入了将多步实验程序打破状态机器的执行代理，与其他代理进行交互以执行每个步骤并分析实验结果。然后将分析结果用于驱动状态过渡，从而实现闭环反馈控制。为了证明其功能，我们应用了代理来校准和操作超导量子处理器，在那里他们自主计划和执行的实验数小时，成功地在人类科学家达到的水平下成功地生产和表征了纠缠的量子状态。我们基于知识的代理系统为管理实验室知识和加速科学发现开辟了新的可能性。

### SmartAgent: Chain-of-User-Thought for Embodied Personalized Agent in Cyber World 
[[arxiv](https://arxiv.org/abs/2412.07472)] [[cool](https://papers.cool/arxiv/2412.07472)] [[pdf](https://arxiv.org/pdf/2412.07472)]
> **Authors**: Jiaqi Zhang,Chen Gao,Liyuan Zhang,Yong Li,Hongzhi Yin
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: SmartAgent：网络世界中体现个性化代理的用户思考
- **领域**: 人工智能
- **摘要**: 具有多模式感知和基于大型视觉模型（LVLM）的多模式感知和推理能力的体现代理的最新进展，在自主互动的真实或网络世界中表现出色，帮助人们在复杂的环境中做出明智的决策。但是，当前的作品通常通过黄金行动轨迹或针对确定目标的理想的以任务为导向的解决方案来优化。该范式认为有限的面向用户的因素，这可能是他们在广泛的个人助理应用程序中绩效降低的原因。为了解决这个问题，我们提出了一种新颖的推理范式（COUT），这是一种新颖的推理范式，从基本行动思维中汲取了一系列思想，以明确和隐性的个性化偏好思考，将个性化因素纳入自主代理学习中。为了定位COUT，我们介绍了SmartAgent，一个代理框架感知网络环境，并将推理的个性化要求作为1）与GUI互动以访问项目池，2）生成用户在以前的操作中暗示的明确要求，以及3）推荐项目以满足用户的隐式要求。为了展示SmartAgent的功能，我们还创建了一个全新的数据集SmartSpot，可提供全阶段的个性化动作涉及的环境。据我们所知，我们的工作是第一个制定cout过程的工作，作为体现个性化代理学习的初步尝试。我们对SmartSpot的广泛实验阐明了SmartAgent在一系列具体和个性化的子任务中的功能。我们将在https://github.com/tsinghua-fib-lab/smartagent上发布纸质通知的代码和数据。

### LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation 
[[arxiv](https://arxiv.org/abs/2412.09237)] [[cool](https://papers.cool/arxiv/2412.09237)] [[pdf](https://arxiv.org/pdf/2412.09237)]
> **Authors**: Yijun Liu,Wu Liu,Xiaoyan Gu,Yong Rui,Xiaodong He,Yongdong Zhang
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: Lmagent：多用户模拟的大规模多模式代理协会
- **领域**: 人工智能
- **摘要**: 多用户行为的可信模拟对于理解复杂的社会系统至关重要。最近，大型语言模型（LLMS）的AI代理已经取得了重大进展，从而使他们能够在各种任务中实现类似人类的智能。但是，真实的人类社会通常是动态和复杂的，涉及许多从事多模式相互作用的人。在本文中，以电子商务情景为例，我们提出了基于多模式LLM的非常大规模和多模式的代理社会。在Lmagent，除了与朋友自由聊天外，代理商还可以自主浏览，购买和审查产品，甚至可以执行直播电子商务。为了模拟这一复杂的系统，我们引入了一种自通促进机制，以增强代理的多模式功能，从而在现有的多代理系统上显着改善了决策性能。此外，我们提出了一种快速的存储机制与小世界模型相结合，以提高系统效率，该效率支持社会中10,000多个代理模拟。对代理行为的实验表明，这些代理在行为指标中实现了与人类的可比性能。此外，与现有的基于LLMS的多代理系统相比，展示了更多不同和有价值的现象，例如牛群行为，这证明了Lmagent在可靠的大规模社交行为模拟中的潜力。

### Seeing the Forest and the Trees: Solving Visual Graph and Tree Based Data Structure Problems using Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2412.11088)] [[cool](https://papers.cool/arxiv/2412.11088)] [[pdf](https://arxiv.org/pdf/2412.11088)]
> **Authors**: Sebastian Gutierrez,Irene Hou,Jihye Lee,Kenneth Angelikas,Owen Man,Sophia Mettille,James Prather,Paul Denny,Stephen MacNeil
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: 14 pages, 4 figures, to be published in ACE 2025
- **标题**: 看到森林和树木：使用大型多模型的求解视觉图和基于树的数据结构问题
- **领域**: 人工智能,计算机视觉和模式识别,计算机与社会
- **摘要**: 生成AI系统的最新进展引起了人们对教育者学术完整性的担忧。除了在解决编程问题和基于文本的多项选择问题方面表现出色外，最近的研究还发现，大型多模式模型（LMM）只能基于图像解决帕森斯问题。但是，此类问题仍然固有地基于文本，并且依赖于模型的功能将代码块的图像转换为其相应的文本。在本文中，我们进一步研究了仅基于图像的LMMS解决图形和树数据结构问题的功能。为了实现这一目标，我们在计算上构建和评估了一个新型的基准数据集，其中包括9,072个不同的图形和树数据结构任务样本，以评估GPT-4O，GPT-4V，Gemini 1.5 Pro，Gemini 1.5 Pro，Gemini 1.5 Flash，Gemini 1.0 1.0 Pro和Claude 3模型家庭的性能。 GPT-4O和Gemini 1.5闪光在树木和图表上表现最好。 GPT-4O在树样品上达到了87.6％的精度，而Gemini 1.5 Flash在图样品上的精度为56.2％。我们的发现突出了结构和视觉变化对模型性能的影响。这项研究不仅引入了LMM基准，以促进复制和进一步探索，而且强调了LMM在解决复杂计算问题方面的潜力，对教学法和评估实践的重要意义。

### GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents 
[[arxiv](https://arxiv.org/abs/2412.10410)] [[cool](https://papers.cool/arxiv/2412.10410)] [[pdf](https://arxiv.org/pdf/2412.10410)]
> **Authors**: Shaofei Cai,Bowei Zhang,Zihao Wang,Haowei Lin,Xiaojian Ma,Anji Liu,Yitao Liang
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: GROOT-2：弱监督的多模式指令后面的代理
- **领域**: 人工智能,机器学习,机器人技术
- **摘要**: 可以遵循多模式说明的发展代理仍然是机器人技术和AI的基本挑战。尽管对未标记的数据集进行大规模的预训练（没有语言指令）使代理人能够学习各种行为，但这些代理通常在遵循指令方面挣扎。在使用指令标签增强数据集的同时，可以减轻此问题，但大规模获取如此高质量的注释是不切实际的。为了解决这个问题，我们将问题作为半监督的学习任务构图，并介绍Groot-2，Groot-2是一种使用新颖的方法训练的多模式指导代理，将弱监督与潜在变量模型相结合。我们的方法由两个关键组成部分组成：受约束的自我映射，该组成部分利用大量未标记的演示来使政策能够学习多种行为，而人类的意图一致性使用，该策略使用较小的标记示范来确保潜在空间反映人类的意图。 Groot-2在四种不同的环境中得到了验证，从视频游戏到机器人操纵，展示了其强大的多模式指导遵循功能。

### TANGO: Training-free Embodied AI Agents for Open-world Tasks 
[[arxiv](https://arxiv.org/abs/2412.10402)] [[cool](https://papers.cool/arxiv/2412.10402)] [[pdf](https://arxiv.org/pdf/2412.10402)]
> **Authors**: Filippo Ziliotto,Tommaso Campari,Luciano Serafini,Lamberto Ballan
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 探戈：无训练的体现AI代理商用于开放世界任务
- **领域**: 人工智能,机器人技术
- **摘要**: 大型语言模型（LLMS）在将各种模块组合在一起以创建可以在图像上执行复杂的推理任务的程序展示了出色的功能。在本文中，我们提出了探戈，这种方法是通过已经观察到的图像的LLM扩展程序组成的方法，旨在将这些功能整合到能够观察和行动世界的体现的代理中。具体而言，通过采用一个简单的点态导航模型与基于内存的探索策略作为指导代理商通过世界的基础原始性，我们展示了一个单个模型如何无需其他培训即可解决不同的任务。我们将LLM任命为撰写提供的原始词以解决特定任务，仅使用提示中的几个封闭式示例。我们在三个关键体现的AI任务上评估我们的方法：开放式对象目标导航，多模式的终身导航和开放的体现问答，实现了最先进的结果，而无需在挑战性的零击场景中进行任何特定的微调。

### From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities 
[[arxiv](https://arxiv.org/abs/2412.11694)] [[cool](https://papers.cool/arxiv/2412.11694)] [[pdf](https://arxiv.org/pdf/2412.11694)]
> **Authors**: Shixin Jiang,Jiafeng Liang,Jiyuan Wang,Xuan Dong,Heng Chang,Weijiang Yu,Jinhua Du,Ming Liu,Bing Qin
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: 35 pages
- **标题**: 从特定的mllms到Omni-Mllm：与多模式对齐的MLLM的调查
- **领域**: 人工智能,计算语言学,机器学习
- **摘要**: 为了解决现实情况下的复杂任务，越来越多的研究人员专注于Omni-Mllms，旨在获得Omni-Modal-Modal的理解和产生。除了任何特定非语言模式的约束之外，Omni-Mllms还将各种非语言模式映射到LLMS的嵌入空间中，并使单个模型中模态的任意组合具有相互作用和理解。在本文中，我们系统地研究了相关研究，并对Omni-Mllms进行了全面的调查。具体而言，我们首先解释了Omni-Mllms的四个核心组成部分，用于统一的多模式建模，并具有细致的分类法，提供了新的观点。然后，我们介绍了通过两阶段培训实现的有效整合，并讨论相应的数据集和评估。此外，我们总结了当前Omni-Mllms的主要挑战，并概述了未来的方向。我们希望本文成为初学者的介绍，并促进相关研究的进步。资源已在https://github.com/threegold116/awesome-omni-mllms上公开提供。

### From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed Instructions In A Multi-Modal Jungle 
[[arxiv](https://arxiv.org/abs/2412.12839)] [[cool](https://papers.cool/arxiv/2412.12839)] [[pdf](https://arxiv.org/pdf/2412.12839)]
> **Authors**: Kaustubh Vyas,Damien Graux,Yijun Yang,Sébastien Montella,Chenxin Diao,Wendi Zhou,Pavlos Vougiouklis,Ruofei Lai,Yang Ren,Keshuang Li,Jeff Z. Pan
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Under review
- **标题**: 从LLM群到PDDL授权的Hive：在多模式丛林中计划自我执行的说明
- **领域**: 人工智能
- **摘要**: 为了响应基于代理的解决方案的呼吁，该解决方案利用了深层模型的生态系统的不断增长的功能，我们介绍了Hive  - 一种全面的解决方案，用于选择适当的模型，并随后计划一组原子动作以满足最终用户的说明。 Hive可以通过一组模型进行操作，并在收到自然语言指令（即用户查询）后，安排和执行可解释的原子行动计划。这些动作可能涉及一个或多个可用模型，以实现总体任务，同时尊重最终用户的特定约束。值得注意的是，Hive处理涉及多模式输入和输出的任务，从而使其能够处理复杂的现实世界查询。我们的系统能够使用PDDL操作授权的基于LLM的正式逻辑骨干链来计划复杂的动作链，同时保证解释性。我们介绍了Muse基准，以便对代理系统的多模式功能进行全面评估。我们的发现表明，我们的框架重新定义了任务选择的最新作品，优于其他竞争系统，这些系统计划跨多个模型进行操作，同时提供透明度保证，同时完全遵守用户约束。

### MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants 
[[arxiv](https://arxiv.org/abs/2412.12661)] [[cool](https://papers.cool/arxiv/2412.12661)] [[pdf](https://arxiv.org/pdf/2412.12661)]
> **Authors**: Hritik Bansal,Daniel Israel,Siyan Zhao,Shufan Li,Tung Nguyen,Aditya Grover
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: 12 figures, 15 tables
- **标题**: Medmax：用于培训生物医学助理的混合模式教学调整
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 混合模式模型中的最新进展已使信息跨图像文本内容灵活地集成。这些模型为开发了能够分析生物医学图像，回答有关它们的复杂问题并预测医疗程序对患者健康的影响的统一生物医学助理的新途径。但是，现有资源面临诸如数据可用性有限，范围范围覆盖范围和限制资源（例如医疗论文）等挑战。为了解决这些差距，我们提出了Medmax，这是第一个大型多模式生物医学指导数据集，用于混合模式基础模型。 Medmax通过147万个实例，包括多种任务，包括多模式内容生成（交织的图像文本数据），生物医学图像字幕和生成，视觉聊天和报告理解。这些任务涵盖了放射学和组织病理学等各种医学领域。随后，我们在MEDMAX数据集上微调了混合模式基础模型，从而实现了重大的性能改进：比变色龙模型增长了26％，而在12个下游生物医学视觉问题交付任务中，GPT-4O的增长率比GPT-4O提高了18.3％。此外，我们引入了一个用于生物医学任务的统一评估套件，提供了一个强大的框架来指导下一代混合模式生物医学AI助手的发展。

### Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2412.12606)] [[cool](https://papers.cool/arxiv/2412.12606)] [[pdf](https://arxiv.org/pdf/2412.12606)]
> **Authors**: YiFan Zhang,Shanglin Lei,Runqi Qiao,Zhuoma GongQue,Xiaoshuai Song,Guanting Dong,Qiuna Tan,Zhe Wei,Peiqing Yang,Ye Tian,Yadong Xue,Xiaofei Wang,Honggang Zhang
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: 33 pages, 33 figures, Work in progress
- **标题**: 多维见解：在大型多模型中基准实现现实世界的个性化
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）的快速发展的领域导致具有显着功能的不同模型的出现。但是，现有的基准无法全面，客观和准确地评估LMM是否与现实情况下人类的各种需求保持一致。为了弥合这一差距，我们提出了多维见解（MDI）基准，其中包括500多个图像，涵盖了人类生活的六个常见情况。值得注意的是，与现有评估相比，MDI基准测试具有两个重要的优势：（1）每个图像都伴随两种类型的问题：简单的问题，以评估模型对图像的理解，以及评估模型分析和推理基本内容超出基本内容的能力的复杂问题。 （2）认识到，面对相同的情况时，不同年龄段的人会有不同的需求和观点，我们的基准测试将问题分为三个年龄类别：年轻人，中年人和老年人。这种设计允许对LMMS能力满足不同年龄段的偏好和需求的能力进行详细评估。借助MDI基准测试，像GPT-4O这样的强大模型在与年龄相关的任务上实现了79％的准确性，这表明现有的LMMS在解决现实世界应用程序方面仍然有很大的改进空间。展望未来，我们预计MDI基准标将开辟新的途径，以使LMM中的现实世界个性化结合。 MDI基准数据和评估代码可在https://mdi-benchmark.github.io/上获得。

### Relational Programming with Foundation Models 
[[arxiv](https://arxiv.org/abs/2412.14515)] [[cool](https://papers.cool/arxiv/2412.14515)] [[pdf](https://arxiv.org/pdf/2412.14515)]
> **Authors**: Ziyang Li,Jiani Huang,Jason Liu,Felix Zhu,Eric Zhao,William Dodds,Neelay Velingker,Rajeev Alur,Mayur Naik
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 基础模型的关系编程
- **领域**: 人工智能,编程语言
- **摘要**: 基础模型具有巨大的潜力，可以实现各种AI应用。这些模型的强大但不完整的性质刺激了广泛的机制，可以通过诸如文章学习，信息检索和代码解释等功能来增强它们。我们提出了Vieira，这是一个声明性的框架，该框架将这些机制统一在使用基础模型编程的一般解决方案中。 Vieira遵循概率关系范式，并将基础模型视为具有关系输入和输出的无状态功能。它通过简化各种子模型的组成来支持此类模型与逻辑程序以及复杂的多模式应用的无缝组合以及复杂的多模式应用，从而支持神经符号应用。我们通过使用支持基础模型作为插件的外国界面扩展扇贝编译器来实现Vieira。我们为包括GPT，CLIP和SAM在内的12种基础模型实施插件。我们评估Vieira的9个具有挑战性的任务，这些任务涵盖了语言，视觉和结构化和矢量数据库。我们的评估表明，维埃拉（Vieira）的程序简洁，可以融合现代基础模型，并且比竞争基线具有可比性或更好的准确性。

### ARTEMIS-DA: An Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics 
[[arxiv](https://arxiv.org/abs/2412.14146)] [[cool](https://papers.cool/arxiv/2412.14146)] [[pdf](https://arxiv.org/pdf/2412.14146)]
> **Authors**: Atin Sakkeer Hussain
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: Artemis-Da：数据分析中多步洞察综合的高级推理和转换引擎
- **领域**: 人工智能,数据库,信息检索,多代理系统
- **摘要**: 本文介绍了数据分析中多步洞察综合的高级推理和转换引擎（Artemis-DA），这是一个新颖的框架，旨在扩大大型语言模型（LLMS），用于解决复杂的多步数据分析任务。 Artemis-Da集成了三个核心组成部分：规划师，将复杂的用户查询分解为结构化的顺序指令，包括数据预处理，转换，预测建模和可视化；编码器动态生成和执行Python代码以实现这些说明；以及图形器，该图将生成的可视化效果解释为获得可行的见解。通过策划这些组件之间的协作，Artemis-Da有效地管理了涉及高级推理，多步转换和综合跨不同数据模式的复杂分析工作流程。该框架在基准（例如WikableQuestions and Tabfact）上实现了最新的（SOTA）性能，证明了其具有精确和适应性的复杂分析任务的能力。通过将LLM的推理功能与自动代码生成，执行和视觉分析相结合，Artemis-DA为多步洞察综合提供了强大的，可扩展的解决方案，以应对数据分析中的广泛挑战。

### A Concept-Centric Approach to Multi-Modality Learning 
[[arxiv](https://arxiv.org/abs/2412.13847)] [[cool](https://papers.cool/arxiv/2412.13847)] [[pdf](https://arxiv.org/pdf/2412.13847)]
> **Authors**: Yuchong Geng,Ao Tang
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 以概念为中心的多模式学习方法
- **领域**: 人工智能,机器学习
- **摘要**: 为了创建一个更高效的AI系统，我们引入了一个新的多模式学习框架，该框架利用具有抽象知识的模态敏锐的概念空间和一组量身定态的投影模型，该模型量身定制，用于处理不同的模态输入并将其映射到概念空间上。该概念空间与特定方式及其相关的投影模型脱钩，重点是学习跨模式普遍适用的抽象知识。随后，嵌入到概念空间中的知识简化了特定于模态投影模型的学习过程。我们在两个流行的任务上评估了框架：图像文本匹配和视觉问题回答。我们的框架在基准模型的同时表现出更有效的学习曲线，在基准模型的情况下达到了性能。

### SubstationAI: Multimodal Large Model-Based Approaches for Analyzing Substation Equipment Faults 
[[arxiv](https://arxiv.org/abs/2412.17077)] [[cool](https://papers.cool/arxiv/2412.17077)] [[pdf](https://arxiv.org/pdf/2412.17077)]
> **Authors**: Jinzhi Wang,Qinfeng Song,Lidong Qian,Haozhou Li,Qinke Peng,Jiangbo Zhang
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 变电站：用于分析变电站设备故障的多模式大型模型方法
- **领域**: 人工智能
- **摘要**: 变电站设备的可靠性对于电力系统的稳定性至关重要，但是传统的故障分析方法在很大程度上依赖手动专业知识，从而限制了它们在处理复杂和大规模数据方面的有效性。本文提出了一种基于多模式大语言模型（MLLM）的变电站设备故障分析方法。我们开发了一个包含40,000个条目的数据库，包括图像，缺陷标签和分析报告，并使用图像到视频生成模型进行数据增强。使用GPT-4生成详细的故障分析报告。基于此数据库，我们开发了FuremationAi，这是第一个专门用于变电站故障分析的模型，并设计了故障诊断知识库以及知识增强方法。实验结果表明，在各种评估指标上，变电站的表现明显胜过现有模型，例如GPT-4，在故障分析，修复建议和预防措施中表现出更高的准确性和实用性，为变电​​站设备故障分析提供了更先进的解决方案。

### VilBias: A Study of Bias Detection through Linguistic and Visual Cues , presenting Annotation Strategies, Evaluation, and Key Challenges 
[[arxiv](https://arxiv.org/abs/2412.17052)] [[cool](https://papers.cool/arxiv/2412.17052)] [[pdf](https://arxiv.org/pdf/2412.17052)]
> **Authors**: Shaina Raza,Caesar Saleh,Emrul Hasan,Franklin Ogidi,Maximus Powers,Veronica Chatrath,Marcelo Lotif,Roya Javadi,Anam Zahid,Vahid Reza Khazaie
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: Under review
- **标题**: Vilbias：通过语言和视觉提示对偏见检测的研究，提出注释策略，评估和关键挑战
- **领域**: 人工智能
- **摘要**: 大语言模型（LLM）和视觉模型（VLM）的集成为解决多模式内容分析中的复杂挑战开辟了新的途径，尤其是在有偏见的新闻检测中。这项研究介绍了VLBIAS，该框架利用最先进的LLM和VLM来检测新闻内容中的语言和视觉偏见。我们提出了一个多模式数据集，其中包括文本内容和来自不同新闻来源的相应图像。我们提出了一个混合注释框架，将基于LLM的注释与人类审查结合在一起，以确保高质量的标签，同时降低成本并提高可扩展性。我们的评估比较了最先进的SLM和LLM的表现（文本和图像），结果表明，尽管SLM在计算上是有效的，但LLMS在识别微妙的框架和文本视觉上不一致方面表现出了较高的准确性。此外，经验分析表明，将视觉提示与文本数据一起纳入偏置检测准确性3％至5％。这项研究提供了对LLM，SLM和VLM的全面探索，作为检测新闻内容中多模式偏见的工具，并突出了它们各自的优势，局限性和未来应用的潜力

### A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information 
[[arxiv](https://arxiv.org/abs/2412.16874)] [[cool](https://papers.cool/arxiv/2412.16874)] [[pdf](https://arxiv.org/pdf/2412.16874)]
> **Authors**: M Anuprabha,Krishna Gurugubelli,V Kesavaraj,Anil Kumar Vuppala
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: 5 pages, 1 figure
- **标题**: 使用语音和文本信息的多模式方法进行构音障碍检测和严重性评估
- **领域**: 人工智能,音频和语音处理
- **摘要**: 构音障碍的自动检测和严重性评估对于为患者提供有针对性的治疗干预措施至关重要。尽管大多数现有研究主要关注语音方式，但本研究介绍了一种利用语音和文本方式的新方法。通过采用跨注意机制，我们的方法了解语音和文本表示之间的声学​​和语言相似性。该方法特别评估了不同严重程度之间的发音偏差，从而提高了质心检测和严重程度评估的准确性。所有实验均已使用UA语音质心数据库进行。当使用说话者依赖和说话者独立的，看不见的单词设置时，已实现了99.53％和93.20％的检测精度和98.12％和51.97％的严重性评估。这些发现表明，通过整合提供参考语言知识的文本信息，已经开发了一个更强大的框架来进行违反检测和评估，从而可能导致更有效的诊断。

### Visual Prompting with Iterative Refinement for Design Critique Generation 
[[arxiv](https://arxiv.org/abs/2412.16829)] [[cool](https://papers.cool/arxiv/2412.16829)] [[pdf](https://arxiv.org/pdf/2412.16829)]
> **Authors**: Peitong Duan,Chin-Yi Chen,Bjoern Hartmann,Yang Li
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 视觉提示，以设计批评生成的迭代精致
- **领域**: 人工智能
- **摘要**: 反馈对于每个设计过程至关重要，例如用户界面（UI）设计，并且自动化设计评论可以显着提高设计工作流程的效率。尽管现有的多模式大型语言模型（LLM）在许多任务中都表现出色，但它们通常在产生高质量的设计评论方面很难 - 这项复杂的任务需要在给定设计的图像中产生详细的设计评论。基于最新的文本输出和视觉提示方法的进步，我们建议对UI评论进行一种迭代视觉提示方法，该方法采用了输入UI屏幕截图和设计指南，并生成了设计注释的列表，以及对相应的边界框，将每个注释映射到ScreenShot中的特定区域。整个过程完全由LLMS驱动，它使用为每个步骤量身定制的少量样本在迭代地完善文本输出和边界框。我们使用Gemini-1.5-Pro和GPT-4O评估了我们的方法，并发现人类专家通常更喜欢管道产生的设计评论，而不是基线的批评，而管道将人类绩效的差距从人类绩效中降低了50％，以降低一个评级度量。为了评估我们对其他多模式任务的方法的普遍性，我们将管道应用于开放式视频对象和属性检测，实验表明我们的方法也表现出了基线的表现。

### Do Multimodal Language Models Really Understand Direction? A Benchmark for Compass Direction Reasoning 
[[arxiv](https://arxiv.org/abs/2412.16599)] [[cool](https://papers.cool/arxiv/2412.16599)] [[pdf](https://arxiv.org/pdf/2412.16599)]
> **Authors**: Hang Yin,Zhifeng Lin,Xin Liu,Bin Sun,Kan Li
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 多模式模型真的了解方向吗？指南指导推理的基准
- **领域**: 人工智能
- **摘要**: 方向推理对于智能系统了解现实世界至关重要。尽管现有作品主要集中在空间推理上，但指南针的方向推理仍然没有被逐渐解散。为了解决这个问题，我们提出了指南针方向推理（CDR）基准，旨在评估多模式模型（MLMS）的方向推理能力。 CDR包括三种类型的图像来测试空间（向上，向下，左，右）和指南针（北，南，东，西）方向。我们的评估表明，大多数MLM都在方向推理上挣扎，通常以随机的猜测水平进行。实验表明，直接使用CDR数据培训会产生有限的改进，因为它需要了解现实世界的物理规则。我们探讨了Mixdata和Cot微调方法的影响，这些方法通过合并多种数据和逐步推理，从而显着提高了指南针方向推理的MLM性能，从而提高了该模型了解方向关系的能力。

### ActPC-Chem: Discrete Active Predictive Coding for Goal-Guided Algorithmic Chemistry as a Potential Cognitive Kernel for Hyperon & PRIMUS-Based AGI 
[[arxiv](https://arxiv.org/abs/2412.16547)] [[cool](https://papers.cool/arxiv/2412.16547)] [[pdf](https://arxiv.org/pdf/2412.16547)]
> **Authors**: Ben Goertzel
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: ACTPC-Chem：目标引导算法化学的离散主动预测编码是基于Hyperon＆Primus AGI的潜在认知核
- **领域**: 人工智能
- **摘要**: 我们探索了一种新颖的范式（标有ACTPC-Chem），用于以生物学启发的，目标引导的人工智能（AI）为中心，该范例以一种在重写规则的算法化学中运行的离散主动预测编码（ACTPC）形式。 ACTPC-Chem被设想为用于高级认知体系结构的基础“认知核”，例如Opencog Hyperon系统，结合了Primus认知架构的基本要素。核心论点是，可以在数据和模型表示Metagraph重写规则以及预测错误，内在和外在的奖励，以及语义约束指导这些规则的持续重新组织和培养的系统中，将数据和模型表示为不断发展的模式，可以将数据和模型表示为不断发展的模式。我们使用虚拟的“机器人错误”思想实验，说明了这样的系统如何自组织以处理涉及延迟和上下文依赖性奖励的具有挑战性的任务，将因果统治推理（AIRIS）和概率逻辑抽象（PLN）整合到发现和利用概念模式和因果约束。接下来，我们描述如何将连续的预测性编码神经网络（在处理嘈杂的感觉数据和电动机控制信号方面出色）与离散ACTPC底物一致合并。最后，我们概述了如何扩展这些想法，以创建类似变形金刚的架构，该体系结构过时，以支持ACTPC指导的基于规则的转换。这种分层的体系结构补充了Airis和PLN，承诺结构化，多模式和逻辑上一致的下一步预测和叙事序列。

### AutoLife: Automatic Life Journaling with Smartphones and LLMs 
[[arxiv](https://arxiv.org/abs/2412.15714)] [[cool](https://papers.cool/arxiv/2412.15714)] [[pdf](https://arxiv.org/pdf/2412.15714)]
> **Authors**: Huatao Xu,Panrong Tong,Mo Li,Mani Srivastava
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: 13 pages
- **标题**: Autolife：使用智能手机和LLM的自动生活日记
- **领域**: 人工智能,计算语言学,人机交互
- **摘要**: 本文介绍了一种新颖的移动传感应用程序 - 生活日记 - 旨在生成用户日常生活的语义描述。我们介绍Autolife，这是一种基于商用智能手机的自动生活日记系统。 Autolife仅从智能手机输入低成本传感器数据（无照片或音频），并且可以自动为用户生成全面的生活期刊。为了实现这一目标，我们首先从多模式传感器数据中得出时间，运动和位置环境，并利用大语言模型（LLMS）的零拍功能，并具有对人类生活的常识知识，以解释不同的环境并产生生命期刊。为了管理任务复杂性和长时间的感应持续时间，提出了一个多层框架，该框架分解了任务，并将LLM与其他用于生活日记的技术无缝集成。这项研究确立了一个现实生活中的数据集作为基准，广泛的实验结果表明，自动生物会产生准确可靠的生活期刊。

### Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage 
[[arxiv](https://arxiv.org/abs/2412.15606)] [[cool](https://papers.cool/arxiv/2412.15606)] [[pdf](https://arxiv.org/pdf/2412.15606)]
> **Authors**: Zhi Gao,Bofei Zhang,Pengxiang Li,Xiaojian Ma,Tao Yuan,Yue Fan,Yuwei Wu,Yunde Jia,Song-Chun Zhu,Qing Li
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: ICLR 2025, https://mat-agent.github.io/
- **标题**: 多模式代理调整：构建VLM驱动的代理以进行有效的工具使用
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 大型语言模型（LLMS）的进步促使多模式代理的开发，这些代理被用作调用外部工具的控制器，提供了一种可行的方法来解决实际任务。在本文中，我们提出了一种多模式代理调整方法，该方法将自动生成多模式的工具插图数据并调整视觉语言模型（VLM）作为强大的工具使用推理的控制器。为了保留数据质量，我们提示GPT-4O迷你模型生成查询，文件和轨迹，然后再进行查询文件和轨迹验证器。基于数据综合管道，我们收集包含具有工具使用轨迹的20K任务的MM-TRAJ数据集。然后，我们通过\ supesline {t} rajectory \ lisewmess {t}在VLMS上使用MM-traj开发t3代理{对GTA和GAIA基准测试的评估表明，T3代理在两个流行的VLM上始终取得改进：minicpm-v-8.5b和{qwen2-vl-7b}，它超过了未经培训的VLMS的$ 20 \％$，显示了提出的数据合成型的效率，该工具是高高的。

### Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.18084)] [[cool](https://papers.cool/arxiv/2412.18084)] [[pdf](https://arxiv.org/pdf/2412.18084)]
> **Authors**: Xuan Lin,Long Chen,Yile Wang,Xiangxiang Zeng,Philip S. Yu
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: 9
- **标题**: 使用大语言模型的多任务分子生成的属性增强说明调整
- **领域**: 人工智能
- **摘要**: 大型语言模型（LLM）广泛应用于各种自然语言处理任务，例如问答和机器翻译。但是，由于缺乏标记的数据和生化特性的手动注释难度，因此分子生成任务的性能仍然受到限制，尤其是对于涉及多专业约束的任务。在这项工作中，我们提出了一个两步的框架PEIT（属性增强的指令调整），以改善与分子相关任务的LLM。在第一步中，我们将文本描述，微笑和生化属性用作多模式输入，以预先训练一种称为PEIT基因的模型，通过对齐多模式表示形式来综合指令数据。在第二步中，我们通过合成数据微调现有的开源LLM，所得的PEIT-LLM可以处理分子字幕，基于文本的分子产生，分子属性预测以及我们新提出的多构造分子生成任务。实验结果表明，我们的预训练的PEIT基因在分子字幕中优于Molt5和Biot5，表明文本描述，结构和生化特性之间的模态很好。此外，PEIT-LLM显示出多任务分子生成的有希望的改善，证明了PEIT框架对各种分子任务的可扩展性。我们在https://github.com/chenlong164/peit中发布代码，构建的指令数据和模型检查点。

### Bridging the Data Provenance Gap Across Text, Speech and Video 
[[arxiv](https://arxiv.org/abs/2412.17847)] [[cool](https://papers.cool/arxiv/2412.17847)] [[pdf](https://arxiv.org/pdf/2412.17847)]
> **Authors**: Shayne Longpre,Nikhil Singh,Manuel Cherep,Kushagra Tiwary,Joanna Materzynska,William Brannon,Robert Mahari,Naana Obeng-Marnu,Manan Dey,Mohammed Hamdy,Nayan Saxena,Ahmad Mustafa Anis,Emad A. Alghamdi,Vu Minh Chien,Da Yin,Kun Qian,Yizhi Li,Minnie Liang,An Dinh,Shrestha Mohanty,Deividas Mataciunas,Tobin South,Jianguo Zhang,Ariel N. Lee,Campbell S. Lund, et al. (18 additional authors not shown)
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-24
> **comment**: ICLR 2025. 10 pages, 5 figures (main paper)
- **标题**: 跨越文本，语音和视频的数据出处差距
- **领域**: 人工智能,计算语言学,计算机与社会,机器学习,多媒体
- **摘要**: AI的进展主要由培训数据的规模和质量驱动。尽管如此，经验分析还是研究了良好的数据集的属性。在这项工作中，我们进行了跨模态的最大和初始纵向审核 - 受欢迎的文本，语音和视频数据集 - 从其详细的采购趋势中浏览，并将其限制对其地理和语言代表。我们的手册分析涵盖了1990  -  2024年之间近4000个公共数据集，涵盖608种语言，798个来源，659个组织和67个国家 /地区。 We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry非商业限制。 Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI.作为对数据集透明度和负责任使用的持续改进的贡献，我们发布了整个多模式审核，使从业者可以在文本，语音和视频中追踪数据出处。

### Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy 
[[arxiv](https://arxiv.org/abs/2412.17759)] [[cool](https://papers.cool/arxiv/2412.17759)] [[pdf](https://arxiv.org/pdf/2412.17759)]
> **Authors**: Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Bhargava Kumar,Amit Agarwal,Ishan Banerjee,Srikant Panda,Tejaswini Kumar
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 大型多模式模型数据集，应用程序类别和分类学调查
- **领域**: 人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 多模式学习是人工智能中快速发展的领域，试图通过整合和分析各种类型的数据（包括文本，图像，音频和视频）来构建更广泛和健壮的系统。受到人类通过多种感官吸收信息的能力的启发，此方法可以实现诸如文本到视频转换，视觉问题的回答和图像字幕之类的应用程序。在此概述中，强调了支持多模式模型（MLLM）的数据集中的最新发展。大规模的多模式数据集是必不可少的，因为它们允许对这些模型进行彻底的测试和培训。该研究强调了他们对学科的贡献，研究了各种数据集，包括用于培训的数据集，特定于领域的任务和现实世界应用程序。它还强调了在一系列方案，可伸缩性和适用性中评估模型性能的关键基准数据集的重要性。由于多模式学习总是在变化，因此克服这些障碍将有助于AI研究，并应用应用新高度。

### An Adaptive Framework for Multi-View Clustering Leveraging Conditional Entropy Optimization 
[[arxiv](https://arxiv.org/abs/2412.17647)] [[cool](https://papers.cool/arxiv/2412.17647)] [[pdf](https://arxiv.org/pdf/2412.17647)]
> **Authors**: Lijian Li
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 用于多视图聚类的自适应框架，利用条件熵优化
- **领域**: 人工智能
- **摘要**: 多视图聚类（MVC）已成为一种强大的技术，用于从以多种观点或模式为特征的数据中提取有价值的见解。尽管取得了重大进展，但现有的MVC方法与有效量化观点之间的一致性和互补性努力，并且尤其容易受到嘈杂观点的不利影响（称为嘈杂的视图缺陷（NVD））。为了应对这些挑战，我们提出了CE-MVC，这是一个新颖的框架，将适应性加权算法与参数解耦的深层模型集成在一起。 CE-MVC利用条件熵的概念和标准化的相互信息，可以定量评估和加权每种观点的信息贡献，从而促进稳健的统一表示形式的构建。参数 - 耦合设计可以独立处理每种视图，从而有效地减轻了噪声的影响并增强了整体聚类性能。广泛的实验表明，CE-MVC胜过现有方法，为多视图聚类任务提供了更具弹性和准确的解决方案。

### ANID: How Far Are We? Evaluating the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance 
[[arxiv](https://arxiv.org/abs/2412.17632)] [[cool](https://papers.cool/arxiv/2412.17632)] [[pdf](https://arxiv.org/pdf/2412.17632)]
> **Authors**: Renyang Liu,Ziyu Lyu,Wei Zhou,See-Kiong Ng
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: Anid：我们有多远？通过多模式指导评估AI合成图像与自然图像之间的差异
- **领域**: 人工智能,计算机视觉和模式识别,多媒体
- **摘要**: 在人工智能产生的内容的快速发展的领域（AIGC）中，主要挑战之一是将AI合成的图像与自然图像区分开。尽管高级AI生成模型在产生视觉上引人入胜的图像方面具有显着的功能，但是当将这些图像与天然图像进行比较时，仍然存在很大的差异。为了系统地调查和量化这些差异，我们介绍了一个AI天然图像差异评估基准，旨在解决关键问题：\ textit {textit {ai基因生成的图像（AIGIS）与真正逼真的图像有多远？ AIGI样本由8个代表性模型生成，同时使用单峰和多模式提示，例如文本对图像（T2I），图像到图像（I2i）以及Text \ text \ textIt {vs。}图像到图像（TI2I）。我们的细粒度评估框架提供了跨五个关键维度的DNAi数据集的全面评估：天真的视觉特征质量，多模式生成的语义对齐，美学吸引力，下游任务适用性和协调的人类验证。广泛的评估结果突出了这些维度之间的显着差异，强调了将定量指标与人类判断的必要性，以使对AI生成的图像质量有整体理解。代码可在\ href {https://github.com/ryliu68/anid} {https://github.com/ryliu68/anid}中获得。

### Retention Score: Quantifying Jailbreak Risks for Vision Language Models 
[[arxiv](https://arxiv.org/abs/2412.17544)] [[cool](https://papers.cool/arxiv/2412.17544)] [[pdf](https://arxiv.org/pdf/2412.17544)]
> **Authors**: Zaitang Li,Pin-Yu Chen,Tsung-Yi Ho
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: 14 pages, 8 figures, AAAI 2025
- **标题**: 保留得分：量化视觉语言模型的越狱风险
- **领域**: 人工智能
- **摘要**: 视觉模型（VLM）的出现是将计算机视觉与大语言模型（LLMS）集成以增强多模式机器学习能力的重大进步。但是，这一进展也使VLMS容易受到复杂的对抗性攻击的影响，从而引起了人们对其可靠性的担忧。本文的目的是评估VLM对越狱攻击的弹性，这些越狱攻击可以损害模型安全的依从性并导致有害的产出。为了评估VLM对对抗性输入扰动保持其鲁棒性的能力，我们提出了一种称为\ textbf {保留得分}的新型指标。保留评分是一种多模式评估指标，包括retention-i和保留-T分数，用于量化VLM的视觉和文本组件中的越狱风险。我们的过程涉及使用条件扩散模型生成合成图像文本对。然后，通过VLM与毒性判断分类器一起预测这些对的毒性评分。通过计算毒性得分的边缘，我们可以以攻击性不足的方式量化VLM的鲁棒性。我们的工作有四个主要贡献。首先，我们证明保留得分可以用作认证的鲁棒性指标。其次，我们证明，与相应的普通VLM相比，大多数具有视觉组件的VLM对越狱攻击的强大不足。此外，我们评估了Black-Box VLM API，并发现Google Gemini中的安全设置显着影响得分和鲁棒性。此外，GPT4V的鲁棒性与双子座的中等设置相似。最后，我们的方法为现有的对抗攻击方法提供了时间效率的替代方案，并在包括Minigpt-4，TendentBlip和Llava在内的VLMS上评估时提供了一致的模型鲁棒性排名。

### MineAgent: Towards Remote-Sensing Mineral Exploration with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.17339)] [[cool](https://papers.cool/arxiv/2412.17339)] [[pdf](https://arxiv.org/pdf/2412.17339)]
> **Authors**: Beibei Yu,Tao Shen,Hongbin Na,Ling Chen,Denqi Li
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 缩小：使用多模式大语言模型进行遥感矿物探索
- **领域**: 人工智能,计算语言学
- **摘要**: 遥感矿物勘探对于确定经济可行的矿物质沉积物至关重要，但它对多模式大语言模型（MLLM）构成了重大挑战。这些包括在域特异性地质知识中的局限性和跨多个遥感图像推理的困难，进一步加剧了长期文化问题。为了解决这些问题，我们提出了一个模块化框架，利用层次结构的判断和决策模块来改善多图像推理和空间光谱整合。在此互补的情况下，我们提出了Mine Bench，这是一种使用地质和高光谱数据来评估特定领域矿物勘探任务中MLLM的基准。广泛的实验证明了缩影的有效性，强调了其在遥感矿物勘探中推进MLLM的潜力。

### Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent 
[[arxiv](https://arxiv.org/abs/2412.18428)] [[cool](https://papers.cool/arxiv/2412.18428)] [[pdf](https://arxiv.org/pdf/2412.18428)]
> **Authors**: Farhad Nooralahzadeh,Yi Zhang,Jonathan Furst,Kurt Stockinger
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: 通过LLM代理商以自然语言解释可解释的多模式数据探索
- **领域**: 人工智能,计算语言学
- **摘要**: 国际企业，组织或医院收集大量存储在数据库，文本文档，图像和视频中的多模式数据。尽管在多模式数据探索的单独领域以及自动将自然语言问题转换为数据库查询语言的数据库系统中，但在数据库系统中取得了进展，但查询数据库系统与其他非结构化模式（如自然语言中的图像）相结合的研究挑战被广泛地尚未开发。在本文中，我们提出了Xmode-一种系统，可以以自然语言进行可解释的多模式数据探索。我们的方法基于以下研究贡献：（1）我们的系统灵感来自现实世界中的用例，该案例使用户能够探索多模式信息系统。 （2）Xmode利用基于LLM的代理AI框架将自然语言问题分解为子任务，例如文本到SQL生成和图像分析。 (3) Experimental results on multi-modal datasets over relational data and images demonstrate that our system outperforms state-of-the-art multi-modal exploration systems, excelling not only in accuracy but also in various performance metrics such as query latency, API costs, planning efficiency, and explanation quality, thanks to the more effective utilization of the reasoning capabilities of LLMs.

### GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agent 
[[arxiv](https://arxiv.org/abs/2412.18426)] [[cool](https://papers.cool/arxiv/2412.18426)] [[pdf](https://arxiv.org/pdf/2412.18426)]
> **Authors**: Kangjia Zhao,Jiahui Song,Leigang Sha,Haozhan Shen,Zhi Chen,Tiancheng Zhao,Xiubo Liang,Jianwei Yin
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: GUI测试领域：用于推进自动gui测试代理的统一基准
- **领域**: 人工智能
- **摘要**: 如今，对GUI代理的研究是AI社区中的热门话题。但是，当前的研究着重于GUI任务自动化，从而限制了各种GUI方案的应用程序范围。在本文中，我们提出了一个正式而全面的环境，以评估自动GUI测试的整个过程（GTARENA），为各种多模式大语言模型的一致运行提供了公平，标准化的环境。我们将测试过程分为三个关键子任务：测试意图生成，测试任务执行和GUI缺陷检测，并基于这些检测来构建基准数据集以进行全面的评估。它使用三种数据类型评估不同模型的性能：实际移动应用程序，具有人为注入缺陷的移动应用程序和合成数据，并在此相关任务中彻底评估了它们的功能。此外，我们提出了一种方法，可以帮助研究人员探索在特定方案中多模式大型模型的性能与其在标准基准测试中的一般能力之间的相关性。实验结果表明，即使是最先进的模型也很难在自动GUI测试的所有子任务中表现良好，从而突出了当前自动GUI测试的当前功能与其实用的现实世界中适用性之间的显着差距。该差距为GUI代理开发的未来方向提供了指导。我们的代码可在https://github.com/zju-aces-ise/chatuitest上找到。

### LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating 
[[arxiv](https://arxiv.org/abs/2412.18424)] [[cool](https://papers.cool/arxiv/2412.18424)] [[pdf](https://arxiv.org/pdf/2412.18424)]
> **Authors**: Chao Deng,Jiale Yuan,Pi Bu,Peijie Wang,Zhong-Zhi Li,Jian Xu,Xiao-Hui Li,Yuan Gao,Jun Song,Bo Zheng,Cheng-Lin Liu
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: Longdocurl：一个全面的多模式长文件基准，整合了理解，推理和定位
- **领域**: 人工智能,计算语言学
- **摘要**: 大型视觉语言模型（LVLM）已改进了文档的理解功能，从而可以处理复杂的文档元素，更长的上下文和更广泛的任务。但是，现有的文档理解基准仅限于处理少数页面，并且无法对布局元素定位进行全面分析。在本文中，我们首先定义了三个主要任务类别：长文档的理解，数值推理和交叉元件定位，然后提出了一个全面的基准测试，Longdocurl，在上面的三个主要任务中集成了三个主要任务，并包括20个基于不同的主要任务分类的子任务，并得到了回答。此外，我们开发了一条半自动化的施工管道，并收集了2,325个高质量的提问对，覆盖了33,000多页的文档，显着优于现有的基准测试。随后，我们对26种不同配置的开源和闭合源模型进行了全面的评估实验，从而揭示了该领域的关键性能差距。

### The Thousand Brains Project: A New Paradigm for Sensorimotor Intelligence 
[[arxiv](https://arxiv.org/abs/2412.18354)] [[cool](https://papers.cool/arxiv/2412.18354)] [[pdf](https://arxiv.org/pdf/2412.18354)]
> **Authors**: Viviane Clay,Niels Leadholm,Jeff Hawkins
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: 千脑项目：感觉运动智能的新范式
- **领域**: 人工智能,神经元和认知
- **摘要**: 在过去的十年中，人工智能迅速发展，主要是由深度学习系统规模的进步驱动。尽管有这些进步，但可以在多样化的现实环境中有效运行的智能系统仍然是一个重大挑战。在这份白皮书中，我们概述了千兆头项目，这是一项正在进行的研究工作，旨在开发一种源自新皮层的操作原理的替代性，补充形式的AI。我们提出了一个早期版本的一个千脑系统，这是一种具有独特的感觉运动剂，它非常适合快速学习各种任务，并最终实现了人类新皮层具有的任何能力。其设计的核心是使用重复计算单元的使用，该模块在哺乳动物大脑中发现的皮质柱上建模。每个学习模块都是一个半独立的单元，可以通过空间结构的参考框架对整个对象进行建模，代表信息，并且既有估计值，并且能够在世界上影响运动。学习是一个快速，关联的过程，类似于大脑中的Hebbian学习，并且在世界的空间结构周围利用归纳偏见，以实现快速而持续的学习。多个学习模块可以通过“皮质消息传递协议”（CMP）在层次和非等级上相互交互，从而创建更多的抽象表示并支持多模式集成。我们概述了激励千脑系统设计的关键原则，并提供了有关Monty的实施的详细信息，这是我们对这种系统的首次实例化。代码可以在https://github.com/thonthonebrainsproject/tbp.monty上找到，以及https://thonconsproject.readme.io/的更详细文档。

### "Did my figure do justice to the answer?" : Towards Multimodal Short Answer Grading with Feedback (MMSAF) 
[[arxiv](https://arxiv.org/abs/2412.19755)] [[cool](https://papers.cool/arxiv/2412.19755)] [[pdf](https://arxiv.org/pdf/2412.19755)]
> **Authors**: Pritam Sil,Bhaskaran Raman,Pushpak Bhattacharyya
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: “我的身材对答案有公正吗？” ：使用反馈（MMSAF）进行多模式的简短答案分级
- **领域**: 人工智能
- **摘要**: 评估在学生的学习过程中起着至关重要的作用，通过提供有关学生在学科的熟练程度的反馈。虽然评估通常会使用简短的答案问题，但通常很难大规模对此类问题进行评分。此外，这些问题通常涉及学生绘制支持图及其文本解释的绘制。此类问题通常会促进多模式识字率，并与基于能力的问题保持一致，这些问题要求学生更深入地认知处理能力。但是，现有文献并不涉及此类答案的自动分级。因此，为了弥合这一差距，我们提出了使用反馈问题（MMSAF）问题以及2197个数据点的数据集。此外，我们还提供了一个自动化框架来生成此类数据集。我们对该数据集的现有大型语言模型（LLM）的评估在正确度标签的水平上达到了55％的总体精度，图像相关性标签的总体准确性为75％。根据人类专家的说法，Pixtral更加符合人类对生物学和化学的判断和价值观的价值观，并且在大多数参数中，Pixtral在5分中达到了4分或更多的分数。

### Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework 
[[arxiv](https://arxiv.org/abs/2412.19684)] [[cool](https://papers.cool/arxiv/2412.19684)] [[pdf](https://arxiv.org/pdf/2412.19684)]
> **Authors**: Jiang Liu,Bolin Li,Haoyuan Li,Tianwei Lin,Wenqiao Zhang,Tao Zhong,Zhelun Yu,Jinghao Wei,Hao Cheng,Wanggui He,Fangxun Shu,Hao Jiang,Zheqi Lv,Juncheng Li,Siliang Tang,Yueting Zhuang
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 提高对高效MLLM的私人领域的理解：无调，自适应，通用及时的优化框架
- **领域**: 人工智能
- **摘要**: 与多模式大型语言模型（MLLM）相比，有效的多模式大型语言模型（EMLLMS）降低了模型大小和计算成本，并且经常在资源受限的设备上部署。但是，由于数据隐私问题，现有的开源EMLLM在预训练过程中很少访问特定于私有域的数据，从而使它们难以直接应用于设备特定的域，例如某些业务场景。为了解决这一弱点，本文着重于EMLLM对私人域的有效适应，特别是在两个领域：1）如何减少数据需求； 2）如何避免参数进行微调。具体而言，我们提出了一个tun \ textbf {\下划线{i}} ng-free，a \ textbf {\ lisesline {d}} aptiv \ textbf {\ textbf {\ textbf {\ posissline {e}}，Univers \ textbf缩写为\ textIt {\ textbf {\ oursethod {}}}，由两个阶段组成：1）根据加强搜索策略的预定提示，生成一个迅速优化策略树以获取优化的优化探针； 2）提示反射基于优化先验初始化提示，然后进行自我反思以进一步搜索和完善提示。通过这样做，\ oureMethod {}优雅地生成了``理想提示''来处理特定于私有域的数据。请注意，我们的方法不需要微调参数，只需要少量数据即可快速适应私人数据的数据分布。跨多个任务的广泛实验表明，与基准相比，我们提出的\ ourshod {}显着提高了效率和性能。

### A Survey on Large Language Model Acceleration based on KV Cache Management 
[[arxiv](https://arxiv.org/abs/2412.19442)] [[cool](https://papers.cool/arxiv/2412.19442)] [[pdf](https://arxiv.org/pdf/2412.19442)]
> **Authors**: Haoyang Li,Yiming Li,Anxin Tian,Tianhao Tang,Zhanchao Xu,Xuejia Chen,Nicole Hu,Wei Dong,Qing Li,Lei Chen
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 基于KV缓存管理的大型语言模型加速度的调查
- **领域**: 人工智能,分布式、并行和集群计算
- **摘要**: 大型语言模型（LLM）彻底改变了广泛的领域，例如自然语言处理，计算机视觉和多模式任务，因为它们能够理解上下文和执行逻辑推理。但是，LLM的计算和内存需求，尤其是在推断期间，在将其扩展到现实世界，长篇下C下文和实时应用程序时会构成重大挑战。键值（KV）缓存管理已成为一种关键的优化技术，用于通过减少冗余计算和改善内存利用来加速LLM推断。这项调查提供了针对LLM加速的KV缓存管理策略的全面概述，将其分为令牌级别，模型级别和系统级优化。令牌级的策略包括KV缓存选择，预算分配，合并，量化和低级别分解，而模型级优化的侧重于架构创新和注意力机制，以增强KV重复使用。系统级别的方法解决内存管理，调度和硬件感知设计，以提高各种计算环境的效率。此外，调查还概述了用于评估这些策略的文本和多模式数据集和基准。通过介绍详细的分类法和比较分析，这项工作旨在为研究人员和从业人员提供有用的见解，以支持开发高效且可扩展的KV缓存管理技术，从而有助于LLM在现实世界中的实际部署。 KV缓存管理的策划纸张列表在：\ href {https://github.com/treai-lab/awesome-kv-cache-management} {https://github.com/treeai-lab/awesome-lab/awesome-kv-cache-management}。

### TravelAgent: Generative Agents in the Built Environment 
[[arxiv](https://arxiv.org/abs/2412.18985)] [[cool](https://papers.cool/arxiv/2412.18985)] [[pdf](https://arxiv.org/pdf/2412.18985)]
> **Authors**: Ariel Noyman,Kai Hu,Kent Larson
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: 21 pages 9 figs
- **标题**: TravelAgent：建筑环境中的生成代理
- **领域**: 人工智能,人机交互
- **摘要**: 在建筑环境中了解人类行为对于设计功能性，以用户为中心的城市空间至关重要。传统的方法，例如手动观察，调查和简化的模拟，通常无法捕获现实世界行为的复杂性和动态。为了解决这些局限性，我们介绍了TravelAgent，这是一个新型的模拟平台，在不同的室内和室外环境下，在不同的上下文和环境条件下建模了跨室内和室外环境的活动模式。 TravelAgent利用生成代理集成到3D虚拟环境中，使代理能够处理多模式的感觉输入并表现出类似人类的决策，行为和适应性。通过实验，包括导航，寻路和自由勘探，我们分析了100个模拟的数据，其中包括跨不同空间布局和代理原型的1898代理步骤，达到了总体任务完成率76％。使用空间，语言和情感分析，我们展示了代理人如何感知，适应或在周围的任务和分配的任务上挣扎。我们的发现突出了TravelAgent作为城市设计，空间认知研究和基于代理的建模的工具的潜力。我们讨论了用于评估和完善空间设计的生成代理时的关键挑战和机遇，并提出TravelAgent作为模拟和理解建筑环境中人类经验的新范式。

### EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior Generation 
[[arxiv](https://arxiv.org/abs/2412.18907)] [[cool](https://papers.cool/arxiv/2412.18907)] [[pdf](https://arxiv.org/pdf/2412.18907)]
> **Authors**: Carl Qi,Dan Haramati,Tal Daniel,Aviv Tamar,Amy Zhang
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: EC-DIFFUSER：通过以实体为中心的行为产生的多对象操作
- **领域**: 人工智能,计算机视觉和模式识别,机器人技术
- **摘要**: 对象操纵是日常任务的常见组成部分，但是学会从高维观察中操纵对象带来了重大挑战。由于状态空间的组合复杂性以及所需的行为的组合复杂性，这些挑战在多对象环境中得到了加剧。尽管最近的方法利用大规模的离线数据从像素观测值中训练模型，但通过缩放来实现性能的提高，但这些方法在具有约束的网络和数据集尺寸的看不见的对象配置中与组成概括相比。为了解决这些问题，我们提出了一种新型的行为克隆（BC）方法，该方法利用基于扩散的优化来利用以对象为中心的表示和以实体为中心的变压器，从而从离线图像数据中有效学习。我们的方法首先将观测值分解为以对象为中心的表示，然后由以实体为中心的变压器处理，该变压器在对象级别上计算注意力，同时预测对象动力学和代理的动作。结合扩散模型捕获多模式行为分布的能力，这会导致多对象任务的实质性改进，更重要的是，可以实现组成概括。我们向具有新颖的对象和目标组成的任务介绍了能够零概括的BC代理，其中包括比训练期间看到的大量对象。我们在网页上提供视频推广：https：//sites.google.com/view/ec-diffuser。

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

该领域共有 1 篇论文

### INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent 
[[arxiv](https://arxiv.org/abs/2412.18174)] [[cool](https://papers.cool/arxiv/2412.18174)] [[pdf](https://arxiv.org/pdf/2412.18174)]
> **Authors**: Haohang Li,Yupeng Cao,Yangyang Yu,Shashidhar Reddy Javaji,Zhiyang Deng,Yueru He,Yuechen Jiang,Zining Zhu,Koduvayur Subbalakshmi,Guojun Xiong,Jimin Huang,Lingfei Qian,Xueqing Peng,Qianqian Xie,Jordan W. Suchow
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: InvestorBench：与LLM代理商一起进行财务决策任务的基准
- **领域**: 计算工程、金融和科学,人工智能,计算金融
- **摘要**: 最近的进步强调了大语言模型（LLM）在财务决策中的潜力。尽管取得了进展，但该领域目前遇到了两个主要挑战：（1）缺乏适应各种财务任务的全面LLM代理框架，以及（2）缺乏标准化基准和一致的数据集来评估代理性能。为了解决这些问题，我们介绍了\ textsc {InvestorBench}，这是第一个专门设计用于评估不同财务决策环境中基于LLM的代理商的基准。 InvestorBench通过提供适用于不同金融产品的全面任务，包括股票，加密货币和交易所交易贸易资金（ETFS），提高了支持LLM的代理商的多功能性。此外，我们使用13种不同的LLM作为骨干模型，在各种市场环境和任务上评估代理框架的推理和决策能力。此外，我们已经策划了多种开源，多模式数据集的收集，并开发了一系列全面的环境来进行财务决策。这建立了一个高度可访问的平台，用于在各种情况下评估金融代理商的绩效。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 83 篇论文

### On Domain-Specific Post-Training for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2411.19930)] [[cool](https://papers.cool/arxiv/2411.19930)] [[pdf](https://arxiv.org/pdf/2411.19930)]
> **Authors**: Daixuan Cheng,Shaohan Huang,Ziyu Zhu,Xintong Zhang,Wayne Xin Zhao,Zhongzhi Luan,Bo Dai,Zhenliang Zhang
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 在针对多模式大语言模型的特定领域的训练中
- **领域**: 计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 近年来，一般多模式大语模型（MLLM）的快速发展。但是，将一般的MLLM适应特定领域（例如科学领域和工业应用）的探索量较少。本文通过训练后，专注于数据综合，培训管道和任务评估，系统地研究了MLLM的领域适应。 （1）数据综合：使用开源模型，我们开发了一个视觉指令合成器，该合成器有效地从域特异性的图像扣对中生成了不同的视觉指导任务。我们的合成任务超过了手动规则，GPT-4和GPT-4V产生的任务，在增强MLLM的域特定性能方面。 （2）训练管道：虽然两阶段的培训（最初是在图像捕获对上，然后是视觉指导任务）通常用于开发一般MLLM，但我们采用了单阶段的训练管道来增强域特异性后培训的任务多样性。 （3）任务评估：我们通过不同来源和尺度的训练后MLLM在两个领域（例如QWEN2-VL-2B，LLAVA-V1.6-8B，LLAMA-3.2-11b）中进行实验，然后在各种区域性特定任务上评估MLLM的性能。为了支持MLLM域适应的进一步研究，我们将开源我们的实施。

### SDR-GNN: Spectral Domain Reconstruction Graph Neural Network for Incomplete Multimodal Learning in Conversational Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2411.19822)] [[cool](https://papers.cool/arxiv/2411.19822)] [[pdf](https://arxiv.org/pdf/2411.19822)]
> **Authors**: Fangze Fu,Wei Ai,Fan Yang,Yuntao Shou,Tao Meng,Keqin Li
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: 17 pages, 8 figures
- **标题**: SDR-GNN：光谱域重建图神经网络，用于在对话情绪识别中不完整的多模式学习
- **领域**: 计算语言学
- **摘要**: 对话中的多模式情感识别（MERC）旨在使用文字，听觉和视觉模态特征对话语情绪进行分类。大多数现有的MERC方法都假设每个话语都具有完整的方式，从而忽略了现实情况下常见的不完整方式问题。最近，图形神经网络（GNNS）在对话中的多模式识别（IMERC）中取得了显着的结果。但是，传统的GNN专注于节点之间的二进制关系，从而限制了它们捕获更复杂，更高级信息的能力。此外，重复的消息传递可能会导致过度平衡，从而降低了其保留基本高频细节的能力。为了解决这些问题，我们提出了一个光谱域重建图神经网络（SDR-GNN），以使对话情感识别中的多模式学习。 SDR-GNN使用基于扬声器和上下文关系的滑动窗口构建了语音语义互动图，以模拟情感依赖性。为了捕获高阶和高频信息，SDR-GNN利用了加权关系聚集，从而确保了跨语音的一致语义特征提取。此外，它在光谱域中执行多频聚集，从而通过提取高频和低频信息来有效恢复不完整的方式。最后，将多头注意力用于保险丝并优化情感识别的功能。在各种现实世界数据集上进行的广泛实验表明，我们的方法在多模式学习中有效，并且优于当前最新方法。

### Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs 
[[arxiv](https://arxiv.org/abs/2411.19187)] [[cool](https://papers.cool/arxiv/2411.19187)] [[pdf](https://arxiv.org/pdf/2411.19187)]
> **Authors**: Anirudh Phukan,Divyansh,Harshit Kumar Morj,Vaishnavi,Apoorv Saxena,Koustava Goswami
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: Accepted to NAACL 2025 Main
- **标题**: 超越logit镜头：在VLMS中进行可靠幻觉检测和接地的上下文嵌入
- **领域**: 计算语言学
- **摘要**: 大型多模型模型（LMM）的快速发展通过利用大语言模型（LLMS）的语言能力并集成了模式特定的编码器，具有显着高级的多模式理解。但是，LMM受到幻觉的困扰，这些幻觉限制了它们的可靠性和采用。尽管传统的检测和减轻这些幻觉的方法通常涉及昂贵的培训或严重依赖外部模型，但使用内部模型功能的最新方法是有希望的选择。在本文中，我们批判性地评估了最先进的无培训技术（Logit Lens）在处理广义视觉幻觉中的局限性。我们介绍了ContextUallens，这是一种精致的方法，它利用LMM中间层的上下文令牌嵌入。这种方法大大改善了各种类别（包括行动和OCR）的幻觉检测和扎根，同时在需要上下文理解的任务上也出色，例如空间关系和属性比较。我们的新型接地技术产生了高度精确的边界框，从而促进了从零拍物分割到接地的视觉问题回答的过渡。我们的贡献为更可靠和可解释的多模式模型铺平了道路。

### ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges 
[[arxiv](https://arxiv.org/abs/2411.18932)] [[cool](https://papers.cool/arxiv/2411.18932)] [[pdf](https://arxiv.org/pdf/2411.18932)]
> **Authors**: Rao Fu,Ziyang Luo,Hongzhan Lin,Zhen Ye,Jing Ma
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: ScratchEval：GPT-4O比我的孩子聪明吗？评估具有视觉编程挑战的大型多模型模型
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）的最新进步展示了令人印象深刻的代码生成功能，主要通过图像对代码基准进行评估。但是，这些基准限于特定的视觉编程方案，在这些方案中，逻辑推理和多模式理解能力分开。为了填补这一空白，我们提出了ScratchEval，这是一种新型的基准测试，旨在评估LMM的视觉编程推理能力。 ScratcheVal基于Scratch，这是一种基于块的视觉编程语言，该语言在儿童编程教育中广泛使用。通过集成视觉元素和嵌入式编程逻辑，ScratchEval要求该模型同时处理视觉信息和代码结构，从而全面评估其编程意图理解能力。我们的评估方法超出了传统的图像对代码映射，并专注于统一的逻辑思维和解决问题的能力，为评估LMM的视觉编程能力提供了一个更全面和具有挑战性的框架。 ScratchEval不仅填补了现有评估方法的空白，而且还为视觉编程领域的LMM的未来开发提供了新的见解。可以在https://github.com/hkbunlp/scratcheval上访问我们的基准。

### Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2412.02104)] [[cool](https://papers.cool/arxiv/2412.02104)] [[pdf](https://arxiv.org/pdf/2412.02104)]
> **Authors**: Yunkai Dang,Kaichen Huang,Jiahao Huo,Yibo Yan,Sirui Huang,Dongrui Liu,Mengxi Gao,Jie Zhang,Chen Qian,Kun Wang,Yong Liu,Jing Shao,Hui Xiong,Xuming Hu
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 可解释且可解释的多模式大语言模型：一项全面调查
- **领域**: 计算语言学
- **摘要**: 人工智能（AI）的快速发展彻底改变了许多领域，大型语言模型（LLM）和计算机视觉（CV）系统分别推动了自然语言理解和视觉处理的进步。这些技术的融合促进了多模式AI的兴起，从而使更丰富的跨模式理解能够跨越文本，视觉，音频和视频方式。尤其是多模式的大语言模型（MLLM）已成为一个强大的框架，在图像文本生成，视觉问题答案和跨模式检索等任务中展示了令人印象深刻的功能。尽管取得了这些进步，但MLLM的复杂性和规模在可解释性和解释性方面带来了重大挑战，对于在高风险应用中建立透明度，可信度和可靠性至关重要。本文提供了有关MLLM的可解释性和解释性的全面调查，提出了一个新颖的框架，该框架将现有研究对三个角度进行分类：（i）数据，（ii）模型，（iii）培训\＆推断。我们系统地分析了从代币级别到嵌入级别表示，评估与建筑分析和设计相关的方法，并探索提高透明度的培训和推理策略。通过比较各种方法，我们确定了它们的优势和局限性，并提出了未来的研究方向，以解决多模式解释性中未解决的挑战。这项调查提供了提高MLLM的可解释性和透明度的基础资源，指导研究人员和从业人员开发更负责任，强大的多模式AI系统。

### Data Uncertainty-Aware Learning for Multimodal Aspect-based Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2412.01249)] [[cool](https://papers.cool/arxiv/2412.01249)] [[pdf](https://arxiv.org/pdf/2412.01249)]
> **Authors**: Hao Yang,Zhenyu Zhang,Yanyan Zhao,Bing Qin
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 基于多模式的情感分析的数据不确定性学习学习
- **领域**: 计算语言学
- **摘要**: 作为一项精细的任务，基于多模式的情感分析（MABSA）主要着重于识别文本图像对中的方面情感信息。但是，我们观察到，很难认识到低质量样本中的各个方面的情感，例如那些倾向于包含噪声的低分辨率图像的情感。在现实世界中，数据的质量通常在不同的样本中有所不同，这种噪声称为数据不确定性。但是，MABSA任务的先前作品以相同的重要性处理不同的质量样本，并忽略了数据不确定性的影响。在本文中，我们提出了一种新型的数据不确定性感知多模式的情感分析方法UA-MABSA，该方法通过数据质量和难度加权了不同样本的丢失。 UA-MABSA采用了一种新颖的质量评估策略，考虑到图像质量和基于方面的跨模式相关性，从而使模型能够更加关注高质量和具有挑战性的样本。广泛的实验表明，我们的方法在Twitter-2015数据集中实现了最新的（SOTA）性能。进一步的分析证明了质量评估策略的有效性。

### Multi-View Incongruity Learning for Multimodal Sarcasm Detection 
[[arxiv](https://arxiv.org/abs/2412.00756)] [[cool](https://papers.cool/arxiv/2412.00756)] [[pdf](https://arxiv.org/pdf/2412.00756)]
> **Authors**: Diandian Guo,Cong Cao,Fangfang Yuan,Yanbing Liu,Guangjie Zeng,Xiaoyan Yu,Hao Peng,Philip S. Yu
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: Accepted to COLING 2025
- **标题**: 多模式讽刺检测的多视图不一致学习
- **领域**: 计算语言学
- **摘要**: 多模式讽刺检测（MSD）对于各种下游任务至关重要。现有的MSD方法倾向于依赖虚假相关性。这些方法通常会错误地将非必需特征的优先级优先考虑，但仍然做出正确的预测，表明训练环境以外的可推广性差。关于这种现象，本文采取了几项倡议。首先，我们确定了导致虚假相关性依赖的两个主要原因。其次，我们通过提出一种新的方法来解决这些挑战，该方法通过对比度学习（MICL）进行多模式讽刺检测来整合多模式不一致。具体而言，我们首先利用不一致性从三个视图中驱动多视图学习：令牌键，实体对象和情感。然后，我们引入了广泛的数据增强，以减轻文本模式的偏见学习。此外，我们构建了一个测试集SPMSD，该测试集包括潜在的伪造相关性，以评估该模型的普遍性。实验结果表明，MICL在基准数据集上具有优势，以及展示了MICL在减轻虚假相关作用方面的进步的分析。

### Dynamic Graph Neural ODE Network for Multi-modal Emotion Recognition in Conversation 
[[arxiv](https://arxiv.org/abs/2412.02935)] [[cool](https://papers.cool/arxiv/2412.02935)] [[pdf](https://arxiv.org/pdf/2412.02935)]
> **Authors**: Yuntao Shou,Tao Meng,Wei Ai,Keqin Li
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: 13 pages, 6 figures
- **标题**: 动态图神经网络在对话中用于多模式情绪识别的动态图神经网络
- **领域**: 计算语言学
- **摘要**: 对话中的多模式情绪识别（MERC）是指通过结合来自多种不同方式（例如音频，图像，文本，视频等）的数据来识别和分类人类情绪状态。大多数现有的多模式情感识别方法都使用GCN来提高性能，但是现有的GCN方法容易过度拟合，无法捕捉说话者情绪的时间依赖。为了解决上述问题，我们提出了MERC的动态图神经常规微分方程网络（DGODE），它结合了情绪的动态变化，以捕获说话者情绪的时间依赖性，并有效地减轻了GCN的过度拟合问题。从技术上讲，DGODE的关键思想是利用自适应Mixhop机制来提高GCN的概括能力，并使用图ODE Evolution网络来表征节点表示的连续动态，并捕获时间依赖性。对两个公开可用的多模式情绪识别数据集进行了广泛的实验表明，与各种基线相比，所提出的DGODE模型的性能优越。此外，提出的DGODE还可以减轻过度平滑的问题，从而实现深入GCN网络的构建。

### Acquired TASTE: Multimodal Stance Detection with Textual and Structural Embeddings 
[[arxiv](https://arxiv.org/abs/2412.03681)] [[cool](https://papers.cool/arxiv/2412.03681)] [[pdf](https://arxiv.org/pdf/2412.03681)]
> **Authors**: Guy Barel,Oren Tsur,Dan Vilenchik
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: COLING 2025
- **标题**: 获得的口味：具有文本和结构嵌入的多模式姿势检测
- **领域**: 计算语言学
- **摘要**: 立场检测在实现广泛的下游应用程序中起着关键作用，从解释到追踪假新闻的传播和拒绝科学事实。尽管大多数立场分类模型都依赖于所讨论的话语的文本表示，但先前的工作表明了对话环境在立场检测中的重要性。在这项工作中，我们介绍了品味 - 一种用于立场检测的多模式结构，它和谐地将基于变形金刚的内容嵌入无监督的结构嵌入。通过对经过预告片的变压器的微调以及通过门控剩余网络（GRN）层与社会嵌入的合并，我们的模型擅长捕获确定姿态时内容与对话结构之间的复杂相互作用。味道在常见的基准测试基准上取得了最新的结果，大大优于一系列强大的基准。比较评估强调了社会基础的好处 - 强调同时利用内容和结构以增强立场检测的关键。

### Multimodal Sentiment Analysis Based on BERT and ResNet 
[[arxiv](https://arxiv.org/abs/2412.03625)] [[cool](https://papers.cool/arxiv/2412.03625)] [[pdf](https://arxiv.org/pdf/2412.03625)]
> **Authors**: JiaLe Ren
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 基于BERT和RESNET的多模式情感分析
- **领域**: 计算语言学
- **摘要**: 随着互联网和社交媒体的快速发展，在情感分析任务中，多模式数据（文本和图像）越来越重要。但是，现有方法难以有效地融合文本和图像特征，从而限制了分析的准确性。为了解决这个问题，提出了将BERT和RESNET结合的多模式情感分析框架。 Bert在自然语言处理中表现出强大的文本表示能力，Resnet在计算机视觉领域具有出色的图像提取性能。首先，BERT用于提取文本特征向量，并使用Resnet用于提取图像特征表示。然后，探索了各种特征融合策略，最后选择基于注意机制的融合模型以充分利用文本和图像之间的互补信息。公共数据集Mava-single上的实验结果表明，与仅使用BERT或RESNET的单模式模型相比，所提出的多模式模型提高了准确性和F1分数，达到74.5％的最佳准确度。这项研究不仅为多模式情感分析提供了新的思想和方法，而且还证明了BERT和RESNET在跨域融合中的应用潜力。将来，将探索更先进的功能融合技术和优化策略，以进一步提高多模式情感分析的准确性和泛化能力。

### U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs 
[[arxiv](https://arxiv.org/abs/2412.03205)] [[cool](https://papers.cool/arxiv/2412.03205)] [[pdf](https://arxiv.org/pdf/2412.03205)]
> **Authors**: Konstantin Chernyshev,Vitaliy Polshkov,Ekaterina Artemova,Alex Myasnikov,Vlad Stepanov,Alexei Miasnikov,Sergei Tilga
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: U-MATH：用于评估LLMS数学技能的大学级别基准
- **领域**: 计算语言学,人工智能
- **摘要**: 当前对LLM中数学技能的评估是有限的，因为现有的基准要么相对较小，主要集中在小学和高中问题上，要么缺乏主题的多样性。此外，任务中的视觉元素的包含在很大程度上还不足。为了解决这些差距，我们介绍了U-Math，这是1,100个未出版的开放式大学水平问题的新基准，这些问题来自教材。它在六个核心受试者中保持平衡，其中20％的多模式问题。鉴于U-Math问题的开放性质，我们采用LLM来判断生成的解决方案的正确性。为此，我们发布了$μ$ -MATH，这是一个数据集，以评估LLMS在判断解决方案中的功能。对通用域，特定于数学和多模式LLM的评估突出了U-MATH所面临的挑战。我们的发现表明，LLM在基于文本的任务上仅达到63％的最大准确性，而视觉问题的最高准确性甚至降低了45％。解决方案评估证明了LLMS具有挑战性，最佳LLM法官的F1得分为80％，$μ$ $  -  $ $。

### ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error Correction 
[[arxiv](https://arxiv.org/abs/2412.03075)] [[cool](https://papers.cool/arxiv/2412.03075)] [[pdf](https://arxiv.org/pdf/2412.03075)]
> **Authors**: Victor Junqiu Wei,Weicheng Wang,Di Jiang,Yuanfeng Song,Lu Wang
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: ASR-EC基准：评估中文ASR错误校正上的大型语言模型
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 自动语音识别（ASR）是语音和自然语言处理领域的基本和重要任务。在许多应用程序（例如语音助手，语音翻译等）中，它是一个固有的构建块。尽管近年来ASR技术的发展，现代ASR系统仍然不可避免地要由于环境噪音，模棱两可，歧义等，因此，ASR的错误校正是至关重要的。由此激励，本文研究了中文的ASR误差校正，这是最受欢迎的语言之一，并享受了世界上大量用户。我们首先创建一个名为\ emph {asr-ec}的基准数据集，其中包含由行业级ASR系统生成的广泛的ASR错误。据我们所知，这是第一个中国ASR误差校正基准。然后，受到\ emph {大语言模型（llms）}的最新进展的启发，我们研究了如何利用LLMS纠正ASR错误的力量。我们将LLMS应用于三个范式中的ASR误差校正。第一个范式正在提示，该范式进一步归类为零射击，很少射击和多步。第二个范式是FineTuning，它具有ASR误差校正数据的FINETUNES LLMS。第三个范式是多模式增强，该增强量集体利用音频和ASR转录本进行误差校正。广泛的实验表明，提示对于ASR误差校正无效。 Finetuning仅对LLM的一部分有效。多模式增强是误差校正和实现最新性能的最有效方法。

### Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction 
[[arxiv](https://arxiv.org/abs/2412.04454)] [[cool](https://papers.cool/arxiv/2412.04454)] [[pdf](https://arxiv.org/pdf/2412.04454)]
> **Authors**: Yiheng Xu,Zekun Wang,Junli Wang,Dunjie Lu,Tianbao Xie,Amrita Saha,Doyen Sahoo,Tao Yu,Caiming Xiong
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: https://aguvis-project.github.io/
- **标题**: Aguvis：自主GUI相互作用的统一纯视觉剂
- **领域**: 计算语言学
- **摘要**: 图形用户界面（GUI）对人类计算机的交互至关重要，但是由于视觉环境的复杂性和可变性，自动化的GUI任务仍然具有挑战性。现有的方法通常依赖于GUI的文本表示，这些表示引入了概括，效率和可扩展性的局限性。在本文中，我们介绍了Aguvis，这是一个统一的基于纯视觉的框架，用于在各个平台上运行的自主GUI代理。我们的方法利用基于图像的观察结果，并以自然语言的基础指示来视觉元素，并采用一致的动作空间来确保跨平台的概括。为了解决以前工作的局限性，我们将明确的计划和推理整合在模型中，增强其自动导航和与复杂数字环境进行交互的能力。我们构建了GUI代理轨迹的大规模数据集，结合了多模式的推理和接地，并采用了两阶段的培训管道，该管道首先着重于一般的GUI接地，然后进行计划和推理。通过全面的实验，我们证明了Aguvis超过了离线和现实世界中的最新方法，据我们所知，这是第一个完全自主的纯Vision GUI代理，能够独立执行而无需与外部封闭代码模型进行协作。我们开源所有数据集，模型和培训食谱，以促进未来的研究，请访问https://aguvis-project.github.io/。

### M$^{3}$D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level Information Extraction 
[[arxiv](https://arxiv.org/abs/2412.04026)] [[cool](https://papers.cool/arxiv/2412.04026)] [[pdf](https://arxiv.org/pdf/2412.04026)]
> **Authors**: Jiang Liu,Bobo Li,Xinran Yang,Na Yang,Hao Fei,Mingyao Zhang,Fei Li,Donghong Ji
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: 14 pages, 9 figures, 6 tables
- **标题**: M $^{3} $ D：用于接地文档级信息提取的多模式，多语言和多任务数据集
- **领域**: 计算语言学
- **摘要**: 多模式信息提取（IE）任务吸引了越来越多的关注，因为许多研究表明，多模式信息受益于文本信息提取。但是，现有的多模式IE数据集主要关注英文文本中的句子级图像效果IE，几乎不关注基于视频的多模式IE和细粒度的视觉接地。因此，为了促进多模式IE的开发，我们构建了一个多语言多语言多语言数据集，名为M $^{3} $ D，具有以下功能：（1）它包含配对的文档级文本和视频，以丰富多模式信息； （2）它支持两种广泛使用的语言，即英语和中文； （3）它包括更多的多模式IE任务，例如实体识别，实体链提取，关系提取和视觉接地。此外，我们的数据集引入了一个未开发的主题，即传记，丰富了多模式IE资源的域。为了为我们的数据集建立基准，我们提出了创新的层次多模式IE模型。该模型有效地利用并通过DeNO的特征融合模块（DFFM）整合了多模式信息。此外，在非理想的情况下，模态信息通常不完整。因此，我们设计了一个缺失的模态构建模块（MMCM），以减轻缺失方式引起的问题。我们的模型分别在英语和中文数据集中的四个任务上达到了53.80％和53.77％的平均性能，这为随后的研究设定了合理的标准。此外，我们进行了更多的分析实验，以验证我们提出的模块的有效性。我们认为，我们的工作可以促进多模式IE领域的发展。

### MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model 
[[arxiv](https://arxiv.org/abs/2412.03930)] [[cool](https://papers.cool/arxiv/2412.03930)] [[pdf](https://arxiv.org/pdf/2412.03930)]
> **Authors**: Yunhe Pang,Bo Chen,Fanjin Zhang,Yanghui Rao,Jie Tang
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 心理：通过多模式结构增强语言模型的有效错误分配检测
- **领域**: 计算语言学,人工智能
- **摘要**: 学术出版物的快速增长加剧了在线数字图书馆中作者名称歧义的问题。尽管名称取得了不利的歧视算法，但累积错误仍在破坏学术系统的可靠性。据估计，在构建百万个规模的Whoiswho Who基准时，将纠正10％以上的纸质作者作业。现有的努力来检测错误的作业是基于语义的方法或基于图的方法，这些方法缺乏充分利用论文的丰富文本属性以及通过纸张属性共发生定义的隐式结构特征。为此，本文介绍了一个结构增强的语言模型，该模型将基于图的方法的关键结构特征与从丰富的纸张属性中的细颗粒语义特征结合在一起，以检测不正确的作业。提出的模型经过高效的多模式多态指令调谐框架训练，该框架结合了任务指导的指令调整，文本属性模式和结构模式。实验结果表明，我们的模型表现优于先前的方法，在KDD CUP 2024的排行榜上取得了最高的表现。我们的代码已公开可用。

### A Self-Learning Multimodal Approach for Fake News Detection 
[[arxiv](https://arxiv.org/abs/2412.05843)] [[cool](https://papers.cool/arxiv/2412.05843)] [[pdf](https://arxiv.org/pdf/2412.05843)]
> **Authors**: Hao Chen,Hui Guo,Baochen Hu,Shu Hu,Jinrong Hu,Siwei Lyu,Xi Wu,Xin Wang
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 虚假新闻检测的自学多模式方法
- **领域**: 计算语言学,机器学习
- **摘要**: 社交媒体的快速增长导致在线新闻内容爆炸，导致误导或虚假信息的传播显着增加。尽管已经广泛应用机器学习技术来检测虚假新闻，但标记的数据集的稀缺仍然是一个至关重要的挑战。错误信息经常以配对的文本和图像出现，其中新闻文章或标题伴随着相关的视觉效果。在本文中，我们介绍了一种用于假新闻分类的自学多模式模型。该模型利用对比度学习，这是一种强大的特征提取方法，它在不需要标记的数据的情况下运行，并集成了大语言模型（LLMS）的优势以共同分析文本和图像特征。 LLM在这项任务中表现出色，因为它们能够处理从广泛的培训语料库中获取多种语言数据的能力。我们在公共数据集上的实验结果表明，所提出的模型的表现优于几种最先进的分类方法，实现了超过85％的精度，精度，召回和F1得分。这些发现突出了该模型在应对多模式假新闻检测挑战方面的有效性。

### An Entailment Tree Generation Approach for Multimodal Multi-Hop Question Answering with Mixture-of-Experts and Iterative Feedback Mechanism 
[[arxiv](https://arxiv.org/abs/2412.05821)] [[cool](https://papers.cool/arxiv/2412.05821)] [[pdf](https://arxiv.org/pdf/2412.05821)]
> **Authors**: Qing Zhang,Haocheng Lv,Jie Liu,Zhiyun Chen,Jianyong Duan,Hao Wang,Li He,Mingying Xv
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: Erratum: We identified an error in the calculation of the F1 score in table 4 reported in a previous version of this work. The performance of the new result is better than the previous one. The corrected values are included in this updated version of the paper. These changes do not alter the primary conclusions of our research
- **标题**: 多式联运多跳的问题的需要，用杂物和迭代反馈机制回答
- **领域**: 计算语言学,人工智能
- **摘要**: 随着大规模语言模型（LLM）的兴起，将多模式信息转换为文本说明以进行多模式多模型的答案，它目前是流行且有效的。但是，我们认为，当前的多模式多跳跃问题的方法回答仍然主要面临两个挑战：1）所检索的包含大量冗余信息的证据，不可避免地导致由于无关的信息误导了预测，因此绩效显着下降。 2）没有可解释的推理步骤的推理过程使该模型难以发现处理复杂问题的逻辑错误。为了解决这些问题，我们提出了一种基于LLMS的方法，但由于LLM的潜在错误而没有严重依赖它们，并创新地将多模式的多跳上问题回答作为共同的需要树的生成和问题答案问题。具体来说，我们设计了一个多任务学习框架，重点是促进跨解释性和预测任务共享共享的共享，同时防止特定于任务的错误通过专家的混合而彼此干扰。之后，我们设计了一种迭代反馈机制，以通过向LLM的联合培训的结果喂养再生占用树木，以进一步增强这两项任务，以迭代地改善潜在的答案。值得注意的是，我们的方法赢得了WebQA官方排行榜的第一名（自2024年4月10日起），并在Multimodalqa上取得了竞争成果。

### Speech Is Not Enough: Interpreting Nonverbal Indicators of Common Knowledge and Engagement 
[[arxiv](https://arxiv.org/abs/2412.05797)] [[cool](https://papers.cool/arxiv/2412.05797)] [[pdf](https://arxiv.org/pdf/2412.05797)]
> **Authors**: Derek Palmer,Yifan Zhu,Kenneth Lai,Hannah VanderHoeven,Mariah Bradford,Ibrahim Khebour,Carlos Mabrey,Jack Fitzgerald,Nikhil Krishnaswamy,Martha Palmer,James Pustejovsky
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-09
> **comment**: 3 pages, 2 figures, appearing at AAAI 2025 Demos Track
- **标题**: 语音还不够：解释常识和参与的非语言指标
- **领域**: 计算语言学,人工智能
- **摘要**: 我们的目标是建立一个可以为小组解决问题和社会动态提供支持的AI合作伙伴。在多方工作组环境中，多模式分析对于识别小组成员的非语言相互作用至关重要。结合他们的口头参与，这为协作和参与提供了整体理解，为AI合作伙伴提供了必要的背景。在此演示中，我们说明了我们目前在课堂上以学生任务为导向的互动中检测和跟踪非语言行为的能力，以及跟踪共同基础和参与度的影响。

### Text Is Not All You Need: Multimodal Prompting Helps LLMs Understand Humor 
[[arxiv](https://arxiv.org/abs/2412.05315)] [[cool](https://papers.cool/arxiv/2412.05315)] [[pdf](https://arxiv.org/pdf/2412.05315)]
> **Authors**: Ashwin Baluja
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 文字不是您所需要的：多模式提示有助于LLMS了解幽默
- **领域**: 计算语言学,计算机与社会
- **摘要**: 尽管大型语言模型（LLM）表现出了各种基于文本的任务的令人印象深刻的自然语言理解能力，但理解幽默仍然是一个持续的挑战。幽默经常是多模式的，依靠语音歧义，节奏和时间来传达意义。在这项研究中，我们探讨了一种简单的多模式提示方法，以幽默理解和解释。我们介绍了一个llm，其中既有笑话的文字和口语，又是使用现成的文本到语音（TTS）系统生成的。与所有经过测试的数据集中的文本提示相比，使用多模式提示可以改善幽默的解释。

### MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale 
[[arxiv](https://arxiv.org/abs/2412.05237)] [[cool](https://papers.cool/arxiv/2412.05237)] [[pdf](https://arxiv.org/pdf/2412.05237)]
> **Authors**: Jarvis Guo,Tuney Zheng,Yuelin Bai,Bo Li,Yubo Wang,King Zhu,Yizhi Li,Graham Neubig,Wenhu Chen,Xiang Yue
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: Mammoth-VL：通过大规模调整指令调整的多模式推理
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 开源多模式大型语言模型（MLLM）在广泛的多模式任务中显示出巨大的潜力。但是，他们的推理功能仍受到现有指导数据集的限制，这些数据集主要是从诸如VQA，AI2D和ChartQA等学术数据集中重新利用的。这些数据集针对简单的任务，仅提供短语级别的答案，而无需任何中间原理。为了应对这些挑战，我们引入了一种可扩展且具有成本效益的方法，用于构建一个大规模的多模式指令调节数据集，其中具有丰富的中间原理，旨在引发COT推理。我们仅使用开放模型，创建一个包含12M指令 - 响应对的数据集，以涵盖具有详细和忠实理由的多样化，推理密集型任务。实验表明，该数据集上的训练MLLM显着提高了推理能力，从而在基准（例如Mathverse（+8.1％），MMMU-PRO（+7％）和Muirbench（+13.3％）等基准上实现了最先进的性能。此外，该模型在基于非争议的基准测试中表现出高达4％的显着改善。消融研究进一步强调了关键组件在数据集构造过程中的重要性，例如重写和自我滤清。

### Multimodal Fact-Checking with Vision Language Models: A Probing Classifier based Solution with Embedding Strategies 
[[arxiv](https://arxiv.org/abs/2412.05155)] [[cool](https://papers.cool/arxiv/2412.05155)] [[pdf](https://arxiv.org/pdf/2412.05155)]
> **Authors**: Recep Firat Cekinel,Pinar Karagoz,Cagri Coltekin
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: Accepted to COLING2025
- **标题**: 通过视觉语言模型进行多模式的事实检查：一种基于分类器的解决方案，具有嵌入策略
- **领域**: 计算语言学
- **摘要**: 这项研究评估了视觉语言模型（VLM）在表示和利用多模式内容进行事实检查方面的有效性。更具体地说，我们研究合并多模式内容是否与仅文本模型相比是否可以提高性能，以及VLMS如何利用文本和图像信息来增强错误信息检测。此外，我们建议使用VLMS基于探测器的解决方案。我们的方法从所选VLM的最后一个隐藏层中提取嵌入，并将它们输入到神经探测分类器中，以进行多类真实性分类。通过对两个事实检查数据集进行的一系列实验，我们证明，尽管多模态可以增强性能，而与使用VLM嵌入相比，与文本和图像编码器的单独嵌入融合得出了较高的结果。此外，提出的神经分类器在利用提取的嵌入方面显着超过了KNN和SVM基准，这突出了其用于多模式事实检查的有效性。

### Findings of the Second BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora 
[[arxiv](https://arxiv.org/abs/2412.05149)] [[cool](https://papers.cool/arxiv/2412.05149)] [[pdf](https://arxiv.org/pdf/2412.05149)]
> **Authors**: Michael Y. Hu,Aaron Mueller,Candace Ross,Adina Williams,Tal Linzen,Chengxu Zhuang,Ryan Cotterell,Leshem Choshen,Alex Warstadt,Ethan Gotlieb Wilcox
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 第二个Babylm挑战的结果：对开发合理的情况进行样本预估计
- **领域**: 计算语言学
- **摘要**: Babylm挑战是社区努力，以缩小人类和计算语言学习者之间的数据效率差距。参与者竞争以1亿字的固定语言数据预算优化语言模型培训。今年，我们发布了改进的文本语料库，以及视觉和语言语料库，以促进对认知上合理的视觉语言模型的研究。比较了针对语法能力，（视觉）答案，实用能力和接地等语法能力的评估任务的提交。参与者可以提交100万文字曲目，100m字的仅文本曲目和/或100m字和图像多模式轨道。从31种采用各种方法的提交中，混合因果掩盖的语言模型架构的表现优于其他方法。没有提交的表现优于多模式轨道中的基线。在后续分析中，我们发现训练失败与跨任务的平均表现之间存在牢固的关系，并且表现最佳的提议对培训数据，培训目标和模型体系结构的更改提出了更改。今年的Babylm挑战表明，在这种情况下，尤其是图像文本建模，仍有很大的创新空间，但是社区驱动的研究可以产生有关小型语言建模有效策略的可行见解。

### FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering 
[[arxiv](https://arxiv.org/abs/2412.07030)] [[cool](https://papers.cool/arxiv/2412.07030)] [[pdf](https://arxiv.org/pdf/2412.07030)]
> **Authors**: Amirhossein Abaskohi,Spandana Gella,Giuseppe Carenini,Issam H. Laradji
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: 20 pages, 11 figures, 10 tables
- **标题**: FM2DS：很少射击的多模式多台化数据综合，知识蒸馏的问题回答
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,信息检索,机器学习
- **摘要**: 多模式的多台面问题回答是一项复杂的任务，需要在多个信息源（例如图像和文本）上进行推理才能回答问题。尽管视觉问题的回答取得了重大进展，但由于缺乏高质量的数据集，多跃波设置仍未开发。当前的方法着眼于单跳问题答案或单一模式，这使它们不适合现实情况，例如分析多模式教育材料，总结冗长的学术文章或解释结合图表，图像和文本的科学研究。为了解决这一差距，我们提出了一种新颖的方法，引入了创建高质量数据集的第一个框架，该框架可以为多模式多ihop问题回答培训模型。我们的方法包括一个5阶段的管道，涉及从Wikipedia获取相关的多模式文档，合成产生高级问题和答案，并通过严格的标准验证它们以确保优质数据。我们通过对合成数据集的培训模型评估我们的方法论，并在两个基准上进行测试，我们的结果表明，凭借相等的样本量，对我们的合成数据进行训练的模型优于那些在平均匹配人类收集的数据的培训的模型以1.9的培训（EM）。我们认为，我们的数据综合方法将成为培训和评估多模式多台面问题答案模型的强大基础。

### OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions 
[[arxiv](https://arxiv.org/abs/2412.06693)] [[cool](https://papers.cool/arxiv/2412.06693)] [[pdf](https://arxiv.org/pdf/2412.06693)]
> **Authors**: Yi-Kai Zhang,Xu-Xiang Zhong,Shiyin Lu,Qing-Guo Chen,De-Chuan Zhan,Han-Jia Ye
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: Omnievalkit：用于评估大语言模型及其Omni-Extensions的模块化，轻巧的工具箱
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习,多媒体
- **摘要**: 大语言模型（LLM）的快速进步已大大扩展了其应用程序，从多语言支持到特定领域的任务和多模式集成。在本文中，我们提出了Omnievalkit，这是一种新型的基准测试工具箱，旨在评估LLMS及其在多语言，多域和多模式功能中的Omni-Extensions。与通常关注单个方面的现有基准不同，Omnievalkit提供了模块化，轻巧且自动化的评估系统。它由一个模块化体系结构结构，其中包括一个静态构建器和动态数据流，从而促进了新模型和数据集的无缝集成。 Omnievalkit支持超过100个LLM和50个评估数据集，涵盖了数千种模型数据组合组合的全面评估。 Omnievalkit致力于创建超轻质且可快速的评估框架，从而使下游应用程序对AI社区更加方便和多功能。

### Optimizing Multi-Task Learning for Enhanced Performance in Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.06249)] [[cool](https://papers.cool/arxiv/2412.06249)] [[pdf](https://arxiv.org/pdf/2412.06249)]
> **Authors**: Zhen Qi,Jiajing Chen,Shuo Wang,Bingying Liu,Hongye Zheng,Chihang Wang
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 优化多任务学习以提高大语模型的性能
- **领域**: 计算语言学,机器学习
- **摘要**: 这项研究旨在在多任务学习框架下基于GPT-4探索大语言模型的性能改进方法，并对两个任务进行实验：文本分类和自动摘要生成。通过共享特征提取器和特定任务模块的组合设计，我们在同一模型中实现了多个任务的知识共享和优化。该实验使用胶水数据集的多个子任务来将多任务模型的性能与单任务GPT-4，GPT-3的多任务版本，BERT BASIC模型和经典BI-LSTM与注意模型进行比较。结果表明，所提出的多任务学习模型在文本分类精度和汇总生成的胭脂值方面优于其他比较模型，这证明了多任务学习在改善模型概括能力和任务之间的协作学习方面的优势。该模型在训练过程中保持稳定的损失收敛率，显示出良好的学习效率和对测试集的适应性。这项研究验证了大语言模型中多任务学习框架的适用性，尤其是在提高模型平衡不同任务的能力方面。将来，通过大型语言模型和多模式数据的结合以及动态任务调整技术的应用，基于多任务学习的框架有望在跨领域的实用应用中发挥更大的作用，并为一般人工智能的发展提供新的想法。

### M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified Sentiment and Emotion Analysis 
[[arxiv](https://arxiv.org/abs/2412.08049)] [[cool](https://papers.cool/arxiv/2412.08049)] [[pdf](https://arxiv.org/pdf/2412.08049)]
> **Authors**: Ao Li,Longwei Xu,Chen Ling,Jinghui Zhang,Pengwei Wang
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: M2SE：用于统一情感和情感分析的多阶段多任务指令调整策略
- **领域**: 计算语言学
- **摘要**: 情绪分析和情绪识别对于诸如人类计算机相互作用和抑郁症检测等应用至关重要。传统的单峰方法通常无法捕获由于来自不同模式的信号冲突而引起的情绪表达的复杂性。当前的多模式大型语言模型（MLLM）在检测微妙的面部表情并解决与情绪相关的任务方面还面临挑战。为了解决这些问题，我们提出了M2SE，这是通用MLLM的多式多任务情感和情感教学调整策略。它采用一种组合方法来培训模型，例如多模式情感分析，情感识别，面部表情识别，情感原因推断和情感原因提取。我们还介绍了情感多任务数据集（EMT），这是一个支持这五个任务的自定义数据集。我们的模型情感宇宙（EMOVERSE）建立在基本的MLLM框架上而没有修改的情况下，但是在接受M2SE策略培训时，它可以在这些任务上取得重大改进。广泛的实验表明，Emoverse优于现有方法，实现最先进的方法会导致情绪和情感任务。这些结果突出了M2SE在增强多模式情感感知方面的有效性。该数据集和代码可在https://github.com/xiaoyaoxinyi/m2se上找到。

### DRUM: Learning Demonstration Retriever for Large MUlti-modal Models 
[[arxiv](https://arxiv.org/abs/2412.07619)] [[cool](https://papers.cool/arxiv/2412.07619)] [[pdf](https://arxiv.org/pdf/2412.07619)]
> **Authors**: Ellen Yi-Ge,Jiechao Gao,Wei Han,Wei Zhu
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 鼓：大型多模式模型的学习演示猎犬
- **领域**: 计算语言学
- **摘要**: 最近，大型语言模型（LLMS）在借助文化学习（ICL）来处理新任务方面表现出了令人印象深刻的能力。在实施ICL的大型视觉模型（LVLM）的研究中，研究人员通常会采用诸如跨不同样本的固定示范，或通过视觉语言嵌入模型直接选择演示的固定策略。这些方法不能保证配置的演示符合LVLM的需求。为了解决这个问题，我们现在提出了一个新颖的框架，\下划线{d}启发\下划线{r} eTriever，用于大M \下划线{u} lti-modal \ underline {m} odel（m} odel（drum），tuns tunes the-tunes thunes fiper-language嵌入模型可以更好地满足LVLM的需求。首先，假设给出了嵌入模型，我们讨论了视觉任务的检索策略。我们建议阐述图像和文本嵌入，以增强检索性能。其次，我们建议通过LVLM的反馈重新将嵌入模型检索到的演示重新排列，并计算训练嵌入模型的列表排名损失。第三，我们提出了一种迭代演示挖掘策略，以改善嵌入模型的训练。通过对3种视觉语言任务（7个基准数据集）进行的大量实验，我们的鼓框架被证明可以有效地通过检索更正确的演示来提高LVLM的context学习表现。

### jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images 
[[arxiv](https://arxiv.org/abs/2412.08802)] [[cool](https://papers.cool/arxiv/2412.08802)] [[pdf](https://arxiv.org/pdf/2412.08802)]
> **Authors**: Andreas Koukounas,Georgios Mastrapas,Bo Wang,Mohammad Kalim Akram,Sedigheh Eslami,Michael Günther,Isabelle Mohr,Saba Sturua,Scott Martens,Nan Wang,Han Xiao
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: 21 pages, 1-10 main paper, 10-12 refs, 12-21 benchmarks
- **标题**: Jina-Clip-V2：文本和图像的多语言多模式嵌入
- **领域**: 计算语言学,计算机视觉和模式识别,信息检索
- **摘要**: 对比性语言图像预读（剪辑）是在共享嵌入空间中对齐图像和文本的高效方法。这些模型被广泛用于诸如跨模式信息检索和多模式理解之类的任务。但是，与专门的文本模型相比，剪辑模型通常在仅有的文本任务方面困难，表现不佳。这种性能差异迫使检索系统依靠单独的模型来完成仅文本和多模式任务。在这项工作中，我们在以前的模型Jina-Clip-v1的基础上介绍了一个精致的框架，该框架利用了多种语言的多任务，多阶段的对比度学习，再加上改进的培训配方以增强单纯的检索。最终的模型Jina-Clip-V2在仅文本和多模式任务上优于其前身，同时增加了多语言支持，对复杂的视觉文档的更好理解和效率提高得益于Matryoshka表示和矢量截断。该模型在多语言媒介和多语言文本检索基准中都与最新的最新表现相当，这是解决统一仅文本和多模式检索系统的挑战。

### Multimodal Latent Language Modeling with Next-Token Diffusion 
[[arxiv](https://arxiv.org/abs/2412.08635)] [[cool](https://papers.cool/arxiv/2412.08635)] [[pdf](https://arxiv.org/pdf/2412.08635)]
> **Authors**: Yutao Sun,Hangbo Bao,Wenhui Wang,Zhiliang Peng,Li Dong,Shaohan Huang,Jianyong Wang,Furu Wei
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 多模式潜在语言建模与下一步扩散
- **领域**: 计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 多模式生成模型需要一种统一的方法来处理离散数据（例如文本和代码）和连续数据（例如，图像，音频，视频）。在这项工作中，我们提出了潜在语言建模（LatentLM），该模型无缝地使用因果变压器集成了连续和离散的数据。具体而言，我们采用各种自动编码器（VAE）来表示连续数据作为潜在向量，并引入了下一步扩散这些向量的自动回归产生。此外，我们开发了$σ$ -VAE来应对方差崩溃的挑战，这对于自回归建模至关重要。广泛的实验证明了潜伏在各种方式上的有效性。在图像生成中，UtentLM在性能和可扩展性方面都超过了扩散变压器。当整合到多模式大语言模型中时，LatentLM提供了一个通用的界面，可以统一多模式的生成和理解。实验结果表明，在扩大训练令牌的情况下，LatentLM与输血和量化量化模型相比，取得了有利的性能。在文本到语音的综合中，LatentLM在说话者的相似性和鲁棒性中优于最先进的VALL-E 2模型，同时需要减少10倍的解码步骤。结果将LetentLM作为一种高效且可扩展的方法，以推动大型多模型。

### TECO: Improving Multimodal Intent Recognition with Text Enhancement through Commonsense Knowledge Extraction 
[[arxiv](https://arxiv.org/abs/2412.08529)] [[cool](https://papers.cool/arxiv/2412.08529)] [[pdf](https://arxiv.org/pdf/2412.08529)]
> **Authors**: Quynh-Mai Thi Nguyen,Lan-Nhi Thi Nguyen,Cam-Van Thi Nguyen
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: Accepted at PACLIC 2024
- **标题**: TECO：通过常识性知识提取，通过提高文本增强的多模式意图识别
- **领域**: 计算语言学
- **摘要**: 多模式意图识别（MIR）的目的是利用各种模式，例如文本，视频和音频来检测用户意图，这对于了解对话系统中的人类语言和上下文至关重要。尽管该领域的进展，但两个主要挑战仍然存在：（1）有效地从鲁棒的文本特征中提取和利用语义信息； （2）有效地对齐和融合非语言方式与口头形式。本文提出了具有常识性知识提取器（TECO）的文本增强，以应对这些挑战。我们首先从生成和检索的知识中提取关系，以丰富文本模式中的上下文信息。随后，我们将视觉和声学表示与这些增强的文本特征结合并整合，以形成凝聚力的多模式表示。我们的实验结果表明，对现有基线方法的重大改进。

### MERaLiON-AudioLLM: Bridging Audio and Language with Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.09818)] [[cool](https://papers.cool/arxiv/2412.09818)] [[pdf](https://arxiv.org/pdf/2412.09818)]
> **Authors**: Yingxu He,Zhuohan Liu,Shuo Sun,Bin Wang,Wenyu Zhang,Xunlong Zou,Nancy F. Chen,Ai Ti Aw
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: https://huggingface.co/MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION
- **标题**: Meralion-audio llm：与大语言模型一起桥接音频和语言
- **领域**: 计算语言学,人工智能
- **摘要**: 我们介绍了Meralion-audiollm（一个网络中的多模式同理学推理和学习），这是针对新加坡的多语言和多元文化景观量身定制的第一个语音介绍模型。 Meralion-Audiollm根据新加坡的国家大型语言模型资金计划开发，集成了先进的语音和文本处理，以解决当地口音和方言的各种语言细微差别，从而在复杂的多语言环境中增强了可访问性和可访问性和可用性。我们的结果表明，语音识别和特定于任务的理解方面的改善，将Meralion-Audiollm定位为针对特定地区AI应用程序的开创性解决方案。我们设想此版本为未来模型设定了旨在解决全球框架中本地化语言和文化背景的先例。

### Semi-IIN: Semi-supervised Intra-inter modal Interaction Learning Network for Multimodal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2412.09784)] [[cool](https://papers.cool/arxiv/2412.09784)] [[pdf](https://arxiv.org/pdf/2412.09784)]
> **Authors**: Jinhao Lin,Yifei Wang,Yanwu Xu,Qi Liu
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: 半iin：半监督内部内部模态互动学习网络，用于多模式情感分析
- **领域**: 计算语言学,人工智能
- **摘要**: 尽管多模式的情感分析是一个值得进一步研究的肥沃研究基础，但当前的方法占据了高注释成本，并受到歧义的标签歧义，而不是高质量标记的数据获取。此外，选择正确的相互作用是必不可少的，因为在各种样本之间，内模互动或模式间相互作用的重要性可能会有所不同。为此，我们提出了半iin，这是一种半监督的内部内部模态相互作用学习网络，用于多模式情感分析。半IIN整合了掩盖的注意力和门控机制，在独立捕获内部和模式间互动信息后，可以有效的动态选择。结合自训练方法，Semi-Iin充分利用了从未标记的数据中汲取的知识。在两个公共数据集（Mosi和Mosei）上的实验结果证明了半IIN的有效性，建立了几个指标的新最新技术。代码可在https://github.com/flow-ljh/semi-iin上找到。

### AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials 
[[arxiv](https://arxiv.org/abs/2412.09605)] [[cool](https://papers.cool/arxiv/2412.09605)] [[pdf](https://arxiv.org/pdf/2412.09605)]
> **Authors**: Yiheng Xu,Dunjie Lu,Zhennan Shen,Junli Wang,Zekun Wang,Yuchen Mao,Caiming Xiong,Tao Yu
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: ICLR2025 Spotlight https://agenttrek.github.io
- **标题**: AgentTrek：通过Web教程引导重播的代理轨迹综合
- **领域**: 计算语言学
- **摘要**: 图形用户界面（GUI）代理可以在数字环境中自动化复杂的任务，但是由于培训高质量的轨迹数据缺乏，它们的开发受到阻碍。现有的方法依赖于昂贵的人类注释，使其大规模不可持续。我们提出了AgentTrek，这是一种可扩展的数据综合管道，该管道通过利用公开可用的教程来生成Web代理轨迹。我们的三阶段方法：（1）使用专门的分类模型自动从Internet收获和过滤教程的文本，（2）将这些文本转换为带有分步说明的结构化任务规范，（3）采用视觉语言模型（VLM）代理在实际环境中执行这些指令，同时VLM基于VLM基于评估的评估轨迹。合成的轨迹包含多种模态，包括具有功能称呼API动作的基于文本的HTML观察以及具有像素级操作的基于视觉的屏幕截图观察。这种多模式数据富含经过思考的推理，使代理可以在文本Web浏览基准（例如Webarena）和Visual Web接地和浏览基准测试（例如ScreenSpot Web和MultoPodionMind2Web）上实现最先进的性能。此外，我们完全自动化的方法大大降低了数据收集成本，而没有人类注释者，每次高质量轨迹的成本仅为0.55美元。我们的工作表明，使用Web教程的指导重播是培训高级GUI代理的实用且可扩展的策略，为更有能力和自主的数字助手铺平了道路。

### ChatTime: A Unified Multimodal Time Series Foundation Model Bridging Numerical and Textual Data 
[[arxiv](https://arxiv.org/abs/2412.11376)] [[cool](https://papers.cool/arxiv/2412.11376)] [[pdf](https://arxiv.org/pdf/2412.11376)]
> **Authors**: Chengsen Wang,Qi Qi,Jingyu Wang,Haifeng Sun,Zirui Zhuang,Jinming Wu,Lei Zhang,Jianxin Liao
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: Accepted by AAAI 2025
- **标题**: 融合：统一的多模式时间序列基础模型桥接数值和文本数据
- **领域**: 计算语言学,机器学习
- **摘要**: 人类专家通常将数值和文本多模式信息整合到分析时间序列。但是，大多数传统的深度学习预测因素仅依赖于单峰值数值数据，使用固定长度窗口进行单个数据集的培训和预测，并且无法适应不同的情况。受驱动的预训练的大型语言模型为时间序列分析引入了新的机会。然而，现有方法要么在培训方面效率低下，无法处理文本信息，要么缺乏零拍摄的预测能力。在本文中，我们将模型时间序列创新为外语，并构建Chattime，这是时间序列和文本处理的统一框架。作为一个开箱即用的多模式时间序列基础模型，Chattime提供了零拍的预测能力，并支持时间序列和文本的双峰输入/输出。我们设计了一系列实验，以验证跨多个任务和方案的融合性能的出色性能，并创建四个多模式数据集来解决数据差距。实验结果证明了融合时间的潜力和效用。

### Drawing the Line: Enhancing Trustworthiness of MLLMs Through the Power of Refusal 
[[arxiv](https://arxiv.org/abs/2412.11196)] [[cool](https://papers.cool/arxiv/2412.11196)] [[pdf](https://arxiv.org/pdf/2412.11196)]
> **Authors**: Yuhao Wang,Zhiyuan Zhu,Heyang Liu,Yusheng Liao,Hongcheng Liu,Yanfeng Wang,Yu Wang
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 绘制线条：通过拒绝的力量增强MLLM的信任度
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 多模式的大语模型（MLLM）在多模式的感知和理解方面表现出色，但它们产生幻觉或不准确的响应的趋势破坏了他们的可信赖性。现有的方法在很大程度上忽略了拒绝响应的重要性，作为增强MLLM可靠性的一种手段。为了弥合这一差距，我们介绍了信息边界吸引的学习框架（INBOL），这是一种新颖的方法，可以使MLLM拒绝在遇到不足的信息时拒绝回答用户查询。据我们所知，在BOL是系统地定义拒绝使用我们论文中提出的信息边界概念的第一个框架。该框架引入了全面的数据生成管道和量身定制的培训策略，以提高模型提供适当拒绝响应的能力。为了评估MLLM的可信赖性，我们进一步提出了一个以用户为中心的对齐目标以及相应的指标。实验结果表明，拒绝准确性的显着提高，而没有明显损害该模型的有益性，从而确立了在构建更值得信赖的MLLM方面的关键进步。

### The Superalignment of Superhuman Intelligence with Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.11145)] [[cool](https://papers.cool/arxiv/2412.11145)] [[pdf](https://arxiv.org/pdf/2412.11145)]
> **Authors**: Minlie Huang,Yingkang Wang,Shiyao Cui,Pei Ke,Jie Tang
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: Under review of Science China
- **标题**: 超人情报与大语言模型的超级对象
- **领域**: 计算语言学
- **摘要**: 由于大型语言模型和多模式模型的快速发展，我们目睹了超人情报。随着这样的超人模型的应用变得越来越流行，这里出现了一个关键的问题：我们如何确保超人模型仍然安全，可靠且与人类价值观保持良好状态？在该职位论文中，我们从学习角度讨论了超级对象的概念，通过概述学习范式从大规模预处理，有监督的微调到对齐培训的转变。我们将超级对准定义为设计有效，有效的对齐算法，以从嘈杂标记的数据（点样本或配对偏好数据）中学习，当任务变得非常复杂，对于人类专家的注释非常复杂并且模型比人类专家更强大。我们重点介绍了超级对准的一些关键研究问题，即，弱至严重的概括，可扩展的监督和评估。然后，我们提出了一个超级对象的概念框架，该框架由三个模块组成：一个攻击者，该攻击者生成了试图揭示学习者模型的弱点的对手查询；一个学习者将通过从评论家模型产生的可扩展反馈以及最少的人类专家中学到的可扩展反馈来提高自己；以及为给定查询响应对产生批评者或解释的评论家，其目标是通过批评来改善学习者。我们在此框架的每个组成部分中讨论了一些重要的研究问题，并突出了一些与我们提出的框架密切相关的有趣的研究思想，例如自我调整，自我播放，自我翻新等。最后，我们重点介绍了一些未来的研究指导，包括确定新的紧急风险和多维一致性。

### VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2412.10704)] [[cool](https://papers.cool/arxiv/2412.10704)] [[pdf](https://arxiv.org/pdf/2412.10704)]
> **Authors**: Manan Suri,Puneet Mathur,Franck Dernoncourt,Kanika Goswami,Ryan A. Rossi,Dinesh Manocha
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: vign：使用多模式检索生成的多模式元素的多文件质量图
- **领域**: 计算语言学
- **摘要**: 了解来自多个文档集合的信息，尤其是具有视觉上丰富元素的文档的信息，对于文档的问题回答非常重要。本文介绍了Visdombench，这是第一个综合基准测试，旨在评估具有丰富多模式内容的多文档设置中的质量检查系统，包括表，图表和演示幻灯片。我们提出了visdomRag，这是一种新型的多式联运增强生成（RAG）方法，同时利用视觉和文本抹布，将强大的视觉检索能力与精致的语言推理相结合。 VistomRag采用了一个多步推理过程，其中包括证据策划以及并发的文本和视觉抹布管道的链条推理。 visdomRag的一个主要新颖性是其一致性受限的模态融合机制，它使推理时间跨模态的推理过程保持一致，以产生连贯的最终答案。这会导致在跨模式分布关键信息并通过隐式上下文归因提高答案可验证性的方案中提高准确性。通过涉及开源和专有大型语言模型的广泛实验，我们在Visdombench上基准了最先进的文档质量质量检查方法。广泛的结果表明，VistomRag的端到端多模式文档质量质量质量质量质量为12-20％。

### Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs Performance 
[[arxiv](https://arxiv.org/abs/2412.10417)] [[cool](https://papers.cool/arxiv/2412.10417)] [[pdf](https://arxiv.org/pdf/2412.10417)]
> **Authors**: Abdelrahman A. Ali,Aya E. Fouda,Radwa J. Hanafy,Mohammed E. Fouda
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 利用音频和文本方式心理健康：LLMS绩效的研究
- **领域**: 计算语言学,人工智能,声音,音频和语音处理
- **摘要**: 精神健康障碍在全球范围内越来越普遍，迫切需要创新的工具来支持早期诊断和干预。这项研究探讨了大语言模型（LLM）在多模式心理健康诊断中的潜力，特别是通过文本和音频方式来检测抑郁症和创伤后应激障碍。使用E-DAIC数据集，我们比较文本和音频模式，以调查LLMS在音频输入中是否可以表现出色或更好。我们进一步研究了两种方式的整合，以确定这是否可以提高诊断准确性，这通常会导致性能指标的改善。我们的分析专门利用自定义的指标；模态优越性评分和分歧分辨分数，以评估联合模态如何影响模型性能。使用组合方式时，Gemini 1.5 Pro模型在二进制抑郁分类中获得了最高分数，F1得分为0.67，平衡精度（BA）为77.4％，在整个数据集中进行了评估。这些结果代表其与文本模式相比的表现增长了3.1％，比音频方式增加了2.7％，突出了整合模态以提高诊断准确性的有效性。值得注意的是，所有结果均以推断为零，突出显示模型的鲁棒性，而无需特定于任务的微调。为了探索不同配置对模型性能的影响，我们使用零射击和少量提示进行了二进制，严重性和多类任务，从而研究了迅速变化对性能的影响。结果表明，文本和音频模式中的Gemini 1.5 Pro之类的模型，以及文本模式中的GPT-4O MINI，通常以平衡的精度超过其他模型，并且在多个任务中得分。

### A Grounded Typology of Word Classes 
[[arxiv](https://arxiv.org/abs/2412.10369)] [[cool](https://papers.cool/arxiv/2412.10369)] [[pdf](https://arxiv.org/pdf/2412.10369)]
> **Authors**: Coleman Haley,Sharon Goldwater,Edoardo Ponti
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: 19 pages, 5 figures
- **标题**: 单词类的基础类型
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 我们在语言类型学中提出了一种扎根的意义方法。我们将来自感知模式的数据（例如图像）视为含义的语言敏捷表示。因此，我们可以量化图像和字幕之间的功能 - 形式的关系。受信息理论的启发，我们定义了“接地”，这是对上下文语义内容的经验度量（以惊人的差异为差异），可以使用多语言多模式模型来计算。作为概念证明，我们将此度量应用于单词类的类型。我们的措施捕获了跨语言的功能（语法）和词汇（内容）类之间的内容不对称性，但与功能类别无法传达内容的观点相矛盾。此外，我们发现了扎根的层次结构（例如名词>形容词>动词）的普遍趋势，并表明我们的度量部分与英语的心理语言具体规范相关。我们发布了30种语言的接地分数数据集。我们的结果表明，接地的类型学方法可以提供有关语言语义功能的定量证据。

### AMuSeD: An Attentive Deep Neural Network for Multimodal Sarcasm Detection Incorporating Bi-modal Data Augmentation 
[[arxiv](https://arxiv.org/abs/2412.10103)] [[cool](https://papers.cool/arxiv/2412.10103)] [[pdf](https://arxiv.org/pdf/2412.10103)]
> **Authors**: Xiyuan Gao,Shubhi Bansal,Kushaan Gowda,Zhu Li,Shekhar Nayak,Nagendra Kumar,Matt Coler
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: This is a preprint version of the paper, submitted and under review at the IEEE Transactions on Affective Computing
- **标题**: 逗乐：一个细心的深度神经网络，用于包含双模式数据增强的多模式讽刺检测
- **领域**: 计算语言学
- **摘要**: 有效地检测讽刺需要对上下文的细微理解，包括声音和面部表情。然而，讽刺检测中多模式计算方法的进展由于数据缺乏而面临挑战。为了解决这个问题，我们介绍了有趣的（用于包含双模式数据增强的多模式讽刺检测的细心深度神经网络）。这种方法利用了多模式讽刺检测数据集（芥末），并引入了两相双峰数据增强策略。第一阶段涉及通过几种二级语言的背面翻译生成各种文本样本。第二阶段涉及基于Fastspeech 2的语音合成系统的完善，该系统专门针对讽刺而定制，以保留讽刺性。除了基于云的文本对语音（TTS）服务外，这种微调的FastSpeech 2系统还为文本增强提供了相应的音频。我们还研究了各种注意机制，以有效地合并文本和音频数据，发现自我注意力是双峰整合的最有效效率。我们的实验表明，这种结合的增强和注意力方法的文本原语模式达到了81.0％的显着F1得分，甚至超过了使用芥末数据集中三种模式的模型。

### Rethinking Comprehensive Benchmark for Chart Understanding: A Perspective from Scientific Literature 
[[arxiv](https://arxiv.org/abs/2412.12150)] [[cool](https://papers.cool/arxiv/2412.12150)] [[pdf](https://arxiv.org/pdf/2412.12150)]
> **Authors**: Lingdong Shen,Qigqi,Kun Ding,Gaofeng Meng,Shiming Xiang
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: 重新思考全面的基准以获取图表理解：科学文献的观点
- **领域**: 计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 科学文献图表通常包含复杂的视觉元素，包括使用这些真实且复杂的图表评估多模型模型的多图形，流程图，结构图等。可更准确地评估其理解能力。但是，现有基准面临局限性：图表类型范围狭窄，基于模板过于简单的问题和视觉元素以及评估方法不足。这些缺点会导致膨胀的性能得分，而当模型遇到现实世界的科学图表时，这些缺点无法保持。为了应对这些挑战，我们引入了一个新的基准测试，科学图质量质量图（SCI-CQA），该图表强调流程图是一个关键但经常被忽略的类别。为了克服图表的局限性和简单的视觉元素，我们在过去十年中从15个顶级计算机科学会议论文中策划了202,760个图像文本对的数据集。经过严格的过滤后，我们将其完善为37,607个高质量图表，并具有上下文信息。 SCI-CQA还引入了一个新的评估框架，其灵感来自人类考试，其中包括5,629个精心策划的问题，包括客观和开放式问题。此外，我们提出了一个有效的注释管道，可大大降低数据注释成本。最后，我们探讨了基于上下文的图表理解，突出了上下文信息在解决以前无法回答的问题中的关键作用。

### A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges 
[[arxiv](https://arxiv.org/abs/2412.11936)] [[cool](https://papers.cool/arxiv/2412.11936)] [[pdf](https://arxiv.org/pdf/2412.11936)]
> **Authors**: Yibo Yan,Jiamin Su,Jianxiang He,Fangteng Fu,Xu Zheng,Yuanhuiyi Lyu,Kun Wang,Shen Wang,Qingsong Wen,Xuming Hu
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: 多模式大语言模型时代的数学推理调查：基准，方法和挑战
- **领域**: 计算语言学
- **摘要**: 数学推理是人类认知的一个核心方面，在许多领域，从教育问题到科学进步至关重要。随着人工通用情报（AGI）的进步，将大语言模型（LLMS）与数学推理任务相结合变得越来越重要。这项调查提供了对多模式大语模型（MLLM）时代数学推理的首次全面分析。我们回顾了自2021年以来发表的200多个研究，并研究了数学插件的最新发展，重点是多模式设置。我们将该领域分为三个维度：基准，方法和挑战。特别是，我们探讨了多模式数学推理管道，以及（M）LLM和相关方法的作用。最后，我们确定了阻碍该领域中AGI实现的五个主要挑战，从而为增强多模式推理能力的未来方向提供了见解。这项调查是研究社区的关键资源，以促进LLMS解决复杂多模式推理任务的能力。

### MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond 
[[arxiv](https://arxiv.org/abs/2412.11538)] [[cool](https://papers.cool/arxiv/2412.11538)] [[pdf](https://arxiv.org/pdf/2412.11538)]
> **Authors**: Muhammad Huzaifah,Geyu Lin,Tianchi Liu,Hardik B. Sailor,Kye Min Tan,Tarun K. Vangani,Qiongqiong Wang,Jeremy H. M. Wong,Nancy F. Chen,Ai Ti Aw
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: Meralion-SpeechenCoder：迈向新加坡及其他地区的语音基础模型
- **领域**: 计算语言学,人工智能,音频和语音处理
- **摘要**: 该技术报告描述了Meralion-SpeechenCoder，这是一个基础模型，旨在支持广泛的下游语音应用程序。 Meralion-Speechencencoder是新加坡国家多模型模型计划的一部分，旨在满足新加坡和周围东南亚地区的语音处理需求。该模型目前主要支持英语，包括新加坡说的品种。我们正在积极扩展数据集，以逐渐涵盖随后的版本中的其他语言。 Meralion-SpeechCoder使用基于掩盖语言建模的自我监督的学习方法对200,000个未标记的语音数据进行了预先训练。我们在下面详细介绍了我们的培训程序和超参数调整实验。我们的评估表明，对自发和新加坡语音基准的改进，以供语音识别，同时仍与其他十个语音任务中的其他最先进的语音编码者保持竞争力。我们致力于释放我们的模型，支持新加坡及其他地区的更广泛的研究努力。

### ACE-$M^3$: Automatic Capability Evaluator for Multimodal Medical Models 
[[arxiv](https://arxiv.org/abs/2412.11453)] [[cool](https://papers.cool/arxiv/2412.11453)] [[pdf](https://arxiv.org/pdf/2412.11453)]
> **Authors**: Xiechi Zhang,Shunfan Zheng,Linlin Wang,Gerard de Melo,Zhu Cao,Xiaoling Wang,Liang He
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: ACE- $ M^3 $：自动功能评估器多模式医疗模型
- **领域**: 计算语言学,人工智能
- **摘要**: 随着多模式大语模型（MLLM）在医学领域的突出性，对评估其有效性的精确评估方法的需求变得至关重要。尽管基准提供了一种可靠的手段来评估MLLM的功能，但用于开放式域评估的传统指标和BLEU等传统指标仅关注令牌重叠，并且可能与人类的判断不符。尽管人类评估更可靠，但它是劳动密集型，昂贵且不可扩展的。基于LLM的评估方法已被证明是有希望的，但是迄今为止，迫切需要医学领域的开源多模式LLM评估者。 To address this issue, we introduce ACE-$M^3$, an open-sourced \textbf{A}utomatic \textbf{C}apability \textbf{E}valuator for \textbf{M}ultimodal \textbf{M}edical \textbf{M}odels specifically designed to assess the question answering abilities of medical MLLMs.它首先利用分支机构体系结构来基于标准的医疗评估标准提供详细的分析和简洁的最终分数。随后，制定了基于奖励令牌的直接优先优化（RTDPO）策略，以节省训练时间，而不会损害我们的模型的性能。广泛的实验证明了我们的Ace- $ M^3 $模型\ footNote {\ url {https://huggingface.co/collections/collections/aiusrtmp/ace-m3-67593297ff397ff391b93e3e3e3e3e3e3e3e3e68}}在评估Medicalmblls的cababils。

### CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval 
[[arxiv](https://arxiv.org/abs/2412.13071)] [[cool](https://papers.cool/arxiv/2412.13071)] [[pdf](https://arxiv.org/pdf/2412.13071)]
> **Authors**: Mohammad Mahdi Abootorabi,Ehsaneddin Asgari
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: accepted at ECIR 2025
- **标题**: 扣子：多语言多模式信息检索的对比语言言论预测
- **领域**: 计算语言学,信息检索,声音,音频和语音处理
- **摘要**: 这项研究介绍了CLASP（对比度语言预处理），这是一种针对音频文本信息检索量身定制的多语言多模式表示。扣子利用口语内容与文本数据之间的协同作用。在培训期间，我们利用了新引入的语音文本数据集，其中包括从小说到宗教的15个不同类别。 CLASP的音频组件将音频谱图与预先训练的自我监管的语音模型集成在一起，而其语言编码对应语的语言采用编码器编码器，以超过100种语言进行编码。这种统一的轻型模型弥合了各种方式和语言之间的差距，从而增强了其在处理和检索多语言和多模式数据方面的有效性。我们跨多种语言的评估表明，CLASP在@1，MRR和Meanr指标中建立新的基准测试，在特定情况下表现优于基于ASR的传统检索方法。

### RCLMuFN: Relational Context Learning and Multiplex Fusion Network for Multimodal Sarcasm Detection 
[[arxiv](https://arxiv.org/abs/2412.13008)] [[cool](https://papers.cool/arxiv/2412.13008)] [[pdf](https://arxiv.org/pdf/2412.13008)]
> **Authors**: Tongguan Wang,Junkai Li,Guixin Su,Yongcheng Zhang,Dongyu Su,Yuxue Hu,Ying Sha
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: RCLMUFN：用于多模式讽刺检测的关系上下文学习和多重融合网络
- **领域**: 计算语言学
- **摘要**: 讽刺通常通过表达与说话者的真实意图相反的含义来传达蔑视或批评的情绪。准确检测讽刺有助于识别和过滤互联网上的不良信息，从而减少恶意诽谤和谣言。尽管如此，对于机器而言，自动讽刺检测的任务仍然是高度挑战性的，因为它在很大程度上取决于复杂的因素，例如关系环境。大多数现有的多模式讽刺检测方法着重于引入图形结构，以在文本和图像之间建立实体关系，同时忽略学习文本和图像之间的关系上下文，这是理解讽刺含义的重要证据。此外，讽刺的含义随着不同上下文的演变而变化，但是现有方法可能在建模这种动态变化的情况下可能不准确，从而限制了模型的概括能力。为了解决上述问题，我们提出了一个关系上下文学习和多重融合网络（RCLMUFN），以进行多模式讽刺检测。首先，我们采用四个功能提取器来全面从原始文本和图像中提取功能，旨在挖掘以前可能被忽略的潜在功能。其次，我们利用关系上下文学习模块来学习文本和图像的上下文信息，并通过浅层和深层交互捕获动态属性。最后，我们采用多重特征融合模块来通过穿透从各种交互环境中得出的多模式特征来增强模型的概括。对两个多模式讽刺检测数据集进行了广泛的实验表明，我们提出的方法实现了最先进的性能。

### Make Imagination Clearer! Stable Diffusion-based Visual Imagination for Multimodal Machine Translation 
[[arxiv](https://arxiv.org/abs/2412.12627)] [[cool](https://papers.cool/arxiv/2412.12627)] [[pdf](https://arxiv.org/pdf/2412.12627)]
> **Authors**: Andong Chen,Yuchen Song,Kehai Chen,Muyun Yang,Tiejun Zhao,Min Zhang
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Work in progress
- **标题**: 使想象力更清晰！用于多模式机器翻译的基于稳定的基于扩散的视觉想象
- **领域**: 计算语言学
- **摘要**: 引入了视觉信息，以增强机器翻译（MT），其有效性在很大程度上取决于与手动图像注释的大量双语平行句子对的可用性。在本文中，我们将一个稳定的基于扩散的想象网络引入多模式大型语言模型（MLLM），以明确生成每个源句子的图像，从而推进多模型MT。特别是，我们通过强化学习来构建启发式人类反馈，以确保生成的图像与源句子的一致性，而无需对图像注释的监督，这破坏了使用MT中使用视觉信息的瓶颈。此外，提出的方法使想象力的视觉信息除了多模式MT之外还可以集成到仅大规模文本的大规模文本MT中。实验结果表明，我们的模型显着优于现有的多模式MT和仅文本MT，尤其是在多模式多模式MT基准上的平均提高超过14个BLEU点。

### FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning 
[[arxiv](https://arxiv.org/abs/2412.12567)] [[cool](https://papers.cool/arxiv/2412.12567)] [[pdf](https://arxiv.org/pdf/2412.12567)]
> **Authors**: Seunghee Kim,Changhyeon Kim,Taeuk Kim
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: FCMR：对金融跨模式多跳跃推理的强大评估
- **领域**: 计算语言学
- **摘要**: 现实世界的决策通常需要整合和推理多种方式的信息。尽管最近的多模式大语言模型（MLLM）在此类任务中表现出了承诺，但它们在各种来源进行多跳上推理的能力仍未得到充分评估。现有基准（例如MMQA）由于（1）数据污染而面临挑战，以及（2）缺乏需要在两种以上方式进行操作的复杂查询，从而阻碍了准确的绩效评估。为了解决这个问题，我们介绍了金融跨模式多跳跃推理（FCMR），这是一种基准，旨在通过敦促它们结合金融领域内文本报告，表格和图表的信息来分析MLLM的推理能力。 FCMR分为三个难度级别，中等和艰难的级别评估。特别是，在硬级别上的问题需要精确的跨模式三跳推理，旨在防止无视任何模式。这款新基准测试的实验表明，即使是最先进的MLLMS，也以表现最好的模型（Claude 3.5十四行诗）在最具挑战性的层面上仅达到30.4％的精度。我们还进行分析以提供有关模型内部运作的见解，包括在信息检索阶段发现关键瓶颈。

### Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence 
[[arxiv](https://arxiv.org/abs/2412.13949)] [[cool](https://papers.cool/arxiv/2412.13949)] [[pdf](https://arxiv.org/pdf/2412.13949)]
> **Authors**: Jinghan He,Kuan Zhu,Haiyun Guo,Junfeng Fang,Zhenglin Hua,Yuheng Jia,Ming Tang,Tat-Seng Chua,Jinqiao Wang
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 用视觉吸引的头部发散破解LVLM中的幻觉代码
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 大型视觉语言模型（LVLM）在将大型语言模型（LLM）与视觉输入相结合，从而实现了先进的多模式推理方面取得了重大进展。尽管他们成功了，但持续的挑战是幻觉 - 生成的文本无法准确地反映出在准确性和可靠性下的视觉内容。现有的方法着眼于对齐训练或解码改进，但主要是在生成阶段解决症状，而无需探测潜在的原因。在这项工作中，我们研究了驱动LVLMS幻觉的内部机制，重点是多头注意模块。具体而言，我们引入了视觉感知的头部差异（VHD），该指标量化了注意力头部输出对视觉上下文的敏感性。基于这一点，我们的发现揭示了视觉上更适合视觉信息的视觉注意力头的存在。但是，该模型对先前语言模式的过度依赖与幻觉密切相关。在这些见解的基础上，我们提出了视觉感知的头部增强（VHR），这是一种通过增强视觉吸引注意力的角色来减轻幻觉的无训练方法。广泛的实验表明，与缓解幻觉的最新方法相比，我们的方法相比具有优越的性能，同时保持高效率，而额外的额外时间可忽略不计。

### RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs 
[[arxiv](https://arxiv.org/abs/2412.13835)] [[cool](https://papers.cool/arxiv/2412.13835)] [[pdf](https://arxiv.org/pdf/2412.13835)]
> **Authors**: Alberto Testoni,Barbara Plank,Raquel Fernández
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 球拍：揭示Visual LLM中被忽视的参考歧义的危险
- **领域**: 计算语言学
- **摘要**: 歧义解决是有效沟通的关键。尽管人类通过对话基础策略毫不费力地解决了歧义，但当前语言模型模仿这些策略的程度尚不清楚。在这项工作中，我们通过介绍Racquet来检查基于图像的问题回答的参考歧义，这是一个精心策划的数据集，针对歧义的不同方面。通过一系列评估，我们揭示了最新的大型多模式模型的重大局限性和过度自信，以解决其响应中的歧义。过度自信的问题与球拍偏置尤其重要，该子集旨在分析一个关键但又没有被忽视的问题：无法解决歧义会导致刻板印象，社会上有偏见的反应。我们的结果强调了为模型配备强大策略以应对不确定性的紧迫性，而无需诉诸于不良的刻板印象。

### MATCHED: Multimodal Authorship-Attribution To Combat Human Trafficking in Escort-Advertisement Data 
[[arxiv](https://arxiv.org/abs/2412.13794)] [[cool](https://papers.cool/arxiv/2412.13794)] [[pdf](https://arxiv.org/pdf/2412.13794)]
> **Authors**: Vageesh Saxena,Benjamin Bashpole,Gijs Van Dijck,Gerasimos Spanakis
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: 40 pages
- **标题**: 匹配：在伴游广告数据中打击人口贩运的多模式作者属性
- **领域**: 计算语言学,人工智能,计算机与社会
- **摘要**: 人口贩运（HT）仍然是一个关键问题，贩运者越来越多地利用在线护送广告（AD）匿名广告受害者。现有的检测方法，包括作者归因（AA），通常以基于文本的分析为中心，并忽略了在线护送广告的多模式性质，通常将文本与图像配对。为了解决这一差距，我们介绍了匹配，这是一个由27,619个独特的文本说明和55,115个独特图像的多模式数据集，这些图像从四个地理区域的美国七个城市的背后送护平台收集。我们的研究对供应商识别和验证任务进行了广泛的基准，仅具有文本，仅视觉和多模式基线，采用多任务（联合）培训目标，以实现卓越的分类和在分发和分发（OOD）数据集方面实现出色的分类和检索性能。集成多模式功能进一步增强了此性能，从而捕获了文本和图像的互补模式。虽然文本仍然是主要的方式，但视觉数据添加了丰富模型性能的风格线索。此外，由于语义重叠率低和护送广告方式之间的模糊联系，诸如剪辑和BLIP2斗争之类的文本图像对齐策略，端到端的多模式训练证明更强大。我们的发现强调了多模式AA（MAA）对抗HT的潜力，从而为LEA提供了可靠的工具来链接广告并破坏贩运网络。

### Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.13702)] [[cool](https://papers.cool/arxiv/2412.13702)] [[pdf](https://arxiv.org/pdf/2412.13702)]
> **Authors**: Kunat Pipatanakul,Potsawee Manakul,Natapong Nitarach,Warit Sirichotedumrong,Surapon Nonesung,Teetouch Jaknamon,Parinthapat Pengpun,Pittawat Taveekitworachai,Adisai Na-Thalang,Sittipong Sripaisarnmongkol,Krisanapong Jirayoot,Kasima Tharnpipitchai
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: technical report, 55 pages
- **标题**: 台风2：一个开放文本和多式联运泰国大型语言模型的家族
- **领域**: 计算语言学,人工智能
- **摘要**: 本文介绍了Typhoon 2，这是针对泰语优化的一系列文本和多模式的大型语言模型。该系列包括文本，视觉和音频的模型。 Typhoon2-Text建立在最先进的开放模型的基础上，例如Llama 3和Qwen2，我们对英语和泰语数据的混合物进行了持续的预训练。我们采用训练后技术来增强泰语语言性能，同时保留基本模型的原始功能。我们在基本和指令调整的变体中都可以使用各种尺寸的文本模型，从1到700亿个参数。为了保护文本，我们发布了Typhoon2-Safety，这是一种用于泰语文化和语言的分类器。 Typhoon2-Vision在保留一般视觉功能（例如图像字幕）的同时，提高了泰语文档的理解。 Typhoon2-Audio引入了端到端的语音到语音模型体系结构，能够处理音频，语音和文本输入并同时生成文本和语音输出。

### Decade of Natural Language Processing in Chronic Pain: A Systematic Review 
[[arxiv](https://arxiv.org/abs/2412.15360)] [[cool](https://papers.cool/arxiv/2412.15360)] [[pdf](https://arxiv.org/pdf/2412.15360)]
> **Authors**: Swati Rajwal
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 慢性疼痛中的十年自然语言处理：系统评价
- **领域**: 计算语言学
- **摘要**: 近年来，自然语言处理（NLP）和公共卫生的交汇处为研究各种领域（包括文本数据集中的慢性疼痛）开辟了创新的途径。尽管NLP在慢性疼痛方面有希望，但文献却分散在各个学科中，并且有必要巩固现有知识，识别文献中的知识差距，并为这个新兴领域的未来研究方向提供信息。这篇评论旨在调查针对慢性疼痛研究设计的基于NLP的干预措施的研究状态。在PubMed，Web of Science，IEEE Xplore，Scopus和ACL选集中制定了搜索策略，以查找2014年至2024年之间在英语中发表的研究。筛选132篇论文后，最终综述中包括了26项研究。这篇评论中的主要发现强调了NLP技术应对慢性疼痛研究中紧迫挑战的重要潜力。在该领域的过去十年中，在分类任务中展示了实现高性能指标（例如，F1> 0.8）的高级方法（例如Roberta和Bert）的利用（例如，F1> 0.8），而无访问的方法如潜在的DIRICHLET分配（LDA）和K-Means和K-Means CMENTING有效地用于探索性分析。结果还揭示了持续的挑战，例如数据集多样性有限，样本量不足以及代表性不足的人群的表示不足。未来的研究应探索多模式数据验证系统，上下文感知的机械建模以及标准化评估指标的开发，以增强慢性疼痛研究中的可重复性和公平性。

### Advanced ingestion process powered by LLM parsing for RAG system 
[[arxiv](https://arxiv.org/abs/2412.15262)] [[cool](https://papers.cool/arxiv/2412.15262)] [[pdf](https://arxiv.org/pdf/2412.15262)]
> **Authors**: Arnau Perez,Xavier Vizcaino
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-20
> **comment**: 12 pages, 3 figures
- **标题**: LLM解析启动的抹布系统的高级摄入过程
- **领域**: 计算语言学,人工智能,信息检索
- **摘要**: 检索增强发电（RAG）系统与处理各种结构复杂性的多模式文档的困难。本文介绍了一种使用LLM驱动的OCR进行新颖的多策略解析方法，以从各种文档类型中提取内容，包括演示文稿和高文本密度文件是否扫描。该方法采用基于节点的提取技术，该技术在不同信息类型之间建立关系并生成上下文感知的元数据。通过实现多模式组装代理和灵活的嵌入策略，该系统可以增强文档的理解和检索功能。跨多个知识库的实验评估证明了该方法的有效性，显示了答案相关性和信息忠诚的改善。

### Analyzing Images of Legal Documents: Toward Multi-Modal LLMs for Access to Justice 
[[arxiv](https://arxiv.org/abs/2412.15260)] [[cool](https://papers.cool/arxiv/2412.15260)] [[pdf](https://arxiv.org/pdf/2412.15260)]
> **Authors**: Hannes Westermann,Jaromir Savelka
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-20
> **comment**: Accepted at AI for Access to Justice Workshop at Jurix 2024, Brno, Czechia. Code and Data available at: https://github.com/hwestermann/AI4A2J_analyzing_images_of_legal_documents
- **标题**: 分析法律文件的图像：迈向多模式LLMS以获取正义
- **领域**: 计算语言学,计算机视觉和模式识别,多媒体
- **摘要**: 与法律制度和政府进行互动需要对可以分布在不同（纸张）文件（例如表格，证书和合同）（例如租赁）等各种信息的各种信息组装和分析。为了了解自己的合法权利，并填写表格以在法院提出索赔或获得政府福利。但是，找到正确的信息，找到正确的表格并填写它们对外行人来说可能具有挑战性。大型语言模型（LLMS）已成为一种有可能解决此差距的强大技术，但仍然依靠用户提供正确的信息，如果仅在复杂的纸质文档中可用信息，这可能会充满挑战且容易出错。我们提出了一项研究，以利用多模式LLM来分析手写纸质形式的图像，以便以结构化格式自动提取相关信息。我们的最初结果是有希望的，但揭示了一些局限性（例如，当图像质量较低时）。我们的工作证明了整合多模式LLM的潜力，以支持外行和自我代表的诉讼人查找和组装相关信息。

### AgentPS: Agentic Process Supervision for Multi-modal Content Quality Assurance through Multi-round QA 
[[arxiv](https://arxiv.org/abs/2412.15251)] [[cool](https://papers.cool/arxiv/2412.15251)] [[pdf](https://arxiv.org/pdf/2412.15251)]
> **Authors**: Gorden Liu,Yu Sun,Ruixiao Sun,Xin Dong,Hongyu Xiong
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-20
> **comment**: 8 pages, 2 figures
- **标题**: 代理：通过多轮质量检查
- **领域**: 计算语言学,人工智能
- **摘要**: 多模式大语言模型（MLLM）的先进处理和推理能力已推动了视力语言（VL）理解任务的实质进展。但是，虽然对由直接逻辑控制的任务有效，但MLLM在推理复杂的相互依存逻辑结构时通常会遇到挑战。为了解决此限制，我们介绍了\ textit {agentps}，这是一个新颖的框架，该框架将代理过程监督整合到MLLMS中，通过在微调过程中回答多轮问题。 \ textIt {agentps}由于其整合过程监督和结构化的顺序推理，因此在专有tiktok数据集的基线MLLM上表现出了显着的性能改进。此外，我们表明，用LLM生成的标签代替人类注销的标签可保留大量的性能增长，从而突出了该框架在工业应用中的实际可扩展性。这些结果将\ textIt {agentps}定位为多模式分类任务的高效和有效的体系结构。它的适应性和可扩展性，尤其是在通过自动注释生成增强时，它使其成为处理大规模，现实世界中挑战的强大工具。

### LMFusion: Adapting Pretrained Language Models for Multimodal Generation 
[[arxiv](https://arxiv.org/abs/2412.15188)] [[cool](https://papers.cool/arxiv/2412.15188)] [[pdf](https://arxiv.org/pdf/2412.15188)]
> **Authors**: Weijia Shi,Xiaochuang Han,Chunting Zhou,Weixin Liang,Xi Victoria Lin,Luke Zettlemoyer,Lili Yu
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: Name change: LlamaFusion to LMFusion
- **标题**: lmfusion：适应多模式生成的预审前的语言模型
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 我们提出了LMFusion，这是一种具有多模式生成能力的验证预读文本的大型语言模型（LLMS）的框架，使它们能够以任意序列理解和生成文本和图像。 LMFusion利用现有的Llama-3的权重来自动加工文本，同时引入其他并行变压器模块，用于处理扩散的图像。在训练过程中，每种模式的数据都被路由到其专用模块：特定于模态的前馈图层，查询键值预测和归一化层独立处理每个模态，而共享的自我发挥层则允许跨文本和图像特征进行交互。通过冻结特定文本的模块并仅训练特定图像的模块，LMFusion可以保留仅文本LLM的语言能力，同时发展强大的视觉理解和发电能力。与从头开始的多峰生成模型的方法相比，我们的实验表明，LMFusion将图像理解提高了20％，并且仅使用50％的拖鞋，同时维持Llama-3的语言能力，将图像生成3.6％提高了3.6％。我们还证明，该框架可以使现有的视觉模型具有多模式生成能力。总体而言，该框架不仅利用文本LLMS中的现有计算投资，而且还可以使语言和视觉能力的平行发展，为有效的多模型模型开发提供了有希望的方向。

### Qwen2.5 Technical Report 
[[arxiv](https://arxiv.org/abs/2412.15115)] [[cool](https://papers.cool/arxiv/2412.15115)] [[pdf](https://arxiv.org/pdf/2412.15115)]
> **Authors**: Qwen,:,An Yang,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chengyuan Li,Dayiheng Liu,Fei Huang,Haoran Wei,Huan Lin,Jian Yang,Jianhong Tu,Jianwei Zhang,Jianxin Yang,Jiaxi Yang,Jingren Zhou,Junyang Lin,Kai Dang,Keming Lu,Keqin Bao,Kexin Yang,Le Yu, et al. (19 additional authors not shown)
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: QWEN2.5技术报告
- **领域**: 计算语言学
- **摘要**: 在本报告中，我们介绍了QWEN2.5，这是一系列旨在满足各种需求的大型语言模型（LLMS）。与以前的迭代相比，QWEN 2.5在训练前和训练后阶段都得到了显着改善。在预训练方面，我们将高质量的预训练数据集从前7万亿代币缩放到18万亿代币。这为常识，专家知识和推理能力提供了坚实的基础。在培训后，我们通过超过100万个样本以及多阶段的增强学习来实施复杂的监督填充。训练后技术增强了人类的偏好，并尤其改善了长文本生成，结构数据分析和随后的教学。为了有效地处理多样化和多样化的用例，我们以丰富的尺寸提出QWEN2.5 LLM系列。开放权重的产品包括基本和指导调整的模型，并提供量化的版本。此外，对于托管解决方案，专有模型当前包括两种混合物（MOE）变体：QWEN2.5-Turbo和Qwen2.5-Plus，均可从Alibaba Cloud Model Studio获得。 Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around大5倍。 QWEN2.5-TURBO和QWEN2.5-Plus分别对GPT-4O-Mini和GPT-4O进行竞争性，提供了卓越的成本效益。此外，作为基础，QWEN2.5模型在训练专业模型（例如QWEN2.5-MATH，QWEN2.5-CODER，QWQ和多模型模型）中发挥了重要作用。

### Progressive Multimodal Reasoning via Active Retrieval 
[[arxiv](https://arxiv.org/abs/2412.14835)] [[cool](https://papers.cool/arxiv/2412.14835)] [[pdf](https://arxiv.org/pdf/2412.14835)]
> **Authors**: Guanting Dong,Chenghao Zhang,Mengjie Deng,Yutao Zhu,Zhicheng Dou,Ji-Rong Wen
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: Working in progress
- **标题**: 通过主动检索进行渐进的多模式推理
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,信息检索
- **摘要**: 多步多模式推理任务对多模式大语言模型（MLLM）构成了重大挑战，并在这种情况下寻找有效的方法来增强其性能仍然是一个尚未解决的问题。在本文中，我们提出了AR-MCT，这是一种通用框架，旨在通过主动检索（AR）和Monte Carlo Tree搜索（MCT）逐步提高MLLM的推理能力。我们的方法始于开发一个统一的检索模块，该模块检索了关键的支持见解，以从混合模式检索语料库中解决复杂的推理问题。为了弥合自动多模式推理验证中的差距，我们采用了MCTS算法与主动检索机制相结合，从而可以自动生成逐步注释。该策略会动态地检索每个推理步骤的关键见解，超越了传统的光束搜索抽样，以提高推理空间的多样性和可靠性。此外，我们引入了一个过程奖励模型，该模型逐渐与多模式推理任务的自动验证保持一致。三个复杂多模式推理基准的实验结果证实了AR-MCTS框架在增强各种多模型的性能方面的有效性。进一步的分析表明，AR-MCT可以优化采样多样性和准确性，从而得出可靠的多模式推理。

### PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children 
[[arxiv](https://arxiv.org/abs/2412.14769)] [[cool](https://papers.cool/arxiv/2412.14769)] [[pdf](https://arxiv.org/pdf/2412.14769)]
> **Authors**: Yiqun Zhang,Xiaocui Yang,Xiaobai Li,Siyuan Yu,Yi Luan,Shi Feng,Daling Wang,Yifei Zhang
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: preprint
- **标题**: psydraw：左撇子儿童心理健康筛查的多模式多模式系统
- **领域**: 计算语言学
- **摘要**: 在中国，左撇子儿童（LBC）人数超过6600万，由于父母的工作迁移而面临严重的心理健康挑战。高危LBC的早期筛查和识别至关重要，但由于精神卫生专业人员的严重短缺，尤其是在农村地区，因此具有挑战性。尽管房屋树人（HTP）测试显示出更高的儿童参与率，但其对专家解释的要求限制了其在资源筛查地区的应用。为了应对这一挑战，我们提出了PsyDraw，这是一种基于多模式大型语言模型的多机构系统，可帮助精神卫生专业人员分析HTP图纸。该系统采用专门的代理来进行特征提取和心理解释，分为两个阶段：全面的功能分析和专业报告生成。对290名小学生的HTP图的评估表明，有71.03％的分析与专业评估达到高度一致性，中等一致性为26.21％，一致性只有2.41％。该系统确定了31.03％的案件需要专业的关注，表明其作为初步筛选工具的有效性。目前部署在试点学校中，\ Method在支持心理健康专业人员（尤其是在资源有限的领域）方面有希望，同时保持心理评估中的高专业标准。

### Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity Benchmark for Multimodal Fake News Detection 
[[arxiv](https://arxiv.org/abs/2412.14686)] [[cool](https://papers.cool/arxiv/2412.14686)] [[pdf](https://arxiv.org/pdf/2412.14686)]
> **Authors**: Hao Guo,Zihan Ma,Zhi Zeng,Minnan Luo,Weixin Zeng,Jiuyang Tang,Xiang Zhao
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 每个假新闻都以其自己的方式是假的：多模式假新闻检测的归因多晶格基准
- **领域**: 计算语言学,人工智能
- **摘要**: 社交平台虽然促进获得信息的访问，但也充满了许多假新闻，造成了负面后果。自动多模式假新闻检测是值得的追求。现有的多模式假新闻数据集仅提供真实或伪造的二进制标签。但是，真正的新闻是一样的，而每个假新闻都是假新闻的。这些数据集无法反映各种类型的多模式假新闻的混合性质。为了弥合差距，我们构建了一个属性的多范围多模式假新闻检测数据集\ amg，揭示了固有的假模式。此外，我们提出了一个多界线线索对准模型\我们的多模式假新闻检测和归因。实验结果表明，\ amg是一个具有挑战性的数据集，其归因设置为未来的研究开辟了新的途径。

### Multi-modal, Multi-task, Multi-criteria Automatic Evaluation with Vision Language Models 
[[arxiv](https://arxiv.org/abs/2412.14613)] [[cool](https://papers.cool/arxiv/2412.14613)] [[pdf](https://arxiv.org/pdf/2412.14613)]
> **Authors**: Masanari Ohi,Masahiro Kaneko,Naoaki Okazaki,Nakamasa Inoue
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 具有视觉语言模型的多模式，多任务，多标准自动评估
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLM）在一系列多模式任务中表现出令人印象深刻的能力。但是，评估VLMS生成的文本质量的现有指标通常集中于针对特定任务的整体评估，例如图像字幕。尽管总体评估对于任何任务都是必不可少的，但优先级的标准可能会根据任务而有所不同，这使得当前指标适应多任务方案的挑战。为了解决这一限制，我们提出了HarmoniceVal，这是一种无参考的综合评估度量标准，汇总了标准分数以自下而上的方式产生总体得分。此外，我们构建了多任务多标准人类评估（MMHE）数据集，该数据集包括在四个多模式任务中的18,000个专家人类判断。我们的实验表明，与传统指标相比，HarmoniceVal与人类判断的相关性更高，同时为每个标准提供数值得分。

### Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation: A Multimodal Generative AI Approach 
[[arxiv](https://arxiv.org/abs/2412.17255)] [[cool](https://papers.cool/arxiv/2412.17255)] [[pdf](https://arxiv.org/pdf/2412.17255)]
> **Authors**: Rafid Ishrak Jahan,Heng Fan,Haihua Chen,Yunhe Feng
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 通过表情符号解释解锁跨语言分析：一种多模式生成的AI方法
- **领域**: 计算语言学,人工智能
- **摘要**: 表情符号在在线交流中变得无处不在，它是一种传达情感和装饰元素的通用媒介。他们的广泛使用超越了语言和文化障碍，增强了理解和促进更具包容性的互动。尽管现有的工作获得了对表情符号理解的宝贵见解，但尚未详细研究表情符号作为普遍情感指标（LLMS）的普遍情感指标（LLMS）的能力。我们的研究旨在调查表情符号通过LLM跨语言和文化作为可靠情绪标记的能力。我们利用Chatgpt的多模式能力来探讨表情符号各种表示的情感，并评估表情符号交流的情绪与来自32个国家 /地区收集的多语言数据集中的文本情感一致。我们的分析表明，基于LLM的表情符号交流情绪的准确性为81.43％，强调表情符号作为普遍情绪标记的重要潜力。我们还发现一种一致的趋势，即随着表情符号的数量的增加，表情符号传达的情绪的准确性会增加。结果增强了表情符号作为全球情感指标的潜力，从社交媒体平台上的跨语性和跨文化情感分析等领域提供了见解。代码：https：//github.com/responsibleailab/emoji-universal-sentiment。

### Lies, Damned Lies, and Distributional Language Statistics: Persuasion and Deception with Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.17128)] [[cool](https://papers.cool/arxiv/2412.17128)] [[pdf](https://arxiv.org/pdf/2412.17128)]
> **Authors**: Cameron R. Jones,Benjamin K. Bergen
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: 37 pages, 1 figure
- **标题**: 谎言，该死的谎言和分销语言统计：具有大语言模型的说服力和欺骗
- **领域**: 计算语言学,计算机与社会,人机交互
- **摘要**: 大型语言模型（LLMS）可以生成与人为文本一样有说服力的内容，并且能够选择性地产生欺骗性的输出。这些能力引起了人们对潜在滥用和意外后果的关注，因为这些系统变得更加广泛地部署。这篇综述综合了最近研究LLM的说服力和欺骗能力和倾向的经验工作，分析了这些能力可能引起的理论风险，并评估了提出的缓解。尽管当前的有说服力的影响相对较小，但各种机制可能会增加其影响，包括微调，多模式和社会因素。我们概述了未来研究的关键开放问题，包括说服力的AI系统如何变成，真相是否具有固有的优势，而不是虚假的固有优势，以及在实践中可能具有不同的缓解策略。

### GME: Improving Universal Multimodal Retrieval by Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2412.16855)] [[cool](https://papers.cool/arxiv/2412.16855)] [[pdf](https://arxiv.org/pdf/2412.16855)]
> **Authors**: Xin Zhang,Yanzhao Zhang,Wen Xie,Mingxin Li,Ziqi Dai,Dingkun Long,Pengjun Xie,Meishan Zhang,Wenjie Li,Min Zhang
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: 32 pages, models at https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct
- **标题**: GME：通过多模式LLMS改善通用多模式检索
- **领域**: 计算语言学,信息检索
- **摘要**: 通用多模式检索（UMR）旨在使用统一模型在各种模式上进行搜索，其中查询和候选者可以由纯文本，图像或两者组合组合。以前的工作已尝试采用多模式大语模型（MLLM），以仅使用文本数据实现UMR。但是，我们的初步实验表明，更多样化的多模式训练数据可以进一步释放MLLM的潜力。尽管具有有效性，但现有的多模式训练数据在模式方面却高度不平衡，这激发了我们开发训练数据合成管道并构建一个大型，高质量的融合模式训练数据集。基于合成训练数据，我们开发了一般的多模式嵌入器（GME），这是一种基于MLLM的密度捕捞剂。此外，我们构建了一个全面的UMR基准（UMRB）来评估我们方法的有效性。实验结果表明，我们的方法在现有的UMR方法中实现了最先进的性能。最后，我们对模型缩放，训练策略进行深入分析，并对模型和合成数据进行消融研究。

### Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.16555)] [[cool](https://papers.cool/arxiv/2412.16555)] [[pdf](https://arxiv.org/pdf/2412.16555)]
> **Authors**: Yanxu Mao,Peipei Liu,Tiehan Cui,Congying Liu,Datao You
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 分裂和征服：一种混合策略击败多式联运大语言模型
- **领域**: 计算语言学
- **摘要**: 大型语言模型（LLM）由于其强大的推理，理解和发电能力而广泛应用于社会的各个领域。但是，与这些模型相关的安全问题变得越来越严重。研究人员已经探索了越狱攻击，作为检测LLM中漏洞的一种重要方法，他们试图通过各种攻击方法诱导这些模型来产生有害内容。然而，现有的越狱方法面临许多局限性，例如过多的查询计数，越狱方式的覆盖率有限，攻击成功率低以及简单的评估方法。为了克服这些限制，本文提出了一种多模式越狱方法：JMLLM。该方法集成了多种策略，以跨文本，视觉和听觉方式进行全面的越狱攻击。此外，我们为多模式越狱研究贡献了一个新的全面数据集：Trijail，其中包括所有三种方式的越狱提示。在13个流行的LLMS上进行的Trijail数据集和基准数据集Advbench上的实验表明了高级攻击成功率和时间间接开销的大幅降低。

### Effective Context Modeling Framework for Emotion Recognition in Conversations 
[[arxiv](https://arxiv.org/abs/2412.16444)] [[cool](https://papers.cool/arxiv/2412.16444)] [[pdf](https://arxiv.org/pdf/2412.16444)]
> **Authors**: Cuong Tran Van,Thanh V. T. Tran,Van Nguyen,Truong Son Hy
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 有效的上下文建模框架在对话中识别情绪
- **领域**: 计算语言学,机器学习
- **摘要**: 对话中的情感认可（ERC）有助于更深入地理解对话中每个话语中说话者传达的情感。最近，图神经网络（GNN）证明了它们在捕获数据关系方面的优势，尤其是在上下文信息建模和多模式融合中。但是，现有方法通常难以充分捕获多种方式与对话环境之间的复杂相互作用，从而限制了它们的表现力。为了克服这些局限性，我们提出了Conxgnn，这是一种基于GNN的新型框架，旨在捕获对话中的上下文信息。 CONXGNN具有两个关键的平行模块：一个多尺度的异质图，可捕获话语对情绪变化的多种影响，以及模拟模态和话语之间多元关系的超图。这些模块的输出集成到融合层中，在该层中应用了跨模式的注意机制来产生上下文富集的表示。此外，CONXGNN通过将重新加权方案纳入损失功能来应对认识少数或语义上类似情绪类别的挑战。 IEMOCAP和MELD基准数据集的实验结果证明了我们方法的有效性，与以前的基线相比，实现了最先进的性能。

### InfoTech Assistant : A Multimodal Conversational Agent for InfoTechnology Web Portal Queries 
[[arxiv](https://arxiv.org/abs/2412.16412)] [[cool](https://papers.cool/arxiv/2412.16412)] [[pdf](https://arxiv.org/pdf/2412.16412)]
> **Authors**: Sai Surya Gadiraju,Duoduo Liao,Akhila Kudupudi,Santosh Kasula,Charitha Chalasani
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: Accepted by IEEE Big Data 2024
- **标题**: Infotech助理：信息技术网站查询的多模式对话代理
- **领域**: 计算语言学
- **摘要**: 这项试点研究介绍了Infotech Assistant的发展，Infotech Assistant是一种特定领域的多模式聊天机器人，该机器人旨在解决桥梁评估和基础设施技术中的查询。通过集成Web数据刮擦，大语言模型（LLM）和检索功能增强的生成（RAG），Infotech Assistant提供了准确且上下文相关的响应。数据，包括文本描述和图像，来自Infotechnology网站上的公开文档，并以JSON格式组织以促进有效的查询。该系统的体系结构包括基于HTML的接口和通过LLM Studio连接到Llama 3.1模型的烧瓶后端。评估结果表明，特定于域的任务的精度约为95％，高相似性得分确认了响应匹配的质量。这种抹布增强的设置使Infotech助手能够处理复杂的多模式查询，并在其响应中提供文本和视觉信息。 Infotech助理表现出强大的潜力，作为基础架构专业人员的可靠工具，在其特定于域的产出中提供了很高的准确性和相关性。

### Application of Multimodal Large Language Models in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2412.16410)] [[cool](https://papers.cool/arxiv/2412.16410)] [[pdf](https://arxiv.org/pdf/2412.16410)]
> **Authors**: Md Robiul Islam
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: 9 pages, 9 figures
- **标题**: 多模式大语模型在自主驾驶中的应用
- **领域**: 计算语言学
- **摘要**: 在这个技术进步的时代，正在实施几种尖端技术，以增强自动驾驶（AD）系统，专注于提高复杂驾驶环境中的安全性，效率和适应性。但是，广告仍然面临一些问题，包括绩效限制。为了解决这个问题，我们对实施多模式大语言模型进行了深入研究。我们构建了一个虚拟问题（VQA）数据集，以微调模型，并解决了MLLM在AD上的性能不佳的问题。然后，我们通过场景理解，预测和决策来分解广告决策过程。思想链已被用来使决策更加完美。我们对自主驾驶的实验和详细分析使MLLM对AD的重要性有多大的了解。

### Experimenting with Multi-modal Information to Predict Success of Indian IPOs 
[[arxiv](https://arxiv.org/abs/2412.16174)] [[cool](https://papers.cool/arxiv/2412.16174)] [[pdf](https://arxiv.org/pdf/2412.16174)]
> **Authors**: Sohom Ghosh,Arnab Maji,N Harsha Vardhan,Sudip Kumar Naskar
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-23
> **comment**: Dataset: https://huggingface.co/datasets/sohomghosh/Indian_IPO_datasets Codes: https://github.com/sohomghosh/Indian_IPO
- **标题**: 尝试多模式信息以预测印度IPO的成功
- **领域**: 计算语言学,普通经济学
- **摘要**: 随着印度经济的持续增长，首次公开产品（IPO）已成为流行的投资途径。随着现代技术简化投资，越来越多的投资者有兴趣在订阅IPO时做出数据驱动的决策。在本文中，我们描述了一种基于机器学习和基于自然语言处理的方法，用于估计IPO是否成功。我们已经广泛研究了IPO申请招股说明书，宏观经济因素，市场状况，灰色市场价格等对IPO成功的各种事实的影响。我们创建了两个与印度公司IPO有关的新数据集。最后，我们研究了如何使用多种方式（文本，图像，数字和分类特征）的信息来估计IPO上市当天的开放，高和收盘价的方向和定价低价。

### Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling 
[[arxiv](https://arxiv.org/abs/2412.15995)] [[cool](https://papers.cool/arxiv/2412.15995)] [[pdf](https://arxiv.org/pdf/2412.15995)]
> **Authors**: Maximillian Chen,Ruoxi Sun,Sercan Ö. Arık
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: 22 pages, 6 figures, 14 tables
- **标题**: 以数据为中心的改进，以增强口语对话建模中的多模式理解
- **领域**: 计算语言学,人工智能,声音,音频和语音处理
- **摘要**: 在不同的现实世界应用中，对话助理越来越受欢迎，这突出了对高级多模式语音建模的需求。语音作为一种自然的交流方式，编码了富用户特定的特征，例如说话率和音调，这对于有效互动至关重要。我们的工作介绍了一种以数据为中心的自定义方法，以有效地增强对话性语音建模中的多模式理解。我们贡献的核心是一种新型的多任务学习范式，涉及设计辅助任务以利用少量语音数据。我们的方法在口语平面基准上实现了最先进的性能，仅使用10％的培训数据和开放式模型，为以音频为中心的会话建模建立了强大而有效的框架。我们还介绍了Ask-QA，这是第一个用于模棱两可的用户请求和动态评估输入的多转向对话的数据集。代码和数据即将到来。

### Error-driven Data-efficient Large Multimodal Model Tuning 
[[arxiv](https://arxiv.org/abs/2412.15652)] [[cool](https://papers.cool/arxiv/2412.15652)] [[pdf](https://arxiv.org/pdf/2412.15652)]
> **Authors**: Barry Menglong Yao,Qifan Wang,Lifu Huang
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: 16 pages, 6 figures
- **标题**: 错误驱动数据有效的大型多模型调整
- **领域**: 计算语言学
- **摘要**: 大型多模型模型（LMM）在众多学术基准中表现出令人印象深刻的表现。但是，微调对于在下游任务上实现令人满意的性能仍然至关重要，而特定于任务的调整样品通常不容易获得或昂贵且耗时。为了解决这个问题，我们提出了一个错误驱动的数据有效调整框架，该框架旨在有效地将通用LMMS适应新出现的任务，而无需任何特定于任务的培训样本。在我们的方法中，首先在目标任务的少量验证集上评估了一个通用的LMM，然后是一个更强大的模型，然后用作教师模型，确定了学生模型的推理步骤中的错误步骤，并分析了其能力差距，从完全解决目标任务中分析了其能力差距。基于这些差距，将有针对性的培训样本从现有的任务不合时宜的数据集进一步检索，以调整学生模型并将其定制为目标任务。我们对三个不同的训练量表和七个任务进行了广泛的实验，这表明我们的训练范式显着有效地改善了LMM在下游任务上的表现，达到平均绩效提高了7.01％。

### Ensuring Consistency for In-Image Translation 
[[arxiv](https://arxiv.org/abs/2412.18139)] [[cool](https://papers.cool/arxiv/2412.18139)] [[pdf](https://arxiv.org/pdf/2412.18139)]
> **Authors**: Chengpeng Fu,Xiaocheng Feng,Yichong Huang,Wenshuai Huo,Baohang Li,Zhirui Zhang,Yunfei Lu,Dandan Tu,Duyu Tang,Hui Wang,Bing Qin,Ting Liu
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 确保图像翻译的一致性
- **领域**: 计算语言学
- **摘要**: 图像机器的翻译任务涉及翻译嵌入图像中的文本，并以图像格式呈现翻译结果。尽管此任务在诸如电影海报翻译和日常场景图像翻译之类的各种情况下都有许多应用，但现有方法经常在整个过程中忽略一致性的方面。我们建议需要在此任务中维护两种类型的一致性：翻译一致性和图像产生一致性。前者需要在翻译过程中包含图像信息，而后者涉及保持文本图像和原始图像的样式之间的一致性，从而确保背景完整性。为了满足这些一致性要求，我们引入了一个名为HCIIT的新颖的两阶段框架（高矛盾内图像翻译），该框架涉及在第一阶段使用多模式的多语言大语言模型的文本图像翻译，并在第二阶段进行扩散模型进行反补偿。在第一阶段使用思想学习链来增强模型在翻译过程中利用图像信息的能力。随后，针对风格持续的文本图像生成训练的扩散模型可确保图像中的文本样式统一性，并保留背景细节。一个包含400,000个风格的伪文本图像对的数据集策划了模型培训。在精选的测试集和真实图像测试集上获得的结果验证了我们框架在确保一致性和产生高质量翻译图像方面的有效性。

### Diving into Self-Evolving Training for Multimodal Reasoning 
[[arxiv](https://arxiv.org/abs/2412.17451)] [[cool](https://papers.cool/arxiv/2412.17451)] [[pdf](https://arxiv.org/pdf/2412.17451)]
> **Authors**: Wei Liu,Junlong Li,Xiwen Zhang,Fan Zhou,Yu Cheng,Junxian He
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Project Page: https://mstar-lmm.github.io
- **标题**: 潜入多模式推理的自我发展训练
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 推理能力对于大型多模型（LMM）至关重要。在没有多模式链的注释数据的情况下，该模型从自己的产出中学到的自我发展培训已成为增强推理能力的有效方法。尽管使用日益增长，但对自我不断发展的培训的全面理解，尤其是在多模式推理的背景下仍然有限。在本文中，我们深入研究了多模式推理的自我发展训练的复杂性，指出了三个关键因素：训练方法，奖励模型和及时变化。我们系统地检查每个因素，并探讨各种配置如何影响培训的有效性。我们的分析为每个因素提供了一系列最佳实践，旨在优化多模式推理。此外，我们探讨了训练期间的自我进化动态以及自动平衡机制在提高性能中的影响。经过所有调查，我们提出了在多模式推理中进行自我不断发展培训的最终食谱，将这些设计选择封装在我们称为MSTAR的框架中（多模式的自我不断发展的推理培训），对于在各种基准上具有不同尺寸的模型通常是有效的，例如在各种基准上进行各种基准，例如超越预先播放的人类的多个多型人类的概述，以实现5个多模型，以实现5个多模型，以实现5个多模型的推理。 minicpm-v-2.5（8b），phi-3.5-Vision（4b）和internvl2（2b）。我们认为，这项研究填补了对多模式推理的自我发展培训的理解，并为未来的研究提供了强大的框架。我们的政策和奖励模型以及收集的数据被发布，以促进多模式推理中的进一步研究。

### Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding 
[[arxiv](https://arxiv.org/abs/2412.17295)] [[cool](https://papers.cool/arxiv/2412.17295)] [[pdf](https://arxiv.org/pdf/2412.17295)]
> **Authors**: Yueqian Wang,Xiaojun Meng,Yuxuan Wang,Jianxin Liang,Qun Liu,Dongyan Zhao
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Published at AAAI 2025
- **标题**: Friends-MMC：用于多模式多方对话的数据集理解
- **领域**: 计算语言学
- **摘要**: 多模式多方对话（MMC）是一个研究较少但重要的研究主题，因为它非常适合现实世界中的情况，因此可能具有更广泛使用的应用程序。与传统的多模式对话相比，MMC需要更强的以角色为中心的理解能力，因为在视觉和文本上下文中都出现了许多对话者。为了促进该问题的研究，我们在本文中介绍了Friends-MC，这是一个MMC数据集，其中包含24,000多种独特的话语与视频上下文。为了探索以角色为中心的对话的理解，我们还注释了每种话语的说话者，视频中出现的面孔的名称和界限。基于此Friends MMC数据集，我们进一步研究了两个基本的MMC任务：对话说话者的识别和对话响应预测，它们都具有视频或图像作为视觉上下文的多方性质。对于对话讲话者的识别，我们证明了现有方法（例如预训练模型）的效率低下，并提出了一种简单而有效的基线方法，该方法利用优化求解器利用两种模式的上下文来实现更好的性能。为了进行对话响应预测，我们在Friend-MMC上微调了生成对话模型，并分析了说话者信息的好处。该代码和数据集可在https://github.com/yellow-binary-tree/friends-mmc上公开获取，因此，在理解对话时，我们呼吁更多地关注扬声器信息。

### Intra- and Inter-modal Context Interaction Modeling for Conversational Speech Synthesis 
[[arxiv](https://arxiv.org/abs/2412.18733)] [[cool](https://papers.cool/arxiv/2412.18733)] [[pdf](https://arxiv.org/pdf/2412.18733)]
> **Authors**: Zhenqi Jia,Rui Liu
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: Accepted by ICASSP 2025
- **标题**: 对话语音综合的内部和模式间环境相互作用建模
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 会话语音综合（CSS）旨在有效地采用多模式对话历史记录（MDH），以适当的对话韵律来生成语音，以实现目标话语。 CSS的主要挑战是建模MDH与目标话语之间的相互作用。请注意，MDH中的文本和语音方式具有自己独特的影响，它们相互补充以对目标话语产生全面的影响。以前的工作未明确对这种模式内和模式间相互作用进行建模。为了解决这个问题，我们提出了一个新的模式内和模式上环境相互作用方案的CSS系统，称为III-CSS。具体而言，在训练阶段，我们将MDH与目标话语中的文本和语音方式相结合，以获得四种模态组合，包括历史文本次文本，历史语音 - 次语语音，历史文本次文本隔壁语音和历史语音 - 隔壁文本。然后，我们设计了两个基于学习的模式内和两个模式间相互作用模块，以深入学习模式内和模式间环境相互作用。在推理阶段，我们采用MDH并采用经过训练的交互模块来完全推断目标话语文本内容的语音韵律。 DailyTalk数据集上的主观和客观实验表明，III-CS在韵律表达方面的表现优于高级基准。代码和语音样本可在https://github.com/ai-s2-lab/i3css上找到。

### Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2412.18619)] [[cool](https://papers.cool/arxiv/2412.18619)] [[pdf](https://arxiv.org/pdf/2412.18619)]
> **Authors**: Liang Chen,Zekun Wang,Shuhuai Ren,Lei Li,Haozhe Zhao,Yunshui Li,Zefan Cai,Hongcheng Guo,Lei Zhang,Yizhe Xiong,Yichi Zhang,Ruoyu Wu,Qingxiu Dong,Ge Zhang,Jian Yang,Lingwei Meng,Shujie Hu,Yulong Chen,Junyang Lin,Shuai Bai,Andreas Vlachos,Xu Tan,Minjia Zhang,Wen Xiao,Aaron Yee, et al. (2 additional authors not shown)
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-25
> **comment**: 69 papes, 18 figures, repo at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction
- **标题**: 对多模式智能的下一步预测：一项全面调查
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习,多媒体,音频和语音处理
- **摘要**: 在自然语言处理中的语言建模基础上，隔壁预测（NTP）已演变为一个多功能的培训目标，以实现各种方式的机器学习任务，从而取得了相当大的成功。随着大型语言模型（LLMS）在文本模式中统一理解和生成任务，最近的研究表明，来自不同模式的任务也可以有效地封装在NTP框架中，将多模式信息转换为代币并预测下一个给定上下文的下一个。这项调查介绍了一项全面的分类学，该分类法统一了通过NTP镜头在多模式学习中统一的理解和产生。提出的分类法涵盖了五个关键方面：多模式令牌化，MMNTP模型体系结构，统一任务表示，数据集\＆评估以及开放挑战。这种新的分类法旨在帮助研究人员探索多模式智能。相关的GitHub存储库收集最新论文和存储库，请访问https://github.com/lmm101/awesome-multimodal-next-token-prediction

### SAFE-MEME: Structured Reasoning Framework for Robust Hate Speech Detection in Memes 
[[arxiv](https://arxiv.org/abs/2412.20541)] [[cool](https://papers.cool/arxiv/2412.20541)] [[pdf](https://arxiv.org/pdf/2412.20541)]
> **Authors**: Palash Nandi,Shivam Sharma,Tanmoy Chakraborty
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: 28 pages, 15 figures, 6 tables
- **标题**: 安全局：模因中强大的仇恨言语检测的结构化推理框架
- **领域**: 计算语言学,计算机与社会
- **摘要**: 模因充当共享敏感思想的隐秘工具，通常需要上下文知识来解释。这使得调节多模式模因具有挑战性，因为现有作品要么缺少细微仇恨类别的高质量数据集，要么依靠低质量的社交媒体视觉效果。在这里，我们策划了两个新颖的多模式仇恨言论数据集MHS和MHS-CON，它们分别在常规和混杂场景中捕获了细粒度的可恨抽象。我们根据几个竞争基线对这些数据集进行基准测试。此外，我们介绍了Safe-Meme（结构化推理框架），这是一种新型的多模式基于基础的框架，采用了Q＆A风格的推理（Safe-Meme-QA）和分层分类（Safe-Meme-H），以在模因中启用强大的仇恨言语检测。 Safe-Meme-QA的表现优于现有基线，在MHS和MHS-CON上的平均提高约为5％和4％。相比之下，Safe-Meme-H在MHS中的平均提高6％，而在MHS-CON中仅表现仅超过多模式基线。我们表明，在常规的细粒仇恨模因检测中，对安全h-H中的单层适配器进行微调胜过完全微调的模型。但是，使用问答设置的完全微调方法更有效地处理混淆案例。我们还系统地检查了错误案例，为拟议的结构化推理框架的鲁棒性和局限性提供了有价值的见解，用于分析可恶的模因。

### Utilizing Multimodal Data for Edge Case Robust Call-sign Recognition and Understanding 
[[arxiv](https://arxiv.org/abs/2412.20467)] [[cool](https://papers.cool/arxiv/2412.20467)] [[pdf](https://arxiv.org/pdf/2412.20467)]
> **Authors**: Alexander Blatt,Dietrich Klakow
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 利用多模式数据进行边缘案例强大的呼叫识别和理解
- **领域**: 计算语言学
- **摘要**: 在各种场景中，基于机器学习的助手系统必须坚固。这对于空中流量控制（ATC）域特别构成。在边缘案例中，架构的鲁棒性尤为明显，例如嘈杂的ATC记录或由于剪辑录音而导致的部分成绩单引起的高单词错误率（WER）成绩单。为了增加呼叫符号识别和理解（CRU）（ATC语音处理中的核心任务）的边缘案例鲁棒性（CRU），我们提出了多模式呼叫仪命令命令恢复模型（CCR）。 CCR架构导致边缘案例性能的提高高达15％。我们在第二个提议的架构Callbert中证明了这一点。具有较少参数的CRU模型可以比CRU的最新情况更快地进行微调，并且在微调过程中更强大。此外，我们证明，对边缘案例进行优化会导致在广泛的操作范围内的准确性明显更高。

### Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.19449)] [[cool](https://papers.cool/arxiv/2412.19449)] [[pdf](https://arxiv.org/pdf/2412.19449)]
> **Authors**: Shuo Wang,Chihang Wang,Jia Gao,Zhen Qi,Hongye Zheng,Xiaoxuan Liao
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: 4 pages
- **标题**: 基于特征对齐的知识蒸馏，以有效地压缩大语言模型
- **领域**: 计算语言学
- **摘要**: 这项研究提出了一种基于大语言模型和特征对齐方式的知识蒸馏算法，旨在有效地将大型预训练模型的知识转移到轻质的学生模型中，从而在保持高模型性能的同时降低计算成本。与传统的软标签蒸馏方法不同，此方法引入了多层特征对齐策略，以深层对齐教师模型的中间特征和注意力机制和学生模型，从而最大程度地保留了教师模型的语义表达能力和上下文建模能力。在方法设计方面，构建了多任务损失函数，包括功能匹配损失，注意对准损失和输出分配匹配损耗，以确保通过关节优化的多级信息传输。对实验对胶水数据集和各种自然语言处理任务进行了全面评估。结果表明，所提出的模型在评估指标（例如困惑，bleu，rouge和cer）方面非常接近最新的GPT-4模型。同时，它远远超过了Deberta，XLNet和GPT-3等基线模型，显示出显着的性能提高和计算效率优势。研究结果表明，功能比对蒸馏策略是一种有效的模型压缩方法，可以在维持模型功能的同时显着降低计算开销和存储要求。可以进一步扩展未来的研究，以自我监督的学习，跨模式特征对齐和多任务转移学习的方式进行扩展，以提供更灵活，有效的解决方案，以用于部署和优化深度学习模型。

### RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting 
[[arxiv](https://arxiv.org/abs/2412.18826)] [[cool](https://papers.cool/arxiv/2412.18826)] [[pdf](https://arxiv.org/pdf/2412.18826)]
> **Authors**: Yilei Jiang,Yingshui Tan,Xiangyu Yue
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: RAPGUARD：通过理由吸引的防御提示来保护多模式的大型语言模型
- **领域**: 计算语言学
- **摘要**: 尽管多模式大型语言模型（MLLM）在视觉推理方面取得了显着进步，但与仅关注文本的模型相比，它们也更容易产生有害内容。现有的防御提示技术依赖于静态的，统一的安全指南，该指南未能说明不同多模式上下文中固有的特定风险。为了解决这些局限性，我们提出了Rapguard，这是一个新颖的框架，该框架使用多模式链的推理来动态生成特定方案的安全提示。 Rapguard通过调整提示来提高每个输入的独特风险，从而有效地减轻有害产出，同时保持良性任务上的高性能，从而提高安全性。我们对多个MLLM基准测试的实验结果表明，Rapguard实现了最先进的安全性能，大大降低了有害内容而不会降低响应质量。

### GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.21036)] [[cool](https://papers.cool/arxiv/2412.21036)] [[pdf](https://arxiv.org/pdf/2412.21036)]
> **Authors**: Shangyu Xing,Changhao Xiang,Yuteng Han,Yifan Yue,Zhen Wu,Xinyu Liu,Zhangtai Wu,Fei Zhao,Xinyu Dai
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: No comments
- **标题**: Gepbench：评估多模式模型的基本几何感知
- **领域**: 计算语言学
- **摘要**: 多模式大语模型（MLLM）在整合视觉和语言理解方面取得了重大进展。现有的基准通常集中在高级语义能力上，例如场景理解和视觉推理，但通常忽略了至关重要的基础能力：几何感知。几何感知涉及了解几何形状，结构和空间关系，这对于支持高级语义任务至关重要。尽管具有重要意义，但在当前的MLLM研究中，这种能力仍然没有得到充实。为了解决这一差距，我们介绍了Gepbench，这是一种新颖的基准测试，旨在评估MLLM的几何感知能力。我们广泛的评估表明，当前最新的MLLM在几何感知任务中表现出明显的缺陷。此外，我们表明，经过GEPBENCH数据训练的模型表明，对广泛的基准任务进行了重大改进，突出了几何感知在启用高级多模式应用中的关键作用。我们的代码和数据集将公开可用。

### Plancraft: an evaluation dataset for planning with LLM agents 
[[arxiv](https://arxiv.org/abs/2412.21033)] [[cool](https://papers.cool/arxiv/2412.21033)] [[pdf](https://arxiv.org/pdf/2412.21033)]
> **Authors**: Gautier Dagan,Frank Keller,Alex Lascarides
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: No comments
- **标题**: Plancraft：用于与LLM代理计划计划的评估数据集
- **领域**: 计算语言学,人工智能
- **摘要**: 我们提出Plancraft，这是一个用于LLM代理的多模式评估数据集。 Plancraft基于Minecraft手工制作GUI具有仅文本和多模式界面。我们包括Minecraft Wiki，以评估工具使用和检索增强发电（RAG），以及Oracle Planner和Oracle Rag Information提取器，以消融现代代理体系结构的不同组件。为了评估决策，Plancraft还包括一个有意无法解决的示例子集，提供了一个现实的挑战，不仅要求代理人完成任务，而且还要求它们是否可以解决。我们基于开源和封闭源LLM和策略对我们的任务进行了比较，并将其绩效与手工制作的计划者进行比较。我们发现LLM和VLM在Plancraft引入的计划问题上挣扎，我们就如何提高其能力提出了建议。

## 密码学和安全(cs.CR:Cryptography and Security)

该领域共有 4 篇论文

### VLSBench: Unveiling Visual Leakage in Multimodal Safety 
[[arxiv](https://arxiv.org/abs/2411.19939)] [[cool](https://papers.cool/arxiv/2411.19939)] [[pdf](https://arxiv.org/pdf/2411.19939)]
> **Authors**: Xuhao Hu,Dongrui Liu,Hao Li,Xuanjing Huang,Jing Shao
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: VLSBENCH：在多模式安全中揭示视觉泄漏
- **领域**: 密码学和安全,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式大语模型（MLLM）的安全问题逐渐成为各种应用中的重要问题。令人惊讶的是，以前的作品表明了一种违反直觉现象，该现象使用文本划分的MLLM与经过图像文本对训练的MLLM进行可比的安全性能。为了解释这种违反直觉现象，我们发现了现有多模式安全基准中的视觉安全信息泄漏（VSIL）问题，即，在文本查询中揭示了图像中潜在的风险和敏感内容。这样，MLLM可以根据文本查询轻松拒绝这些敏感的文本图像查询。但是，在现实世界中，没有VSIL的图像文本对很常见，并且被现有的多模式安全基准忽略了。为此，我们构建了多模式的视觉无泄漏安全基准（VLSBench），以防止使用2.4k图像文本对从图像到文本查询的视觉安全性泄漏。实验结果表明，VLSBENCH对包括Llava，Qwen2-Vl，Llama3.2-Vision和GPT-4O在内的开源和封闭源MLLM构成了重大挑战。这项研究表明，文本对齐足以用于与VSIL的多模式安全场景，而多模式比对是无VSIL的多模式安全场景的更有希望的解决方案。请在以下网址查看我们的代码和数据：https：//hxhcreate.github.io/vlsbench.github.io/

### LUMIA: Linear probing for Unimodal and MultiModal Membership Inference Attacks leveraging internal LLM states 
[[arxiv](https://arxiv.org/abs/2411.19876)] [[cool](https://papers.cool/arxiv/2411.19876)] [[pdf](https://arxiv.org/pdf/2411.19876)]
> **Authors**: Luis Ibanez-Lissen,Lorena Gonzalez-Manzano,Jose Maria de Fuentes,Nicolas Anciaux,Joaquin Garcia-Alfaro
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: Lumia：单峰和多模式成员推理攻击的线性探测利用内部LLM状态
- **领域**: 密码学和安全,人工智能
- **摘要**: 大型语言模型（LLMS）越来越多地用于各种应用中，但对成员推理的担忧并行增长。以前的努力集中在黑色到灰色的盒子模型上，因此忽略了内部LLM信息的潜在收益。为了解决这个问题，我们建议使用线性探针（LP）作为一种通过检查LLMS的内部激活来检测成员推理攻击（MIA）的方法。我们称为Lumia的方法将LPS逐层应用，以获取模型内部工作的细粒度数据。我们在几个模型体系结构，大小和数据集中测试了此方法，包括单峰和多模式任务。在单峰MIA中，Lumia比以前的技术在曲线（AUC）下的平均增益为15.71％。值得注意的是，Lumia在65.33％的病例中达到了AUC> 60％ - 相对于最新状态，增量为46.80％。此外，我们的方法揭示了关键的见解，例如最可检测到的MIA的模型层。在多模型模型中，LP表示视觉输入可以显着有助于检测MIA-在85.90％的实验中，AUC> 60％达到60％。

### Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.05934)] [[cool](https://papers.cool/arxiv/2412.05934)] [[pdf](https://arxiv.org/pdf/2412.05934)]
> **Authors**: Ma Teng,Jia Xiaojun,Duan Ranjie,Li Xinfeng,Huang Yihao,Chu Zhixuan,Liu Yang,Ren Wenqi
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 启发式诱导的多模式风险分销越狱攻击多模式大型语言模型
- **领域**: 密码学和安全,人工智能
- **摘要**: 随着多模式大语言模型（MLLM）的快速发展，对其安全性的担忧越来越多地吸引了学术界和工业的关注。尽管MLLM容易受到越狱攻击的影响，但是设计有效的多模式越狱攻击却带来了独特的挑战，尤其是考虑到商业模型中各种方式实施的独特保护措施。以前的作品将风险集中在单一模式中，从而导致越狱表现有限。在本文中，我们提出了一种启发式诱导的多模式风险分配越狱攻击方法，称为HIMRD，该方法由两个要素组成：多模式风险分配策略和启发式诱导的搜索策略。多模式的风险分配策略用于分割多种模式的有害指令，以有效规避MLLM的安全保护。启发式诱导的搜索策略确定了两种提示：增强理解的提示，这有助于MLLM重建恶意提示，以及诱导的提示，这增加了肯定产量而不是拒绝的可能性，从而实现了成功的越狱攻击。广泛的实验表明，这种方法有效地发现了MLLM中的漏洞，在七个流行的开源MLLM中达到了90％的平均攻击成功率，而在三个流行的封闭源MLLM中，平均攻击​​成功率约为68％。我们的代码很快就会发布。警告：本文包含令人反感和有害的例子，建议读者酌处权。

### Adversarial Hubness in Multi-Modal Retrieval 
[[arxiv](https://arxiv.org/abs/2412.14113)] [[cool](https://papers.cool/arxiv/2412.14113)] [[pdf](https://arxiv.org/pdf/2412.14113)]
> **Authors**: Tingwei Zhang,Fnu Suya,Rishi Jha,Collin Zhang,Vitaly Shmatikov
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 多模式检索中的对抗性枢纽
- **领域**: 密码学和安全,信息检索
- **摘要**: 集线器是高维矢量空间中的现象，其中来自自然分布的单点异常接近许多其他点。这是信息检索的一个众所周知的问题，它导致某些项目意外（错误）似乎与许多查询有关。在本文中，我们研究了攻击者如何利用枢纽将多模式检索系统中的任何图像或音频输入转换为对抗中心。对抗枢纽可用于注入通用的对抗内容（例如，垃圾邮件），这些内容将以响应数千个不同的查询以及针对与特定攻击者选择概念相关的查询的目标攻击而检索。我们提出了一种创建对抗性枢纽并评估基准多模式检索数据集和图像到图像检索系统的方法，该方法基于Pinecone的教程，Pinecone是一个流行的矢量数据库。例如，在文本捕捉到图像检索中，单个对抗集线器被检索为25,000个测试查询中超过21,000个最相关的图像最相关的图像（相比之下，最常见的天然枢纽是仅对102个查询的前1个响应）。我们还研究了缓解自然中心的技术是否是针对对抗枢纽的有效防御，并表明它们对针对与特定概念相关的查询的枢纽无效。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 457 篇论文

### Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation 
[[arxiv](https://arxiv.org/abs/2411.19951)] [[cool](https://papers.cool/arxiv/2411.19951)] [[pdf](https://arxiv.org/pdf/2411.19951)]
> **Authors**: Shukang Yin,Chaoyou Fu,Sirui Zhao,Yunhang Shen,Chunjiang Ge,Yan Yang,Zuwei Long,Yuhan Dai,Yongdong Luo,Haoyu Cao,Tong Xu,Xing Sun,Caifeng Shan,Ran He,Enhong Chen
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: Project page: https://github.com/VITA-MLLM/Sparrow
- **标题**: 麻雀：带有文本对图像增强的数据效率的视频-LLM
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 近年来，多模式大语言模型（MLLM）在视觉理解领域的成功取得了成功。这些模型的成功在很大程度上可以归因于主要的缩放定律，该定律定律指出，较大的参数大小和数据量有助于更好的性能。值得注意的是，数据缩放主要由自动数据管道提供动力，该数据管道围绕LLM的自我指导。范式已被认为是理所当然的一段时间，但是长期以来对这些数据扩展的有效性的研究已被忽略了很长时间。在这种情况下，这项工作通过合成数据重新缩放，并专注于从以数据为中心的角度开发视频LLMS。我们的主要研究方法是通过视频数据微调预训练的图像插件，并通过数据扩展研究学习效率。我们的初步实验的结果表明，当简单地扩展视频数据样本时，学习效率现象低，通过我们的探测，这可以归因于缺乏教学多样性。针对这个问题，我们提出了一种称为Sparrow的数据增强方法，该方法从纯文本指令数据中综合了类似视频的样本。将这些综合样品与视频数据混合在一起，可实现更有效的训练方案。通过全面的实验，我们证明了我们提出的方法可以达到与经过更多样品训练的基线相当甚至优越的性能。同时，我们发现合并这些合成样本可以在不使用长视频数据培训的情况下提高视频理解的性能。代码和数据示例可在https://github.com/vita-mllm/sparrow上找到。

### SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2411.19860)] [[cool](https://papers.cool/arxiv/2411.19860)] [[pdf](https://arxiv.org/pdf/2411.19860)]
> **Authors**: Philipp Wolters,Johannes Gilg,Torben Teepe,Fabian Herzog,Felix Fent,Gerhard Rigoll
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: 18 pages, 11 figures
- **标题**: SPARC：3D对象检测的稀疏雷达相机融合
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在这项工作中，我们提出了SPARC，这是一种用于3D感知的新型稀疏融合变压器，将多视图像语义与雷达和摄像机点特征集成在一起。雷达和相机方式的融合已成为自主驾驶系统的有效感知范式。尽管传统的方法利用了密集的鸟类视图（BEV）的架构进行深度估算，但通过以对象为中心的方法论，基于当代查询的变压器在仅相机检测中表现出色。但是，由于隐式深度建模，这些基于查询的方法在假阳性检测和定位精度中表现出局限性。我们通过三个关键贡献解决了这些挑战：（1）用于跨模式特征对准的稀疏叶片融合（SFF），（2）用于精确对象定位的范围 - 适应性雷达聚集（RAR），以及（3）局部自我关注（LSA）用于聚焦查询聚集。与需要计算密集型BEV网格渲染的现有方法相反，SPARC直接在编码点特征上运行，从而实现了效率和准确性的实质性提高。对Nuscenes和Truckscenes基准的经验评估表明，SPARC明显优于现有的基于BEV的密集和基于查询的稀疏检测器。我们的方法可实现67.1 NDS和63.1 AMOTA的最先进性能指标。代码和预估计的模型可在https://github.com/phi-wol/sparc上找到。

### MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks 
[[arxiv](https://arxiv.org/abs/2411.19786)] [[cool](https://papers.cool/arxiv/2411.19786)] [[pdf](https://arxiv.org/pdf/2411.19786)]
> **Authors**: Yiming Wu,Wei Ji,Kecheng Zheng,Zicheng Wang,Dong Xu
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: Five figures, six tables
- **标题**: MOTE：多生成任务的学习运动文本扩散模型
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 最近，由于诸如DeNoising扩散模型和大型语言模型之类的激励性生成模型，人类运动分析经历了很大的改进。而现有方法主要集中于通过文本描述生成动议并忽略倒数任务。在本文中，我们提出了〜\ textbf {Mote}，这是一个统一的多模式模型，可以通过同时学习运动和文本的边际，条件和联合分布来处理各种任务。 MOTE使我们能够通过简单地修改输入上下文来处理配对的文本生成，运动字幕和文本驱动的运动生成。具体而言，MOTE由三个组件组成：运动编码器 - 编码器（MED），文本编码器码头（TED）和Moti-On-Text扩散模型（MTDM）。特别是，对MED和TED进行了训练，用于提取潜在的嵌入，然后分别从提取的嵌入中重建运动序列和文本描述。另一方面，MTDM在输入上下文上执行了迭代的降解过程，以处理各种任务。基准数据集的实验结果证明了我们提出的方法在文本到动作生成和运动字幕上的竞争性能方面的出色性能。

### LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos 
[[arxiv](https://arxiv.org/abs/2411.19772)] [[cool](https://papers.cool/arxiv/2411.19772)] [[pdf](https://arxiv.org/pdf/2411.19772)]
> **Authors**: Tiantian Geng,Jinrui Zhang,Qingni Wang,Teng Wang,Jinming Duan,Feng Zheng
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: 18 pages, 15 figures
- **标题**: Longvale：远见 - 语言 - 事实的基准测试，以时刻的全视频感知到时间感知。
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习,多媒体
- **摘要**: 尽管在视频理解方面取得了令人印象深刻的进步，但大多数努力仍限于粗粒或仅视觉视频任务。但是，现实世界中的视频包含Omni-Modal信息（视觉，音频和语音），其中包含一系列构成凝聚力故事情节的事件。缺乏具有精细元素事件注释的多模式视频数据和手动标记的高成本是全面的Omni-Modosity视频感知的主要障碍。为了解决这一差距，我们提出了一条自动管道，该管道由高质量的多模式视频滤波，语义相干 - 模式事件边界检测和跨模式相关性事件事件字幕组成。通过这种方式，我们介绍了Longvale，这是有史以来的第一个愿景 - 语言事件，理解包括105K Omni-Modal事件的基准，具有精确的时间边界和8.4k高质量的长视频中的详细关系和详细的关系感知字幕。此外，我们构建了一个基线，该基线利用Longvale启用视频大语模型（LLMS）首次获得Omni-Modation File-Graine Timer Video Gealble。广泛的实验证明了Longvale在促进全面的多模式视频理解方面的有效性和巨大潜力。

### SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical VQA Tasks 
[[arxiv](https://arxiv.org/abs/2411.19688)] [[cool](https://papers.cool/arxiv/2411.19688)] [[pdf](https://arxiv.org/pdf/2411.19688)]
> **Authors**: Kim-Celine Kahl,Selen Erkan,Jeremias Traub,Carsten T. Lüth,Klaus Maier-Hein,Lena Maier-Hein,Paul F. Jaeger
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: Sure-VQA：在医疗VQA任务中对鲁棒性评估的系统理解
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 视觉语言模型（VLM）在医疗任务中具有巨大的潜力，例如视觉问答（VQA），它们可以充当患者和临床医生的互动助手。然而，他们对看不见数据的分配变化的稳健性仍然是安全部署的关键问题。评估这种鲁棒性需要一个受控的实验设置，该设置允许对模型的行为进行系统的见解。但是，我们证明当前的设置无法提供足够的彻底评估，从而限制了它们准确评估模型鲁棒性的能力。为了解决这一差距，我们的工作介绍了一个名为Sure-VQA的新颖框架，以三个关键要求为中心，以克服当前的陷阱，并系统地分析VLMS的鲁棒性：1）由于合成转变的鲁棒性不一定会转化为现实的变化，因此应在现实世界上衡量稳健性，以至于在现实世界上均可衡量VED的固有数据。 2）传统的令牌匹配指标通常无法捕获基本语义，因此需要使用大语言模型（LLMS）才能进行更准确的语义评估； 3）模型性能通常由于缺失理智基准而缺乏可解释性，因此应报告有意义的基线，以评估对VLM的多模式影响。为了证明该框架的相关性，我们对三个具有四种不同类型的分布变化的医学数据集的各种微调方法的鲁棒性进行了研究。我们的研究揭示了一些重要的发现：1）不利用图像数据的理智基准可以表现出色； 2）我们确认洛拉是表现最好的PEFT方法； 3）没有PEFT方法始终超过其他人的稳健性。代码可在https://github.com/iml-dkfz/sure-vqa上提供。

### Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings 
[[arxiv](https://arxiv.org/abs/2411.19628)] [[cool](https://papers.cool/arxiv/2411.19628)] [[pdf](https://arxiv.org/pdf/2411.19628)]
> **Authors**: Qiong Wu,Wenhao Lin,Weihao Ye,Yiyi Zhou,Xiaoshuai Sun,Rongrong Ji
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 通过动态视觉出口和经验发现加速多模式大语模型
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习,多媒体
- **摘要**: 在现有的多种语言模型（MLLM）中过度使用视觉令牌通常会表现出明显的冗余，并带来过昂贵的计算。为了了解这个问题，我们首先就MLLM的注意力行为进行了广泛的经验研究，并总结了MLLMS中的三个主要推断阶段：（i）最初很快就会完成令牌之间的早期融合。 （ii）随后进行模型内建模。 （iii）多模式推理}恢复并持续到推理结束。特别是，我们透露，视觉令牌将停止为文本令牌接收到足够的图像信息而产生明显的视觉冗余时的推理。基于这些广义观察，我们提出了一种简单而有效的方法来提高MLLM的效率，称为动态视觉出口（DYVTE）。 Dyvte使用轻质的超网络来感知文本令牌状态，并在一定层之后确定所有视觉令牌的去除，从而解决观察到的视觉冗余。为了验证VTE，我们将其应用于包括Llava，Vila，Eagle和Internvl在内的一组MLLM，并在一堆基准测试上进行了广泛的实验。实验结果不仅显示了我们VTE在提高MLLM效率方面的有效性，而且还产生了MLLM的一般建模模式，从而很好地促进了对MLLM的深入了解。我们的代码匿名在https://github.com/doubtedsteam/dyvte上发布。

### Deepfake Media Generation and Detection in the Generative AI Era: A Survey and Outlook 
[[arxiv](https://arxiv.org/abs/2411.19537)] [[cool](https://papers.cool/arxiv/2411.19537)] [[pdf](https://arxiv.org/pdf/2411.19537)]
> **Authors**: Florinel-Alin Croitoru,Andrei-Iulian Hiji,Vlad Hondru,Nicolae Catalin Ristea,Paul Irofti,Marius Popescu,Cristian Rusu,Radu Tudor Ionescu,Fahad Shahbaz Khan,Mubarak Shah
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 生成AI时代的DeepFake媒体生成和检测：调查和前景
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体,声音,音频和语音处理
- **摘要**: 随着生成建模的最新进展，Deepfake内容的现实主义一直在稳定增长，甚至达到人们经常无法在网上检测到操纵的媒体内容的地步，从而被欺骗到各种骗局中。在本文中，我们调查了深层生成和检测技术，包括该领域的最新发展，例如扩散模型和神经辐射场。我们的文献评论涵盖了所有DeepFake媒体类型，包括图像，视频，音频和多模式（视听）内容。我们根据用于更改或生成伪造内容的程序来确定各种深层效果。我们进一步构建了深层产生和检测方法的分类法，说明了使用这些方法的重要组和域的重要组。接下来，我们收集用于DeepFake检测的数据集，并在最受欢迎的数据集上提供最佳性能DeepFake检测器的更新排名。此外，我们开发了一种新型的多模式基准，以评估分布含量含量的深层探测器。结果表明，最新的检测器无法推广到未见的深泡产生器产生的深泡含量。最后，我们提出未来的方向，以获取强大而强大的深层探测器。我们的项目页面和新基准可以在https://github.com/croitorualin/biodeep上找到。

### Interleaved-Modal Chain-of-Thought 
[[arxiv](https://arxiv.org/abs/2411.19488)] [[cool](https://papers.cool/arxiv/2411.19488)] [[pdf](https://arxiv.org/pdf/2411.19488)]
> **Authors**: Jun Gao,Yongqi Li,Ziqiang Cao,Wenjie Li
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 交织模式链
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 促使经过思考链（COT）提示大型语言模型（LLMS）在获得最终答案之前产生一系列中间推理步骤。但是，当过渡到视觉模型（VLM）时，他们的仅文本原理努力表达与原始图像的细粒度关联。在本文中，我们提出了一个包含图像的多模式链，称为\ textbf {交织模式链（ICOT）}，该链产生了由配对的视觉和文本合理的顺序推理步骤，以推断最终答案。从直觉上讲，新颖的ICOT要求VLMS能够产生精细的交织模式含量，这对于当前的VLM很难实现。考虑到所需的视觉信息通常是输入图像的一部分，我们建议\ textbf {注意驱动选择（ADS）}以实现现有VLMS的ICOT。 ADS智能地插入了输入图像的区域，以生成具有可忽略的额外延迟的交织模式推理步骤。广告仅依赖VLM的注意图而无需参数化，因此它是一种可以推广到VLMS频谱的插件策略。我们应用广告以在两个流行的不同体系结构的VLM上实现ICOT。对三个基准测试的广泛评估表明，与现有的多模式COT提示方法相比，ICOT促使ICOT促使其实质性（最高14 \％）和可解释性提高。

### Effective Fine-Tuning of Vision-Language Models for Accurate Galaxy Morphology Analysis 
[[arxiv](https://arxiv.org/abs/2411.19475)] [[cool](https://papers.cool/arxiv/2411.19475)] [[pdf](https://arxiv.org/pdf/2411.19475)]
> **Authors**: Ruoqi Wang,Haitao Wang,Qiong Luo
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 有效对视觉模型的微调进行精确的星系形态分析
- **领域**: 计算机视觉和模式识别,星系天体物理学,人工智能,机器学习
- **摘要**: 星系形态分析涉及通过其形状和结构对星系进行分类。对于此任务，在大型，带注释的天文数据集上的直接培训域特异性模型有效但昂贵。相比之下，较小的天文图像集中的微调视觉基础模型更为有效，但通常会导致精度较低。为了利用这两种方法的好处并解决其缺点，我们提出了Galaxalign，这是一种新颖的方法，可以微调预培训的基础模型，以实现天文任务的高精度。具体而言，我们的方法将对比度学习架构扩展到微调中的三种数据：（1）一组代表星系形状和结构的示意图符号，（2）这些符号的文本标签，以及（3）星系图像。这样，Galaxalign不仅消除了对昂贵的预处理的需求，而且还提高了微调的有效性。关于银河系分类和相似性搜索的广泛实验表明，我们的方法通过合并特定于域的多模式知识来有效地对天文任务进行微调预训练的模型。

### ForgerySleuth: Empowering Multimodal Large Language Models for Image Manipulation Detection 
[[arxiv](https://arxiv.org/abs/2411.19466)] [[cool](https://papers.cool/arxiv/2411.19466)] [[pdf](https://arxiv.org/pdf/2411.19466)]
> **Authors**: Zhihao Sun,Haoran Jiang,Haoran Chen,Yixin Cao,Xipeng Qiu,Zuxuan Wu,Yu-Gang Jiang
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: ForgerySleuth：授权多模式大语言模型用于图像操纵检测
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 多模式的大型语言模型已为各种多模式任务解开了新的可能性。但是，它们在图像操纵检测中的潜力仍未开发。直接应用于IMD任务时，M-LLM通常会产生幻觉和过度思考的推理文本。为了解决这一点，在这项工作中，我们提出了ForgerySleuth，该ForgerySleuth利用M-LLM执行全面的线索融合，并生成分割输出，指示被篡改的特定区域。此外，我们通过固定链提示构建了伪造数据集，其中包括分析和推理文本以升级图像操纵检测任务。还引入了数据引擎以构建用于训练阶段的大规模数据集。我们的广泛实验证明了伪造分析的有效性，并表明ForgerySleuth显着超过了现有的概括，鲁棒性和解释性的方法。

### Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing 
[[arxiv](https://arxiv.org/abs/2411.19460)] [[cool](https://papers.cool/arxiv/2411.19460)] [[pdf](https://arxiv.org/pdf/2411.19460)]
> **Authors**: Hosu Lee,Junho Kim,Hyunjun Kim,Yong Man Ro
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: Project page: https://ivy-lvlm.github.io/Video-MA2MBA/
- **标题**: 一次查看每一帧：视频-MA $^2 $ MBA，以使用多轴梯度检查点有效的长格式视频理解
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 随着视频数据的规模和复杂性的不断增长，有效地处理长视频序列会带来重大挑战，这是由于内存的二次增加和与现有基于变压器的大型多模式模型（LMM）相关的计算需求的二次增加。为了解决这些问题，我们介绍了Video-MA $^2 $ MBA，这是一种新颖的体系结构，将状态空间模型（SSM）纳入MAMBA-2框架中，以取代注意力机制。这允许LMM在时间和内存需求方面线性扩展，使得处理长期视频内容的可行性。此外，我们提高了引入多轴梯度检查点（MA-GC）方法的内存效率，该方法通过仅保留多个计算轴的基本激活来策略性地管理内存。与标准梯度检查点相比，我们的方法大大减少了内存足迹。经验分析表明，视频 -  $^2 $ MBA可以处理广泛的视频序列 - 等于数百万令牌或两个小时以上连续序列，以1 fps-on单个GPU。通过维护时间动态的详细捕获，我们的模型提高了长期视频理解任务中响应的准确性和相关性，从而证明了与现有框架相比的实质优势。

### Adaptive Interactive Segmentation for Multimodal Medical Imaging via Selection Engine 
[[arxiv](https://arxiv.org/abs/2411.19447)] [[cool](https://papers.cool/arxiv/2411.19447)] [[pdf](https://arxiv.org/pdf/2411.19447)]
> **Authors**: Zhi Li,Kai Zhao,Yaqi Wang,Shuai Wang
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 通过选择引擎的多模式医学成像的自适应互动分割
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 在医学图像分析中，实现快速，高效和准确的分割对于自动诊断和治疗至关重要。尽管深度学习的最新进展显着提高了细分精度，但当前的模型通常面临适应性和概括的挑战，尤其是在处理多模式医学成像数据时。这些局限性源于成像方式与医学数据固有复杂性之间的实质性变化。为了应对这些挑战，我们建议以SAM2为基础的战略驱动的交互式分割模型（Siseg），该模型通过集成选择引擎来增强各种医学成像方式的细分性能。为了减轻内存瓶颈并优化2D图像序列推断期间的及时框架选择，我们开发了一个自动化系统，即自适应框架选择引擎（AFSE）。该系统动态选择最佳提示框架，而无需大量的先前医学知识，并通过交互式反馈机制增强了模型推理过程的解释性。我们在涵盖7种代表性医学成像方式的10个数据集上进行了广泛的实验，证明了Siseg模型在多模式任务中的强大适应性和泛化。项目页面和代码将可用：[url]。

### Libra: Leveraging Temporal Images for Biomedical Radiology Analysis 
[[arxiv](https://arxiv.org/abs/2411.19378)] [[cool](https://papers.cool/arxiv/2411.19378)] [[pdf](https://arxiv.org/pdf/2411.19378)]
> **Authors**: Xi Zhang,Zaiqiao Meng,Jake Lever,Edmond S. L. Ho
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: 30 pages, 5 figures, Adding Appendix
- **标题**: 天秤座：利用时间图像进行生物医学放射学分析
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 放射学报告生成（RRG）需要先进的医学图像分析，有效的时间推理和准确的文本生成。尽管多模式大语言模型（MLLM）与预训练的视觉编码器保持一致以增强视觉语言的理解，但大多数现有方法依赖于单像分析或基于规则的启发式方法来处理多个图像，无法完全利用多模式医学数据集中的时间信息。在本文中，我们介绍了天秤座，这是一种针对胸部X射线报告生成的时间感知的MLLM。天秤座将特定于放射学的图像编码器与新型的时间对齐连接器（TAC）相结合，旨在准确捕获和整合成对电流和先前图像之间的时间差异。模仿CXR数据集的广泛实验表明，天秤座在类似缩放的MLLM中建立了新的最新基准测试，在临床相关性和词汇准确性方面都设定了新的标准。

### Automatic Prompt Generation and Grounding Object Detection for Zero-Shot Image Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2411.19220)] [[cool](https://papers.cool/arxiv/2411.19220)] [[pdf](https://arxiv.org/pdf/2411.19220)]
> **Authors**: Tsun-Hin Cheung,Ka-Chun Fung,Songjiang Lai,Kwan-Ho Lin,Vincent Ng,Kin-Man Lam
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: Accepted to APSIPA ASC 2024
- **标题**: 自动及时生成和接地对象检测零照片图像异常检测
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 确定工业产品中的缺陷和异常是一项关键的质量控制任务。传统的手动检查方法缓慢，主观且容易出错。在这项工作中，我们建议使用多模式机器学习管道的新型零射训练方法，用于自动化工业图像异常检测，该方法由三个基础模型组成。我们的方法首先使用大型语言模型，即GPT-3。生成文本提示，描述了正常产品和异常产品的预期外观。然后，我们使用称为接地Dino的接地对象检测模型将产品定位在图像中。最后，我们使用零击图像文本匹配模型（称为夹子）将裁剪的产品图像贴片与生成的提示进行比较，以识别任何异常。我们在两个工业产品图像的数据集（即MVTEC-AD和Visa）上进行的实验证明了这种方法的有效性，在检测各种类型的缺陷和异常情况下，无需进行模型培训，就可以实现高精度。我们提出的模型可以在工业制造环境中有效，可扩展和客观的质量控制。

### SOWing Information: Cultivating Contextual Coherence with MLLMs in Image Generation 
[[arxiv](https://arxiv.org/abs/2411.19182)] [[cool](https://papers.cool/arxiv/2411.19182)] [[pdf](https://arxiv.org/pdf/2411.19182)]
> **Authors**: Yuhan Pei,Ruoyu Wang,Yongqi Yang,Ye Zhu,Olga Russakovsky,Yu Wu
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: Project page: https://pyh-129.github.io/SOW/
- **标题**: 播种信息：图像生成中与MLLM的上下文连贯性
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 源自物理学中的扩散现象，描述了粒子的随机运动和碰撞，扩散生成模型模拟了沿denoising轨迹的数据空间中的随机行走。这允许信息跨区域扩散，从而产生和谐的结果。但是，扩散模型中信息扩散的混乱和混乱性质通常会导致图像区域之间的不希望干扰，从而导致细节保存和上下文不一致。在这项工作中，我们通过将无序扩散作为文本视频到图像生成（TV2I）任务的强大工具来解决这些挑战，从而实现了像素级条件的保真度，同时在整个图像中保持视觉和语义连贯性。我们首先引入循环单向扩散（Cow），该扩散（Cow）为精确信息传输提供了有效的单向扩散框架，同时最大程度地减少了破坏性干扰。在牛的基础上，我们进一步提出了选择性的单向扩散（SOW），该扩散（SOW）利用多模式大型语言模型（MLLM）来阐明图像中的语义和空间关系。基于这些见解，SOW结合了注意机制，根据上下文关系动态调节扩散的方向和强度。广泛的实验证明了受控信息扩散的尚未开发的潜力，为以学习方式提供了一种更具适应性和多功能生成模型的途径。

### HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos 
[[arxiv](https://arxiv.org/abs/2411.19167)] [[cool](https://papers.cool/arxiv/2411.19167)] [[pdf](https://arxiv.org/pdf/2411.19167)]
> **Authors**: Prithviraj Banerjee,Sindi Shkodrani,Pierre Moulon,Shreyas Hampali,Shangchen Han,Fan Zhang,Linguang Zhang,Jade Fountain,Edward Miller,Selen Basol,Richard Newcombe,Robert Wang,Jakob Julian Engel,Tomas Hodan
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: arXiv admin note: substantial text overlap with arXiv:2406.09598
- **标题**: HOT3D：从Egentric多视频视频中的3D中的手和对象跟踪
- **领域**: 计算机视觉和模式识别,人工智能,机器人技术
- **摘要**: 我们介绍了HOT3D，这是一个公开可用的数据集，用于以3D为中心的手和对象跟踪。该数据集提供了超过833分钟的多视图RGB/单色图像流（超过3.70万图像），显示了19个主题与33个不同的刚性对象相互作用，诸如眼睛凝视或场景点云之类的多模式信号，以及全面的基础真相注释以及包括对象，手和相机的3D poses和cambers and camers and cockeras and and and and and and and and Brocks和3dd ddd and and and and and and and and and and and and and and and Brocks和3D。除了简单的接送/观察/放置动作外，HOT3D还包含类似厨房，办公室和客厅环境中典型动作的场景。该数据集由Meta：Aria Project Aria（轻型AR/AI眼镜的研究原型）和Quest 3（Quest 3）录制的数据集，并以数百万个单位销售的VR HESSED QUEST 3。地面真相的姿势是通过专业的运动捕获系统使用的小光学标记来获得的。在Umetrack中提供手注释，Mano格式和物体由3D网格用内部扫描仪获得的PBR材料表示。在我们的实验中，我们证明了多视图中心数据对三个流行任务的有效性：3D手跟踪，6DOF对象姿势估计和未知内物体的3D抬起。评估的多视图方法，其基准测试由HOT3D唯一启用，大大优于其单视图。

### On Moving Object Segmentation from Monocular Video with Transformers 
[[arxiv](https://arxiv.org/abs/2411.19141)] [[cool](https://papers.cool/arxiv/2411.19141)] [[pdf](https://arxiv.org/pdf/2411.19141)]
> **Authors**: Christian Homeyer,Christoph Schnörr
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: WICCV2023
- **标题**: 关于带有变压器的单眼视频的移动对象分割
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 从单个移动相机中移动对象检测和分割是一项艰巨的任务，需要了解识别，运动和3D几何形状。将识别和重建的结合归结为融合问题，在该问题中，外观和运动特征需要组合以进行分类和分割。在本文中，我们提出了一种用于单眼运动分割的新型融合体系结构-M3Former，它利用了变压器的强劲性能进行分割和多模式融合。由于来自单眼视频的重建运动是不适合的，因此我们系统地分析了此问题的不同2D和3D运动表示形式及其对分割性能的重要性。最后，我们分析了培训数据的效果，并表明需要各种数据集以在Kitti和Davis上实现SOTA性能。

### Detailed Object Description with Controllable Dimensions 
[[arxiv](https://arxiv.org/abs/2411.19106)] [[cool](https://papers.cool/arxiv/2411.19106)] [[pdf](https://arxiv.org/pdf/2411.19106)]
> **Authors**: Xinran Wang,Haiwen Zhang,Baoteng Li,Kongming Liang,Hao Sun,Zhongjiang He,Zhanyu Ma,Jun Guo
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: 11 pages, 8 figures
- **标题**: 具有可控维度的详细对象描述
- **领域**: 计算机视觉和模式识别
- **摘要**: 对象描述对于视力受损的个体理解和比较对象之间的差异起着重要作用。最近的多模式大语模型（MLLM）具有强大的感知能力，并具有产生以对象为中心的描述的令人印象深刻的潜力。但是，此类模型生成的描述通常可能包含很多与用户意图无关或错过一些重要对象维度详细信息的内容。在特殊情况下，用户可能只需要对象的某些维度的详细信息。在本文中，我们提出了一个无训练的对象说明简化管道，尺寸裁缝，旨在增强对象描述中用户指定的细节。该管道包括三个步骤：尺寸提取，擦除和补充，将描述分解为用户指定的维度。尺寸裁缝不仅可以提高对象详细信息的质量，而且还可以在基于用户偏好的特定维度中提供灵活性。我们进行了广泛的实验，以证明尺寸裁缝在可控对象描述上的有效性。值得注意的是，拟议的管道可以一致地提高最近的MLLM的性能。该代码当前可在https://github.com/xin-ran-w/controllableboctdescription上访问。

### ObjectRelator: Enabling Cross-View Object Relation Understanding in Ego-Centric and Exo-Centric Videos 
[[arxiv](https://arxiv.org/abs/2411.19083)] [[cool](https://papers.cool/arxiv/2411.19083)] [[pdf](https://arxiv.org/pdf/2411.19083)]
> **Authors**: Yuqian Fu,Runze Wang,Yanwei Fu,Danda Pani Paudel,Xuanjing Huang,Luc Van Gool
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 对象介质：以自我为中心和以Exo-exo的视频启用跨视图对象关系理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在本文中，我们专注于自我exo对象通讯任务，这是计算机视觉领域的一个新兴挑战，旨在绘制以以自我为中心的和以易月为中心的视图。我们介绍了一种旨在解决此任务的新颖方法，它具有两个新的模块：多模式条件融合（MCFUSE）和基于SSL的跨视图对象对准（Xobjalign）。 MCFUSE有效地融合了语言和视觉条件，以增强目标对象定位，而Xobjalign通过自我监督的对准策略在视图中实现对象表示的一致性。广泛的实验证明了对象旋转器的有效性，在EGO2EXO和EXO2EGO任务上实现了最新的性能，并具有最小的其他参数。这项工作为未来的跨视图对象关系的研究奠定了基础，理解强调了利用多模式指导和跨视图对准的潜力。将发布代码和模型以朝着这一方向发展进一步研究。

### I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt Generation for Text-Guided Multi-Mask Inpainting 
[[arxiv](https://arxiv.org/abs/2411.19050)] [[cool](https://papers.cool/arxiv/2411.19050)] [[pdf](https://arxiv.org/pdf/2411.19050)]
> **Authors**: Nicola Fanelli,Gennaro Vessio,Giovanna Castellano
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: Accepted at WACV 2025
- **标题**: 我梦想着我的绘画：通过迅速生成文本引导的多面具介入的迅速生成MLLM和扩散模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 介入的重点是填充图像的缺失或损坏区域，以与周围内容和样式无缝融合。尽管有条件的扩散模型已被证明对文本指导的介绍有效，但我们介绍了多面具涂料的新任务，其中使用不同的提示同时对多个区域同时填写了底漆。此外，我们为诸如LLAVA等多模式LLM的微型LLM设计了一个微调程序，以自动使用损坏的图像作为输入来自动提示。这些模型可以生成有用的详细及时建议，以填充蒙面区域。然后将生成的提示馈送到稳定的扩散，该提示是通过整流的跨注意事项对多掩膜镶嵌问题进行微调的，将提示在其指定的区域内强加于填充。 Wikiart的数字化绘画和密集字幕图像数据集的实验表明，我们的管道可提供创造性和准确的授课结果。我们的代码，数据和受过训练的模型可在https://cilabuniba.github.io/i-dream-my-painting上找到。

### PCDreamer: Point Cloud Completion Through Multi-view Diffusion Priors 
[[arxiv](https://arxiv.org/abs/2411.19036)] [[cool](https://papers.cool/arxiv/2411.19036)] [[pdf](https://arxiv.org/pdf/2411.19036)]
> **Authors**: Guangshun Wei,Yuan Feng,Long Ma,Chen Wang,Yuanfeng Zhou,Changjian Li
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: PCDREAMER：通过多视图扩散先验完成点云完成
- **领域**: 计算机视觉和模式识别,图形
- **摘要**: 本文介绍了PCDREAMER，这是一种用于点云完成的新方法。传统方法通常从部分点云中提取特征以预测缺失区域，但是较大的解决方案空间通常会导致结果不令人满意。最新的方法已经开始使用图像作为额外的指导，有效地提高了性能，但是在实践中获得图像和部分点云的配对数据和部分点云是具有挑战性的。为了克服这些局限性，我们利用大型模型中相对一致的多视图扩散先验，以产生对所需形状的新看法。所得图像集编码全局和局部形状提示，这对于形状完成尤其有益。为了充分利用先验，我们设计了一个形状融合模块，用于从多模式输入（\ ie，图像和点云）中产生最初的完整形状，以及一个后续形状整合模块，以通过从扩散率引起的不一致性引入的不可靠点来获得最终的完整形状。广泛的实验结果表明了我们的出色表现，尤其是在恢复细节时。

### Perception of Visual Content: Differences Between Humans and Foundation Models 
[[arxiv](https://arxiv.org/abs/2411.18968)] [[cool](https://papers.cool/arxiv/2411.18968)] [[pdf](https://arxiv.org/pdf/2411.18968)]
> **Authors**: Nardiena A. Pratama,Shaoyang Fan,Gianluca Demartini
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 视觉内容的感知：人类与基础模型之间的差异
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 人类注销的内容通常用于训练机器学习（ML）模型。但是，最近，语言和多模式的基础模型已被用来取代和扩展人类注释者的努力。这项研究比较了代表不同社会经济环境的图像的人类生成和ML生成的注释。我们的目标是了解感知的差异并确定内容解释中的潜在偏见。我们的数据集包括来自各个地理区域的人们和收入水平洗手的图像。我们通过语义比较人类和ML生成的注释，并评估其对预测模型的影响。我们的结果表明，从低级的角度，即出现的单词类型和句子结构的类型，人类和机器注释之间的相似性很低，但与它们在不同地区之间感知的图像的相似性或不同相似。此外，人类注释在班级水平上导致了最佳和最平衡的区域分类表现，而ML对象和ML字幕则最适合收入回归。人类和机器在感知图像时缺乏偏见的相似性突出了它们比最初所感知的更相似。将人类注释用于区域分类和机器注释进行收入回归的优越和公平的表现表明，注释中图像的质量和判别特征的重要性。

### Agri-LLaVA: Knowledge-Infused Large Multimodal Assistant on Agricultural Pests and Diseases 
[[arxiv](https://arxiv.org/abs/2412.02158)] [[cool](https://papers.cool/arxiv/2412.02158)] [[pdf](https://arxiv.org/pdf/2412.02158)]
> **Authors**: Liqiong Wang,Teng Jin,Jinyu Yang,Ales Leonardis,Fangyi Wang,Feng Zheng
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Agri-llava：知识注册的大型多式联运助手
- **领域**: 计算机视觉和模式识别
- **摘要**: 在一般领域中，大型多模型模型（LMM）取得了重大进步，但挑战持续将其应用于特定领域，尤其是农业。作为全球经济的骨干，农业面临着许多挑战，由于害虫和疾病的复杂性，可变性，快速蔓延和高抵抗力，害虫和疾病特别关注。本文专门解决了这些问题。我们在农业领域中构建了第一个遵循数据集的多模式指令，涵盖了221种类型的害虫和疾病，其中约有400,000个数据条目。该数据集旨在探索和应对害虫和疾病控制方面的独特挑战。基于此数据集，我们提出了一种知识资格的培训方法，以开发农业多模式对话系统Agri-llava。为了加速该领域的进步并激发更多的研究人员参与，我们为农业害虫和疾病设计了多样化且挑战性的评估基准。实验结果表明，在农业多模式对话和视觉理解中，农业洛瓦（Agri-llava）出色，提供了解决农业害虫和疾病的新见解和方法。通过开放我们的数据集和模型，我们旨在促进农业领域内LMM的研发，并做出重大贡献，以应对农业害虫和疾病的挑战。所有资源都可以在https://github.com/kki2eve/agri-llava上找到。

### Personalized Multimodal Large Language Models: A Survey 
[[arxiv](https://arxiv.org/abs/2412.02142)] [[cool](https://papers.cool/arxiv/2412.02142)] [[pdf](https://arxiv.org/pdf/2412.02142)]
> **Authors**: Junda Wu,Hanjia Lyu,Yu Xia,Zhehao Zhang,Joe Barrow,Ishita Kumar,Mehrnoosh Mirtaheri,Hongjie Chen,Ryan A. Rossi,Franck Dernoncourt,Tong Yu,Ruiyi Zhang,Jiuxiang Gu,Nesreen K. Ahmed,Yu Wang,Xiang Chen,Hanieh Deilamsalehy,Namyong Park,Sungchul Kim,Huanrui Yang,Subrata Mitra,Zhengmian Hu,Nedim Lipka,Dang Nguyen,Yue Zhao, et al. (2 additional authors not shown)
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 个性化的多模式大语言模型：调查
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,信息检索
- **摘要**: 多模式的大型语言模型（MLLM）由于其最新性能以及整合多种数据模式（例如文本，图像和音频）的能力而变得越来越重要，以高精度执行复杂的任务。本文介绍了一项有关个性化多式模式模型的全面调查，重点是其建筑，培训方法和应用。我们提出了一种直观的分类法，以对将MLLM个性化的技术分类为个人用户，并相应地讨论这些技术。此外，我们讨论了如何在适当的时候将这些技术组合或适应，以突出它们的优势和基本的理由。我们还提供了现有研究中调查的个性化任务的简洁摘要以及常用的评估指标。此外，我们总结了可用于基准个性化MLLM的数据集。最后，我们概述了关键的开放挑战。这项调查旨在为寻求理解和推动个性化多模式大型语言模型的研究人员和从业人员提供宝贵资源。

### WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image 
[[arxiv](https://arxiv.org/abs/2412.02141)] [[cool](https://papers.cool/arxiv/2412.02141)] [[pdf](https://arxiv.org/pdf/2412.02141)]
> **Authors**: Yuci Liang,Xinheng Lyu,Meidan Ding,Wenting Chen,Jipeng Zhang,Yuexiang Ren,Xiangjian He,Song Wu,Sen Yang,Xiyue Wang,Xiaohan Xing,Linlin Shen
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: 38 pages, 22 figures, 35 tables
- **标题**: WSi-llava：整个幻灯片图像的多模式大语言模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 计算病理学的最新进展产生了斑块级的多模式大型语言模型（MLLMS），但是这些模型的限制是由于它们无法全面分析整个幻灯片图像（WSIS），并且它们绕过病理学家依靠依靠诊断的至关重要的形态学特征的趋势。为了应对这些挑战，我们首先引入了WSI-Bench，这是一种大规模形态学的基准，其中包含30种癌症类型的9,850 WSI的180K VQA对，旨在评估MLLM对准确诊断至关重要的形态学特征的理解。在这个基准测试的基础上，我们提出了WSI-LLAVA，这是一种用于Gigapixel WSI的新型框架，采用了三阶段的训练方法：WSI-TEXT对齐，功能空间对齐和特定于任务的指令调整。为了更好地评估病理环境中的模型性能，我们开发了两个专业的WSI指标：WSI-CRECISION和WSI-RELEVANCE。实验结果表明，WSI-LALAVA在所有能力维度上都优于现有模型，并在形态学分析方面取得了重大改进，从而在形态学理解和诊断准确性之间建立了明显的相关性。

### GSOT3D: Towards Generic 3D Single Object Tracking in the Wild 
[[arxiv](https://arxiv.org/abs/2412.02129)] [[cool](https://papers.cool/arxiv/2412.02129)] [[pdf](https://arxiv.org/pdf/2412.02129)]
> **Authors**: Yifan Jiao,Yunhao Li,Junhua Ding,Qing Yang,Song Fu,Heng Fan,Libo Zhang
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: 14 pages, 12 figures
- **标题**: GSOT3D：朝着野外通用的3D单一对象跟踪
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们提出了一种新颖的基准GSOT3D，旨在促进野外通用3D单一对象跟踪（SOT）的发展。具体来说，GSOT3D提供了620个序列，带有123K帧，并涵盖了54个对象类别的广泛选择。每个序列都具有多种方式，包括点云（PC），RGB图像和深度。这允许GSOT3D支持各种3D跟踪任务，例如PC上的单模式3D SOT和RGB-PC或RGB-D上的多模式3D SOT，从而大大拓宽了3D对象跟踪的研究方向。为了提供高质量的人均3D注释，所有序列均以多发精致的检查和精致进行手动标记。据我们所知，GSOT3D是专门针对各种通用3D对象跟踪任务的最大基准。为了了解现有的3D跟踪器的性能并为GSOT3D的未来研究提供比较，我们评估了八种代表性的基于云的跟踪模型。我们的评估结果表明，这些模型在GSOT3D上大大降级，并且需要更多的努力来进行鲁棒和通用的3D对象跟踪。此外，为了鼓励未来的研究，我们提出了一个名为Prot3D的简单而有效的通用3D跟踪器，该跟踪器通过渐进的空间 - 周期网络本地定位目标对象，并以很大的边距优于所有当前解决方案。通过释放GSOT3D，我们希望在未来的研究和应用中进一步进一步跟踪3D跟踪。我们的基准和模型以及评估结果将在我们的网页https://github.com/ailovejinx/gsot3d上公开发布。

### Towards Universal Soccer Video Understanding 
[[arxiv](https://arxiv.org/abs/2412.01820)] [[cool](https://papers.cool/arxiv/2412.01820)] [[pdf](https://arxiv.org/pdf/2412.01820)]
> **Authors**: Jiayuan Rao,Haoning Wu,Hao Jiang,Ya Zhang,Yanfeng Wang,Weidi Xie
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: Technical Report; Project Page: https://jyrao.github.io/UniSoccer/
- **标题**: 迈向通用足球视频理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 作为一项全球著名的运动，足球吸引了世界各地的球迷们广泛的兴趣。本文旨在为足球视频理解开发一个全面的多模式框架。具体来说，我们在本文中做出了以下贡献：（i）迄今为止，我们介绍了SoccerReplay-1988，这是最大的多模式足球数据集，其中包含1,988个完整匹配的视频和详细注释，并具有自动注释管道； （ii）我们介绍了足球领域中的第一个视觉语言基础模型，即MatchVision，该模型利用了跨足球视频的时空信息，并在各种下游任务中脱颖而出； （iii）我们对事件分类，评论生成和多视图犯规识别进行了广泛的实验和消融研究。 MatchVision展示了所有它们的最先进性能，这基本上超过了现有模型，这突出了我们提出的数据和模型的优越性。我们认为，这项工作将为体育理解研究提供标准范式。

### COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training 
[[arxiv](https://arxiv.org/abs/2412.01814)] [[cool](https://papers.cool/arxiv/2412.01814)] [[pdf](https://arxiv.org/pdf/2412.01814)]
> **Authors**: Sanghwan Kim,Rui Xiao,Mariana-Iuliana Georgescu,Stephan Alaniz,Zeynep Akata
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 宇宙：视觉语言预训练的跨模式自我介绍
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 受过对比损失训练的视觉语言模型（VLM）在各种视觉和语言任务中取得了重大进步。但是，对比损失的全球性质使VLM主要集中在前景对象上，忽略了图像中的其他关键信息，这限制了它们在下游任务中的有效性。为了应对这些挑战，我们提出了Cosmos：跨模式的自我介绍，以进行视觉预训练，将新颖的文本作品策略和跨意思模块整合到一个自我监督的学习框架中。我们创建图像和文本的全局和本地视图（即多模式增强），这对于在VLM中的自我验证至关重要。我们进一步介绍了一个跨意义的模块，使宇宙能够通过跨模式自我鉴定损失来学习优化的全面跨模式表示。宇宙始终在下游任务（包括检索，分类和语义分割）上胜过以前的强大基线。此外，它超过了在视觉感知和上下文理解任务中在较大数据集中训练的基于夹的模型。

### Attacks on multimodal models 
[[arxiv](https://arxiv.org/abs/2412.01725)] [[cool](https://papers.cool/arxiv/2412.01725)] [[pdf](https://arxiv.org/pdf/2412.01725)]
> **Authors**: Viacheslav Iablochnikov,Alexander Rogachev
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: 19 pages, 13 figures, 3 tables
- **标题**: 对多模型的攻击
- **领域**: 计算机视觉和模式识别
- **摘要**: 如今，能够以聊天格式同时使用各种方式的模型正在越来越受欢迎。尽管如此，仍存在对这些模型的潜在攻击问题，尤其是考虑到其中许多包括开源组件。重要的是要研究这些组件的脆弱性是否是继承的，以及在行业中使用此类模型时的危险性。这项工作致力于研究对此类模型的各种攻击并评估其概括能力。现代VLM模型（LLAVA，BLIP等）经常使用其他模型的预训练部分，因此本研究的主要部分侧重于它们，特别是夹层体系结构及其图像编码器（夹具vit）和各种补丁攻击变化。

### LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant 
[[arxiv](https://arxiv.org/abs/2412.01720)] [[cool](https://papers.cool/arxiv/2412.01720)] [[pdf](https://arxiv.org/pdf/2412.01720)]
> **Authors**: Yikun Liu,Pingan Chen,Jiayin Cai,Xiaolong Jiang,Yao Hu,Jiangchao Yao,Yanfeng Wang,Weidi Xie
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: LAMRA：大型多模式作为您的高级检索助手
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着多模式信息检索的快速发展，出现了日益复杂的检索任务。现有方法主要依赖于特定于任务的视觉模型微调，通常是那些接受图像文本对比度学习的训练的方法。在本文中，我们探讨了重新假设生成大型多模型（LMM）进行检索的可能性。这种方法可以在同一配方下统一所有检索任务，更重要的是，可以在没有其他培训的情况下推断出未见的检索任务。我们的贡献可以在以下各个方面进行总结：（i）我们介绍了Lamra，这是一个多功能框架，旨在增强LMM的能力，具有复杂的检索和重新计算功能。 （ii）对于检索，我们采用了两阶段的培训策略，其中包括仅限语言的预训练和多模式指导调整，以逐步提高LMM的检索性能。 （iii）为了重读，我们对重点和列表的重新训练进行了联合培训，提供了两种不同的方法来进一步提高检索性能。 （iv）广泛的实验结果强调了我们方法在处理十多个检索任务方面的功效，在监督和零照片的设置中表现出了稳健的性能，包括涉及以前看不见的检索任务的方案。

### VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval 
[[arxiv](https://arxiv.org/abs/2412.01558)] [[cool](https://papers.cool/arxiv/2412.01558)] [[pdf](https://arxiv.org/pdf/2412.01558)]
> **Authors**: Dhiman Paul,Md Rizwan Parvez,Nabeel Mohammed,Shafin Rahman
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: :I.2.10; I.2.7
- **标题**: 视频：用于联合视频的功能改进和交叉任务对齐变压器突出显示和记录获取
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视频突出显示检测和力矩检索（HD/MR）在视频分析中至关重要。最近的联合预测变压器模型通常忽略了他们的交叉任务动力学以及视频文本对齐和精致。此外，大多数模型通常使用有限的单向注意机制，从而在捕获视频和文本模式之间的相互依赖性时产生弱集成的表示和次优性能。尽管大型语言和视觉模型（LLM/LVLM）在各个领域都占有重要地位，但它们在该领域的应用仍然相对不受影响。 Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation.此外，（iv）我们引入了自适应误差惩罚和改善学习的坚硬正/负损失，并且（v）利用BLIP-2（例如BLIP-2）来增强了使用LVLMS生成的合成数据来增强多模式特征集成和智能预处理。关于QVHighlights，TVSUM和Charades-STA基准测试的全面实验表明了最先进的性能。代码和模型可在https://github.com/dpaul06/videolights上找到。

### Divide-and-Conquer: Confluent Triple-Flow Network for RGB-T Salient Object Detection 
[[arxiv](https://arxiv.org/abs/2412.01556)] [[cool](https://papers.cool/arxiv/2412.01556)] [[pdf](https://arxiv.org/pdf/2412.01556)]
> **Authors**: Hao Tang,Zechao Li,Dong Zhang,Shengfeng He,Jinhui Tang
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: Accepted by IEEE TPAMI. Project page: https://cser-tang-hao.github.io/contrinet.html
- **标题**: 分裂和框架：用于RGB-T显着对象检测的汇合三流网络
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: RGB热对象检测旨在查明可见和热红外图像对齐对中的突出物体。传统的编码器架构虽然是为跨模式相互作用而设计的，但可能没有充分考虑到源自有缺陷方式的噪声的稳健性。受分层人类视觉系统的启发，我们提出了Contrinet，这是一种使用分隔和拼接策略的强大汇合三流网络。具体而言，Continet包括三个流：两个特定于模态的流探索RGB和热模态的线索，而第三个模态互补流则整合了两种模态的线索。 Continet提出了几个显着的优势。它在模态共享的联合编码器中结合了模态诱导的特征调制器，以最大程度地减少模式间差异并减轻有缺陷的样品的影响。此外，分离流中的基础残差空间金字塔模块扩大了接受场，从而捕获了多尺度的上下文信息。此外，来自模态互补流中的模态感知的动态聚合模块，从两个特定于模态的流动中动态汇总了与显着性相关的线索。利用所提出的平行三流框架，我们进一步完善了通过流动融合策略从不同流中得出的显着图，从而为最终预测提供了高质量的全分辨率显着图。为了评估我们方法的鲁棒性和稳定性，我们收集了全面的RGB-T SOD基准VT-IMAG，涵盖了各种现实世界中具有挑战性的场景。关于公共基准和我们的VT-IMAG数据集的广泛实验表明，在共同和具有挑战性的场景中，Continet始终优于最先进的竞争者。

### SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2412.01550)] [[cool](https://papers.cool/arxiv/2412.01550)] [[pdf](https://arxiv.org/pdf/2412.01550)]
> **Authors**: Chunlin Yu,Hanqing Wang,Ye Shi,Haoyang Luo,Sibei Yang,Jingyi Yu,Jingya Wang
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: seqafford：通过多模式大语言模型的顺序3D负担推理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 3D负担性细分旨在将人类的指示与3D对象的可接触区域联系起来，以进行具体操作。现有的努力通常遵守单对象，单物范围的范式，每种负担能力类型或明确指令都严格对应于特定的负担区域，并且无法处理长途任务。这样的范式无法积极理解复杂的用户意图，这通常意味着顺序负担。在本文中，我们介绍了连续的3D负担推理任务，该任务通过从麻烦的用户意图中推理，然后将它们分解为一系列的分段图来扩展传统范式。在此方面，我们构建了第一个基于指令的负担分割基准，其中包括对单个和顺序负担的推理，包括180K指导点云对。基于基准，我们建议我们的模型Seqafford，以解锁具有额外的负担能力分段能力的3D多模式大型语言模型，从而确保了在凝聚力框架中使用世界知识和细粒度的负担得起的基础的推理。我们进一步引入了多个语言点集成模块，以赋予3D密集的预测。广泛的实验评估表明，我们的模型在建立的良好方法上表现出凭借顺序推理能力的开放世界概括。

### HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2412.01407)] [[cool](https://papers.cool/arxiv/2412.01407)] [[pdf](https://arxiv.org/pdf/2412.01407)]
> **Authors**: Zehuan Wu,Jingcheng Ni,Xiaodong Wang,Yuxin Guo,Rui Chen,Lewei Lu,Jifeng Dai,Yuwen Xiong
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Holodrive：自动驾驶的整体2D-3D多模式街道一代
- **领域**: 计算机视觉和模式识别
- **摘要**: 生成模型已显着改善了用于自动驾驶的相机图像或LiDAR点云上的发电和预测质量。但是，现实世界中的自主驾驶系统使用多种输入模式，通常是相机和激光镜头，它们包含用于生成的互补信息，而现有生成方法则忽略了这一关键功能，从而导致生成的结果仅涵盖单独的2D或3D信息。为了填补2d-3d多模式联合生成的空白以进行自动驾驶，在本文中，我们建议我们的框架\ emph {holodrive}共同生成相机图像和激光点云。我们在异质生成模型之间采用BEV到相机和摄像头转换模块，并在2D生成模型中引入深度预测分支，以消除从图像空间到BEV空间的未投影，然后扩展方法，扩展方法以通过添加时间结构和精心设计的渐进培训来预测未来。此外，我们对单帧生成和世界模型基准进行实验，并证明我们的方法在产生指标方面会导致对SOTA方法的显着性能提高。

### MoTrans: Customized Motion Transfer with Text-driven Video Diffusion Models 
[[arxiv](https://arxiv.org/abs/2412.01343)] [[cool](https://papers.cool/arxiv/2412.01343)] [[pdf](https://arxiv.org/pdf/2412.01343)]
> **Authors**: Xiaomin Li,Xu Jia,Qinghe Wang,Haiwen Diao,Mengmeng Ge,Pengxiang Li,You He,Huchuan Lu
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: Accepted by ACM MM 2024, code will be released in https://github.com/XiaominLi1997/MoTrans
- **标题**: Motrans：具有文本驱动视频扩散模型的定制运动转移
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的验证文本对视频（T2V）模型在生成具有基本运动或相机运动的现实视频方面表现出了令人印象深刻的能力。但是，这些模型在产生复杂的以人为中心的动作时表现出显着的局限性。当前的努力主要集中在一系列包含特定运动的视频上的微调模型上。他们通常无法有效地解开运动和有限参考视频中的外观，从而削弱了运动模式的建模能力。为此，我们提出了Motrans，这是一种定制运动转移方法，可在新环境中产生类似运动的视频生成。具体而言，我们引入了一个基于多模式的大语言模型（MLLM）的重新启动器，以扩展初始提示，以更多地关注外观和外观注入模块，以适应从视频框架到运动建模过程的前面外观。这些互补的多模式表示，来自重新提示和视频框架促进了外观的建模，并促进了外观和运动的解耦。此外，我们设计了一种运动特异性嵌入，以进一步增强特定运动的建模。实验结果表明，我们的方法可以有效地从单数或多个参考视频中学习特定的运动模式，从而对定制视频生成中的现有方法表现出色。

### Enhancing Perception Capabilities of Multimodal LLMs with Training-Free Fusion 
[[arxiv](https://arxiv.org/abs/2412.01289)] [[cool](https://papers.cool/arxiv/2412.01289)] [[pdf](https://arxiv.org/pdf/2412.01289)]
> **Authors**: Zhuokun Chen,Jinwu Hu,Zeshuai Deng,Yufeng Wang,Bohan Zhuang,Mingkui Tan
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 通过无训练的融合，增强多模式LLM的感知能力
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式LLMS（MLLMS）通过将视觉编码器与语言模型对准编码，以具有视觉功能的语言模型。增强MLLM视觉感知的现有方法通常涉及设计更强大的视觉编码器，这需要探索庞大的设计空间并通过语言模型重新对齐每个潜在的编码器，从而导致高昂的培训成本。在本文中，我们介绍了VisionFuse，这是一个新颖的集成框架，该框架有效地利用了来自现成的MLLM的多个视觉编码器来增强视觉感知，而无需进行额外的培训。我们的方法是由观察到的动机，即不同的MLLM倾向于考虑到相同的查询和图像的不同区域。此外，我们发现，MLLM家族中视觉编码器的特征分布，一组共享相同验证的LLM的MLLMS非常相位。在这些见解的基础上，VisionFuse通过串联由家庭中选定的MLLM的视觉编码者产生的令牌来丰富视觉上下文。通过合并这些MLLM的语言模型的参数，VisionFuse允许单个语言模型与各种视觉编码器保持一致，从而大大减少了部署开销。我们使用各种MLLM组合对多个多模式基准进行了全面的评估，这表明了多模式任务的实质性改进。值得注意的是，当将Minigemini-8b和Slime-8B整合时，VisionFuse的平均性能增长超过4％。

### Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Model 
[[arxiv](https://arxiv.org/abs/2412.01282)] [[cool](https://papers.cool/arxiv/2412.01282)] [[pdf](https://arxiv.org/pdf/2412.01282)]
> **Authors**: Qianhan Feng,Wenshuo Li,Tong Lin,Xinghao Chen
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Align-KD：移动视觉模型的横式模式对齐知识
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉模型（VLM）为多模式任务带来了强大的理解和推理能力。同时，还出现了对移动设备上有能力的ARITICKENT的巨大需求，例如AI Assistant Software。一些努力试图将VLM迁移到Edge设备以扩大其应用程序范围。简化模型结构是一种常见方法，但是随着模型的缩小，性能和大小之间的权衡变得越来越困难。知识蒸馏（KD）可以帮助模型改善综合功能，而不会增加规模或数据量。但是，大多数现有的大型模型蒸馏技术仅考虑在单模式LLM上的应用，或者仅使用教师为学生创建新的数据环境。这些方法都没有考虑到VLM中最重要的跨模式对齐知识的蒸馏。我们提出了一种称为Align-KD的方法，以指导学生模型学习在浅层层发生的跨模式匹配。教师还可以帮助学生根据文本的重点学习视觉令牌嵌入文本空间的投影。在Align-KD的指导下，1.7B MobileVLM V2模型可以从7B教师模型中学习丰富的知识，并以训练损失的轻度设计，并在两个培训子集的下分别在6个基准中提高6个基准分数。代码可在以下网址提供：https：//github.com/fqhank/align-kd。

### Ponder & Press: Advancing Visual GUI Agent towards General Computer Control 
[[arxiv](https://arxiv.org/abs/2412.01268)] [[cool](https://papers.cool/arxiv/2412.01268)] [[pdf](https://arxiv.org/pdf/2412.01268)]
> **Authors**: Yiqin Wang,Haoji Zhang,Jingqi Tian,Yansong Tang
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Ponder＆Press：将视觉GUI代理推向通用计算机控制
- **领域**: 计算机视觉和模式识别
- **摘要**: 大多数现有的GUI代理通常取决于HTML源代码或可访问性树等非视觉输入，从而限制了它们在各种软件环境和平台上的灵活性。当前的多模式大型语言模型（MLLMS）擅长使用视觉来接地现实世界对象，提供了潜在的替代方案。但是，由于现实世界对象和GUI元素之间的语义差距，他们经常在准确地将GUI元素（有效GUI自动化的关键要求）上挣扎。在这项工作中，我们介绍了Ponder＆Press，这是一个仅使用视觉输入的一般计算机控制的划分和混合框架。我们的方法将通用的MLLM与“解释器”相结合，负责将高级用户说明转化为详细的动作描述，而GUI特定的MLLM则作为“定位器”，该“定位器”精确地定位了用于放置操作的GUI元素。通过利用纯粹的视觉输入，我们的代理商提供了适用于广泛应用的多功能，类人类的相互作用范式。 Ponder＆Press定位器在GUI接地基准测试中优于现有型号 +22.5％。在各种GUI环境中的离线和交互式代理基准（包括网页，桌面软件和移动UI）都证明了Ponder＆Press Framework达到了最新的性能，突出了Visual GUI代理的潜力。请参阅项目主页https://invinciblewyq.github.io/ponder-press-page/

### Multimodal Fusion Learning with Dual Attention for Medical Imaging 
[[arxiv](https://arxiv.org/abs/2412.01248)] [[cool](https://papers.cool/arxiv/2412.01248)] [[pdf](https://arxiv.org/pdf/2412.01248)]
> **Authors**: Joy Dhar,Nayyar Zaidi,Maryam Haghighat,Puneet Goyal,Sudipta Roy,Azadeh Alavi,Vikas Kumar
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: 10 pages
- **标题**: 多模式的融合学习以及双重注意医学成像
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式融合学习在分类各种疾病（例如皮肤癌和脑肿瘤）方面已显示出巨大的希望。但是，现有方法面临三个关键局限性。首先，由于关注特定疾病，他们通常缺乏对其他诊断任务的普遍性。其次，他们没有完全利用来自不同方式的多个健康记录来学习强大的互补信息。最后，他们通常依靠一种注意机制，缺少各种方式内部和各种方式的多种关注策略的好处。为了解决这些问题，本文提出了一种双重鲁棒信息融合注意机制（DRIFA），该机制利用了两个注意模块，即多分支融合注意模块和多模式信息融合注意模块。 DRIFA可以与任何深神经网络集成，形成一个多模式融合学习框架，称为DRIFA-NET。我们表明，Drifa的多分支融合注意力学会了每种模式的增强表示形式，例如皮肤镜，PAP涂片，MRI和CT-SCAN，而多模式信息融合注意模块学习了更精致的多模式共享表示，从而改善了跨多个任务的网络化并增强整体性能。此外，为了估算DRIFA-NET预测的不确定性，我们采用了蒙特卡洛辍学策略。在五个具有不同方式的公开数据集上进行的广泛实验表明，我们的方法始终超过最先进的方法。该代码可在https://github.com/misti1203/drifa-net上找到。

### OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones? 
[[arxiv](https://arxiv.org/abs/2412.01175)] [[cool](https://papers.cool/arxiv/2412.01175)] [[pdf](https://arxiv.org/pdf/2412.01175)]
> **Authors**: Zijian Chen,Tingzhu Chen,Wenjun Zhang,Guangtao Zhai
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: Accepted by ICLR 2025 as a Poster. 31 pages, 18 figures
- **标题**: Obi-Bench：LMM可以帮助研究Oracle骨头上的古代文字吗？
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们介绍了Obi-Bench，这是一种整体基准，该基准精心旨在系统地评估大型多模式模型（LMMS）在整个过程的Oracle骨骨头铭文（OBI）处理任务上，要求专家级领域知识和故意认知。 Obi-Bench包括5,523个精心收集的各种图像，涵盖了五个关键领域问题：识别，重新加入，分类，检索和解密。这些图像涵盖了几个世纪的考古发现和前线学者的多年研究，包括从开挖到合成等多阶段字体的外观，例如原始的甲骨文，墨水垃圾，甲骨文骨头碎片，裁剪的单个字符和手绘字符。与现有的基准不同，OBI Bench专注于具有OBI特定知识的先进视觉感知和推理，挑战LMMS执行类似于专家面临的任务。对6个专有LMM的评估以及17个开源LMM凸显了Obi-Bench提出的重大挑战和需求。在某些精细的感知任务中，即使是最新版本的GPT-4O，Gemini 1.5 Pro和QWEN-VL-MAX仍然远离公共水平的人类。但是，它们的表现与未经训练的人类在破译任务中相当，表明在提供新的解释性观点和产生创造性的猜测方面表明了出色的能力。我们希望Obi Bench能够促进社区开发特定领域的多模式基础模型，以更深入地发现和增强这些未开发的LMMS潜力。

### Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation 
[[arxiv](https://arxiv.org/abs/2412.01027)] [[cool](https://papers.cool/arxiv/2412.01027)] [[pdf](https://arxiv.org/pdf/2412.01027)]
> **Authors**: Bolin Lai,Felix Juefei-Xu,Miao Liu,Xiaoliang Dai,Nikhil Mehta,Chenguang Zhu,Zeyi Huang,James M. Rehg,Sangmin Lee,Ning Zhang,Tong Xiao
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: 18 pages, 16 figures, 5 tables
- **标题**: 释放对自回归模型的内在学习，以进行几个弹药的图像操纵
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，文本指导的图像操纵经历了显着的进步。为了减轻语言歧义，对训练集中代表人数不足或难以纯粹用语言描述的说明，使用了视觉示例的射击学习。但是，从视觉提示中学习需要强大的推理能力，这是扩散模型正在努力的。为了解决这个问题，我们介绍了一种新型的多模式自回旋模型，称为$ \ textbf {instamanip} $，它可以$ \ textbf {insta} $ ntly学习一个新的图像$ \ textbf {manip} $ textbf {manip} $ textbf {manip} $ textual and Visual Guidance vife textual and Visual Guidance in Textual and Visual Guidance textual和Visual Guardance texement of context学习，并应用其新的图像。具体而言，我们提出了一种创新的群体自我注意机制，以将内在的学习过程分为两个单独的阶段 - 学习和应用，这将复杂的问题简化为两个更轻松的任务。我们还引入了一种关系正则化方法，以进一步将图像转换特征与示例图像中的无关内容相关。广泛的实验表明，我们的方法超过了以前的几张图像操纵模型（在人类评估中$ \ geq $ 19％）。我们还发现，通过增加示例图像的数量或多样性，可以进一步提高我们的模型。

### WAFFLE: Multimodal Floorplan Understanding in the Wild 
[[arxiv](https://arxiv.org/abs/2412.00955)] [[cool](https://papers.cool/arxiv/2412.00955)] [[pdf](https://arxiv.org/pdf/2412.00955)]
> **Authors**: Keren Ganon,Morris Alper,Rachel Mikulinsky,Hadar Averbuch-Elor
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: Accepted to WACV 2025. Project page: https://tau-vailab.github.io/WAFFLE/
- **标题**: 华夫饼：野外的多模式平面图
- **领域**: 计算机视觉和模式识别
- **摘要**: 建筑物是人类文化的核心特征，并且越来越多地通过计算方法进行分析。但是，关于计算建设理解的最新著作主要集中在建筑物的自然图像上，忽略了定义建筑物结构的基本要素 - 其平面图。相反，现有的关于平面图理解的作品的范围极为有限，通常集中在单个语义类别和区域的平面图（例如，单个国家的公寓平面图）。在这项工作中，我们介绍了Waffle，这是一种新型的多模式平面图了解近20K平面图图像和元数据的数据集，该数据集是根据涉及各种建筑类型，位置和数据格式的互联网数据策划的。通过使用大型语言模型和多模式基础模型，我们从这些图像及其随附的嘈杂元数据中策划和提取语义信息。我们表明，华夫饼在新的构建理解任务（包括歧视性和生成性）上的进展，使用先前的数据集是不可行的。我们将与我们的代码和经过训练的模型一起公开发布华夫饼干，为研究社区提供了学习建筑物语义的新基础。

### VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation 
[[arxiv](https://arxiv.org/abs/2412.00927)] [[cool](https://papers.cool/arxiv/2412.00927)] [[pdf](https://arxiv.org/pdf/2412.00927)]
> **Authors**: Weiming Ren,Huan Yang,Jie Min,Cong Wei,Wenhu Chen
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: Project Page: https://tiger-ai-lab.github.io/VISTA/
- **标题**: Vista：通过视频时空增强增强长期和高分辨率视频理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的大型多模型（LMM）在处理和理解长期或高分辨率视频方面面临重大挑战，这主要是由于缺乏高质量的数据集。为了从以数据为中心的角度解决此问题，我们提出了Vista，Vista是一个简单而有效的时空增强框架，从现有的视频关注数据集中综合了长期和高分辨率的视频指导对 - 构成长期和高分辨率的视频指令对。 Vista在空间和时间上结合了视频，以创建具有延长持续时间和增强分辨率的新合成视频，随后生成与这些新综合视频有关的问答式视频。基于此范式，我们开发了七种视频增强方法，并策划Vista-400k，这是一个旨在增强长期和高分辨率视频理解的视频指令数据集。在我们的数据上对各种视频LMM进行填充，导致四个具有挑战性的基准的平均提高为3.3％，以供长期理解。此外，我们介绍了第一个全面的高分辨率视频理解基准HRVIDEOBENCH，我们的填充模型在其上获得了6.5％的性能增长。这些结果突出了我们框架的有效性。

### Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification 
[[arxiv](https://arxiv.org/abs/2412.00876)] [[cool](https://papers.cool/arxiv/2412.00876)] [[pdf](https://arxiv.org/pdf/2412.00876)]
> **Authors**: Wenxuan Huang,Zijie Zhai,Yunhang Shen,Shaosheng Cao,Fei Zhao,Xiangfeng Xu,Zheyu Ye,Shaohui Lin
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: Code is available at https://github.com/Osilly/dynamic_llava
- **标题**: 动态式：通过动态视觉语言上下文稀疏的高效多模式大型语言模型
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 多模式大型语言模型（MLLM）在视力理解，推理和互动方面取得了巨大的成功。但是，推理计算和记忆随着解码过程的产生令牌的产生而逐渐增加，直接影响MLLM的功效。现有的方法试图减少视力上下文冗余以实现有效的MLLM。不幸的是，在解码阶段，预填充阶段降低视力环境的效率受益。为了解决这个问题，我们提出了一个动态的视觉语言上下文稀疏框架动态式差异，该框架动态降低了预填充阶段中视觉上下文的冗余，并减少了解码过程中生成语言上下文的内存和计算开销。动态式式设计针对不同的推理模式设计了量身定制的稀疏推理方案，即预填充，在有或没有KV缓存的情况下进行解码，以实现MLLM的有效推理。实际上，在预填充阶段，Dynamic-lalava可以将计算消耗减少$ \ sim $ 75 \％。同时，在整个MLLM的整个生成过程中，Dynamic-llava在没有KV缓存的情况下降低了$ \ sim $ 50 \％的计算消耗，同时在使用KV缓存解码时节省了$ \ sim $ \ sim $ 50 \％\％的GPU内存，这是由于视觉语言语言上下文sparcarsification。广泛的实验还表明，与全文推断基准相比，动态式闭合可以对具有可忽略的理解和产生能力降解甚至性能提高的MLLM有效推断。代码可在https://github.com/osilly/dynamic_llava上找到。

### Particle-based 6D Object Pose Estimation from Point Clouds using Diffusion Models 
[[arxiv](https://arxiv.org/abs/2412.00835)] [[cool](https://papers.cool/arxiv/2412.00835)] [[pdf](https://arxiv.org/pdf/2412.00835)]
> **Authors**: Christian Möller,Niklas Funk,Jan Peters
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 使用扩散模型从点云中基于粒子的6D对象姿势效果估计
- **领域**: 计算机视觉和模式识别
- **摘要**: 从单个角度来看对象姿势估计仍然是一个具有挑战性的问题。特别是，部分可观察性，阻塞和对象对称最终导致姿势歧义。为了说明这一多模式，这项工作提出了培训基于扩散的生成模型，以进行6D对象姿势估计。在推断过程中，训练有素的生成模型允许采样多个颗粒，即姿势假设。为了将这些信息提炼成单个姿势估算，我们提出了两种新颖而有效的姿势选择策略，这些策略不需要任何其他培训或计算密集型操作。此外，虽然许多现有的姿势估计方法主要集中在图像域上，并且仅包含用于最终姿势改进的深度信息，但我们的模型仅在点云数据上运行。该模型因此利用了点云处理中的最新进展，并在SE（3） - 等级的潜在空间上运行，该空间构成了粒子选择策略的基础，并允许改善推理时间。我们的详尽实验结果表明，我们在LineMod数据集上的竞争性能，并展示了我们的设计选择的有效性。代码可在https://github.com/zitronian/6dposediffusion中找到。

### AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment 
[[arxiv](https://arxiv.org/abs/2412.00833)] [[cool](https://papers.cool/arxiv/2412.00833)] [[pdf](https://arxiv.org/pdf/2412.00833)]
> **Authors**: Yan Li,Yifei Xing,Xiangyuan Lan,Xin Li,Haifeng Chen,Dongmei Jiang
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: AlignMamba：通过本地和全球跨模式对齐增强多模式的MAMBA
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 由于模态之间固有的异质性，跨模式比对对于多模式表示融合至关重要。尽管基于变压器的方法在建模模式之间显示了有希望的结果，但它们的二次计算复杂性将其适用性限制在长期或大规模数据中。尽管最近的基于Mamba的方法达到了线性复杂性，但它们的顺序扫描机制在全面建模跨模式关系中带来了基本挑战。为了解决这一限制，我们提出了AlignMamba，这是一种多模式融合的有效方法。具体而言，以最佳运输为基础，我们引入了一个局部的跨模式比对模块，该模块明确地学习了不同模态之间的令牌级对应关系。此外，我们提出了基于最大平均差异的全局跨模式对齐损失，以隐式地强制执行不同模态分布之间的一致性。最后，局部和全局对齐后的单峰表示传递给MAMBA主链，以进一步的跨模式相互作用和多模式融合。对完整和不完整的多模式融合任务进行的广泛实验证明了该方法的有效性和效率。

### EventGPT: Event Stream Understanding with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.00832)] [[cool](https://papers.cool/arxiv/2412.00832)] [[pdf](https://arxiv.org/pdf/2412.00832)]
> **Authors**: Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Xin Meng,Fei Richard Yu,Xiangyang Ji,Ming Li
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: EventGPT：使用多模式大语言模型的事件流理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 事件摄像机记录视觉信息为异步像素更改流，在不令人满意的照明或高动力条件下在场景中表现出色。现有的多模式大型语言模型（MLLM）集中在天然RGB图像上，在事件数据效果更好的情况下失败。在本文中，我们介绍了EventGpt，这是我们的最佳知识，这是第一个用于事件流理解的MLLM，这标志着将大型语言模型（LLMS）与事件流理解相结合的开创性尝试。为了减轻巨大的领域差距，我们开发了一个三阶段优化范式，以逐步为预训练的LLM配备能够理解基于事件的场景的能力。我们的EventGPT包括一个事件编码器，然后是时空聚合器，线性投影仪，事件语言适配器和LLM。首先，由GPT生成的RGB图像文本对被利用来热身线性投影仪，指的是Llava，因为自然图像和语言方式之间的差距相对较小。其次，我们构建了一个合成但大的数据集，N-Imagenet-Chat，由事件框架和相应的文本组成，以启用时空聚合器的使用并训练事件语言适配器，从而更加与语言空间保持一致。最后，我们收集了一个指令数据集Event-Chat，其中包含广泛的现实数据来微调整个模型，从而进一步增强了其概括能力。我们构建了一个全面的基准，实验表明，事件节在发电质量，描述性准确性和推理能力方面超过了先前的最新MLLM。

### MIMIC: Multimodal Islamophobic Meme Identification and Classification 
[[arxiv](https://arxiv.org/abs/2412.00681)] [[cool](https://papers.cool/arxiv/2412.00681)] [[pdf](https://arxiv.org/pdf/2412.00681)]
> **Authors**: S M Jishanul Islam,Sahid Hossain Mustakim,Sadia Ahmmed,Md. Faiyaz Abdullah Sayeedi,Swapnil Khandoker,Syed Tasdid Azam Dhrubo,Nahid Hossain
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: Accepted (Poster) - NeurIPS 2024 Workshop MusIML
- **标题**: 模仿：多模式伊斯兰恐怖模因识别和分类
- **领域**: 计算机视觉和模式识别
- **摘要**: 反穆斯林仇恨言论已经在模因中出现，其特征是与上下文相关的和修辞的信息，它使用看似模仿幽默但传达伊斯兰情感的文本和图像。这项工作介绍了一个新颖的数据集，并根据视觉和语言变压器（VILT）提出了一个分类器，专门针对通过整合视觉和文本表示形式来识别模因中的反穆斯林仇恨。我们的模型利用模因图像和合并文本之间的联合模态嵌入，以捕获模因文化独有的细微伊斯兰恐惧叙事，从而提供了高检测精度和互操作性。

### Towards Unified Molecule-Enhanced Pathology Image Representation Learning via Integrating Spatial Transcriptomics 
[[arxiv](https://arxiv.org/abs/2412.00651)] [[cool](https://papers.cool/arxiv/2412.00651)] [[pdf](https://arxiv.org/pdf/2412.00651)]
> **Authors**: Minghao Han,Dingkang Yang,Jiabei Cheng,Xukun Zhang,Linhao Qu,Zizhi Chen,Lihua Zhang
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: 21 pages, 11 figures, 7 tables
- **标题**: 通过整合空间转录组学迈向统一的分子增强病理学图像表示学习
- **领域**: 计算机视觉和模式识别,基因组学
- **摘要**: 多模式预训练模型的最新进展具有明显的高级计算病理学。但是，当前的方法主要依赖于视觉语言模型，这可能从分子角度施加局限性并导致性能瓶颈。在这里，我们介绍了统一的分子增强病理图像表示框架（裁判）。裁判旨在利用基因表达谱的互补信息来指导多模式的预训练，从而增强了病理图像表示学习的分子意识。我们证明，该分子观点为学习病理学图像嵌入提供了强大的，任务无义的训练信号。由于配对数据的稀缺性，收集了大约400万个空间转录组基因表达的条目以训练基因编码器。通过利用强大的预训练的编码器，裁判员将编码器对齐超过697K病理的图像 - 基因表达对。在各种分子相关的下游任务中都证明了裁判的表现，包括基因表达预测，点分类和整个幻灯片图像中的突变状态预测。我们的发现突出了多模式数据集成和开放新途径在探索计算病理学方面的有效性。代码和预训练的权重可从https://github.com/hanminghao/umpire获得。

### A Lesson in Splats: Teacher-Guided Diffusion for 3D Gaussian Splats Generation with 2D Supervision 
[[arxiv](https://arxiv.org/abs/2412.00623)] [[cool](https://papers.cool/arxiv/2412.00623)] [[pdf](https://arxiv.org/pdf/2412.00623)]
> **Authors**: Chensheng Peng,Ido Sobol,Masayoshi Tomizuka,Kurt Keutzer,Chenfeng Xu,Or Litany
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 夹层中的一堂课：3D高斯分裂生成的教师指导的扩散与2D监督
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们引入了一个用于高斯碎片的扩散模型，即Splatdiffusion，以使单个图像的三维结构能够生成三维结构，从而解决了将2D输入提升到3D的不良性质。现有方法依赖于确定性的，前馈预测，这限制了他们从2D数据中处理3D推断固有歧义的能力。扩散模型最近显示了有望作为3D数据的强大生成模型，包括高斯夹层。但是，标准扩散框架通常需要目标信号，而被授予的信号则处于相同的方式，考虑到3D数据的稀缺性，这具有挑战性。为了克服这一点，我们提出了一种新颖的培训策略，该策略将deno的模式与监督模式解剖。通过使用确定性模型作为嘈杂的教师来创建噪声信号，并从单步到多个步骤降级的过渡到由图像渲染损失监督的多个步骤，与确定性教师相比，我们的方法显着提高了绩效。此外，我们的方法是灵活的，因为它可以从各种3D高斯Splat（3DG）教师中学习，但适应性最低。我们通过超越了两个不同的确定性模型作为教师的表现来证明这一点，从而强调了我们框架的潜在普遍性。我们的方法进一步结合了指导机制，以从多种视图中汇总信息，并在可用多种视图时提高重建质量。对象级和场景级数据集的实验结果证明了我们框架的有效性。

### Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction 
[[arxiv](https://arxiv.org/abs/2412.00556)] [[cool](https://papers.cool/arxiv/2412.00556)] [[pdf](https://arxiv.org/pdf/2412.00556)]
> **Authors**: Shiyu Zhao,Zhenting Wang,Felix Juefei-Xu,Xide Xia,Miao Liu,Xiaofang Wang,Mingfu Liang,Ning Zhang,Dimitris N. Metaxas,Licheng Yu
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: Technical report, 18 pages
- **标题**: 通过搜索最佳视觉令牌减少来加速多模式大语模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 盛行多模式的大型语言模型（MLLMS）将输入图像作为视觉令牌编码，并将其馈入语言骨架，类似于大型语言模型（LLMS）处理文本令牌的方式。但是，随着图像的分辨率，视力令牌的数量二次增加，导致了巨大的计算成本。在本文中，我们考虑从两种情况下提高MLLM的效率，（i）降低计算成本而不会降低性能。 （ii）通过给定的预算提高绩效。我们从主要的发现开始，每个视觉令牌的排名在第一层以外的每一层中都相似。基于它，我们假设基本的顶视令牌的数量不会沿层增加。因此，对于场景I，我们提出了一种贪婪的搜索算法（g-search），以找到最少的视觉令牌，以保持从浅层到深处的每一层。有趣的是，G-Search能够根据我们的假设达到最佳减少策略。对于场景II，基于G-Search的减少策略，我们设计了一个参数Sigmoid函数（P-Sigmoid）来指导MLLM每一层的降低，其参数通过贝叶斯优化优化。广泛的实验表明，我们的方法可以显着加速这些流行的MLLM，例如llava和intervl2型号的$ 2 \ times $ $ \ times $，没有性能下降。当预算有限时，我们的方法还远远超过其他令牌减少方法，在效率和有效性之间取得了更好的权衡。

### Human Action CLIPS: Detecting AI-generated Human Motion 
[[arxiv](https://arxiv.org/abs/2412.00526)] [[cool](https://papers.cool/arxiv/2412.00526)] [[pdf](https://arxiv.org/pdf/2412.00526)]
> **Authors**: Matyas Bohacek,Hany Farid
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 人类动作剪辑：检测AI生成的人类运动
- **领域**: 计算机视觉和模式识别,人工智能,图形
- **摘要**: 成熟的AI生成的视频生成继续穿越了奇特的山谷的旅程，以产生与现实无法区分的内容。与许多令人兴奋和创造性的应用程序混合在一起的是损害个人，组织和民主国家的恶意应用。我们描述了一种有效而强大的技术，可以将真实与AI生成的人类运动区分开。该技术利用多模式的语义嵌入，使其对通常混淆更低至中层方法的洗钱类型可靠。该方法是针对视频剪辑的定制数据集进行评估的，该数据集具有由七个文本到视频AI模型生成的人类动作并匹配真实镜头。

### Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding 
[[arxiv](https://arxiv.org/abs/2412.00493)] [[cool](https://papers.cool/arxiv/2412.00493)] [[pdf](https://arxiv.org/pdf/2412.00493)]
> **Authors**: Duo Zheng,Shijia Huang,Liwei Wang
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: 14 pages, 4 figures
- **标题**: Video-3D LLM：学习位置感知的视频表示，用于3D场景理解
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式大语言模型（MLLM）的快速发展对各种多模式任务产生了重大影响。但是，这些模型在需要在3D环境中需要空间理解的任务中面临挑战。已经做出了增强MLLM的努力，例如合并点云功能，但是模型的学会表示形式与3D场景的固有复杂性之间存在相当大的差距。这种差异很大程度上源于对2D数据的MLLM的培训，这限制了它们在理解3D空间中的有效性。为了解决这个问题，在本文中，我们提出了一个新颖的通才模型，即视频-3d LLM，以了解3D场景的理解。通过将3D场景视为动态视频，并将编码的3D位置纳入这些表示形式，我们的Video-3D LLM更准确地将视频表示与真实的空间上下文对齐。此外，我们已经实施了最大的覆盖范围抽样技术，以优化计算成本和绩效效率之间的平衡。广泛的实验表明，我们的模型在几个3D场景上实现了最先进的性能，了解包括扫描仪，Multi3Drefer，Scan2Cap，Scanqa和SQA3D。

### Jailbreak Large Vision-Language Models Through Multi-Modal Linkage 
[[arxiv](https://arxiv.org/abs/2412.00473)] [[cool](https://papers.cool/arxiv/2412.00473)] [[pdf](https://arxiv.org/pdf/2412.00473)]
> **Authors**: Yu Wang,Xiaofei Zhou,Yichen Wang,Geyuan Zhang,Tianxing He
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 越狱通过多模式链接大型视觉模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着大型视觉模型（VLM）的重大发展，人们对它们的潜在滥用和滥用的担忧迅速发展。先前的研究强调了VLMS对越狱攻击的脆弱性，精心制作的投入可以使该模型产生违反道德和法律标准的内容。但是，由于有害内容过度暴露和缺乏隐秘的恶意指导，现有的方法与GPT-4O这样的最新VLM（如GPT-4O）苦苦挣扎。在这项工作中，我们提出了一个新颖的越狱攻击框架：多模式链接（MML）攻击。 MML从密码学中汲取灵感，利用文本和图像模式的加密解码过程来减轻恶意信息的过度曝光。为了使模型的输出与恶意的意图相结合，MML采用了一种称为“邪恶对齐”的技术，在视频游戏制作场景中构建了攻击。全面的实验证明了MML的有效性。具体而言，MML越狱为Safebench的攻击成功率为97.80％，MM-Safebench的攻击成功率为97.80％，Hades-Dataset的攻击成功率为97.81％。我们的代码可从https://github.com/wangyu-ovo/mml获得

### AgriBench: A Hierarchical Agriculture Benchmark for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.00465)] [[cool](https://papers.cool/arxiv/2412.00465)] [[pdf](https://arxiv.org/pdf/2412.00465)]
> **Authors**: Yutong Zhou,Masahiro Ryo
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: Accepted by CVPPA @ECCV2024. Dataset: https://github.com/Yutong-Zhou-cv/AgriBench
- **标题**: Agribench：多模式模型的分层农业基准
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们介绍了Agribench，这是第一个旨在评估农业应用多模型模型（MM-LLMS）的农业基准。 To further address the agriculture knowledge-based dataset limitation problem, we propose MM-LUCAS, a multimodal agriculture dataset, that includes 1,784 landscape images, segmentation masks, depth maps, and detailed annotations (geographical location, country, date, land cover and land use taxonomic details, quality scores, aesthetic scores, etc), based on the Land Use/Cover Area Frame Survey (LUCAS)数据集包含有关欧盟（EU）领土土地使用和土地覆盖的可比统计数据。这项工作提出了推进农业MM-llms的开创性观点，并且仍在进行中，为未来的特定专家基于知识的MM-llms提供了宝贵的见解。

### ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2412.00447)] [[cool](https://papers.cool/arxiv/2412.00447)] [[pdf](https://arxiv.org/pdf/2412.00447)]
> **Authors**: Xubing Ye,Yukang Gan,Yixiao Ge,Xiao-Ping Zhang,Yansong Tang
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: 11 pages, 4 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉语言模型（LVLM）在多模式任务中取得了重大成功。但是，在资源有限的设备上，处理较长的视觉令牌的计算成本可能非常昂贵。先前的方法已经确定了大语言模型（LLM）解码器层中的视觉令牌的冗余，并通过使用预定的或固定比率修剪令牌来减少令牌，从而减少了计算开销。尽管如此，我们观察到，修剪比的影响在不同的LLM层和实例（图像推出对）上有所不同。因此，必须制定层面和实例的视觉令牌修剪策略，以有效平衡计算成本和模型性能。我们提出了ATP-llava，这是一种新颖的方法，可以自适应地确定每个LLM层的实例特定令牌修剪比。具体而言，我们引入了一个自适应令牌修剪（ATP）模块，该模块根据输入实例自适应地计算重要性得分和修剪阈值。 ATP模块可以在任何两个LLM层之间无缝集成，并具有可忽略的计算开销。此外，我们开发了一种空间增强修剪（SAP）策略，该策略以代币的冗余和空间建模的观点来修剪视觉令牌。我们的方法在保持性能的同时将平均代币计数减少了75％，在七个广泛使用的基准中，降解仅为1.9％。可以通过https://yxxxb.github.io/atp-llava-page/访问项目页面。

### Fusing Physics-Driven Strategies and Cross-Modal Adversarial Learning: Toward Multi-Domain Applications 
[[arxiv](https://arxiv.org/abs/2412.00341)] [[cool](https://papers.cool/arxiv/2412.00341)] [[pdf](https://arxiv.org/pdf/2412.00341)]
> **Authors**: Hana Satou,Alan Mitkiy
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 融合物理驱动的策略和跨模式对抗性学习：迈向多域应用
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 跨模式对抗学习和物理驱动的方法的融合代表了解决复杂多模式任务和科学计算中挑战的尖端方向。这篇综述着重于系统地分析这两种方法如何协同整合，以增强各种应用领域的性能和鲁棒性。通过解决诸如模态差异，有限的数据可用性和模型鲁棒性不足之类的关键障碍，本文突出了基于物理学的优化框架在促进有效且可解释的对抗性扰动产生中的作用。该综述还探讨了跨模式对抗学习的重大进步，包括在图像跨模式检索（例如，红外和RGB匹配），科学计算（例如，求解部分微分方程式）以及在视觉系统中物理一致性约束下进行优化等任务中的应用。通过检查理论基础和实验结果，这项研究证明了结合这些方法来处理复杂场景并提高多模式系统安全性的潜力。最后，我们概述了未来的方向，提出了一个新颖的框架，该框架通过对抗性优化将物理原则统一，为研究人员提供了一种具有理论和实际意义的鲁棒和适应性的跨模式学习方法的途径。

### SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters 
[[arxiv](https://arxiv.org/abs/2412.00174)] [[cool](https://papers.cool/arxiv/2412.00174)] [[pdf](https://arxiv.org/pdf/2412.00174)]
> **Authors**: Jianping Jiang,Weiye Xiao,Zhengyu Lin,Huaizhong Zhang,Tianxiang Ren,Yang Gao,Zhiqian Lin,Zhongang Cai,Lei Yang,Ziwei Liu
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Solami：与3D自动角色的沉浸式互动的社会视觉语言行动建模
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 人是社交动物。如何为3D自主角色配备可以感知，理解和与人互动的类似社会智能仍然是一个开放但有创始的问题。在本文中，我们介绍了Solami，这是第一个端到端的社会视觉语言行动（VLA）建模框架，用于与3D自治角色的沉浸式互动。具体而言，Solami从三个方面构建了3D自治角色：（1）社会VLA体系结构：我们提出了一个统一的社交VLA框架，以基于用户的多模式输入来生成多模式响应（语音和运动），以驱动与社交互动的字符。 （2）交互式多模式数据：我们提出Synmsi，这是一种由自动管道生成的合成多模式交互数据集，仅使用现有运动数据集来解决数据稀缺问题。 （3）沉浸式VR接口：我们开发了一个VR接口，使用户能够与这些角色的各种架构驱动的这些角色进行沉浸式互动。广泛的定量实验和用户研究表明，我们的框架会导致更精确和自然的角色响应（在语音和运动中）与延迟较低的用户期望一致。

### ROSE: Revolutionizing Open-Set Dense Segmentation with Patch-Wise Perceptual Large Multimodal Model 
[[arxiv](https://arxiv.org/abs/2412.00153)] [[cool](https://papers.cool/arxiv/2412.00153)] [[pdf](https://arxiv.org/pdf/2412.00153)]
> **Authors**: Kunyang Han,Yibo Hu,Mengxue Qu,Hailin Shi,Yao Zhao,Yunchao Wei
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 玫瑰：通过贴片感知大型多模型革新开放式密集分段
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 剪辑和大型多模型模型（LMM）的进步已启用了开放式录音率和自由文本细分，但是现有模型仍然需要预定义的类别提示，从而限制了自由形式的类别自生。大多数分割的LMM还局限于稀疏预测，从而限制了其在开放式环境中的适用性。相比之下，我们提出了Rose，这是一种革命性的开放设定的密集分段LMM，可以通过贴片感知来实现密集的面具预测和开放类别的生成。我们的方法将每个图像斑块视为感兴趣的独立区域，从而使模型能够同时预测密集和稀疏掩盖。此外，新设计的指令 - 响应范式充分利用了LMM的生成和概括功能，实现了与封闭设置约束或预定义类别无关的类别预测。为了进一步增强蒙版细节和类别的精度，我们介绍了基于对话的改进范式，将上一步的预测结果与文本提示进行了修订。广泛的实验表明，Rose在统一框架中实现了各种细分任务的竞争性能。代码将发布。

### DLaVA: Document Language and Vision Assistant for Answer Localization with Enhanced Interpretability and Trustworthiness 
[[arxiv](https://arxiv.org/abs/2412.00151)] [[cool](https://papers.cool/arxiv/2412.00151)] [[pdf](https://arxiv.org/pdf/2412.00151)]
> **Authors**: Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Ser-Nam Lim,Rajiv Ramnath
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Dlava：文档语言和视觉助理，以提高解释性和可信度的答案本地化
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 文档视觉问题回答（VQA）要求模型在复杂的视觉布局中解释文本信息，并理解空间关系以根据文档图像回答问题。现有方法通常缺乏解释性，并且无法精确地将答案定位在文档中，从而阻碍用户验证响应和理解推理过程的能力。此外，标准指标（如平均归一化Levenshtein的相似性（ANLS））的重点是文本准确性，但忽略了空间正确性。我们介绍了Dlava，这是一种新颖的方法，可增强多模式大型语言模型（MLLM），并具有文档VQA的答案本地化功能。我们的方法将图像注释直接集成到MLLM管道中，从而通过使用户能够追踪模型的推理来提高可解释性。我们介绍了OCR依赖性和无OCR架构，并采用无OCR方法消除了对单独的文本识别组件的需求，从而降低了复杂性。据我们所知，Dlava是在多模式质量质量质量质量检查中引入答案本地化的第一种方法，这标志着增强用户信任并降低AI幻觉的风险迈出了重要的一步。我们的贡献包括通过在空间注释的视觉内容中进行响应来提高可解释性和可靠性，在MLLM中引入答案本地化，提出一种简化的管道，该管道将MLLM与文本检测模块结合在一起，并使用文本和空间精确度量进行全面评估，包括联合（IOU）的相互作用。标准数据集的实验结果表明，DLAVA可以达到SOTA性能，从而显着提高了模型透明度和可靠性。我们的方法为文档VQA设定了一个新的基准，强调了精确答案本地化和模型解释性的关键重要性。

### Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers 
[[arxiv](https://arxiv.org/abs/2412.00142)] [[cool](https://papers.cool/arxiv/2412.00142)] [[pdf](https://arxiv.org/pdf/2412.00142)]
> **Authors**: Chancharik Mitra,Brandon Huang,Tianning Chai,Zhiqiu Lin,Assaf Arbelle,Rogerio Feris,Leonid Karlinsky,Trevor Darrell,Deva Ramanan,Roei Herzig
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 稀疏注意向量：生成多模型的特征是判别视觉分类器
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 在各种视觉语言（VL）任务（例如图像字幕或视觉问题答案）等各种视觉语言（VL）任务，例如Llava和Qwen-Vl Excel，例如Llava和Qwen-Vl Excel，例如Llava和Qwen-Vl Excel，例如Llava和Qwen-Vl Excel，例如生成大型多模型。尽管性能很强，但LMM并不直接适用于基础判别视觉任务（即需要离散标签预测的任务），例如图像分类和多项选择VQA。利用LMM进行判别任务的一个关键挑战是从生成模型中提取有用的功能。为了克服这个问题，我们提出了一种在模型潜在空间中查找功能的方法，以更有效地利用LMM来进行判别任务。为此，我们提出了稀疏的注意向量（SAVS） - 一种无芬太的方法，它利用LMMS中稀疏注意力头部激活（少于1 \％的头部）作为VL任务的强大功能。只有很少的示例，SAV证明了与各种判别任务的各种射击和填充基线相比，相比之下。我们的实验还暗示，SAV可以通过其他示例来扩展性能，并推广到类似的任务，将SAV确定为有效和健壮的多模式特征表示。

### Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads 
[[arxiv](https://arxiv.org/abs/2412.00127)] [[cool](https://papers.cool/arxiv/2412.00127)] [[pdf](https://arxiv.org/pdf/2412.00127)]
> **Authors**: Siqi Kou,Jiachun Jin,Chang Liu,Ye Ma,Jian Jia,Quan Chen,Peng Jiang,Zhijie Deng
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Orthus：带有特定于模态头的自回旋交织的图像文本生成
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 我们介绍了Orthus，这是一种自动回归（AR）变压器，它擅长生成给定的文本提示的图像，根据视觉输入回答问题，甚至可以制作冗长的图像文本交织的内容。与统一多模型建模的先前艺术不同，Orthus在AR建模原理下同时应对离散的文本令牌和连续图像特征。视觉信号的连续处理可最大程度地减少图像理解和产生的信息损失，而完全AR公式则使模态之间的相关性表征直接。使Orthus能够利用这些优势的关键机制在于其模态特定的头部 - 一种常规语言建模（LM）头预测离散文本令牌，一个扩散头会在主链输出上生成连续的图像特征。我们通过使用软替代方案在现有统一AR模型中替换矢量量化（VQ）操作来设计有效的策略，并用软替代替代媒介，并将扩散头引入扩散头，然后将添加的模块调整为重建图像，我们可以轻松地创建Orthus-base模型（例如，在MERE 72 A100 GPU小时内）。 Orthus-base可以进一步接受训练后，以更好地模型交织的图像和文本。从经验上讲，Orthus超过了跨标准基准测试的竞争基线，包括Show-O和Chameleon，使用7B参数达到了0.58，MME-P得分为1265.8。 Orthus还显示出非凡的混合模式生成能力，反映了处理复杂的实用生成任务的潜力。

### SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments 
[[arxiv](https://arxiv.org/abs/2412.00114)] [[cool](https://papers.cool/arxiv/2412.00114)] [[pdf](https://arxiv.org/pdf/2412.00114)]
> **Authors**: Yue Cao,Yun Xing,Jie Zhang,Di Lin,Tianwei Zhang,Ivor Tsang,Yang Liu,Qing Guo
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 场景图：现实环境中针对视觉语言模型的场景连接印刷对手计划者
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉模型（LVLM）在解释视觉内容方面表现出了显着的功能。尽管现有作品展示了这些模型故意放置对抗文本的脆弱性，但这些文本通常很容易被识别为异常。在本文中，我们提出了第一种生成场景连接的印刷对抗攻击的方法，这些攻击误导了高级LVLM，同时通过基于LLM的代理的能力保持视觉自然性。我们的方法解决了三个关键问题：要生成哪些对抗性文本，将其放置在场景中以及如何无缝集成。我们提出了一个无训练的多模式LLM驱动的场景连接印刷对抗计划（ScenetAp），该计划采用了三阶段的过程：场景理解，对抗性计划和无缝集成。 ScenetEtap利用经过思考的推理来理解场景，制定有效的对抗文本，战略性地计划其位置，并提供图像中自然整合的详细说明。接下来是场景连接的文本脱口机，该文本脱口机使用局部扩散机制执行攻击。我们通过打印和将生成的补丁放在物理环境中，展示其实际含义，将方法扩展到实际情况。广泛的实验表明，即使在捕获了物理设置的新图像之后，我们的场景相互作用的对抗文本也成功地误导了最新的LVLM，包括Chatgpt-4O。我们的评估表明，攻击成功率显着提高，同时保持视觉自然性和上下文适当性。这项工作突出了当前视觉语言模型中的脆弱性，以实现复杂的场景互动攻击，并提供了对潜在防御机制的见解。

### ElectroVizQA: How well do Multi-modal LLMs perform in Electronics Visual Question Answering? 
[[arxiv](https://arxiv.org/abs/2412.00102)] [[cool](https://papers.cool/arxiv/2412.00102)] [[pdf](https://arxiv.org/pdf/2412.00102)]
> **Authors**: Pragati Shuddhodhan Meshram,Swetha Karthikeyan,Bhavya,Suma Bhat
> **First submission**: 2024-11-27
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Electrovizqa：多模式LLM在电子视觉问题上的表现如何？
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 多模式的大语言模型（MLLM）对其处理多模式数据的能力引起了极大的关注，从而增强了对复杂问题的上下文理解。 MLLM在诸如视觉问题回答（VQA）之类的任务中证明了出色的功能；但是，他们经常在基本的工程问题上挣扎，并且缺乏专门的数据集来培训数字电子等主题。为了解决这一差距，我们提出了一个名为ElectrovizQA的基准数据集，专为评估MLLM在本科课程中常见的数字电子电路问题上的性能。该数据集是针对数字电子中VQA任务量身定制的第一个此类数据集，其中包括大约626个视觉问题，提供了数字电子主题的全面概述。本文严格评估了MLLM可以理解和解决数字电子电路问题的程度，从而提供了对其在该专业领域内的能力和局限性的见解。通过介绍此基准数据集，我们旨在激励MLLM在工程教育中的应用中进一步的研究和发展，最终弥合性能差距并增强这些模型在技术领域的功效。

### MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image 
[[arxiv](https://arxiv.org/abs/2412.00060)] [[cool](https://papers.cool/arxiv/2412.00060)] [[pdf](https://arxiv.org/pdf/2412.00060)]
> **Authors**: Shezheng Song,Chengxiang He,Shasha Li,Shan Zhao,Chengyu Wang,Tianwei Yan,Xiaopeng Li,Qian Wan,Jun Ma,Jie Yu,Xiaoguang Mao
> **First submission**: 2024-11-25
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Mosabench：用于评估多模式大语言模型的多对象情绪分析基准理解复杂图像
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式的大语言模型（MLLM）在高级语义任务中显示出了显着的进步，例如视觉问答答案，图像字幕和情感识别。但是，尽管有进步，但仍缺乏标准化的基准来评估多对象情绪分析中的MLLMS性能，这是语义理解的关键任务。为了解决这一差距，我们介绍了Mosabench，这是一个专门用于多对象情绪分析的新型评估数据集。 Mosabench包括大约1,000张具有多个对象的图像，要求MLLM独立评估每个对象的情感，从而反映现实世界中的复杂性。 Mosabench中的关键创新包括基于距离的目标注释，用于评估以标准化产出的后处理以及改进的评分机制。我们的实验揭示了当前MLLM的显着局限性：虽然某些模型（例如Mplug-Owl和Qwen-VL2）表现出对与情感相关的特征的有效关注，而其他模型则表现出散射的焦点和性能下降，尤其是随着物体之间的空间距离的增加。这项研究强调了MLLM的需求，以提高复杂，多对象情感分析任务的准确性，并确立Mosabench作为推进MLLM中情感分析功能的基础工具。

### Human Multi-View Synthesis from a Single-View Model:Transferred Body and Face Representations 
[[arxiv](https://arxiv.org/abs/2412.03011)] [[cool](https://papers.cool/arxiv/2412.03011)] [[pdf](https://arxiv.org/pdf/2412.03011)]
> **Authors**: Yu Feng,Shunsi Zhang,Jian Shu,Hanfeng Zhao,Guoliang Pang,Chi Zhang,Hao Wang
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: 人类多视图从单视图模型：转移的身体和面部表示
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 从单视图生成多视图的人类图像是一个复杂而重大的挑战。尽管多视图对象产生的最新进展通过扩散模型显示出令人印象深刻的结果，但人类的新型视图综合仍受到3D人类数据集的有限可用性的限制。因此，许多现有模型都难以生产现实的人体形状或准确捕获细粒度的面部细节。为了解决这些问题，我们提出了一个创新的框架，该框架利用了人体和面部表征进行多视图人类综合。具体而言，我们使用在大型人数据集上预测的单视图模型来开发多视图的主体表示，旨在将单视图模型的2D知识扩展到多视图扩散模型。此外，为了增强模型的详细恢复能力，我们将转移的多模式面部特征集成到我们训练的人类扩散模型中。基准数据集的实验评估表明，我们的方法的表现优于当前的最新方法，在多视图人类合成中实现了卓越的性能。

### Progressive Vision-Language Prompt for Multi-Organ Multi-Class Cell Semantic Segmentation with Single Branch 
[[arxiv](https://arxiv.org/abs/2412.02978)] [[cool](https://papers.cool/arxiv/2412.02978)] [[pdf](https://arxiv.org/pdf/2412.02978)]
> **Authors**: Qing Zhang,Hang Guo,Siyuan Yang,Qingli Li,Yan Wang
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: 逐步视觉语言提示单分支多级多级单元语义分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 病理细胞语义分割是计算病理学中的一项基本技术，对于诸如癌症诊断和有效治疗之类的应用至关重要。鉴于在各种器官中都存在多种细胞类型，并且在细胞大小和形状方面存在细微的差异，多阶级的多级细胞分割尤其具有挑战性。大多数现有方法采用多分支框架来增强特征提取，但通常会导致复杂的体系结构。此外，由于复杂的质地细节，对视觉信息的依赖限制了多类分析中的性能。为了应对这些挑战，我们提出了一种使用单个分支（Monch）的多级多级细胞语义分割方法，该方法利用视觉语言输入。具体而言，我们设计了一种分层特征提取机制，可为分割各种形状的细胞（包括高频，卷积和拓扑特征）提供粗到精细的特征。受文本和多元透明视觉功能的协同作用的启发，我们引入了一个渐进的提示解码器，以协调多模式信息，从而集成了从细粒度到粗粒度的特征，以更好地捕获上下文。 Pannuke数据集的广泛实验，具有显着的类别失衡和细微的细胞大小和形状变化，表明Monch的表现优于最先进的细胞分割方法和视觉语言模型。代码和实施将公开可用。

### Who Brings the Frisbee: Probing Hidden Hallucination Factors in Large Vision-Language Model via Causality Analysis 
[[arxiv](https://arxiv.org/abs/2412.02946)] [[cool](https://papers.cool/arxiv/2412.02946)] [[pdf](https://arxiv.org/pdf/2412.02946)]
> **Authors**: Po-Hsuan Huang,Jeng-Lin Li,Chin-Po Chen,Ming-Ching Chang,Wei-Chao Chen
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: Accepted by WACV2025
- **标题**: 谁带来了飞盘：通过因果关系分析，在大视力语言模型中探测隐藏的幻觉因子
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体
- **摘要**: 大型视觉模型（LVLM）的最新进展显着增强了他们与自然语言一起理解视觉投入的能力。但是，在其实际应用程序中，一个主要的挑战是幻觉，LVLMS会产生不存在的视觉元素，从而侵蚀用户信任。驱动这种多模式幻觉的基本机制知之甚少。最小的研究阐明了诸如天空，树木或草田等环境涉及LVLM幻觉。我们假设隐藏的因素，例如物体，上下文和语义前后背景结构，引起了幻觉。这项研究提出了一种新的因果方法：一种识别这些隐藏因素的幻觉探测系统。通过分析图像，文本提示和网络显着性之间的因果关系，我们会系统地探索干预措施以阻止这些因素。我们的实验发现表明，基于我们的分析的直接技术可以显着减少幻觉。此外，我们的分析表明，有可能编辑网络内部设备以最大程度地减少幻觉输出。

### STORM: Strategic Orchestration of Modalities for Rare Event Classification 
[[arxiv](https://arxiv.org/abs/2412.02805)] [[cool](https://papers.cool/arxiv/2412.02805)] [[pdf](https://arxiv.org/pdf/2412.02805)]
> **Authors**: Payal Kamboj,Ayan Banerjee,Sandeep K. S. Gupta
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: Accepted in IEEE Asilomar Conference on Signals, Systems, and Computers, 2024
- **标题**: 风暴：稀有事件分类的战略编排
- **领域**: 计算机视觉和模式识别
- **摘要**: 在诸如生物医学之类的领域中，专家见解对于选择人工智能（AI）方法的最有用的方式至关重要。但是，使用所有可用方式构成挑战，尤其是在确定每种模式对性能的影响并优化其组合以进行准确分类时。传统方法诉诸于手动反复试验，缺乏辨别最相关方式的系统框架。此外，尽管多模式学习可以利用所有可用方式从不同来源集成信息，但通常是不切实际的和不必要的。为了解决这个问题，我们引入了基于熵的算法风暴，以解决罕见事件的模态选择问题。该算法系统地评估了单个模式及其组合的信息内容，从而确定了稀有班级分类任务必不可少的最歧视性特征。通过癫痫发作区检测案例研究，我们证明了算法在增强分类性能方面的功效。通过选择有用的模式子集，我们的方法为更有效的AI驱动生物医学分析铺平了道路，从而在临床环境中提高了疾病诊断。

### MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual Cues 
[[arxiv](https://arxiv.org/abs/2412.02734)] [[cool](https://papers.cool/arxiv/2412.02734)] [[pdf](https://arxiv.org/pdf/2412.02734)]
> **Authors**: Zhaofeng Hu,Sifan Zhou,Shibo Zhao,Zhihang Yuan
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: Accepted by ICRA 2025
- **标题**: MVCTRACK：通过多模式指导的虚拟提示提高3D点云跟踪
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 3D单一对象跟踪对于自动驾驶和机器人技术至关重要。现有的方法通常在稀疏和不完整的点云方案中挣扎。为了解决这些限制，我们提出了一个多模式引导的虚拟提示投影（MVCP）方案，该方案生成虚拟提示以丰富稀疏点云。此外，我们根据生成的虚拟提示引入了增强的跟踪器MVCTRACK。具体而言，MVCP方案无缝将RGB传感器集成到基于激光雷达的系统中，并利用一组2D检测来创建密集的3D虚拟提示，从而显着改善了点云的稀疏性。这些虚拟提示可以自然地与现有的基于激光雷达的3D跟踪器集成，从而获得可观的性能增长。广泛的实验表明，我们的方法在Nuscenes数据集上实现了竞争性能。

### AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information? 
[[arxiv](https://arxiv.org/abs/2412.02611)] [[cool](https://papers.cool/arxiv/2412.02611)] [[pdf](https://arxiv.org/pdf/2412.02611)]
> **Authors**: Kaixiong Gong,Kaituo Feng,Bohao Li,Yibing Wang,Mofan Cheng,Shijia Yang,Jiaming Han,Benyou Wang,Yutong Bai,Zhuoran Yang,Xiangyu Yue
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: Project page: https://av-odyssey.github.io/
- **标题**: AV-Odyssey替补席：您的多模式LLM可以真正了解视听信息吗？
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体,声音,音频和语音处理
- **摘要**: 最近，多模式的大型语言模型（MLLM），例如GPT-4O，Gemini 1.5 Pro和Reka Core，已扩大其功能，包括视觉和音频方式。尽管这些模型在广泛的视听应用中表现出令人印象深刻的性能，但我们提出的deaftest表明，MLLM经常与人类发现的简单任务挣扎：1）确定两种声音中的哪一个更大，而2）确定两种声音中的哪个声音更高。在这些观察结果的激励下，我们介绍了Av-Odyssey Bench，这是一个全面的视听基准测试，旨在评估这些MLLM是否可以真正理解视听信息。该基准包括4,555个精心制作的问题，每个问题都包含文本，视觉和音频组件。为了成功地推断答案，模型必须有效利用视觉和音频输入的线索。为了确保对MLLM响应的精确和客观评估，我们将问题构成多项选择，从而消除了对人类评估或LLM辅助评估的需求。我们基准了一系列封闭式和开源模型，并总结了观察结果。通过揭示当前模型的局限性，我们旨在为未来的数据集收集和模型开发提供有用的见解。

### OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2412.02592)] [[cool](https://papers.cool/arxiv/2412.02592)] [[pdf](https://arxiv.org/pdf/2412.02592)]
> **Authors**: Junyuan Zhang,Qintong Zhang,Bin Wang,Linke Ouyang,Zichen Wen,Ying Li,Ka-Ho Chow,Conghui He,Wentao Zhang
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: Work in progress
- **标题**: OCR阻碍抹布：评估OCR对检索生成的级联影响
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过整合外部知识以减少幻觉并结合最新信息而无需重新培训，可以通过集成外部知识来增强大语言模型（LLM）。作为抹布的重要组成部分，外部知识库通常是通过使用光学角色识别（OCR）从非结构化PDF文档中提取结构化数据来构建的。但是，鉴于OCR的不完善预测和结构化数据的固有不均匀表示，知识库不可避免地包含各种OCR噪声。在本文中，我们介绍了Ohrbench，这是理解OCR对抹布系统的级联影响的第一个基准。 OHRBENCH包括8,561个从七个现实世界的抹布应用程序域中精心选择的非结构化文档图像，以及文档中的多模式元素得出的8,498个问答对，提出了用于抹布的现有OCR解决方案。为了更好地理解OCR对抹布系统的影响，我们确定了两种主要类型的OCR噪声：语义噪声和格式化噪声，并应用扰动以生成一组结构化数据，并具有不同程度的每个OCR噪声。使用Ohrbench，我们首先对当前的OCR解决方案进行了全面的评估，并揭示没有一个有能力为抹布系统构建高质量的知识库。然后，我们系统地评估了这两种噪声类型的影响，并证明了OCR噪声和抹布性能之间的趋势关系。我们的ohrbench，包括PDF文档，问答和地面真相结构化数据，网址为：https：//github.com/opendatalab/ohr-bench

### MedTet: An Online Motion Model for 4D Heart Reconstruction 
[[arxiv](https://arxiv.org/abs/2412.02589)] [[cool](https://papers.cool/arxiv/2412.02589)] [[pdf](https://arxiv.org/pdf/2412.02589)]
> **Authors**: Yihong Chen,Jiancheng Yang,Deniz Sayin Mercadier,Hieu Le,Pascal Fua
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: MEDTET：4D心脏重建的在线运动模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一种从稀疏术中数据重建3D心脏运动的新方法。尽管现有方法可以从完整的3D体积成像中准确地重建3D器官的几何形状，但在通常有限观察到的数据（例如几个2D帧或1D信号）的手术干预措施中，无法实时使用它们。我们提出了一个多功能框架，用于从此类部分数据中重建3D运动。它将3D空间离散到具有符号距离值的可变形四面体网格中，在维持对运动动力学的明确控制的同时，提供了隐式无限分辨率。鉴于从术前的全容量数据重建的最初的3D模型，配备通用观察编码器的系统可以从完整3D体积，几个2D MRI切片甚至1D信号中重建相干的3D心脏运动。关于心脏干预情景的广泛实验表明，我们从各种稀疏的实时观察结果中产生合理和解剖上一致的3D运动重建的能力，突出了其多模式心脏成像的潜力。我们的代码和模型将在https://github.com/scalsol/medtet上提供。

### Copy-Move Forgery Detection and Question Answering for Remote Sensing Image 
[[arxiv](https://arxiv.org/abs/2412.02575)] [[cool](https://papers.cool/arxiv/2412.02575)] [[pdf](https://arxiv.org/pdf/2412.02575)]
> **Authors**: Ze Zhang,Enyuan Zhao,Ziyi Wan,Jie Nie,Xinyue Liang,Lei Huang
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: 7 figs, 7 tables
- **标题**: 复制移动伪造的检测和遥感图像的问答
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 本文介绍了遥感复制移动问题回答（RSCMQA）的任务。与传统的遥感视觉问题回答（RSVQA）不同，RSCMQA专注于解释复杂的篡改场景和推断对象之间的关系。根据国防安全和土地资源监控的实际需求，我们开发了一个准确，全面的全球数据集，用于遥感图像复制移动问题答案，名为RS-CMQA-2.-2.-2.1M。这些图像是从14个国家 /地区的29个不同地区收集的。此外，我们已经完善了平衡的数据集RS-CMQA-B，以解决遥感领域中长尾数据的长期问题。此外，我们提出了一个区域歧视性指导性的多模式CMQA模型，该模型通过利用提示提示来回答有关篡改图像的问题的准确性，以利用提示源和篡改域之间的差异和连接。广泛的实验表明，与一般VQA和RSVQA模型相比，我们的方法为RS-CMQA提供了更强的基准。我们的数据集和代码可在https://github.com/shenyedepisa/rscmqa上找到。

### SJTU:Spatial judgments in multimodal models towards unified segmentation through coordinate detection 
[[arxiv](https://arxiv.org/abs/2412.02565)] [[cool](https://papers.cool/arxiv/2412.02565)] [[pdf](https://arxiv.org/pdf/2412.02565)]
> **Authors**: Joongwon Chae,Zhenyu Wang,Peiwu Qin
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: 15 pages, 3 figures
- **标题**: SJTU：多模型模型中通过坐标检测进行统一分割的空间判断
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管视觉理解方面取得了重大进展，但在多模式体系结构中实施图像分割仍然是现代人工智能系统中的基本挑战。现有的视觉语言模型主要依赖于骨干架构或基于剪辑的嵌入学习，它表明了细粒度的空间定位和操作能力的固有局限性。本文介绍了SJTU：多模型模型中的空间判断 - 通过坐标检测朝着统一的分割，该框架利用空间坐标理解来桥梁视觉互动和精确的细分，从而通过自然语言指令实现了准确的目标识别。该框架提出了一种通过多模式空间中的空间推断将分割技术与视觉模型集成的方法。通过利用标准化的坐标检测来边界框并将其转换为可行的分割输出，我们在多模式体系结构中的空间和语言表示之间建立了连接。实验结果表明，基准数据集的性能卓越，可可2017年的IOU得分为0.5958，而Pascal VOC上的得分为0.6758。对单个NVIDIA RTX 3090 GPU进行测试，分辨率图像的平均推理时间为每张图像7秒，这表明该框架在准确性和实践可部署性方面的有效性。该项目代码可从https://github.com/jw-chae/sjtu获得

### Multimodal Remote Sensing Scene Classification Using VLMs and Dual-Cross Attention Networks 
[[arxiv](https://arxiv.org/abs/2412.02531)] [[cool](https://papers.cool/arxiv/2412.02531)] [[pdf](https://arxiv.org/pdf/2412.02531)]
> **Authors**: Jinjin Cai,Kexin Meng,Baijian Yang,Gang Shao
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: 使用VLM和双重跨度注意网络的多模式遥感场景分类
- **领域**: 计算机视觉和模式识别
- **摘要**: 遥感场景分类（RSSC）是一项至关重要的任务，在土地使用和资源管理方面具有不同的应用程序。尽管基于图像的单形化方法表现出希望，但它们经常在诸如较高的阶级方差和类间相似性等局限性方面挣扎。合并文本信息可以通过提供其他上下文和语义理解来增强分类，但是手动文本注释是劳动密集型和昂贵的。在这项工作中，我们提出了一个新颖的RSSC框架，该框架将大型视觉模型（VLMS）产生的文本说明集成为辅助模式，而不会产生昂贵的手动注释成本。为了充分利用视觉数据和文本数据之间的潜在互补性，我们提出了一个基于双重交叉注意的网络，将这些模式融合到统一的表示中。在五个RSSC数据集中进行定量和定性评估的广泛实验表明，我们的框架始终优于基线模型。我们还验证了VLM生成的文本描述的有效性，与人类注销的描述相比。此外，我们设计了一个零射击分类方案，以表明可以有效地利用学习学到的多模式表示。这项研究为利用RSSC任务中的文本信息提供了新的机会，并提供了有希望的多模式融合结构，为未来的研究提供了见解和灵感。代码可在以下网址找到：https：//github.com/cjr7/multiatt-rssc

### CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy 
[[arxiv](https://arxiv.org/abs/2412.02210)] [[cool](https://papers.cool/arxiv/2412.02210)] [[pdf](https://arxiv.org/pdf/2412.02210)]
> **Authors**: Zhibo Yang,Jun Tang,Zhaohai Li,Pengfei Wang,Jianqiang Wan,Humen Zhong,Xuejing Liu,Mingkun Yang,Peng Wang,Shuai Bai,LianWen Jin,Junyang Lin
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: 23 pages, 14 figures; The code will be released soon
- **标题**: CC-OR：一种全面且具有挑战性的OCR基准，用于评估扫盲中的大型多模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）在用自然语言指令识别文档图像时表现出了令人印象深刻的性能。但是，目前尚不清楚识字能力在多大程度上具有丰富的结构和细粒度的视觉挑战。当前的景观缺乏有效衡量LMM识字能力的全面基准。现有的基准通常受到狭窄方案和指定任务的限制。为此，我们介绍了CC-OCR，这是一个具有各种场景，任务和挑战的全面基准。 CC-OCR包括四个以OCR为中心的曲目：多场景文本阅读，多语言文本阅读，文档解析和关键信息提取。它包括39个子集，其中有7,058个完整的注释图像，其中41％来自实际应用，并首次发布。我们评估了九种突出的LMM，并揭示了这些模型的优势和劣势，尤其是在文本接地，多取向和重复的幻觉中。 CC-OR的目的是全面评估LMM在以OCR为中心的任务上的能力，从而促进该关键领域的持续进展。

### VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding 
[[arxiv](https://arxiv.org/abs/2412.02186)] [[cool](https://papers.cool/arxiv/2412.02186)] [[pdf](https://arxiv.org/pdf/2412.02186)]
> **Authors**: Kangsan Kim,Geon Park,Youngwan Lee,Woongyeong Yeo,Sung Ju Hwang
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: Videoicl：基于信心的迭代术语学习，以进行分发视频理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视频大型多模型（LMM）的最新进展已大大提高了他们的视频理解和推理能力。但是，他们的性能下降了培训数据中代表性不足的分数（OOD）任务。由于高度计算成本，诸如OOD数据集上的微调诸如进行微调的传统方法是不切实际的。虽然具有演示示例的文章学习（ICL）在不进行微调的情况下显示出有希望的概括性能和图像语言任务，但由于视频LMM中的上下文长度有限，因此将ICL应用于视频语言任务面临挑战，因为视频需要更长的标记长度。为了解决这些问题，我们提出了Videoicl，这是一个针对OOD任务的新型视频学习框架，它引入了基于相似性的相关示例选择策略和基于置信的迭代推理方法。这允许选择最相关的示例，并根据相似性（用于推断）对它们进行排名。如果生成的响应具有较低的置信度，我们的框架会选择新的示例并再次执行推断，迭代完善结果，直到获得高信心响应为止。这种方法通过在不产生高成本的情况下扩展有效上下文长度来改善OOD视频理解性能。多个基准测试的实验结果表明性能的显着增长，尤其是在特定领域的方案中，为更广泛的视频理解应用程序奠定了基础。代码将在https://github.com/kangsankim07/videoicl上发布

### Anatomically-Grounded Fact Checking of Automated Chest X-ray Reports 
[[arxiv](https://arxiv.org/abs/2412.02177)] [[cool](https://papers.cool/arxiv/2412.02177)] [[pdf](https://arxiv.org/pdf/2412.02177)]
> **Authors**: R. Mahmood,K. C. L. Wong,D. M. Reyes,N. D'Souza,L. Shi,J. Wu,P. Kaviani,M. Kalra,G. Wang,P. Yan,T. Syeda-Mahmood
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: mber:RPI12
- **标题**: 自动化胸部X射线报告的解剖学事实检查
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着大型视觉模型的出现，可以仅使用医学图像作为以简单提示为指导的输入而生成现实的放射学报告。但是，由于他们对发现的描述中的事实错误，它们的实际效用受到了限制。在本文中，我们提出了一个新的模型，用于可解释的事实检查，该模型识别了报告中指示的发现及其位置中的错误。具体而言，我们分析了自动报告方法造成的错误类型，并得出了图像的新合成数据集，并从地面真实数据集中及其发现的真实和虚假描述及其位置配对。然后，在此Datsaset上训练了一个新的多标签跨模式对比回归网络。我们评估了所得的事实检查模型及其在纠正各种基准数据集上的几种SOTA自动化报告工具生成的报告中的实用性，结果指出，通过此类错误检测和校正，报告质量提高了40 \％。

### CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2412.03859)] [[cool](https://papers.cool/arxiv/2412.03859)] [[pdf](https://arxiv.org/pdf/2412.03859)]
> **Authors**: Hui Zhang,Dexiang Hong,Tingwei Gao,Yitong Wang,Jie Shao,Xinglong Wu,Zuxuan Wu,Yu-Gang Jiang
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: 16 pages, 13 figures
- **标题**: CreatiLayOut：用于创意布局到图像生成的暹罗多模式扩散变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 扩散模型因其产生不仅具有视觉吸引力而且具有高艺术质量的图像的能力而被认可。结果，已经提出了布局到图像（L2i）生成，以利用特定区域的位置和描述，以使其更加精确，可控制。但是，以前的方法主要集中于基于UNET的模型（例如SD1.5和SDXL），以及有限的努力探索了多模式扩散变压器（MM-DITS），这些变压器（MM-DITS）表现出强大的图像生成功能。为布局到图像生成启用MM-DIT似乎很简单，但由于如何在多种模式中引入，集成和平衡布局的复杂性，因此具有挑战性。为此，我们探索了各种网络变体，以有效地将布局指南纳入MM-DIT，并最终呈现Siamlayout。为了继承MM-DIT的优势，我们使用一组单独的网络权重来处理布局，将其视为与图像和文本方式同样重要。同时，为了减轻模式之间的竞争，我们将图像 - 图像相互作用与图像文本旁边的一个暹罗分支机构解倒和，并在后期将它们融合在一起。此外，我们贡献了一个名为Layoutsam的大规模布局数据集，其中包括270万个图像文本对和1070万个实体。每个实体都用一个边界框和详细说明注释。我们进一步构建了Layoutsam-eval基准测试，作为评估L2I发电质量的综合工具。最后，我们介绍了布局设计师，该设计师在布局计划中挖掘了大型语言模型的潜力，从而将其转变为布局生成和优化方面的专家。我们的代码，模型和数据集将在https://creatilayout.github.io上找到。

### EditScout: Locating Forged Regions from Diffusion-based Edited Images with Multimodal LLM 
[[arxiv](https://arxiv.org/abs/2412.03809)] [[cool](https://papers.cool/arxiv/2412.03809)] [[pdf](https://arxiv.org/pdf/2412.03809)]
> **Authors**: Quang Nguyen,Truong Vu,Trong-Tung Nguyen,Yuxin Wen,Preston K Robinette,Taylor T Johnson,Tom Goldstein,Anh Tran,Khoi Nguyen
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: EDITSCOUT：通过具有多模式LLM的基于扩散的编辑图像定位锻造区域
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像编辑技术是用于转换，调整，删除或以其他方式更改图像的工具。最近的研究大大提高了图像编辑工具的功能，从而创建了与真实图像几乎没有区别的影照相和语义知情区域的创建，从而在数字取证和媒体信誉中提出了新的挑战。尽管当前的图像法医技术擅长于通过传统图像操纵方法产生的锻造区域进行定位，但当前的功能难以将基于扩散的技术创建的区域定位。为了弥合这一差距，我们提出了一个新颖的框架，该框架集成了多模式大语言模型（LLM），以增强推理能力，以将基于扩散模型的编辑方法产生的图像中定位篡改区域。通过利用LLM的上下文和语义优势，我们的框架在Magicbrush，Autopplice和Perfbrush（基于新型扩散数据集）数据集上取得了令人鼓舞的结果，在MIOU和F1-SCORE指标中的先前方法都优于先前的方法。值得注意的是，我们的方法在Perfbrush数据集上出色，这是一个自我结构的测试集，具有以前看不见的编辑类型。在这里，传统方法通常步履蹒跚，得分明显低，我们的方法表明了有希望的表现。

### VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding 
[[arxiv](https://arxiv.org/abs/2412.03735)] [[cool](https://papers.cool/arxiv/2412.03735)] [[pdf](https://arxiv.org/pdf/2412.03735)]
> **Authors**: Chaoyu Li,Eun Woo Im,Pooyan Fazli
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）最近显示出在视频理解，在内容推理和跟随指导下任务方面的卓越方面取得了重大进步。但是，幻觉的问题（模型都会产生不准确或误导性内容）在视频域中仍未被忽视。基于观察到的MLLM的视觉编码器通常在视觉上不同但在语义上相似的视频对之间进行努力，我们引入了Vidhalluc，Vidhalluc是最大的基准，该基准旨在检查MLLMS中的幻觉，以了解视频理解任务。 Vidhalluc评估了三个关键维度的幻觉：（1）动作，（2）时间序列和（3）场景过渡。 Vidhalluc由5,002个视频组成，基于语义相似性和视觉差异配对，重点是最有可能发生幻觉的情况。通过全面的测试，我们的实验表明，大多数MLLM都容易受到这些维度幻觉的影响。此外，我们提出了一种无训练的方法，该方法通过在推理过程中将dinov2的空间显着性信息纳入到重新持续的视觉特征来减少幻觉。我们的结果表明，恐龙始终提高维德哈卢克的表现，平均提高了3.02％的缓解所有任务之间的幻觉。可以通过$ \ href {https://vid-halluc.github.io/} {\ text {this this link}} $访问vidhalluc基准和恐龙代码。

### Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension 
[[arxiv](https://arxiv.org/abs/2412.03704)] [[cool](https://papers.cool/arxiv/2412.03704)] [[pdf](https://arxiv.org/pdf/2412.03704)]
> **Authors**: Xiyao Wang,Zhengyuan Yang,Linjie Li,Hongjin Lu,Yuancheng Xu,Chung-Ching Lin,Kevin Lin,Furong Huang,Lijuan Wang
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 使用视觉值模型进行扩展推理时间搜索，以改善视觉理解
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 尽管视觉模型（VLM）取得了重大进步，但缺乏通过缩放推理时间计算来提高响应质量的有效方法。已知这种能力是最近大型语言模型研究中自我改善模型的核心步骤。在本文中，我们提出了可以指导VLM推理时间搜索的视觉价值模型（VISVM），以更好的视觉理解生成响应。具体而言，Visvm不仅评估当前搜索步骤中生成的句子质量，而且还可以预料到当前步骤可能导致的后续句子的质量，从而提供了长期价值。通过这种方式，VISVM将VLMS从容易产生的句子到幻觉或细节不足，从而产生更高质量的响应。实验结果表明，与贪婪的解码和其他视觉奖励信号相比，VISVM引导的搜索可以显着增强VLMs具有更丰富的视觉细节和更少的幻觉的描述性字幕的能力。此外，我们发现具有VISVM引导字幕的自我训练模型可以改善VLM在广泛的多模式基准中的性能，这表明有可能开发自我改善的VLM。我们的价值模型和代码可从https://github.com/si0wang/visvm获得。

### Personalizing Multimodal Large Language Models for Image Captioning: An Experimental Analysis 
[[arxiv](https://arxiv.org/abs/2412.03665)] [[cool](https://papers.cool/arxiv/2412.03665)] [[pdf](https://arxiv.org/pdf/2412.03665)]
> **Authors**: Davide Bucciarelli,Nicholas Moratelli,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: ECCV 2024 Workshop on Green Foundation Models
- **标题**: 个性化图像字幕的多模式大语言模型：实验分析
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体
- **摘要**: 图像字幕的任务需要算法来生成视觉输入的自然语言描述。最近的进步已经看到图像字幕研究与大语言模型（LLMS）和多模式LLM的发展之间存在融合，例如GPT-4V和Gemini，它们将仅文本LLM的功能扩展到了多种方式。本文研究了多模式LLM是否可以通过评估其在各种图像描述基准上的性能来取代传统图像字幕网络。我们通过微调方法（包括及时的学习，前缀调整和低级别的适应性）探讨了这些模型的零射击功能及其对不同语义域的适应性。我们的结果表明，尽管多模式LLMS实现了令人印象深刻的零击性能，对特定领域进行微调，同时保持其概括能力完整仍然具有挑战性。我们讨论了这些发现对未来研究图像字幕的含义以及更适应性的多模式LLM的发展。

### Streaming Detection of Queried Event Start 
[[arxiv](https://arxiv.org/abs/2412.03567)] [[cool](https://papers.cool/arxiv/2412.03567)] [[pdf](https://arxiv.org/pdf/2412.03567)]
> **Authors**: Cristobal Eyzaguirre,Eric Tang,Shyamal Buch,Adrien Gaidon,Jiajun Wu,Juan Carlos Niebles
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 查询事件开始的流媒体检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 机器人技术，自动驾驶，增强现实以及许多体现的计算机视觉应用程序必须快速对实时展开的用户定义事件做出反应。我们通过提出一项新的任务来解决此设置，以用于对查询事件开始（SDQES）的多模式视频理解流程检测。 SDQE的目的是确定自然语言查询所描述的复杂事件的开始，精度很高，延迟较低。我们介绍了一个基于EGO4D数据集的新基准，以及新的特定任务指标，以研究以Egintric视频设置的流式传输多模式检测。受到NLP中参数有效的微调方法的启发，我们提出了基于适配器的基线，该基线可以启用图像到视频传输学习，从而允许有效的在线视频建模。我们评估了三个视觉主机和三个适配器体系结构在短剪辑和未修饰的视频设置上。

### Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2412.03565)] [[cool](https://papers.cool/arxiv/2412.03565)] [[pdf](https://arxiv.org/pdf/2412.03565)]
> **Authors**: Wujian Peng,Lingchen Meng,Yitong Chen,Yiweng Xie,Yang Liu,Tao Gui,Hang Xu,Xipeng Qiu,Zuxuan Wu,Yu-Gang Jiang
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: Project page at https://inst-it.github.io
- **标题**: Inst-it：通过明确的视觉提示指令调整来增强多模式实例的理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）随着教学调整的发展取得了重大突破。但是，尽管现有模型可以在整体层面上理解图像和视频，但他们仍然在实例级别的理解中挣扎，这需要更加细微的理解和一致性。实例级别的理解是至关重要的，因为它专注于我们最感兴趣的特定元素。令人兴奋的是，现有作品发现，最新的LMMS在提供明确的视觉提示时表现出强大的实例理解能力。在此激励的情况下，我们引入了一条由GPT-4O协助的自动注释管道，以通过明确的视觉提示从图像和视频中提取实例级别的信息，例如指南。在这条管道的基础上，我们提出了Inst-IT，这是通过明确的视觉提示说明调整来理解LMM的解决方案。 Inst-IT由一个基准组成，用于诊断多模式实例级别的理解，大规模指令调节数据集以及连续的指导训练训练范式，以有效增强现有LMMS的时空实例能力。实验结果表明，随着Inst-IT的提升，我们的模型不仅在Inst-It基准席上取得了出色的性能，而且在各种通用图像和视频理解基准测试基准中都有显着改进。这强调了我们的数据集不仅可以提高实例级别的理解，而且增强了通用图像和视频理解的整体功能。

### FLAIR: VLM with Fine-grained Language-informed Image Representations 
[[arxiv](https://arxiv.org/abs/2412.03561)] [[cool](https://papers.cool/arxiv/2412.03561)] [[pdf](https://arxiv.org/pdf/2412.03561)]
> **Authors**: Rui Xiao,Sanghwan Kim,Mariana-Iuliana Georgescu,Zeynep Akata,Stephan Alaniz
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 才能：具有细粒度信息的图像表示的VLM
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 剪辑在大规模对齐图像和文本方面显示出令人印象深刻的结果。但是，它捕获详细的视觉特征的能力仍然有限，因为剪辑与全球级别的图像和文本匹配。为了解决这个问题，我们提出了风格，精细的语言信息图像表示，这种方法利用长而详细的图像描述来学习本地化的图像嵌入。通过对描述有关图像细节细节的细节的各种子饰品进行采样，我们训练视觉模型不仅会产生全局嵌入，还会产生特定于文本的图像表示。我们的模型在本地图像令牌之上引入了文本条件的注意集合，以产生精细的图像表示，这些图像表示在检索详细的图像内容方面表现出色。我们在现有的多模式检索基准和新引入的精细颗粒检索任务上都实现了最先进的性能，该任务评估了视觉语言模型的局部图像内容的能力。此外，我们的实验证明了在30m图像文本对捕获细粒的视觉信息方面受过训练的Flair的有效性，包括零弹性语义分割，优于在数十亿对上训练的模型。代码可在https://github.com/explainableml/flair上找到。

### Perception Tokens Enhance Visual Reasoning in Multimodal Language Models 
[[arxiv](https://arxiv.org/abs/2412.03548)] [[cool](https://papers.cool/arxiv/2412.03548)] [[pdf](https://arxiv.org/pdf/2412.03548)]
> **Authors**: Mahtab Bigverdi,Zelun Luo,Cheng-Yu Hsieh,Ethan Shen,Dongping Chen,Linda G. Shapiro,Ranjay Krishna
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 多模式模型（MLMS）仍然在基本的视觉感知任务中面临挑战，而专门模型出色。需要大约3D结构推理的任务受益于深度估计，而大约2D对象实例的推理则从对象检测中受益。但是，MLMS无法产生中间深度或盒子来进行推理。对相关数据的填充MLM并不能很好地概括，并且将计算外包到专门的视觉工具中过于算法和内存感知。为了解决这个问题，我们介绍了感知令牌，旨在协助语言不足的推理任务的固有图像表示形式。感知令牌充当辅助推理令牌，类似于语言模型中的经过思考的提示。例如，在与深度相关的任务中，具有感知令牌的扩大的MLM可以通过生成深度图作为令牌来理解，从而使其有效地解决问题。我们提出了Aurora，这是一种训练方法，该方法可以增强具有感知令牌的MLM，以改善视觉输入的推理。 Aurora利用VQVAE将中间图像表示，例如深度映射到令牌化的格式和边界框令牌，然后将其用于多任务训练框架。 Aurora在计数基准方面取得了显着的改进：眨眼的 +10.8％，CVBench的 +11.3％，种子板台上的 +8.3％，超过了跨数据集的概括性芬太尼方法。它还改善了相对深度：眨眼的超过6％。有了感知令牌，Aurora扩大了MLM的范围，而不是基于语言的推理，为更有效的视觉推理功能铺平了道路。

### Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2412.03467)] [[cool](https://papers.cool/arxiv/2412.03467)] [[pdf](https://arxiv.org/pdf/2412.03467)]
> **Authors**: Neale Ratzlaff,Man Luo,Xin Su,Vasudev Lal,Phillip Howard
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 多模式指令调整后，无培训语言推理降解的降解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式模型通常将强大的大语言模型（LLM）与视觉编码器相结合，然后通过指令调整对多模式数据进行培训。尽管此过程使LLM适应了多模式设置，但尚不清楚这种适应是否会损害其原始语言推理能力。在这项工作中，我们探讨了多模式指令调整对语言推理性能的影响。我们专注于Llava，这是一个领先的多模式框架，将Vicuna或Mistral等LLM与夹具构想组合在一起。我们将原始LLM的性能与八个语言推理任务中的多模式适应的对应物进行了比较。我们的实验产生了几个关键见解。首先，多模式学习的影响在Vicuna和Mistral之间有所不同：我们观察到在大多数任务中Vicuna的语言推理中的语言推理中的退化。其次，虽然多模式指令学习始终在数学推理任务（例如GSM8K）上持续降低性能，但它增强了常识性推理任务的性能（例如CommonSenseQa）。最后，我们证明了一种无训练的模型合并技术可以有效地减轻在多模式适应的Mistral中观察到的语言推理降解，甚至可以提高视觉任务的性能。

### PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation 
[[arxiv](https://arxiv.org/abs/2412.03409)] [[cool](https://papers.cool/arxiv/2412.03409)] [[pdf](https://arxiv.org/pdf/2412.03409)]
> **Authors**: Ao Wang,Hui Chen,Jianchao Tan,Kefeng Zhang,Xunliang Cai,Zijia Lin,Jungong Han,Guiguang Ding
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: 12 pages, 5 figures;
- **标题**: prefixKV：自适应前缀kV缓存是远景指导跟踪模型需要有效生成的模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，大型视觉模型（LVLMS）因其强大的产生和推理能力而迅速迅速获得了多种模态输入。但是，这些模型在推理过程中产生了大量的计算和内存开销，这极大地阻碍了实际情况下的有效部署。冗长的输入和输出序列需要的广泛键值（KV）缓存，特别是导致高推理成本。基于此，最近的工作调查了减少KV高速缓存尺寸以提高效率的方法。尽管有效，但它们通常忽略了跨层KV矢量的独特重要性分布，并在接下来的标记预测期间维持每层的缓存大小相同。这会导致某些层的重大上下文信息丢失，从而导致绩效下降。为了解决这个问题，我们提出prefixkv。它重构确定所有图层的KV高速缓存大小的挑战，以搜索最佳全局前缀配置的任务。通过基于二进制搜索的自适应层保留配方，可以在每一层中保存最大的上下文信息，从而促进生成。广泛的实验表明，与其他方法相比，我们的方法实现了最先进的性能。它表现出卓越的推理效率和发电质量的权衡，显示了实用应用的潜力。代码可在\ url {https://github.com/thu-mig/prefixkv}上找到。

### A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs 
[[arxiv](https://arxiv.org/abs/2412.03324)] [[cool](https://papers.cool/arxiv/2412.03324)] [[pdf](https://arxiv.org/pdf/2412.03324)]
> **Authors**: Wangbo Zhao,Yizeng Han,Jiasheng Tang,Zhikai Li,Yibing Song,Kai Wang,Zhangyang Wang,Yang You
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 及时的针迹节省了九个：小VLM是加速大VLM的精确指南
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLM）在各种多模式任务中都表现出了杰出的成功，但是由于处理众多的视觉令牌，大型VLMS遇到了巨大的效率挑战。加速大型VLM推理的一种有前途的方法是使用部分信息，例如来自特定层的注意图，以评估令牌重要性和修剪较少必不可少的令牌。但是，我们的研究揭示了三个关键见解：（i）部分注意信息不足以准确识别关键的视觉令牌，从而导致次优的性能，尤其是在低令牌保留率下； （ii）全球关注信息，例如在所有层中汇总的注意图，更有效地保留了必不可少的令牌，并在积极的修剪下保持可比的性能。但是，所有层的注意力图都需要完整的推理通过，这增加了计算负载，因此在现有方法中是不切实际的。 （iii）从一个小的VLM汇总的全局注意图与大型VLM相似，这表明是有效的替代方法。基于这些发现，我们介绍了\ textbf {traine-free}方法，\ usewissline {\ textbf {s}} mall vlm \ vlm \ lissionline {\ textbf {g}} uidance加速\ supplline \ useverline \ suesterline {\ textbf {\ textbf {l}} arge vlms {具体而言，我们采用了从小型VLM汇总的注意图来指导大型VLM中的视觉令牌修剪。此外，开发了一种早期退出机制，以充分使用小型VLM的预测，仅在必要时动态调用较大的VLM，从而在准确性和计算之间产生较高的权衡。对11个基准进行的广泛评估证明了SGL的有效性和概括性，可在保持竞争性能的同时，达到91 \％的修剪比率。

### DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2412.03255)] [[cool](https://papers.cool/arxiv/2412.03255)] [[pdf](https://arxiv.org/pdf/2412.03255)]
> **Authors**: Qingdong He,Jinlong Peng,Pengcheng Xu,Boyuan Jiang,Xiaobin Hu,Donghao Luo,Yong Liu,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: DynamicControl：改进文本对图像生成的自适应条件选择
- **领域**: 计算机视觉和模式识别
- **摘要**: 为了增强文本对图像扩散模型的可控性，当前的控制网状模型探索了各种控制信号以指示图像属性。但是，现有方法要么处理条件效率低下，要么使用固定数量的条件，这并不能完全解决多种条件及其潜在冲突的复杂性。这强调了对创新方法的需求，以有效地管理多种条件，以实现更可靠和详细的图像综合。为了解决这个问题，我们提出了一个新颖的框架DynamicControl，该框架支持各种控制信号的动态组合，从而可以自适应选择不同数量和类型的条件。我们的方法始于双周期控制器，该控制器通过利用预先训练的条件生成模型和判别模型来为所有输入条件生成初始实际得分排序。该控制器评估了提取条件和输入条件之间的相似性，以及与源图像的像素级相似性。然后，我们集成了多模式的大语言模型（MLLM），以构建有效的条件评估者。该评估者根据双周期控制器的分数排名优化条件的排序。我们的方法共同优化了MLLM和扩散模型，利用MLLM的推理功能来促进多条件文本到图像（T2I）任务。最终的排序条件被馈入并行多控制适配器，该适配器从动态视觉条件中学习特征地图并将其集成以调节ControlNet，从而增强对生成的图像的控制。通过定量和定性比较，DynamicControl在各种条件控制下的可控性，发电质量和合成性方面都证明了其优于现有方法的优势。

### AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning 
[[arxiv](https://arxiv.org/abs/2412.03248)] [[cool](https://papers.cool/arxiv/2412.03248)] [[pdf](https://arxiv.org/pdf/2412.03248)]
> **Authors**: Yiwu Zhong,Zhuoming Liu,Yin Li,Liwei Wang
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: 12 pages, 2 figures
- **标题**: 目的：通过令牌合并和修剪对多模式LLM的适应性推断
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 大型语言模型（LLMS）已使创建多模式LLM，这些LLM对图像和视频等视觉数据表现出强烈的理解。但是，这些模型通常依赖于视觉编码器的广泛视觉令牌，从而导致高计算需求，从而限制了其在资源约束环境中的适用性和长篇文章任务。在这项工作中，我们为多模式LLMS提出了一种无训练的自适应推理方法，该方法可以适应最低效率下降的广泛效率要求。我们的方法包括a）基于LLM前嵌入相似性的迭代令牌合并，b）基于多模式的重要性，LLM层中的渐进令牌修剪。使用简约的设计，我们的方法可以应用于视频和图像LLM。对各种视频和图像基准的广泛实验表明，我们的方法大大减少了计算负载（例如，$ \ textbf {7倍} $减少失败的同时保留视频和图像llms的性能。此外，在类似的计算成本下，我们的方法的表现优于长期视频理解中的最新方法（例如，MLVU上的$ \ textbf {+4.6} $）。此外，我们的深入分析提供了对令牌冗余和LLM层行为的见解，为未来设计有效的多模式LLM的研究提供了指导。我们的代码将在https://github.com/lavi-lab/aim上找到。

### Task-driven Image Fusion with Learnable Fusion Loss 
[[arxiv](https://arxiv.org/abs/2412.03240)] [[cool](https://papers.cool/arxiv/2412.03240)] [[pdf](https://arxiv.org/pdf/2412.03240)]
> **Authors**: Haowen Bai,Jiangshe Zhang,Zixiang Zhao,Yichen Wu,Lilun Deng,Yukun Cui,Tao Feng,Shuang Xu
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 任务驱动的图像融合与可学习的融合损失
- **领域**: 计算机视觉和模式识别
- **摘要**: 与任何单个来源相比，多模式图像融合汇总了来自多个传感器源的信息，从而实现了卓越的视觉质量和感知特征，通常会增强下游任务。但是，当前用于下游任务的融合方法仍然使用预定义的融合目标，这些目标可能会导致下游任务不匹配，从而限制了自适应指导并降低了模型的灵活性。为了解决这个问题，我们提出了任务驱动的图像融合（TDFusion），这是一个融合了由任务损失指导的可学习融合损失的融合框架。具体而言，我们的融合损失包括由称为“损失产生模块”的神经网络建模的可学习参数。该模块以元学习方式的下游任务的损失来监督。一旦融合模块通过融合损失进行了优化，学习目标是最大程度地减少融合图像的任务损失。融合模块和损耗模块之间的迭代更新可确保融合网络旨在最小化任务损失，从而指导融合过程实现任务目标。 TDFusion的培训仅依赖于下游任务的损失，使其适应任何特定任务。它可以应用于融合和任务网络的任何体系结构。实验证明了TDFusion在融合和任务相关的应用程序中的性能，包括四个公共融合数据集，语义细分和对象检测。代码将发布。

### MaterialPicker: Multi-Modal Material Generation with Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2412.03225)] [[cool](https://papers.cool/arxiv/2412.03225)] [[pdf](https://arxiv.org/pdf/2412.03225)]
> **Authors**: Xiaohe Ma,Valentin Deschaintre,Miloš Hašan,Fujun Luan,Kun Zhou,Hongzhi Wu,Yiwei Hu
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 物料贴：具有扩散变压器的多模式材料生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 高质量的材料生成是虚拟环境创作和逆渲染的关键。我们提出了一种多模式材料发电机，利用扩散变压器（DIT）体系结构，改进和简化了从文本提示和/或照片中创建高质量材料的情况。我们的方法可以基于材料样品的图像作物生成材料，即使捕获的表面被扭曲，以角度或部分遮住的角度观察，就像在自然场景的照片中一样。我们进一步允许用户指定文本提示，以为生成提供其他指导。我们将基于预先训练的DIT的视频生成器列入材料生成器，每个材料图都被视为视频序列中的框架。我们在定量和定性上评估我们的方法，并表明它可以比以前的工作更多样化的材料产生和更好的失真校正。

### TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation 
[[arxiv](https://arxiv.org/abs/2412.03069)] [[cool](https://papers.cool/arxiv/2412.03069)] [[pdf](https://arxiv.org/pdf/2412.03069)]
> **Authors**: Liao Qu,Huichao Zhang,Yiheng Liu,Xu Wang,Yi Jiang,Yiming Gao,Hu Ye,Daniel K. Du,Zehuan Yuan,Xinglong Wu
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: https://byteflow-ai.github.io/TokenFlow/
- **标题**: TokenFlow：用于多模式理解和生成的统一图像令牌
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们提出了TokenFlow，这是一种新型的统一图像令牌，它弥合了多模式理解与产生之间的长期差距。先前的研究尝试采用单个重建靶向矢量量化（VQ）编码器来统一这两个任务。我们观察到，理解和产生需要从根本上不同的视觉信息粒度。这导致了重要的权衡，尤其是在多模式理解任务中损害绩效。 TokenFlow通过创新的双编码书体系结构来应对这一挑战，该架构将语义和像素级的功能学习分解，同时通过共享的映射机制保持其对齐方式。该设计使得直接访问两个高级语义表示，对于理解任务和通过共享索引生成必不可少的细粒度视觉特征至关重要。我们的广泛实验表明，令牌在多个维度上的优势。利用TokenFlow，我们首次证明离散的视觉输入可以超过LLAVA-1.5 13B在理解性能方面，实现7.2 \％的平均改善。对于图像重建，我们在384*384分辨率下获得了0.63的强大得分。此外，TokenFlow在自回归图像生成中建立了最先进的性能，其元化得分为0.55，在256*256分辨率下为0.55，可实现与SDXL的可比结果。

### CLAP: Unsupervised 3D Representation Learning for Fusion 3D Perception via Curvature Sampling and Prototype Learning 
[[arxiv](https://arxiv.org/abs/2412.03059)] [[cool](https://papers.cool/arxiv/2412.03059)] [[pdf](https://arxiv.org/pdf/2412.03059)]
> **Authors**: Runjian Chen,Hang Zhang,Avinash Ravichandran,Hyoungseob Park,Wenqi Shao,Alex Wong,Ping Luo
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 拍手：通过曲率采样和原型学习，融合3D感知的无监督3D表示学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 无监督的3D表示学习减轻了为融合感知任务标记多模式3D数据的负担。在不同的预训练范式中，基于可区分呈现的方法表现出最大的希望。但是，由于使用图像处理大点云的计算成本，现有作品分别对每种方式进行了预训练。因此，尚未利用高级语义（来自图像）和3D结构（来自点云）的相互益处。为了解决这一差距，我们提出了一种用于图像和点云的无监督的基于可区分呈现的预训练方法，称为拍手，曲率采样缩写和可学习的原型。具体而言，我们的方法通过曲率采样克服了计算障碍，以选择用于预训练的更有信息的点/像素。为了揭示其互补性带来的绩效好处，我们建议使用可学习的原型在共同特征空间中代表3D场景的一部分，并期望最大化训练方案将每种模态的嵌入与原型相关联。我们进一步提出了交换预测损失，该损失通过原型探索其相互作用以及克矩阵正则化项，以维持训练稳定性。与先前的SOTA预训练方法相比，对Nuscenes和Waymo数据集的实验表明，拍手的性能增长高达100％。代码和模型将发布。

### Cross-Self KV Cache Pruning for Efficient Vision-Language Inference 
[[arxiv](https://arxiv.org/abs/2412.04652)] [[cool](https://papers.cool/arxiv/2412.04652)] [[pdf](https://arxiv.org/pdf/2412.04652)]
> **Authors**: Xiaohuan Pei,Tao Huang,Chang Xu
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 跨自我kV缓存修剪，以进行有效的视觉推断
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: KV缓存修剪已成为一种有希望的技术，用于降低长期文化自动回归生成中的记忆和计算成本。视觉模型（VLM）的现有方法通常依赖于大语言模型（LLM）的自我注意力分数来识别和修剪无关的令牌。但是，这些方法忽略了模态之间固有的分布差异，通常会导致令牌重要性估计和关键视觉令牌的过度变化。为了解决这个问题，我们提出将注意力分数分解为模式内的注意力（在同一模态内）和模式间注意（跨模态），从而通过独立地管理这些独特的注意力类型来实现更精确的KV缓存修剪。此外，我们引入了N-Softmax功能，以抵消修剪引起的分配变化，保留注意力评分的原始平滑度并确保稳定的性能。我们的最终无训练方法，\ textbf {c} ross- \ textbf {s} elf \ textbf {p} runing（csp），与具有完整的KV缓存模型相比，具有竞争性能，同时表现出了明显优于先前的修剪方法。对涵盖29个多模式数据集的基准Milebench进行了广泛的评估，证明了CSP的有效性，在诸如对话性体现的对话（例如KV Cache预算降低13.6 \％）的同时，在诸如对话性体现的对话（例如对话性体现的对话）上实现了41 \％的绩效提高。该代码可在https://github.com/terrypei/csp上获得

### Using Diffusion Priors for Video Amodal Segmentation 
[[arxiv](https://arxiv.org/abs/2412.04623)] [[cool](https://papers.cool/arxiv/2412.04623)] [[pdf](https://arxiv.org/pdf/2412.04623)]
> **Authors**: Kaihua Chen,Deva Ramanan,Tarasha Khurana
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: project page: https://diffusion-vas.github.io
- **标题**: 使用扩散先验进行视频分段
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类中的对象永久性是一种基本提示，即使在现场被完全遮住了物体，也有助于理解对象的持久性。当今的对象细分中的方法不能说明世界的这种阿米达人本质，仅用于分割可见或模态对象。几乎没有阿莫达方法。单图像分割方法无法处理更好地推断出使用时间信息来推断的高级遮挡，而多帧方法仅集中于分割刚性对象。为此，我们建议通过将其制定为有条件的生成任务来解决视频分割，并利用视频生成模型中的基础知识。我们的方法很简单；我们将这些模型重新利用以根据对象的模态面膜框架以及上下文伪深度地图的条件来了解哪个对象边界可能被遮挡，因此扩展以使对象的完整范围幻觉。接下来是内容完成阶段，能够为对象的遮挡区域进行涂漆。我们将方法与四个数据集上的各种最先进的方法进行了基准测试，并在对象的遮挡区域中显示出高达13％的Amodal分割的急剧提高。

### Assessing and Learning Alignment of Unimodal Vision and Language Models 
[[arxiv](https://arxiv.org/abs/2412.04616)] [[cool](https://papers.cool/arxiv/2412.04616)] [[pdf](https://arxiv.org/pdf/2412.04616)]
> **Authors**: Le Zhang,Qian Yang,Aishwarya Agrawal
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 评估和学习单形愿景和语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 单峰视觉和语言模型的对齐程度如何？尽管先前的工作已经接近回答这个问题，但他们的评估方法并未直接转化为在实践视觉语言任务中如何使用这些模型。在本文中，我们提出了一种受线性探测启发的直接评估方法，以评估视力对准。我们确定SSL视觉模型的一致性程度取决于其SSL训练目标，并且我们发现SSL表示的聚类质量对对齐性能的影响比其线性可分离性更强。接下来，我们介绍了图像和语言（帆）的快速对齐，这是一个有效的转移学习框架，该框架将预贴上的单峰视觉和语言模型与下游视觉语言任务保持一致。由于帆利用了经过审计的单形模型的优势，因此与经过从头开始训练的剪辑（如剪辑）相比，多模式对齐的配对图形数据要少得多（6％）配对的图像文本数据。帆训练只需要一个A100 GPU，5个小时的培训，并且可以容纳32,768的批量尺寸。 SAIL在ImageNet上达到73.4％的零射精精度（相对于夹子的72.7％），并以零拍的检索，复杂的推理和语义分割效果出色。此外，SAIL改善了视觉编码器的语言兼容性，进而提高了多模式大语模型的性能。整个代码库和模型权重是开源：https：//lezhang7.github.io/sail.github.io/

### MageBench: Bridging Large Multimodal Models to Agents 
[[arxiv](https://arxiv.org/abs/2412.04531)] [[cool](https://papers.cool/arxiv/2412.04531)] [[pdf](https://arxiv.org/pdf/2412.04531)]
> **Authors**: Miaosen Zhang,Qi Dai,Yifan Yang,Jianmin Bao,Dongdong Chen,Kai Qiu,Chong Luo,Xin Geng,Baining Guo
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: 37 pages, 32 figures, github link: https://github.com/microsoft/MageBench
- **标题**: Magebench：将大型多模型桥接到代理
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: LMM显示出令人印象深刻的视觉理解能力，并有可能应用于代理商，这需要强大的推理和计划能力。然而，现有的基准主要评估其在语言部分中的推理能力，在该语言部分中完全由文本组成。我们考虑在决策过程中不断更新视觉信号并要求视觉信号的情况。这种链中的链接推理范式更与多模式代理的需求更加一致，同时很少评估。在本文中，我们介绍了Magebench，这是一种以推理能力为导向的多模式基准标准，尽管具有轻重量的环境，但仍带来了重大的推理挑战并具有实质性的实用价值。该基准目前包括三种类型的环境：WebUI，Sokoban和Football，包括483种不同的情况。它彻底验证了代理商的知识和工程能力，视觉智能和互动技巧。结果表明，只有少数产品级模型比随机表演更好，并且它们都远远不如人类水平。更具体地说，我们发现当前的模型严重缺乏根据视觉反馈以及视觉想象，交织的图像文本长上下文处理和其他能力来修改计划的能力。我们希望我们的工作将从成为代理商的角度为LMM提供优化方向。我们在https://github.com/microsoft/magebench上发布代码和数据。

### p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay 
[[arxiv](https://arxiv.org/abs/2412.04449)] [[cool](https://papers.cool/arxiv/2412.04449)] [[pdf](https://arxiv.org/pdf/2412.04449)]
> **Authors**: Jun Zhang,Desen Meng,Ji Qi,Zhenpeng Huang,Tao Wu,Limin Wang
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: Technical Report; Code released at https://github.com/MCG-NJU/p-MoD
- **标题**: p-mod：通过渐进式衰减建立深度MLLM的混合物
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 尽管多模式大语模型（MLLM）在各种任务中的表现出色，但大量的培训和推理成本仍阻碍了他们的进步。大多数计算源于变压器解码器处理的大量视觉令牌。在本文中，我们建议通过利用深度（MOD）机制的混合物来构建有效的MLLM，其中每个变压器解码器层都选择基本视觉令牌来处理冗余的基本视觉令牌。但是，将mod整合到MLLM中是不平凡的。为了应对训练和推理稳定性的挑战以及有限的训练数据，我们使用两个新颖的设计适应了MOD模块：Tanh门控重量归一化（Tanhnorm）（Tanhorm）和对称令牌重新重量重量（String）。此外，我们观察到，视觉令牌在更深层的层中表现出更高的冗余，因此设计了渐进式衰减（PRD）策略，该策略逐渐逐步逐步降低令牌的保留比，并采用移动的余弦时间表。这种至关重要的设计充分释放了MOD的潜力，从而显着提高了我们的模型的效率和性能。为了验证我们的方法的有效性，我们在14个基准中使用两个基线模型进行了广泛的实验。我们的模型P-Mod匹配甚至超过了基线模型的性能，在推理期间只有55.6％的TFLOPS和53.8％的KV高速缓存存储，而训练期间只有77.7％的GPU小时。

### MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation 
[[arxiv](https://arxiv.org/abs/2412.04448)] [[cool](https://papers.cool/arxiv/2412.04448)] [[pdf](https://arxiv.org/pdf/2412.04448)]
> **Authors**: Longtao Zheng,Yifan Zhang,Hanzhong Guo,Jiachun Pan,Zhenxiong Tan,Jiahao Lu,Chuanxin Tang,Bo An,Shuicheng Yan
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: Project Page: https://memoavatar.github.io
- **标题**: 备忘录：记忆引导的表达性说话视频生成的扩散
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频扩散模型的最新进展已解锁了现实的音频驱动视频发电的新潜力。但是，在生成的会话视频中实现无缝的音频同步，保持长期身份一致性并在产生的自然音频一致性表达式中产生自然的音频表达式。为了应对这些挑战，我们提出了记忆引导的情感感知传播（备忘录），这是一种端到端音频驱动的肖像画动画方法，以生成身份符合身份和表现力的说话视频。我们的方法围绕两个关键模块构建：（1）记忆引导的时间模块，该模块通过开发记忆状态来存储从过去的上下文中存储信息，从而增强长期身份一致性和运动平滑度，以通过线性注意来指导时间建模； （2）情绪感知的音频模块，该模块以多模式的注意取代了传统的交叉注意，以增强音频视频的互动，同时通过情感自适应层标准来检测到音频到完善面部表情的情绪。广泛的定量和定性结果表明，备忘录在各种图像和音频类型上产生更现实的会话视频，在整体质量，音频唇同步，身份一致性和表达情绪一致性方面的表现优于最先进的方法。

### Grounding Descriptions in Images informs Zero-Shot Visual Recognition 
[[arxiv](https://arxiv.org/abs/2412.04429)] [[cool](https://papers.cool/arxiv/2412.04429)] [[pdf](https://arxiv.org/pdf/2412.04429)]
> **Authors**: Shaunak Halbe,Junjiao Tian,K J Joseph,James Seale Smith,Katherine Stevo,Vineeth N Balasubramanian,Zsolt Kira
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 图像中的接地描述会导致零拍的视觉识别
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 视觉语言模型（VLMS）之类的剪辑模型因其在开放式摄影概念上执行零拍视觉识别的能力而受到珍惜。这是通过选择其文本表示与查询图像具有最高相似性的对象类别来实现的。尽管在某些领域取得了成功，但这种方法在识别精细粒度实体以及推广到未被培训分布所捕获的概念方面而努力。最近的工作试图通过在测试时间集成类别描述来缓解这些挑战，尽管有适度的改进。我们将这些有限的收益归因于图像和描述表示之间的根本未对准，这根源于剪辑的训练结构。在本文中，我们提出了谷物，这是一种旨在同时在精细水平和粗糙水平上对齐表示的新预处理策略。我们的方法学会了在图像区域中共同基础文本描述，以及将总体字幕与全局图像表示一致。为了推动这种预训练，我们利用冷冻的多模式大语言模型（MLLM）来得出大规模合成注释。与11种不同图像分类数据集的当前最新方法相比，我们证明了模型的增强零击性能。此外，我们介绍了Products-2023，这是一种新的，具有新概念的新策划，手动标记的数据集，并展示了我们模型通过对其进行基准测试来识别这些概念的能力。我们的模型对其他下游任务（例如检索）的重大改进进一步突出了我们方法学到的卓越表现质量。可在https://github.com/shaunak27/grain-clip上获得代码。

### Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion 
[[arxiv](https://arxiv.org/abs/2412.04424)] [[cool](https://papers.cool/arxiv/2412.04424)] [[pdf](https://arxiv.org/pdf/2412.04424)]
> **Authors**: Jiuhai Chen,Jianwei Yang,Haiping Wu,Dianqi Li,Jianfeng Gao,Tianyi Zhou,Bin Xiao
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 佛罗伦萨-VL：具有生成视觉编码器和深度融合的增强视觉语言模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 我们提出了佛罗伦萨-VL，这是一个新的多式模式大型语言模型（MLLM）的新家族，其具有丰富的视觉表示由Florence-2（生成视觉基础模型）产生。与经过对比学习训练的广泛使用的夹具式视觉变压器不同，佛罗伦萨-2可以捕获视觉特征的不同级别和方面，这些级别和方面更具用途，可以适应多种下游任务。我们提出了一种新颖的功能融合结构和一种创新的训练配方，该配方有效地将佛罗伦萨-2的视觉特征集成到预识别的LLMS中，例如Phi 3.5和Llama 3。尤其是，我们提出了“深度呼吸融合（DbFusion）”，以融合从不同深度和多个提示下提取的视觉特征。我们的模型培训由整个模型的端到端预审计，然后是对投影层和LLM的填充，在精心设计的多种开源数据集的配方上，其中包括高质量的图像字幕和指令对。我们对佛罗伦萨VL视觉特征的定量分析和可视化表明，它比流行视觉编码器在视觉对齐中具有优势，在这种视觉对齐中，丰富的深度和呼吸在其中扮演着重要的角色。佛罗伦萨-VL对涵盖一般VQA，感知，幻觉，OCR，图表，知识密集型理解等的各种多模式和以视觉为中心的基准的现有最先进的MLLM实现了重大改进，以促进未来的研究，我们的模型和完整的培训配方。 https://github.com/jiuhaichen/florence-vl

### Reflective Teacher: Semi-Supervised Multimodal 3D Object Detection in Bird's-Eye-View via Uncertainty Measure 
[[arxiv](https://arxiv.org/abs/2412.04337)] [[cool](https://papers.cool/arxiv/2412.04337)] [[pdf](https://arxiv.org/pdf/2412.04337)]
> **Authors**: Saheli Hazra,Sudip Das,Rohit Choudhary,Arindam Das,Ganesh Sistu,Ciaran Eising,Ujjwal Bhattacharya
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 反思教师：通过不确定性度量的半监督多模式3D对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 已经发现，在Bird's-eye-View（BEV）中，应用伪标记技术在半监督的3D对象检测（SSOD）中是有利的，以用于自主驾驶，尤其是在标记数据受限的情况下。在文献中，学生网络已使用指数移动平均值（EMA）来调整教师网络的权重。但是，同样的引起教师网络中的灾难性遗忘。在这项工作中，我们通过介绍一个新颖的反思教师概念来解决这个问题，在该概念中，学生经过标记和伪标记的数据培训，而其知识则通过正规机逐步传递给教师，以确保保留以前的知识。此外，我们提出了几何学意识到的BEV融合（GA-BEVFUSION），以有效地对齐多模式BEV特征，从而降低了模态 - 相机和激光镜头之间的差异。这有助于绘制在LIDAR点之间与空间先验可靠地嵌入的精确几何信息，以从相机图像中提取语义信息。我们在Nuscenes和Waymo数据集上进行的实验证明了：1）在完全监督和半监督的设置中，对最先进方法的性能提高了； 2）反思性教师与使用完整标记的数据集的其他完全监督的方法相比，分别仅25％和22％的Nuscenes和Waymo数据集标记数据的数据。

### Liquid: Language Models are Scalable and Unified Multi-modal Generators 
[[arxiv](https://arxiv.org/abs/2412.04332)] [[cool](https://papers.cool/arxiv/2412.04332)] [[pdf](https://arxiv.org/pdf/2412.04332)]
> **Authors**: Junfeng Wu,Yi Jiang,Chuofan Ma,Yuliang Liu,Hengshuang Zhao,Zehuan Yuan,Song Bai,Xiang Bai
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: Technical report. Project page: https://foundationvision.github.io/Liquid/
- **标题**: 液体：语言模型是可扩展的和统一的多模式发电机
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一种自动回归生成范式的Liquid，它通过将图像将图像纳入离散代码并学习这些代码嵌入与文本令牌以及视觉和语言共享特征空间中的文本令牌一起学习，从而无缝整合视觉理解和生成。与以前的多模式大型语言模型（MLLM）不同，Liquid使用单个大语言模型（LLM）实现了这种集成，从而消除了对外部预审预周审经嵌入（例如剪辑）的需求。 Liquid首次发现了缩放定律，即由于模型大小的增加而统一的视觉和语言任务训练会减少统一的训练。此外，统一的令牌空间可以使视觉生成和理解任务相互增强，从而有效地消除了早期模型中看到的典型干扰。我们表明，现有的LLM可以作为液体的强大基础，节省100倍的培训成本，同时超过多模式能力的变色龙，并保持与Llama 2（如Llama2）相当的语言表现。液体还优于SD v2.1和SD-XL（MJHQ-30K上的5.47的FID）等液体模型，在视觉语言和仅文本任务方面都表现出色。这项工作表明，QWEN2.5和GEMMA2等LLM是强大的多模式发电机，提供了可扩展的解决方案，可增强视觉理解和产生。代码和模型将在https://github.com/foundationvision/liquid上发布。

### FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression 
[[arxiv](https://arxiv.org/abs/2412.04317)] [[cool](https://papers.cool/arxiv/2412.04317)] [[pdf](https://arxiv.org/pdf/2412.04317)]
> **Authors**: Bo Tong,Bokai Lai,Yiyi Zhou,Gen Luo,Yunhang Shen,Ke Li,Xiaoshuai Sun,Rongrong Ji
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: Flashsloth：通过嵌入式视觉压缩的闪电多模式大型语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 尽管能力方面有很大的飞跃，但多模式的大语言模型（MLLM）倾向于在实际使用方面像懒惰一样行事，即缓慢的响应和较大的潜伏期。最近的努力致力于构建微小的MLLM，以提高效率，但是大量的视觉令牌仍然使用了它们的实际加速。在本文中，我们提出了一个强大而快速的MLLM，称为Flashsloth。与以前的努力不同，Flashsloth专注于在压缩其冗余语义的过程中提高视觉令牌的描述能力。尤其是，Flashsloth引入了嵌入式视觉压缩设计，以捕获视觉上显着和指导相关的图像信息，从而以较少的视觉令牌来实现出色的多模式性能。进行了广泛的实验以验证所提出的闪光洞，并且还对一堆很小但强的MLLM进行了全面比较，例如InternVL2，minicpm-v2和qwen2-vl。实验结果表明，与这些高级小型MLLM相比，我们的Flashsloth可以大大减少视觉令牌，训练记忆和计算复杂性的数量，同时在各种VL任务上保持高性能。

### SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model 
[[arxiv](https://arxiv.org/abs/2412.04292)] [[cool](https://papers.cool/arxiv/2412.04292)] [[pdf](https://arxiv.org/pdf/2412.04292)]
> **Authors**: Zhenglin Huang,Jinwei Hu,Xiangtai Li,Yiwei He,Xingyu Zhao,Bei Peng,Baoyuan Wu,Xiaowei Huang,Guangliang Cheng
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: CVPR-2025
- **标题**: SIDA：社交媒体图像深层检测，具有大型多模型的本地化和解释
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 生成模型在创建高度逼真的图像方面的快速发展为错误信息传播带来了重大风险。例如，当在社交媒体上共享的综合图像会误导广泛的观众并侵蚀数字内容的信任，从而导致严重的影响。尽管有一些进展，但学术界尚未为社交媒体创建一个大型且多样化的DeepFake检测数据集，也没有为解决此问题的有效解决方案设计。 In this paper, we introduce the Social media Image Detection dataSet (SID-Set), which offers three key advantages: (1) extensive volume, featuring 300K AI-generated/tampered and authentic images with comprehensive annotations, (2) broad diversity, encompassing fully synthetic and tampered images across various classes, and (3) elevated realism, with images that are predominantly indistinguishable from genuine ones through仅视觉检查。此外，利用大型多模型的特殊功能，我们提出了一个新的图像深击检测，本地化和解释框架，名为SIDA（社交媒体图像检测，本地化和解释助手）。 SIDA不仅可以辨别图像的真实性，而且还通过掩盖预测来描述被篡改的区域，并提供了对模型判断标准的文本解释。与SID-SET和其他基准测试的最先进的DeepFake检测模型相比，广泛的实验表明，SIDA在多元化的环境中实现了卓越的性能。代码，模型和数据集将发布。

### Customize Segment Anything Model for Multi-Modal Semantic Segmentation with Mixture of LoRA Experts 
[[arxiv](https://arxiv.org/abs/2412.04220)] [[cool](https://papers.cool/arxiv/2412.04220)] [[pdf](https://arxiv.org/pdf/2412.04220)]
> **Authors**: Chenyang Zhu,Bin Xiao,Lin Shi,Shoukun Xu,Xu Zheng
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 自定义分段的任何模型，用于多模式语义分割与洛拉专家的混合物
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近的细分市场模型（SAM）代表了缩放分割模型的重大突破，在RGB模式下的各种下游应用程序中提供了强劲的性能。但是，将SAM直接应用于新兴的视觉方式，例如深度和事件数据会导致多模式分割任务中的次优性能。在本文中，我们首次尝试通过提出针对不同输入视觉方式的低级适应专家（MOE-LORA）的混合物来调整SAM的多模式语义分割。通过仅训练Moe-Lora层，同时将SAM的重量冻结在一起，Sam的强大概括和分割功能可以保留用于下游任务。具体来说，为了解决跨模式的不一致，我们提出了一种新型的MOE路由策略，该策略可适应跨模态生成加权特征，从而增强了多模式特征集成。此外，我们通过调整SAM的分割头并引入辅助分割头来结合多尺度的分割头，以结合多尺度特征，以有效地改善分割性能，从而结合了多尺度的特征提取和融合。在三个多模式基准上进行了广泛的实验：交付，缪斯和MCUBES。结果一致地表明，所提出的方法在各种情况下都显着优于最先进的方法。值得注意的是，在缺失模式的特别具有挑战性的条件下，我们的方法表现出可观的性能增长，与现有方法相比，提高了32.15％。

### BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2412.04086)] [[cool](https://papers.cool/arxiv/2412.04086)] [[pdf](https://arxiv.org/pdf/2412.04086)]
> **Authors**: Nefeli Andreou,Varsha Vivek,Ying Wang,Alex Vorobiov,Tiffany Deng,Raja Bala,Larry Davis,Betty Mohler Tesch
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 体型：评估文本到图像生成中人体的现实主义
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 准确地从文本中生成人体的图像仍然是最先进的文本对图像模型的挑战性问题。通常观察到的与身体相关的伪影包括额外或缺失的肢体，不现实的姿势，模糊的身体部位等。当前，对此类文物的评估在很大程度上依赖于耗时的人类判断，从而限制了规模上基准模型的能力。我们通过提出Bodymetric来解决这一问题，Bodymetric是一种可学习的指标，可以预测图像中的身体现实主义。 Bodymetric在现实主义标签和多模式信号上进行了训练，包括从输入图像推断出的3D身体表示以及文本描述。为了促进这种方法，我们设计了一条注释管道，以收集有关人体现实主义的专家评级，从而为这项任务提供了新的数据集，即身体现实主义。消融研究支持我们的体系结构选择，并在捕获2D图像中捕获身体相关的伪影方面利用3D人体的重要性。与评估图像中总体用户偏好的并发指标相比，身体指标特异性反映了与身体相关的伪影。我们通过以前不可大的规模不可行的应用来证明身体的实用性。特别是，我们使用Bodymetric来基准文本对图像模型产生逼真的人体的产生能力。我们还根据预测的现实主义评分来证明体形测量在对产生的图像中的有效性。

### A Framework For Image Synthesis Using Supervised Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2412.03957)] [[cool](https://papers.cool/arxiv/2412.03957)] [[pdf](https://arxiv.org/pdf/2412.03957)]
> **Authors**: Yibin Liu,Jianyu Zhang,Li Zhang,Shijian Li,Gang Pan
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 使用监督对比学习的图像合成框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 文本对图像（T2I）的生成旨在产生与文本描述相对应的逼真的图像。事实证明，生成对抗网络（GAN）在这项任务中取得了成功。典型的T2i gan是两种方法，该方法首先从对齐的图像文本对中预处理一个模式间表示，然后在此基础上使用GAN来训练图像发生器。但是，这种表示忽略了内模式的语义对应关系，例如具有相同标签的图像。小修道院中的语义标签描述了具有潜在的跨图像关系的固有分布模式，这是对文本描述的补充，以理解图像的完整特征。在本文中，我们提出了一个框架，通过标签指导的监督对比度学习来利用间和内模式的对应关系。我们将T2i gan扩展到训练阶段和生成阶段的两个参数共享对比分支。该集成有效地将语义上相似的图像文本对表示形式簇，从而促进了高质量图像的产生。我们通过单对象数据集CUB和多对象数据集可可证明了我们在四个新型T2I GAN上的框架，从而实现了Image Generation评估的INPECTION评分（IS）和Frechet Inception距离（FID）指标的显着改善。值得注意的是，在更复杂的多对象可可中，我们的框架分别提高了30.1％，27.3％，16.2％和17.1％，分别为attngan，dm-gan，ssa-gan和galip。我们还通过与其他标签引导的T2i gan进行比较来验证我们的优势。结果肯定了我们方法在推进T2i一代最先进的GAN方面的有效性和竞争力

### AIpparel: A Large Multimodal Generative Model for Digital Garments 
[[arxiv](https://arxiv.org/abs/2412.03937)] [[cool](https://papers.cool/arxiv/2412.03937)] [[pdf](https://arxiv.org/pdf/2412.03937)]
> **Authors**: Kiyohiro Nakayama,Jan Ackermann,Timur Levent Kesdogan,Yang Zheng,Maria Korosteleva,Olga Sorkine-Hornung,Leonidas J. Guibas,Guandao Yang,Gordon Wetzstein
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: The project website is at georgenakayama.github.io/AIpparel/
- **标题**: Aipparel：数字服装的大型多模式生成模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 服装对人类的生活至关重要，提供保护，反映文化身份和展示个人风格。然而，服装的创作仍然是一个耗时的过程，这在很大程度上是由于设计它们的手动工作。为了简化此过程，我们引入了Aipparel，这是一种大型的多模式，用于生成和编辑缝纫模式。我们的模型微型款式在定制的大型大规模数据集中，其中包括120,000多种独特的服装，每套都有多模式注释，包括文本，图像和缝纫图案，上面有多种模型（LMM）。此外，我们提出了一种新颖的代币化方案，该方案简洁地编码了这些复杂的缝纫模式，以便LLM可以学会有效地预测它们。 Aipparel在单模式任务中实现了最先进的性能，包括文本对手和图像到工程预测，并启用了新颖的多模式生成应用，例如互动服装编辑。项目网站位于georgenakayama.github.io/aipparel/。

### MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2412.03927)] [[cool](https://papers.cool/arxiv/2412.03927)] [[pdf](https://arxiv.org/pdf/2412.03927)]
> **Authors**: Ming-Chang Chiu,Shicheng Wen,Pin-Yu Chen,Xuezhe Ma
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: 8 pages, 13 tables, 2 figures
- **标题**: Megacoin：增强视觉模型的中粒色感知
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在视觉模型（VLM）中，感知和解释颜色和物理环境的能力对于实现上下文准确的理解和相互作用至关重要。然而，尽管多模型建模的进步，但仍然存在严格评估模型可识别微妙色彩变化和空间环境的能力的专业数据集，这是情境理解和可靠的现实应用程序部署的关键要素。为了实现这一目标，我们策划了Megacoin，这是一种基于\ emph {reamph {real}图像具有各种上下文属性的高质量，标记的数据集。 Megacoin由两个部分组成：Megacoin-Instruct，它是VLMS的监督微调（SFT）数据集；以及Megacoin-Bench，这是一个带注释的测试集，可以用作独立的QA数据集。 Megacoin〜为220,000张真实图像提供了三个带注释的功能：前景颜色，背景颜色和对象的物理环境的描述，构成了660k人类注释。另外，巨型菌可以应用于基准域概括（DG）算法。我们在VLM的线性探测设置中探索基准DG方法，并显示一些新的见解。最后但并非最不重要的一点是，我们表明，包括GPT-4O在内的VLM具有低标准的颜色识别能力，并且对Megacoin进行微调可以改善视觉评估任务的性能。在某些情况下，Megacoin微调的小规模开放量模型（例如Llava和Bunny）的表现可以胜过封闭源GPT-4O。我们希望Megacoin的公用事业可以在Directions上阐明VLM可以改善并为域泛化算法提供更复杂的平台。

### DiffSign: AI-Assisted Generation of Customizable Sign Language Videos With Enhanced Realism 
[[arxiv](https://arxiv.org/abs/2412.03878)] [[cool](https://papers.cool/arxiv/2412.03878)] [[pdf](https://arxiv.org/pdf/2412.03878)]
> **Authors**: Sudha Krishnamurthy,Vimal Bhat,Abhinav Jain
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: Published in Proceedings of ECCV, Workshop on Assistive Computer Vision and Robotics, 2024
- **标题**: DIFFSIGN：具有增强现实主义的AI辅助生成可自定义的手语视频
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，几种流媒体服务的扩散使世界各地的各种受众都可以观看相同的媒体内容，例如电影或电视节目。虽然正在添加翻译和配音服务以使当地受众访问内容，但支持具有不同能力的人（例如聋哑人和听力障碍（DHH）社区）的内容的支持仍在落后。我们的目标是通过使用现实和表现力的合成签名者生成手语视频，使DHH社区更容易访问媒体内容。在全球范围内使用相同的签名者，可能会有有限的吸引力。因此，我们的方法结合了参数建模和生成建模，以生成逼真的合成签名，并根据用户偏好自定义外观。我们首先通过优化参数模型来重新定位人类手语构成3D手语的化身。然后，使用渲染的化身的高保真姿势来调节使用基于扩散的生成模型生成的合成签名者的姿势。合成签名者的外观由通过视觉适配器提供的图像提示控制。我们的结果表明，使用我们的方法生成的手语视频比仅在文本提示下的扩散模型生成的视频具有更好的时间一致性和现实性。我们还支持多模式提示，以允许用户进一步自定义签名者的外观以适应多样性（例如肤色，性别）。我们的方法对于签名匿名也很有用。

### Category-Adaptive Cross-Modal Semantic Refinement and Transfer for Open-Vocabulary Multi-Label Recognition 
[[arxiv](https://arxiv.org/abs/2412.06190)] [[cool](https://papers.cool/arxiv/2412.06190)] [[pdf](https://arxiv.org/pdf/2412.06190)]
> **Authors**: Haijing Liu,Tao Pu,Hefeng Wu,Keze Wang,Liang Lin
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: 15 pages
- **标题**: 开放式多标签识别的类别自适应跨模式语义精致和转移
- **领域**: 计算机视觉和模式识别
- **摘要**: 从剪辑的概括能力中受益，最近的视觉语言预训练（VLP）模型表现出令人印象深刻的能力，可以在日常图像中捕获几乎任何视觉概念。然而，由于在开放式摄影设置中存在看不见的类别，现有的算法难以有效地捕获类别之间的强烈语义相关性，从而在开放式多摄氏度多标签识别（OV-MLR）上产生了次优的性能。此外，各种对象类别的判别区域数量的实质变化与当前方法中使用的固定数贴片匹配未对准，从而引入了嘈杂的视觉提示，从而阻碍了目标语义的准确捕获。为了应对这些挑战，我们提出了一种新型的自适应跨模式语义精致和转移（c $^2 $ srt）框架，以探索每个类别和不同类别的语义相关性，以类别自适应的方式探索。提出的框架由两个互补模块，即类别内语义改进（ISR）模块和类别间的语义传输（IST）模块组成。具体而言，ISR模块利用VLP模型的跨模式知识自适应地找到一组局部歧视区域，这些区域最能代表目标类别的语义。 IST模块通过利用LLMS的常识能力来构建一个类别自适应的相关图，并将语义知识从相关的可见类别转移到看不见的类别。 OV-MLR基准测试的广泛实验清楚地表明，所提出的C $^2 $ SRT框架的表现优于当前最新算法。

### One-shot Human Motion Transfer via Occlusion-Robust Flow Prediction and Neural Texturing 
[[arxiv](https://arxiv.org/abs/2412.06174)] [[cool](https://papers.cool/arxiv/2412.06174)] [[pdf](https://arxiv.org/pdf/2412.06174)]
> **Authors**: Yuzhu Ji,Chuanxia Zheng,Tat-Jen Cham
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: This article has been accepted for publication in IEEE Transactions on Multimedia
- **标题**: 通过遮挡刺激流动预测和神经纹理的一击人体运动转移
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类运动转移旨在通过驾驶视频来为静态源图像进行动画动画。尽管人类运动转移的最新进展导致了结果的显着改善，但对于具有2D身体地标，骨架和语义面膜的方法，由于运动和驱动姿势之间的相对差异很大，运动和驱动姿势之间的对应关系很大，但由于运动和发音复杂性的差异很大。另外，密度的准确性和精度降低了基于神经渲染方法的图像质量。为了解决外观和几何形状对于运动传递的重要性，我们提出了一个统一的框架，将多尺度的特征翘曲和神经纹理映射结合在一起，以恢复更好的2D外观和2.5D几何形状，部分是通过从密度上利用信息，但可以从密度上利用其固有的有限精度。我们的模型通过共同训练并融合它们来利用多种方式，从而使其能够强大的神经纹理特征，以应对几何误差以及多尺度密集的运动流，从而更好地保留外观。完整和半视频主体视频数据集的实验结果表明，我们的模型可以很好地概括并获得竞争成果，并且在处理诸如具有实质性自我估计的案例之类的挑战性案例中特别有效。

### Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity 
[[arxiv](https://arxiv.org/abs/2412.06171)] [[cool](https://papers.cool/arxiv/2412.06171)] [[pdf](https://arxiv.org/pdf/2412.06171)]
> **Authors**: Huaxin Zhang,Xiaohao Xu,Xiang Wang,Jialong Zuo,Xiaonan Huang,Changxin Gao,Shanjun Zhang,Li Yu,Nong Sang
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: 21 pages
- **标题**: Holmes-Vau：在任何颗粒状上迈向长期视频异常理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们如何使模型能够理解在不同的时间尺度和上下文上发生的视频异常？传统的视频异常理解（VAU）方法集中在框架级异常预测上，常常缺少复杂和多样化的现实世界异常的解释性。最近的多模式方法利用视觉和文本数据，但缺乏捕获短期和长期异常的层次结构注释。为了应对这一挑战，我们介绍了Hivau-70k，这是一个大规模的基准，用于跨越任何粒度的层次视频异常理解。我们开发了一个半自动化的注释引擎，该引擎通过使用大语言模型（LLMS）将手动视频分割与递归自由文本注释相结合，从而有效地扩展了高质量的注释。这导致在剪辑级，事件级别和视频级段组织中组织了超过70,000个多粒子注释。为了在长视频中有效检测有效的异常检测，我们提出了以异常为重点的时间抽样器（ATS）。 ATS与具有密度感知的采样器的异常得分器集成了基于异常得分的自适应选择帧，从而确保多模式LLM集中在富含异常的区域上，从而显着提高了效率和准确性。广泛的实验表明，我们的分层指导数据显着提高了异常理解。集成的ATS和视觉模型在处理长视频中的表现优于传统方法。我们的基准和模型可在https://github.com/pipixin321/holmesvau上公开获得。

### AgentAlign: Misalignment-Adapted Multi-Agent Perception for Resilient Inter-Agent Sensor Correlations 
[[arxiv](https://arxiv.org/abs/2412.06142)] [[cool](https://papers.cool/arxiv/2412.06142)] [[pdf](https://arxiv.org/pdf/2412.06142)]
> **Authors**: Zonglin Meng,Yun Zhang,Zhaoliang Zheng,Zhihao Zhao,Jiaqi Ma
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: AnatentalIgn：弹性间传感器相关性的未对准的多代理感知
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 鉴于其能力在连接的自动化车辆（CAV）和智能基础设施中利用共享信息来解决感应遮挡和范围限制问题，因此合作感引起了广泛的关注。但是，现有的研究忽略了多代理设置中脆弱的多传感器相关性，因为异质的代理传感器测量非常容易受到环境因素的影响，从而导致弱化的传感器间相互作用。不同的操作条件和其他实际因素不可避免地会引入多因素噪声，并导致多传感器的错位，从而使多代理多模式感知的部署在现实世界中尤其具有挑战性。在本文中，我们提出了Agentalign，这是一种现实世界中的异质跨模式特征对准框架，以有效解决这些多模式错位问题。我们的方法引入了跨模式特征对准空间（CFA）和异质剂特征比对（HAFA）机制，以动态统一各种代理的多模式特征。此外，我们提出了一种新颖的V2xset-noise数据集，该数据集在不同的环境条件下模拟了逼真的传感器缺陷，从而促进了对我们方法鲁棒性的系统评估。对V2X-Real和V2XSet-Noise基准测试的广泛实验表明，我们的框架实现了最先进的性能，强调了其在合作自动驾驶中实现现实世界应用的潜力。可控的V2XSET噪声数据集和发电管道将来将在将来发布。

### MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization 
[[arxiv](https://arxiv.org/abs/2412.06141)] [[cool](https://papers.cool/arxiv/2412.06141)] [[pdf](https://arxiv.org/pdf/2412.06141)]
> **Authors**: Kangyu Zhu,Peng Xia,Yun Li,Hongtu Zhu,Sheng Wang,Huaxiu Yao
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: MMEDPO：将医学视觉语言模型与临床感知的多模式优化优化
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 大型视觉模型（LVLM）的进步推动了其在医疗领域的应用。但是，医疗LVLM（MED-LVLMS）遇到了由于模式错位而遇到的事实挑战，在这种情况下，模型优先考虑文本知识而不是视觉输入，从而导致幻觉与医学图像中的信息相矛盾。以前通过偏好优化增强MED-LVLM中的模态对准的尝试不足以减轻偏好数据中的临床相关性，从而使这些样品易于区分并降低对齐的效率。为了应对这一挑战，我们提出了MMEDPO，这是一种新型的多模式医学偏好优化方法，该方法考虑了优先样品的临床相关性，以增强MED-LVLM比对。 MMEDPO通过引入两种类型的分配来策划多模式偏好数据：（1）通过目标MED-LVLMS或GPT-4O注入的合理幻觉以产生医学上不准确的反应，（2）通过局部病变的忽视病变区域，破坏了对关键领域的视觉理解。然后，我们根据来自多个Med-llms和Visual工具的得分计算每个样本的临床相关性，并将这些分数整合到优先优化过程中，作为权重，从而有效比对。我们的实验表明，MMEDPO显着提高了MED-LVLM的事实准确性，通过在MED-VQA和报告生成任务中平均14.2％和51.7％，通过平均14.2％和51.7％来实现对现有偏好优化方法的实质性改进。我们的代码可在https://github.com/aiming-lab/mmedpo中找到。

### GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis 
[[arxiv](https://arxiv.org/abs/2412.06089)] [[cool](https://papers.cool/arxiv/2412.06089)] [[pdf](https://arxiv.org/pdf/2412.06089)]
> **Authors**: Ashish Goswami,Satyam Kumar Modi,Santhosh Rishi Deshineni,Harman Singh,Prathosh A. P,Parag Singla
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 葡萄：组成T2i合成的生成平面编辑框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 文本对图像（T2i）的生成通过扩散模型取得了重大进展，从而从文本提示中产生了照片真实的图像。尽管取得了这种进步，但现有方法仍在遵循复杂的文本提示时面临挑战，尤其是那些需要组成和多步推理的方法。鉴于如此复杂的说明，SOTA模型通常会在忠实地建模对象属性及其之间的关系中犯错误。在这项工作中，我们提出了T2i合成的替代范式，将复杂多步生成的任务分解为三个步骤，（a）生成：我们首先使用现有扩散模型（b）计划生成图像：我们使用多模式LLMS（MLLM）（MLLMS）来确定一定的属性，并在其属性中确定一定的属性，并在单个物体中确定错误，并构成单个对象的纠正，并构成一个构成的属性，并识别出构成的属性，并识别出构成的属性，并识别出构成的属性，并识别出构成的属性，并识别出一种误差。编辑计划。 （c）编辑：我们利用现有的文本指导的图像编辑模型来顺序执行我们的编辑计划，以获取所需的图像，该图像忠于原始说明。我们的方法从本质上是模块化的，无训练的事实，可以在图像生成和编辑模型的任何组合中应用。作为另外的贡献，我们还开发了一个能够组成编辑的模型，这进一步有助于提高我们提出的方法的整体准确性。我们的方法可以灵活地交易推理时间，并在构图文本提示上进行性能。我们对3个基准和10个T2I模型进行了广泛的实验评估，包括Dalle-3和最新的-SD-3.5LAGE。我们的方法不仅提高了SOTA模型的性能，最高3分，还减少了弱模型和更强模型之间的性能差距。 $ \ href {https://dair-iitd.github.io/grape/} {https://dair-iitd.github.io/grape/} $

### Chimera: Improving Generalist Model with Domain-Specific Experts 
[[arxiv](https://arxiv.org/abs/2412.05983)] [[cool](https://papers.cool/arxiv/2412.05983)] [[pdf](https://arxiv.org/pdf/2412.05983)]
> **Authors**: Tianshuo Peng,Mingsheng Li,Hongbin Zhou,Renqiu Xia,Renrui Zhang,Lei Bai,Song Mao,Bin Wang,Conghui He,Aojun Zhou,Botian Shi,Tao Chen,Bo Zhang,Xiangyu Yue
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: Chimera Homepage: https://alpha-innovator.github.io/chimera_page
- **标题**: 嵌合体：通过特定领域的专家改善通才模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）的最新进展强调了通过增加图像文本配对数据来扩展缩放的重要性，从而在一般任务上实现了令人印象深刻的性能。尽管在广泛的应用中有效性，但通才模型主要在以自然图像为主的网络规模数据集中进行培训，从而牺牲了针对特定领域的特定任务的专业功能，这些任务需要广泛的域名先验知识。此外，由于通才模型与专家之间的代表性差距和不平衡的优化，直接整合针对特定领域的专家模型，这是具有挑战性的。为了应对这些挑战，我们引入了Chimera，这是一种可扩展且低成本的多模式管道，旨在提高现有LMM与特定领域的专家的能力。具体而言，我们设计了一种渐进培训策略，将专家模型的功能集成到通才LMM的输入中。为了解决由良好的一般视觉编码器引起的不平衡优化，我们介绍了一种新型的通才特殊协作掩盖（GSCM）机制。这导致了一个多功能模型，该模型在图表，表格，数学和文档域中都表现出色，从而在多模式推理和视觉内容提取任务上实现了最先进的性能，这两者都是评估现有LMM的具有挑战性的任务。

### Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.05939)] [[cool](https://papers.cool/arxiv/2412.05939)] [[pdf](https://arxiv.org/pdf/2412.05939)]
> **Authors**: Xiao Xu,Tianhao Niu,Yuxi Xie,Libo Qin,Wanxiang Che,Min-Yen Kan
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: A manuscript that should have been Arxived in May :)
- **标题**: 探索多模式大语言模型的多元概念注释
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 多模式大语模型（MLLM）在视觉任务中出色 - 通过仅预先培训粗粒概念注释（例如，图像标题）。我们假设整合细粒概念注释（例如对象标签和对象区域）将进一步提高性能，因为两个数据粒度都以概念表示的广度和深度相互补充。我们介绍了一个新的数据集，其中包含用于MLLM的多模式多模式概念注释（MMGIC）。在构建MMGIC时，我们探讨了不同数据配方对多模式理解和生成的影响。我们的分析表明，在我们的结构化模板和一个一般的MLLM框架下，多元融合的概念注释相互整合和补充。我们清楚地探索并展示了MMGIC的潜力，以帮助MLLM更好地定位和学习概念，使视觉和语言保持多种粒度。我们通过研究MMGIC和图像和图像之间的公平比较和有效的合作，进一步验证了我们的假设，即关于12个多模式理解和生成基准测试的合作数据，例如，它们的适当组合实现了3.95％和2.34％的绝对改进，而不是单独的图像 - 攻击数据，而在教皇和种子板上仅实现了单独的图像 - 接收数据。代码，数据和模型将在https://github.com/looperxx/mmgic上找到。

### GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting and Meshing 
[[arxiv](https://arxiv.org/abs/2412.05908)] [[cool](https://papers.cool/arxiv/2412.05908)] [[pdf](https://arxiv.org/pdf/2412.05908)]
> **Authors**: Jianing Zhang,Yuchao Zheng,Ziwei Li,Qionghai Dai,Xiaoyun Yuan
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: GBR：高保真高斯分裂和网格的生成捆绑包的精致
- **领域**: 计算机视觉和模式识别
- **摘要**: 高斯碎片的有效表示和使用连续的高斯原语对3D场景的渲染引起了人们的关注。但是，由于几何信息和光度法信息有限，它在稀疏视图输入中挣扎，导致深度，形状和纹理的歧义。我们提出了GBR：生成束细化，这是一种仅使用4-6个输入视图的高保真高斯分裂和网络的方法。 GBR集成了神经束调节模块，以增强几何精度和生成深度细化模块，以提高几何形状忠诚度。更具体地说，神经束调整模块集成了基础网络，以产生未经未受的图像的初始3D点图和点匹配，然后进行捆绑调整优化，以提高多视图一致性和点云精度。生成深度改进模块采用基于扩散的策略来增强几何细节和保真度，同时保留规模。最后，对于高斯脱落优化，我们提出了一个多模式损耗函数，其中包含深度和正常一致性，几何正则化和伪视图监督，从而在稀疏视图条件下提供了强大的指导。广泛使用数据集的实验表明，GBR在稀疏视图输入下的现有方法显着优于现有方法。此外，GBR还展示了重建和渲染大型现实场景的能力，例如Teng王子和大城市的馆，仅使用6个观看次数提供了出色的细节。

### doScenes: An Autonomous Driving Dataset with Natural Language Instruction for Human Interaction and Vision-Language Navigation 
[[arxiv](https://arxiv.org/abs/2412.05893)] [[cool](https://papers.cool/arxiv/2412.05893)] [[pdf](https://arxiv.org/pdf/2412.05893)]
> **Authors**: Parthib Roy,Srinivasa Perisetla,Shashank Shriram,Harsha Krishnaswamy,Aryan Keskar,Ross Greer
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: DOSCENES：具有自然语言指令的自主驾驶数据集用于人类互动和视觉导航
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 人际关系机器人系统，尤其是自动驾驶汽车（AV），必须有效地将人类指示纳入其运动计划中。本文介绍了Doscenes，这是一种新型数据集，旨在促进人体车辆教学相互作用的研究，重点关注直接影响车辆运动的短期指令。通过使用自然语言指令和参考性标签注释多模式传感器数据，Doscenes弥合了指令和驾驶响应之间的差距，从而实现了上下文感知和适应性计划。与专注于排名或场景级别推理的现有数据集不同，Doscenes强调了与静态和动态场景对象相关的可行指令。该框架解决了先前研究中的局限性，例如依赖模拟数据或预定义的操作集，通过支持现实世界中的细微差别和灵活的响应。这项工作为制定学习策略奠定了基础，该策略将人类的指示无缝整合到自主系统中，从而推进了安全有效的人车协作，以进行视觉导航。我们在https://www.github.com/rossgreer/doscenes上公开提供数据

### MG-3D: Multi-Grained Knowledge-Enhanced 3D Medical Vision-Language Pre-training 
[[arxiv](https://arxiv.org/abs/2412.05876)] [[cool](https://papers.cool/arxiv/2412.05876)] [[pdf](https://arxiv.org/pdf/2412.05876)]
> **Authors**: Xuefeng Ni,Linshan Wu,Jiaxin Zhuang,Qiong Wang,Mingxiang Wu,Varut Vardhanabhuti,Lihai Zhang,Hanyu Gao,Hao Chen
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: 10 Pages
- **标题**: MG-3D：多元素知识增强的3D医学视觉语言预训练
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在众多临床应用中，3D医学图像分析至关重要。但是，标记数据和有限的概括能力的稀缺性阻碍了AI授权模型的发展。放射学报告很容易访问，并且可以用作弱监督的信号。然而，在3D医学图像分析中，大规模视力语言预训练（VLP）仍未得到充实。具体而言，对多元格式放射学语义及其在患者之间的相关性的研究不足导致大规模卷报告数据的利用不足。考虑在大规模数据（47.1k）上预先培训的多任务VLP方法（47.1K），考虑到多任务VLP方法（47.1K），通过以下两个方面：1：1）确定每个挑战，确定每个挑战的挑战：1重建，确保不同模态的患者内特征在凝聚力上代表相同的语义含量； 2）基于患者的细粒度报告相关性，将患者间的视觉语义相关联，并通过对比度学习对全球个体差异保持敏感性，从而增强了歧视性特征表示。此外，我们深入研究了扩展法，以探索潜在的绩效改善。进行了九个单项和跨模式临床任务的全面评估以评估模型功效。对内部和外部数据集进行的广泛实验证明了MG-3D的卓越可传递性，可伸缩性和概括，展示了其在3D医学图像分析中推进功能表示的潜力。代码将提供：https：//github.com/xuefeng-ni/mg-3d。

### [CLS] Token Tells Everything Needed for Training-free Efficient MLLMs 
[[arxiv](https://arxiv.org/abs/2412.05819)] [[cool](https://papers.cool/arxiv/2412.05819)] [[pdf](https://arxiv.org/pdf/2412.05819)]
> **Authors**: Ao Wang,Fengyuan Sun,Hui Chen,Zijia Lin,Jungong Han,Guiguang Ding
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: 12 pages,4 figures
- **标题**: [CLS]令牌告诉无训练的有效MLLM所需的一切
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大型语言模型（MLLM）最近在各种视力语言任务中表现出了很强的表现，在计算机视觉中引起了极大的关注。但是，由于高计算成本和内存要求，它们的有效部署仍然是一个重大挑战。认识到视觉方式中信息的冗余性，最近的研究探索了压缩MLLM中视觉令牌以以无训练方式提高效率的方法。尽管它们有效，但现有的方法诸如快速的方法依靠视觉令牌和及时文本令牌之间的注意力作为重要性指标，从而忽略了与响应文本的相关性，从而引入了感知偏见。在本文中，我们证明在MLLMS中，视觉编码器中的[Cls]令牌本质地知道哪些视觉令牌对MLLM很重要。在此之前，我们引入了一种简单而有效的方法，用于无火车的视觉令牌压缩，称为VTC-CLS。首先，它利用[CLS]令牌在视觉令牌上的注意力评分是修剪视觉令牌的重要指标。此外，我们还探索了[CLS]令牌从不同层得出的重要性分数，以更全面地捕获关键的视觉信息。广泛的实验表明，与基线方法相比，我们的VTC-CLS在各种任务上实现了最新的性能。它还以无训练的方式带来的计算成本大大降低，突出了其有效性和优势。代码和模型可在\ url {https://github.com/thu-mig/vtc-cls}上获得。

### SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2412.05818)] [[cool](https://papers.cool/arxiv/2412.05818)] [[pdf](https://arxiv.org/pdf/2412.05818)]
> **Authors**: Leigang Qu,Haochuan Li,Wenjie Wang,Xiang Liu,Juncheng Li,Liqiang Nie,Tat-Seng Chua
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: project page: https://silmm.github.io/
- **标题**: SILMM：自我提出的大型多模型，用于构图文本对图像生成
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,多媒体
- **摘要**: 大型多模型模型（LMM）在多模式理解和产生中表现出了令人印象深刻的能力，从而推动了文本到图像生成的进步。但是，实现LMM的准确文本图像对齐，尤其是在组成方案中，仍然具有挑战性。现有的方法，例如用于多步生成的布局计划以及从人类反馈或AI反馈中学习，在很大程度上取决于迅速的工程，昂贵的人类注释以及持续升级，限制灵活性和可扩展性。在这项工作中，我们引入了模型不可复的迭代自我改进框架（SILMM），该框架可以使LMMS能够通过直接偏好优化（DPO）提供有用的自我反馈，并优化文本图像对齐。 DPO可以很容易地应用于使用离散视觉令牌作为中间图像表示的LMM；虽然它不太适合具有连续视觉特征的LMM，但由于获得发电概率是具有挑战性的。为了使SILMM具有连续特征的LMM，我们提出了一种多样性机制，以获得不同的表示和基于内核的连续DPO进行对齐。对三个组成的文本到图像生成基准的广泛实验验证了SILMM的有效性和优势，显示了T2i-Compbench ++的改善超过30％，在DPG基础上的改善约为20％。

### Compositional Image Retrieval via Instruction-Aware Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2412.05756)] [[cool](https://papers.cool/arxiv/2412.05756)] [[pdf](https://arxiv.org/pdf/2412.05756)]
> **Authors**: Wenliang Zhong,Weizhi An,Feng Jiang,Hehuan Ma,Yuzhi Guo,Junzhou Huang
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-09
> **comment**: 9 pages, 8 figures
- **标题**: 构图图像通过说明 - 感知对比度学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 组成的图像检索（CIR）涉及基于与文本配对的图像的构成查询检索目标图像，该查询指定了对视觉参考的修改或更改。 CIR本质上是一项指令跟随任务，因为该模型需要对图像进行解释和应用修改。实际上，由于下游任务中注释数据的稀缺性，因此需要零射击CIR（ZS-CIR）。尽管基于剪辑的现有ZS-CIR模型显示出令人鼓舞的结果，但它们在解释和遵循修改指令方面的能力仍然有限。一些研究试图通过合并大型语言模型（LLM）来解决这个问题。但是，这些方法仍然面临有效整合多模式信息和指导理解的挑战。为了应对上述挑战，我们提出了一种新颖的嵌入方法，该方法利用指令调整的多模式LLM（MLLM）生成组成的表示，从而显着增强了在图像和说明之间进行全面整合的能力后的说明。但是，直接应用MLLM引入了一个新的挑战，因为MLLM主要是为文本生成而设计的，而不是按照CIR中的要求嵌入提取。为了解决这个问题，我们引入了两阶段的培训策略，以有效地学习一个联合多模式嵌入空间，并通过在类似于CIR格式的三重率数据集中调整模型来进一步完善遵循修改指令的能力。在四个公共数据集上进行了广泛的实验：FashionIQ，CIRR，Genecis和Circo展示了我们的模型的出色性能，超过了最先进的基线的差距。代码可在GitHub存储库中找到。

### RSUniVLM: A Unified Vision Language Model for Remote Sensing via Granularity-oriented Mixture of Experts 
[[arxiv](https://arxiv.org/abs/2412.05679)] [[cool](https://papers.cool/arxiv/2412.05679)] [[pdf](https://arxiv.org/pdf/2412.05679)]
> **Authors**: Xu Liu,Zhouhui Lian
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: rsunivlm：一种统一的视觉语言模型，用于通过专家的面向粒度的混合物进行遥感
- **领域**: 计算机视觉和模式识别
- **摘要**: 遥感视觉语言模型（RS VLM）在遥感（RS）图像理解的任务中取得了长足的进步。在多模式推理和多转向对话中表现良好，现有模型缺乏像素级的理解和与多图像输入的斗争。在这项工作中，我们提出了Rsunivlm，这是一种统一的端到端RS VLM，旨在跨多个粒度，包括图像级，区域级别和像素级任务，旨在全面的视觉理解。 RSUNIVLM在多图像分析中也有效地表现，并具有变化检测和更改字幕的实例。为了增强模型在不增加模型大小的情况下捕获不同级别的视觉信息的能力，我们设计了一种名为“面向粒度的专家”的新型体系结构，以将模型限制为约10亿参数。我们还基于RS和通用域中的各种现有数据集构建一个大规模的RS指令遵循数据集，其中包括各种任务，例如对象本地化，视觉问题答案和语义细分。已经进行了大量实验，以验证在各种RS任务中验证提出的RSUNIVLM至最新的优势。代码和模型将在\ href {https://github.com/xuliu-cyber/rsunivlm} {there}中获得。

### Multimodal Biometric Authentication Using Camera-Based PPG and Fingerprint Fusion 
[[arxiv](https://arxiv.org/abs/2412.05660)] [[cool](https://papers.cool/arxiv/2412.05660)] [[pdf](https://arxiv.org/pdf/2412.05660)]
> **Authors**: Xue Xian Zheng,M. M. Ur Rahma,Bilal Taha,Mudassir Masood,Dimitrios Hatzinakos,Tareq Al-Naffouri
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 使用基于相机的PPG和指纹融合的多模式生物识别验证
- **领域**: 计算机视觉和模式识别
- **摘要**: 从智能手机获得的基于摄像头的光绘画学（PPG）对个性化医疗保健和安全身份验证表现出了巨大的希望。本文提出了一个多模式生物识别系统，该系统集成了从视频中提取的PPG信号和指纹数据，以提高用户验证的准确性。该系统要求用户将指尖放在相机镜头上几秒钟，从而捕获和处理独特的生物特征特征。我们的方法采用一个神经网络，具有两个结构化状态空间模型（SSM）编码器来管理不同的模式。指纹图像被转换为​​像素序列，并与分段的PPG波形一起输入编码器。然后，一种跨模式的注意机制提取精致的特征表示，而面向分布的对比损失函数将这些特征在统一的潜在空间内对齐。实验结果表明，在单课和双课程身份验证方案中，系统在各种评估指标中的卓越性能。

### Biological Brain Age Estimation using Sex-Aware Adversarial Variational Autoencoder with Multimodal Neuroimages 
[[arxiv](https://arxiv.org/abs/2412.05632)] [[cool](https://papers.cool/arxiv/2412.05632)] [[pdf](https://arxiv.org/pdf/2412.05632)]
> **Authors**: Abd Ur Rehman,Azka Rehman,Muhammad Usman,Abdullah Shahid,Sung-Min Gho,Aleum Lee,Tariq M. Khan,Imran Razzak
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 使用性感知的对抗性变异自动编码器与多模式的生物学脑年龄估计
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大脑老化涉及结构和功能变化，因此是脑健康的关键生物标志物。结合结构磁共振成像（SMRI）和功能磁共振成像（fMRI）具有通过利用互补数据来改善大脑年龄估计的潜力。但是，fMRI数据比SMRI嘈杂，使多模式融合复杂化。传统的融合方法通常会引入更多的噪音，而不是有用的信息，与单独使用SMRI相比，这可以降低准确性。在本文中，我们提出了一个新型的多模式框架，用于利用性感知的对抗性变异自动编码器（SA-AVAE），以用于生物脑年龄估计。我们的框架集成了对抗性和变分学习，以有效地将潜在特征与两种模式相关。具体而言，我们将潜在空间分解为特定于模态的代码和共享代码，以分别表示跨模态的互补信息和共同信息。为了增强解开，我们将跨重建和共享距离比率损失作为正规化项引入。重要的是，我们将性信息纳入学到的潜在代码中，使该模型能够通过集成的回归器模块捕获大脑年龄估计的性别衰老模式。我们使用公开可用的OpenBHB数据集评估了我们的模型，这是用于脑年龄估计的全面多站点数据集。消融研究和与最先进方法的比较的结果表明，我们的框架表现优于现有方法，并且在各个年龄段之间表现出明显的鲁棒性，强调了其在早期发现神经退行性疾病时实时临床应用的潜力。

### Dif4FF: Leveraging Multimodal Diffusion Models and Graph Neural Networks for Accurate New Fashion Product Performance Forecasting 
[[arxiv](https://arxiv.org/abs/2412.05566)] [[cool](https://papers.cool/arxiv/2412.05566)] [[pdf](https://arxiv.org/pdf/2412.05566)]
> **Authors**: Andrea Avogaro,Luigi Capogrosso,Franco Fummi,Marco Cristani
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-09
> **comment**: Accepted at the 27th International Conference on Pattern Recognition (ICPR 2024)
- **标题**: DIF4FF：利用多模式扩散模型和图形神经网络以进行准确的新时尚产品性能预测
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在快时的行业中，过量生产和未售出的库存会带来重大的环境问题。对未发布的项目的精确销售预测可以大大提高行业的效率和利润。但是，由于没有过去的数据和不断变化的趋势，很难预测全新风格的成功。具体而言，当前使用的确定性模型在遇到培训数据之外的项目时与域转移斗争。最近提出的扩散模型使用连续的时间扩散过程解决了这个问题。具体而言，这些模型使我们能够预测新项目的销售，从而减轻确定性模型遇到的域转移挑战。结果，本文提出了DIF4FF，这是一种新型时尚产品性能预测（NFPPF）的新型两阶段管道，该管道利用了基于与特定衣服相关的多模式数据的扩散模型的功能。 DIF4FF首先利用基于多模式得分的扩散模型来预测随着时间的推移多种服装的多个销售轨迹。预测是使用强大的图形卷积网络（GCN）体系结构来完善的。通过利用GCN在时间和空间数据中捕获长期依赖性并在这两个维度之间寻求最佳解决方案的能力，DIF4FF提供了文献中最准确，最有效的预测系统，可预测新项目的销售。我们测试了NFPPF的事实上标准Visuelle的DIF4FF，从而实现了新的最新结果。

### CLIP-TNseg: A Multi-Modal Hybrid Framework for Thyroid Nodule Segmentation in Ultrasound Images 
[[arxiv](https://arxiv.org/abs/2412.05530)] [[cool](https://papers.cool/arxiv/2412.05530)] [[pdf](https://arxiv.org/pdf/2412.05530)]
> **Authors**: Xinjie Sun,Boxiong Wei,Yalong Jiang,Liquan Mao,Qi Zhao
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: 4 pages, 2 figures, submitted to IEEE Signal Processing Letters
- **标题**: 夹式 -  tnseg：超声图像中甲状腺结节分割的多模式混合框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 超声图像中的甲状腺结节分割对于准确的诊断和治疗计划至关重要。但是，现有方法在细分准确性，可解释性和概括方面面临挑战，这阻碍了其性能。这封信提出了一个新颖的框架，即剪辑-TNSEG，通过将多模式的大型模型与神经网络体系结构整合在一起来解决这些问题。剪辑 -  tnseg由两个主要分支组成：粗粒分支，该分支从冷冻夹模型中提取高级语义特征和细颗粒分支，该分支使用U-NET样式残留块捕获细颗粒特征。这些功能由预测头融合并处理，以生成精确的分割图。剪贴TNSEG利用粗粒细分的分支来通过文本和高级视觉特征来增强语义理解，而细粒的分支则完善了空间细节，从而实现了精确和稳健的分割。对公众和我们新收集的数据集进行了广泛的实验证明了其竞争性能。我们的代码和原始数据集可在https://github.com/jayxjsun/clip-tnseg上找到。

### TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action 
[[arxiv](https://arxiv.org/abs/2412.05479)] [[cool](https://papers.cool/arxiv/2412.05479)] [[pdf](https://arxiv.org/pdf/2412.05479)]
> **Authors**: Zixian Ma,Jianguo Zhang,Zhiwei Liu,Jieyu Zhang,Juntao Tan,Manli Shu,Juan Carlos Niebles,Shelby Heinecke,Huan Wang,Caiming Xiong,Ranjay Krishna,Silvio Savarese
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 炸玉米饼：学习具有合成链的多模式动作模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 虽然开源多模式模型在简单的问题回答任务上表现良好，但它们通常会失败，这些问题需要多个功能，例如细粒度识别，视觉接地和推理，并且需要多步解决方案。我们提出了Taco，这是一个多模式大型动作模型家族，旨在提高这种复杂，多模式和多模式任务的性能。在推断期间，炸玉米饼产生了思想和动作链（COTA），通过调用外部工具（例如OCR，深度估计和计算器）来执行中间步骤，然后集成了思想和动作输出以产生相干响应。为了培训炸玉米饼，我们创建了一个由GPT-4O和Python程序生成的1M超过1M合成COTA痕迹的大数据集。然后，我们尝试各种数据过滤和混合技术，并获得293K高质量COTA示例的最终子集。该数据集使炸玉米饼能够学习复杂的推理和动作路径，超越了在只有直接答案的教学调整数据训练的现有模型。我们的模型炸玉米饼的表现优于8个基准的指令调整的基线，平均提高了3.6％，在涉及OCR，数学推理和空间推理的MMVET任务中，增长了15％。对高质量COTA痕迹的培训为复杂的多模式推理设定了新的标准，突出了对促进开源Mutli-Modal-Modal模型的功能的结构化多步指令调整的需求。

### Text to Blind Motion 
[[arxiv](https://arxiv.org/abs/2412.05277)] [[cool](https://papers.cool/arxiv/2412.05277)] [[pdf](https://arxiv.org/pdf/2412.05277)]
> **Authors**: Hee Jae Kim,Kathakoli Sengupta,Masaki Kuribayashi,Hernisa Kacorri,Eshed Ohn-Bar
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: Accepted at NeurIPS 2024
- **标题**: 文字盲目运动
- **领域**: 计算机视觉和模式识别
- **摘要**: 盲目的人对世界的看法与被发现的人不同，这可能会导致独特的运动特征。例如，在交叉路口越过时，盲人可能会有不同的运动模式，例如从直路上转向或使用基于触摸的探索在路缘和障碍物周围。这些行为似乎对嵌入在自动驾驶汽车等技术中的运动模型似乎不太可预测。然而，以前尚未研究3D运动模型捕获这种行为的能力，因为现有的3D人类运动的数据集目前缺乏多样性，并且对被发现的人有偏见。在这项工作中，我们介绍了盲道，这是第一个多式模式运动基准，适用于盲人的行人。我们使用可穿戴传感器收集3D运动数据，其中11名盲人参与者在现实世界中的八个不同路线中导航。此外，我们提供丰富的文本描述，以捕获盲人行人的独特运动特征及其与导航辅助（例如，白色的甘蔗或导犬）和环境的相互作用。我们基于最新的3D人类预测模型，通过现成的和基于培训的方法来解决我们的新任务。为了促进更安全，更可靠的系统，可以无缝地推理其环境中各种各样的人类运动，我们的文本和动感基准可在https://blindways.github.io上获得。

### SimC3D: A Simple Contrastive 3D Pretraining Framework Using RGB Images 
[[arxiv](https://arxiv.org/abs/2412.05274)] [[cool](https://papers.cool/arxiv/2412.05274)] [[pdf](https://arxiv.org/pdf/2412.05274)]
> **Authors**: Jiahua Dong,Tong Wu,Rui Qian,Jiaqi Wang
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: SIMC3D：使用RGB图像的简单对比度3D预处理框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D对比度学习范式通过对点云数据进行预处理表明了下游任务的表现出色。最近的进步涉及与3D点云相关的其他2D图像先验，以进一步改进。但是，这些现有框架受到可用点云数据集限制范围的限制，这主要是由于获得点云数据的高成本。为此，我们首次提出了SIMC3D，这是一个简单但有效的3D对比学习框架，是从纯RGB图像数据中预处理3D骨架。 SIMC3D具有三个吸引人的特性进行对比的3D预处理。 （1）纯图像数据：SIMC3D简化了仅使用RBG图像的昂贵3D点云和3D骨架的依赖性。通过采用深度估计和合适的数据处理，单眼合成点云显示出3D预训练的巨大潜力。 （2）简单的框架：传统的多模式框架通过使用额外的2D主链通过2D先验的3D进行了预处理，从而增加了计算费用。在本文中，我们从经验上证明了2D模式的主要优势源于局部信息的纳入。受到这一有见地的观察的启发，SIMC3D直接采用2D位置嵌入作为更强的对比目标，从而消除了2D骨架的必要性，并带来了可观的性能改进。 （3）强大的性能：SIMC3D优于以前的方法，这些方法利用了地面真实点云数据来预处理各种下游任务。此外，可以通过组合多个图像数据集来进一步增强SIMC3D的性能，从而展示其可扩展性的重要潜力。该代码将在https://github.com/dongjiahua/simc3d上找到。

### Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling 
[[arxiv](https://arxiv.org/abs/2412.05271)] [[cool](https://papers.cool/arxiv/2412.05271)] [[pdf](https://arxiv.org/pdf/2412.05271)]
> **Authors**: Zhe Chen,Weiyun Wang,Yue Cao,Yangzhou Liu,Zhangwei Gao,Erfei Cui,Jinguo Zhu,Shenglong Ye,Hao Tian,Zhaoyang Liu,Lixin Gu,Xuehui Wang,Qingyun Li,Yimin Ren,Zixuan Chen,Jiapeng Luo,Jiahao Wang,Tan Jiang,Bo Wang,Conghui He,Botian Shi,Xingcheng Zhang,Han Lv,Yi Wang,Wenqi Shao, et al. (17 additional authors not shown)
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: Technical Report
- **标题**: 通过模型，数据和测试时间缩放扩大开源多模型的性能边界
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们介绍了InternVL 2.5，这是一个高级多式模式模型（MLLM）系列，该系列基于InternVL 2.0，维护其核心模型体系结构，同时在培训和测试策略以及数据质量中引入了显着增强功能。在这项工作中，我们深入研究了模型缩放与性能之间的关系，系统地探索了视觉编码器，语言模型，数据集大小和测试时间配置的性能趋势。 Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet.值得注意的是，我们的模型是第一个超过MMMU基准测试的70％的开源MLLM，它通过经过三链（COT）推理实现了3.7分的改进，并展示了强大的测试时间扩展潜力。我们希望该模型通过为开发和应用多模式AI系统设定新的标准来为开源社区做出贡献。 huggingface演示请参见https://huggingface.co/spaces/opengvlab/internvl

### CompCap: Improving Multimodal Large Language Models with Composite Captions 
[[arxiv](https://arxiv.org/abs/2412.05243)] [[cool](https://papers.cool/arxiv/2412.05243)] [[pdf](https://arxiv.org/pdf/2412.05243)]
> **Authors**: Xiaohui Chen,Satya Narayan Shukla,Mahmoud Azab,Aashu Singh,Qifan Wang,David Yang,ShengYun Peng,Hanchao Yu,Shen Yan,Xuewen Zhang,Baosheng He
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: COMPCAP：用复合标题改进多模式的大型语言模型
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 多模式大语言模型（MLLM）如何理解复合图像？复合图像（顺式）是通过合并多个视觉元素（例如图表，海报或屏幕截图）来创建的合成视觉效果，而不是直接被相机捕获。尽管CI在现实世界中普遍存在，但最近的MLLM开发主要集中于解释自然图像（NIS）。我们的研究表明，当前的MLLM在准确理解CI，通常努力提取信息或基于这些图像进行复杂的推理方面面临重大挑战。我们发现，CIS的现有培训数据主要用于提问的任务（例如，在ChartQA和ScienceQA等数据集中），而高质量的图像符合数据集（对于强大的视觉语言对准至关重要）仅适用于NIS。为了弥合这一差距，我们引入了复合字幕（Compcap），这是一个灵活的框架，该框架利用大型语言模型（LLM）和自动化工具以准确且详细的字幕合成CIS。使用compcap，我们策划了compcap-118k，这是一个包含六种CI类型的118K图像捕获对的数据集。我们通过有三种尺寸的监督微调MLLM来验证Compcap-118K的有效性：XGEN-MM-INST.-4B和LLAVA-NEXT-NEXT-VICUNA-7B/13B。经验结果表明，Compcap-118K显着增强了MLLM对CI的了解，在11个基准中，平均增长率分别为1.7％，2.0％和2.9％。

### LinVT: Empower Your Image-level Large Language Model to Understand Videos 
[[arxiv](https://arxiv.org/abs/2412.05185)] [[cool](https://papers.cool/arxiv/2412.05185)] [[pdf](https://arxiv.org/pdf/2412.05185)]
> **Authors**: Lishuai Gao,Yujie Zhong,Yingsen Zeng,Haoxian Tan,Dengjie Li,Zheng Zhao
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: linvt：授权您的图像级大型语言模型以了解视频
- **领域**: 计算机视觉和模式识别,机器学习,多媒体
- **摘要**: 大型语言模型（LLM）已被广泛用于各种任务，激励我们开发基于LLM的视频助手。我们没有从头开始训练，而是提出一个模块，将任意训练的基于良好的图像的LLMS转换为视频LLM（接受视频数据培训后）。为了更好地适应图像插件来处理视频，我们介绍了两个设计原则：线性转换，以保留原始的视觉语言对齐方式和代表性信息凝结，从冗余视频内容中。在这些原则的指导下，我们提出了一个插件线性视频令牌（LINVT），该视频令牌（LINVT）使现有的Image-llms能够理解视频。我们用六个最近的Visual LLM进行了基准测试：Aquila，Blip-3，Internvl2，Mipha，Molmo和Qwen2-Vl，展示了LINVT的高兼容性。基于LINVT的LLMS在各种视频基准中实现了最先进的性能，这说明了Linvt在多模式视频理解中的有效性。

### LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation 
[[arxiv](https://arxiv.org/abs/2412.05148)] [[cool](https://papers.cool/arxiv/2412.05148)] [[pdf](https://arxiv.org/pdf/2412.05148)]
> **Authors**: Donald Shenaj,Ondrej Bohdal,Mete Ozay,Pietro Zanuttigh,Umberto Michieli
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: 17 pages, 20 figures
- **标题**: lora.rar：通过超级核武器学会合并劳拉，以进行主题风格的图像生成
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 图像生成模型的最新进步已通过用户定义的主题（内容）和样式启用了个性化的图像创建。先前的工作通过基于优化的方法合并相应的低级适应参数（LORAS）来实现个性化，这些方法在计算上是要求的，并且不适合在智能手机等资源受限设备上实时使用。为了解决这个问题，我们介绍了Lora $。$ RAR，这种方法不仅可以提高图像质量，而且在合并过程中实现了超过$ 4000 \ times $的显着速度。 lora $。$ rar预训练在各种内容风格的洛拉对上的超级net工作，学习了一种有效的合并策略，该策略将概括为新的，看不见的内容式配对，实现快速，高质量的个性化。此外，我们确定了现有评估指标的限制，以实现内容式质量，并使用多模式大语言模型（MLLM）提出了新协议，以进行更准确的评估。通过MLLM评估和人类评估，我们的方法在内容和风格的忠诚度上都大大优于当前的艺术状态。

### How to Squeeze An Explanation Out of Your Model 
[[arxiv](https://arxiv.org/abs/2412.05134)] [[cool](https://papers.cool/arxiv/2412.05134)] [[pdf](https://arxiv.org/pdf/2412.05134)]
> **Authors**: Tiago Roxo,Joana C. Costa,Pedro R. M. Inácio,Hugo Proença
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 如何从模型中挤出解释
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 当今，深度学习模型在执行各种任务时的可靠性已被广泛使用。但是，他们通常不会提供决定背后的推理，这是一个重要的缺点，尤其是对于更敏感的领域，例如生物识别技术，安全和医疗保健。提供可解释性的最常用方法在基于模型梯度反向传播的图像上创建感兴趣区域的视觉注意力图。尽管这是一种可行的方法，但当前方法是针对图像设置和默认/标准深度学习模型的，这意味着它们需要进行大量改编才能在视频/多模式设置和自定义体系结构上工作。本文提出了一种可解释性的方法，该方法是基于挤压和激发（SE）块的新颖使用，从而产生视觉注意热图。通过在任何模型的分类层之前包括一个SE块，我们可以通过SE向量操纵（SE块的关键组件之一）来检索最有影响力的功能。我们的结果表明，这种新的基于SE的可解释性可以应用于图像和视频/多模式设置中的各种模型，即使用主动扬声器检测数据集的Celeba和行为生物特征的面部特征的生物识别。此外，我们的建议不会损害原始任务的模型性能，并且具有竞争性的结果，并在最新的对象数据集中采用当前的可解释性方法，强调了其在生物识别环境中在不同数据中执行的稳健性。

### Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for Radiology Report Generation 
[[arxiv](https://arxiv.org/abs/2412.04954)] [[cool](https://papers.cool/arxiv/2412.04954)] [[pdf](https://arxiv.org/pdf/2412.04954)]
> **Authors**: Xi Zhang,Zaiqiao Meng,Jake Lever,Edmond S. L. Ho
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: Accepted by BioNLP@ACL 2024
- **标题**: rrg24上的gla-ai4biom：放射学报告生成的视觉指导调整适应
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 我们介绍了一种以放射学为重点的视觉语言模型，旨在从胸部X射线制作放射学报告。以先前的发现，即大型语言模型（LLMS）可以在与预审计的视觉编码器保持一致时获得多模式的功能，我们在胸部X射线图像上表现出相似的潜力。这种集成增强了模型理解和描述胸部X射线图像的能力。我们的模型将图像编码器与基于Vicuna-7b体系结构的微型LLM结合在一起，使其能够以显着的准确性生成放射学报告的不同部分。训练过程涉及一种两阶段的方法：（i）胸部X射线特征与LLM（II）的初始对齐，然后进行放射学报告生成的微调。

### Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.04939)] [[cool](https://papers.cool/arxiv/2412.04939)] [[pdf](https://arxiv.org/pdf/2412.04939)]
> **Authors**: Zehao Wang,Xinpeng Liu,Xiaoqian Wu,Yudonglin Zhang,Zhou Fang,Yifan Fang,Junfu Pu,Cewu Lu,Yong-Lu Li
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 动词海市rage：在多模式模型中揭幕和评估动词概念幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）最近引起了极大的关注，并在各种任务中表现出了出色的功能，例如OCR，VQA，字幕，$ \ textit {etc} $。但是，幻觉仍然是一个持续的问题。尽管已经提出了许多方法来减轻幻觉，并取得了显着的改进，但这些方法主要集中于减轻有关$ \ textbf {对象/名词相关} $概念的幻觉。动词概念是理解人类行为至关重要的，在很大程度上被忽略了。在本文中，据我们所知，我们是$ \ textbf {first} $研究$ \ textbf {动词幻觉} $ MLLM的现象。我们的发现表明，大多数最先进的MLLM都患有严重动词幻觉。为了评估现有缓解方法对动词幻觉的对象概念幻觉的有效性，我们评估了这些方法，发现它们没有有效地解决动词幻觉。为了解决这个问题，我们提出了一种新颖的基于知识的动词知识调整方法来减轻动词幻觉。实验结果表明，我们的方法大大降低了与动词相关的幻觉。 $ \ textit {我们的代码和数据将被公开可用} $。

### EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation 
[[arxiv](https://arxiv.org/abs/2412.04903)] [[cool](https://papers.cool/arxiv/2412.04903)] [[pdf](https://arxiv.org/pdf/2412.04903)]
> **Authors**: Yongxin Wang,Meng Cao,Haokun Lin,Mingfei Han,Liang Ma,Jin Jiang,Yuhao Cheng,Xiaodan Liang
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: 19 pages
- **标题**: EACO：通过关键观察增强多模式LLM的对齐
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 多模式大型语言模型（MLLM）在利用指令微调特定数据集的各种视觉问题答案和推理任务上取得了显着进度。他们还可以从人类注释的偏好数据中学习，以增强其推理能力并减轻幻觉。大多数首选项数据都是从模型本身生成的。但是，现有方法需要高质量的关键标签，这些标签昂贵，并且依赖于人类或专有模型，例如GPT-4V。在这项工作中，我们提出通过临界观察（EACO）在MLLM中加强对齐，该观察值通过自我生成的偏好数据对齐MLLM，仅在经济上仅使用5K图像。我们的方法始于收​​集和完善评分指令调查数据集以训练评论家的关键评估模型。这位评论家观察了跨多个维度的模型响应，选择了首选和非脱颖而出的输出，以进行精制的直接偏好优化（DPO）调整。为了进一步提高模型性能，我们在偏好调整后采用了额外的监督微调阶段。 EACO在HallusionBench上将整体幻觉降低了65.6％，并将MME认知的推理能力提高了21.8％。 EACO在多个基准测试中，EACO比LLAVA-V1.6-Mistral-7b提高了8.5％。值得注意的是，EACO还显示了开源MLLM的潜在关键能力，这表明EACO是提高MLLM能力的可行途径。

### Hero-SR: One-Step Diffusion for Super-Resolution with Human Perception Priors 
[[arxiv](https://arxiv.org/abs/2412.07152)] [[cool](https://papers.cool/arxiv/2412.07152)] [[pdf](https://arxiv.org/pdf/2412.07152)]
> **Authors**: Jiangang Wang,Qingnan Fan,Qi Zhang,Haigen Liu,Yuhang Yu,Jinwei Chen,Wenqi Ren
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: 16 pages, 9 figures
- **标题**: 英雄-SR：与人类感知先验的超分辨率的一步扩散
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于扩散模型的强大先验，最近的方法在解决现实世界超级分辨率（REAL-SR）方面表现出了希望。但是，达到人类感知需求的语义一致性和知觉自然性仍然很困难，尤其是在重大降级和各种输入复杂性的条件下。为了解决这个问题，我们提出了英雄 -  SR，这是一个基于人类感知先验的一步基于扩散的SR框架。 Hero-SR由两个新型模块组成：动态时步模块（DTSM），它们可以自适应地选择了灵活地符合人类感知标准的最佳扩散步骤，以及开放世界的多模式监督（OWMS），该步骤（OWMS）可以通过剪辑来提高语义一致性和感知自然性来整合图像和文本域的指导。通过这些模块，Hero-SR生成了高分辨率的图像，不仅可以保留复杂的细节，还反映了人类的感知偏好。广泛的实验证明了英雄 -  SR可以在REAL-SR中实现最先进的表现。该代码将在纸质接受后公开获得。

### MM-PoE: Multiple Choice Reasoning via. Process of Elimination using Multi-Modal Models 
[[arxiv](https://arxiv.org/abs/2412.07148)] [[cool](https://papers.cool/arxiv/2412.07148)] [[pdf](https://arxiv.org/pdf/2412.07148)]
> **Authors**: Sayak Chakrabarty,Souradip Pal
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: MM-POE：多项选择推理通过。使用多模式模型消除的过程
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 本文介绍了多项选择推理。使用多模式模型消除过程，本文称为多模式消除过程（MM-POE）。这种新颖的方法是为了增强视觉视觉推理任务中视觉模型（VLM）的功效而设计的。 MM-POE与传统方法分开评估每个选项的传统方法采用了双步得分范式，最初识别和排除了令人难以置信的选择，随后集中于最可能的剩余选项。这种方法模仿了人类的测试策略，在选择最佳响应之前，个人通常会消除明显的答案。我们在三个基准数据集中进行的经验评估表明，MM-POE显着改善了当代最先进的VLM的零击和几乎没有射击的性能。至关重要的是，这种方法不仅扩大了淘汰过程对多模式上下文的应用，而且还可以允许几乎没有实验，从而解决了仅在零拍设置中使用POE的两个主要局限性，并且仅在语言框架中使用。结果，MM-POE不仅完善了VLM的推理能力，而且还扩大了其对复杂的视觉提问方案的适用性。所有支持我们工作的代码和文档均可在https://pypi.org/project/mm-poe/上获得，使研究人员和从业人员可以轻松整合并进一步开发这些技术。

### A multimodal ensemble approach for clear cell renal cell carcinoma treatment outcome prediction 
[[arxiv](https://arxiv.org/abs/2412.07136)] [[cool](https://papers.cool/arxiv/2412.07136)] [[pdf](https://arxiv.org/pdf/2412.07136)]
> **Authors**: Meixu Chen,Kai Wang,Payal Kapur,James Brugarolas,Raquibul Hannan,Jing Wang
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: 10 pages, 3 figures, 4 tables
- **标题**: 一种多模式的合奏方法，用于清除细胞肾细胞癌治疗结果预测
- **领域**: 计算机视觉和模式识别,定量方法
- **摘要**: 目的：透明细胞肾细胞癌（CCRCC）可靠的癌症预后模型可以增强个性化治疗。我们开发了一个多模式集成模型（MMEM），该模型（MMEM）整合了预处理的临床数据，多摩学数据和组织病理学全幻灯片图像（WSI）数据，以预测CCRCC患者的总生存率（OS）和无病生存（DFS）。方法：我们分析了来自癌症基因组肾脏肾脏透明细胞癌（TCGA-KIRC）数据集的226名患者，其中包括OS，DFS随访数据以及五种数据模式：临床数据，WSIS和三个多组分数据集（mRNA，miRNA，miRNA和DNA甲基化）。为OS和DFS建立了单独的生存模型。带有正向特征选择的COX-PROUTARTIANS危害（CPH）模型用于临床和多摩变数据。使用Resnet和三种通用基础模型提取WSI的功能。基于深度学习的CPH模型预测了使用编码的WSI功能的生存。所有模型的风险评分都是根据培训性能组合的。结果：使用一致性指数（C-指数）和AUROC评估了性能。基于临床功能的CPH模型获得了OS和DFS任务的最高重量。在基于WSI的模型中，通用基础模型（UNI）取得了最佳性能。最终的MMEM模型超过了单模模型，达到0.820（OS）和0.833（DFS）的C-指标，AUROC值为0.831（3年患者死亡）和0.862（癌症复发）。使用预测的风险中值对高风险组进行分层，与单模模型相比，对数秩检验在OS和DFS中的性能都提高了。结论：MMEM是CCRCC患者的第一个多模式模型，它整合了五种数据模式。它在预后能力方面的表现优于单模式模型，如果被独立验证，则有可能协助CCRCC患者管理。

### DiffCLIP: Few-shot Language-driven Multimodal Classifier 
[[arxiv](https://arxiv.org/abs/2412.07119)] [[cool](https://papers.cool/arxiv/2412.07119)] [[pdf](https://arxiv.org/pdf/2412.07119)]
> **Authors**: Jiaqing Zhang,Mingxiang Cao,Xue Yang,Kai Jiang,Yunsong Li
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: diffclip：几乎没有语言驱动的多模式分类器
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型（例如对比性语言图像预处理（剪辑））在用语言信息分析自然图像时表现出令人印象深刻的性能。但是，由于用于训练的图像文本对的可用性有限，这些模型通常会遇到挑战，例如遥感。为了解决此问题，我们引入了DiffClip，这是一个新颖的框架，可扩展剪辑以有效传达全面的语言驱动的语义信息，以准确地分类高维多模式遥感图像。 DiffClip是一种几次学习方法，它利用未标记的图像进行预处理。它采用无监督的面具扩散学习来捕获不同方式的分布而无需标签。模态共享的图像编码器将多模式数据映射到统一的子空间中，从模态跨模态提取具有一致参数的共享特征。训练有素的图像编码器通过将视觉表示形式与剪辑中的类标签文本信息保持一致，从而进一步增强了学习。通过集成这些方法，DIFFCLIP使用最少数量的图像文本对显着提高了剪辑性能。我们评估了广泛使用的高维多模式数据集的DIFFCLIP，这证明了其在解决几乎没有带注释的分类任务方面的有效性。与夹子相比，DIFFCLIP在三个遥感数据集中的总体准确性提高了10.65％，而仅利用2张图像文本对。该代码已在https://github.com/icey-zhang/diffclip上发布。

### Maya: An Instruction Finetuned Multilingual Multimodal Model 
[[arxiv](https://arxiv.org/abs/2412.07112)] [[cool](https://papers.cool/arxiv/2412.07112)] [[pdf](https://arxiv.org/pdf/2412.07112)]
> **Authors**: Nahid Alam,Karthik Reddy Kanjula,Surya Guthikonda,Timothy Chung,Bala Krishna S Vegesna,Abhipsha Das,Anthony Susevski,Ryan Sze-Yin Chan,S M Iftekhar Uddin,Shayekh Bin Islam,Roshan Santhosh,Snegha A,Drishti Sharma,Chen Liu,Isha Chaturvedi,Genta Indra Winata,Ashvanth. S,Snehanshu Mukherjee,Alham Fikri Aji
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 玛雅：指令固定的多语言多模式模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 大型视觉模型（VLM）的快速发展导致了学术基准的令人印象深刻的成果，主要是用语言广泛的语言。但是，当前VLM处理低资源语言和各种文化背景的能力仍然存在很大的差距，这在很大程度上是由于缺乏高质量，多样化和安全性的数据。因此，这些模型通常很难以一种没有毒性的方式理解低资源语言和文化细微差别。为了解决这些限制，我们介绍了一种开源的多语言模型Maya。我们的贡献是三个方面：1）基于LLAVA预处理数据集的八种语言的多语言图像文本预处理数据集； 2）对LLAVA数据集中毒性的彻底分析，然后创建跨八种语言的新型无毒版本； 3）支持这些语言的多语言图像文本模型，增强视觉任务中的文化和语言理解。可在https://github.com/nahidalam/maya上找到代码。

### ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models 
[[arxiv](https://arxiv.org/abs/2412.07012)] [[cool](https://papers.cool/arxiv/2412.07012)] [[pdf](https://arxiv.org/pdf/2412.07012)]
> **Authors**: Jieyu Zhang,Le Xue,Linxin Song,Jun Wang,Weikai Huang,Manli Shu,An Yan,Zixian Ma,Juan Carlos Niebles,Silvio Savarese,Caiming Xiong,Zeyuan Chen,Ranjay Krishna,Ran Xu
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: code: https://github.com/JieyuZ2/ProVision dataset: https://huggingface.co/datasets/Salesforce/ProVision-10M
- **标题**: 条款：以编程性扩展为中心的以视觉为中心的指令数据
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着多模式应用程序的兴起，指导数据对于培训能够理解基于图像的查询的多模式模型而变得至关重要。现有实践依赖于强大但昂贵的大语言模型（LLM）或多模式模型（MLMS）来产生指导数据。这些通常容易出现幻觉，许可问题，而发电过程通常很难扩展和解释。在这项工作中，我们提出了一种编程方法，该方法采用场景图作为图像和人工编写程序的符号表示，以系统地综合以视觉为中心的指令数据。我们的方法确保了数据生成过程的可解释性和可控性，并在保持事实准确性的同时有效地扩展。通过实施24个单图像，14个多图像指令生成器和场景图生成管道的套件，我们为任何给定的图像构建了一个可扩展的，具有成本效益的系统：提供有关对象，属性，关系，深度等的各种问题。应用于视觉基因组和Datacomp数据集，我们生成了超过1000万个指令数据点，配置10M，并在MLMS的预训练和说明调谐阶段中利用它们。当在指令调整阶段采用时，我们的单图像指令数据可在2D拆分上提高7％，而CVBench的3D拆分率为8％，QBENCH2，REALWORLDQA和MMMU的性能提高了3％。我们的多图像指导数据导致了8％的螳螂效应。将我们的数据纳入XGEN-MM-4B的预训练和微调阶段，导致11个基准的平均提高1.6％。

### SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations 
[[arxiv](https://arxiv.org/abs/2412.06878)] [[cool](https://papers.cool/arxiv/2412.06878)] [[pdf](https://arxiv.org/pdf/2412.06878)]
> **Authors**: Zhaorun Chen,Francesco Pinto,Minzhou Pan,Bo Li
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: 43 pages, 20 figures
- **标题**: Safewatch：带有透明说明的视频护栏模型之后有效的安全政策
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 随着生成性AI的兴起和高质量视频生成的快速增长，视频护栏变得比以往任何时候都变得更加重要，以确保跨平台的安全性。然而，当前的视频护栏要么过于简单，要么依赖于在有限的不安全类别的简单政策上训练的纯分类模型，这些模型缺乏详细的解释，或者促使具有长期安全指南的多模式大语言模型（MLLMS），这些模型具有长期的安全指南，这些指南效率低下且无效，可用于护卫现实世界中的现实世界中的内容。为了弥合这一差距，我们提出了Safewatch，这是一种有效的基于MLLM的视频护栏模型，旨在遵循自定义的安全策略，并以零拍的方式提供具有特定于内容的解释的多标签视频护栏输出。特别是，与传统的基于MLLM的护栏不同，这些护栏编码所有安全策略自动审核，导致效率低下和偏见，Safewatch独特地编码了每个政策块并行地编码每个政策块，并消除其位置偏见，使所有政策均同时同时参与。此外，为了提高效率和准确性，Safewatch结合了一种策略感知的视觉令牌修剪算法，该算法可自适应地选择每个政策的最相关的视频令牌，从而丢弃嘈杂或无关紧要的信息。这允许更加集中，符合政策的护栏，并大大减少计算开销。考虑到现有视频护栏基准测试的局限性，我们提出了Safewatch-Bench，这是一个大规模的视频护栏基准测试，其中包括超过200万个视频，涵盖了六个安全类别，涵盖了30多个任务，以确保对所有潜在安全方案进行全面覆盖。 Safewatch在Safewatch-Bench上的表现优于SOTA，基准的13.6％，削减成本为10％，并提供了LLM和人类评论证实的顶级解释。

### Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation 
[[arxiv](https://arxiv.org/abs/2412.06785)] [[cool](https://papers.cool/arxiv/2412.06785)] [[pdf](https://arxiv.org/pdf/2412.06785)]
> **Authors**: Ruihan Gao,Kangle Deng,Gengshan Yang,Wenzhen Yuan,Jun-Yan Zhu
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: Accepted to NeurIPS 2024. Project webpage: https://ruihangao.github.io/TactileDreamFusion/ Code: https://github.com/RuihanGao/TactileDreamFusion
- **标题**: 触觉梦幻：为3D代的触觉感测
- **领域**: 计算机视觉和模式识别,图形
- **摘要**: 3D生成方法显示了以扩散图像先验为动力的视觉令人信服的结果。但是，它们通常无法产生逼真的几何细节，从而导致表面过光滑或几何细节不准确地在反照率图中烘烤。为了解决这个问题，我们引入了一种新方法，该方法将触摸作为一种附加模式，以改善生成的3D资产的几何细节。我们设计了一个轻巧的3D纹理字段，以合成视觉和触觉纹理，并在视觉和触觉域上的2D扩散模型先验的指导下进行。我们将视觉纹理生成在高分辨率触觉正常上调节，并使用自定义的texturedReamBooth指导基于补丁的触觉纹理细化。我们进一步提出了一个多部分生成管道，使我们能够综合各个地区的不同纹理。据我们所知，我们是第一个利用高分辨率触觉传感来增强3D代任务的几何细节的人。我们在文本到3D和图像到3D设置中都评估了我们的方法。我们的实验表明，我们的方法提供了定制且逼真的细几何纹理，同时保持了两种视力和触摸方式之间的准确对齐。

### Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2412.06777)] [[cool](https://papers.cool/arxiv/2412.06777)] [[pdf](https://arxiv.org/pdf/2412.06777)]
> **Authors**: Xin Fei,Wenzhao Zheng,Yueqi Duan,Wei Zhan,Masayoshi Tomizuka,Kurt Keutzer,Jiwen Lu
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: Code is available at: https://github.com/Barrybarry-Smith/Driv3R
- **标题**: DRIV3R：学习密集的4D重建用于自动驾驶
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 对于自主驾驶感知，实时4D重建仍然是一个至关重要的挑战。大多数现有的方法都依赖于自学或多模式传感器融合的深度估计。在本文中，我们提出了DRIV3R，这是一种基于DUST3R的框架，可直接从多视图图像序列中回归人均点图。为了实现流式密集的重建，我们维护一个内存池，以推理传感器之间的空间关系和动态时间上下文，以增强多视图3D一致性和时间整合。此外，我们采用4D流预测器来识别场景中的移动对象，以指导我们的网络更多地关注重建这些动态区域。最后，我们以无优化的方式将所有人均指数始终如一地与世界坐标系保持一致。我们对大规模Nuscenes数据集进行了广泛的实验，以评估我们方法的有效性。 DRIV3R优于4D动态场景重建中的先前框架，与需要全局对齐方式的方法相比，推理速度更快15倍。代码：https：//github.com/barrybarry-smith/driv3r。

### ContRail: A Framework for Realistic Railway Image Synthesis using ControlNet 
[[arxiv](https://arxiv.org/abs/2412.06742)] [[cool](https://papers.cool/arxiv/2412.06742)] [[pdf](https://arxiv.org/pdf/2412.06742)]
> **Authors**: Andrei-Robert Alexandrescu,Razvan-Gabriel Petec,Alexandru Manole,Laura-Silvia Diosan
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: 9 pages, 5 figures, 2 tables
- **标题**: Contrail：使用ControlNet的逼真的铁路图像合成框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 深度学习成为无处不在的范式，因为它在众多领域中的出色效果和适用性。但是，该方法遭受了实现此类模型潜力所需的数据需求的高需求。图像合成的人工智能的不断增长的子场旨在通过设计能够创建原始和逼真的图像的智能模型来解决此限制，这可能会大大减少对真实数据的需求。稳定的扩散产生范式最近推进了最先进的方法，以超过所有以前的基准。在这项工作中，我们根据新型稳定扩散模型ControlNet提出了缩进框架，我们通过多模式调节方法赋予了缩影框架。我们实验了合成铁路图像生成的任务，在该任务中，我们通过使用逼真的合成图像丰富数据集来改善特定于铁路的任务的性能，例如铁路语义分割。

### VP-MEL: Visual Prompts Guided Multimodal Entity Linking 
[[arxiv](https://arxiv.org/abs/2412.06720)] [[cool](https://papers.cool/arxiv/2412.06720)] [[pdf](https://arxiv.org/pdf/2412.06720)]
> **Authors**: Hongze Mi,Jinyuan Li,Xuying Zhang,Haoran Cheng,Jiahao Wang,Di Sun,Gang Pan
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: VP-MEL：视觉提示指导的多模式实体链接
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式实体链接（MEL）是一项旨在将多模式环境中提及与知识库中相应实体联系起来的任务，由于近年来其广泛的应用，引起了很多关注。但是，现有的MEL方法通常依靠提及单词作为检索提示，这限制了其有效利用图像和文本信息的能力。这种依赖使梅尔在某些情况下与准确检索实体斗争，尤其是当焦点放在图像对象或文本中缺少单词时。为了解决这些问题，我们引入了视觉提示指导多模式实体链接（VP-MEL）任务。给定文本图像对，VP-MEL的目的是将图像中的标记区域（即视觉提示）链接到知识库中的相应实体。为了促进此任务，我们提供了一个新的数据集VPWiki，该数据集是专门为VP-MEL设计的。此外，我们提出了一个名为IIER的框架，该框架使用视觉提示来增强视觉特征提取，并利用验证的侦探-VLM模型来捕获潜在信息。 VPWIKI数据集的实验结果表明，对于VP-MEL任务，IER的表现优于多个基准测试的基线方法。

### ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance 
[[arxiv](https://arxiv.org/abs/2412.06673)] [[cool](https://papers.cool/arxiv/2412.06673)] [[pdf](https://arxiv.org/pdf/2412.06673)]
> **Authors**: Chunwei Wang,Guansong Lu,Junwei Yang,Runhui Huang,Jianhua Han,Lu Hou,Wei Zhang,Hang Xu
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: Illume：阐明您的LLM以查看，绘画和自我增强
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们介绍了Illume，这是一种统一的多模式大型语言模型（MLLM），该模型通过统一的下一步预测公式无缝地将多模式的理解和发电能力整合到单个大语言模型中。为了解决图像文本对齐通常所需的大型数据集大小，我们建议通过设计视觉令牌来提高数据效率，该视觉令牌包括语义信息和进行渐进的多阶段训练程序。这种方法可将数据集大小降低至1500万，以预期 - 超过通常需要的四倍 - 同时使用现有的统一MLLM（例如Janus）实现竞争性甚至卓越的性能。此外，为了促进理解能力和发电能力之间的协同增强（在先前的作品中都没有探索），我们引入了一种新型的自我增强的多模式对准方案。该方案监督了MLLM，以自我评估文本描述与自我生成的图像之间的一致性，从而促进模型更准确地解释图像，并避免由图像生成中的未对准而导致的不现实和不正确的预测。基于广泛的实验，我们提出的Illume脱颖而出，并与各种基准的最先进的统一MLLM和专业模型竞争，以进行多模式理解，生成和编辑。

### The Narrow Gate: Localized Image-Text Communication in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2412.06646)] [[cool](https://papers.cool/arxiv/2412.06646)] [[pdf](https://arxiv.org/pdf/2412.06646)]
> **Authors**: Alessandro Serra,Francesco Ortu,Emanuele Panizon,Lucrezia Valeriani,Lorenzo Basile,Alessio Ansuini,Diego Doimo,Alberto Cazzaniga
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 狭窄的门：视觉模型中的局部图像文本通信
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 多模式训练的最新进展显着改善了统一模型中图像理解和产生的整合。这项研究调查了视觉模型（VLM）如何处理图像实现任务，特别关注如何处理视觉信息并将其传输到文本域。我们将同时生成图像和文本的VLM与仅输出文本的VLM进行比较，突出了信息流的关键差异。我们发现，在具有多模式输出的模型中，图像和文本嵌入在残差流中更加分开。此外，模型在如何从视觉上到文本令牌交换的信息各不相同。仅输出文本的VLM展示了分布式通信模式，其中通过多个图像令牌交换信息。相比之下，经过培训的图像和文本生成的模型依赖于单个令牌，该令牌是视觉信息的狭窄大门。我们证明，烧毁这个单一令牌会显着恶化图像理解任务的性能。此外，修改此令牌可以有效地转向图像语义，表明有针对性的本地干预措施可以可靠地控制模型的全局行为。

### Detecting Facial Image Manipulations with Multi-Layer CNN Models 
[[arxiv](https://arxiv.org/abs/2412.06643)] [[cool](https://papers.cool/arxiv/2412.06643)] [[pdf](https://arxiv.org/pdf/2412.06643)]
> **Authors**: Alejandro Marco Montejano,Angela Sanchez Perez,Javier Barrachina,David Ortiz-Perez,Manuel Benavent-Lledo,Jose Garcia-Rodriguez
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 通过多层CNN模型检测面部图像操作
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 数字图像操纵技术的快速演变对内容验证构成了重大挑战，诸如稳定的扩散和journey之类的模型产生了高度现实但合成的图像，这些图像可以欺骗人类的感知。这项研究开发并评估了专门针对这些操纵图像的专门量身定制的卷积神经网络（CNN）。该研究对三个逐渐复杂的CNN架构进行了比较分析，评估了它们在各种面部图像修改中对操作进行分类和定位的能力。正则化和优化技术是系统合并的，以改善特征提取和性能。结果表明，所提出的模型在区分操纵图像和真实图像的情况下达到了高达76 \％的准确性，超过了传统方法。这项研究不仅强调了CNN在增强数字媒体验证工具的鲁棒性方面的潜力，而且还为有效的建筑适应和培训策略提供了有关低构成环境的见解。未来的工作将通过扩展架构来处理更多样化的操纵技术并集成多模式数据以提高检测功能，从而建立在这些发现的基础上。

### 3D Spatial Understanding in MLLMs: Disambiguation and Evaluation 
[[arxiv](https://arxiv.org/abs/2412.06613)] [[cool](https://papers.cool/arxiv/2412.06613)] [[pdf](https://arxiv.org/pdf/2412.06613)]
> **Authors**: Chun-Peng Chang,Alain Pagani,Didier Stricker
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: MLLM中的3D空间理解：歧义和评估
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在图像字幕和问答等任务上取得了重大进展。但是，尽管这些模型可以生成现实的字幕，但它们通常在提供精确的说明方面努力，尤其是在复杂的3D环境中本地化和歧义对象时。此功能至关重要，因为MLLM与协作机器人系统更加集成。在目标对象被类似物体（干扰物）包围的情况下，机器人必须提供清晰的，空间意识的说明，以有效地指导人类。我们将这一挑战称为上下文对象的本地化和歧义，这比传统的3D密集字幕施加更严格的约束，尤其是在确保目标排他性方面。作为回应，我们提出了简单而有效的技术，以增强模型的本地化和歧义目标对象的能力。我们的方法不仅在评估句子相似性的常规指标上实现了最新的性能，而且还通过3D视觉接地模型证明了3D空间理解的改善。

### From Uncertainty to Trust: Enhancing Reliability in Vision-Language Models with Uncertainty-Guided Dropout Decoding 
[[arxiv](https://arxiv.org/abs/2412.06474)] [[cool](https://papers.cool/arxiv/2412.06474)] [[pdf](https://arxiv.org/pdf/2412.06474)]
> **Authors**: Yixiong Fang,Ziran Yang,Zhaorun Chen,Zhuokai Zhao,Jiawei Zhou
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: Code is released at https://github.com/kigb/DropoutDecoding
- **标题**: 从不确定性到信任：增强具有不确定性引导辍学解码的视觉模型的可靠性
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 大型视觉模型（LVLMS）在多模式任务中表现出显着的功能，但容易误解视觉输入，通常会导致幻觉和不可靠的输出。为了应对这些挑战，我们提出了辍学解码，这是一种新颖的推理时间方法，它量化了视觉令牌的不确定性，并有选择地掩盖了不确定的令牌以改善解码。我们的方法通过将其投影到文本空间上并将其分解为质地和认知成分来衡量每个视觉令牌的不确定性。具体而言，我们专注于认识论不确定性，后者更有效地捕获了与感知相关的错误。受辍学的启发，我们引入了不确定性引导的令牌辍学，该辍学将辍学原理应用于输入视觉令牌而不是模型参数，而在推理而不是训练期间。通过从蒙版解码上下文集合中汇总预测，辍学解码可降低由视觉令牌误解引起的错误。对包括椅子，宝座和MMBench在内的基准评估的评估表明，辍学大大减少了对象幻觉（OH），并提高了不同视觉环境中LVLM输出的可靠性和质量。

### Ranked from Within: Ranking Large Multimodal Models for Visual Question Answering Without Labels 
[[arxiv](https://arxiv.org/abs/2412.06461)] [[cool](https://papers.cool/arxiv/2412.06461)] [[pdf](https://arxiv.org/pdf/2412.06461)]
> **Authors**: Weijie Tu,Weijian Deng,Dylan Campbell,Yu Yao,Jiyang Zheng,Tom Gedeon,Tongliang Liu
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 从内部排名：对无标签的视觉问题回答的大型多模式排名
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着大型多模型模型（LMM）越来越多地在不同的应用程序中部署，因此需要适应性的现实世界模型排名的需求已成为最重要的。传统的评估方法在很大程度上以数据集为中心，依赖于固定的，标记的数据集和监督指标，这些指标是资源密集型的，并且可能缺乏对新型方案的普遍性，强调了无人监督排名的重​​要性。在这项工作中，我们通过利用其不确定性信号（例如SoftMax概率）来探索LMM的无监督模型排名。我们在视觉问题上评估了最新的LMM（例如LLAVA），以回答基准，从而分析了基于不确定性的指标如何反映模型性能。我们的发现表明，从软马克斯分布中得出的不确定性得分为跨多种任务进行排名的模型提供了强大的一致基础。这一发现使LMM在现实世界中的排名，无标记的数据以进行视觉问题回答，为在不需要手动注释而无需手动注释的情况下提供了选择跨不同域模型的实用方法。

### World knowledge-enhanced Reasoning Using Instruction-guided Interactor in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2412.06324)] [[cool](https://papers.cool/arxiv/2412.06324)] [[pdf](https://arxiv.org/pdf/2412.06324)]
> **Authors**: Mingliang Zhai,Cheng Li,Zengyuan Guo,Ningrui Yang,Xiameng Qin,Sanyuan Zhao,Junyu Han,Ji Tao,Yuwei Wu,Yunde Jia
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: AAAI 2025. 14 pages. Supplementary Material
- **标题**: 在自主驾驶中使用指导引导的互动者的世界知识增强推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 具有广泛世界知识的多模式大语模型（MLLM）振兴了自主驾驶，尤其是在可感知区域内的推理任务中。但是，当面对感知限制的领域（动态或静态遮挡区域）时，MLLM努力将感知能力与世界知识有效地整合到推理上。这些感知限制的区域可以掩盖关键的安全信息，尤其是对于弱势道路使用者。在本文中，我们提出了一个框架，该框架旨在通过增强感知能力和世界知识的整合来提高感知限制条件下的自主驾驶绩效。具体而言，我们提出了一个插件指导引导的交互模块，该模块弥合了模态差距并大大减少了输入序列长度，从而使其能够有效适应多视频视频输入。此外，为了更好地将世界知识与与驾驶相关的任务相结合，我们收集并完善了一个大规模的多模式数据集，其中包括200万自然语言QA对，170万个接地任务数据。为了评估该模型对世界知识的利用，我们引入了一个包括200K QA对的对象级风险评估数据集，其中这些问题需要多步推理利用世界知识来解决。广泛的实验验证了我们提出的方法的有效性。

### LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations 
[[arxiv](https://arxiv.org/abs/2412.06322)] [[cool](https://papers.cool/arxiv/2412.06322)] [[pdf](https://arxiv.org/pdf/2412.06322)]
> **Authors**: Mingjie Xu,Mengyang Wu,Yuzhi Zhao,Jason Chun Lok Li,Weifeng Ou
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: Accepted by the WACV 2025, including supplementary material
- **标题**: llava spacesgg：开放式摄影场景图的视觉指示调谐与增强的空间关系
- **领域**: 计算机视觉和模式识别
- **摘要**: 场景图生成（SGG）将视觉场景转换为结构化图表，为复杂的视觉任务提供了更深入的场景理解。但是，现有的SGG模型通常忽略了开放式摄影背景下的基本空间关系和与概括的斗争。为了解决这些局限性，我们建议Llava-spacegg，这是一种多式联运大语言模型（MLLM），设计用于开放式摄氏SGG，具有增强的空间关系建模。要训​​练它，我们收集了sgg指令调用数据集，名为SpaceSgg。该数据集是通过在我们的数据构建管道中使用开源模型组合可公开可用的数据集和合成数据来构建的。它结合了对象位置，对象关系和深度信息，从而产生了三种数据格式：空间SGG描述，提问和对话。为了增强MLLM的固有功能到SGG任务的转移，我们引入了两个阶段的培训范式。实验表明，Llava-SpaceGG优于其他开放式SGG方法，与基线相比，将召回率提高了8.6％，平均召回率增加了28.4％。我们的代码库，数据集和受过训练的模型在以下URL上可以在GitHub上公开访问：https：//github.com/endlinc/llava-pacesgg。

### Mastering Collaborative Multi-modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness 
[[arxiv](https://arxiv.org/abs/2412.06293)] [[cool](https://papers.cool/arxiv/2412.06293)] [[pdf](https://arxiv.org/pdf/2412.06293)]
> **Authors**: Qifan Yu,Zhebei Shen,Zhongqi Yue,Yang Wu,Wenqiao Zhang,Yunfei Li,Juncheng Li,Siliang Tang,Yueting Zhuang
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: 14 pages, 7 figures
- **标题**: 掌握协作多模式数据选择：关注信息性，独特性和代表性
- **领域**: 计算机视觉和模式识别
- **摘要**: 指令调整微调预训练的多模式大型语言模型（MLLM）来处理现实世界任务。但是，视觉指导数据集的快速扩展引入了数据冗余，导致了过度的计算成本。我们提出了一个协作框架，DataTailor，该框架利用了三个关键原则 - 信息，独特性和代表性 - 有效的数据选择。我们认为，有价值的样本应为任务提供信息，并表示样本分布（即，不是异常值）。我们进一步提出了针对每个原理评分的实用方法，该方法会自动适应给定数据集而无需乏味的超参数调整。对各种基准的全面实验表明，DataTailor仅使用15％的数据实现了全数据调整的100.8％，从而大大降低了计算成本，同时保持了较高的结果。这体现了MLLM发展中的“更少”的哲学。

### ZeroKey: Point-Level Reasoning and Zero-Shot 3D Keypoint Detection from Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.06292)] [[cool](https://papers.cool/arxiv/2412.06292)] [[pdf](https://arxiv.org/pdf/2412.06292)]
> **Authors**: Bingchen Gong,Diego Gomez,Abdullah Hamdi,Abdelrahman Eldesokey,Ahmed Abdelreheem,Peter Wonka,Maks Ovsjanikov
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: Project website is accessible at https://sites.google.com/view/zerokey
- **标题**: Zerokey：从大语言模型中检测点级推理和零射击3D键盘检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一种新型的零击方法，用于3D形状的键盘检测。视觉数据上的点级推理具有挑战性，因为它需要精确的本地化功能，即使对于诸如Dino或Clip之类的强大模型，也会构成问题。 3D KePoint检测的传统方法在很大程度上取决于注释的3D数据集和广泛的监督培训，从而限制了它们对新类别或域的可扩展性和适用性。相比之下，我们的方法利用了多模式大语言模型（MLLM）中嵌入的丰富知识。具体而言，我们首次证明了用于训练最近的MLLM的像素级注释可以用于在3D模型上提取和命名出色的关键点，而无需任何地面真相标签或监督。实验评估表明，与监督方法相比，我们的方法在标准基准方面取得了竞争性能，尽管在培训过程中不需要任何3D关键点注释。我们的结果突出了将语言模型集成到局部3D形状理解的潜力。这项工作为跨模式学习开辟了新的途径，并强调了MLLM在3D计算机视觉挑战方面的有效性。

### iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2412.06263)] [[cool](https://papers.cool/arxiv/2412.06263)] [[pdf](https://arxiv.org/pdf/2412.06263)]
> **Authors**: Lianyu Hu,Fanhua Shang,Liang Wan,Wei Feng
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: Illava：在大型多模型中，图像价值不到1/3输入令牌
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们介绍了Illava，这是一种简单的方法，可以在当前的大型视觉模型（LVLMS）上无缝部署，以通过几乎无损的模型性能大大增加吞吐量，而无需进一步的训练。 Illava通过找到并逐渐将冗余令牌与准确且快速的算法合并来实现这一目标，该算法只能在一步之内合并数百个令牌。尽管某些以前的方法已经在推理阶段直接探索或合并令牌以加速模型，但我们的方法在两个关键设计中都表现出色和吞吐量。首先，尽管大多数以前的方法仅尝试保存大语模型（LLMS）的计算，但我们的方法加速了图像编码器和LLMS在LVLMS中的正向通行，这两者在推理过程中都占据了很大一部分时间。其次，我们的方法将修剪代币的有益信息回收到现有的令牌中，从而避免了直接删除上下文令牌，例如以前的方法引起性能损失。 Illava可以接近2 $ \ times $ the吞吐量，并将内存成本降低一半，而不同尺度（包括7B，13B和34B）的型号的0.2 \％-0.5 \％的性能下降。在不同领域的任务（包括单图像，多图像和视频）上，Illava表现出强烈的概括性，并始终如一地有希望的效率。我们最终提供了丰富的可视化，以显示每个步骤中Illava的合并过程，这表明了对LVLMS中计算资源分布的见解。代码可从https://github.com/hulianyuyy/illava获得。

### MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents 
[[arxiv](https://arxiv.org/abs/2412.08014)] [[cool](https://papers.cool/arxiv/2412.08014)] [[pdf](https://arxiv.org/pdf/2412.08014)]
> **Authors**: Yun Xing,Nhat Chung,Jie Zhang,Yue Cao,Ivor Tsang,Yang Liu,Lei Ma,Qing Guo
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 魔术：通过协作LLM代理在上下文中掌握身体对抗性
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在驾驶场景中的物理对抗攻击可以暴露视觉感知模型中的关键漏洞。但是，由于不同的现实世界环境以及保持视觉自然性的要求，开发此类攻击仍然具有挑战性。在这一挑战的基础上，我们将身体对抗性攻击重新制定为单发斑块生成问题。我们的方法通过深层生成模型生成对抗性补丁，该模型考虑特定场景上下文，从而在匹配环境中直接进行物理部署。主要的挑战在于同时实现两个目标：生成对抗性补丁，这些贴片有效地误导对象检测系统，同时确定场景中上下文适当的部署。我们提出了魔术（在上下文中掌握物理对抗性的掌握），这是一个由多模式LLM代理提供动力的新型框架，以应对这些挑战。魔术会自动理解场景上下文，并通过语言和视觉功能的协同互动来生成对抗性补丁。特别是，魔术协调三个专业的LLM代理：Adv-Patch生成代理（Gagent）通过战略性及时工程进行文本到图像模型来掌握欺骗性补丁。 Adv-Patch部署代理（DAGENT）通过根据场景理解确定最佳部署策略来确保上下文连贯性。自我检查代理（EAGENT）通过对这两个过程进行批判性的监督和迭代性完善来完成这一三部曲。我们在数字和物理层面上验证了我们的方法，即nuimage和手动捕获的现实场景，在该场景中，统计和视觉结果都证明了我们的魔术对于攻击广泛应用的对象检测系统（即Yolo和Detr系列）具有强大的有效性。

### 3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark 
[[arxiv](https://arxiv.org/abs/2412.07825)] [[cool](https://papers.cool/arxiv/2412.07825)] [[pdf](https://arxiv.org/pdf/2412.07825)]
> **Authors**: Wufei Ma,Haoyu Chen,Guofeng Zhang,Celso M de Melo,Alan Yuille,Jieneng Chen
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: Project page: https://3dsrbench.github.io
- **标题**: 3DSRBENCH：全面的3D空间推理基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D空间推理是分析和解释3D空间内对象的位置，方向和空间关系的能力。这使模型能够对3D场景进行全面的了解，从而使其适用于更广泛的领域，例如自动导航，机器人技术和AR/VR。尽管大型多模型模型（LMM）在各种图像和视频理解任务中取得了显着的进步，但对各种自然图像进行3D空间推理的功能的研究较少。在这项工作中，我们介绍了第一个综合的3D空间推理基准3DSRBENCH，其中12种问题类型的2,772个手动注释的视觉问题回答。我们通过平衡数据分布并采用新颖的FlipeVal策略来对3D空间推理能力进行强有力，彻底的评估。进一步研究3D空间推理的鲁棒性W.R.T.相机3D观点，我们的3DSRBENCH包括两个子集，其中包含3D空间推理问题，上面有常见和不常见的观点的配对图像。我们基准了广泛的开源和专有的LMM，在3D意识的各个方面（例如高度，方向，位置和多对象推理）揭示了它们的局限性，以及它们在带有不常见的相机视图的图像上降低了性能。我们的3DSRBENCH提供了有关具有强大3D推理能力的LMM的未来发展的宝贵发现和见解。我们的项目页面和数据集可用https://3dsrbench.github.io。

### Learning to Correction: Explainable Feedback Generation for Visual Commonsense Reasoning Distractor 
[[arxiv](https://arxiv.org/abs/2412.07801)] [[cool](https://papers.cool/arxiv/2412.07801)] [[pdf](https://arxiv.org/pdf/2412.07801)]
> **Authors**: Jiali Chen,Xusen Hei,Yuqi Xue,Yuancheng Wei,Jiayuan Xie,Yi Cai,Qing Li
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-11
> **comment**: Accepted by ACM MM 2024
- **标题**: 学习纠正：可解释的反馈生成，用于视觉常识性推理干扰物
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 大型多模型模型（LMM）在视觉常识性推理（VCR）任务中表现出色，该任务旨在回答基于图像中视觉常识的多项选择问题。但是，LMM在出现时纠正干扰物中潜在的视觉常识错误的能力尚未探索。从人类老师如何挑战干扰者来测试学生对概念或技能的理解并帮助他们识别和纠正答案错误的灵感，我们是LMMS的开创性研究，以模拟此错误校正过程。为此，我们使用GPT-4作为``老师''来收集可解释的反馈数据集VCR-DF进行错误校正，这是评估LMMS识别误解并阐明VCR分散分散分心因素错误的原因的能力的基准。此外，我们提出了一个基于LMM的教学专家指导反馈生成（PEIFG）模型，以结合可学习的专家提示和多模式指导作为反馈生成的指导。实验结果表明，我们的PEIFG明显胜过现有的LMM。我们认为，我们的基准为评估LMM的功能提供了新的方向。

### BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities 
[[arxiv](https://arxiv.org/abs/2412.07769)] [[cool](https://papers.cool/arxiv/2412.07769)] [[pdf](https://arxiv.org/pdf/2412.07769)]
> **Authors**: Sahal Shaji Mullappilly,Mohammed Irfan Kurpath,Sara Pieri,Saeed Yahya Alseiari,Shanavas Cholakkal,Khaled Aldahmani,Fahad Khan,Rao Anwer,Salman Khan,Timothy Baldwin,Hisham Cholakkal
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: Bimedix2：生物医学专家LMM，用于多种医学方式
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了Bimedix2，这是一种双语（阿拉伯语）生物医学专家大型多模型模型（LMM），其统一体系结构可以整合文本和视觉方式，从而实现高级图像理解和医学应用。 BIMEDIX2利用Llama3.1体系结构并集成文本和视觉功能，以促进英语和阿拉伯语中的无缝互动，从而支持基于文本的输入以及涉及医学图像的多转交谈。该模型对广泛的双语医疗保健数据集进行了培训，该数据集由文本和图像模式的1600万个不同的医学互动样本组成，以阿拉伯语和英语混合。我们还提出了第一个基于双语的GPT-4O医疗LMM基准，名为Bimed-Mbench。 BIMEDIX2在基于文本的任务和基于图像的任务上都基准测试，从而在几种医疗基准中实现了最先进的性能。它的表现优于医疗LLM评估基准中的最新模型。我们的模型还为多模式医学评估设定了新的基准，英语评估的增长超过9％，阿拉伯语评估提高了20％。此外，它在上坡的事实准确性评估中超过了大约9％的GPT-4，并且在各种医学视觉问题答案，报告生成和报告摘要任务中都表现出色。项目页面，包括源代码和受过训练的模型，可在https://github.com/mbzuai-oryx/bimedix2上获得。

### SAT: Spatial Aptitude Training for Multimodal Language Models 
[[arxiv](https://arxiv.org/abs/2412.07755)] [[cool](https://papers.cool/arxiv/2412.07755)] [[pdf](https://arxiv.org/pdf/2412.07755)]
> **Authors**: Arijit Ray,Jiafei Duan,Reuben Tan,Dina Bashkirova,Rose Hendrix,Kiana Ehsani,Aniruddha Kembhavi,Bryan A. Plummer,Ranjay Krishna,Kuo-Hao Zeng,Kate Saenko
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: Project webpage: http://arijitray1993.github.io/SAT/
- **标题**: SAT：多模式模型的空间能力训练
- **领域**: 计算机视觉和模式识别,人工智能,图形,机器人技术
- **摘要**: 空间感知是智力的基本组成部分。尽管许多研究强调，大型多模式模型（MLM）努力推理空间，但它们仅测试静态空间推理，例如对物体的相对位置进行分类。同时，现实世界的部署需要动态功能，例如透视和以自我为中心的行动识别。作为改善空间智能的路线图，我们引入了SAT，空间才能训练，这超出了静态相对对象位置的问题，而不是动态的任务。 SAT包含218K的问题回答对，可在培训和测试集中使用22K合成场景。使用照片现实的物理引擎生成，我们的数据集可以任意缩放并易于扩展到新的动作，场景和3D资产。我们发现，即使在静态问题上表现良好的MLM也难以准确回答动态的空间问题。 Further, we show that SAT instruction-tuning data improves not only dynamic spatial reasoning on SAT, but also zero-shot performance on existing real-image spatial benchmarks: $23\%$ on CVBench, $8\%$ on the harder BLINK benchmark, and $18\%$ on VSR.当在SAT上调整指令时，我们的13B模型与空间推理中较大的专有MLM（如GPT4-V和Gemini-3-1.0）匹配。我们的数据/代码可在http://arijitray1993.github.io/sat/上获得。

### ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer 
[[arxiv](https://arxiv.org/abs/2412.07720)] [[cool](https://papers.cool/arxiv/2412.07720)] [[pdf](https://arxiv.org/pdf/2412.07720)]
> **Authors**: Jinyi Hu,Shengding Hu,Yuxuan Song,Yufei Huang,Mingxuan Wang,Hao Zhou,Zhiyuan Liu,Wei-Ying Ma,Maosong Sun
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: ACDIT：插值自回归有条件建模和扩散变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近对综合多模型模型的兴趣激增需要统一不同的方式。但是，统一患有不同的方法论。连续视觉生成需要完全基于基于扩散的方法，尽管它与文本域中的自回归建模有所不同。我们认为，自回归建模，即基于过去的确定性经验来预测未来，对于开发视觉生成模型和潜在的统一多模式模型仍然至关重要。在本文中，我们探讨了自回归建模与全参数扩散到建模视觉信息之间的插值。从本质上讲，我们提出了ACDIT，这是一种自回旋的有条件扩散变压器，其中扩散的块大小，即自回旋单元的大小，可以灵活地调整以插入令牌自动化和全序列扩散之间的插值。 ACDIT易于实现，就像在训练过程中创建跳过的可乐注意面罩（SCAM）一样简单。在推断期间，该过程在扩散denoising和自回归解码之间迭代，可以充分利用KV-CACHE。我们验证了ACDIT对图像和视频生成任务的有效性。我们还证明，尽管接受了扩散目标的训练，但ACDIT受益于自回旋建模，ACDIT可以无缝地使用。对自回归建模和扩散之间的权衡的分析表明，ACDIT的潜力可用于长途视觉生成任务。这些优势使其成为未来统一模型的骨干。

### GEXIA: Granularity Expansion and Iterative Approximation for Scalable Multi-grained Video-language Learning 
[[arxiv](https://arxiv.org/abs/2412.07704)] [[cool](https://papers.cool/arxiv/2412.07704)] [[pdf](https://arxiv.org/pdf/2412.07704)]
> **Authors**: Yicheng Wang,Zhikang Zhang,Jue Wang,David Fan,Zhenlin Xu,Linda Liu,Xiang Hao,Vimal Bhat,Xinyu Li
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: GEXIA：可扩展多透明视频学习的粒度扩展和迭代近似
- **领域**: 计算机视觉和模式识别
- **摘要**: 在各种视频学习任务中，实现多元数据的交叉模式对齐的挑战仍然存在。我们提出了一种从两个关键角度来应对这一挑战的方法：数据和建模。鉴于没有多层次的视频预处理数据集，我们引入了使用集成和压缩操作的粒度扩展（GEX）方法，以扩大单个粒度数据集的粒度。为了更好地建模多透明数据，我们引入了一个迭代近似模块（IAM），该模块（IAM）将多透明的视频和文本嵌入到一个统一的，低维的语义空间中，同时保留了跨模式对齐的基本信息。此外，GEXIA高度可扩展，没有对视频文本粒度数量对齐的限制。我们在七个基准数据集中评估了三类视频任务的工作，展示了最新的或可比的性能。值得注意的是，我们的模型在涉及长期视频理解的任务中表现出色，即使预处理数据集仅包含简短的视频剪辑。

### DriveMM: All-in-One Large Multimodal Model for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2412.07689)] [[cool](https://papers.cool/arxiv/2412.07689)] [[pdf](https://arxiv.org/pdf/2412.07689)]
> **Authors**: Zhijian Huang,Chengjian Feng,Feng Yan,Baihui Xiao,Zequn Jie,Yujie Zhong,Xiaodan Liang,Lin Ma
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 驱动器：自动驾驶的多模型多模型
- **领域**: 计算机视觉和模式识别,多媒体,机器人技术
- **摘要**: 大型多模型模型（LMM）通过合并大型语言模型表明了自主驾驶（AD）中出色的理解和解释能力。尽管取得了进步，但当前的数据驱动AD方法倾向于集中于单个数据集和特定任务，从而忽略了它们的整体能力和概括能力。为了弥合这些差距，我们提出了Drivemm，这是一种旨在处理各种数据输入（例如图像和多视图视频）的通用大型多模型模型，同时执行广泛的AD任务，包括感知，预测和计划。最初，该模型经过课程预训练以处理各种的视觉信号并执行基本的视觉理解和感知任务。随后，我们扩展并标准化了各种与AD相关的数据集，以微调模型，从而导致自动驾驶的多合一LMM。为了评估一般能力和概括能力，我们对六个公共基准进行评估，并在看不见的数据集上进行零射击转移，在该数据集中，Drivemm在所有任务中都实现了最先进的绩效。我们希望Drivemm作为现实世界中未来端到端自动驾驶应用程序的有前途的解决方案。带代码的项目页面：https：//github.com/zhijian11/drivemm。

### OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations 
[[arxiv](https://arxiv.org/abs/2412.07626)] [[cool](https://papers.cool/arxiv/2412.07626)] [[pdf](https://arxiv.org/pdf/2412.07626)]
> **Authors**: Linke Ouyang,Yuan Qu,Hongbin Zhou,Jiawei Zhu,Rui Zhang,Qunshu Lin,Bin Wang,Zhiyuan Zhao,Man Jiang,Xiaomeng Zhao,Jin Shi,Fan Wu,Pei Chu,Minghao Liu,Zhenxiang Li,Chao Xu,Bo Zhang,Botian Shi,Zhongying Tu,Conghui He
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: Omnidocbench：通过全面注释进行基准分析多样的PDF文档解析
- **领域**: 计算机视觉和模式识别,人工智能,信息检索
- **摘要**: 文档内容提取在计算机视觉中至关重要，尤其是满足大语言模型（LLM）和检索增强生成（RAG）技术的高质量数据需求。但是，当前的文件解析方法在多样性和全面评估方面受到了重大局限性。为了应对这些挑战，我们引入了Omnidocbench，这是一种新型的多源基准测试，旨在推动自动化文档内容提取。 Omnidocbench包括精心策划和注释的高质量评估数据集，其中包括九种不同的文档类型，例如学术论文，教科书，幻灯片等。我们的基准测试提供了一个灵活而全面的评估框架，其中有19个布局类别标签和14个属性标签，可在整个数据集，单个模块或特定数据类型上启用多层评估。使用Omnidocbench，我们对现有模块化管道和多模式端到端方法进行详尽的比较分析，强调了它们在处理文档多样性并确保公平评估时的局限性。 Omnidocbench为文档内容提取字段建立了强大，多样化和公平的评估标准，为未来的进步提供了重要的见解，并促进了文档解析技术的开发。代码和数据集可在https://github.com/opendatalab/omnidocbench中找到。

### PVP: Polar Representation Boost for 3D Semantic Occupancy Prediction 
[[arxiv](https://arxiv.org/abs/2412.07616)] [[cool](https://papers.cool/arxiv/2412.07616)] [[pdf](https://arxiv.org/pdf/2412.07616)]
> **Authors**: Yujing Xue,Jiaxiang Liu,Jiawei Du,Joey Tianyi Zhou
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: PVP：3D语义占用预测的极性表示
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，基于极地坐标的表示显示了3D感知任务的希望。与笛卡尔方法相比，极地网格提供了可行的替代方案，在覆盖较大区域的同时，在附近的空间中提供了更好的细节保存。但是，由于不均匀的划分，它们面临特征失真。为了解决这些问题，我们介绍了极地体素占用预测变量（PVP），这是一种在极坐标中运行的新型3D多模式预测指标。 PVP具有两个关键的设计元素以克服失真：全局代表传播（GRP）模块，将全局空间数据集成到3D卷中，而平面分解卷积（PD-CONV）将3D变形简化为2D卷积。这些创新使PVP能够胜过现有的方法，从而在OpenOccupancy数据集上实现了MIOU和IOU指标的显着改善。

### DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation 
[[arxiv](https://arxiv.org/abs/2412.07589)] [[cool](https://papers.cool/arxiv/2412.07589)] [[pdf](https://arxiv.org/pdf/2412.07589)]
> **Authors**: Jianzong Wu,Chao Tang,Jingbo Wang,Yanhong Zeng,Xiangtai Li,Yunhai Tong
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: The project page is https://jianzongwu.github.io/projects/diffsensei/
- **标题**: DIFFSENSEI：桥接自定义漫画生成的多模式LLM和扩散模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 故事可视化是从文本描述中创建视觉叙事的任务，它通过文本到图像生成模型的进展。但是，这些模型通常缺乏对角色外观和相互作用的有效控制，尤其是在多个特定场景中。为了解决这些限制，我们提出了一个新任务：\ textbf {自定义漫画生成}并引入\ textbf {diffsensei}，这是一个专门设计的，专门设计用于具有动态多字符控制的漫画。 DIFFSENSEI将基于扩散的图像发生器与多模式大型语言模型（MLLM）集成在一起，该模型充当了与文本兼容的身份适配器。我们的方法采用蒙版的交叉注意力，无缝地包含特征，实现无直接像素转移的精确布局控制。此外，基于MLLM的适配器调整了字符特征，以与面板特定的文本提示保持一致，从而可以灵活地调整角色表达式，姿势和动作。我们还介绍了\ textbf {mangazero}，这是一个针对此任务量身定制的大规模数据集，包含43,264个漫画页和427,147个注释的面板，支持可视化跨顺序帧的各种字符交互和运动的可视化。广泛的实验表明，DIFFSENSEI优于现有模型，通过启用文本适应的字符自定义标志着漫画生成的显着进步。项目页面是https://jianzongwu.github.io/projects/diffsensei/。

### Multimodal Contextualized Support for Enhancing Video Retrieval System 
[[arxiv](https://arxiv.org/abs/2412.07584)] [[cool](https://papers.cool/arxiv/2412.07584)] [[pdf](https://arxiv.org/pdf/2412.07584)]
> **Authors**: Quoc-Bao Nguyen-Le,Thanh-Huy Le-Nguyen
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: 9 pages, 4 figures
- **标题**: 多模式上下文化支持以增强视频检索系统
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 当前的视频检索系统，尤其是在竞赛中使用的系统，主要集中于查询单个关键帧或图像，而不是编码整个剪辑或视频段。但是，查询通常会描述一系列帧而不是特定图像的动作或事件。在分析单个帧时，这会导致信息不足，从而导致不准确的查询结果。此外，仅从图像（关键框架）中提取嵌入式的嵌入方式没有足够的信息来编码从视频中推断出的更高级别，更抽象的见解。这些模型倾向于仅描述框架中存在的对象，缺乏更深入的理解。在这项工作中，我们提出了一个集成了最新方法的系统，引入了一种提取多模式数据的新型管道，并将来自多个框架的信息包含在视频中，从而使模型能够抽象捕获潜在含义的高级信息，重点是从视频剪辑中推断出的内容，而不是仅仅从一个单个图像中进行对象检测。

### Hallucination Elimination and Semantic Enhancement Framework for Vision-Language Models in Traffic Scenarios 
[[arxiv](https://arxiv.org/abs/2412.07518)] [[cool](https://papers.cool/arxiv/2412.07518)] [[pdf](https://arxiv.org/pdf/2412.07518)]
> **Authors**: Jiaqi Fan,Jianhua Wu,Hongqing Chu,Quanbo Ge,Bingzhao Gao
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 在交通情况下，消除幻觉消除和语义增强框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉模型（LVLM）在多模式理解和发电任务中表现出了显着的功能。但是，这些模型偶尔会产生幻觉文本，从而导致描述似乎合理但与图像不符。这种现象可能导致自主驾驶系统的错误驾驶决策。为了应对这一挑战，本文提出了HCOENET，这是一种插件链纠正方法，旨在消除对象幻觉并为初始响应中忽略的关键对象生成增强的描述。具体而言，HCOENET采用交叉检查机制来过滤实体并直接从给定图像中提取关键对象，从而丰富描述性文本。教皇基准的实验结果表明，HCOENET分别提高了Mini-InternVL-4B和Mplug-Owl3模型的F1得分，分别提高了12.58％和4.28％。此外，使用在公开校园中收集的图像的定性结果进一步突出了所提出方法的实际适用性。与GPT-4O模型相比，HCOENET可实现可比的描述性能，同时显着降低了成本。最后，为流量方案创建了两个新颖的语义理解数据集，Coda_DESC和Nuscenes_desc，以支持未来的研究。这些代码和数据集可在https://github.com/fjq-tongji/hcoenet上公开获得。

### CoMA: Compositional Human Motion Generation with Multi-modal Agents 
[[arxiv](https://arxiv.org/abs/2412.07320)] [[cool](https://papers.cool/arxiv/2412.07320)] [[pdf](https://arxiv.org/pdf/2412.07320)]
> **Authors**: Shanlin Sun,Gabriel De Araujo,Jiaqi Xu,Shenghan Zhou,Hanwen Zhang,Ziheng Huang,Chenyu You,Xiaohui Xie
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: Project Page: https://gabrie-l.github.io/coma-page/
- **标题**: 昏迷：多模式代理的组成人类运动产生
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，3D人类运动的产生已取得了长足的进步。尽管最先进的方法已经显着提高了性能，但他们仍然在培训数据中看不见的复杂和详细的动议挣扎，这在很大程度上是由于运动数据集的稀缺性以及产生新培训示例的高昂成本。为了应对这些挑战，我们介绍了昏迷，这是一种基于代理的解决方案，用于复杂的人类运动，编辑和理解。昏迷利用多个由大语言和视觉模型提供动力的协作代理，以及具有掩码变压器的运动生成器，具有特定于身体的编码器和代码簿，以进行细粒度控制。我们的框架可以通过详细的说明，文本引导的运动编辑以及自我纠正来生成短运动序列，以提高质量。对HumanML3D数据集的评估表明了针对最新方法的竞争性能。此外，我们创建了一组上下文，构图和长文本提示，用户研究表明，我们的方法明显优于现有方法。

### Driving with InternVL: Oustanding Champion in the Track on Driving with Language of the Autonomous Grand Challenge at CVPR 2024 
[[arxiv](https://arxiv.org/abs/2412.07247)] [[cool](https://papers.cool/arxiv/2412.07247)] [[pdf](https://arxiv.org/pdf/2412.07247)]
> **Authors**: Jiahan Li,Zhiqi Li,Tong Lu
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 与Internvl：Oustanding Champion一起驾驶在CVPR 2024上的自主挑战赛语言中的赛道上
- **领域**: 计算机视觉和模式识别
- **摘要**: 该技术报告描述了我们使用CVPR 2024 Autonomous Grand Challenge的语言跟踪使用的方法。我们利用了强大的开源多模式InternVL-1.5，并在竞争数据集Drivelm-Nuscenes上进行了全参数微调。为了有效地处理Nuscenes和无缝继承Intervl出色的多模式理解功能的多视图图像，我们以特定方式格式化和串联了多视图图像。这确保了最终模型可以满足竞争任务的特定要求，同时利用Intervl强大的图像理解能力。同时，我们设计了一种简单的自动注释策略，该策略将Drivelm-Nuscenes中对象的中心点转换为相应的边界框。结果，我们的单个模型在最终领先地板上的得分为0.6002。

### Repetitive Action Counting with Hybrid Temporal Relation Modeling 
[[arxiv](https://arxiv.org/abs/2412.07233)] [[cool](https://papers.cool/arxiv/2412.07233)] [[pdf](https://arxiv.org/pdf/2412.07233)]
> **Authors**: Kun Li,Xinge Peng,Dan Guo,Xun Yang,Meng Wang
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: To be published in IEEE Transactions on Multimedia
- **标题**: 用混合时间关系建模进行重复的动作计数
- **领域**: 计算机视觉和模式识别
- **摘要**: 重复的动作计数（RAC）旨在计算视频中发生的重复动作的数量。在现实世界中，重复的行动具有巨大的多样性，并带来了许多挑战（例如，观点变化，不统一的时期和行动中断）。现有基于RAC的时间自相似性矩阵（TSSM）的现有方法被困在不足的捕获动作期间，当应用于复杂的每日视频时。为了解决此问题，我们提出了一种名为Hybrid Perimal关系建模网络（HTRM-NET）的新型方法，以构建RAC的多样化TSSM。 HTRM-NET主要由三个关键组成部分组成：双模式时间自相似性矩阵建模，随机矩阵掉落和局部时间上下文建模。具体而言，我们通过双模式（自我注意事项和双 - 柔软）操作构建时间自相似性矩阵，从而从行和列的相关性的组合中产生了不同的矩阵表示。为了进一步增强矩阵表示形式，我们建议将随机矩阵掉落模块合并，以明确指导矩阵的通道学习。之后，我们将视频帧的局部时间上下文和学习矩阵注入时间相关建模，这可以使模型足够强大，以应对容易出错的情况，例如动作中断。最后，多尺度矩阵融合模块设计为在多尺度矩阵中自适应地汇总时间相关性。跨跨数据库和跨数据库的广泛实验表明，所提出的方法不仅胜过当前的最新方法，而且还表现出强大的能力，可以准确地计算出未见动作类别的重复性动作。值得注意的是，我们的方法在MAE中超过20.04 \％，而OBO中的22.76 \％。

### GaGA: Towards Interactive Global Geolocation Assistant 
[[arxiv](https://arxiv.org/abs/2412.08907)] [[cool](https://papers.cool/arxiv/2412.08907)] [[pdf](https://arxiv.org/pdf/2412.08907)]
> **Authors**: Zhiyang Dou,Zipeng Wang,Xumeng Han,Chenhui Qiang,Kuiran Wang,Guorong Li,Zhibei Huang,Zhenjun Han
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: GAGA：迈向互动全球地理位置助理
- **领域**: 计算机视觉和模式识别
- **摘要**: 全球地理位置旨在预测世界上任何地方捕获的图像的地理位置，是计算机视觉领域中最具挑战性的任务之一。在本文中，我们介绍了一个创新的互动全球地理分配助理，名为Gaga，该助理构建在蓬勃发展的大型视觉模型（LVLMS）上。 Gaga发现了图像中的地理线索，并将它们与LVLM中嵌入的广泛的世界知识结合在一起，以确定地理位置，同时还为预测结果提供了理由和解释。我们进一步设计了一种新型的交互式地理位置方法，该方法超过了传统的静态推理方法。它允许用户干预，纠正或为预测提供线索，从而使模型更加灵活和实用。 GAGA的开发依赖于新提出的多模式全球地理位置（MG-GEO）数据集，这是500万个高质量图像文本对的全面集合。 Gaga在GWS15K数据集上实现了最先进的性能，在该国级别提高了准确性4.57％，在城市级别提高了2.92％，从而树立了新的基准。这些进步代表了具有全球适用性的高度准确，交互式地理位置系统的重大飞跃。

### LLaVA-Zip: Adaptive Visual Token Compression with Intrinsic Image Information 
[[arxiv](https://arxiv.org/abs/2412.08771)] [[cool](https://papers.cool/arxiv/2412.08771)] [[pdf](https://arxiv.org/pdf/2412.08771)]
> **Authors**: Ke Wang,Hong Xuan
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: llava-zip：带有内在图像信息的自适应视觉令牌压缩
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 使用指令遵守数据（例如LLAVA）的多模式大语模型（MLLM）在行业中取得了巨大进展。这些模型的一个主要限制是，在大型语言模型（LLMS）中，视觉令牌消耗了最大令牌极限的很大一部分，从而导致计算需求增加，并且当提示包括多个图像或视频时，性能下降。行业解决方案通常通过增加计算能力来减轻此问题，但是在资源有限的学术环境中，这种方法不太可行。在这项研究中，我们提出了基于LLAVA-1.5的动态特征图减少（DFMR），以应对视觉令牌过载的挑战。 DFMR动态压缩视觉令牌，释放令牌容量。 Our experimental results demonstrate that integrating DFMR into LLaVA-1.5 significantly improves the performance of LLaVA in varied visual token lengths, offering a promising solution for extending LLaVA to handle multi-image and video scenarios in resource-constrained academic environments and it can also be applied in industry settings for data augmentation to help mitigate the scarcity of open-domain image-text pair datasets in the continued pretraining 阶段。

### Beyond Knowledge Silos: Task Fingerprinting for Democratization of Medical Imaging AI 
[[arxiv](https://arxiv.org/abs/2412.08763)] [[cool](https://papers.cool/arxiv/2412.08763)] [[pdf](https://arxiv.org/pdf/2412.08763)]
> **Authors**: Patrick Godau,Akriti Srivastava,Tim Adler,Lena Maier-Hein
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 超越知识孤岛：医学成像民主化的任务指纹AI
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 医学成像AI的领域目前正在进行快速转化，有条理的研究越来越多地转化为临床实践。尽管取得了这些成功，但研究仍遭受了知识孤岛，阻碍协作和进步：现有知识分散在出版物之间，许多细节仍未出版，而隐私法规则限制了数据共享。本着AI民主化的精神，我们提出了一个在医学图像分析领域安全知识转移的框架。我们方法的关键是数据集“指纹”，即特征分布的结构化表示，可以量化任务相似性。我们通过转移神经体系结构，预处理，增强策略和多任务学习来测试71项不同的任务和12种医学成像方式。根据全面的分析，我们的方法优于识别相关知识并促进协作模型培训的传统方法。我们的框架促进了医学成像中AI的民主化，并可能成为促进更快的科学进步的宝贵工具。

### Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions 
[[arxiv](https://arxiv.org/abs/2412.08737)] [[cool](https://papers.cool/arxiv/2412.08737)] [[pdf](https://arxiv.org/pdf/2412.08737)]
> **Authors**: Jiarui Zhang,Ollie Liu,Tianyu Yu,Jinyi Hu,Willie Neiswanger
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: 33 pages, 22 figures, 5 tables, 7 algorithms
- **标题**: 欧几里得：带有合成高保真视觉描述的增压多模式LLM
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 近年来，多模式大型语言模型（MLLM）取得了迅速的进步，但仍在低级视觉感知（LLVP）中继续挣扎 - 尤其是能够准确描述图像的几何细节的能力。该功能对于机器人技术，医学图像分析和制造等领域的应用至关重要。在本文中，我们首先引入Geoperception，这是一种基准测试，旨在评估MLLM从图像中准确转录2D几何信息的能力。使用此基准测试，我们证明了领先的MLLM的局限性，然后进行全面的经验研究，以探索改善其在几何任务上的绩效的策略。我们的发现突出了某些模型体系结构，培训技术和数据策略的好处，包括使用高保真综合数据和使用数据课程的多阶段培训。值得注意的是，我们发现数据课程使模型能够学习挑战性的几何学理解他们无法从头开始学习的任务。利用这些见解，我们开发了欧几里得，这是一个专门针对强大低级几何感知的模型家族。尽管纯粹对合成多模式数据进行了训练，但欧几里得表现出强大的泛化能力，可以进行新颖的几何形状。例如，Euclid在某些Geoperception基准任务上优于最佳的封闭源模型Gemini-1.5-Pro，在所有任务中平均高达58.56％，平均为10.65％。

### StreamChat: Chatting with Streaming Video 
[[arxiv](https://arxiv.org/abs/2412.08646)] [[cool](https://papers.cool/arxiv/2412.08646)] [[pdf](https://arxiv.org/pdf/2412.08646)]
> **Authors**: Jihao Liu,Zhiding Yu,Shiyi Lan,Shihao Wang,Rongyao Fang,Jan Kautz,Hongsheng Li,Jose M. Alvare
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: StreamChat：与流视频聊天
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了Streamchat，这是一种新颖的方法，可增强大型多模型（LMM）与流视频内容的相互作用功能。在流相互作用方案中，现有方法仅依赖于提出问题时可用的可视信息，因为该模型仍未意识到流视频的后续更改，从而导致了重大延迟。 StreamChat通过在每个解码步骤中更新视觉上下文来解决此限制，从而确保模型在整个解码过程中都利用了最新的视频内容。此外，我们引入了一种灵活，有效的基于交叉说法的体系结构，以处理动态流输入，同时维持流式相互作用的推理效率。此外，我们构建了一个新的密集指令数据集，以促进流相互作用模型的训练，并以平行的3D旋转机制进行补充，该机制编码了视觉和文本令牌的相对时间信息。实验结果表明，与最先进的视频LMM相比，StreamChat在既定的图像和视频基准测试中实现了竞争性能，并且在流相互作用方案中表现出了较高的功能。

### A Dual-Module Denoising Approach with Curriculum Learning for Enhancing Multimodal Aspect-Based Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2412.08489)] [[cool](https://papers.cool/arxiv/2412.08489)] [[pdf](https://arxiv.org/pdf/2412.08489)]
> **Authors**: Nguyen Van Doan,Dat Tran Nguyen,Cam-Van Thi Nguyen
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: Accepted at PACLIC 2024
- **标题**: 通过课程学习来增强多模式的情感分析的双模型denoising方法
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 基于多模式方面的情感分析（MABSA）结合了文本和图像来执行情感分析，但通常与无关或误导性的视觉信息进行斗争。现有的方法通常会解决句子图像denoising或方面形象降解，但无法全面处理两种类型的噪声。为了解决这些局限性，我们提出了Dualde，这是一种新的方法，其中包括两个不同的组成部分：混合课程DeNoising模块（HCD）和方面 -  Enhance DeNoising模块（AED）。 HCD模块通过合并灵活的课程学习策略来增强句子图像降级，该策略优先考虑清洁数据的培训。同时，AED模块通过一种侧面引导的注意机制来减轻方面形象 - 图像噪声，该机制滤除了与感兴趣的特定方面无关的嘈杂的视觉区域。我们的方法证明了在解决句子图像和方面形象噪声方面的有效性，这是通过基准数据集的实验评估所证明的。

### Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification 
[[arxiv](https://arxiv.org/abs/2412.08406)] [[cool](https://papers.cool/arxiv/2412.08406)] [[pdf](https://arxiv.org/pdf/2412.08406)]
> **Authors**: Neng Dong,Shuanglin Yan,Liyan Zhang,Jinhui Tang
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 可见的红外人重新识别的嵌入和丰富明确的语义
- **领域**: 计算机视觉和模式识别
- **摘要**: 可见的红外人员重新识别（VireID）检索了跨不同方式具有相同身份的行人图像。现有方法仅从图像中学习视觉内容，缺乏感知高级语义的能力。在本文中，我们提出了一个嵌入和丰富的显式语义（EEES）框架，以学习语义上丰富的跨模式行人表示。我们的方法提供了一些贡献。首先，通过多种大型语言视觉模型的合作，我们开发了显式的语义嵌入（ESE），该语义嵌入（ESE）会自动为行人提供语言描述，并将图像文本对对齐为公共空间，从而学习与显式语义相关的视觉内容。其次，认识到多视图信息的互补性，我们提出了跨视图赔偿（CVSC），该补偿（CVSC）构建了多视图图像 - 文本对表示，建立了他们的多对多匹配，并将知识传播到单视图表示，从而凭借其缺失的交叉视频范围的视觉内容来补偿其视觉内容。第三，为了消除诸如不同模态的嘈杂语义（例如相互冲突的颜色属性），我们设计了跨模式语义纯化（CMSP），该语义限制了模式间图像型文本对对之间的距离，以至于接近模式内模态图像文本对之间的距离，从而进一步增强了视觉内容的模式不变性。最后，实验结果证明了所提出的EEE的有效性和优势。

### HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2412.08378)] [[cool](https://papers.cool/arxiv/2412.08378)] [[pdf](https://arxiv.org/pdf/2412.08378)]
> **Authors**: Shiding Zhu,Wenhui Dong,Jun Song,Yingbo Wang,Yanan Guo,Bo Zheng
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: 11 pages, 4 figures
- **标题**: HYVILM：使用视觉模型的混合编码器增强细粒度识别
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近，人们对多模式大语言模型（MLLM）的能力越来越感兴趣。目前，一种常见的方法涉及将原始的高分辨率图像动态裁剪成较小的子图像，然后将其馈送到一个在低分辨率图像上预先训练的视觉编码器中。但是，这种种植方法通常会截断原始图像中的对象和连接区域，从而导致语义中断。为了解决此限制，我们介绍了Hyvilm，该Hyvilm旨在处理任何分辨率的图像，同时在编码过程中保留整体上下文。具体来说，我们：（i）设计一个名为Hybrid编码器的新型视觉编码器，该编码不仅编码单个子图像，而且还与详细的全局视觉特征进行交互，从而显着提高了该模型编码高分辨率图像的能力。 （ii）为动态裁剪方法提出了一种最佳特征融合策略，从而有效利用了视觉编码器不同层的信息。与最先进的MLLM在同一设置下相比，我们的Hyvilm在十分之九的任务中的表现优于现有的MLLM。具体而言，Hyvilm在TextVQA任务上的性能提高了9.6％，并且在DOCVQA任务上提高了6.9％。

### Lightweight Method for Interactive 3D Medical Image Segmentation with Multi-Round Result Fusion 
[[arxiv](https://arxiv.org/abs/2412.08315)] [[cool](https://papers.cool/arxiv/2412.08315)] [[pdf](https://arxiv.org/pdf/2412.08315)]
> **Authors**: Bingzhi Shen,Lufan Chang,Siqi Chen,Shuxiang Guo,Hao Liu
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 用于交互式3D医疗图像分割的轻量级方法与多轮结果融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 在医学成像中，通常需要对病变或器官进行精确注释。但是，3D体积图像通常由数百或数千个切片组成，使注释过程非常耗时和费力。最近，由于其在交互式分割中出色的零弹性概括能力，该细分都引起了任何模型（SAM）的广泛关注。尽管研究人员已经探索了将SAM适应医疗应用程序（例如使用SAM适配器或构建3D SAM模型），但仍然存在一个关键问题：传统的CNN网络可以在此任务中实现相同的强零弹性概括吗？在本文中，我们提出了用于3D医疗图像分割（Lim-NET）的轻型交互式网络，这是一种新型方法，证明了紧凑型CNN模型的潜力。 Lim-Net建立在2D CNN主链的基础上，通过从用户提示中生成2D提示掩码来启动细分。然后通过内存模块通过3D序列传播此掩码。为了完善和稳定相互作用的结果，多轮结果融合（MRF）模块选择并合并了多个回合的最佳掩模。我们在多个数据集中进行的广泛实验表明了Lim-Net的竞争性能。与基于SAM的模型相比，它表现出对看不见数据的更强概括，具有竞争力的精度，同时需要更少的相互作用。值得注意的是，Lim-Net的轻量级设计在部署和推理效率方面具有很大的优势，而GPU存储器消耗较低，适用于资源受限的环境。这些有前途的结果表明，lim-net可以用作强大的基线，与流行的SAM模型进行补充和对比，以进一步增强有效的互动医学图像分割。该代码将在\ url {https://github.com/goodtime-123/lim-net}发布。

### Template Matters: Understanding the Role of Instruction Templates in Multimodal Language Model Evaluation and Training 
[[arxiv](https://arxiv.org/abs/2412.08307)] [[cool](https://papers.cool/arxiv/2412.08307)] [[pdf](https://arxiv.org/pdf/2412.08307)]
> **Authors**: Shijian Wang,Linxin Song,Jieyu Zhang,Ryotaro Shimizu,Ao Luo,Li Yao,Cunjian Chen,Julian McAuley,Hanqian Wu
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: Code: https://github.com/shijian2001/TemplateMatters
- **标题**: 模板事项：了解教学模板在多模式语言模型评估和培训中的作用
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的多模式模型（MLMS）评估和培训方法忽略了教学格式的影响，呈现出室内的大象问题。先前的研究通过手动制定指令来解决此问题，由于多样性和可伸缩性的限制，无法产生重大见解。在这项工作中，我们建议通过将随机采样的位置同义词填充到加权采样的元模板中，使我们能够全面地检查MLM在不同的指令模板上，使我们能够全面地检查MLM的性能，从而使能够产生超过39b唯一模板组合的程序化模板生成器。我们在五个基准数据集上的八个常见MLMS进行的实验表明，MLM具有较高的模板敏感性，不同模板之间的性能差距最多为29％。我们通过模板生成器进一步增强LLAVA-1.5的指令调整数据集，并在LLAVA-1.5-7B和LLLAVA-1.5-13B上执行指令调整。与我们增强数据集的相同规模的MLM相比，在我们的增强数据集中调整的模型达到了最佳的整体性能，最多是我们增强数据集的规模的75倍，强调了指导模板在MLM培训中的重要性。该代码可从https://github.com/shijian2001/templatematters获得。

### Position-aware Guided Point Cloud Completion with CLIP Model 
[[arxiv](https://arxiv.org/abs/2412.08271)] [[cool](https://papers.cool/arxiv/2412.08271)] [[pdf](https://arxiv.org/pdf/2412.08271)]
> **Authors**: Feng Zhou,Qi Zhang,Ju Dai,Lei Li,Qing Fan,Junliang Xing
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: Accepted by AAAI25
- **标题**: 剪辑模型的位置引导点云完成
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 点云完成旨在恢复由设备缺陷或有限观点引起的部分几何形状和拓扑形状。当前方法要么仅依赖于点云的3D坐标来完成其完成，要么将其他图像与具有良好校准的固有参数结合在一起，以指导缺失零件的几何估计。尽管这些方法通过直接预测完整点的位置来实现出色的性能，但提取的特征缺乏有关缺失区域位置的细粒度信息。为了解决这个问题，我们提出了一种快速有效的方法，将单峰框架扩展到多模式框架。这种方法结合了一个旨在通过加权地图学习机制增强缺失零件的空间信息的位置感知模块。此外，我们基于现有的单峰点云完成数据集建立了一个点文本图像copci-ti和MVP-TI，并使用预训练的视觉模型模型剪辑为3D形状提供更丰富的详细信息，从而增强性能。广泛的定量和定性实验表明，我们的方法优于最先进的点云完成方法。

### Illusory VQA: Benchmarking and Enhancing Multimodal Models on Visual Illusions 
[[arxiv](https://arxiv.org/abs/2412.08169)] [[cool](https://papers.cool/arxiv/2412.08169)] [[pdf](https://arxiv.org/pdf/2412.08169)]
> **Authors**: Mohammadmostafa Rostamkhani,Baktash Ansari,Hoorieh Sabzevari,Farzan Rahmani,Sauleh Eetemadi
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 虚幻的VQA：视觉错觉上的基准测试和增强多模式模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 近年来，视觉问题回答（VQA）取得了长足的进步，尤其是在整合视觉和语言理解的多模型的出现中。但是，现有的VQA数据集经常忽略图像幻觉引入的复杂性，这对人类的感知和模型解释都构成了独特的挑战。在这项研究中，我们介绍了一项名为Infusory VQA的新颖任务，以及四个专业数据集：IllusionMnist，IllusionFashionMnist，IllusionAnimals和FillusionChar。这些数据集旨在评估最先进的多模型在识别和解释视觉错觉时的性能。我们评估了各种模型的零拍摄性能，在我们的数据集中进行了微调选定的模型，并提出了一种简单而有效的解决方案，用于使用高斯和模糊的低通滤波器进行幻觉检测。我们表明，这种方法显着提高了模型的性能，并且在幻觉上的blip-2中，没有任何微调，它的表现就超过了人类。我们的发现突出了人类和模型对幻觉感知之间的差异，并证明了微调和特定的预处理技术可以显着增强模型的鲁棒性。这项工作有助于在多模式模型中发展更类似人类的视觉理解，并提出了使用可学习参数调整过滤器的未来方向。

### A Review of Intelligent Device Fault Diagnosis Technologies Based on Machine Vision 
[[arxiv](https://arxiv.org/abs/2412.08148)] [[cool](https://papers.cool/arxiv/2412.08148)] [[pdf](https://arxiv.org/pdf/2412.08148)]
> **Authors**: Guiran Liu,Binrong Zhu
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: 9 pages, This paper has been accepted for publication at RICAI 2024
- **标题**: 基于机器视觉的智能设备故障诊断技术的综述
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 本文对机械设备故障诊断方法进行了全面的综述，重点介绍了基于变压器的模型所带来的进步。它详细介绍了变形金刚的结构，工作原理和好处，尤其是其自我注意的机制和平行计算功能，这些功能推动了其在自然语言处理和计算机视觉中的广泛应用。讨论突出了关键变压器模型变体，例如视觉变压器（VIT）及其扩展，这些变体利用自我来提高视觉任务的准确性和效率。此外，本文研究了基于变压器的方法在机械系统中的智能故障诊断中的应用，展示了它们从复杂传感器数据中提取和识别模式的卓越能力，以进行精确的故障识别。尽管取得了这些进步，但仍有挑战，包括依赖广泛标记的数据集，重大的计算需求以及在资源受限设备上部署模型时遇到的困难。为了解决这些局限性，本文提出了未来的研究方向，例如开发轻型变压器体系结构，整合多模式数据源，并增强对多样化的操作条件的适应性。这些努力旨在进一步扩展基于变压器的方法在机械故障诊断中的应用，从而使它们更加稳健，高效且适合于现实世界中的工业环境。

### Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2412.08125)] [[cool](https://papers.cool/arxiv/2412.08125)] [[pdf](https://arxiv.org/pdf/2412.08125)]
> **Authors**: Quang-Hung Le,Long Hoang Dang,Ngan Le,Truyen Tran,Thao Minh Le
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 在大型视觉模型中进行扎根推理的进行性多粒子对齐
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 现有的大型视觉模型（LVLM）在跨多模式输入的匹配概念方面表现出色，但与实体之间的组成概念和高级关系斗争。本文介绍了渐进式多粒性视觉对齐（Promvil），这是一个新型框架，旨在增强LVLMS执行接地的组成视觉推理任务的能力。我们的方法构建了多模式对齐的层次结构，从简单到复杂的概念。通过逐步将文本描述与相应的视觉区域保持一致，我们的模型学会了从较低级别的上下文信息来为更高级别的推理提供信息。为了促进这一学习过程，我们介绍了一个数据生成过程，该过程创建了一个来自视觉基因组的新型数据集，提供了广泛的嵌套组成视觉语言对。实验结果表明，我们的Promvil框架在各种视觉接地和构图问题回答任务上的表现明显优于基准。该代码可在以下网址提供：https：//github.com/lqh52/promvil。

### Seeing Syntax: Uncovering Syntactic Learning Limitations in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2412.08111)] [[cool](https://papers.cool/arxiv/2412.08111)] [[pdf](https://arxiv.org/pdf/2412.08111)]
> **Authors**: Sri Harsha Dumpala,David Arps,Sageev Oore,Laura Kallmeyer,Hassan Sajjad
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 查看语法：在视觉模型中发现句法学习限制
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 视觉语言模型（VLM）是多模式应用的基础模型，例如图像字幕和文本图像生成。最近的研究强调了VLM文本编码器中的局限性，特别是在构图和语义理解等领域，尽管这些局限性的根本原因尚不清楚。在这项工作中，我们旨在通过分析句法信息，这是由VLMS的文本编码编码的基本语言属性之一来解决这一差距。我们对具有不同目标函数，参数大小和训练数据大小以及单态语言模型（ULMS）进行比较的VLM进行彻底分析，以编码语法知识的能力。我们的发现表明，与VLMS中的ULM文本编码相比，ULM文本编码更有效地获取句法信息。 VLM文本编码者学到的句法信息主要是由训练预训练目标形成的，该目标比其他因素（例如模型体系结构，模型大小或预训练数据的体积）起着更为重要的作用。模型表现出不同的层面趋势，其中夹子性能在各个层上下降，而对于其他模型，中层层富含编码的句法知识。

### Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation 
[[arxiv](https://arxiv.org/abs/2412.08108)] [[cool](https://papers.cool/arxiv/2412.08108)] [[pdf](https://arxiv.org/pdf/2412.08108)]
> **Authors**: Hee-Seon Kim,Minbeom Kim,Changick Kim
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 双重逆向扰动：图像和文本的欺骗性视觉语言模型都带有单个扰动
- **领域**: 计算机视觉和模式识别,计算语言学,密码学和安全
- **摘要**: 大型视觉模型（VLM）通过将视觉编码器与大语言模型（LLMS）集成在一起，在多模式任务中表现出了显着的性能。但是，这些模型仍然容易受到对抗性攻击的影响。在此类攻击中，通用对抗扰动（UAP）特别强大，因为单个优化的扰动会误导各种输入图像的模型。在这项工作中，我们介绍了专门为VLMS设计的新型UAP：双重跨性扰动（Doubly-uap），能够在图像和文本输入中普遍欺骗VLM。为了成功破坏视觉编码器的基本过程，我们分析了注意机制的核心组成部分。在将中间层中的值向量识别为最脆弱的矢量之后，我们以冷冻模型以无标签的方式优化了双重UAP。尽管被开发为LLM的黑盒子，但双重UAP还是在VLM上达到了很高的攻击成功率，在视觉语言任务上始终优于基线方法。广泛的消融研究和分析进一步证明了双重UAP的鲁棒性，并提供了有关它如何影响内部注意机制的见解。

### Enhancing Multimodal Large Language Models Complex Reason via Similarity Computation 
[[arxiv](https://arxiv.org/abs/2412.09817)] [[cool](https://papers.cool/arxiv/2412.09817)] [[pdf](https://arxiv.org/pdf/2412.09817)]
> **Authors**: Xiaofeng Zhang,Fanshuo Zeng,Yihao Quan,Zheng Hui,Jiawei Yao
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: 通过相似性计算增强多模式大语模型复杂原因
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 多模式的大型语言模型已经快速增长，并且已经出现了许多不同的模型。 LVLMS的可解释性仍然是一个不足的区域。尤其是面对更复杂的任务，例如经过思考的推理，其内部机制仍然类似于很难破译的黑匣子。通过研究图像和文本之间的相互作用和信息流，我们注意到在诸如LLAVA1.5之类的模型中，与文本具有语义相关的图像令牌更可能在LLM解码层中具有信息流收敛，并且这些图像令牌获得了更高的注意力分数。但是，那些与文本无关的图像令牌没有信息流融合，并且它们的注意力分数很小。为了有效地利用图像信息，我们提出了一种新的图像令牌减少方法Simignore，该方法旨在通过计算图像和文本嵌入之间的相似性以及忽略与文本无关紧要的图像令牌来提高LVLM的复杂推理能力。通过广泛的实验，我们证明了方法对复杂推理任务的有效性。可以从\ url {https://github.com/fanshuozeng/simignore}访问本文的源代码。

### ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation 
[[arxiv](https://arxiv.org/abs/2412.09754)] [[cool](https://papers.cool/arxiv/2412.09754)] [[pdf](https://arxiv.org/pdf/2412.09754)]
> **Authors**: Ali Athar,Xueqing Deng,Liang-Chieh Chen
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: VICAS：用于使用字幕与接地分段结合整体和像素级视频理解的数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的最新进展扩大了视频理解的研究，主要关注高级任务，例如视频字幕和提问。同时，较小的工作范围解决了密集的Pixel-Presise分割任务，通常涉及类别引导或基于转介的对象分割。尽管这两个研究方向对于开发具有人级视频理解的模型至关重要，但它们在很大程度上分别演变为具有不同的基准和体系结构。本文旨在通过介绍VICA（一个包含数千个具有挑战性的视频的新数据集）来统一这些努力，每个数据集都带有详细的，人工编写的字幕，并在时间上保持一致的像素准确的掩码，用于多个具有短语接地的物体。我们的基准测试对整体/高级理解和语言引导，Pixel-presise细分的模型进行了评估。我们还提出了经过精心验证的评估措施，并提出了一个有效的模型架构，可以解决我们的基准。项目页面位于https://ali2500.github.io/vicas-project/

### Diffusion-Enhanced Test-time Adaptation with Text and Image Augmentation 
[[arxiv](https://arxiv.org/abs/2412.09706)] [[cool](https://papers.cool/arxiv/2412.09706)] [[pdf](https://arxiv.org/pdf/2412.09706)]
> **Authors**: Chun-Mei Feng,Yuanyang He,Jian Zou,Salman Khan,Huan Xiong,Zhen Li,Wangmeng Zuo,Rick Siow Mong Goh,Yong Liu
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Accepted by International Journal of Computer Vision
- **标题**: 扩散增强的测试时间适应文本和图像增强
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的测试时间提示调整（TPT）方法集中在单模式数据上，主要增强图像并使用置信度等级来过滤不准确的图像。但是，尽管图像生成模型可以产生视觉上的图像，但单模式数据增强技术仍然无法捕获不同方式提供的全面知识。此外，我们注意到，当增强图像的数量受到限制时，基于TPT的方法的性能会大大下降，考虑到生成增强的计算费用，这并不罕见。为了解决这些问题，我们介绍了一种新型的测试时间适应方法IT3A，该方法利用预先训练的生成模型将每个测试样品的多模式增强从未知的新域增加。通过结合预先训练的视觉和语言模型的增强数据，我们增强了模型适应未知新测试数据的能力。此外，为确保在生成各种视觉和文本增强功能时准确地保留关键语义，我们使用原始测试数据在增强图像和文本的逻辑之间使用余弦相似性过滤。这个过程使我们能够过滤一些虚假的增强和组合不足。为了利用生成模型跨不同模式提供的多种增强功能，我们用适配器替换了提示调整，以提高灵活性，以利用文本模板。我们在带有分布偏移和域差距的测试数据集上的实验表明，在零照片设置中，IT3A的表现优于最先进的测试时间及时调整方法，精度的提高了5.50％。

### Vision-Language Models Represent Darker-Skinned Black Individuals as More Homogeneous than Lighter-Skinned Black Individuals 
[[arxiv](https://arxiv.org/abs/2412.09668)] [[cool](https://papers.cool/arxiv/2412.09668)] [[pdf](https://arxiv.org/pdf/2412.09668)]
> **Authors**: Messi H. J. Lee,Soyeon Jeon
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: 视觉语言模型代表黑色的黑人个体，比较轻的黑人黑人更均匀
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLMS）将大型语言模型（LLM）功能与图像处理结合在一起，启用了诸如图像字幕和文本到图像生成之类的任务。然而，担心它们会扩大人类偏见的潜力，包括肤色偏见。肤色偏见，黑皮肤的人面临的刻板印象比较轻的人面临更大的陈规定型观念，在社会科学中有充分的文献记录，但在人工智能（AI）中仍未探索，尤其是在VLMS中。尽管在社会科学中有充分的文献记录，但这种偏见在AI中的探索仍然不足，尤其是在VLM中。使用GAN FACE数据库，我们对黑人男性和女性的计算机生成图像进行了采样，从而控制了肤色变化，同时使其他功能保持恒定。然后，我们要求VLM撰写有关这些面孔的故事，并比较产生的故事的同质性。 VLM在四种模型中的三个模型中，VLM产生的关于黑色黑人个体的黑人黑人比较轻的人更均匀，而黑人妇女的代表始终比所有模型中的黑人男性都更为同质。相互作用效应显示，肤色对两个VLM的女性产生了更大的影响，而另外两个则显示出不显着的结果，反映了已知的刻板印象模式。这些发现强调了从单模式AI系统到多模型模型的偏见的传播，并强调了需要进一步研究以解决AI中的偏见的必要性。

### From Noise to Nuance: Advances in Deep Generative Image Models 
[[arxiv](https://arxiv.org/abs/2412.09656)] [[cool](https://papers.cool/arxiv/2412.09656)] [[pdf](https://arxiv.org/pdf/2412.09656)]
> **Authors**: Benji Peng,Chia Xin Liang,Ziqian Bi,Ming Liu,Yichao Zhang,Tianyang Wang,Keyu Chen,Xinyuan Song,Pohsun Feng
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: 从噪音到细微差别：深度生成图像模型的进步
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 自2021年以来，基于深度学习的图像生成已经发生了范式转变，其标志是基本的建筑突破和计算创新。通过审查建筑创新和经验结果，本文分析了从传统生成方法到高级体系结构的过渡，重点是计算有效的扩散模型和视觉变压器体系结构。我们研究了稳定扩散，DALL-E和一致性模型的最新发展如何重新定义了图像合成的能力和性能界限，同时解决了效率和质量方面的持续挑战。我们的分析重点是潜在空间表示，跨注意机制和参数有效的培训方法，这些方法能够在资源约束下加速推断。尽管更有效的训练方法可以更快地推断，但高级控制机制（例如ControlNet和区域注意系统）同时改善了生成精度和内容自定义。我们调查了增强的多模式理解和零发电能力如何重塑整个行业的实际应用。我们的分析表明，尽管发电质量和计算效率取得了显着进步，但在开发具有资源意识的体系结构和可解释的工业应用程序系统方面仍然存在着关键的挑战。本文结束了绘制有希望的研究方向，包括神经结构优化和可解释的生成框架。

### Doe-1: Closed-Loop Autonomous Driving with Large World Model 
[[arxiv](https://arxiv.org/abs/2412.09627)] [[cool](https://papers.cool/arxiv/2412.09627)] [[pdf](https://arxiv.org/pdf/2412.09627)]
> **Authors**: Wenzhao Zheng,Zetian Xia,Yuanhui Huang,Sicheng Zuo,Jie Zhou,Jiwen Lu
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Code is available at: https://github.com/wzzheng/Doe
- **标题**: DOE-1：大世界模型的闭环自动驾驶
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 端到端的自主驾驶因其从大量数据中学习的潜力而受到了越来越多的关注。但是，大多数现有的方法仍然是开环，并且缺乏可扩展性，缺乏高级相互作用以及效率低下的决策。在本文中，我们探讨了一个用于自动驾驶的闭环框架，并提出了一个大型驾驶世界模型（DOE-1），以进行统一的感知，预测和计划。我们将自主驾驶作为下一代生成问题，并使用多模式令牌来完成不同的任务。具体而言，我们使用自由形式的文本（即场景描述）进行感知，并直接在带有图像令牌的RGB空间中生成未来的预测。为了计划，我们采用了一个感知的令牌仪来有效地将操作编码为离散令牌。我们以端到端和统一的方式训练多模式变压器以自动加注产生感知，预测和计划令牌。广泛使用的Nuscenes数据集上的实验证明了DOE-1在各种任务中的有效性，包括视觉提问，动作条件的视频生成和运动计划。代码：https：//github.com/wzzheng/doe。

### EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM 
[[arxiv](https://arxiv.org/abs/2412.09618)] [[cool](https://papers.cool/arxiv/2412.09618)] [[pdf](https://arxiv.org/pdf/2412.09618)]
> **Authors**: Zhuofan Zong,Dongzhi Jiang,Bingqi Ma,Guanglu Song,Hao Shao,Dazhong Shen,Yu Liu,Hongsheng Li
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Tech report
- **标题**: EasyRef：通过多模式LLM进行扩散模型的Omni Generate Group图像参考
- **领域**: 计算机视觉和模式识别
- **摘要**: 已经见证了扩散模型个性化的重大成就。传统的无调方法主要通过将其图像嵌入为注入条件来编码多个参考图像，但是这种独立于图像的操作无法在图像之间执行相互作用以捕获多个参考中的一致的视觉元素。尽管基于调整的低级适应（LORA）可以通过训练过程有效地在多个图像中提取一致的元素，但它需要为每个不同的图像组进行特定的登录。本文介绍了EasyRef，这是一种新颖的插件适应方法，它使扩散模型可以在多个参考图像和文本提示下进行调节。为了有效利用多个图像中一致的视觉元素，我们利用了多模式大语言模型（MLLM）的多图像理解和跟踪功能，促使其基于指令捕获一致的视觉元素。此外，通过适配器将MLLM的表示形式注入扩散过程中，可以轻松地概括为看不见的域，从而在看不见的数据中挖掘一致的视觉元素。为了减轻计算成本并增强细粒细节保存，我们介绍了有效的参考汇总策略和渐进培训计划。最后，我们介绍了MRBENCH，这是一种新的多参考图像生成基准。实验结果表明，EasyRef超过了无调的方法，例如IP-ADAPTER和基于调整的方法，例如Lora，可以达到跨不同领域的优质美学质量和稳健的零拍概括。

### V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding 
[[arxiv](https://arxiv.org/abs/2412.09616)] [[cool](https://papers.cool/arxiv/2412.09616)] [[pdf](https://arxiv.org/pdf/2412.09616)]
> **Authors**: Junqi Ge,Ziyi Chen,Jintao Lin,Jinguo Zhu,Xihui Liu,Jifeng Dai,Xizhou Zhu
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: The code and models will be available at https://github.com/OpenGVLab/V2PE
- **标题**: V2PE：改善具有可变视觉位置编码的视觉模型的多模式的长篇小说能力
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLMS）在处理各种多模式任务方面表现出了有希望的能力，但是它们在长篇文章的场景中挣扎，尤其是在涉及视频，高分辨率图像或冗长的图像文本文档的任务中。在我们的工作中，我们首先使用增强的长篇文化多模式数据集对VLM的长篇文化功能进行了经验分析。我们的发现表明，直接应用用于文本代币的位置编码机制为视觉令牌是次优的，当编码的位置编码超过模型的上下文窗口时，VLM性能急剧降低。为了解决这个问题，我们提出了可变的视觉位置编码（V2PE），这是一种新型的位置编码方法，该方法采用可变和较小的增量来实现视觉令牌，从而可以对长多模式序列进行更有效的管理。我们的实验证明了V2PE的有效性，增强了VLM在长期多模式环境中有效理解和推理的能力。我们进一步将V2PE与增强的长篇小说多模式数据集整合在一起，以微调开源VLM，InternVL2。微型模型在标准和长篇小说多模式任务上都达到了强劲的性能。值得注意的是，当训练数据集的序列长度增加到256K代币时，该模型能够处理最高1M令牌的多模式序列，从而突出了其现实世界中长篇小说应用程序的潜力。

### Olympus: A Universal Task Router for Computer Vision Tasks 
[[arxiv](https://arxiv.org/abs/2412.09612)] [[cool](https://papers.cool/arxiv/2412.09612)] [[pdf](https://arxiv.org/pdf/2412.09612)]
> **Authors**: Yuanze Lin,Yunsheng Li,Dongdong Chen,Weijian Xu,Ronald Clark,Philip H. S. Torr
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Technical Report
- **标题**: Olympus：用于计算机视觉任务的通用任务路由器
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 我们介绍了Olympus，这是一种新方法，将多模式大语言模型（MLLM）转换为一个能够处理各种计算机视觉任务的统一框架。利用控制器MLLM，Olympus代表在图像，视频和3D对象上超过20个专用任务，以实现专用模块。这种基于指令的路由可以通过链接的动作来实现复杂的工作流程，而无需训练重型生成模型。 Olympus可以轻松地与现有的MLLM集成，从而扩展其功能，并具有可比的性能。实验结果表明，奥林巴斯在20个任务中达到了94.75％的平均路由准确性，而精确度为91.82％，在链式动作方案中，其有效性是可以解决各种计算机视觉任务的通用任务路由器的有效性。项目页面：http：//yuanze-lin.me/olympus_page/

### SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding 
[[arxiv](https://arxiv.org/abs/2412.09604)] [[cool](https://papers.cool/arxiv/2412.09604)] [[pdf](https://arxiv.org/pdf/2412.09604)]
> **Authors**: Hao Li,Changyao Tian,Jie Shao,Xizhou Zhu,Zhaokai Wang,Jinguo Zhu,Wenhan Dou,Xiaogang Wang,Hongsheng Li,Lewei Lu,Jifeng Dai
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: Synergen-vl：与视觉专家和令牌折叠的协同图像理解和产生
- **领域**: 计算机视觉和模式识别
- **摘要**: 大语言模型（LLM）的显着成功已扩展到多模式领域，在图像理解和产生方面取得了出色的表现。开发整合这些功能的统一多模式大语模型（MLLM）的最新努力已显示出令人鼓舞的结果。但是，现有的方法通常涉及模型架构或训练管道中的复杂设计，从而增加了模型培训和扩展的困难。在本文中，我们提出了Synergen-Vl，这是一种能够理解和生成的简单而功能强大的无编码器MLLM。为了解决现有的无编码统一MLLM中确定的挑战，我们介绍了令牌折叠机制和基于视觉的渐进式一致性预处理策略，这些策略有效地支持了高分辨率图像的理解，同时降低了训练的复杂性。在接受具有统一的下一步预测目标的大规模混合图像文本数据培训之后，Synergen-VL实现或超过了具有可比或较小参数尺寸的现有无编码器的统一MLLM的性能，并通过特定于任务特定的目的是前部模型来缩小差距，从而突出了一个有希望的未来统一Mllms的途径。我们的代码和模型应发布。

### Do Multimodal Large Language Models See Like Humans? 
[[arxiv](https://arxiv.org/abs/2412.09603)] [[cool](https://papers.cool/arxiv/2412.09603)] [[pdf](https://arxiv.org/pdf/2412.09603)]
> **Authors**: Jiaying Lin,Shuquan Ye,Rynson W. H. Lau
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Project page: https://jiaying.link/HVSBench/
- **标题**: 多模式大语模型是否像人类一样？
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大型语言模型（MLLM）在各种视觉任务上取得了令人印象深刻的结果，利用了大型语言模型的最新进步。但是，一个关键的问题仍然没有解决：MLLM是否对人类的视觉信息有类似的看法？目前的基准缺乏从这个角度评估MLLM的能力。为了应对这一挑战，我们介绍了HVSbench，这是一种大规模的基准测试，旨在评估MLLM和人类视觉系统（HVS）之间的一致性，以反映人类视觉的基本视觉任务。 HVSBench策划了超过85K的多模式样本，跨越了HVS中的13个类别和5个字段，包括突出，替代，优先考虑，免费观看和搜索。广泛的实验证明了我们的基准在提供MLLM的全面评估方面的有效性。具体来说，我们评估了13个MLLM，表明即使是最佳模型也显示出重大改进的空间，大多数人仅取得了适度的结果。我们的实验表明，HVSbench提出了对尖端MLLM的新挑战。我们认为，HVSbench将有助于对人类协调和可解释的MLLM进行研究，这标志着了解MLLM的感知和处理视觉信息的关键步骤。

### InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions 
[[arxiv](https://arxiv.org/abs/2412.09596)] [[cool](https://papers.cool/arxiv/2412.09596)] [[pdf](https://arxiv.org/pdf/2412.09596)]
> **Authors**: Pan Zhang,Xiaoyi Dong,Yuhang Cao,Yuhang Zang,Rui Qian,Xilin Wei,Lin Chen,Yifei Li,Junbo Niu,Shuangrui Ding,Qipeng Guo,Haodong Duan,Xin Chen,Han Lv,Zheng Nie,Min Zhang,Bin Wang,Wenwei Zhang,Xinyue Zhang,Jiaye Ge,Wei Li,Jingwen Li,Zhongying Tu,Conghui He,Xingcheng Zhang, et al. (4 additional authors not shown)
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Github Repo: https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive
- **标题**: internlm-xcomposer2.5-omnilive：一种用于长期流式视频和音频互动的综合多模式系统
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 创建可以长期与人类认知类似的环境互动的AI系统一直是一个长期的研究目标。多模式大语言模型（MLLM）的最新进步在开放世界的理解方面取得了重大进步。但是，连续和同时流媒体感知，记忆和推理的挑战在很大程度上尚未得到探索。当前的MLLM受其序列到序列体系结构的约束，这限制了其处理输入和同时生成响应的能力，类似于在感知时无法思考。此外，由于保留所有信息变得昂贵且效率低下，因此依靠长篇小说来存储历史数据是不切实际的。因此，该项目并没有依靠单个基础模型执行所有功能，而是从专业的通才AI的概念中汲取灵感，并引入了分离的流媒体感知，推理和记忆机制，从而实现了通过流视频和音频输入的实时交互。所提出的框架InternLM-XCOMPOSER2.5-OMNILIVE（IXC2.5-OL）由三个关键模块组成：（1）流媒体感知模块：进程实时的多模式信息，将关键细节存储在内存中，并触发对用户查询的响应推理。 （2）多模式的长期内存模块：将短期和长期记忆集成，将短期记忆压缩到长期记忆中，以有效地检索和提高准确性。 （3）推理模块：响应查询并执行推理任务，与感知和内存模块进行协调。该项目模拟了类似人类的认知，使多模式大型语言模型可以随着时间的流逝提供连续和适应性的服务。

### OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation 
[[arxiv](https://arxiv.org/abs/2412.09585)] [[cool](https://papers.cool/arxiv/2412.09585)] [[pdf](https://arxiv.org/pdf/2412.09585)]
> **Authors**: Jitesh Jain,Zhengyuan Yang,Humphrey Shi,Jianfeng Gao,Jianwei Yang
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Project Page: https://praeclarumjj3.github.io/ola_vlm/
- **标题**: OLA-VLM：具有辅助嵌入蒸馏的多模式LLM中的视觉感知
- **领域**: 计算机视觉和模式识别
- **摘要**: 开发当代MLLM的标准实践是将视觉编码器的特征喂入LLM中，并在自然语言监督下进行训练。在这项工作中，我们为通过视觉观点（客观）（即，完全自然的语言监督对MLLM的视觉理解能力都在优化的情况下优化中间LLM表示的机会。为此，我们提出了OLA-VLM，这是第一种方法将知识从一组目标视觉表示中提取到LLM的隐藏表示形式。首先，我们在MLLM的预训练阶段中制定目标，作为预测性视觉嵌入和下一个文本token预测的耦合优化。其次，我们研究了仅通过自然语言监督培训的MLLM，并确定这些模型中视觉表示质量与其下游性能之间的正相关。此外，在探测我们的OLA-VLM时，由于嵌入优化的态度，我们观察到了改善的表示质量。第三，我们证明我们的OLA-VLM优于单个和多编码的基准，证明了我们的方法优于明确将相应的功能馈送到LLM的优势。尤其是，OLA-VLM在各种基准测试中的平均幅度提高了2.5％，而CV BEC的深度任务显着提高了8.7％。我们的代码在https://github.com/shi-labs/ola-vlm上进行开源。

### Exemplar Masking for Multimodal Incremental Learning 
[[arxiv](https://arxiv.org/abs/2412.09549)] [[cool](https://papers.cool/arxiv/2412.09549)] [[pdf](https://arxiv.org/pdf/2412.09549)]
> **Authors**: Yi-Lun Lee,Chen-Yu Lee,Wei-Chen Chiu,Yi-Hsuan Tsai
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Project page: https://github.com/YiLunLee/Exemplar_Masking_MCIL
- **标题**: 用于多模式增量学习的示例性掩蔽
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式增量学习需要从多种模式中消化信息，同时同时学习新知识，而无需忘记以前学习的信息。这项任务面临许多挑战，主要包括基于示例的方法中多模式数据的较大存储大小以及在巨大的多模型模型上进行填充的计算要求。在本文中，我们利用参数有效的调整方案来减轻微调的负担，并提出示例掩盖框架以有效地重播旧知识。具体而言，根据注意力的重量和不同模式之间的相关性掩盖了非重要令牌，大大降低了示例的存储大小，因此在同一内存缓冲区下节省了更多的示例。此外，我们设计了一种多模式数据增强技术，以使示例多样化以重播先验知识。在实验中，我们不仅在现有的多模式数据集中评估了我们的方法，而且还将Imagenet-R数据集扩展到多模式数据集作为真实世界应用程序，其中字幕是通过查询多模式大型语言模型（例如，指令Blip）生成的字幕。广泛的实验表明，在相同有限的记忆缓冲区下，我们的示例掩蔽框架对灾难性遗忘更有效，更强大。代码可在https://github.com/yilunlee/exemplar_masking_mcil上找到。

### Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition 
[[arxiv](https://arxiv.org/abs/2412.09501)] [[cool](https://papers.cool/arxiv/2412.09501)] [[pdf](https://arxiv.org/pdf/2412.09501)]
> **Authors**: Zhisheng Zhong,Chengyao Wang,Yuqi Liu,Senqiao Yang,Longxiang Tang,Yuechen Zhang,Jingyao Li,Tianyuan Qu,Yanwei Li,Yukang Chen,Shaozuo Yu,Sitong Wu,Eric Lo,Shu Liu,Jiaya Jia
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Tech report
- **标题**: Lyra：一个高效且以语音为中心的Omni认知框架
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 随着多模式的大语言模型（MLLM）的发展，超越单域功能的扩展对于满足对更具用途和高效AI的需求至关重要。但是，以前的Omni模型没有充分探索语音，忽略了其与多模式的整合。我们介绍了Lyra，这是一种有效的MLLM，可增强多模式能力，包括高级的长音理解，合理的理解，跨模式效率和无缝的语音相互作用。为了实现效率和以语言为中心的能力，Lyra采用了三种策略：（1）利用现有的开源大型模型和拟议的多模式Lora来减少培训成本和数据要求； （2）使用潜在的多模式正常器和提取器来增强语音与其他方式之间的关系，从而增强模型性能； （3）构建一个高质量的广泛数据集，其中包括150万个多模式（语言，视觉，音频）数据样本和12K长的语音样本，使Lyra能够处理复杂的长语音输入并获得更强大的OMNI认知。与其他Omni方法相比，Lyra在各种视觉语言，视觉语音和语音语言基准上实现了最先进的表现，同时也使用了更少的计算资源和更少的培训数据。

### Towards Robust and Fair Vision Learning in Open-World Environments 
[[arxiv](https://arxiv.org/abs/2412.09439)] [[cool](https://papers.cool/arxiv/2412.09439)] [[pdf](https://arxiv.org/pdf/2412.09439)]
> **Authors**: Thanh-Dat Truong
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: PhD Dissertation
- **标题**: 在开放世界的环境中进行健壮而公平的学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 论文为视觉学习中的公平和鲁棒性提供了四个关键贡献。首先，为了解决大规模数据要求的问题，本文提出了一种新颖的公平域适应方法，这些方法来自两种主要的新型新型研究结果，这些研究结果是，《肉类最大的可能性和公平性适应性学习》。其次，为了实现视力学习的开放世界建模的能力，本论文提出了一个新颖的开放世界公平持续学习框架。该研究方向的成功是两个研究界的结果，即公平持续学习和开放世界的持续学习。第三，由于视觉数据通常是从多个相机视图中捕获的，因此强大的视觉学习方法应能够在视图跨视图中建模不变特征。为了实现这一期望的目标，本文中的研究将提出一个基于新颖的几何跨视图适应框架，以学习跨视图的稳健特征表示。最后，随着大型视频和多模式数据的最新增加，了解特征表示并改善大型视觉基础模型的鲁棒性至关重要。因此，本文将提出基于新颖的变压器方法，以改善针对多模式和时间数据的稳健特征表示。然后，将提出一种新型的域泛化方法，以改善视觉基础模型的鲁棒性。该研究的理论分析和实验结果表明了所提出的方法的有效性，证明了它们与先前的研究相比表现出色。本文的贡献提高了机器视觉学习的公平性和鲁棒性。

### Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation 
[[arxiv](https://arxiv.org/abs/2412.09428)] [[cool](https://papers.cool/arxiv/2412.09428)] [[pdf](https://arxiv.org/pdf/2412.09428)]
> **Authors**: Baisen Wang,Le Zhuo,Zhaokai Wang,Chenxi Bao,Wu Chengjing,Xuecheng Nie,Jiao Dai,Jizhong Han,Yue Liao,Si Liu
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: 具有明确的桥梁和检索额的多模式音乐生成
- **领域**: 计算机视觉和模式识别,多媒体,声音,音频和语音处理
- **摘要**: 多模式音乐的旨在从各种输入方式中产生音乐，包括文本，视频和图像。现有方法使用常见的嵌入空间进行多模式融合。尽管它们在其他模式中有效，但它们在多模式音乐生成中的应用仍面临数据稀缺，较弱的跨模式一致性和有限的可控性的挑战。本文通过使用文本和音乐的明确桥进行多模式对齐来解决这些问题。我们介绍了一种名为Visual Music Bridge（VMB）的新颖方法。具体而言，多模式音乐描述模型将视觉输入转换为详细的文本描述，以提供文本桥梁。一个双轨音乐检索模块，结合了广泛而有针对性的检索策略，以提供音乐桥并启用用户控制。最后，我们设计了一个明确的音乐生成框架，以基于两个桥梁生成音乐。我们在视频到音乐，图像到音乐，文本到音乐和可控的音乐生成任务以及可控性实验上进行了实验。结果表明，与以前的方法相比，VMB显着提高了音乐质量，方式和自定义对齐方式。 VMB为在多媒体领域的应用程序设定了可解释和表现力的多模式音乐生成的新标准。演示和代码可在https://github.com/wbs2788/vmb上找到。

### MultiEYE: Dataset and Benchmark for OCT-Enhanced Retinal Disease Recognition from Fundus Images 
[[arxiv](https://arxiv.org/abs/2412.09402)] [[cool](https://papers.cool/arxiv/2412.09402)] [[pdf](https://arxiv.org/pdf/2412.09402)]
> **Authors**: Lehan Wang,Chongchong Qi,Chubin Ou,Lin An,Mei Jin,Xiangbin Kong,Xiaomeng Li
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Accepted at IEEE TMI
- **标题**: Multieye：来自眼底图像的OCT增强视网膜疾病识别的数据集和基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的关于眼底和OCT图像的多模式学习方法大多需要两种方式可用并严格配对进行培训和测试，在临床情况下这似乎不太实用。为了扩大临床应用的范围，我们制定了一种新颖的环境，“来自眼底图像的Oct增强疾病识别”，该设置允许在训练阶段使用未配对的多模式数据，并依靠广泛的底面照片进行测试。为了进行基准测试，我们介绍了第一个大型多模式多级多级数据集，用于眼科诊断，多型疾病，并提出了一种OCT辅助概念蒸馏方法（OCT-CODA），该方法采用语义上丰富的概念来从OCT图像中提取与疾病相关的知识，并将其利用它们并将其利用为Fundus模型。具体而言，我们将图像概念的关系视为依次从OCT教师模型到底底学生模型的有用知识的联系，该链接大大改善了基于眼底图像的诊断性能，并将跨模式知识转移提出到可解释的过程中。通过有关多疾病分类任务的广泛实验，我们提出的OCT-CODA表现出了显着的结果和可解释性，显示出巨大的临床应用潜力。我们的数据集和代码可在https://github.com/xmed-lab/multieye上找到。

### Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine 
[[arxiv](https://arxiv.org/abs/2412.09278)] [[cool](https://papers.cool/arxiv/2412.09278)] [[pdf](https://arxiv.org/pdf/2412.09278)]
> **Authors**: Xiaoshuang Huang,Lingdong Shen,Jia Liu,Fangxin Shang,Hongxiang Li,Haifeng Huang,Yehui Yang
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Accepted by AAAI2025
- **标题**: 使用像素级洞察生物医学的多模式大型语言模型
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 近年来，多模式大语模型（MLLM）取得了显着进步，证明了发展智能生物医学助理的可行性。但是，当前的生物医学MLLM主要集中在图像级别的理解并将相互作用限制为文本命令上，从而限制了其能力边界和使用的灵活性。在本文中，我们引入了一种新型的端到端多模式的大型语言模型，该模型名为Medplib，该模型具有像素级的理解。令人兴奋的是，它支持视觉问题答案（VQA），任意像素级提示（点，边界框和自由形状的形状）和像素级接地。我们提出了一种新型的Experts（MOE）多阶段训练策略，该培训将MOE分为单独的训练阶段，以提供视觉语言专家模型和像素接地专家模型，然后使用MOE进行微调。该策略有效地协调多任务学习，同时在推断上保持相当于单个专家模型的计算成本。为了推进生物医学MLLM的研究，我们介绍了医学复杂的视觉问题答案数据集（Mecovqa），该数据集包括8种模态，用于复杂的医学成像问题答案和图像区域的理解。实验结果表明，Medplib已在多个医学视觉语言任务中取得了最新的结果。更重要的是，在对像素接地任务的零射门评估中，Medplib在MDICE指标上分别以19.7和15.6的利润来领导最佳的大型和大型模型。代码，数据和模型检查点将在https://github.com/shawnhuang497/medplib上公开提供。

### FD2-Net: Frequency-Driven Feature Decomposition Network for Infrared-Visible Object Detection 
[[arxiv](https://arxiv.org/abs/2412.09258)] [[cool](https://papers.cool/arxiv/2412.09258)] [[pdf](https://arxiv.org/pdf/2412.09258)]
> **Authors**: Ke Li,Di Wang,Zhangyuan Hu,Shaofeng Li,Weiping Ni,Lin Zhao,Quan Wang
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: This work is accepted by AAAI 2025
- **标题**: FD2-NET：用于红外可见对象检测的频率驱动特征分解网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 红外可见对象检测（IVOD）试图利用红外和可见图像中的互补信息，从而增强在复杂环境中检测器的性能。但是，现有方法通常会忽略互补信息的频率特征，例如可见图像中丰富的高频细节以及红外图像中有价值的低频热信息，从而限制了检测性能。为了解决这个问题，我们引入了一个新型的频率驱动特征分解网络，用于IVOD，称为FD2-NET，该网络有效地捕获了跨多模式视觉空间互补信息的唯一频率表示。具体而言，我们提出了一个特征分解编码器，其中高频单元（HFU）利用离散的余弦变换来捕获代表性的高频特征，而低频单元（LFU）采用动态接收场来模拟多元化对象的多规模上下文。接下来，我们采用无参数互补的优势策略，通过无缝的频率频率重耦来增强多模式的特征。此外，我们创新设计了一种多模式重建机制，该机制恢复了在特征提取过程中丢失的图像细节，从而进一步利用了来自红外和可见图像的互补信息，以增强整体代表性。广泛的实验表明，FD2-NET在各种IVOD基准测试中胜过最先进的模型（SOTA）模型，即LLVIP（96.2％MAP），FLIR（82.9％MAP）和M3FD（83.5％MAP）。

### Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering 
[[arxiv](https://arxiv.org/abs/2412.09230)] [[cool](https://papers.cool/arxiv/2412.09230)] [[pdf](https://arxiv.org/pdf/2412.09230)]
> **Authors**: Sai Bhargav Rongali,Mohamad Hassan N C,Ankit Jha,Neha Bhargava,Saurabh Prasad,Biplab Banerjee
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: ef:WACV2025
- **标题**: 基础模型和自适应功能选择：视频问题回答的协同方法
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 本文应对视频提问的复杂挑战（VideoQA）。尽管取得了显着的进展，但当前方法仍未有效地将问题与视频框架和语义对象级抽象整合在一起，以创建问题吸引的视频表示。我们介绍了本地全球问题意识到的视频嵌入（LGQAVE），该视频嵌入（LGQAVE）结合了三个主要创新，以更好地整合多模式知识并强调与特定问题相关的语义视觉概念。 LGQAVE通过使用跨注意机制超越传统的临时框架采样，该机制精确地识别了有关问题的最相关框架。它使用独特的图捕获了这些帧中对象的动力学，并将其与迷你型模型相关语义。这些图由问题感知的动态图形变压器（Q-DGT）处理，该图形完善的输出以开发细微的全局和本地视频表示。附加的跨意义模块集成了这些本地和全局嵌入，以生成最终的视频嵌入，语言模型用于生成答案。跨多个基准测试的广泛评估表明，LGQAVE在提供准确的多选择和开放式答案方面显着胜过现有模型。

### MS2Mesh-XR: Multi-modal Sketch-to-Mesh Generation in XR Environments 
[[arxiv](https://arxiv.org/abs/2412.09008)] [[cool](https://papers.cool/arxiv/2412.09008)] [[pdf](https://arxiv.org/pdf/2412.09008)]
> **Authors**: Yuqi Tong,Yue Qiu,Ruiyang Li,Shi Qiu,Pheng-Ann Heng
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: IEEE AIxVR 2025
- **标题**: MS2MESH-XR：XR环境中的多模式素描到网格生成
- **领域**: 计算机视觉和模式识别,人机交互,多媒体
- **摘要**: 我们提出了MS2Mesh-XR，这是一种新颖的多模式草图到网格生成管道，使用户可以使用由语音输入辅助的手绘草图在扩展现实（XR）环境中创建现实的3D对象。在特定的情况下，用户可以在虚拟环境中使用自然的手机在空中使用自然的手机进行直观地绘制对象。通过集成语音输入，我们根据绘制的草图和解释的文本提示来设计ControlNet来推断现实的图像。然后，用户可以查看并选择其首选图像，然后使用卷积重建模型将其重建为详细的3D网格。特别是，我们提出的管道可以在不到20秒内生成高质量的3D网格，从而可以在运行时XR场景中进行沉浸式可视化和操纵。我们通过XR设置中的两种用例证明了管道的实用性。通过利用自然用户输入和最先进的生成AI功能，我们的方法可以显着促进基于XR的创意生产并增强用户体验。我们的代码和演示将在以下网址提供：https：//yueqiu0911.github.io/MS2Mesh-xr/

### Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations? 
[[arxiv](https://arxiv.org/abs/2412.08973)] [[cool](https://papers.cool/arxiv/2412.08973)] [[pdf](https://arxiv.org/pdf/2412.08973)]
> **Authors**: Yifan Zhang,Junhui Hou
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Under review
- **标题**: 对比度蒸馏是否足以学习全面的3D表示？
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近探索了跨模式对比度蒸馏以学习有效的3D表示。但是，现有方法主要集中于模态共享的特征，忽略了训练过程中特定于模式的特征，从而导致次优表示。在本文中，我们从理论上分析了3D表示学习的当前对比方法的局限性，并提出了一个新框架，即CMCR，以解决这些缺点。通过更好地整合模式共享和特定于模态特征，我们的方法可以改善传统方法。具体来说，我们介绍了蒙版的图像建模和占用估计任务，以指导网络学习更全面的模式特异性特征。此外，我们提出了一个新颖的多模式统一代码簿，该代码本可以学习跨不同方式共享的嵌入空间。此外，我们引入了几何增强的掩蔽图像建模，以进一步增强3D表示学习。广泛的实验表明，我们的方法减轻了传统方法所面临的挑战，并且在下游任务中始终优于现有的图像到较大对比蒸馏方法。代码将在https://github.com/eaphan/cmcr上找到。

### Multimodal Industrial Anomaly Detection by Crossmodal Reverse Distillation 
[[arxiv](https://arxiv.org/abs/2412.08949)] [[cool](https://papers.cool/arxiv/2412.08949)] [[pdf](https://arxiv.org/pdf/2412.08949)]
> **Authors**: Xinyue Liu,Jianyuan Wang,Biao Leng,Shuo Zhang
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: 通过跨模式反向蒸馏进行多模式工业异常检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 知识蒸馏（KD）已在无监督的工业图像异常检测（AD）中进行了广泛的研究，但其在无监督的多模式AD中的应用仍未得到充实。现有的基于KD的方法用于使用融合的多模式特征来获得教师表示的多模式广告面临挑战。在融合的教师功能中可能不会有效地捕获一种方式中的异常，从而导致检测失败。此外，这些方法并不能完全利用丰富的内部和模式间信息。在本文中，我们提出了基于多分支设计的跨模式反向蒸馏（CRD），以实现多模式工业广告。通过将独立的分支分配给每种模式，我们的方法可以在每种模态内更精细地检测异常。此外，我们通过设计跨模式过滤器和放大器来增强蒸馏过程中模态之间的相互作用。有了跨模式映射的想法，允许学生网络更好地学习正常功能，同时确保所有模式中的异常都可以有效地检测到。对MVTEC 3D-AD数据集的实验验证表明，我们的方法在多模式异常检测和定位中实现了最先进的性能。

### Multi-modal and Multi-scale Spatial Environment Understanding for Immersive Visual Text-to-Speech 
[[arxiv](https://arxiv.org/abs/2412.11409)] [[cool](https://papers.cool/arxiv/2412.11409)] [[pdf](https://arxiv.org/pdf/2412.11409)]
> **Authors**: Rui Liu,Shuwei He,Yifan Hu,Haizhou Li
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: 9 pages,2 figures, Accepted by AAAI'2025
- **标题**: 多模式和多尺度空间环境理解沉浸式视觉文本到语音
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **摘要**: 视觉文本对语音（VTTS）的目的是将环境形象作为提示，以综合口语内容的回响语音。这项任务的挑战在于从图像中理解空间环境。已经尝试从空间图像的RGB空间中提取全局空间视觉信息。但是，局部和深度图像信息对于理解以前的作品忽略的空间环境至关重要。为了解决这些问题，我们提出了一种新型的多模式和多尺度空间环境理解方案，以实现沉浸式VTT，称为M2SE-VTTS。多模式旨在同时使用空间图像的RGB和深度空间，以学习更全面的空间信息，并且多尺度旨在同时对本地和全球空间知识进行建模。具体来说，我们首先将RGB和深度图像分为补丁，并采用双子座生成的环境字幕来指导当地的空间理解。之后，多模式和多尺度特征是由本地感知的全球空间理解集成的。通过这种方式，M2SE-VTT有效地建模了多模式空间环境中局部和全局空间环境之间的相互作用。客观和主观评估表明，我们的模型在环境语音生成中的表现优于先进的基线。代码和音频样本可在以下网址提供：https：//github.com/ai-s2-lab/m2se-vtts。

### Leveraging Retrieval-Augmented Tags for Large Vision-Language Understanding in Complex Scenes 
[[arxiv](https://arxiv.org/abs/2412.11396)] [[cool](https://papers.cool/arxiv/2412.11396)] [[pdf](https://arxiv.org/pdf/2412.11396)]
> **Authors**: Antonio Carlos Rivera,Anthony Moore,Steven Robinson
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 利用检索提示的标签在复杂场景中以大量视力语言理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉任务中的对象感知推理对当前模型构成了重大挑战，尤其是在处理看不见的对象，减少幻觉并在复杂的视觉场景中捕获细粒度的关系。为了解决这些局限性，我们提出了视觉吸引的检索提示（VRAP）框架，该框架是一种生成的方法，通过将检索 - 夸大的对象标签集成到其提示中，从而增强了大型视觉模型（LVLMS）。 VRAP介绍了一条新颖的管道，其中使用验证的视觉编码器和场景图解析器提取结构化标签，包括对象，属性和关系。这些标签充满了外部知识，并将其纳入LLM的输入中，从而实现了详细而准确的推理。我们在包括VQAV2，GQA，Vizwiz和Coco在内的多种视觉基准中评估VRAP，以在细粒度的推理和多模式理解中实现最先进的性能。此外，我们的消融研究强调了检索标签和对比度学习的重要性，而人类评估则证实了VRAP产生准确，详细和上下文相关的响应的能力。值得注意的是，VRAP通过消除运行时检索来减少推理潜伏期40％。这些结果表明，VRAP是推进对象感知的多模式推理的强大，有效的框架。

### Unimodal and Multimodal Static Facial Expression Recognition for Virtual Reality Users with EmoHeVRDB 
[[arxiv](https://arxiv.org/abs/2412.11306)] [[cool](https://papers.cool/arxiv/2412.11306)] [[pdf](https://arxiv.org/pdf/2412.11306)]
> **Authors**: Thorben Ortmann,Qi Wang,Larissa Putzar
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项研究中，我们探讨了通过Meta Quest Pro虚拟现实（VR）耳机捕获的面部表达识别（FER）在VR设置中捕获的面部表达激活（FEA）的潜力。利用表情符号数据库（EMOHEVRDB），我们比较了几种单峰方法，并且具有七个情感类别的静态FER任务的精度高达73.02％。此外，我们将FEA和图像数据集成在多模式方法中，观察到识别精度的显着提高。中间融合方法的最高精度为80.42％，显着超过了EmohevrdB图像数据报告的69.84％的基线评估结果。我们的研究是第一个利用EmoHevrdB独特的FEA数据进行单峰和多模式静态FER的研究，为VR设置中的FER建立了新的基准。我们的发现突出了融合互补方式的潜力，以提高VR设置中的FER精度，在这种情况下，基于图像的传统方法受到头部安装显示器（HMD）的遮挡的严重限制。

### Multimodal Class-aware Semantic Enhancement Network for Audio-Visual Video Parsing 
[[arxiv](https://arxiv.org/abs/2412.11248)] [[cool](https://papers.cool/arxiv/2412.11248)] [[pdf](https://arxiv.org/pdf/2412.11248)]
> **Authors**: Pengcheng Zhao,Jinxing Zhou,Yang Zhao,Dan Guo,Yanxiang Chen
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: Accepted by AAAI-2025
- **标题**: 视频视频解析的多模式感知语义增强网络
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 视听视频解析任务旨在识别并在音频或视觉流中或两者兼有的所有事件中识别并定位所有事件。为每个音频/视觉段捕获准确的事件语义至关重要。先前的作品直接利用提取的整体音频和视觉特征，用于内部和跨模式的时间相互作用。但是，每个片段可能包含多个事件，从而导致语义上混合的整体特征，这些特征可能会导致语义干扰或跨模式相互作用：一个段的事件语义可能包含来自其他段的无关事件的语义。为了解决此问题，我们的方法从班级感知功能解耦（CAFD）模块开始，该模块明确将语义混合功能分解为不同的类别功能，包括多个特定于事件的特定功能和专用背景功能。脱钩的类功能使我们的模型能够从其他段中包含的清晰匹配的类中为每个段选择性地汇总有用的语义，从而防止了无关类的语义干扰。具体而言，我们进一步设计了一个用于编码跨模式关系的细粒语义增强模块。它包括一个细分事件的共发生建模（SECM）块和局部全球语义融合（LGSF）块。 SECM借助新的事件同时出现损失，利用同一时间戳中同一事件的类间依赖性。 LGSF通过将相关的语义纳入更具信息丰富的全球视频功能，从而进一步增强了每个细分市场的事件语义。广泛的实验验证了提出的模块和损失功能的有效性，从而导致了新的最新解析性能。

### Distribution-Consistency-Guided Multi-modal Hashing 
[[arxiv](https://arxiv.org/abs/2412.11216)] [[cool](https://papers.cool/arxiv/2412.11216)] [[pdf](https://arxiv.org/pdf/2412.11216)]
> **Authors**: Jin-Yu Liu,Xian-Ling Mao,Tian-Yi Che,Rong-Cheng Tu
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 分布一致性引导的多模式哈希
- **领域**: 计算机视觉和模式识别,人工智能,信息检索
- **摘要**: 由于其快速速度和较低的存储要求，多模式的哈希方法已经获得了普及。其中，监督方法通过使用标签作为监督信号来表现出更好的性能，与无监督的方法相比。目前，对于几乎所有监督的多模式哈希方法，有一个隐藏的假设，即训练集没有嘈杂的标签。但是，由于在现实世界中的手动标记，标签通常被错误地注释，这会极大地损害检索性能。为了解决这个问题，我们首先通过实验发现了显着的分布一致性模式，即标签中每个类别的存在或不存在的1-0分布与相对于类别中心的哈希码相似性分数的高低分布一致。然后，在这种模式的启发下，我们提出了一种新颖的分布抗性引导的多模式哈希（DCGMH），该模式旨在过滤和重建噪声标签以增强检索性能。具体而言，提出的方法首先随机初始化了几个类别中心，这些中心用于计算相似性分数的高低分布。然后，通过发现的分布一致性模式将嘈杂和干净的标签分别滤除，以减轻嘈杂标签的影响；随后，通过分布一致性模式间接设计的校正策略将应用于被过滤的嘈杂标签，纠正高信心的标签，同时将低信心的校正标签纠正为无标记的学习，从而进一步增强了模型的性能。在三个广泛使用的数据集上进行的广泛实验证明了该方法的优势与多模式检索任务中的最新基准相比。该代码可从https://github.com/liujinyu1229/dcgmh获得。

### GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control 
[[arxiv](https://arxiv.org/abs/2412.11198)] [[cool](https://papers.cool/arxiv/2412.11198)] [[pdf](https://arxiv.org/pdf/2412.11198)]
> **Authors**: Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Pedro M B Rezende,Yasaman Haghighi,David Brüggemann,Isinsu Katircioglu,Lin Zhang,Xiaoran Chen,Suman Saha,Marco Cannici,Elie Aljalbout,Botao Ye,Xi Wang,Aram Davtyan,Mathieu Salzmann,Davide Scaramuzza,Marc Pollefeys,Paolo Favaro,Alexandre Alahi
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 宝石：一种可推广的自我视频多模式世界模型，用于细颗粒，对象动态和场景组成控制
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了GEM，这是一种可概括的自我视频多模式模型，它使用参考框架，稀疏特征，人类姿势和自我trajoctory来预测未来的框架。因此，我们的模型对对象动态，自我运动和人类姿势具有精确的控制。 GEM生成配对的RGB和深度输出，以获得更丰富的空间理解。我们介绍自动回调的噪声时间表，以实现稳定的长途世代。我们的数据集由跨自主驾驶，中心人类活动和无人机飞行等域的4000多个小时的多模式数据组成。伪标签用于获取深度图，自我 -  trajoctory和人类姿势。我们使用全面的评估框架，包括对对象操纵（COM）度量的新控制，以评估可控性。实验表明，GEM在长期产生多样化，可控的场景和时间一致性方面表现出色。代码，模型和数据集是完全开源的。

### A Comprehensive Survey of Action Quality Assessment: Method and Benchmark 
[[arxiv](https://arxiv.org/abs/2412.11149)] [[cool](https://papers.cool/arxiv/2412.11149)] [[pdf](https://arxiv.org/pdf/2412.11149)]
> **Authors**: Kanglei Zhou,Ruizhi Cai,Liyuan Wang,Hubert P. H. Shum,Xiaohui Liang
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 对行动质量评估的全面调查：方法和基准测试
- **领域**: 计算机视觉和模式识别
- **摘要**: 行动质量评估（AQA）定量评估人类行动的质量，提供自动评估，以减少人类判断中的偏见。它的应用程序涵盖了体育分析，技能评估和医疗服务等领域。 AQA的最新进展引入了创新的方法，但是类似的方法经常跨不同领域交织在一起，突出了阻碍系统评价的分散性质。此外，缺乏统一的基准和有限的计算比较阻碍了对AQA方法的一致评估和公平评估。在这项工作中，我们通过系统地分析超过150个与AQA相关的论文来开发层次分类法，构建统一的基准，并对当前趋势，挑战和未来的方向进行深入分析，从而解决这些差距。我们的层次分类学对基于输入方式（视频，骨架，多模式）及其特定特征的AQA方法进行了分类，从而突出了各种方法的进化和相互关系。为了促进标准化，我们提出了一个统一的基准测试，并集成了不同的数据集，以评估评估精度和计算效率。最后，我们回顾了新兴任务特定的应用程序，并确定了AQA中未经探索的挑战，从而为未来的研究方向提供了可行的见解。这项调查旨在加深对AQA进度的了解，促进方法比较并指导未来的创新。可以在https://zhoukanglei.github.io/aqa-survey上找到项目网页。

### Combating Multimodal LLM Hallucination via Bottom-Up Holistic Reasoning 
[[arxiv](https://arxiv.org/abs/2412.11124)] [[cool](https://papers.cool/arxiv/2412.11124)] [[pdf](https://arxiv.org/pdf/2412.11124)]
> **Authors**: Shengqiong Wu,Hao Fei,Liangming Pan,William Yang Wang,Shuicheng Yan,Tat-Seng Chua
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: 16 pages, 10 figures, accepted by AAAI 25
- **标题**: 通过自下而上的整体推理对抗多模式LLM幻觉
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的最新进展表现出了前所未有的能力，可以推进各种视觉任务。但是，MLLM面临幻觉面临重大挑战，以及与输入数据不符的误导性输出。尽管为打击MLLM幻觉而付出了现有的努力，但仍未解决一些关键的挑战。首先，虽然当前的方法积极专注于在感知层面上解决错误，但在认知水平上的另一种重要类型可以忽略需要事实常识的认知水平。此外，现有方法可能无法找到一种更有效的表示视觉输入的方法，这仍然是触发视觉幻觉的关键瓶颈。此外，MLLM经常会被错误的文本输入误导并引起幻觉，而不幸的是，这种类型的问题长期以来一直被现有研究所忽视。受到人类直觉处理幻觉的启发，本文介绍了一个新颖的自下而上推理框架。我们的框架通过验证和集成感知级信息与认知级别的常识知识，系统地解决视觉和文本输入中的潜在问题，从而确保更可靠的输出。广泛的实验表明，在将MLLM与所提出的框架集成后，多个幻觉基准的显着改善。深入的分析揭示了我们方法在解决感知和认知水平幻觉方面的巨大潜力。

### Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval 
[[arxiv](https://arxiv.org/abs/2412.11077)] [[cool](https://papers.cool/arxiv/2412.11077)] [[pdf](https://arxiv.org/pdf/2412.11077)]
> **Authors**: Yuanmin Tang,Xiaoting Qin,Jue Zhang,Jing Yu,Gaopeng Gou,Gang Xiong,Qingwei Ling,Saravan Rajmohan,Dongmei Zhang,Qi Wu
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 理由 - 重新划分：一阶段反思性的思想链，用于无训练的零弹药组成图像检索
- **领域**: 计算机视觉和模式识别
- **摘要**: 组成的图像检索（CIR）旨在检索与参考图像相似的目标图像，同时集成用户指定的文本修改，从而更精确地捕获用户意图。现有的无培训零射击CIR（ZS-CIR）方法通常采用两个阶段的过程：它们首先为参考图像生成标题，然后使用大型语言模型进行推理来获得目标描述。但是，这些方法缺少关键的视觉细节和有限的推理能力，从而导致次优的检索性能。为了应对这些挑战，我们提出了一种新颖的，无训练的单阶段方法，对ZS-CIR（OSRCIR）的一阶段反射性链链推理（OSRCIR），该方法采用多模式的大语言模型来保留单阶段的视觉信息在单阶段推理过程中，从而消除了在两阶段方法中看到的信息损失。我们的反思性链条框架通过将意图与参考图像的上下文提示保持一致，从而进一步提高了解释的准确性。 OSRCIR在多个任务中的现有无培训方法中实现了1.80％至6.44％的绩效提高，为ZS-CIR创造了新的最先进的方法，并增强了其在视觉应用程序中的效用。我们的代码将在https://github.com/pter61/osrcir2024/上找到。

### HC-LLM: Historical-Constrained Large Language Models for Radiology Report Generation 
[[arxiv](https://arxiv.org/abs/2412.11070)] [[cool](https://papers.cool/arxiv/2412.11070)] [[pdf](https://arxiv.org/pdf/2412.11070)]
> **Authors**: Tengfei Liu,Jiapu Wang,Yongli Hu,Mingjie Li,Junfei Yi,Xiaojun Chang,Junbin Gao,Baocai Yin
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: Accepted by AAAI2025
- **标题**: HC-LLM：放射学报告的历史受限大语模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 放射学报告生成（RRG）模型通常专注于单个考试，通常忽略了历史视觉或文本数据的整合，这对于患者的随访至关重要。传统方法通常在合并历史信息时会遇到长序列依赖性，但是大型语言模型（LLMS）在文化中的学习中表现出色，非常适合分析纵向医学数据。鉴于此，我们提出了一个新型的RRG历史受限的大语言模型（HC-LLM）框架，通过限制纵向图像及其相应报告之间的一致性和差异，从而使LLM具有纵向报告生成能力的能力。具体而言，我们的方法从纵向胸部X射线和诊断报告中提取了时间共享和特定时间的特征，以捕获疾病的进展。然后，我们通过应用模式内相似性约束来确保一致的表示，并与具有多模式对比度和结构约束的模态之间对齐各种特征。这些组合的约束有效地指导LLM生成诊断报告，以准确反映疾病的进展，从而在纵向模拟数据集中获得最新的结果。值得注意的是，即使在测试过程中没有历史数据的情况下，我们的方法也可以很好地表现，并且很容易适应其他多模式的大型模型，从而增强了其多功能性。

### Overview of TREC 2024 Medical Video Question Answering (MedVidQA) Track 
[[arxiv](https://arxiv.org/abs/2412.11056)] [[cool](https://papers.cool/arxiv/2412.11056)] [[pdf](https://arxiv.org/pdf/2412.11056)]
> **Authors**: Deepak Gupta,Dina Demner-Fushman
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: TREC概述2024医学视频问答（MEDVIDQA）曲目
- **领域**: 计算机视觉和模式识别
- **摘要**: 人工智能（AI）的关键目标之一是开发多模式系统，该系统促进了使用自然语言查询来与视觉世界（图像和视频）的沟通。早期的医学问题作品回答主要集中于文本和视觉（图像）方式，这可能是在回答需要演示的问题时效率低下。近年来，由于引入了大规模语言视觉数据集以及有效的深神经技术的发展，从而弥合了语言和视觉理解之间的鸿沟，因此取得了重大进展。在众多视觉和语言任务中已经进行了改进，例如视觉字幕的视觉响应和自然语言视频本地化。关于语言愿景的大多数现有工作都集中在创建数据集并为开放域应用程序开发解决方案。我们认为，医疗视频可能会为许多急救，医疗和医学教育问题提供最佳答案。随着对AI的兴趣越来越多，以支持临床决策并改善患者的参与度，因此有必要探索此类挑战并开发有效的医学语言视频理解和产生算法。为此，我们介绍了新的任务，以促进设计可以理解医疗视频以提供自然语言问题的视觉答案的系统，并具有多模式能力，以从医学视频中生成指导步骤。这些任务有可能支持可以使公众和医疗专业人员受益的复杂下游应用程序的开发。

### From Simple to Professional: A Combinatorial Controllable Image Captioning Agent 
[[arxiv](https://arxiv.org/abs/2412.11025)] [[cool](https://papers.cool/arxiv/2412.11025)] [[pdf](https://arxiv.org/pdf/2412.11025)]
> **Authors**: Xinran Wang,Muxi Diao,Baoteng Li,Haiwen Zhang,Kongming Liang,Zhanyu Ma
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: A technical report. Project: https://github.com/xin-ran-w/CapAgent
- **标题**: 从简单到专业：组合可控的图像字幕代理
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 可控的图像字幕代理（Pagagent）是一个创新的系统，旨在弥合图像字幕任务中用户简单性和专业级输出之间的差距。 Pagagent会自动将用户提供的简单说明转换为详细的专业说明，从而实现精确和上下文感知的字幕生成。通过利用多模式大语言模型（MLLM）和外部工具，例如对象检测工具和搜索引擎，该系统可确保字幕遵守指定的指南，包括情感，关键字，焦点和格式化。代码透明地控制字幕过程的每个步骤，并在每个步骤中展示其推理和工具使用情况，从而促进用户的信任和参与度。该项目代码可在https://github.com/xin-ran-w/capagent上获得。

### Heterogeneous Graph Transformer for Multiple Tiny Object Tracking in RGB-T Videos 
[[arxiv](https://arxiv.org/abs/2412.10861)] [[cool](https://papers.cool/arxiv/2412.10861)] [[pdf](https://arxiv.org/pdf/2412.10861)]
> **Authors**: Qingyu Xu,Longguang Wang,Weidong Sheng,Yingqian Wang,Chao Xiao,Chao Ma,Wei An
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: N/A
- **标题**: RGB-T视频中多个微小对象跟踪的异质图变压器
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 跟踪多个小物体由于外观较弱和功能有限，因此极具挑战性。现有的多目标跟踪算法通常集中在单模式场景上，并忽略由多个遥控传感器捕获的微小对象的互补特性。为了通过整合来自多个来源的互补信息来增强跟踪性能，我们提出了一个新颖的框架，称为{HGT-TRACK（基于异构图形变压器的多微小对象跟踪）}。具体而言，我们首先采用基于变压器的编码器来嵌入来自不同模式的图像。随后，我们利用异质图变压器从多种模态来汇总空间和时间信息来生成检测和跟踪特征。此外，我们引入了一个目标重新检测模块（REDET），以通过保持不同方式的一致性来确保轨道连续性。此外，本文介绍了RGB-T融合多个微小对象跟踪的第一个基准VT vt-tiny-mot（可见性小型多对象跟踪）。对VT微型计算机进行了广泛的实验，结果证明了我们方法的有效性。与其他最先进的方法相比，我们的方法在MOTA（多目标跟踪精度）和ID-F1分数方面取得了更好的性能。代码和数据集将在https://github.com/xuqingyu26/hgtmt上提供。

### Attention-driven GUI Grounding: Leveraging Pretrained Multimodal Large Language Models without Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2412.10840)] [[cool](https://papers.cool/arxiv/2412.10840)] [[pdf](https://arxiv.org/pdf/2412.10840)]
> **Authors**: Hai-Ming Xu,Qi Chen,Lei Wang,Lingqiao Liu
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: Accepted to AAAI 2025
- **标题**: 注意力驱动的GUI接地：利用预审慎的多模式大型语言模型而无需微调
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的最新进步对其自主互动和解释图形用户界面（GUIS）的能力产生了重大兴趣。这些系统中的一个主要挑战是基于基于GUI映像和相应的文本查询的关键GUI组件（例如文本或图标）。传统上，这项任务依赖于使用专门的培训数据的微调MLLM来直接预测组件位置。但是，在本文中，我们提出了一种新颖的无调注意驱动的接地（TAG）方法，该方法利用了预计的MLLM中固有的注意力模式来完成此任务，而无需进行其他微调。我们的方法涉及在精心构造的查询提示中识别和汇总特定令牌的注意图。应用于MiniCPM-LALAMA3-V 2.5，一种最先进的MLLM，我们的无调方法可实现与基于调整的方法相当的性能，并且在文本本地化方面取得了显着成功。此外，我们证明了我们的基于注意图的接地技术可显着优于Minicpm-llama3-V 2.5的直接定位预测，这突出了使用预读取的MLLM的注意图的潜力，并为该领域的未来创新铺平了道路。

### Low-Biased General Annotated Dataset Generation 
[[arxiv](https://arxiv.org/abs/2412.10831)] [[cool](https://papers.cool/arxiv/2412.10831)] [[pdf](https://arxiv.org/pdf/2412.10831)]
> **Authors**: Dengyang Jiang,Haoyu Wang,Lei Zhang,Wei Wei,Guang Dai,Mengmeng Wang,Jingdong Wang,Yanning Zhang
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: Preprint
- **标题**: 低偏见的通用注释数据集生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 事实证明，在一般注释的数据集（例如ImageNet）上，包括众多手动收集的图像的通用注释数据集（例如，ImageNet）上的预训练骨干网络具有类别注释是必不可少的，这是必不可少的。但是，那些手动收集的图像通常表现出偏见，这在类别或域之间是不可转移的，从而导致模型的概括能力变性。为了减轻此问题，我们提出了低偏见的通用注释数据集生成框架（LBGEN）。我们旨在直接生成具有类别注释的低偏置图像，而不是昂贵的手动收集。为了实现这一目标，我们建议利用多模式基础模型（例如剪辑）的优势，以在语言定义的低偏置语义空间中对齐图像。具体而言，我们开发了双层语义对齐损失，这不仅迫使所有生成的图像都以对抗性学习方式与属于目标数据集的所有类别的语义分布一致，而且还需要每个生成的图像以匹配其类别名称的语义描述。此外，我们进一步将现有的图像质量评分模型投入到质量保证损失中，以保留生成的图像的质量。通过利用这两个损失函数，我们可以通过仅使用目标数据集中的所有类别名称作为输入来微调预训练的扩散模型，从而获得低偏置的图像生成模型。实验结果证实，与手动标记的数据集或其他合成数据集相比，我们生成的低偏置数据集的利用会导致跨各种任务的不同骨干网络的稳定概括能力增强，尤其是在手动标记的样品稀缺的任务中。

### Rebalanced Vision-Language Retrieval Considering Structure-Aware Distillation 
[[arxiv](https://arxiv.org/abs/2412.10761)] [[cool](https://papers.cool/arxiv/2412.10761)] [[pdf](https://arxiv.org/pdf/2412.10761)]
> **Authors**: Yang Yang,Wenjuan Xi,Luping Zhou,Jinhui Tang
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 重新平衡的视觉语言检索考虑结构感知蒸馏
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视力语言检索旨在根据另一种模式的查询来搜索一种模式中的类似实例。主要目的是学习潜在共同空间中的跨模式匹配表示。实际上，跨模式匹配的基础假设是模态平衡，其中每种模态都包含足够的信息来代表其他模态。但是，噪声干扰和模态功能不全通常会导致模态失衡，从而使其在实践中成为常见现象。失衡对检索性能的影响仍然是一个悬而未决的问题。在本文中，我们首先证明，当存在不平衡的模态时，最终的跨模式匹配通常是跨模式检索的最佳选择。当面对不平衡的模态时，公共空间中实例的结构固有地受到了影响，对跨模式相似性测量构成了挑战。为了解决这个问题，我们强调了有意义的结构保留匹配的重要性。因此，我们提出了一种简单而有效的方法，通过学习结构保存的匹配表示来重新平衡跨模式匹配。具体而言，我们设计了一种新型的多叶度跨模式匹配，该匹配将结构感知的蒸馏与跨模式匹配损失一起。虽然跨模式匹配损耗约束实例级匹配，但结构感知的蒸馏进一步使学习的匹配表示和模式内表示之间的几何一致性通过开发的关系匹配。与基线模型相比，在不同数据集上进行的广泛实验肯定了我们方法的出色跨模式检索性能，同时增强了单模式检索能力。

### Optimizing Vision-Language Interactions Through Decoder-Only Models 
[[arxiv](https://arxiv.org/abs/2412.10758)] [[cool](https://papers.cool/arxiv/2412.10758)] [[pdf](https://arxiv.org/pdf/2412.10758)]
> **Authors**: Kaito Tanaka,Benjamin Tan,Brian Wong
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 通过仅解码器模型优化视觉互动
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLM）已成为多模式任务的关键推动因素，但是它们对单独的视觉编码器的依赖引入了效率，可扩展性和模态对准方面的挑战。为了解决这些局限性，我们提出了Mudaif（具有自适应输入融合的多模式统一解码器），这是一种仅解码器视觉语言模型，该模型通过新颖的视觉适配器（VTA）和适应性共同的共同指导机制无缝地整合了视觉和文本输入。通过消除对视觉编码器的需求，Mudaif实现了提高的效率，灵活性和跨模式的理解。 Mudaif在4500万图像文本对的大规模数据集上进行了训练，Mudaif始终超过多个基准测试的最先进方法，包括VQA，图像字幕和多模式推理任务。广泛的分析和人类评估表明，Mudaif的鲁棒性，泛化能力和实际可用性，将其确立为无编码视觉模型的新标准。

### OmniHD-Scenes: A Next-Generation Multimodal Dataset for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2412.10734)] [[cool](https://papers.cool/arxiv/2412.10734)] [[pdf](https://arxiv.org/pdf/2412.10734)]
> **Authors**: Lianqing Zheng,Long Yang,Qunshu Lin,Wenjin Ai,Minghao Liu,Shouyi Lu,Jianan Liu,Hongze Ren,Jingyue Mo,Xiaokai Bai,Jie Bai,Zhixiong Ma,Xichan Zhu
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: Omnihd-Scenes：用于自动驾驶的下一代多模式数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 深度学习的快速发展加剧了对自动驾驶算法使用的全面数据的需求。高质量的数据集对于开发有效数据驱动的自动驾驶解决方案至关重要。下一代自动驾驶数据集必须是多模式的，并结合了来自高级传感器的数据，这些数据具有广泛的数据覆盖率，详细的注释和不同的场景表示形式。为了满足这一需求，我们提出了OmniHd-Scenes，这是一种大规模的多模式数据集，可提供全面的全向高清数据。 Omnihd-Scenes数据集结合了来自128束梁激光雷达，六个相机和六个4D成像雷达系统的数据，以实现完整的环境感知。该数据集包含1501个夹子，每个夹子长约30秒，总计超过450K同步帧和超过585万个同步传感器数据点。我们还提出了一个新颖的4D注释管道。迄今为止，我们已经注释了200个剪辑，其中超过514k精确的3D边界框。这些剪辑还包括静态场景元素的语义分割注释。此外，我们还引入了一条新型的自动化管道，以生成密集的占用地面真理，从而有效利用非钥匙框架的信息。除了提出的数据集外，我们还建立了3D检测和语义占用预测的全面评估指标，基线模型和基准。这些基准测试利用环绕视图摄像机和4D成像雷达来探索用于自动驾驶应用的具有成本效益的传感器解决方案。广泛的实验证明了我们的低成本传感器构型及其在不利条件下的鲁棒性的有效性。数据将在https://www.2077ai.com/omnihd-scenes上发布。

### Bridging Vision and Language: Modeling Causality and Temporality in Video Narratives 
[[arxiv](https://arxiv.org/abs/2412.10720)] [[cool](https://papers.cool/arxiv/2412.10720)] [[pdf](https://arxiv.org/pdf/2412.10720)]
> **Authors**: Ji-jun Park,Soo-joon Choi
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 桥接视觉和语言：在视频叙述中建模因果关系和时间性
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频字幕是多模式机器学习领域的关键任务，旨在为视频内容生成描述性和连贯的文本叙述。尽管大型视觉模型（LVLM）显示出很大的进步，但它们通常很难捕获复杂的视频序列中固有的因果和时间动态。为了解决这一限制，我们提出了一个增强的框架，将因果关系推理模块（CTRM）集成到最新的LVLM中。 CTRM包括两个关键组成部分：因果动力学编码器（CDE）和时间关系学习者（TRL），它们集体编码因果关系依赖性和视频帧的时间一致性。我们进一步设计了一种多阶段学习策略，以优化模型，结合大规模视频文本数据集的预训练，对因果带注释的数据进行微调以及对比度对齐，以更好地嵌入相干性。对标准基准（例如MSVD和MSR-VTT）的实验结果表明，我们的方法在自动指标（Cider，Bleu-4，Rouge-L）和人类评估中的现有方法都优于现有方法，并获得了更加流利，相干和相关标题。这些结果证明了我们方法在产生字幕的有效性，并具有丰富的因果关系叙事。

### MambaPro: Multi-Modal Object Re-Identification with Mamba Aggregation and Synergistic Prompt 
[[arxiv](https://arxiv.org/abs/2412.10707)] [[cool](https://papers.cool/arxiv/2412.10707)] [[pdf](https://arxiv.org/pdf/2412.10707)]
> **Authors**: Yuhao Wang,Xuehu Liu,Tianyu Yan,Yang Liu,Aihua Zheng,Pingping Zhang,Huchuan Lu
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: This work is accepted by AAAI2025. More modifications may be performed
- **标题**: MAMBAPRO：多模式对象重新识别具有MAMBA聚合和协同及时的及
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 多模式对象重新识别（REID）旨在通过利用来自不同方式的互补图像信息来检索特定对象。最近，在传统的单模式对象REID任务中，大规模的预训练模型（如剪辑）表现出了令人印象深刻的性能。但是，对于多模式对象REID，它们仍未探索。此外，当前的多模式聚集方法在处理不同模态的长序列时具有明显的局限性。为了解决上述问题，我们介绍了一个名为Mambapro的新型框架，用于多模式对象REID。要具体而言，我们首先采用并行的进发件适配器（PFA）来调整夹子到多模式对象reid。然后，我们提出协同残留提示（SRP），以指导多模式特征的联合学习。最后，利用Mamba的出色可扩展性来长期序列，我们将Mamba聚集（MA）引入以有效地模拟不同模态之间的相互作用。结果，Mambapro可以提取更高复杂性的更健壮的功能。在三个多模式对象REID基准（即RGBNT201，RGBNT100和MSVR310）上进行了广泛的实验验证了我们提出的方法的有效性。源代码可从https://github.com/924973292/mambapro获得。

### MEATRD: Multimodal Anomalous Tissue Region Detection Enhanced with Spatial Transcriptomics 
[[arxiv](https://arxiv.org/abs/2412.10659)] [[cool](https://papers.cool/arxiv/2412.10659)] [[pdf](https://arxiv.org/pdf/2412.10659)]
> **Authors**: Kaichen Xu,Qilong Wu,Yan Lu,Yinan Zheng,Wenlin Li,Xingjie Tang,Jun Wang,Xiaobo Sun
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: AAAI 2025. Code: https://github.com/wqlzuel/MEATRD
- **标题**: MEATRD：多模式异常组织区域检测通过空间转录组学增强
- **领域**: 计算机视觉和模式识别,机器学习,定量方法
- **摘要**: 在临床诊断和病理研究中，检测受影响组织中异常组织区域（ATR）至关重要。常规的自动化ATR检测方法，主要基于单独基于组织学图像，在ATR和正常组织具有微妙的视觉差异的情况下，它会摇摇欲坠。最近的空间转录组学（ST）技术介绍了整个组织区域的基因表达式，提供了用于检测ATR的分子观点。但是，缺乏ATR检测方法，可以有效利用组织学图像和ST的补充信息。为了解决这一差距，我们提出了MEATRD，这是一种整合组织学图像和ST数据的新型ATR检测方法。 MEATRD经过训练，可以从其多模式嵌入中重建正常组织斑点（Inliers）的基因表达谱，然后学习基于潜在多模式重建误差的一级分类AD模型。该策略统一了基于重建和一级分类方法的优势。 MEATRD的核心是创新的蒙版图双意见变压器（MGDAT）网络，它不仅促进了交叉模式和跨节点信息共享，而且还解决了基于重建AD方法中常见的过度概要问题。此外，我们证明了在MGDAT中生成的多模式瓶颈编码中，对特定于模式的，与任务相关的信息进行了整理和凝结，这标志着对多模式瓶颈编码的信息属性的首次理论分析。在八个真正的ST数据集中进行的广泛评估揭示了MEATRD在ATR检测中的出色性能，超过了各种最新的AD方法。值得注意的是，MEATRD也证明了鉴于ATR的识别，该ATR仅显示出与正常组织的视觉偏差轻微的偏差。

### DeMo: Decoupled Feature-Based Mixture of Experts for Multi-Modal Object Re-Identification 
[[arxiv](https://arxiv.org/abs/2412.10650)] [[cool](https://papers.cool/arxiv/2412.10650)] [[pdf](https://arxiv.org/pdf/2412.10650)]
> **Authors**: Yuhao Wang,Yang Liu,Aihua Zheng,Pingping Zhang
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: This work is accepted by AAAI2025. More motifications may be performed
- **标题**: 演示：多模式对象重新识别的专家的基于特征的分离混合物
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式对象重新识别（REID）旨在通过结合来自多种模式的互补信息来检索特定对象。现有的多模式对象REID方法主要集中于异质特征的融合。但是，他们经常忽略多模式成像中动态质量的变化。此外，不同方式之间的共享信息可以削弱特定于模式的信息。为了解决这些问题，我们提出了一个新颖的功能学习框架，称为Demo，用于多模式对象REID，该框架使用专家混合物适应性地平衡了脱钩的功能。要具体而言，我们首先部署了一个斑块集成的特征提取器（PIFE）来提取多晶格和多模式特征。然后，我们引入了分层脱钩模块（HDM），以将多模式特征与非重叠形式脱脱，从而保留了模态唯一性并增加了特征多样性。最后，我们提出了专家（ATMOE）的注意力触发的混合物，该混合物用脱钩特征得出的动态注意力替代了传统的门控。使用这些模块，我们的演示可以生成更强大的多模式功能。对三个多模式对象REID基准测试的广泛实验充分验证了我们方法的有效性。源代码可从https://github.com/924973292/demo获得。

### CATALOG: A Camera Trap Language-guided Contrastive Learning Model 
[[arxiv](https://arxiv.org/abs/2412.10624)] [[cool](https://papers.cool/arxiv/2412.10624)] [[pdf](https://arxiv.org/pdf/2412.10624)]
> **Authors**: Julian D. Santamaria,Claudia Isaza,Jhony H. Giraldo
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 目录：相机陷阱语言引导的对比度学习模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 基础模型（FMS）在各种计算机视觉任务中都取得了成功，例如图像分类，对象检测和图像分段。但是，当这些模型在与培训数据集不同分布的数据集上测试时，这些任务仍然具有挑战性。这对于在摄像机陷阱图像中识别动物物种尤其有问题，在摄像机陷阱图像中，我们在照明，伪装和闭塞等因素方面具有差异。在本文中，我们提出了相机陷阱语言引导的对比度学习（目录）模型，以解决这些问题。我们的方法结合了多个FMS来从相机陷阱数据中提取视觉和文本特征，并使用对比度损耗功能来训练模型。我们在两个基准数据集上评估了目录，并表明它的表现优于摄像机陷阱图像识别中先前的最新方法，尤其是当训练和测试数据具有不同的动物物种或来自不同地理区域时。我们的方法证明了将FMS与多模式融合和对比度学习结合使用的潜力，以解决相机陷阱图像识别中的域移动。目录代码可在https://github.com/julian075/catalog上公开获得。

### Towards Unified Benchmark and Models for Multi-Modal Perceptual Metrics 
[[arxiv](https://arxiv.org/abs/2412.10594)] [[cool](https://papers.cool/arxiv/2412.10594)] [[pdf](https://arxiv.org/pdf/2412.10594)]
> **Authors**: Sara Ghazanfari,Siddharth Garg,Nicolas Flammarion,Prashanth Krishnamurthy,Farshad Khorrami,Francesco Croce
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 迈向统一的基准和多模式感知指标的模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 人类对跨单模式输入的相似性的看法非常复杂，这使得开发准确模仿它的自动指标具有挑战性。通用视觉语言模型，例如剪辑和大型多模式模型（LMMS），可以用作零摄像的感知指标，并且最近的一些工作开发了专门从事狭窄感知任务的模型。但是，现有的感知指标与人类感知保持一致的程度尚不清楚。为了调查这个问题，我们介绍了Unisim-Bench，这是一个涵盖7个多模式感知相似性任务的基准测试，共有25个数据集。我们的评估表明，尽管通用模型平均表现出色，但它们通常落后于个人任务的专业模型。相反，针对特定任务进行微调的指标无法很好地推广到看不见的任务。作为朝着统一的多任务感知相似度度量的第一步，我们在Unisim基础台上任务的子集上微调了基于编码器的生成视觉模型。这种方法产生的平均表现最高，在某些情况下，甚至超过了任务特定的模型。然而，这些模型仍然在概括方面挣扎着看不见的任务，强调了学习一个坚固，统一的感知相似性度量的持续挑战，能够捕捉人类相似性的观念。代码和型号可在https://github.com/saraghazanfari/unisim上找到。

### EVLM: Self-Reflective Multimodal Reasoning for Cross-Dimensional Visual Editing 
[[arxiv](https://arxiv.org/abs/2412.10566)] [[cool](https://papers.cool/arxiv/2412.10566)] [[pdf](https://arxiv.org/pdf/2412.10566)]
> **Authors**: Umar Khalid,Hasan Iqbal,Azib Farooq,Nazanin Rahnavard,Jing Hua,Chen Chen
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: Technical Report
- **标题**: EVLM：跨维视觉编辑的自我反射多模式推理
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于模棱两可的说明编辑复杂的视觉内容仍然是视觉建模中的一个具有挑战性的问题。尽管现有模型可以将内容与之相关，但他们通常很难掌握参考图像或场景中的基本意图，从而导致编辑未对准。我们介绍了编辑视觉模型（EVLM），该系统旨在解释与参考视觉效果结合使用的此类说明，从而产生精确和上下文意识到的编辑提示。利用思想链（COT）推理和KL-Divergence目标优化（KTO）对准技术，EVLM捕获主观编辑偏好，而无需二进制标签。 EVLM在30,000个COT示例的数据集上进行了微调，并由人类评估人员进行了依据，EVLM表现出与人类意图的一致性的显着改善。跨图像，视频，3D和4D编辑任务的实验表明，EVLM生成连贯的高质量指令，为复杂的视觉语言应用提供了可扩展的框架。

### The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion 
[[arxiv](https://arxiv.org/abs/2412.10523)] [[cool](https://papers.cool/arxiv/2412.10523)] [[pdf](https://arxiv.org/pdf/2412.10523)]
> **Authors**: Changan Chen,Juze Zhang,Shrinidhi K. Lakshmikanth,Yusu Fang,Ruizhi Shao,Gordon Wetzstein,Li Fei-Fei,Ehsan Adeli
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: Project page: languageofmotion.github.io
- **标题**: 运动语言：统一3D人类运动的口头和非语言语言
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类的交流本质上是多模式的，涉及言语和非语言提示，例如语音，面部表情和身体手势。建模这些行为对于理解人类互动和创建可以在游戏，电影和虚拟现实等应用中自然沟通的虚拟字符至关重要。但是，现有的运动生成模型通常仅限于特定的输入方式 - 语音，文本或运动数据，并且无法完全利用可用数据的多样性。在本文中，我们提出了一个新颖的框架，该框架使用多模式模型来统一口头和非语言语言，以进行人类运动的理解和产生。该模型可以灵活地采用文本，语音和运动或将其作为输入的任何组合。再加上我们新颖的训练策略，我们的模型不仅可以在共同语音的手势生成上实现最先进的表现，而且还需要较少的培训数据。我们的模型还解锁了一系列新颖的任务，例如可编辑的手势产生和动作的情感预测。我们认为，统一人类运动的口头和非语言语言对于现实世界的应用至关重要，语言模型为实现这一目标提供了强大的方法。项目页面：LagansofMotion.github.io。

### DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts 
[[arxiv](https://arxiv.org/abs/2412.10510)] [[cool](https://papers.cool/arxiv/2412.10510)] [[pdf](https://arxiv.org/pdf/2412.10510)]
> **Authors**: Tobias Braun,Mark Rothermel,Marcus Rohrbach,Anna Rohrbach
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 诽谤：多模式专家的动态基于证据的事实检查
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 虚假信息的扩散需要可靠且可扩展的事实检验解决方案。我们向多模式专家（Defame）介绍了动态的基于证据的事实检查，这是一种用于开放域，文本图像索赔验证的模块化的，零射门的MLLM管道。 Defame在六阶段的过程中运行，动态选择工具和搜索深度以提取和评估文本和视觉证据。与先前的仅文本，缺乏解释性或仅依赖参数知识的方法不同，诽谤执行端到端验证，在生成结构化的多模式报告时，在索赔和证据中考虑图像。对流行的基准Verite，Averitec和Mocheg的评估表明，Defame超过了所有以前的方法，将自己确立为新的最新事实检查系统，用于单模式和多模式事实检查。此外，我们引入了一个新的基准测试，SoiperReview24+，其特征是GPT4O知识截止后的主张，以避免数据泄漏。在这里，诽谤极大地超过了GPT的基准链基线，表明时间概括性和实时事实检查的潜力。

### CognitionCapturer: Decoding Visual Stimuli From Human EEG Signal With Multimodal Information 
[[arxiv](https://arxiv.org/abs/2412.10489)] [[cool](https://papers.cool/arxiv/2412.10489)] [[pdf](https://arxiv.org/pdf/2412.10489)]
> **Authors**: Kaifan Zhang,Lihuo He,Xin Jiang,Wen Lu,Di Wang,Xinbo Gao
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: CognitionCapturer：通过多模式信息从人EEG信号中解码视觉刺激
- **领域**: 计算机视觉和模式识别,人工智能,信号处理
- **摘要**: 脑电图（EEG）信号由于其非侵入性性质和在解码视觉刺激中的高时间敏感性而引起了研究人员的极大关注。 However, most recent studies have focused solely on the relationship between EEG and image data pairs, neglecting the valuable ``beyond-image-modality" information embedded in EEG signals. This results in the loss of critical multimodal information in EEG. To address this limitation, we propose CognitionCapturer, a unified framework that fully leverages multimodal data to represent EEG signals. Specifically, CognitionCapturer trains模式专家编码每种模态以从脑电图中提取交叉模式，然后在映射eeg嵌入夹子嵌入空间之前引入一个扩散，然后使用预预性的生成模型，使用的框架可以将视觉刺激带入高度和结构上。通过广泛的实验，我们证明了CognitionCapturer在定性和定量上都优于最先进的方法。代码：https：//github.com/xiaozhangyes/cognitioncapturer。

### CrossVIT-augmented Geospatial-Intelligence Visualization System for Tracking Economic Development Dynamics 
[[arxiv](https://arxiv.org/abs/2412.10474)] [[cool](https://papers.cool/arxiv/2412.10474)] [[pdf](https://arxiv.org/pdf/2412.10474)]
> **Authors**: Yanbing Bai,Jinhua Su,Bin Qiao,Xiaoran Ma
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 用于跟踪经济发展动力学的跨维特授权地理空间 - 智能可视化系统
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 及时，准确的经济数据对于有效的决策至关重要。可以通过多模式传感和分布式计算方面的进步来解决数据及时性和空间分辨率的当前挑战。我们介绍了Sexeconic，这是一种可扩展的系统，用于通过多模式图像和深度学习来跟踪经济动态。它建立在变压器框架的基础上，使用交叉注意力集成了遥感和街道视图图像，而夜间轻度数据则作为弱监督。该系统在县级的经济预测中实现了0.8363的R平方值，并使用分布式计算将处理时间减少到23分钟。其用户友好的设计包括基于VUE3的前端，并带有BAIDU地图可视化和基于Python的后端自动化任务，例如图像下载和预处理。有理由使政策制定者和研究人员拥有有效的资源分配和经济计划工具。

### Enriching Multimodal Sentiment Analysis through Textual Emotional Descriptions of Visual-Audio Content 
[[arxiv](https://arxiv.org/abs/2412.10460)] [[cool](https://papers.cool/arxiv/2412.10460)] [[pdf](https://arxiv.org/pdf/2412.10460)]
> **Authors**: Sheng Wu,Xiaobao Wang,Longbiao Wang,Dongxiao He,Jianwu Dang
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-16
> **comment**: ef:AAAI 2025
- **标题**: 通过文本情感描述进行视觉审计内容来丰富多模式的情感分析
- **领域**: 计算机视觉和模式识别,人工智能,声音,音频和语音处理
- **摘要**: 多模式情感分析（MSA）是一个关键的研究领域，试图通过合并文本，音频和视觉数据来全面地揭示人类情绪。然而，在音频和视频表达式中辨别微妙的情感细微差别构成了一个巨大的挑战，尤其是当各个细分市场的情感极性看起来相似时。在本文中，我们的目标是聚焦音频和视觉方式与情绪相关的属性，以促进在视觉审计场景中细微的情感转变的背景下多模式融合。为此，我们介绍了Deva，这是一个基于文本情感描述的渐进式融合框架。 Deva采用情感描述生成器（EDG）将原始音频和视觉数据传输到文本化的情感描述中，从而扩大了它们的情感特征。然后将这些描述与源数据集成在一起，以产生更丰富的增强功能。此外，DEVA结合了文本引导的渐进式融合模块（TPF），利用不同级别的文本作为核心模式指南。该模块逐渐融合了视觉原语的次要模式，以减轻文本和视觉原语模态之间的差异。与最新模型相比，有关广泛使用情感分析基准数据集的实验结果基准数据集（包括MOSI，MOSEI和CH-SIMS）强调了显着的增强。此外，细粒度的情感实验证实了Deva对微妙的情绪变化的强大灵敏度。

### Geo-LLaVA: A Large Multi-Modal Model for Solving Geometry Math Problems with Meta In-Context Learning 
[[arxiv](https://arxiv.org/abs/2412.10455)] [[cool](https://papers.cool/arxiv/2412.10455)] [[pdf](https://arxiv.org/pdf/2412.10455)]
> **Authors**: Shihao Xu,Yiyang Luo,Wei Shi
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: Geo-llava：一种用于解决元文化学习的几何数学问题的大型多模式模型
- **领域**: 计算机视觉和模式识别,人工智能,计算几何
- **摘要**: 几何数学问题对大语言模型（LLM）构成了重大挑战，因为它们涉及视觉元素和空间推理。当前的方法主要依赖于符号角色意识来解决这些问题。考虑到几何问题解决问题是一个相对新生的领域，具有有限的合适数据集，目前几乎没有解决固体几何问题解决问题的工作，我们通过从中国高中教育网站中采购几何数据来收集几何质疑数据集，称为GEOMATH。它包含可靠的几何问题和答案，并通过准确的推理步骤作为现有平面几何数据集的补偿。此外，我们提出了一个名为Geo-llava的大型多模式（LMM）框架，该框架在训练阶段将检索增强与监督的微调（SFT）结合在训练阶段，称为元训练，并在推理过程中采用了文本学习（ICL）来提高性能。我们使用ICL的微调模型在GEOQA数据集和Geomath数据集的选定问题上分别获得了65.25％和42.36％的最新性能，并具有适当的推理步骤。值得注意的是，我们的模型最初赋予了解决固体几何问题的能力，并支持了合理的固体几何形状描述和解决问题的步骤的生成。我们的研究为进一步探索多模式数学问题解决方案的LLM奠定了基础，尤其是在几何数学问题中。

### Multi-level Matching Network for Multimodal Entity Linking 
[[arxiv](https://arxiv.org/abs/2412.10440)] [[cool](https://papers.cool/arxiv/2412.10440)] [[pdf](https://arxiv.org/pdf/2412.10440)]
> **Authors**: Zhiwei Hu,Víctor Gutiérrez-Basulto,Ru Li,Jeff Z. Pan
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-16
> **comment**: Accepted at KDD'25
- **标题**: 多模式实体链接的多级匹配网络
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 链接（MEL）的多模式实体旨在将多模式上下文中的歧义提及与多模式知识库中的相应实体联系起来。大多数现有的MEL方法基于表示学习或视觉和语言的预训练机制，用于探索多种方式之间的互补作用。但是，这些方法受到了两个局限性。一方面，他们忽略了考虑来自相同模式的负样本的可能性。另一方面，它们缺乏捕获双向跨模式相互作用的机制。为了解决这些问题，我们为多模式实体链接（M3EL）提出了一个多级匹配网络。具体而言，M3EL由三个不同的模块组成：（i）多模式特征提取模块，该模块使用多模式编码器提取特定于模态的表示，并引入模式内对比度学习子模块，以获得基于Uni-Mododal差异的更好的歧视性嵌入； （ii）一个模式内匹配网络模块，其中包含两个匹配粒度的级别：粗粒粒度的全球到全球和细粒度的全局到局部，以实现局部和全球级别的模式内相互作用； （iii）应用双向策略，文本到视觉和视觉到文本匹配的跨模式匹配网络模块，以实现双向交叉模式交互。在Wikimel，Richpediamel和Wikidiverse数据集上进行的广泛实验表明，与最先进的基线相比，M3EL的出色表现。

### Automatic Image Annotation for Mapped Features Detection 
[[arxiv](https://arxiv.org/abs/2412.10438)] [[cool](https://papers.cool/arxiv/2412.10438)] [[pdf](https://arxiv.org/pdf/2412.10438)]
> **Authors**: Maxime Noizet,Philippe Xu,Philippe Bonnifait
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-16
> **comment**: ef:2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024), Oct 2024, Abu Dhabi, United Arab Emirates
- **标题**: 映射功能检测的自动图像注释
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 检测道路功能是自动驾驶和本地化的关键推动力。例如，在道路环境中广泛使用的极点可靠检测可以改善本地化。现代深度学习的感知系统需要大量注释的数据。自动注释避免了耗时和昂贵的手动注释。由于自动方法容易出现错误，因此管理注释不确定性对于确保正确学习过程至关重要。在同一数据集上融合多个注释源可以是减少错误的有效方法。这不仅提高了注释的质量，而且还提高了感知模型的学习。在本文中，我们考虑了图像中三种自动注释方法的融合：高精度向量图与LIDAR，图像分割和激光雷达分割的特征投影。我们的实验结果表明，通过对手动注释的图像进行比较评估，多模式自动注释对POL检测的显着优势。最后，所得的多模式融合用于使用未标记的数据微调对象检测模型，以用于极碱基检测，以显示通过增强网络专业化实现的总体改进。该数据集公开可用。

### COEF-VQ: Cost-Efficient Video Quality Understanding through a Cascaded Multimodal LLM Framework 
[[arxiv](https://arxiv.org/abs/2412.10435)] [[cool](https://papers.cool/arxiv/2412.10435)] [[pdf](https://arxiv.org/pdf/2412.10435)]
> **Authors**: Xin Dong,Sen Jia,Hongyu Xiong
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: COEF-VQ：通过级联的多模式LLM框架具有成本效益的视频质量理解
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近，随着最近的多模式大语言模型（MLLM）技术的出现，已经有可能利用其视频理解能力在不同的分类任务上。实际上，如果我们需要在线部署MLLM，我们将面临GPU资源的巨大要求。在本文中，我们提出了COEF-VQ，这是一种新颖的MLLM框架，以对Tiktok进行更好的视频质量理解。为此，我们首先提出了一个MLLM融合所有视觉，文本和音频信号，然后以轻质模型作为预滤波阶段和MLLM作为罚款阶段，开发一个级联框架，从而大大减少了GPU资源的需求，同时仅保留MLLM的绩效。为了证明COEF-VQ的有效性，我们将这个新框架部署在Tiktok的视频管理平台（VMP）上，并对与视频质量理解有关的两个内部任务进行了一系列详细的实验。我们表明，COEF-VQ在这两个任务中带来了限制资源消耗的大量绩效提高。

### Personalized and Sequential Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2412.10419)] [[cool](https://papers.cool/arxiv/2412.10419)] [[pdf](https://arxiv.org/pdf/2412.10419)]
> **Authors**: Ofir Nabati,Guy Tennenholtz,ChihWei Hsu,Moonkyung Ryu,Deepak Ramachandran,Yinlam Chow,Xiang Li,Craig Boutilier
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-16
> **comment**: Link to PASTA dataset: https://www.kaggle.com/datasets/googleai/pasta-data
- **标题**: 个性化和顺序的文本形象生成
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,系统与控制
- **摘要**: 我们解决了个性化的交互式文本对图像（T2I）生成的问题，设计了增强学习（RL）代理，该代理通过一系列及时的扩展来迭代地改善用户为用户生成的图像。使用人类评估者，我们创建了一个新型的顺序偏好数据集，我们将利用该数据集以及大型开源（非序列）数据集。我们使用EM策略构建用户质量和用户选择模型，并确定不同的用户偏好类型。然后，我们利用大型多模式模型（LMM）和基于价值的RL方法为用户提出个性化和多样化的迅速扩展。我们的个性化和顺序的文本到图像代理（面食）扩展了具有个性化多转弯功能的T2I模型，促进了协作共同创建，并解决了用户意图中的不确定性或不确定性。我们使用人类评估者评估面食，与基线方法相比显示出显着改善。我们还发布了顺序评估者数据集和模拟用户评估者的交互，以支持个性化的多转化T2i生成中的未来研究。

### UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities 
[[arxiv](https://arxiv.org/abs/2412.10372)] [[cool](https://papers.cool/arxiv/2412.10372)] [[pdf](https://arxiv.org/pdf/2412.10372)]
> **Authors**: Muhammad Uzair Khattak,Shahina Kunhimon,Muzammal Naseer,Salman Khan,Fahad Shahbaz Khan
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: Code, models and demo available at https://github.com/mbzuai-oryx/UniMed-CLIP
- **标题**: 未染色的纸条：朝着统一的图像文本预处理范式，用于多种医学成像方式
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过对比度学习训练的视觉语言模型（VLM）在自然图像任务中取得了显着的成功。但是，由于缺乏公开访问的大规模医学图像文本数据集，它们在医疗领域的应用仍受到限制。现有的医疗VLM可以训练封闭式专有的或相对较小的开源数据集，但不能很好地概括。同样，大多数模型仍然针对单个或有限数量的医学成像域，再次限制了其对其他方式的适用性。为了解决这一差距，我们介绍了一个未涉及的大规模开源多模式医学数据集，其中包括六种不同成像方式的530万个图像文本对：X射线，CT，MRI，超声，病理，病理学和底面。使用数据收集框架开发了Unimed，该框架利用大型语言模型（LLM）将特定于模态的分类数据集转换为图像文本格式，同时将现有的图像文本数据从医疗域中纳入，从而促进了可扩展的VLM预处理。我们使用Unimed训练了Unimed-CLIP，这是一种统一的VLM，用于六种模式，这些模式显着优于现有的通才VLM，并匹配了特定于模式的医学VLM，在零击评估中取得了显着的收益。例如，无染色的CLIP通过绝对增益为+12.61，对生物胶囊（对专有数据训练）进行改进，平均21个数据集，同时使用3倍的培训数据。为了促进未来的研究，我们在https://github.com/mbzuai-orex/unimed-clip上发布未结合的数据集，培训代码和模型。

### Apollo: An Exploration of Video Understanding in Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2412.10360)] [[cool](https://papers.cool/arxiv/2412.10360)] [[pdf](https://arxiv.org/pdf/2412.10360)]
> **Authors**: Orr Zohar,Xiaohan Wang,Yann Dubois,Nikhil Mehta,Tong Xiao,Philippe Hansen-Estruch,Licheng Yu,Xiaofang Wang,Felix Juefei-Xu,Ning Zhang,Serena Yeung-Levy,Xide Xia
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: https://apollo-lmms.github.io
- **标题**: Apollo：大型多模型中对视频理解的探索
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 尽管将视频感知能力迅速整合到大型多模型模型（LMM）中，但推动视频理解的基本机制仍然很少了解。因此，在没有适当的理由或分析的情况下，该领域的许多设计决策都是做出的。培训和评估此类模型的高计算成本，再加上有限的开放研究，阻碍了视频LMM的发展。为了解决这个问题，我们提出了一项全面的研究，该研究有助于发现什么有效地推动了LMM中的视频理解。我们首先要批判性地检查与视频LMM研究和发现缩放一致性相关的高计算要求的主要贡献者，其中设计和培训决策在较小的型号和数据集（最高尺寸）上有效地转移到较大的模型中。利用这些见解，我们探索了视频LMM的许多特定于视频特定方面，包括视频采样，体系结构，数据组成，培训时间表等。例如，我们证明了训练期间的FPS采样比均匀的框架采样非常可取，哪些视觉编码器最适合视频表示。在这些发现的指导下，我们介绍了阿波罗（Apollo），这是一个最先进的LMM家族，可在不同的模型尺寸上实现出色的性能。我们的型号可以有效地感知长时间的视频，而Apollo-3B在LongvideObench上的表现优于现有的$ 7 $ B型号，令人印象深刻的55.1。 Apollo-7b是最先进的，而MLVU为70.9，而Video-MME则是63.3。

### Robust image classification with multi-modal large language models 
[[arxiv](https://arxiv.org/abs/2412.10353)] [[cool](https://papers.cool/arxiv/2412.10353)] [[pdf](https://arxiv.org/pdf/2412.10353)]
> **Authors**: Francesco Villani,Igor Maljkovic,Dario Lazzaro,Angelo Sotgiu,Antonio Emanuele Cinà,Fabio Roli
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 使用多模式大型语言模型的强大图像分类
- **领域**: 计算机视觉和模式识别,密码学和安全,机器学习
- **摘要**: 深度神经网络容易受到对抗性例子的影响，即精心制作的输入样本，这些样本可能会导致模型以高信心做出错误的预测。为了减轻这些漏洞，已经提出了基于对抗性训练和基于检测的防御能力，以提前增强模型。但是，这些方法中的大多数都集中在单个数据模式上，探讨了视觉模式与输入的文本描述之间的关系。在本文中，我们提出了一种新颖的防御，多挡板，旨在将这些防御与多模式信息相结合和补充，以进一步增强其鲁棒性。当在输入的文本和视觉表示之间没有对齐时，多模式的多模式大型语言模型可以检测对抗性示例，并避免不确定分类。使用健壮和非舒适图像分类模型对CIFAR-10和Imagenet数据集进行了广泛的评估，表明可以轻松地集成多屏幕以检测和拒绝对抗性示例，从而优于原始防御。

### A dual contrastive framework 
[[arxiv](https://arxiv.org/abs/2412.10348)] [[cool](https://papers.cool/arxiv/2412.10348)] [[pdf](https://arxiv.org/pdf/2412.10348)]
> **Authors**: Yuan Sun,Zhao Zhang,Jorge Ortiz
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 双重对比框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在当前的多模式任务中，模型通常会冻结编码器和解码器，同时将中间层调整为特定于任务的目标，例如区域字幕。区域级的视觉理解给大型视觉语言模型带来了重大挑战。虽然有限的空间意识是一个已知的问题，但尤其是粗粒的预处理会加剧优化潜在表示以进行有效的编码器对准器的困难。我们提出了AlignCap，这是一个框架，旨在通过潜在空间的细粒对齐来增强区域级别的理解。我们的方法引入了一个新型的潜在特征改进模块，该模块增强了条件潜在的空间表示形式，以提高区域级别的字幕性能。我们还提出了一种创新的对齐策略，即语义空间对齐模块，从而提高了多模式表示的质量。此外，我们在两个模块中以新颖的方式结合对比度学习，以进一步增强区域级字幕性能。为了解决空间局限性，我们采用一般对象检测方法（上帝）方法作为数据预处理管道，从而增强了区域层面的空间推理。广泛的实验表明，我们的方法显着改善了各种任务的区域级字幕性能

### Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining 
[[arxiv](https://arxiv.org/abs/2412.10342)] [[cool](https://papers.cool/arxiv/2412.10342)] [[pdf](https://arxiv.org/pdf/2412.10342)]
> **Authors**: Zhiqi Ge,Juncheng Li,Xinglei Pang,Minghe Gao,Kaihang Pan,Wang Lin,Hao Fei,Wenqiao Zhang,Siliang Tang,Yueting Zhuang
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 虹膜：以适应性重点和自我限制打破GUI的复杂性
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 越来越多地利用数字代理在交互式数字环境（例如网页，软件应用程序和操作系统）中自动化任务。尽管基于文本的代理（LLMS）构建的基于文本的代理由于平台特定的API而经常需要频繁更新，但通过直接与图形用户界面（GUIS）进行交互，利用多模式大语言模型（MLLM）的视觉试剂提供了增强的适应性。但是，这些代理在视觉感知中面临重大挑战，尤其是在处理高分辨率，视觉上复杂的数字环境时。本文介绍了Iris，这是一种基础视觉剂，通过两项关键创新来解决这些挑战：信息敏感的种植（ISC）和自我缩写双重学习（SRDL）。 ISC使用边缘检测算法动态识别并优先考虑视觉密集区域，从而通过将更多的计算资源分配给具有较高信息密度的区域，从而实现有效的处理。 SRDL通过利用双学习环路来增强代理人处理复杂任务的能力，在这种环路中，改进（描述UI元素）加强接地（定位元素），反之亦然，而无需其他带注释的数据。经验评估表明，IRIS仅具有850K GUI注释的多个基准测试中实现最先进的性能，使用10倍更多的培训数据的方法优于表现。这些改进进一步转化为Web和OS代理下游任务的显着增长。

### BrushEdit: All-In-One Image Inpainting and Editing 
[[arxiv](https://arxiv.org/abs/2412.10316)] [[cool](https://papers.cool/arxiv/2412.10316)] [[pdf](https://arxiv.org/pdf/2412.10316)]
> **Authors**: Yaowei Li,Yuxuan Bian,Xuan Ju,Zhaoyang Zhang,Ying Shan,Yuexian Zou,Qiang Xu
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: WebPage available at https://liyaowei-stu.github.io/project/BrushEdit/
- **标题**: Brushedit：多合一图像介绍和编辑
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 使用基于反转和基于指令的方法的扩散模型的开发，图像编辑已显着提高。但是，由于反转噪声的结构化性质，当前基于反转的方法在大型修改（例如添加或删除对象）方面遇到了困难，这阻碍了实质性的变化。同时，基于指令的方法通常将用户限制为黑框操作，从而限制了指定编辑区域和强度的直接交互。为了解决这些局限性，我们提出了Brushedit，这是一种新型的基于介绍的指导引导的图像编辑范式，该范式利用多模式大型语言模型（MLLMS）和图像介入模型来启用自动，用户友好型和交互式的自由形式指令编辑。具体而言，我们设计了一个系统，可以通过在代理合件框架中集成MLLM和一个双支流图像介绍模型来实现自由形式的指令编辑，以执行编辑类别分类，主要对象识别，掩码获取和编辑区域的编辑区域。广泛的实验表明，我们的框架有效地结合了MLLM和介入模型，在包括掩码区域保存和编辑效应相干的七个指标上实现了卓越的性能。

### DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding 
[[arxiv](https://arxiv.org/abs/2412.10302)] [[cool](https://papers.cool/arxiv/2412.10302)] [[pdf](https://arxiv.org/pdf/2412.10302)]
> **Authors**: Zhiyu Wu,Xiaokang Chen,Zizheng Pan,Xingchao Liu,Wen Liu,Damai Dai,Huazuo Gao,Yiyang Ma,Chengyue Wu,Bingxuan Wang,Zhenda Xie,Yu Wu,Kai Hu,Jiawei Wang,Yaofeng Sun,Yukun Li,Yishi Piao,Kang Guan,Aixin Liu,Xin Xie,Yuxiang You,Kai Dong,Xingkai Yu,Haowei Zhang,Liang Zhao, et al. (2 additional authors not shown)
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: DeepSeek-VL2：Experts视觉语言模型的混合物，用于高级多模式理解
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 我们提出了DeepSeek-vl2，这是一系列高级的大型专家（MOE）视觉模型，通过两个关键的主要升级，可以显着改善其前身DeepSeek-vl。对于视觉组件，我们结合了一种动态瓷砖视觉编码策略，旨在处理具有不同纵横比的高分辨率图像。对于语言组件，我们利用多头潜在注意机制来利用DeepSeekmoe模型，该模型将键值缓存压缩到潜在的矢量中，以实现有效的推理和高吞吐量。 DeepSeek-VL2经过改进的视觉数据集培训，在各种任务中展示了较高的功能，包括但不限于视觉询问答案，光学角色识别，文档/表格/图表/图表的理解以及视觉接地。我们的模型系列由三个变体组成：DeepSeek-Vl2微型，DeepSeek-Vl2-Small和DeepSeek-Vl2，分别为1.0B，2.8B和4.5B激活参数。与现有的基于开源密度和基于MOE的模型相比，DeepSeek-VL2具有相似或更少的激活参数的竞争性或最先进的性能。代码和预培训模型可在https://github.com/deepseek-ai/deepseek-vl2上公开访问。

### Prompt-Guided Mask Proposal for Two-Stage Open-Vocabulary Segmentation 
[[arxiv](https://arxiv.org/abs/2412.10292)] [[cool](https://papers.cool/arxiv/2412.10292)] [[pdf](https://arxiv.org/pdf/2412.10292)]
> **Authors**: Yu-Jhe Li,Xinyang Zhang,Kun Wan,Lantao Yu,Ajinkya Kale,Xin Lu
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: 17 pages. Work done during 2023 summer and has been released
- **标题**: 及时引导的面具提案，以进行两阶段开放式视频分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们应对开放式胶卷细分的挑战，在该挑战中，我们需要使用文本提示作为我们的输入来识别不同环境中各个类别的对象。为了克服这一挑战，现有方法经常使用剪辑等多模式模型，这些模型将图像和文本特征结合在共享的嵌入空间中，以弥合有限和广泛的词汇识别之间的差距，从而实现了两阶段的方法：在第一阶段，面具发电机采用输入图像来产生蒙版建议，并在第二阶段中基于第二阶段的目标，该目标是基于Quy of the Quey of the Query the Query the Query the Query the Query the Query the Query the QUERY。但是，预期的目标掩码可能不存在于生成的蒙版建议中，这导致了意外的输出掩码。在我们的工作中，我们提出了一种新颖的方法，名为迅速引导的面具提案（PMP），其中掩码生成器发表输入文本提示并生成以这些提示为指导的掩模。与没有输入提示的没有输入提示的蒙版提案相比，PMP生成的掩码与输入提示更好。为了实现PMP，我们设计了文本令牌和查询令牌之间的跨注意机制，该机制能够在每次解码后能够生成及时引导的掩模建议。我们将PMP与几项现有作品结合在一起，该作品采用了基于查询的分割主链，并且在五个基准数据集上进行的实验证明了这种方法的有效性，显示了对当前两阶段模型的显着改善（在MIOU方面，绝对性能增长1％〜3％）。这些基准测试的性能的稳定改善表明我们提出的轻巧及时感知方法的有效概括。

### Learning Complex Non-Rigid Image Edits from Multimodal Conditioning 
[[arxiv](https://arxiv.org/abs/2412.10219)] [[cool](https://papers.cool/arxiv/2412.10219)] [[pdf](https://arxiv.org/pdf/2412.10219)]
> **Authors**: Nikolai Warner,Jack Kolb,Meera Hahn,Vighnesh Birodkar,Jonathan Huang,Irfan Essa
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 从多模式调节学习复杂的非刚性图像编辑
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们专注于将给定的人（特别是一个人的单一图像）插入新的场景。我们的方法建立在稳定的扩散之上，可产生自然的图像，同时具有文本和姿势高度控制。为了实现这一目标，我们需要对成对的图像进行训练，这是第一个与人一起参考图像，第二个“目标图像”显示了同一个人（带有不同的姿势，可能在不同的背景中）。另外，我们需要一个文本标题，以描述新姿势相对于参考图像中的姿势。在本文中，我们呈现符合此标准的新型数据集，我们使用以人为中心和动作范围的视频和使用多模式LLM来自动总结文本字幕的人类姿势差异。我们证明，身份保存是“野外”场景中更具挑战性的任务，尤其是人与物体之间存在相互作用的场景。结合了噪音标题的弱监督和鲁棒的2D姿势，可以提高人对象相互作用的质量。

### WordVIS: A Color Worth A Thousand Words 
[[arxiv](https://arxiv.org/abs/2412.10155)] [[cool](https://papers.cool/arxiv/2412.10155)] [[pdf](https://arxiv.org/pdf/2412.10155)]
> **Authors**: Umar Khan,Saifullah,Stefan Agne,Andreas Dengel,Sheraz Ahmed
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: WordVis：价值一千个字的颜色
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 文档分类被认为是自动文档处理系统中的关键要素。近年来，多模式方法在文档分类中变得越来越流行。尽管有所改善，但由于对大量培训数据和广泛的计算能力的要求，这些方法在行业中的利用不足。在本文中，我们试图通过将文本功能直接嵌入视觉空间来解决这些问题，从而允许基于图像的分类器使用文档分类中的小规模数据集获得最新的结果。为了评估我们方法对有限数据产生的视觉特征的功效，我们在标准数据集烟草-3482上进行了测试。我们的实验显示了基于图像的分类器的巨大改善，使用RESNET50在没有文档预培训的情况下实现了4.64％的改善。它还为烟草-3482数据集的最佳精度设置了新记录，使用基于图像的DocxClassifier，没有文档预培训，得分为91.14％。该方法的简单性，其资源需求和随后的结果为其在工业用例中的使用提供了良好的前景。

### Timealign: A multi-modal object detection method for time misalignment fusing in autonomous driving 
[[arxiv](https://arxiv.org/abs/2412.10033)] [[cool](https://papers.cool/arxiv/2412.10033)] [[pdf](https://arxiv.org/pdf/2412.10033)]
> **Authors**: Zhihang Song,Lihui Peng,Jianming Hu,Danya Yao,Yi Zhang
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: 8 pages, 3 figures
- **标题**: timealign：在自动驾驶中进行时间错位融合的多模式对象检测方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于它们更好地使用了来自不同传感器的互补数据，因此多模式感知方法在自主驾驶场中蓬勃发展。这种方法取决于传感器之间的校准和同步，以获取准确的环境信息。已经有关于自主驾驶对象检测过程中的空间对准鲁棒性的研究，但是，对时间对齐的研究相对较少。实际上，在实验中，激光点云对于实时数据传输更具挑战性，我们的研究使用LIDAR的历史框架在LIDAR数据滞后时更好地对齐功能。我们设计了一个timealign模块，以预测和将激光雷达功能与观察结果结合在一起，以解决基于SOTA GraphBev框架的时间不对对准。

### GT23D-Bench: A Comprehensive General Text-to-3D Generation Benchmark 
[[arxiv](https://arxiv.org/abs/2412.09997)] [[cool](https://papers.cool/arxiv/2412.09997)] [[pdf](https://arxiv.org/pdf/2412.09997)]
> **Authors**: Sitong Su,Xiao Cai,Lianli Gao,Pengpeng Zeng,Qinhong Du,Mengqi Li,Heng Tao Shen,Jingkuan Song
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: GT23D板凳：全面的一般文本到3D代基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 一般文本到3D（GT23D）的最新进展很大。但是，由于数据集和指标中的问题，缺乏基准阻碍了系统的评估和进步：1）最大的3D数据集OBJAVERSE遭受省略的注释，混乱和低质量。 2）现有指标仅评估文本图像对齐，而无需考虑3D级质量。为此，我们是第一个为GT23D提供全面的基准的人，称为GT23D板凳，由：1）400K高保真和组织良好的3D数据集组成，该数据集通过系统性的注释 - 组织滤波器管道来策划OBJAVESSION中的问题； 2）全面的3D感知评估指标，该指标涵盖10个明确定义的指标，彻底考虑了GT23D的多维度。值得注意的是，GT23D板凳具有三个属性：1）多模式注释。我们的数据集用64视图深度图，正常地图，渲染图像和粗到限制字幕的每个3D对象注释每个3D对象。 2）整体评估维度。我们的指标分为a）文本3D对齐度衡量文本对齐，并具有多粒性视觉3D表示； b）3D视觉质量考虑纹理保真度，多视图一致性和几何形状正确性。 3）有价值的见解。我们深入研究了当前的GT23D基线在不同的评估维度上的性能，并提供有见地的分析。广泛的实验表明，我们的注释和指标与人类偏好保持一致。

### Visual Object Tracking across Diverse Data Modalities: A Review 
[[arxiv](https://arxiv.org/abs/2412.09991)] [[cool](https://papers.cool/arxiv/2412.09991)] [[pdf](https://arxiv.org/pdf/2412.09991)]
> **Authors**: Mengmeng Wang,Teli Ma,Shuo Xin,Xiaojun Hou,Jiazheng Xing,Guang Dai,Jingdong Wang,Yong Liu
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 视觉对象跟踪各种数据模式：评论
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视觉对象跟踪（fot）是计算机视觉中的一个有吸引力且重要的研究领域，旨在识别和跟踪目标对象是任意和类别的视频序列中的特定目标。投票技术可以在各种情况下应用，处理RGB，热红外和点云等不同方式的数据。此外，由于没有一个传感器能够处理所有动态和不同环境，因此还研究了多模式的vot。本文介绍了单模式和多模式投票的最新进展，尤其是深度学习方法的全面调查。具体而言，我们首先回顾了三种类型的主流单模式投票，包括RGB，热红外和点云跟踪。特别是，我们结论了四个广泛使用的单模式框架，将它们的模式抽象并分类现有的继承。然后，我们总结了四种多模式投票，包括RGB深度，RGB-Thermal，RGB-LIDAR和RGB语言。此外，对比较结果进行了大量对讨论方式的基准。最后，我们提供了建议和有见地的观察，激发了这种快速发展的文献的未来发展。

### IQViC: In-context, Question Adaptive Vision Compressor for Long-term Video Understanding LMMs 
[[arxiv](https://arxiv.org/abs/2412.09907)] [[cool](https://papers.cool/arxiv/2412.09907)] [[pdf](https://arxiv.org/pdf/2412.09907)]
> **Authors**: Sosuke Yamao,Natsuki Miyahara,Yuki Harazono,Shun Takeuchi
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: The first and second authors contributed equally to this work
- **标题**: iQVIC：在上下文中，问题自适应视觉压缩机长期视频理解LMMS
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着视频数据的复杂性日益增加以及对更有效的长期时间理解的需求，现有的长期视频理解方法通常无法准确捕获和分析扩展视频序列。这些方法通常很难在更长的时间内保持性能，并处理视频内容中的复杂依赖性。为了解决这些局限性，我们为长期视频理解提出了一个简单而有效的大型多模型模型框架，该框架结合了一种新型的视觉压缩机，即中文，问题自适应视觉压缩机（IQVIC）。由人类选择性关注和内在记忆机制启发的关键思想是引入一种新颖的视觉压缩机，并结合了有效的内存管理技术，以增强长期的视频问题回答。我们的框架利用IQVIC是一种基于变压器的视觉压缩机，可实现问题条件的内部文化压缩，这与依赖完整视频视觉功能的现有方法不同。这有选择地提取相关信息，大大降低了内存令牌要求。通过基于Infinibench的新数据集进行长期视频理解的广泛实验，以及用于现有方法评估的标准基准测试，我们证明了我们所提出的IQVIC框架的有效性及其优于视频理解准确性和记忆效率的最先进方法。

### MulSMo: Multimodal Stylized Motion Generation by Bidirectional Control Flow 
[[arxiv](https://arxiv.org/abs/2412.09901)] [[cool](https://papers.cool/arxiv/2412.09901)] [[pdf](https://arxiv.org/pdf/2412.09901)]
> **Authors**: Zhe Li,Yisheng He,Lei Zhong,Weichao Shen,Qi Zuo,Lingteng Qiu,Zilong Dong,Laurence Tianruo Yang,Weihao Yuan
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: MULSMO：双向控制流程多模式的风格化运动产生
- **领域**: 计算机视觉和模式识别
- **摘要**: 在遵守给定内容提示的同时，生成符合目标样式的运动序列需要适应内容和样式。在现有方法中，信息通常仅从样式流向内容，这可能会导致样式和内容之间的冲突，从而损害集成。不同的是，在这项工作中，我们在样式和内容之间建立了双向控制流，还将样式调整为内容，在这种情况下，样式符合条件的碰撞得到了减轻，并且样式的动力学在集成中得到了更好的保存。此外，我们将风格化运动的产生从一种模式（即样式运动）扩展到多种方式，包括通过对比度学习的文本和图像，从而导致对运动产生的灵活风格控制。广泛的实验表明，我们的方法显着优于不同数据集的先前方法，同时也可以实现多模式信号控制。我们方法的代码将公开可用。

### Building a Multi-modal Spatiotemporal Expert for Zero-shot Action Recognition with CLIP 
[[arxiv](https://arxiv.org/abs/2412.09895)] [[cool](https://papers.cool/arxiv/2412.09895)] [[pdf](https://arxiv.org/pdf/2412.09895)]
> **Authors**: Yating Yu,Congqi Cao,Yueran Zhang,Qinyi Lv,Lingtong Min,Yanning Zhang
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: Accepted by AAAI 2025
- **标题**: 用剪辑构建一个多模式时空专家，以零拍动识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 零射击动作识别（ZSAR）需要协作多模式时空的理解。然而，鉴于从视觉和文本角度捕获基本的时间动力学方面的固有限制，ZSAR的填充剪辑直接产生了次优的性能，尤其是在遇到具有精细颗粒时空差异的新型动作时。在这项工作中，我们提出了时空动态二人组（STDD），这是一种基于夹的新型框架，旨在协同理解多模式时空动力学。对于视觉方面，我们提出了一个有效的时空交叉注意，该时空互相注意在空间注意力之前和之后使用简单但有效的操作灵活地捕获时空动力学，而无需添加其他参数或增加计算复杂性。对于语义方面，我们通过全面构建动作语义知识图（askG）来得出细微的文本提示来进行时空文本增强。 AskG基于将动作分解为空间外观和时间运动的想法，详细阐述了静态和动态概念及其相互关系。在训练阶段，框架级的视频表示形式与及时的细微差别文本表示精心排列，这些表示由冷冻剪辑中的视频表示同时调节，以增强通用性。广泛的实验验证了我们的方法的有效性，该方法在挑战性的ZSAR设置下始终超过了流行的视频基准（即动力学-600，UCF101和HMDB51）的最新方法。

### Selective State Space Memory for Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2412.09875)] [[cool](https://papers.cool/arxiv/2412.09875)] [[pdf](https://arxiv.org/pdf/2412.09875)]
> **Authors**: Chee Ng,Yuen Fung
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 大型视觉模型的选择性状态空间记忆
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型视觉模型（LVLM）在广泛的多模式任务中表现出了出色的性能。但是，针对特定领域应用的这些模型进行微调仍然是一个计算密集的挑战。本文介绍了状态空间内存集成（SSMI），这是一种有效的LVLM微调方法的新方法。通过将基于MAMBA的轻质状态空间模块集成到LVLM体系结构中，SSMI可以有效地捕获长距离依赖性并注入特定于任务的视觉和顺序模式。与传统的微调方法不同，SSMI仅需要更新模型参数的一小部分，从而使其在计算上有效且可扩展。基准数据集（包括可可字幕，VQA和FlickR30K）上的实验表明，SSMI在保持稳健性和概括能力的同时，实现了最先进的性能。全面的分析进一步验证了SSMI在效率，适应性和可解释性方面的优势，将其定位为对大规模视觉模型进行微调的令人信服的解决方案。

### Dynamic Cross-Modal Alignment for Robust Semantic Location Prediction 
[[arxiv](https://arxiv.org/abs/2412.09870)] [[cool](https://papers.cool/arxiv/2412.09870)] [[pdf](https://arxiv.org/pdf/2412.09870)]
> **Authors**: Liu Jing,Amirul Rahman
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 稳健语义位置预测的动态跨模式对齐
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式社交媒体帖子中的语义位置预测是一项至关重要的任务，其中具有个性化服务和人类流动性分析中的应用程序。本文介绍了\ textit {上下文化视觉语言对齐（COVLA）}，这是一个歧视框架，旨在解决本任务中固有的上下文歧义和情态差异的挑战。 Covla利用上下文对齐模块（CAM）来增强跨模式特征对齐和跨模式融合模块（CMF），以动态整合文本和视觉信息。在基准数据集上进行的广泛实验表明，COVLA显着胜过最先进的方法，在准确度上获得了2.3 \％的提高，而F1得分的提高了2.5 \％。消融研究验证了CAM和CMF的贡献，而人类评估则突出了预测的上下文相关性。此外，鲁棒性分析表明，Covla在嘈杂条件下保持高性能，使其成为现实世界应用的可靠解决方案。这些结果强调了Covla在推进语义位置预测研究中的潜力。

### DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for Semi-Supervised Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2412.12492)] [[cool](https://papers.cool/arxiv/2412.12492)] [[pdf](https://arxiv.org/pdf/2412.12492)]
> **Authors**: Qingtao Pan,Wenhao Qiao,Jingjiao Lou,Bing Ji,Shuo Li
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: Duss：半监督医学图像分割的双语语义相似性观察视觉模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 半监督的医学图像细分（SSMIS）使用一致性学习来正规化模型培训，从而减轻了像素手动注释的负担。但是，它通常受到低质量伪标签的错误监督。视觉语言模型（VLM）通过引入文本及时引导的多模式监督信息，具有增强伪标签的巨大潜力。然而，它面临跨模式问题：获得的消息往往与多个目标相对应。为了解决上述问题，我们提出了针对SSMIS的双语语义相似性VLM（dusss）。具体而言，1）双重对比学习（DCL）旨在通过捕获跨模态的每种模态和语义相关性的内在表示来提高跨模式语义一致性。 2）为了鼓励学习多种语义对应关系，提出了语义相似性策略（SSS），并将其注入DCL中的每个对比度学习过程中，通过基于分布的不确定性水平来监督语义相似性。此外，一种新型的基于VLM的SSMIS网络旨在补偿伪标签的质量缺陷。它利用验证的VLM来生成文本及时指导的监督信息，完善伪标签以更好地一致性正则化。实验结果表明，在三个公共数据集（QATA-COV19，BM-SEEG和MONUSEG）上，我们的DUSS以82.52％，74.61％和78.03％的骰子实现了出色的性能。

### PromptDet: A Lightweight 3D Object Detection Framework with LiDAR Prompts 
[[arxiv](https://arxiv.org/abs/2412.12460)] [[cool](https://papers.cool/arxiv/2412.12460)] [[pdf](https://arxiv.org/pdf/2412.12460)]
> **Authors**: Kun Guo,Qiang Ling
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted by AAAI 2025
- **标题**: 提示：带有激光雷达提示的轻量级3D对象检测框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 多相机3D对象检测旨在使用多个摄像机在3D空间中检测和定位对象，由于其成本效益权衡，这引起了更多的关注。但是，这些方法通常在缺乏由摄像机的自然弱点引起的缺乏准确的深度估计而困难。最近，已经提出了用于解决此问题的3D对象检测方法的多模式融合和知识蒸馏方法，该问题在训练阶段耗时，对记忆成本不友好。鉴于此，我们提出了提示件，这是一个轻巧但有效的3D对象检测框架，该框架是由2D基础模型中迅速学习的成功所激发的。我们提出的框架提示DET包括两个积分组件：一个基于摄像头的检测模块，以BEVDET和BEVDEPTH之类的模型为例，以及一个激光辅助的提示器。 LIDAR辅助的提示器将激光点作为互补信号，并具有最小的额外训练参数。值得注意的是，由于我们的迅速设计，我们的框架是灵活的，它不仅可以用作轻巧的多模式融合方法，而且还可以用作推理阶段中3D对象检测的仅摄像头方法。对Nuscenes的广泛实验验证了所提出的及时数据的有效性。作为一个多模式检测器，及时DET与仅相机基线相比，最多最多的22.8 \％和21.1 \％的NDS，少于2 \％的额外参数。没有激光雷达点，提示插图仍然可以实现最多的2.4 \％地图和4.0 \％nds的改进，几乎对摄像机检测推理时间没有影响。

### LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters through Modality Linear Representation-Steering 
[[arxiv](https://arxiv.org/abs/2412.12359)] [[cool](https://papers.cool/arxiv/2412.12359)] [[pdf](https://arxiv.org/pdf/2412.12359)]
> **Authors**: Jinhe Bi,Yujun Wang,Haokun Chen,Xun Xiao,Artur Hecker,Volker Tresp,Yunpu Ma
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: LLAVA转向：通过模态表示较少参数的视觉说明调整，少500倍。
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 通过将视觉表示形式集成到大型语言模型（LLMS）中，多模式大型语言模型（MLLM）具有显着高级的视觉任务。从LLMS继承的文本模式使MLLM具有诸如以下教学和文本学习之类的能力。相反，视觉方式通过利用丰富的语义内容，空间信息和接地功能来增强下游任务的性能。这些内在模式在各种视觉任务中协同起作用。我们的研究最初揭示了这些方式之间的持续不平衡，文字在视觉教学调整过程中通常主导产出产生。当同时使用完整的微调和参数效率微调（PEFT）方法时，这种不平衡会发生。然后，我们发现重新平衡这些方式可以显着减少所需训练参数的数量，从而激发了进一步优化视觉教学调整的方向。我们介绍了模态线性表示（Mores）以实现目标。 Mores有效地重新平衡了整个模型中的内在方式，其中关键思想是通过每个模型层的视觉子空间中的线性变换来引导视觉表示。为了验证我们的解决方案，我们组成了LLAVA转向，这是一套与拟议的Mores方法集成的模型。评估结果表明，组成的LLAVA转向模型平均需要比LORA需求少的500倍，同时仍能在三个视觉基准和八个视觉提问的任务中达到可比的性能。最后，我们介绍了LLAVA转向工厂，这是一个内部开发的平台，它使研究人员能够通过基于组件的架构来快速自定义各种MLLM，以无缝整合最先进的模型，并评估其内在方式不平衡。

### OmniPrism: Learning Disentangled Visual Concept for Image Generation 
[[arxiv](https://arxiv.org/abs/2412.12242)] [[cool](https://papers.cool/arxiv/2412.12242)] [[pdf](https://arxiv.org/pdf/2412.12242)]
> **Authors**: Yangyang Li,Daqing Liu,Wu Liu,Allen He,Xinchen Liu,Yongdong Zhang,Guoqing Jin
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: WebPage available at https://tale17.github.io/omni/
- **标题**: 无所不知：图像生成的学习视觉概念
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 创意视觉概念的生成通常会从参考图像中的特定概念中汲取灵感，从而产生相关结果。但是，现有方法通常被限制在单一概念生成上，或者在多相关概念方案中不相关的概念很容易破坏，从而导致概念困惑和阻碍创造性的产生。为了解决这个问题，我们提出了无所不知的，这是一种视觉概念，用于创造性图像生成的方法。我们的方法学习以自然语言为指导的解开概念表示，并训练扩散模型以结合这些概念。我们利用多模式提取器的丰富语义空间来从给定的图像和概念指导中实现概念分离。为了通过不同的语义解开概念，我们构建了一个配对的概念删除数据集（PCD-200K），其中每对都共享诸如内容，样式和构图之类的相同概念。我们通过对比的正交解剖（COD）训练管道学习了散布的概念表示，然后将其注入到生成的其他扩散跨注意层中。一组块嵌入旨在在扩散模型中调整每个块的概念域。广泛的实验表明，我们的方法可以产生高质量的，概念的结果，对文本提示和所需概念产生高保真度。

### Multimodal Approaches to Fair Image Classification: An Ethical Perspective 
[[arxiv](https://arxiv.org/abs/2412.12165)] [[cool](https://papers.cool/arxiv/2412.12165)] [[pdf](https://arxiv.org/pdf/2412.12165)]
> **Authors**: Javon Hickmon
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-17
> **comment**: Bachelor's thesis
- **标题**: 公平图像分类的多模式方法：道德观点
- **领域**: 计算机视觉和模式识别,人工智能,计算机与社会,机器学习
- **摘要**: 在人工智能快速发展的领域中，机器的感知对于提高性能变得至关重要。图像分类系统越来越多地与各种应用程序不可或缺，从医学诊断到图像生成；但是，这些系统经常表现出有害偏见，可能导致不公平和歧视性结果。如果数据未仔细平衡和过滤，则依赖于单个数据模式的机器学习系统，即仅图像或仅文本，可以夸大培训数据中存在的隐藏偏见。即便如此，这些模型在不当背景下使用时仍会损害代表性不足的人群，例如政府机构使用预测性警务加强种族偏见。本文探讨了技术与伦理的交集，在公平图像分类模型的开发中。具体而言，我专注于提高使用多种方式来应对有害人口偏见的公平性和方法。集成多模式方法，将视觉数据与其他模式（例如文本和元数据）结合在一起，可以使这项工作增强图像分类系统的公平性和准确性。该研究对图像数据集和分类算法中的现有偏见进行了严格的研究，提出了缓解这些偏见的创新方法，并评估了在实际情况下部署此类系统的道德含义。通过全面的实验和分析，论文展示了多模式技术如何有助于更公平和伦理的AI解决方案，最终主张优先考虑公平性的负责任的AI实践。

### Causal Diffusion Transformers for Generative Modeling 
[[arxiv](https://arxiv.org/abs/2412.12095)] [[cool](https://papers.cool/arxiv/2412.12095)] [[pdf](https://arxiv.org/pdf/2412.12095)]
> **Authors**: Chaorui Deng,Deyao Zhu,Kunchang Li,Shi Guang,Haoqi Fan
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: 22 figures, 21 pages
- **标题**: 生成建模的因果扩散变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们将因果扩散作为扩散模型的自回旋（AR）对应物。这是一个下一步的预测框架，对离散和连续模式都很友好，并且与现有的下一步预测模型（如Llama和GPT）兼容。尽管最近的工作试图将扩散与AR模型相结合，但我们表明，将顺序分解引入扩散模型可以大大提高其性能，并在AR和扩散生成模式之间实现平稳的过渡。因此，我们提出了因果融合 - 仅解码器的变压器，该变压器在顺序令牌和扩散噪声水平上双重归因于数据，从而导致了成像网生成基准的最新结果，同时也享受了AR的优势，即产生任意数量的代币，以实现内部接触性的推理。我们通过联合图像生成和字幕模型进一步展示了Causalfusion的多模式功能，并展示了因果融合的零击中图像中的图像操作的能力。我们希望这项工作可以为社区提供有关离散和连续数据培训多模型模型的全新视角。

### Instruction-based Image Manipulation by Watching How Things Move 
[[arxiv](https://arxiv.org/abs/2412.12087)] [[cool](https://papers.cool/arxiv/2412.12087)] [[pdf](https://arxiv.org/pdf/2412.12087)]
> **Authors**: Mingdeng Cao,Xuaner Zhang,Yinqiang Zheng,Zhihao Xia
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Project page: https://ljzycmd.github.io/projects/InstructMove/
- **标题**: 通过观察事物如何移动来基于指导的图像操纵
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了一条新型的数据集构造管道，该管道采样了视频中的成对帧，并使用多模式大语言模型（MLLM）来生成基于培训指导的图像操作模型的编辑说明。视频框架固有地保留了主题和场景的身份，从而确保编辑过程中的内容保存一致。此外，视频数据捕获了多种自然动力学，例如非刚性主题运动和复杂的摄像头运动 - 否则很难对其进行建模，从而成为可扩展数据集构建的理想来源。使用这种方法，我们创建了一个新的数据集来训练指令move，这是一种基于指令的复杂操作的模型，这些模型很难通过合成生成的数据集实现。我们的模型展示了在调整主题姿势，重新排列元素和更改相机视角等任务中的最新性能。

### CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology 
[[arxiv](https://arxiv.org/abs/2412.12077)] [[cool](https://papers.cool/arxiv/2412.12077)] [[pdf](https://arxiv.org/pdf/2412.12077)]
> **Authors**: Yuxuan Sun,Yixuan Si,Chenglu Zhu,Xuan Gong,Kai Zhang,Pingyi Chen,Ye Zhang,Zhongyi Shui,Tao Lin,Lin Yang
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: 22 pages, 13 figures
- **标题**: CPATH-OMNI：用于贴片和整个幻灯片图像分析的统一的多模式基础模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模型模型（LMM）的出现为病理带来了重大进步。先前的研究主要集中于单独训练补丁级和全扫描图像（WSI）级别模型，从而限制了跨斑块和WSIS的学习知识的集成，并导致了冗余模型。在这项工作中，我们介绍了CPATH-OMNI，这是前150亿参数LMM，旨在统一补丁和WSI级别的图像分析，整合了两个级别的各种任务，包括分类，视觉问答答案，字幕和视觉参考提示。广泛的实验表明，CPATH-OMNI在42个数据集中的39个不同任务中实现了七个不同任务的最新性能（SOTA），表现优于或匹配针对单个任务的特定任务模型。此外，我们为CPATH-OMNI，CPATH-CLIP开发了一种专门的病理夹视觉处理器，该处理器首次集成了不同的视觉模型，并将大型语言模型合并为文本编码器，以构建更强大的剪辑模型，从而在9个零弹药上实现SOTA性能，并在九个零射击和四个几个弹药数据集中实现SOTA性能。我们的发现突出了CPATH-OMNI统一多种病理任务的能力，展示了其简化和推进病理学基础模型领域的潜力。

### CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding 
[[arxiv](https://arxiv.org/abs/2412.12075)] [[cool](https://papers.cool/arxiv/2412.12075)] [[pdf](https://arxiv.org/pdf/2412.12075)]
> **Authors**: Guo Chen,Yicheng Liu,Yifei Huang,Yuping He,Baoqi Pei,Jilan Xu,Yali Wang,Tong Lu,Limin Wang
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: 14 pages, 9 figures
- **标题**: CG基础：线索的问题回答基准，以了解长期视频理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 大多数现有的视频理解多模式大语言模型（MLLM）仅关注简短视频的基准。长期视频理解的基准数量有限通常仅依赖于多项选择问题（MCQ）。但是，由于基于MCQ的评估的固有局限性以及MLLM的推理能力的提高，模型可以通过将简短的视频理解与消除相结合而不真正理解视频内容来纯粹通过将当前答案提供给当前的答案。为了解决这一差距，我们介绍了CG Bench，这是一种新颖的基准测试，旨在在长视频中回答有关线索的问题。 CG Bench强调了该模型检索问题相关线索的能力，从而提高了评估信誉。它具有1,219个手动策划的视频，该视频由具有14个主要类别，171个次要类别和638个三级类别的颗粒系统分类，使其成为长期视频分析的最大基准。基准包括三种主要问题类型的12,129对质量检查：感知，推理和幻觉。为了补偿基于纯MCQ评估的缺点，我们设计了两种基于线索的新型评估方法：线索接地的白框和黑匣子评估，以评估该模型是否基于对视频的正确理解产生答案。我们评估了CG板上的多个封闭源和开源MLLM。结果表明，与短视频相比，当前模型在理解长视频方面的表现明显不佳，开源和商业模型之间存在很大的差距。我们希望CG Bench能够推动更具可信赖和有能力的MLLM的开发，以进行长时间的视频理解。所有注释和视频数据均在https://cg-bench.github.io/leaderboard/上发布。

### Gramian Multimodal Representation Learning and Alignment 
[[arxiv](https://arxiv.org/abs/2412.11959)] [[cool](https://papers.cool/arxiv/2412.11959)] [[pdf](https://arxiv.org/pdf/2412.11959)]
> **Authors**: Giordano Cicchetti,Eleonora Grassucci,Luigi Sigillo,Danilo Comminiello
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted at ICLR 2025
- **标题**: Gramian多模式表示学习和对齐方式
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 人类的看法将视觉，听力和语言等多种方式融入了对周围现实的统一理解。尽管最近的多模式模型通过通过对比度学习对齐方式取得了重大进展，但在扩展到多种方式时，它们的解决方案不合适。这些模型通常将每种模式与指定的锚点保持一致，而不会确保彼此之间的所有模式对齐，从而在需要共同理解多种模态的任务中表现出色。在本文中，我们从结构上重新考虑了多模式学习的成对常规方法，并提出了新型的Gramian表示对准度量（GRAM），该方法克服了上述局限性。 Gram在高维空间中直接学习然后将$ N $模态对准，在该空间中，模态嵌入通过最小化$ k $维的平行量的Gramian卷来确保模态向量跨越，从而确保了所有模态的几何形式同时对齐。克可以在任何下游方法中替代余弦的相似性，以2至$ n $模式保持，并在以前的相似性度量方面提供更有意义的一致性。基于革兰氏的新型对比损失函数增强了高维嵌入空间中多模型模型的对齐，从而在下游任务（例如视频ADIO-text检索和音频视频分类）中导致了新的最新性能。项目页面，代码和验证模型可在https://ispamm.github.io/gram/上找到。

### Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided Self-Supervised Learning 
[[arxiv](https://arxiv.org/abs/2412.11952)] [[cool](https://papers.cool/arxiv/2412.11952)] [[pdf](https://arxiv.org/pdf/2412.11952)]
> **Authors**: Yuti Liu,Shice Liu,Junyuan Gao,Pengtao Jiang,Hao Zhang,Jinwei Chen,Bo Li
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted by AAAI 2025
- **标题**: 通过多尺度文本引导的自我监督学习，推进全面的美学见解
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 图像美学评估（IAA）是一项重要而复杂的任务，需要分析和评估图像的美学价值，并确定其重点和改进领域。 IAA的传统方法通常集中于单个美学任务，并且标有标记的数据集不足，从而损害了深入的美学理解。尽管通过采用多模式大语言模型（MLLM）来克服这一挑战，但此类模型仍然不发达出于IAA的目的。为了解决这个问题，我们提出了一个综合的美学MLLM，能够细微的美学见解。我们方法的核心是一种创新的多尺度文本引导的自我监督学习技术。该技术具有多尺度的特征对齐模块，并以自我监督的方式利用大量未标记的数据，以在结构和功能上增强美学能力。经验证据表明，伴随着广泛的指导调整，我们的模型在多个任务中设定了新的最新基准测试，包括审美评分，美学评论和个性化的图像美学评估。值得注意的是，它在审美提出的新兴任务中也证明了零射击学习能力。此外，对于个性化的图像美学评估，我们利用了文化学习的潜力，并展示了其固有的优势。

### PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension 
[[arxiv](https://arxiv.org/abs/2412.11906)] [[cool](https://papers.cool/arxiv/2412.11906)] [[pdf](https://arxiv.org/pdf/2412.11906)]
> **Authors**: Kun Ouyang,Yuanxin Liu,Shicheng Li,Yi Liu,Hao Zhou,Fandong Meng,Jie Zhou,Xu Sun
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: Punchbench：在多模式的重点理解中对MLLM进行基准测试
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 多模式的打孔线涉及图像映射对传达的幽默或讽刺，是在线多媒体平台上的一种流行沟通方式。随着多模式大语言模型（MLLM）的快速发展，必须评估他们有效理解这些拳的能力。但是，现有的针线理解上的基准有三个主要局限性：1）允许模型仅依靠文本的语言快捷方式，2）缺乏问题多样性，3）狭窄地关注多模式内容的特定领域（例如，卡通）。为了解决这些局限性，我们引入了一个多模式\ textbf {punch}线理解\ textbf {bench} mark，命名为\ textbf {punchbench}，该标记是针对Punchline理解的准确而全面的评估。为了提高评估精度，我们通过修改原始字幕来生成同义词和反义词标题，从而减轻了标题中快捷方式的影响。为了提供全面的评估，Punchbench结合了来自各个领域的各种问题格式和图像捕获。在此基础上，我们进行了广泛的评估，并揭示了最先进的MLLM和人类在重点理解中的差距。为了提高重点理解，我们提出了简单到复杂的问题链（SCOQ）策略，从而使模型能够通过首先掌握简单的问题来逐步解决复杂的问题。 SCOQ有效地提高了各种MLLM在打孔板上的性能，超过了文本学习和经过思考链。

### GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training 
[[arxiv](https://arxiv.org/abs/2412.11863)] [[cool](https://papers.cool/arxiv/2412.11863)] [[pdf](https://arxiv.org/pdf/2412.11863)]
> **Authors**: Renqiu Xia,Mingsheng Li,Hancheng Ye,Wenjie Wu,Hongbin Zhou,Jiakang Yuan,Tianshuo Peng,Xinyu Cai,Xiangchao Yan,Bin Wang,Conghui He,Botian Shi,Tao Chen,Junchi Yan,Bo Zhang
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Our code is available at https://github.com/Alpha-Innovator/GeoX
- **标题**: GEOX：通过统一的形式视觉语言预训练解决几何问题
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 尽管他们掌握了一般任务，但多模式的大语言模型（MLLM）与自动几何问题解决（GPS）斗争，这需要理解图表，解释符号和执行复杂的推理。这种局限性源于它们对自然图像和文本的预训练，以及在解决问题过程中缺乏自动验证。此外，当前的几何专家受到特定于任务的设计的限制，从而使其在更广泛的几何问题上的有效性降低。为此，我们提出了GEOX，这是一个多模式的大型模型，重点是几何理解和推理任务。鉴于几何图符号和自然图像文本之间存在显着差异，我们引入了单峰预训练以开发图表编码器和符号解码器，从而增强了对几何图像和语料库的理解。此外，我们引入了几何语言对准，这是一种有效的预训练范式，它弥合了单峰几何专家之间的模态差距。我们提出了一个发电机和抽动器变压器（GS形式），以生成歧视性查询并消除不均匀分布的几何信号的不信息表示。最后，Geox受益于视觉教学调整，使其能够将几何图像和问题作为输入并生成可验证的解决方案。实验表明，Geox在公开认可的基准（例如GEOQA，UNIGEO，几何3K和PGPS9K）上均优于通才和几何专家。

### IDEA-Bench: How Far are Generative Models from Professional Designing? 
[[arxiv](https://arxiv.org/abs/2412.11767)] [[cool](https://papers.cool/arxiv/2412.11767)] [[pdf](https://arxiv.org/pdf/2412.11767)]
> **Authors**: Chen Liang,Lianghua Huang,Jingwu Fang,Huanzhang Dou,Wei Wang,Zhi-Fan Wu,Yupeng Shi,Junge Zhang,Xin Zhao,Yu Liu
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: Idea-Bench：生成模型距专业设计有多远？
- **领域**: 计算机视觉和模式识别
- **摘要**: 现实世界的设计任务（例如图画书的创建，使用角色集，照片修饰，视觉效果和字体传输）是高度多样而复杂的，需要深入解释和从说明，描述和参考图像中提取各种元素。最终的图像通常会隐式地从参考或用户输入中捕获关键功能，从而使开发可以有效解决此类任务的模型变得具有挑战性。尽管现有的视觉生成模型可以根据提示产生高质量的图像，但即使使用ControlNets和Loras等适配器增强，它们即使增强了各种形式以及多种输入和输出的专业设计场景中的巨大限制。为了解决这个问题，我们介绍了Ideas Bench，这是一个全面的基准测试，其中包含100个现实世界的设计任务，包括渲染，视觉效果，故事板，图画书，字体，基于样式的字体，基于风格的定义性能生成，并拥有275个测试用例，以彻底评估模型的通用性生成功能。值得注意的是，即使表现最佳的模型也只能在Idea-Bench上实现22.48，而最佳通用模型也只能达到6.81。我们提供了这些结果的详细分析，强调了固有的挑战，并提供了可行的改进方向。此外，我们提供了18个具有多模式大语模型（MLLM）基于多模式的自动评估技术的代表性任务的子集，以促进快速模型开发和比较。我们在https://github.com/ali-vilab/idea-bench上发布了基准数据，评估工具包和在线排行榜，旨在将生成模型的进步推向更广泛和适用的智能设计系统。

### LMM-Regularized CLIP Embeddings for Image Classification 
[[arxiv](https://arxiv.org/abs/2412.11663)] [[cool](https://papers.cool/arxiv/2412.11663)] [[pdf](https://arxiv.org/pdf/2412.11663)]
> **Authors**: Maria Tzelepi,Vasileios Mezaris
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted for publication, 26th Int. Symp. on Multimedia (IEEE ISM 2024), Tokyo, Japan, Dec. 2024. This is the authors' "accepted version"
- **标题**: 用于图像分类的LMM登记夹嵌入
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 在本文中，我们使用强大的剪辑视觉语言模型处理图像分类任务。我们的目标是通过提出一种基于新型的大型多模型（LMM）的正则化方法，使用剪辑的图像编码器提高分类性能。所提出的方法使用LMM来提取数据集图像的语义描述。然后，它使用剪辑的文本编码器冷冻，以获取相应的文本嵌入并计算平均语义类描述。随后，我们通过添加分类头来调整剪辑的图像编码器，除了主要分类目标外，我们还具有附加的辅助目标，并与图像编码器输出一起训练它。额外的目标迫使图像编码器输出处的嵌入与它们相应的LMM生成的平均语义类描述相似。通过这种方式，它产生具有增强歧视能力的嵌入，从而提高了分类性能。通过在三个图像分类数据集上进行的大量实验来验证提出的正则化方法的有效性。

### VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video Prompting 
[[arxiv](https://arxiv.org/abs/2412.11621)] [[cool](https://papers.cool/arxiv/2412.11621)] [[pdf](https://arxiv.org/pdf/2412.11621)]
> **Authors**: Muhammet Furkan Ilaslan,Ali Koksal,Kevin Qinhong Lin,Burak Satar,Mike Zheng Shou,Qianli Xu
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted for The 39th Annual AAAI Conference on Artificial Intelligence 2025 in Main Track, 19 pages, 24 figures
- **标题**: VG-TVP：通过视觉接地文本视频提示的多模式程序计划
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 大型语言模型（LLM）的代理在程序任务中表现出了希望，但是通过文本和视频来帮助用户的多模式说明的潜力仍然不足。为了解决这一差距，我们提出了视觉接地的文本视频提示（VG-TVP）方法，该方法是一种新颖的LLM授权的多模式程序计划（MPP）框架。鉴于指定的高级目标，它产生了凝聚力的文本和视频程序计划。主要挑战是实现文本和视觉信息，时间连贯性以及程序计划的准确性。 VG-TVP利用了LLM的零摄像推理能力，视频字幕模型的视频到文本生成能力以及扩散模型的文本对视频生成能力。 VG-TVP通过提出字幕（foc）方法的新型融合以及使用文本到视频桥（T2V-B）和视频对文本桥（V2T-B）来改善模式之间的相互作用。它们允许LLMS指导视觉上的文本计划和文本基础视频计划的产生。为了解决适合MPP的数据集的稀缺性，我们策划了一个新的数据集，称为日常生活任务程序计划（每日PPP）。我们进行全面的实验和基准测试以评估人类偏好（有关文本和视觉信息，时间连贯性和计划准确性）。我们的VG-TVP方法的表现优于每日PPP数据集上的单峰基线。

### CLIP-SR: Collaborative Linguistic and Image Processing for Super-Resolution 
[[arxiv](https://arxiv.org/abs/2412.11609)] [[cool](https://papers.cool/arxiv/2412.11609)] [[pdf](https://arxiv.org/pdf/2412.11609)]
> **Authors**: Bingwen Hu,Heng Liu,Zhedong Zheng,Ping Liu
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: 11 pages, 10 figures
- **标题**: 剪辑-SR：超分辨率的协作语言和图像处理
- **领域**: 计算机视觉和模式识别
- **摘要**: 卷积神经网络（CNN）具有高级图像超分辨率（SR），但是大多数基于CNN的方法仅依赖于基于像素的转换，通常会导致伪影和模糊，尤其是在严重的下采样（例如8x或16x）中。最近的文本指导的SR方法试图利用文本信息以获得增强的细节，但是它们经常在有效的对齐方式上挣扎，从而导致语义连贯性不一致。为了解决这些局限性，我们引入了一种多模式语义增强方法，该方法将文本语义与视觉特征结合在一起，有效地解决语义不匹配和高度降级的LR图像中的细节损失。我们提出的多模式协作框架可以在重要的上刻度因素下生产现实和高质量的SR图像。该框架集成了文本和图像输入，采用及时的预测指标，文本图像融合块（Tifblock）和迭代修复模块与剪辑（对比性语言图像预处理）功能一起，以指导渐进的增强过程，并具有细粒度对齐。这种对准产生的高分辨率输出具有清晰的细节和语义连贯性，即使在大型缩放因素下也是如此。通过广泛的比较实验和消融研究，我们验证了方法的有效性。此外，通过合并文本语义指导，我们的技术可以在保持语义连贯性的同时具有一定程度的超分辨率编辑性。

### Exploring More from Multiple Gait Modalities for Human Identification 
[[arxiv](https://arxiv.org/abs/2412.11495)] [[cool](https://papers.cool/arxiv/2412.11495)] [[pdf](https://arxiv.org/pdf/2412.11495)]
> **Authors**: Dongyang Jin,Chao Fan,Weihua Chen,Shiqi Yu
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: 从多种步态方式探索人类身份的多种步态方式
- **领域**: 计算机视觉和模式识别
- **摘要**: 作为一种软生物特征，步态可以反映远距离的个体的独特步行模式，从而表现出一种有前途的人类识别技术。在RGB视频中隐藏的步态无关线索中，剪影和骨骼虽然在视觉上紧凑，但长期以来一直是最普遍的步态方式。最近，已经尝试了一些尝试引入更有用的数据表格，例如人类解析和光流图像，以捕获步态特征以及多支分支的架构。但是，由于模型设计和实验设置的不一致，我们认为这些流行的步态方式中的全面，公平的比较研究仍然缺乏涉及代表性能力和融合策略探索的涉及。从细粒度和整体与像素的运动建模的角度来看，这项工作对三种流行的步态表示形式进行了深入研究，即剪影，人体解析和光流以及各种融合评估，并实现了相似性和差异。根据获得的见解，我们进一步制定了C $^2 $融合策略，因此建立了我们的新框架Multigait ++。 c $^2 $融合可以保留共同点，同时突出差异以丰富步态特征的学习。为了验证我们的发现和结论，进行了对Gait3D，Grow，CCPG和Sustech1k的广泛实验。该代码可在https://github.com/shiqiyu/opengait上找到。

### Pre-training a Density-Aware Pose Transformer for Robust LiDAR-based 3D Human Pose Estimation 
[[arxiv](https://arxiv.org/abs/2412.13454)] [[cool](https://papers.cool/arxiv/2412.13454)] [[pdf](https://arxiv.org/pdf/2412.13454)]
> **Authors**: Xiaoqi An,Lin Zhao,Chen Gong,Jun Li,Jian Yang
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Accepted to AAAI 2025
- **标题**: 预先训练的密度感知姿势变压器，用于鲁塔激光雷达的3D人姿势估计
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 随着自主驾驶的快速发展，基于激光雷达的3D人姿势估计（3D HPE）已成为研究重点。但是，由于激光圈捕获点云的噪音和稀疏性，强大的人姿势估计仍然具有挑战性。大多数现有方法都使用时间信息，多模式融合或SMPL优化来纠正偏差结果。在这项工作中，我们仅通过对低质量点云的内在属性进行建模来获取3D HPE的足够信息。因此，提出了一种简单而功能强大的方法，它为点云的建模和增强提供了见解。具体而言，我们首先提出了一个简洁有效的密度感知姿势变压器（DAPT），以获得稳定的关键点表示。通过使用一组关节锚和精心设计的交换模块，从具有不同密度的点云中提取有效信息。然后使用1D热图来表示关键点的精确位置。其次，提出了一种综合的LiDar人类合成和增强方法来预先培训该模型，从而使其能够先验获得更好的人体。我们通过随机采样人类位置和方向以及通过添加激光级掩模来增加点云的多样性。已经在多个数据集上进行了广泛的实验，包括IMU注销的LIDARHUMAN26M，SLOPER4D和手动注释Waymo Open DataSet V2.0（Waymo），HumanM3。我们的方法在所有情况下都展示了SOTA性能。特别是，与Waymo上的LPFormer相比，我们将平均MPJPE降低了$ 10.0mm $。与Sloper4D上的PRN相比，我们显然将平均MPJPE降低了20.7毫米$。

### Bringing Multimodality to Amazon Visual Search System 
[[arxiv](https://arxiv.org/abs/2412.13364)] [[cool](https://papers.cool/arxiv/2412.13364)] [[pdf](https://arxiv.org/pdf/2412.13364)]
> **Authors**: Xinliang Zhu,Michael Huang,Han Ding,Jinyu Yang,Kelvin Chen,Tao Zhou,Tal Neiman,Ouye Xie,Son Tran,Benjamin Yao,Doug Gray,Anuj Bindal,Arnab Dhua
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: ef:Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024
- **标题**: 将多模式带到亚马逊视觉搜索系统
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像到图像匹配的匹配已在计算机视觉社区中进行了很好的研究。先前的研究主要集中于培训深度度量学习模型，以匹配查询图像和画廊图像之间的视觉模式。在这项研究中，我们表明，纯图像对图像匹配受到与局部视觉模式相匹配引起的误报。为了减轻这个问题，我们建议利用视力训练预处理研究的最新进展。具体而言，我们将其他图像文本对齐损失引入深度度量学习，这是对图像到图像匹配损失的限制。借助文本（例如产品标题）和图像对之间的其他对齐方式，该模型可以明确地从两种模式中学习概念，从而避免匹配低级视觉特征。我们逐步开发了两个变体，一个3塔和一个4塔模型，后者需要一个简短的文本查询输入。通过广泛的实验，我们表明这种变化导致对图像匹配问题的图像有很大的改进。我们进一步利用了该模型进行多模式搜索，该搜索同时采用图像和重新制定文本查询以提高搜索质量。离线和在线实验均显示出主要指标的强烈改进。具体而言，我们看到图像匹配的相对改进为4.95％的相对改善，即通过3塔模型的速率单击速率，而4塔模型则进一步改善了1.13％。

### Interpretable deformable image registration: A geometric deep learning perspective 
[[arxiv](https://arxiv.org/abs/2412.13294)] [[cool](https://papers.cool/arxiv/2412.13294)] [[pdf](https://arxiv.org/pdf/2412.13294)]
> **Authors**: Vasiliki Sideri-Lampretsa,Nil Stolt-Ansó,Huaqi Qiu,Julian McGinnis,Wenke Karbole,Martin Menten,Daniel Rueckert
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: 20 Pages
- **标题**: 可解释的可变形图像注册：几何深度学习观点
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 可变形的图像注册提出了一个具有挑战性的问题，与大多数深度学习任务不同，必须考虑多个坐标系之间的复杂关系。尽管数据驱动的方法显示出有望建模复杂非线性转换的有希望的功能，但现有作品采用标准深度学习体系结构，假设它们是一般的黑框求解器。我们认为，了解学习的操作如何在源和目标域中的功能之间执行模式匹配是构建强大，数据效率和可解释的体系结构的关键。我们为设计可解释的注册框架提供了理论基础：分开的特征提取和变形建模，动态接收场以及数据驱动的变形函数对两个空间域之间关系的意识。基于这个基础，我们制定了一个端到端的过程，以粗到精细的方式完善转换。我们的体系结构采用了使用几何深度学习原理的空间连续变形模型函数，因此避免了将重新采样到常规网格的有问题方法之间的常规网格之间的连续改进。我们进行定性调查，以突出建筑的有趣的可解释性属性。最后，我们的性能指标比单模式间和多模式的大脑登记以及纵向视网膜内部受试者内部注册的挑战性任务显着改善了性能指标。我们使代码公开可用

### MotionBridge: Dynamic Video Inbetweening with Flexible Controls 
[[arxiv](https://arxiv.org/abs/2412.13190)] [[cool](https://papers.cool/arxiv/2412.13190)] [[pdf](https://arxiv.org/pdf/2412.13190)]
> **Authors**: Maham Tanveer,Yang Zhou,Simon Niklaus,Ali Mahdavi Amiri,Hao Zhang,Krishna Kumar Singh,Nanxuan Zhao
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Project website: [https://motionbridge.github.io/]
- **标题**: Motionbridge：动态视频与灵活控件有关
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过在两个图像帧之间生成合理且平稳的过渡，视频Inbetinging是视频编辑和长时间视频综合的重要工具。传统作品缺乏产生复杂大型动作的能力。尽管最近的视频生成技术在创造高质量的结果方面具有强大的功能，但它们通常缺乏对中间帧细节的良好控制，这可能导致与创意思维不符的结果。我们介绍了MotionBridge，这是一个统一的视频Inbettoring框架，该框架允许灵活的控件，包括轨迹中风，钥匙帧，口罩，指南像素和文本。但是，在统一框架中学习这种多模式控件是一项具有挑战性的任务。因此，我们设计了两个发电机，以忠实地提取控制信号，并通过双分支嵌入者编码特征以解决歧义。我们进一步介绍了课程培训策略，以平稳学习各种控制。广泛的定性和定量实验表明，这种多模式控制能够更具动态，可定制和上下文准确的视觉叙事。

### CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2412.12932)] [[cool](https://papers.cool/arxiv/2412.12932)] [[pdf](https://arxiv.org/pdf/2412.12932)]
> **Authors**: Zihui Cheng,Qiguang Chen,Jin Zhang,Hao Fei,Xiaocheng Feng,Wanxiang Che,Min Li,Libo Qin
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Accepted at AAAI 2025; Project Page: https://github.com/czhhzc/CoMT
- **标题**: COMT：大型视觉模型上多模式思想链的新型基准测试
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉模型（LVLM）最近在多模式任务中表现出惊人的成功，包括多模式链链（MCOT）推理的进步。尽管取得了这些成功，但当前的基准仍遵循传统的范式，具有多模式输入和文本模式输出，这导致了重要的缺点，例如丢失的视觉操作和模糊的表达式。在此激励的情况下，我们引入了一系列新型的多模式思想（COMT）基准，以解决这些局限性。与传统的MCOT基准不同，COMT需要多模式输入和多模式推理输出，旨在模仿像人类一样的推理，以固有地整合视觉操作。具体而言，COMT包括四个类别：（1）视觉创建，（2）视觉删除，（3）视觉更新和（4）视觉选择，以全面探索复杂的视觉操作和实际场景中的简洁表达。我们评估了COMT的各种LVLM和策略，揭示了对当前方法的能力和局限性的一些关键见解。我们希望COMT能够激发更多关于将多模式生成引入推理过程的研究。

### DoPTA: Improving Document Layout Analysis using Patch-Text Alignment 
[[arxiv](https://arxiv.org/abs/2412.12902)] [[cool](https://papers.cool/arxiv/2412.12902)] [[pdf](https://arxiv.org/pdf/2412.12902)]
> **Authors**: Nikitha SR,Tarun Ram Menta,Mausoom Sarkar
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: DOPTA：使用补丁文本对齐来改进文档布局分析
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式学习的出现带来了文档AI的重大改进。现在将文档视为多模式实体，并将文本和视觉信息纳入下游分析。但是，在这个空间中的作品通常集中在文本方面，以视觉空间为辅助信息。尽管有些作品探索了基于纯视觉的技术以了解文档图像的理解，但它们需要将OCR识别为推理期间的输入，或者在学习过程中与文本不符。因此，我们提出了一种新颖的图像文本对齐技术，专门设计用于利用文档图像中的文本信息以提高视觉任务的性能。我们的文档编码器模型dopta-经过此技术训练，在广泛的文档图像理解任务上表现出了强劲的性能，而无需推理过程中的OCR。结合辅助重建目标，DOPTA始终超过较大的模型，同时使用明显较小的训练预定。 DOPTA还为D4LA和FUNSD设定了新的最先进的结果，这是两个具有挑战性的文档视觉分析基准。

### FocusChat: Text-guided Long Video Understanding via Spatiotemporal Information Filtering 
[[arxiv](https://arxiv.org/abs/2412.12833)] [[cool](https://papers.cool/arxiv/2412.12833)] [[pdf](https://arxiv.org/pdf/2412.12833)]
> **Authors**: Zheng Cheng,Rendong Wang,Zhicheng Wang
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: 11 pages, 4 figures
- **标题**: FOCUSCHAT：通过时空信息过滤的文本引导的长视频理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，多模式大型语言模型取得了重大进展。但是，用户意图缺乏指导的视觉信息可能会导致冗余计算，并涉及不必要的视觉噪音，尤其是在长时间的，未修剪的视频中。为了解决这个问题，我们提出了FocusChat，这是一种文本引导的多模式大语言模型（LLM），该模型（LLM）强调与用户提示相关的视觉信息。详细说明，我们的模型首先经历了语义提取模块，该模块分别包括一个视觉语义分支和文本语义分支，分别提取图像和文本语义。使用时空滤波模块（STFM）将两个分支组合在一起。 STFM启用明确的空间级信息过滤和隐式时间级特征过滤，以确保视觉令牌与用户的查询紧密相位。它降低了输入到LLM的视觉令牌的基本数量。 PocusChat在零拍实验中的视频听起来显着胜过视频听觉，使用较小的训练数据，只有16个视觉令牌被占据。它在几次实验中获得的结果与最新的实验相当，只有0.72m的预训练数据。

### ComprehendEdit: A Comprehensive Dataset and Evaluation Framework for Multimodal Knowledge Editing 
[[arxiv](https://arxiv.org/abs/2412.12821)] [[cool](https://papers.cool/arxiv/2412.12821)] [[pdf](https://arxiv.org/pdf/2412.12821)]
> **Authors**: Yaohui Ma,Xiaopeng Hong,Shizhou Zhang,Huiyun Li,Zhilin Zhu,Wei Luo,Zhiheng Ma
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Extended version for paper accepted to AAAI 2025. Project Page: https://github.com/yaohui120/ComprehendEdit
- **标题**: ConsependedIt：多模式知识编辑的全面数据集和评估框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型多模式模型（MLLM）彻底改变了自然语言处理和视觉理解，但通常包含过时或不准确的信息。当前的多模式知识编辑评估的范围有限且可能有偏见，重点是狭窄的任务，并且未能评估对内域样本的影响。为了解决这些问题，我们介绍了ConseldendIt，这是一个全面的基准，其中包括来自多个数据集的八个不同任务。我们提出了两个新颖的指标：知识概括指数（KGI）和知识保存指数（KPI），它们评估了对内域样品的编辑影响，而不依赖于AI合成样本。基于框架的见解，我们建立了层次结构的内在编辑（HICE），这是一种基线方法，采用两阶段方法来平衡所有指标的性能。这项研究为多模式知识编辑提供了更全面的评估框架，揭示了该领域的独特挑战，并提供了一种基线方法，证明了性能的改善。我们的工作为未来的研究开辟了新的观点，并为MLLM开发更强大，更有效的编辑技术奠定了基础。 https://github.com/yaohui120/comprehendit可以在https://github.com/yaohui120/comprehendit上获得ConaldendEdit基准和实施代码。

### Open-World Panoptic Segmentation 
[[arxiv](https://arxiv.org/abs/2412.12740)] [[cool](https://papers.cool/arxiv/2412.12740)] [[pdf](https://arxiv.org/pdf/2412.12740)]
> **Authors**: Matteo Sodano,Federico Magistri,Jens Behley,Cyrill Stachniss
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Submitted to PAMI
- **标题**: 开放世界的全景分割
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 感知是自主行动视觉系统（例如自动驾驶汽车）的关键基础。至关重要的是，这些系统能够理解其周围环境，以便安全，坚固。此外，部署在不受约束的现实世界中的自主系统必须能够处理以前从未见过的新型情况和对象。在本文中，我们解决了开放世界的泛群分段的问题，即在测试时间发现新的语义类别和新对象实例的任务，同时在我们逐步发现的类别之间执行一致性。我们提出了CON2MAV，这是一种开放世界泛滥分割的方法，它扩展了我们以前的工作Contmav，该方法是为开放世界语义分割而开发的。通过跨多个数据集的广泛实验，我们表明我们的模型在开放世界细分任务上实现了最新的结果，同时仍在竞争性类别上进行竞争性。我们将在接受后开放我们的实施。此外，我们提出了恐慌（上下文中的泛异常），这是评估自主驾驶场景中开放世界泛型分割的基准。该数据集使用安装在汽车上的多模式传感器套件记录，可在语义和实例级别上对异常对象进行高质量的，像素的注释。我们的数据集包含800张图像，其中有50多个未知类别，即培训集中未出现的类和4000个对象实例，这使其成为自主驾驶场景中开放世界分段任务的极具挑战性的数据集。我们在隐藏的测试集上为多个开放世界任务提供竞争。我们的数据集和竞赛可在https://www.ipb.uni-bonn.de/data/panic上获得。

### PolSAM: Polarimetric Scattering Mechanism Informed Segment Anything Model 
[[arxiv](https://arxiv.org/abs/2412.12737)] [[cool](https://papers.cool/arxiv/2412.12737)] [[pdf](https://arxiv.org/pdf/2412.12737)]
> **Authors**: Yuqing Wang,Zhongling Huang,Shuxin Yang,Hao Tang,Xiaolan Qiu,Junwei Han,Dingwen Zhang
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: The manuscript is 15 pages long, includes 14 figures and 5 tables
- **标题**: Polsam：偏振散射机制知情节段
- **领域**: 计算机视觉和模式识别
- **摘要**: Polsar数据由于其丰富而复杂的特征带来了独特的挑战。现有的数据表示，例如复杂值数据，偏光特征和振幅图像。但是，这些格式通常面临与可用性，可解释性和数据完整性有关的问题。 Polsar的大多数特征提取网络很小，限制了它们有效捕获功能的能力。为了解决这些问题，我们提出了偏光散射机制所采用的SAM（POLSAM），SAM（POLSAM）是一个增强片段的任何模型（SAM），该模型（SAM）整合了域特异性散射特性和新颖的及时生成策略。 Polsam引入了微波视觉数据（MVD），这是一种源自极化分解和语义相关性的轻巧且可解释的数据表示。我们提出了两个关键组件：特征级融合提示提示（FFP），它融合了伪色的SAR图像和MVD的视觉令牌，以解决冷冻SAM编码器中的模态不相容，以及使用语义序列的稀疏和密集的序列，使用语义序列提示了语义级别的融合提示（SFP）。 Physar-seg数据集的实验结果表明，POLSAM显着胜过现有的基于SAM的和多模式融合模型，提高了分割精度，降低数据存储和加速推理时间。源代码和数据集将在\ url {https://github.com/xai4sar/polsam}上公开提供。

### GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models 
[[arxiv](https://arxiv.org/abs/2412.12735)] [[cool](https://papers.cool/arxiv/2412.12735)] [[pdf](https://arxiv.org/pdf/2412.12735)]
> **Authors**: Mukai Li,Lei Li,Shansan Gong,Qi Liu
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Working in progress
- **标题**: 长颈鹿：扩展视觉语言模型上下文长度的设计选择
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 视觉语言模型（VLMS）在处理多模式输入方面表现出令人印象深刻的功能，但需要处理多个图像和高分辨率视频的视觉剂等应用程序，需求增强了远程建模。此外，现有的开源VLM缺乏系统的探索来扩展其上下文长度，商业模型通常提供有限的细节。为了解决这个问题，我们旨在建立一个有效的解决方案，以增强VLM的长篇小说性能，同时在短上下文场景中保留其能力。为了实现这一目标，我们通过从数据策划到上下文窗口扩展和利用的广泛实验设置来做出最佳的设计选择：（1）我们分析数据源和长度分布来构建ETVLM-数据配方，以平衡各场景的性能； （2）我们检查了现有的位置扩展方法，确定其局限性，并将M-Rope ++作为增强的方法提出；我们还选择仅使用混合源数据来指导骨干。 （3）我们讨论如何更好地利用扩展上下文窗口并提出混合分辨率培训。我们提出了基于QWEN-VL系列模型，提出了长颈鹿，该长颈鹿有效地扩展到128K长度。通过广泛的长篇小说VLM基准评估，例如Videomme和Viusal Haystacks，我们的长颈鹿在类似尺寸的开源长VLMS中实现了最先进的性能，并且与商业模型GPT-4V具有竞争力。我们将打开代码，数据和模型。

### ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation Detecting and Grounding 
[[arxiv](https://arxiv.org/abs/2412.12718)] [[cool](https://papers.cool/arxiv/2412.12718)] [[pdf](https://arxiv.org/pdf/2412.12718)]
> **Authors**: Zhenxing Zhang,Yaxiong Wang,Lechao Cheng,Zhun Zhong,Dan Guo,Meng Wang
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: 12 pages, 6 figures
- **标题**: 尽快：前进的语义对齐促进多模式的操纵检测和接地
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 我们尽快提出了一个用于检测和接地多模式介质操纵的新框架（DGM4）。UPON彻底检查，我们观察到，图像和文本之间准确的细粒细粒跨模式的语义比对对于准确操纵检测和接地至关重要。虽然现有的DGM4方法罕见地关注跨模式对齐，从而阻碍了操纵检测到进一步的准确性。为了解决这个问题，这项工作针对的是提高语义一致性学习以促进这项任务。特别是，我们利用现成的多模式大型语言模型（MLLM）和大语言模型（LLMS）来构建配对的图像文本对，尤其是对于操纵的实例。随后，进行了跨模式对准学习以增强语义一致性。除了明确的辅助线索外，我们还设计了一种操纵引导的交叉注意（MGCA），以提供隐含的指导，以增强操纵感知。借助在培训期间可用的基础真相，MGCA鼓励模型更多地集中于操纵组件的同时，同时淡化了普通的组件，从而增强了模型捕获操作的能力。在DGM4数据集上进行了广泛的实验，结果表明，我们的模型可以超过比较方法，并明确地缘。

### Dense Audio-Visual Event Localization under Cross-Modal Consistency and Multi-Temporal Granularity Collaboration 
[[arxiv](https://arxiv.org/abs/2412.12628)] [[cool](https://papers.cool/arxiv/2412.12628)] [[pdf](https://arxiv.org/pdf/2412.12628)]
> **Authors**: Ziheng Zhou,Jinxing Zhou,Wei Qian,Shengeng Tang,Xiaojun Chang,Dan Guo
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Accepted by AAAI 2025. Project page: https://github.com/zzhhfut/CCNet-AAAI2025. Jinxing Zhou and Dan Guo are the corresponding authors
- **标题**: 在跨模式的一致性和多阶梯粒度协作中，密集的视听事件定位
- **领域**: 计算机视觉和模式识别
- **摘要**: 在视听学习领域，大多数研究任务仅专注于简短的视频。本文重点介绍了更实用的密集视听事件本地化（davel）任务，从而推进了视听场景的理解，以了解更长的，未修剪的视频。该任务旨在识别和时间识别在音频和视觉流中同时发生的所有事件。通常，每个视频都包含多个类的密集事件，这些事件可能会在时间轴上重叠，每个都显示各种持续时间。鉴于这些挑战，有效利用视听关系和各种粒度编码的时间特征变得至关重要。为了应对这些挑战，我们引入了一个新颖的CCNET，其中包括两个核心模块：跨模式一致性协作（CMCC）和多阶段粒度协作（MTGC）。具体而言，CMCC模块包含两个分支：跨模式相互作用分支和一个时间一致性门控分支。前分支通过编码视听关系促进了跨模态的一致事件语义的聚合，而后者分支指导了一个模式的重点，指出了另一种模式中的关键事件与事件相关的时间区域。 MTGC模块包括一个粗到精细的协作块和精细的合作块，在粗粒度的时间特征之间提供了双向支持。在UNAV-100数据集上进行了广泛的实验验证了我们的模块设计，从而在密集的视听事件本地化中产生了新的最先进的性能。该代码可从https://github.com/zzhhfut/ccnet-aaai2025获得。

### PBVS 2024 Solution: Self-Supervised Learning and Sampling Strategies for SAR Classification in Extreme Long-Tail Distribution 
[[arxiv](https://arxiv.org/abs/2412.12565)] [[cool](https://papers.cool/arxiv/2412.12565)] [[pdf](https://arxiv.org/pdf/2412.12565)]
> **Authors**: Yuhyun Kim,Minwoo Kim,Hyobin Park,Jinwook Jung,Dong-Geol Choi
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: 4 pages, 3 figures, 1 Table
- **标题**: PBVS 2024解决方案：极端长尾分布中SAR分类的自制学习和抽样策略
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式学习研讨会（PBVS 2024）旨在通过利用合成孔径雷达（SAR）数据来提高自动目标识别（ATR）系统的性能，这很难解释，但仍未受到天气条件和可见光光线的影响，以及可见光的光，以及用于同步学习的电动光（EO）数据。子任务（称为多模式空中视图图像挑战 - 分类）重点是基于一组SAR-EO图像对及其各自的类标签来预测低分辨率航空图像的类标签。提供的数据集由SAR-EO对组成，其特征是严重的长尾分布，最大和最小的类之间的差异超过1000倍，因此难以应用典型的长尾方法。另外，SAR和EO数据集之间的域差异使标准多模式方法的有效性变得复杂。为了应对这些重大挑战，我们提出了一种两阶段的学习方法，该方法利用了自我监督的技术，与多模式学习和通过SAR-EO翻译进行了多模式学习和推断，以有效地利用EO。在PBVS 2024多模式空中视图图像挑战的最终测试阶段 - 分类（SAR分类）任务，我们的模型的精度为21.45％，AUC为0.56，总分为0.30，使我们在比赛中排名第9。

### Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking 
[[arxiv](https://arxiv.org/abs/2412.12561)] [[cool](https://papers.cool/arxiv/2412.12561)] [[pdf](https://arxiv.org/pdf/2412.12561)]
> **Authors**: Wenjun Huang,Yang Ni,Hanning Chen,Yirui He,Ian Bryant,Yezi Liu,Mohsen Imani
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: 告诉我要跟踪的内容：注入强大的语言指南以增强参考多对象跟踪
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 参考多对象跟踪（RMOT）是一项新兴的跨模式任务，旨在根据语言表达式定位任意数量的目标，并在视频中不断跟踪它们。这项复杂的任务涉及对多模式数据进行推理，并与时间关联进行精确的目标定位。但是，由于任务的性质，先前的研究忽略了新生儿目标和现有目标之间的数据分布不平衡。此外，它们仅间接融合多模式的特征，努力为新生儿目标检测提供明确的指导。为了解决上述问题，我们采取了协作匹配策略来减轻失衡的影响，从而提高了在保持跟踪性能的同时检测新生儿目标的能力。在编码器中，我们集成并增强了交叉模式和多尺度融合，克服了先前工作中的瓶颈，在该工作中，有限的多模式信息在特征地图之间共享和相互作用。在解码器中，我们还开发了一种转介供入的改编版，该改编可以通过查询令牌提供明确的推荐指导。与先前的作品相比，实验展示了我们模型的出色性能（+3.42％），证明了我们的设计有效性。

### Content-style disentangled representation for controllable artistic image stylization and generation 
[[arxiv](https://arxiv.org/abs/2412.14496)] [[cool](https://papers.cool/arxiv/2412.14496)] [[pdf](https://arxiv.org/pdf/2412.14496)]
> **Authors**: Ma Zhuoqi,Zhang Yixuan,You Zejun,Tian Long,Liu Xiyang
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 内容风格的删除表示可控艺术图像风格和发电的表示形式
- **领域**: 计算机视觉和模式识别
- **摘要**: 可控的艺术图像样式和生成旨在通过学习的艺术风格渲染文本或图像提供的内容，其中内容和样式的解耦是获得令人满意的结果的关键。但是，当前的内容和样式删除方法主要依赖于图像信息进行监督，这导致了两个问题：1）模型只能支持样式或内容输入的一种模式； 2）不完整的删除，从而导致参考图像的语义干扰。为了解决上述问题，本文提出了一种内容式表示形式，解开可控艺术图像风格和生成的方法。我们构建一个Wikistyle+数据集由具有相应的样式和内容的文本描述的艺术品组成。基于多模式数据集，我们提出了一个指导扩散模型的分离内容和样式表示。首先通过Q形成剂来学习分离的表示形式，然后使用可学习的多步跨注意层注入预训练的扩散模型，以更好地控制风格。这种方法允许模型适应来自不同方式的输入。实验结果表明，我们的方法在多模式监督下实现了参考图像中的内容和样式的彻底删除，从而在生成的输出中可以和谐地集成内容和样式，从而成功地产生了样式符合风格和表现力的样式化图像。

### QADM-Net: Multi-Level Quality-Adaptive Dynamic Network for Reliable Multimodal Classification 
[[arxiv](https://arxiv.org/abs/2412.14489)] [[cool](https://papers.cool/arxiv/2412.14489)] [[pdf](https://arxiv.org/pdf/2412.14489)]
> **Authors**: Shu Shen,Tong Zhang,C. L. Philip Chen
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: 9 pages, 5 figures
- **标题**: QADM-NET：用于可靠多模式分类的多级质量自适应动态网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 在许多情况下，多模式的机器学习取得了显着的进步，但其可靠性因不同的样本质量而损害了。在本文中，我们发现当前的多模式分类方法缺乏针对样品特异性深度和参数的动态网络，无法实现可靠的推断。为此，提出了一个新的多模式可靠分类框架，称为多级质量自适应动态多模式网络（QADM-NET）。 QADM-NET首先采用了一种基于无噪声原型和无分类器设计的新方法，以可靠地估计每个样品在模态和特征水平上的质量。然后，它通过\ textbf {\ textit {全局置信度归一化深度（gcnd）}}机制实现了特定于样本的网络深度。通过跨模态和样本的深度，\ textIt {\ textbf {gcnd}}有效地减轻了具有挑战性的模态输入对动态深度可靠性的影响。此外，QADM-NET通过\ textbf {\ textIt {layer-wise greedy参数（LGP）}}提供了由功能级质量驱动的机制。 \ textbf {\ textit {lgp}}中的跨模性层面的贪婪策略设计了一个可靠的参数预测范式，用于首次具有可变深度的多模式网络。在四个数据集上进行的实验表明，QADM-NET在分类性能和可靠性方面显着优于最先进的方法，表现出强大的适应性，以多种质量的数据适应性。

### MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval 
[[arxiv](https://arxiv.org/abs/2412.14475)] [[cool](https://papers.cool/arxiv/2412.14475)] [[pdf](https://arxiv.org/pdf/2412.14475)]
> **Authors**: Junjie Zhou,Zheng Liu,Ze Liu,Shitao Xiao,Yueze Wang,Bo Zhao,Chen Jason Zhang,Defu Lian,Yongping Xiong
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: MeGapairs：通用多模式检索的大量数据合成
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 尽管对多模式检索的需求迅速增长，但由于缺乏培训数据，该领域的进展仍然严重限制。在本文中，我们介绍了MeGapairs，这是一种新型的数据综合方法，该方法利用视觉语言模型（VLM）和开放域图像，以及该方法生成的大量合成数据集。我们的经验分析表明，MeGapairs生成了高质量的数据，从而使多模式检索器能够显着超过从现有数据集中培训的基线模型。此外，由于MeGapair仅依赖一般图像语料库和开源VLM，因此可以轻松地扩展它，从而可以持续改进检索性能。在此阶段，我们制作了超过2600万次培训实例，并使用此数据培训了几种不同尺寸的型号。这些新模型在4个流行的图像检索（CIR）基准和MMEB提供的36个数据集中的最高总体性能中实现了最先进的零拍性能。他们还通过额外的下游微调表现出显着的性能提高。我们生产的数据集，训练有素的模型和数据合成管道将公开使用，以促进该领域的未来发展。

### Multimodal Latent Diffusion Model for Complex Sewing Pattern Generation 
[[arxiv](https://arxiv.org/abs/2412.14453)] [[cool](https://papers.cool/arxiv/2412.14453)] [[pdf](https://arxiv.org/pdf/2412.14453)]
> **Authors**: Shengqi Liu,Yuhao Cheng,Zhuo Chen,Xingyu Ren,Wenhan Zhu,Lincheng Li,Mengxiao Bi,Xiaokang Yang,Yichao Yan
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Our project page: https://shengqiliu1.github.io/SewingLDM
- **标题**: 复杂缝纫模式产生的多模式潜扩散模型
- **领域**: 计算机视觉和模式识别,图形,机器学习
- **摘要**: 由于CG友好且灵活的编辑性质，服装设计中的缝纫图案正在受到越来越多的关注。以前的缝纫模式生成方法已经能够生产出精美的衣服，但很难设计具有详细控制的复杂服装。为了解决这些问题，我们提出了SewingLdm，这是一种多式联运生成模型，生成由文本提示，身体形状和服装草图控制的缝纫模式。最初，我们将缝纫模式的原始向量扩展到更全面的表示形式中，以涵盖更复杂的细节，然后将其压缩到紧凑的潜在空间中。为了学习潜在空间中的缝纫图案分布，我们设计了一个两步训练策略，以注入多模式的条件，即\ ie，身体形状，文本提示和服装草图，以确保生成的服装是身体适合的和细节控制的。全面的定性和定量实验表明了我们提出的方法的有效性，在复杂的服装设计和各种身体适应性方面显着超过了先前的方法。我们的项目页面：https：//shengqiliu1.github.io/sewingldm。

### FedPIA -- Permuting and Integrating Adapters leveraging Wasserstein Barycenters for Finetuning Foundation Models in Multi-Modal Federated Learning 
[[arxiv](https://arxiv.org/abs/2412.14424)] [[cool](https://papers.cool/arxiv/2412.14424)] [[pdf](https://arxiv.org/pdf/2412.14424)]
> **Authors**: Pramit Saha,Divyanshu Mishra,Felix Wagner,Konstantinos Kamnitsas,J. Alison Noble
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Accepted for publication in AAAI 2025 (Main Track)
- **标题**: FEDPIA-在多模式联合学习中，将适配器杠杆化的适配器杠杆化用于填充基础模型
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 大型视觉模型通常需要大型文本和图像数据集，以进行有效的微调。但是，由于严格的隐私法规，从各个站点（尤其是医疗保健）中收集数据是具有挑战性的。另一种选择是在最终用户设备（例如在医疗诊所）上微调这些模型，而无需将数据发送到服务器。这些本地客户通常具有有限的计算能力和小型数据集，这还不足以自行进行大型VLM。对这些情况的幼稚解决方案是利用参数有效的微调（PEFT）策略，并应用联合学习（FL）算法来结合学习的适配器权重，从而尊重资源限制和数据隐私。但是，这种方法并不能完全利用多个适配器的知识，这些适配器对各种数据分布和各种任务进行了培训。这些适配器受到数据异质性和任务异质性的不利影响，导致次优融合。为此，我们提出了一个名为FEDPIA的新颖框架，该框架通过在服务器中引入置换和集成局部适配器的置换和整合，以改善服务器和全球适配器在利用Wasserstein Barycenters中的固定和整合，以改善客户端特定和客户端 - 竞技知识的混合。在集成之前，此层置换有助于弥合本地和全局适配器的参数空间的差距。我们在五个不同的医学视觉语言fl任务设置中使用48个医疗图像数据集进行了2000多个客户级实验，其中包括视觉问题答案以及图像和基于报告的多标签疾病检测。我们的实验涉及各种客户设置，十种不同的方式和两个VLM骨架，这表明FEDPIA始终超过最先进的PEFT-FL-FL基线。

### Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception 
[[arxiv](https://arxiv.org/abs/2412.14233)] [[cool](https://papers.cool/arxiv/2412.14233)] [[pdf](https://arxiv.org/pdf/2412.14233)]
> **Authors**: Yanpeng Sun,Jing Hao,Ke Zhu,Jiang-Jiang Liu,Yuxiang Zhao,Xiaofan Li,Gang Zhang,Zechao Li,Jingdong Wang
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: An open-source data engine for generating detailed image captions
- **标题**: 具有视觉专家的描述性标题增强了多模式感知
- **领域**: 计算机视觉和模式识别
- **摘要**: 培训大型多模型模型（LMM）依赖于连接图像和语言的描述性图像标题。现有方法要么从LMM模型中提取标题，要么从Internet图像中构造字幕或通过人类的标题。我们建议利用现成的视觉专家，这些视觉专家是从注释的图像中训练的，最初不是用于图像字幕，以增强图像标题。我们的方法名为DCE，探索对象低级别和细粒度属性（例如，深度，情感和细粒类别）和对象关系（例如，相对位置和人类对象相互作用（HOI）），并将属性结合到描述性标题中。实验表明，此类视觉专家能够提高视觉理解任务的性能以及从更准确的视觉理解中受益的推理。我们将发布源代码和管道，以便将其他视觉专家轻松合并到管道中。 DCE管道和数据集的完整源代码将在\ url {https://github.com/syp2ysy/dce}上找到。

### Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces 
[[arxiv](https://arxiv.org/abs/2412.14171)] [[cool](https://papers.cool/arxiv/2412.14171)] [[pdf](https://arxiv.org/pdf/2412.14171)]
> **Authors**: Jihan Yang,Shusheng Yang,Anjali W. Gupta,Rilyn Han,Li Fei-Fei,Saining Xie
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Project page: https://vision-x-nyu.github.io/thinking-in-space.github.io/
- **标题**: 在太空中思考：多模式大语模型如何看，记住和回忆空间
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类拥有视觉空间智能，以记住顺序视觉观察中的空间。但是，在数百万级视频数据集上训练的多模式大语言模型（MLLM）也可以``从视频中思考''？我们提出了一个新颖的基于视频的视觉空间智能基准（VSI基准），该基准超过5,000个问答对，并发现MLLM表现出竞争性的 - 尽管是亚人类 - 视觉空间智能。我们探究模型，以表达他们在语言和视觉上的思维方式，并发现尽管空间推理能力仍然是MLLM达到更高基准性能的主要瓶颈，但本地世界模型和空间意识确实在这些模型中出现了。值得注意的是，普遍的语言推理技术（例如，经营链，自一致性，思想树）无法提高性能，而在提问期间明确产生认知图提高了MLLMS的空间距离能力。

### FashionComposer: Compositional Fashion Image Generation 
[[arxiv](https://arxiv.org/abs/2412.14168)] [[cool](https://papers.cool/arxiv/2412.14168)] [[pdf](https://arxiv.org/pdf/2412.14168)]
> **Authors**: Sihui Ji,Yiyang Wang,Xi Chen,Xiaogang Xu,Hao Luo,Hengshuang Zhao
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: https://sihuiji.github.io/FashionComposer-Page
- **标题**: FashionComposer：作曲时尚形象生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了构图时尚形象生成的FashionComposer。与以前的方法不同，FashionComposer具有很高的灵活性。它采用了多模式输入（即文本提示，参数人类模型，服装图像和面部图像），并支持人类的外观，姿势和图形个性化，并在一个通过中分配多个服装。为了实现这一目标，我们首先开发了一个能够处理多种输入方式的通用框架。我们构建缩放训练数据，以增强模型的强大组成能力。为了无缝地容纳多个参考图像（服装和面部），我们将这些引用在单个图像中以“资产库”的形式组织，并采用参考UNET来提取外观特征。为了将外观特征注入生成的结果中的正确像素，我们提出了主题结合的注意。它将不同“资产”的外观特征与相应的文本特征绑定在一起。通过这种方式，该模型可以根据其语义来理解每个资产，支持任意数字和参考图像的类型。作为一个全面的解决方案，FashionComposer还支持许多其他应用程序，例如人类专辑的生成，多样化的虚拟尝试任务等。

### MetaMorph: Multimodal Understanding and Generation via Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2412.14164)] [[cool](https://papers.cool/arxiv/2412.14164)] [[pdf](https://arxiv.org/pdf/2412.14164)]
> **Authors**: Shengbang Tong,David Fan,Jiachen Zhu,Yunyang Xiong,Xinlei Chen,Koustuv Sinha,Michael Rabbat,Yann LeCun,Saining Xie,Zhuang Liu
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Project page at tsb0601.github.io/metamorph
- **标题**: Metamorph：通过教学调整的多模式理解和生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项工作中，我们提出了视觉预测性教学调整（VPIT） - 一种简单有效的视觉指导调谐扩展，使经过审计的LLM可以快速变成能够同时产生文本和视觉令牌的统一自动回归模型。 VPIT教授一个LLM，以预测以指令遵循格式策划的任何图像和文本数据的输入序列的连续视觉令牌。我们的实证研究揭示了VPIT的几种有趣的特性：（1）视觉产生能力是改善视觉理解的自然副产品，并且可以通过少量的生成数据有效地解锁； （2）尽管我们发现理解和发电是互惠互利的，但了解数据比发电数据更有效地有助于两者。在这些发现的基础上，我们训练了我们的Metamorph模型，并在视觉理解和发电上取得了竞争性能。在视觉产生中，Metamorph可以利用LLM预处理获得的世界知识和推理能力，并克服其他一代模型所表现出的常见故障模式。我们的结果表明，LLMS可能具有强大的“先前”视力功能，可以通过相对简单的说明调整过程有效地适应视觉理解和产生。

### AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities 
[[arxiv](https://arxiv.org/abs/2412.14123)] [[cool](https://papers.cool/arxiv/2412.14123)] [[pdf](https://arxiv.org/pdf/2412.14123)]
> **Authors**: Guillaume Astruc,Nicolas Gonthier,Clement Mallet,Loic Landrieu
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: Anysat：任何分辨率，尺度和方式的地球观察模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 地理空间模型必须根据决议，量表和方式适应地球观察数据的多样性。但是，现有方法可以预期固定的输入配置，从而限制其实际适用性。我们提出了AnySAT，这是一种基于关节嵌入预测架构（JEPA）和解决自适应空间编码器的多模型模型，使我们能够以自我监督的方式对高度异构数据进行训练。为了证明这种统一方法的优势，我们编译了Geoplex，该集合的$ 5 $多模式数据集具有不同的特性和$ 11 $不同的传感器。然后，我们同时在这些不同的数据集上训练一个强大的模型。一旦进行了微调，我们将在Geoplex的数据集上取得更好或附近的最新结果，并获得$ 4 $的$ 4 $额外费用，以$ 5 $环境监控任务：土地覆盖地图，树种物种识别，作物类型分类，变化检测和洪水细分。代码和型号可在https://github.com/gastruc/anysat上找到。

### A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future 
[[arxiv](https://arxiv.org/abs/2412.14056)] [[cool](https://papers.cool/arxiv/2412.14056)] [[pdf](https://arxiv.org/pdf/2412.14056)]
> **Authors**: Shilin Sun,Wenbin An,Feng Tian,Fang Nan,Qidong Liu,Jun Liu,Nazaraf Shah,Ping Chen
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: 多模式可解释的人工智能的评论：过去，现在和未来
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,多媒体
- **摘要**: 人工智能（AI）通过计算能力的进步和大规模数据集的增长而迅速发展。但是，这种进步在解释AI模型的“黑盒”性质方面也加大了挑战。为了解决这些问题，可以解释的AI（XAI）出现，重点是透明度和解释性，以增强人类对AI决策过程的理解和信任。在多模式数据融合和复杂的推理方案的背景下，多模式可解释AI（MXAI）的建议集成了多种模式以进行预测和解释任务。同时，大语言模型（LLM）的出现导致了自然语言处理的显着突破，但它们的复杂性进一步加剧了MXAI问题。为了对MXAI方法的开发进行关键见解，并为建立更透明，公平和值得信赖的AI系统提供了重要的指导，我们从历史的角度回顾了MXAI方法，并将它们分类为四个时代：传统的机器学习，深度学习，深度学习，歧视性基础模型和生成的LLMS。我们还审查了MXAI研究中使用的评估指标和数据集，最后讨论了未来的挑战和方向。与此评论有关的项目已在https://github.com/shilinsun/mxai_review上创建。

### InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.14006)] [[cool](https://papers.cool/arxiv/2412.14006)] [[pdf](https://arxiv.org/pdf/2412.14006)]
> **Authors**: Cong Wei,Yujie Zhong,Haoxian Tan,Yingsen Zeng,Yong Liu,Zheng Zhao,Yujiu Yang
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 指示：通过多模式大语言模型统一指示的视觉细分
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过多模式大语言模型（MLLM）的增强，图像和视频域的文本引导的通用分割模型最近已取得了迅速的进步。但是，这些方法通常是针对特定领域分开开发的，可以忽略这两个领域的任务设置和解决方案的相似性。在本文中，我们根据指示的视觉分割（IVS）定义了在图像和视频级别上引用分割和推理分割的结合。相应地，我们建议使用IVS配备MLLM的端到端分段管道。具体来说，我们采用一种对象感知的视频感知器来从参考帧中提取时间和对象信息，从而促进全面的视频理解。此外，我们介绍了视觉引导的多粒性文本融合，以更好地将全球和详细的文本信息与细粒度的视觉指导整合在一起。通过利用多任务和端到端的培训，指示可以展示各种图像和视频细分任务的卓越性能，通过单个模型超越了分割专家和基于MLLM的方法。我们的代码可在https://github.com/congvvc/instructsseg上找到。

### LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer 
[[arxiv](https://arxiv.org/abs/2412.13871)] [[cool](https://papers.cool/arxiv/2412.13871)] [[pdf](https://arxiv.org/pdf/2412.13871)]
> **Authors**: Yipeng Zhang,Yifan Liu,Zonghao Guo,Yidan Zhang,Xuesong Yang,Chi Chen,Jun Song,Bo Zheng,Yuan Yao,Zhiyuan Liu,Tat-Seng Chua,Maosong Sun
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: llava-uhd v2：通过分层窗口变压器整合高分辨率特征金字塔的MLLM
- **领域**: 计算机视觉和模式识别
- **摘要**: 在多模式的大语言模型（MLLM）中，视觉变压器（VIT）被广泛用于视觉编码。但是，它们在解决通用MLLM任务方面的表现并不令人满意。我们将其归因于缺乏各种视觉水平的信息，这阻碍了语言生成所需的各种语义粒度的一致性。为了解决这个问题，我们提出了Llava-UHD V2，这是一个围绕层次结构窗口变压器的高级MLLM，可以通过构建和集成高分辨率特征金字塔来捕获各种视觉粒度。 As a vision-language projector, Hiwin transformer comprises two primary modules: (i) an inverse feature pyramid, constructed by a ViT-derived feature up-sampling process utilizing high-frequency details from an image pyramid, and (ii) hierarchical window attention, focusing on a set of key sampling features within cross-scale windows to condense multi-level feature maps.广泛的实验表明，Llava-UHD V2在流行基准上的现有MLLM表现出色。值得注意的是，我们的设计与基线方法相比，在14个基准测试中的平均增长率为3.7％，例如，DOCVQA的平均增长率为9.3％。我们将公开使用所有数据，模型检查点和代码，以促进未来的研究。

### Do Language Models Understand Time? 
[[arxiv](https://arxiv.org/abs/2412.13845)] [[cool](https://papers.cool/arxiv/2412.13845)] [[pdf](https://arxiv.org/pdf/2412.13845)]
> **Authors**: Xi Ding,Lei Wang
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Accepted for publication in the Companion Proceedings of the ACM Web Conference (WWW Companion 2025)
- **标题**: 语言模型了解时间吗？
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 大型语言模型（LLM）彻底改变了基于视频的计算机视觉应用程序，包括动作识别，异常检测和视频摘要。视频固有地构成了独特的挑战，将空间复杂性与静态图像或文本数据中不存在的时间动态相结合。当前使用LLM的视频理解方法通常依赖于验证的视频编码器来提取时空特征和文本编码器以捕获语义含义。这些表示形式集成在LLM框架中，从而实现了各种视频任务的多模式推理。但是，关键的问题仍然存在：LLM可以真正理解时间的概念，以及他们如何有效地理解视频中的时间关系？这项工作批判性地研究了LLM在视频处理中的作用，并特别关注其时间推理能力。我们确定了LLM与预审慎的编码器之间相互作用的关键局限性，从而揭示了它们建模长期依赖性和抽象时间概念（例如因果关系和事件进程）的能力的差距。此外，我们分析了现有视频数据集提出的挑战，包括偏见，缺乏时间注释以及限制了对LLMS的时间理解的特定领域的限制。为了解决这些差距，我们探讨了有希望的未来方向，包括LLM和编码器的共同发展，具有明确的时间标签的丰富数据集的开发以及用于整合空间，时间和语义推理的创新体系结构。通过解决这些挑战，我们旨在提高LLM的时间理解，从而在视频分析及其他方面发挥其全部潜力。我们的论文的GitHub存储库可以在https://github.com/darcyddx/video-llm上找到。

### CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers 
[[arxiv](https://arxiv.org/abs/2412.13810)] [[cool](https://papers.cool/arxiv/2412.13810)] [[pdf](https://arxiv.org/pdf/2412.13810)]
> **Authors**: Dimitrios Mallis,Ahmet Serdar Karadeniz,Sebastian Cavada,Danila Rukhovich,Niki Foteinopoulou,Kseniya Cherenkova,Anis Kacem,Djamila Aouada
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: CAD助剂：工具调节的VLLM作为通用CAD任务解决器
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术
- **摘要**: 我们建议使用AI辅助设计的通用CAD代理CAD辅助。我们的方法是基于强大的愿景和大型语言模型（VLLM）作为计划者和使用CAD特定工具的工具功能范式。 CAD辅助因素通过在配备了FreeCad软件的Python解释器上迭代执行的操作来解决多模式用户查询，该操作是通过其Python API访问的。我们的框架能够评估生成的CAD命令对几何形状的影响，并根据CAD设计的不断发展状态调整后续操作。我们考虑了广泛的CAD特异性工具，包括草图图像参数器，渲染模块，2D横截面生成器和其他专用例程。在多个CAD基准测试中评估了CAD辅助因子，在该基准测试中，它的表现优于VLLM基准和受监管的特定任务方法。除了现有的基准测试之外，我们从定性地展示了工具增强的VLLM作为跨不同工作流程的通用CAD求解器的潜力。

### Modeling Multi-modal Cross-interaction for Multi-label Few-shot Image Classification Based on Local Feature Selection 
[[arxiv](https://arxiv.org/abs/2412.13732)] [[cool](https://papers.cool/arxiv/2412.13732)] [[pdf](https://arxiv.org/pdf/2412.13732)]
> **Authors**: Kun Yan,Zied Bouraoui,Fangyun Wei,Chang Xu,Ping Wang,Shoaib Jameel,Steven Schockaert
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: In Transactions on Multimedia Computing Communications and Applications. arXiv admin note: text overlap with arXiv:2112.01037
- **标题**: 基于本地特征选择，对多标签的多牌差异图像分类进行建模
- **领域**: 计算机视觉和模式识别
- **摘要**: 多标签少数图像分类（ML-FSIC）的目的是将语义标签分配给图像，在每个标签中只有少量培训示例的设置中。多标签设置的关键特征是图像通常具有多个标签，通常指在图像的不同区域中出现的对象。当估计标签原型时，在基于公制的设置中，确定哪些区域与哪些标签相关，但是训练数据的数量有限，局部特征的嘈杂性质使得这一高度挑战性。作为解决方案，我们提出了一种逐渐完善标签原型的策略。首先，我们使用单词嵌入来初始化原型，这使我们能够利用有关标签含义的先验知识。其次，利用这些初始原型，我们使用损失变化测量（LCM）策略从训练图像（即支持集）中选择最有可能代表给定标签的局部特征。第三，我们通过使用多模式的交叉交互机制汇总这些代表性的局部特征来构建标签的最终原型，该特征再次依赖于初始单词基于嵌入的原型。关于可可，帕斯卡VOC，范围内和临时主义的实验表明，我们的模型显着改善了当前的最新技术。

### JoVALE: Detecting Human Actions in Video Using Audiovisual and Language Contexts 
[[arxiv](https://arxiv.org/abs/2412.13708)] [[cool](https://papers.cool/arxiv/2412.13708)] [[pdf](https://arxiv.org/pdf/2412.13708)]
> **Authors**: Taein Son,Soo Won Seo,Jisong Kim,Seok Hwan Lee,Jun Won Choi
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Accepted to AAAI Conference on Artificial Intelligence 2025, 10 pages, 6 figures
- **标题**: Jovale：使用视听和语言上下文在视频中检测人类行为
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频动作检测（VAD）需要在视频中进行本地化和分类，该实例固有地由各种信息源组成，例如音频，视觉提示和周围场景环境。为VAD有效地利用此多模式信息带来了重大挑战，因为该模型必须精确地识别与动作相关的提示。在这项研究中，我们介绍了一种新型的多模式VAD体系结构，称为以参与者为中心的视觉，音频，语言编码器（Jovale）。 Jovale是将音频和视觉特征与场景描述上下文集成到来自大容量图像字幕模型的第一个VAD方法。 Jovale的核心是以演员为中心的音频，视觉和场景描述性信息的聚合，从而可以自适应地整合至关重要的特征，以识别每个演员的行为。我们开发了一个基于变压器的体系结构，即以演员为中心的多模式融合网络，该网络专为捕获参与者及其多模式环境之间的动态交互。我们对包括AVA，UCF101-24和JHMDB51-21在内的三个突出的VAD基准测试的评估表明，合并多模式信息可显着提高性能，从而在该领域设置新的先进性能。

### GLCF: A Global-Local Multimodal Coherence Analysis Framework for Talking Face Generation Detection 
[[arxiv](https://arxiv.org/abs/2412.13656)] [[cool](https://papers.cool/arxiv/2412.13656)] [[pdf](https://arxiv.org/pdf/2412.13656)]
> **Authors**: Xiaocan Chen,Qilin Yin,Jiarui Liu,Wei Lu,Xiangyang Luo,Jiantao Zhou
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: GLCF：一个全局本地的多模式相干分析框架，用于说话面部发电检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 说话的面部生成（TFG）允许仅使用面部图像和随附的文字制作任何角色的栩栩如生的说话视频。滥用这项技术可能会对社会构成重大风险，从而迫切需要研究相应的检测方法。但是，缺乏公共数据集阻碍了该领域的研究。在本文中，我们构建了第一个大规模的多幕科说话面部数据集（MSTF），其中包含22个音频和视频伪造技术，从而填补了该字段中数据集的空白。该数据集涵盖了11个一代方案和20多个语义方案，更接近TFG的实际应用程序。此外，我们还提出了一个TFG检测框架，该框架利用TFG视频的多模式含量中的全球和局部连贯性分析。因此，引入了以区域为中心的平滑度检测模块（RSFDM）和存在范围的捕获时间框架聚集模块（DCTAM），以评估TFG视频的全局时间相干性，从而汇总了多元透明空间信息。此外，视觉原告融合模块（V-AFM）旨在评估局部时间视角内的视听连贯性。全面的实验证明了我们数据集的合理性和挑战，同时也表明了我们所提出的方法的优越性与最先进的深层检测方法相比。

### RelationField: Relate Anything in Radiance Fields 
[[arxiv](https://arxiv.org/abs/2412.13652)] [[cool](https://papers.cool/arxiv/2412.13652)] [[pdf](https://arxiv.org/pdf/2412.13652)]
> **Authors**: Sebastian Koch,Johanna Wald,Mirco Colosi,Narunas Vaskevicius,Pedro Hermosilla,Federico Tombari,Timo Ropinski
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Project page: https://relationfield.github.io
- **标题**: 关系菲尔德：关联辐射领域的任何内容
- **领域**: 计算机视觉和模式识别
- **摘要**: 神经辐射场是一个新兴的3D场景表示形式，最近甚至扩展了以通过从视觉语言模型中提取开放式摄影特征来学习场景理解的功能。但是，当前方法主要集中于以对象为中心的表示，支持对象分割或检测，而了解对象之间的语义关系仍然很大程度上没有探索。为了解决这一差距，我们提出了RealationField，这是直接从神经辐射场中提取对象间关系的第一种方法。关系场表示对象之间的关系是神经辐射场中的射线对，有效地扩展了其公式，以包括隐式关系查询。为了教授RealationField Complex，开放式摄影关系，关系知识是从多模式LLM中提炼出来的。为了评估RealationField，我们解决了开放式摄影3D场景图生成任务和关系指导实例细分，从而在这两个任务中都实现了最新的性能。请参阅项目网站https://relationfield.github.io。

### G-VEval: A Versatile Metric for Evaluating Image and Video Captions Using GPT-4o 
[[arxiv](https://arxiv.org/abs/2412.13647)] [[cool](https://papers.cool/arxiv/2412.13647)] [[pdf](https://arxiv.org/pdf/2412.13647)]
> **Authors**: Tony Cheng Tong,Sirui He,Zhiwen Shao,Dit-Yan Yeung
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: G-Veval：用于使用GPT-4O评估图像和视频字幕的多功能度量
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 视觉字幕的评估指标很重要，但尚未彻底探索。诸如Bleu，Meteor，Acider和Rouge之类的传统指标经常错过语义深度，而训练有素的指标（例如剪辑得分，PAC-S和Polos）在零照片的情况下受到限制。基于先进的语言模型的指标也努力与细微的人类偏好保持一致。为了解决这些问题，我们介绍了G-Veval，这是一种受G-Eval启发并由新GPT-4O提供支持的新颖指标。 G-Veval在大型多模式模型中使用经过思考的推理，并支持三种模式：无参考，仅参考和合并，可容纳视频和图像输入。我们还建议MSVD-Eval（用于视频字幕评估的新数据集），以为人类专家和评估指标建立一个更透明，更一致的框架。它旨在通过引入准确性，完整性，简洁性和相关性（ACCR）的不同维度来解决现有数据集中缺乏明确的标准。广泛的结果表明，G-VEVAL的表现优于与人类注释相关的现有方法，如Kendall Tau-B和Kendall Tau-C所测量。这为各种字幕任务提供了灵活的解决方案，并为大型语言模型理解视频内容的简单而有效的方法提出了一种直接而有效的方法，为自动字幕的进步铺平了道路。代码可从https://github.com/ztangaj/gveval获得

### Self-control: A Better Conditional Mechanism for Masked Autoregressive Model 
[[arxiv](https://arxiv.org/abs/2412.13635)] [[cool](https://papers.cool/arxiv/2412.13635)] [[pdf](https://arxiv.org/pdf/2412.13635)]
> **Authors**: Qiaoying Qu,Shiyu Shen
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 自我控制：掩盖自回归模型的更好条件机制
- **领域**: 计算机视觉和模式识别
- **摘要**: 自回归的有条件图像生成算法能够生成与给定文本或图像条件一致的影像图像，并且具有广泛应用的巨大潜力。然而，大多数流行的自回归图像生成方法在很大程度上依赖于矢量量化，而代码书的固有特征却给实现高质量的图像生成带来了巨大的挑战。为了解决这一限制，本文介绍了一个新型的条件简介网络，用于连续掩盖自回归模型。所提出的自我控制网络可减轻向量量化对生成图像质量的负面影响，同时在生成过程中增强条件控制。特别是，自我控制网络是在连续的掩码自回归生成模型上构建的，该模型将多模式的条件信息（包括文本和图像）结合到统一的自动回归序列中。通过一种自我注意的机制，网络能够根据特定条件生成可控制的图像。自我控制网络丢弃了常规的基于跨注意的条件融合机制，并有效地统一了同一空间内的条件和生成信息，从而促进了更多无缝学习和多模式特征的融合。

### Query-centric Audio-Visual Cognition Network for Moment Retrieval, Segmentation and Step-Captioning 
[[arxiv](https://arxiv.org/abs/2412.13543)] [[cool](https://papers.cool/arxiv/2412.13543)] [[pdf](https://arxiv.org/pdf/2412.13543)]
> **Authors**: Yunbin Tu,Liang Li,Li Su,Qingming Huang
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Accepted by AAAI 2025
- **标题**: 以查询为中心的视听认知网络，以获取瞬间检索，分割和逐步计算
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 视频已成为互联网上最受欢迎的多媒体格式。为了更好地获取视频内容，提出了一个新主题，包括视频检索，力矩检索，瞬间细分和逐步攻击。开创性的工作选择了基于培训的剪辑模型进行视频检索，并利用它作为功能提取器，用于在多任务学习范式中解决的其他三个挑战性任务。然而，由于无视跨模式的层次结构和关联关系，这项工作仍在努力学习对用户偏爱内容的全面认知。在本文中，在浅至深度原理的指导下，我们提出了一个以查询为中心的视听认知（quag）网络，以构建一个可靠的多模式表示，以进行矩检修，分割和逐步攻击。具体而言，我们首先设计了模态性感知，以通过对视觉和音频模态之间的全局对比度对齐和局部细粒度相互作用进行建模，以获得丰富的视听内容。然后，我们设计了以查询为中心的认知，该认知使用深层查询来对浅水级别的视听表示进行时间通道过滤。这可以认识用户偏爱的内容，从而为三个任务获得以查询为中心的音频表示。广泛的实验表明，Quag在雇用上实现了SOTA结果。此外，我们测试基于查询的视频摘要任务并验证其良好的概括。

### Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for Fine-grained Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2412.13541)] [[cool](https://papers.cool/arxiv/2412.13541)] [[pdf](https://arxiv.org/pdf/2412.13541)]
> **Authors**: Jingyao Wang,Yuxuan Yang,Wenwen Qiang,Changwen Zheng,Hui Xiong
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: 13 pages, Submitted to TMM in 30-May-2024
- **标题**: 时空临时性的面向多模式的元学习，以识别细粒度
- **领域**: 计算机视觉和模式识别,机器学习,神经和进化计算
- **摘要**: 细粒度的情绪识别（FER）在各个领域（例如疾病诊断，个性化建议和多媒体挖掘）中起着至关重要的作用。但是，现有的FER方法在现实世界应用中面临三个主要挑战：（i）它们依靠大量不断注释的数据来确保准确性，因为情绪在现实中是复杂而模棱两可的，这是昂贵且耗时的； （ii）他们无法捕获不断变化的情绪模式引起的时间异质性，因为他们通常假定抽样期内的时间相关性是相同的； （iii）他们不考虑不同情况下不同情况的空间异质性，也就是说，情绪信息在不同数据中的分布可能具有偏见或干扰。为了应对这些挑战，我们提出了一个时空的临时模糊为导向的多模式元学习框架（ST-F2M）。具体而言，ST-F2M首先将多模式视频划分为多个视图，并且每个视图都对应于一种情感的一种方式。相同情绪的多个随机选择的视图构成了元训练任务。接下来，ST-F2M使用具有空间和时间卷积的集成模块来编码每个任务的数据，以反映空间和时间异质性。然后，它基于广义模糊规则为每个任务添加模糊的语义信息，这有助于处理情绪的复杂性和歧义。最后，ST-F2M通过元信息神经网络学习与情绪相关的一般元知识，以实现快速，强大的细粒情绪识别。广泛的实验表明，就准确性和模型效率而言，ST-F2M的表现优于各种最新方法。此外，我们构建了消融研究和进一步的分析，以探讨为什么ST-F2M表现良好。

### Language-guided Medical Image Segmentation with Target-informed Multi-level Contrastive Alignments 
[[arxiv](https://arxiv.org/abs/2412.13533)] [[cool](https://papers.cool/arxiv/2412.13533)] [[pdf](https://arxiv.org/pdf/2412.13533)]
> **Authors**: Mingjian Li,Mingyuan Meng,Shuchang Ye,Michael Fulham,Lei Bi,Jinman Kim
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 语言指导的医学图像分割，具有目标信息多层对比对准
- **领域**: 计算机视觉和模式识别
- **摘要**: 医学图像分割在现代医学图像分析中至关重要，这可以帮助诊断各种疾病。最近，语言引导的分割方法显示了自动化图像分割的有希望的结果，其中文本报告被纳入了指导。这些文本报告包含临床医生给出的图像印象和见解，提供了辅助指导。但是，这些方法忽略了两种不同模态之间的固有模式差距，这导致了亚最佳图像文本特征融合，而没有适当的交叉模式特征对齐。对比对准被广泛用于将图像文本语义与表示学习相关联；但是，尚未利用它来弥合语言引导的分割中的模式差距，该分段依赖于微妙的低级图像细节来代表疾病。现有的对比对准方法通常是Algin高级全局图像语义语义，而无需涉及低级别的局部目标信息，因此无法探索用于语言指导的细分细分的细粒文本指南。在这项研究中，我们提出了一个具有目标信息多层对比对准（TMCA）的语言引导分割网络。 TMCA启用目标有目标的跨模式对齐和细粒文本指导，以弥合语言引导的细分中的模式差距。具体来说，我们介绍：1）目标敏感的语义距离模块，该模块可以实现颗粒图像 - 文本对齐建模，以及2）多级比对策略，该策略指导低级图像特征的文本指导。此外，提出了一个语言引导的目标增强模块，以利用对齐的文本重定向注意，以重点关注关键的局部图像特征。在4个涉及3种医学成像方式的4个图像文本数据集上进行了广泛的实验，这表明我们的TMCA取得了出色的性能。

### GCA-3D: Towards Generalized and Consistent Domain Adaptation of 3D Generators 
[[arxiv](https://arxiv.org/abs/2412.15491)] [[cool](https://papers.cool/arxiv/2412.15491)] [[pdf](https://arxiv.org/pdf/2412.15491)]
> **Authors**: Hengjia Li,Yang Liu,Yibo Zhao,Haoran Cheng,Yang Yang,Linxuan Xia,Zekai Luo,Qibo Qiu,Boxi Wu,Tu Zheng,Zheng Yang,Deng Cai
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: GCA-3D：朝着3D发电机的广义和一致的域适应
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，出现了3D生成域的适应性，以使预训练的发电机适应其他域，而无需收集大量的数据集和摄像头姿势分布。通常，它们利用大规模训练的文本对图扩散模型来合成目标域的图像，然后微调3D模型。但是，它们遭受了数据生成的繁琐管道，这不可避免地引入了源域和合成数据集之间的姿势偏差。此外，它们没有被概括以支持一击图像引导的域的适应，这是由于更严重的姿势偏见和单个图像参考引入的其他身份偏见更具挑战性。为了解决这些问题，我们提出了GCA-3D，这是一种普遍且一致的3D域适应方法，而没有复杂的数据生成管道。与以前的管道方法不同，我们引入了多模式深度感知的评分蒸馏采样损失，以非交流方式有效地适应3D生成模型。这种多模式损失可以在文本提示和一击图像提示改编中启用GCA-3D。此外，它利用量渲染模块的每种实体深度图来减轻过度拟合的问题并保留结果的多样性。为了增强姿势和身份一致性，我们进一步提出了分层的空间一致性损失，以使源和目标域中生成的图像之间的空间结构对齐。实验表明，GCA-3D在效率，概括，姿势准确性和身份一致性方面优于先前的方法。

### Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage 
[[arxiv](https://arxiv.org/abs/2412.15484)] [[cool](https://papers.cool/arxiv/2412.15484)] [[pdf](https://arxiv.org/pdf/2412.15484)]
> **Authors**: Saehyung Lee,Seunghyun Yoon,Trung Bui,Jing Shi,Sungroh Yoon
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 迈向强大的超详细图像字幕：一种多重方法和双重评估指标
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语模型（MLLM）在产生高度详细的字幕方面表现出色，但经常产生幻觉。我们的分析表明，现有的幻觉检测方法在详细的标题上遇到了困难。我们将其归因于MLLM在其生成的文本上的依赖，而不是随着序列长度的增长而不是输入图像。为了解决这个问题，我们提出了一种多构方法，该方法利用LLM-MLLM协作来纠正给定标题。此外，我们介绍了一个评估框架和一个基准数据集，以促进对详细标题的系统分析。我们的实验表明，我们提出的评估方法比现有指标更好地与人类事实判断更好，并且改善MLLM事实的现有方法可能会在超详细图像字幕任务中缺乏。相反，我们提出的方法显着提高了字幕的事实准确性，甚至改善了GPT-4V产生的方法。最后，我们通过证明MLLM在VQA基准测试上的性能可能与其生成详细的图像标题的能力无关，从而强调了以VQA为中心的基准测试的局限。

### Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis 
[[arxiv](https://arxiv.org/abs/2412.15322)] [[cool](https://papers.cool/arxiv/2412.15322)] [[pdf](https://arxiv.org/pdf/2412.15322)]
> **Authors**: Ho Kei Cheng,Masato Ishii,Akio Hayakawa,Takashi Shibuya,Alexander Schwing,Yuki Mitsufuji
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: Project page: https://hkchengrex.github.io/MMAudio
- **标题**: 驯服高质量视频与原子综合的多模式联合培训
- **领域**: 计算机视觉和模式识别,机器学习,声音,音频和语音处理
- **摘要**: 我们建议使用新型的多模式联合训练框架MMAUDIO合成视频和可选文本条件，以合成高质量和同步音频。与仅在（有限）视频数据的情况下进行的单模式训练相反，MMAUDIO是通过更大尺寸，容易获得的文本原告数据共同培训的，以学会生成语义上一致的高质量音频样本。此外，我们通过条件同步模块改善了视听同步，该模块将视频条件与框架级别的音频潜在的条件保持一致。 Mmaudio经过培训，以流动匹配的目标培训，就音频质量，语义对齐和视听同步方面，在公共模型中实现了新的视频到原始模型，同时具有较低的推理时间（1.23s以生成8S剪辑）和仅157m的参数。 Mmaudio在文本到原告的一代中还取得了令人惊讶的竞争性能，表明联合培训并不妨碍单模性能。代码和演示可在以下网址提供：https：//hkchengrex.github.io/mmaudio

### OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2412.15208)] [[cool](https://papers.cool/arxiv/2412.15208)] [[pdf](https://arxiv.org/pdf/2412.15208)]
> **Authors**: Shuo Xing,Chengyuan Qian,Yuping Wang,Hongyuan Hua,Kexin Tian,Yang Zhou,Zhengzhong Tu
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: The 3rd WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD) 2025
- **标题**: 开放式：用于端到端自动驾驶的开源多模型
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 自多模式大语言模型（MLLM）出现以来，它们在广泛的现实应用程序中产生了重大影响，尤其是在自动驾驶（AD）中。他们处理复杂的视觉数据和有关复杂驾驶场景的理由的能力为端到端广告系统的新范式铺平了道路。但是，由于现有的微调方法需要大量资源，包括广泛的计算能力，大规模数据集和大量资金，因此为AD开发端到端模型的进展一直很慢。从推理计算的最新进步中汲取灵感，我们提出了Openemma，这是一个基于MLLM的开源端到端框架。通过合并经过思考的推理过程，与利用各种MLLM的基线相比，开emma可以取得重大改进。此外，Openemma在各种具有挑战性的驾驶场景中表现出有效性，可推广性和鲁棒性，为自主驾驶提供了更有效的方法。我们在https://github.com/taco-group/openemma中发布所有代码。

### EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues 
[[arxiv](https://arxiv.org/abs/2412.15190)] [[cool](https://papers.cool/arxiv/2412.15190)] [[pdf](https://arxiv.org/pdf/2412.15190)]
> **Authors**: Sagar Soni,Akshay Dudhane,Hiyam Debary,Mustansar Fiaz,Muhammad Akhtar Munir,Muhammad Sohail Danish,Paolo Fraccaro,Campbell D Watson,Levente J Klein,Fahad Shahbaz Khan,Salman Khan
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 地球：将多感觉的地球观测转向交互式对话
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过交互式视觉语言模型（VLM）对广阔的地球观察数据的自动分析可以解锁环境监测，灾难响应和资源管理的新机会。现有的通用VLM在遥感数据上表现不佳，而最近的地理空间VLM仍仅限于固定分辨率和几乎没有传感器模式。在本文中，我们介绍了专为地球观察（EO）数据设计的对话助手Earthdial，将复杂的，多感官的地球观测转化为交互式自然语言对话。 Earthdial支持多光谱，多时间和多分辨率图像，可实现各种遥感任务，包括分类，检测，字幕，问题答案，视觉推理和视觉接地。为了实现这一目标，我们引入了广泛的指令调整数据集，其中包括超过11.11亿的指令对，涵盖RGB，合成孔径雷达（SAR）以及多光谱模态，例如近红外（NIR）和红外线。此外，Earthdial处理更改检测等应用的双向和多时间序列分析。我们对37个下游应用的广泛实验结果表明，地球的表现优于现有的通用和域特异性模型，从而在各种EO任务中实现了更好的概括。

### Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM 
[[arxiv](https://arxiv.org/abs/2412.15156)] [[cool](https://papers.cool/arxiv/2412.15156)] [[pdf](https://arxiv.org/pdf/2412.15156)]
> **Authors**: Yatai Ji,Jiacheng Zhang,Jie Wu,Shilong Zhang,Shoufa Chen,Chongjian GE,Peize Sun,Weifeng Chen,Wenqi Shao,Xuefeng Xiao,Weilin Huang,Ping Luo
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 提示-A-VIDEO：通过偏好一致的LLM提示您的视频扩散模型
- **领域**: 计算机视觉和模式识别,计算语言学,多媒体
- **摘要**: 文本到视频模型通过优化高质量的文本视频对取得了显着的进步，其中文本提示在确定输出视频的质量方面发挥了关键作用。但是，实现所需的输出通常需要多次修订和迭代推断，以完善用户提供的提示。当前用于提示提示的自动方法会遇到挑战，例如将其应用于文本对视频扩散模型时，诸如模态构成，成本范围和模型 - 纳维尔。为了解决这些问题，我们引入了一个基于LLM的及时适应框架，称为及时的A-Video，该框架在制作以视频为中心的，无劳动和偏好的提示提示中擅长于特定的视频扩散模型。我们的方法涉及精心制作的两阶段优化和对齐系统。最初，我们进行了奖励引导的及时进化管道，以自动创建最佳提示池并利用它们进行LLM的监督微调（SFT）。然后，使用多维奖励来为SFT模型生成成对数据，然后进行直接偏好优化（DPO）算法，以进一步促进偏好比对。通过广泛的实验和比较分析，我们验证了跨不同生成模型的及时video的有效性，突出了其突破视频生成界限的潜力。

### Movie2Story: A framework for understanding videos and telling stories in the form of novel text 
[[arxiv](https://arxiv.org/abs/2412.14965)] [[cool](https://papers.cool/arxiv/2412.14965)] [[pdf](https://arxiv.org/pdf/2412.14965)]
> **Authors**: Kangning Li,Zheyang Jia,Anyu Ying
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: Movie2Story：理解视频和以新颖文本形式讲故事的框架
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **摘要**: 近年来，大型模型取得了重大进步，并伴随着许多高质量基准，用于评估其理解能力的各个方面。但是，大多数现有基准主要集中于静态图像任务中的空间理解。尽管某些基准测试将评估扩展到时间任务，但它们在涉及长时间视频和丰富辅助信息的复杂环境下评估文本生成方面缺乏。为了解决这一限制，我们提出了一种新颖的基准：多模式故事生成基准（MSBench），旨在评估富含辅助信息的方案中文本生成功能。我们的工作介绍了一种创新的自动数据集生成方法，以确保获得准确的辅助信息。一方面，我们利用现有数据集并应用自动化流程来生成新的评估数据集，从而大大减少了手动工作。另一方面，我们通过系统过滤来完善辅助数据，并利用最先进的模型来确保地面图数据集的公平性和准确性。我们的实验表明，当前的多模式大语言模型（MLLM）在拟议的评估指标下次优，突出了其能力的显着差距。为了应对这些挑战，我们提出了一种新颖的模型体系结构和方法，以更好地处理整体过程，从而证明了基准的改进。

### Multimodal Hypothetical Summary for Retrieval-based Multi-image Question Answering 
[[arxiv](https://arxiv.org/abs/2412.14880)] [[cool](https://papers.cool/arxiv/2412.14880)] [[pdf](https://arxiv.org/pdf/2412.14880)]
> **Authors**: Peize Li,Qingyi Si,Peng Fu,Zheng Lin,Yan Wang
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: AAAI 2025
- **标题**: 基于检索的多图像问题回答的多模式假设摘要
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于检索的多图像问答（QA）任务涉及检索多个与问题相关的图像并综合这些图像以生成答案。传统的“检索到回答”管道通常会遭受级联错误，因为QA的训练目标未能优化检索阶段。为了解决这个问题，我们提出了一种新颖的方法，以有效地将信息引入和引用质量检查中。鉴于要检索的图像，我们采用了多模式的大语言模型（视觉透视图）和大型语言模型（文本透视图），以获取问题形式和描述形式中的多模式假设摘要。通过结合视觉和文本视角，MHYS可以更具体地捕获图像内容，并取代检索中的真实图像，从而消除了模态差距，通过转换为文本到文本检索并有助于改善检索。为了更有利地引入QA检索，我们采用对比度学习将查询（问题）与MHYS保持一致。此外，我们提出了一种粗略的策略，用于计算句子级别和单词级别的相似性得分，以进一步增强检索并滤除不相关的细节。我们的方法比RETVQA的最先进方法可实现3.7％的绝对提高，比剪辑提高了14.5％。全面的实验和详细的消融研究证明了我们方法的优越性。

### Multi-Level Embedding and Alignment Network with Consistency and Invariance Learning for Cross-View Geo-Localization 
[[arxiv](https://arxiv.org/abs/2412.14819)] [[cool](https://papers.cool/arxiv/2412.14819)] [[pdf](https://arxiv.org/pdf/2412.14819)]
> **Authors**: Zhongwei Chen,Zhao-Xu Yang,Hai-Jun Rong
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 具有一致性和不变性学习的多层嵌入和对齐网络，用于跨视图地理位置定位
- **领域**: 计算机视觉和模式识别
- **摘要**: 跨视图地理定位（CVGL）涉及通过检索最相似的GPS标记的卫星图像来确定无人机图像的定位。但是，平台之间的成像差距通常很重要，并且观点中的变化很大，这限制了现有方法有效地关联跨视图特征并提取一致和不变特征的能力。此外，现有方法通常会忽略改善模型性能时计算和存储需求增加的问题。为了应对这些限制，我们提出了一个轻巧增强的对齐网络，称为多层嵌入和对齐网络（平均值）。平均网络使用渐进的多层次增强策略，全球到本地关联和跨域对准，从而使特征交流跨越层次。这允许在不同级别上有效连接功能，并学习强大的跨视图一致映射和模态不变的功能。此外，Mean采用浅骨干网络，结合了轻巧的分支设计，有效地降低了参数计数和计算复杂性。与最先进的模型相比，大学1652和SUE-200数据集的实验结果表明，这意味着将参数计数降低62.17％，计算复杂性降低70.99％，同时保持竞争性甚至优越的性能。我们的代码和模型将在https://github.com/ischenawei/mean上发布。

### TextSleuth: Towards Explainable Tampered Text Detection 
[[arxiv](https://arxiv.org/abs/2412.14816)] [[cool](https://papers.cool/arxiv/2412.14816)] [[pdf](https://arxiv.org/pdf/2412.14816)]
> **Authors**: Chenfan Qu,Jian Liu,Haoxing Chen,Baihan Yu,Jingjing Liu,Weiqiang Wang,Lianwen Jin
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: The first work for explainable tampered text detection
- **标题**: 文本理：迈向可解释的篡改文本检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，由于其在信息安全性中的重要作用，篡改的文本检测引起了越来越多的关注。尽管现有方法可以检测到篡改的文本区域，但这种检测的解释尚不清楚，这使得预测不可靠。为了解决这个问题，我们建议通过大型多模型解释自然语言篡改文本检测的基础。为了填补此任务的数据差距，我们提出了一个大规模，全面的数据集ETTD，其中包含针对篡改文本区域的像素级注释和描述篡改文本异常的自然语言注释。采用多种方法来提高提议的数据的质量。例如，引入了精心设计的查询以生成使用GPT4O的高质量异常描述。提出了融合的掩码提示，以减少查询GPT4O生成异常描述时的混淆。为了自动过滤低质量的注释，我们还建议提示GPT4O在描述异常之前识别篡改文本，并以低OCR准确性过滤响应。为了进一步改善可解释的篡改文本检测，我们提出了一个称为Textsleuth的简单而有效的模型，该模型通过专注于可疑区域，从而改善了细粒度的感知和跨域的概括，并具有两阶段的分析范式和辅助接地提示。 ETTD数据集和公共数据集的广泛实验验证了所提出方法的有效性。还提供了深入的分析以激发进一步的研究。我们的数据集和代码将是开源的。

### FiVL: A Framework for Improved Vision-Language Alignment 
[[arxiv](https://arxiv.org/abs/2412.14672)] [[cool](https://papers.cool/arxiv/2412.14672)] [[pdf](https://arxiv.org/pdf/2412.14672)]
> **Authors**: Estelle Aflalo,Gabriela Ben Melech Stan,Tiep Le,Man Luo,Shachar Rosenman,Sayak Paul,Shao-Yen Tseng,Vasudev Lal
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: FIVL：改进视觉路线的框架
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉语言模型（LVLM）在整合视觉和文本输入以进行多模式推理方面取得了重大进展。但是，反复出现的挑战是确保这些模型像语言内容一样有效地利用视觉信息，而两种模态是为了提出准确答案的。我们假设由于当前LVLM缺乏有效的视觉接地而产生了幻觉。这个问题扩展到视觉基准，在该基准中很难使图像必不可少，以确保答案的生成，尤其是在视觉提问的任务中。在这项工作中，我们介绍了FIVL，这是一种构建旨在训练LVLM的数据集的新颖方法，以增强视觉接地并评估其实现它的有效性。这些数据集可以用于培训和评估LVLM使用图像内容作为实质证据而不是仅依靠语言先验的能力，从而提供了对模型对视觉信息的依赖的见解。为了证明我们的数据集的实用性，我们介绍了一项创新的培训任务，该任务以验证方法和可解释性的应用以及应用程序的效果优于基准。该代码可在https://github.com/intellabs/fivl上找到。

### Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.14660)] [[cool](https://papers.cool/arxiv/2412.14660)] [[pdf](https://arxiv.org/pdf/2412.14660)]
> **Authors**: Zijun Chen,Wenbo Hu,Guande He,Zhijie Deng,Zheng Zhang,Richang Hong
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: Accepted to COLING 2025
- **标题**: 揭示不确定性：深入研究多模型模型的校准和性能
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,机器学习
- **摘要**: 多模式大语言模型（MLLM）结合了诸如图像字幕和视觉问题回答等任务的视觉和文本数据。适当的不确定性校准至关重要，但具有挑战性，对于在医疗保健和自动驾驶等领域的可靠使用。本文研究了代表性的MLLM，重点介绍了它们在各种情况下的校准，包括在视觉微调之前和之后，以及基本LLM的多模式训练之前和之后。我们观察到其性能误解，同时，在这些情况下，校准没有显着差异。我们还强调了文本和图像之间的不确定性如何以及它们的整合如何影响整体不确定性。为了更好地理解MLLM的错误校准及其自我评估不确定性的能力，我们构建了IDK（我不知道）数据集，这是评估它们如何处理未知数的关键。我们的发现表明，MLLM倾向于给出答案而不是承认不确定性，但是这种自我评估通过适当的及时调整改善。最后，为了校准MLLM并增强模型的可靠性，我们提出了诸如温度缩放和迭代及时优化之类的技术。我们的结果为改进MLLM的有效和负责任的部署提供了见解。代码和IDK数据集：https：//github.com/hfutml/calibration-mllm。

### RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios 
[[arxiv](https://arxiv.org/abs/2412.14643)] [[cool](https://papers.cool/arxiv/2412.14643)] [[pdf](https://arxiv.org/pdf/2412.14643)]
> **Authors**: Jie Huang,Ruibing Hou,Jiahe Zhao,Hong Chang,Shiguang Shan
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: 13 pages
- **标题**: REFHCM：在以人为中心的情况下参考感知的统一模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 以人为中心的观念在现实应用中起着至关重要的作用。尽管最近以人为中心的作品取得了令人印象深刻的进步，但这些努力通常受到视觉领域的限制，并且与人类指示缺乏互动，从而限制了它们在诸如聊天机器人和体育分析等更广泛的情况下的适用性。本文介绍了人类的看法，其中参考提示指定了图像中感兴趣的人。为了解决新任务，我们提出了REFHCM（引用以人为本的模型），这是一个统一的框架，旨在整合各种以人为中心的参考任务。具体来说，RefHCM采用序列合并将原始的多模式数据转换为语义令牌。这种标准化的表示使REFHCM能够将各种以人为中心的介绍任务重新调整为序列到序列范式，并使用普通的编码器变压器架构解决。 REFHCM受益于统一的学习策略，有效地促进了跨任务的知识转移，并在处理复杂推理方面表现出了不可预见的能力。这项工作是第一次尝试使用通用框架来解决人类看法的尝试，同时建立了为该领域设定新标准的相应基准。广泛的实验展示了REFHCM在以人为本的参考任务中的竞争性甚至更高的表现。代码和数据在https://github.com/jjjymmm/refhcm上公开。

### Unified Image Restoration and Enhancement: Degradation Calibrated Cycle Reconstruction Diffusion Model 
[[arxiv](https://arxiv.org/abs/2412.14630)] [[cool](https://papers.cool/arxiv/2412.14630)] [[pdf](https://arxiv.org/pdf/2412.14630)]
> **Authors**: Minglong Xue,Jinhong He,Shivakumara Palaiahnakote,Mingliang Zhou
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 统一图像恢复和增强：降级校准循环重建扩散模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 对于众多计算机视觉应用而言，图像恢复和增强至关重要，但是有效统一这些任务仍然是一个重大挑战。受扩散模型的迭代改进功能的启发，我们提出了CyclerDM，这是一个新型框架，旨在统一恢复和增强任务，同时实现高质量的映射。具体而言，CyclerDM首先通过两个阶段扩散推理过程学习降解域，粗糙的正常域和正常域之间的映射关系。随后，我们使用离散小波转换将最终校准过程传输到小波低频域，从而通过利用特定于任务的频率空间从频域的角度执行细粒度的校准。为了提高恢复质量，我们为分解的小波高频域设计功能增益模块以消除冗余特征。此外，我们采用多模式的文本提示和傅立叶变换来驱动稳定的denoising并降低推理过程中的随机性。经过广泛的验证后，可以有效地将CyclerDM推广到各种图像恢复和增强任务，同时仅需要少量的训练样本在重建质量和知觉质量的各种基准上才能显着优越。源代码将在https://github.com/hejh8/cyclerdm上找到。

### Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry, and Internal Properties 
[[arxiv](https://arxiv.org/abs/2412.14592)] [[cool](https://papers.cool/arxiv/2412.14592)] [[pdf](https://arxiv.org/pdf/2412.14592)]
> **Authors**: Wenqiao Li,Bozhong Zheng,Xiaohao Xu,Jinye Gan,Fading Lu,Xiang Li,Na Ni,Zheng Tian,Xiaonan Huang,Shenghua Gao,Yingna Wu
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 多传感器对象异常检测：统一外观，几何和内部属性
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 物体异常检测对于工业质量检查至关重要，但是传统的单传感器方法面临着关键的局限性。他们无法捕获广泛的异常类型，因为单个传感器通常被限制为外观，几何结构或内部特性。为了克服这些挑战，我们介绍了Mulsen-AD，这是第一个高分辨率，多传感器异常检测数据集，适用于工业应用。 Mulsen-AD统一了来自RGB摄像机，激光扫描仪和锁定红外热力计的数据，可有效捕获外观，几何变形和内部缺陷。该数据集跨越了15种具有多种现实世界异常的工业产品。我们还提出了Mulsen-AD基准，这是一种旨在评估多传感器方法的基准，并提出了Mulsen-Triplead，这是一种决策级融合算法，该算法将这三种模态集成了可靠的，无用的对象对象异常检测。我们的实验表明，多传感器融合基本上优于单传感器方法，在对象级检测准确性上达到了96.1％的AUROC。这些结果强调了将多传感器数据集成以进行全面的工业异常检测的重要性。

### GCS-M3VLT: Guided Context Self-Attention based Multi-modal Medical Vision Language Transformer for Retinal Image Captioning 
[[arxiv](https://arxiv.org/abs/2412.17251)] [[cool](https://papers.cool/arxiv/2412.17251)] [[pdf](https://arxiv.org/pdf/2412.17251)]
> **Authors**: Teja Krishna Cherukuri,Nagur Shareef Shaik,Jyostna Devi Bodapati,Dong Hye Ye
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: This paper has been accepted for presentation at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)
- **标题**: GCS-M3VLT：引导背景自我注意力基于多模式医学视觉语言变压器用于视网膜图像字幕
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: 视网膜图像分析对于诊断和治疗眼病至关重要，但是由于图像质量和病理的可变性，尤其是标记的数据有限，从图像中产生准确的医学报告仍然具有挑战性。以前的基于变压器的模型努力在有限的监督下整合视觉和文本信息。作为回应，我们提出了一种新型的视觉语言模型，用于视网膜图像字幕，该模型通过引导的上下文自我注意机制结合了视觉和文本特征。即使在数据筛选方案中，这种方法也捕获了复杂的细节和全球临床环境。在DeepeyeNet数据集上进行的广泛实验表明，0.023 BLEU@4改进以及重大的定性进步，突出了我们模型在生成全面的医疗标题方面的有效性。

### Modality-Aware Shot Relating and Comparing for Video Scene Detection 
[[arxiv](https://arxiv.org/abs/2412.17238)] [[cool](https://papers.cool/arxiv/2412.17238)] [[pdf](https://arxiv.org/pdf/2412.17238)]
> **Authors**: Jiawei Tan,Hongxing Wang,Kang Dang,Jiaxin Li,Zhilong Ou
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 与视频场景检测有关的模式感知的镜头和比较
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 视频场景检测涉及评估每个镜头及其周围环境是否属于同一场景。实现这一目标需要精心关联多模式线索，$ \ it {e.g。} $视觉实体和放置方式，在镜头中并比较每个镜头周围的语义变化。但是，大多数方法都平等地对待多模式语义，并且没有检查镜头的两个方面之间的上下文差异，从而导致了次优的检测性能。在本文中，我们提出了$ \ bf {m} $ oditaly-$ \ bf {a} $ ware $ \ bf {s} $ hot $ \ bf {r} $兴高采烈和$ \ bf {c} $ oppareing方法（MASRC），可以使自己自身的特征和位置的相似之处相似，并以相似的范围和相似度的相似之处，并以相似的方式和地点相似。更改明确编码。具体而言，为了充分利用视觉实体的潜力并在建模射击关系中放置方式，我们从实体语义上挖掘了长期的射击相关性，同时揭示了来自位置语义的短期射击相关性。这样，我们可以学习独特的射击功能，这些功能可以巩固场景中的连贯性并扩大场景之间的区分性。一旦配备了独特的射击功能，我们就通过相似性卷积，进一步编码了每个目标射击的先前和成功射击之间的关系，这有助于识别场景结束镜头。我们验证了MASRC中提出的组件的广泛适用性。公共基准数据集的广泛实验结果表明，拟议的MASRC显着进步了视频场景检测。

### CharGen: High Accurate Character-Level Visual Text Generation Model with MultiModal Encoder 
[[arxiv](https://arxiv.org/abs/2412.17225)] [[cool](https://papers.cool/arxiv/2412.17225)] [[pdf](https://arxiv.org/pdf/2412.17225)]
> **Authors**: Lichen Ma,Tiezhu Yue,Pei Fu,Yujie Zhong,Kai Zhou,Xiaoming Wei,Jie Hu
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: Chargen：具有多模式编码器的高准确的角色级视觉文本生成模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，基于扩散的视觉文本生成模型已取得了重大进步。尽管这些方法在视觉文本渲染中的有效性正在迅速改善，但在呈现复杂的视觉文本时，它们仍然会遇到诸如不准确的字符和中风的挑战。在本文中，我们提出了Chargen，这是一个高度准确的角色级别的视觉文本生成和编辑模型。具体而言，Chargen采用了一个字符级的多模式编码器，该编码器不仅提取字符级文本嵌入，而且还通过字符编码Glyph图像。这使其能够更有效地捕获细粒度的交叉模式特征。此外，我们引入了Chargen的新知觉损失，以增强角色形状的监督并解决生成文本中不准确的中风问题。值得一提的是，可以将Chargen集成到现有的扩散模型中，以高精度生成视觉文本。 Chargen显着提高了文本渲染精度，在公共基准（例如AnyText基准和Mario-eval）中的最新方法优于最新方法，分别提高了8％以上和6％。值得注意的是，夏根在中国测试集上的准确性提高了5.5％。

### Where am I? Cross-View Geo-localization with Natural Language Descriptions 
[[arxiv](https://arxiv.org/abs/2412.17007)] [[cool](https://papers.cool/arxiv/2412.17007)] [[pdf](https://arxiv.org/pdf/2412.17007)]
> **Authors**: Junyan Ye,Honglin Lin,Leyan Ou,Dairong Chen,Zihao Wang,Conghui He,Weijia Li
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: 11 pages, 6 figures
- **标题**: 我在哪里？跨视图与自然语言描述
- **领域**: 计算机视觉和模式识别
- **摘要**: 跨视图地理位置定位通过将街道视图图像与地理标签卫星图像或OSM匹配来识别街道视图图像的位置。但是，大多数研究都集中在图像到图像检索上，较少解决文本引导的检索，这对于诸如行人导航和紧急响应之类的应用至关重要。在这项工作中，我们介绍了一项新型任务，用于使用自然语言描述进行跨视图地理位置定位，该任务旨在根据场景文本检索相应的卫星图像或OSM数据库。为了支持这项任务，我们通过从多个城市收集跨视图数据并采用场景文本生成方法来构建CVG-TEXT数据集，该方法利用大型多模型的注释能力来生成具有本地化详细信息的高质量场景文本描述的高质量场景文本描述。功能。在解释性方面，它不仅提供了相似性得分，还提供了检索原因。更多信息可以在https://yejy53.github.io/cvg-text/上找到。

### PromptDresser: Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask 
[[arxiv](https://arxiv.org/abs/2412.16978)] [[cool](https://papers.cool/arxiv/2412.16978)] [[pdf](https://arxiv.org/pdf/2412.16978)]
> **Authors**: Jeongho Kim,Hoiyeong Jin,Sunghyun Park,Jaegul Choo
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: 20 pages
- **标题**: 提示器：通过生成文本提示和及时感知的掩码提高虚拟试验的质量和可控性
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近的虚拟尝试方法通过微调预训练的文本对图像扩散模型来利用其强大的生成能力，从而提出了进步。但是，在虚拟试验中使用文本提示仍未得到充实。本文解决了一项文本编辑的虚拟尝试任务，该任务根据提供的服装图像更改服装项目，同时根据文本说明编辑佩戴风格（例如，tucking风格，合身）。在文本编辑的虚拟尝试中，存在三个关键方面：（i）设计丰富的文本描述，以配对的人透明数据训练模型，（ii）解决现有人员服装的文本信息的冲突，干扰了新服装的产生，并在启动层面上适应了构图，以确保构图的构图，以确保构图的构图，以确保构图的构图，以确保置于内部的降解范围，以使其对文本置于降级的范围，以确定其文本置于降级的过程中新衣服。为了解决这些方面，我们提出了提示仪，这是一种文本编辑的虚拟尝试模型，利用大型多模式模型（LMM）帮助，以基于生成的文本提示来实现高质量和多功能操作。我们的方法利用LMM通过文化学习来独立地为人和服装图像生成详细的文本描述，包括姿势细节和使用最低的人为成本编辑属性。此外，为了确保编辑区域，我们根据文本的提示调整涂层面具。我们发现，我们的方法利用详细的文本提示，不仅可以增强文本编辑性，而且可以有效地传达服装细节，这些细节很难单独通过图像捕获，从而增强了图像质量。我们的代码可在https://github.com/rlawjdghek/promptdresser上找到。

### Linguistics-Vision Monotonic Consistent Network for Sign Language Production 
[[arxiv](https://arxiv.org/abs/2412.16944)] [[cool](https://papers.cool/arxiv/2412.16944)] [[pdf](https://arxiv.org/pdf/2412.16944)]
> **Authors**: Xu Wang,Shengeng Tang,Peipei Song,Shuo Wang,Dan Guo,Richang Hong
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: Accepted by ICASSP 2025
- **标题**: 语言 - 视觉单调一致网络用于手语的生产
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 手语的生产（SLP）旨在生成与口语句子相对应的标志视频，其中符号颜色转换为姿势（G2P）是关键步骤。由于跨模式的语义差距以及缺乏用于强大监督对准的单词行动对应标签，因此SLP在语言学视觉一致性中遇到了巨大的挑战。在这项工作中，我们为SLP提出了一个基于变压器的语言学视觉单调一致网络（LVMCN），该网络通过跨模式语义对准器（CSA）和多模态语义比较器（MSC）（MSC）（MSC）限制了语言 - 视觉线索中的细粒度跨模式单调对准和粗粒的多模式语义线索。在CSA中，我们通过计算跨模式特征序列之间的余弦相似性矩阵（即，细颗粒的标志光泽和动作的顺序一致性）来限制相应光泽和姿势序列之间的隐式一致性。至于MSC，我们在批处理数据中基于配对和未配对的样本构建了多模式三重态。通过将相应的视觉对靠近拉开，并将其推开非相应的文本 - 视觉对，我们限制了在相应的光泽和姿势序列之间的语义共存在度（即，粗粒细节的文本句子和符号视频的语义一致性和视频的语义一致性）。在流行的Phoenix14T基准测试上进行的广泛实验表明，LVMCN的表现优于最先进的实验。

### TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction 
[[arxiv](https://arxiv.org/abs/2412.16919)] [[cool](https://papers.cool/arxiv/2412.16919)] [[pdf](https://arxiv.org/pdf/2412.16919)]
> **Authors**: Xuying Zhang,Yutong Liu,Yangguang Li,Renrui Zhang,Yufei Liu,Kai Wang,Wanli Ouyang,Zhiwei Xiong,Peng Gao,Qibin Hou,Ming-Ming Cheng
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: TAR3D：通过下一部分预测创建高质量的3D资产
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了TAR3D，这是一个新型框架，由3D感知的矢量量化变量自动编码器（VQ-VAE）和生成性预训练的变压器（GPT）组成，以生成高质量的3D资产。这项工作的核心洞察力是将下一个预测范式的多模式统一和有希望的学习能力迁移到有条件的3D对象生成。为了实现这一目标，3D VQ-VAE首先将各种3D形状编码为紧凑型三层潜在空间，并利用从可训练的代码簿中的一组离散表示形式，以在查询点占用的监督下重建细粒度的几何形状。然后，配备了称为TRIPE的自定义三层位置嵌入的3D GPT，以自动回归方式预测使用预填充提示令牌的代码簿索引序列，以便可以部分对3D几何形状的组成进行部分建模。关于Shapenet和Objaverse的广泛实验表明，TAR3D可以在文本到3D和Image-3D任务中实现优于现有方法的发电质量

### MAGIC++: Efficient and Resilient Modality-Agnostic Semantic Segmentation via Hierarchical Modality Selection 
[[arxiv](https://arxiv.org/abs/2412.16876)] [[cool](https://papers.cool/arxiv/2412.16876)] [[pdf](https://arxiv.org/pdf/2412.16876)]
> **Authors**: Xu Zheng,Yuanhuiyi Lyu,Lutao Jiang,Jiazhou Zhou,Lin Wang,Xuming Hu
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 魔术++：通过层次模态选择的高效且有弹性的模态语义分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们解决了具有挑战性的模态语义分割（质量），旨在以每种特征粒度为中心的价值。具有所有可用的视觉方式的训练，并有效地融合了它们的任意组合对于语义细分中的稳健多模式融合至关重要，尤其是在现实世界中，但迄今为止仍未探索。现有方法通常将RGB放置在中心，将其他方式视为次要的，从而导致不对称的建筑。但是，在夜间之类的场景中，单独的RGB可能会限制，例如，诸如事件数据之类的模式出色。因此，一个弹性的融合模型必须动态地适应每种模式的优势，同时补偿弱输入。为此，我们引入了Magic ++框架，该魔术++框架包括两个关键的插件模块，以有效的多模式融合和层次结构选择，可以配备各种背链模型。首先，我们引入了一个多模式交互模块，以从输入多模式批处理中有效地处理特征，并使用渠道和空间方面的指导提取互补场景信息。最重要的是，提出了一个统一的多尺度任意模式选择模块，以利用汇总特征作为基准，以基于分层特征空间的相似性分数对多模式特征进行排名。这样，我们的方法可以在每个功能粒度上消除对RGB模式的依赖性，并在确保细分性能的同时更好地克服传感器故障和环境噪声。在常见的多模式设置下，我们的方法在现实世界和合成基准测试中都达到了最先进的性能。此外，我们的方法在新型的模态不平衡环境中是优越的，在这种情况下，它的表现要优于先前的艺术。

### CoF: Coarse to Fine-Grained Image Understanding for Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.16869)] [[cool](https://papers.cool/arxiv/2412.16869)] [[pdf](https://arxiv.org/pdf/2412.16869)]
> **Authors**: Yeyuan Wang,Dehong Gao,Bin Li,Rujiao Long,Lei Yi,Xiaoyan Cai,Libin Yang,Jinxia Zhang,Shanqing Yu,Qi Xuan
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: 5 pages, Accepted by ICASSP2025, full paper
- **标题**: COF：多模式大语言模型的粗到细粒度的图像理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 大型语言模型（LLM）的令人印象深刻的表现促使研究人员开发了多模式LLM（MLLM），该LLM（MLLM）显示出各种多模式任务的巨大潜力。但是，当前的MLLM经常努力有效地应对细粒度的多模式挑战。我们认为，这种限制与模型的视觉接地功能紧密相关。视觉编码器的限制空间意识和感知敏锐度经常导致图像中无关的背景信息的干扰，从而忽略了微妙但至关重要的细节。结果，实现细粒度的区域视觉理解变得困难。在本文中，我们将多模式理解分为两个阶段，从粗糙到细（COF）。在第一阶段，我们提示MLLM找到答案的大致区域。在第二阶段，我们通过视觉及时工程进一步增强了模型对图像中相关区域的关注，从而调整了相关区域的注意力重量。反过来，这可以改善下游任务中的视觉接地和整体性能。我们的实验表明，这种方法显着提高了基线模型的性能，证明了显着的概括和有效性。我们的COF方法可在线https://github.com/gavin001201/cof在线获得。

### Human-Guided Image Generation for Expanding Small-Scale Training Image Datasets 
[[arxiv](https://arxiv.org/abs/2412.16839)] [[cool](https://papers.cool/arxiv/2412.16839)] [[pdf](https://arxiv.org/pdf/2412.16839)]
> **Authors**: Changjian Chen,Fei Lv,Yalong Guan,Pengcheng Wang,Shengjie Yu,Yifan Zhang,Zhuo Tang
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: Accepted by TVCG2025
- **标题**: 人类指导的图像生成用于扩展小规模培训图像数据集
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在某些现实世界应用中的计算机视觉模型（例如，稀有野生动物观察）的性能受到少量可用图像的限制。使用预训练的生成模型扩展数据集是解决此限制的有效方法。但是，由于自动生成过程是不可控制的，因此生成的图像通常受到多样性的限制，其中一些图像是不希望的。在本文中，我们提出了一种人类引导的图像生成方法，以进行更可控制的数据集扩展。我们开发一种具有理论保证的多模式投影方法，以促进对原始图像和生成图像的探索。根据探索，用户可以完善提示并重新生成图像，以提高性能。由于直接完善提示对于新手用户来说是具有挑战性的，因此我们开发了一种示例级提示方法，以使其更容易。使用此方法，用户只需要提供示例级别的反馈（例如，哪些样本是不希望的）即可获得更好的提示。通过对多模式投影方法的定量评估，在分类和对象检测任务的案例研究中改善了模型性能以及来自专家的积极反馈，可以证明我们方法的有效性。

### IMVB7t: A Multi-Modal Model for Food Preferences based on Artificially Produced Traits 
[[arxiv](https://arxiv.org/abs/2412.16807)] [[cool](https://papers.cool/arxiv/2412.16807)] [[pdf](https://arxiv.org/pdf/2412.16807)]
> **Authors**: Mushfiqur Rahman Abir,Md. Tanzib Hosain,Md. Abdullah-Al-Jubair,M. F. Mridha
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: Accepted in Proceedings of the 3rd International Conference on Computing Advancements, 2024
- **标题**: IMVB7T：基于人工产生的性状的食品偏好的多模式模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类的行为和互动受周围环境中存在的视觉刺激的深刻影响。这种影响扩展到生活的各个方面，尤其是食物消费和选择。在我们的研究中，我们采用了各种模型来从环境图像中提取不同的属性。具体而言，我们确定了五个关键属性，并基于五个不同模型的某些检测模型采用集合模型IMVB7，导致0.85分。此外，我们进行了调查，以响应视觉刺激而辨别食物偏好的模式。利用这些调查中收集的见解，我们根据鉴定属性的合并为IMVB7T 0.96标记，使用决策树提出建议。这项研究是基础步骤，为进一步探索该跨学科领域铺平了道路。

### SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization 
[[arxiv](https://arxiv.org/abs/2412.16771)] [[cool](https://papers.cool/arxiv/2412.16771)] [[pdf](https://arxiv.org/pdf/2412.16771)]
> **Authors**: Tan-Hanh Pham,Hoang-Nam Le,Phu-Vinh Nguyen,Chris Ngo,Truong-Son Hy
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: 10 pages
- **标题**: Silvar：语音驱动的多模型用于推理视觉问题的回答和对象本地化
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型已显示出跨任务的显着功能，包括视觉问题回答和图像字幕。但是，大多数模型都依靠基于文本的说明，从而限制了它们在人机相互作用中的有效性。此外，语言模型的质量取决于推理和提示技术（例如COT），在使用语音说明时，这些技术仍未得到充实。为了应对这些挑战，我们提出了Silvar，这是一种新颖的端到端多模型，该模型使用语音说明在视觉问题回答中进行推理。此外，我们研究了推理技术的水平，包括对话，简单和复杂的语音教学。西尔瓦（Silvar）建立在剪辑，耳语和美洲驼（3.1-8B）上，通过允许用户提供口头或文字说明，从而实现直观的互动。为此，我们介绍了一个数据集，旨在通过基于语音的推理任务来挑战模型以进行对象本地化。该数据集增强了模型能够处理和解释来自口语输入的视觉场景的能力，而不是对象识别到基于推理的交互。实验表明，尽管基于语音的说明面临挑战，但Silvar在MMMU和ScienceQA基准上取得了SOTA的性能。我们认为，席尔瓦（Silvar）将激发下一代多模式推理模型，迈向人工通用情报。我们的代码和数据集可在此处提供。

### REO-VLM: Transforming VLM to Meet Regression Challenges in Earth Observation 
[[arxiv](https://arxiv.org/abs/2412.16583)] [[cool](https://papers.cool/arxiv/2412.16583)] [[pdf](https://arxiv.org/pdf/2412.16583)]
> **Authors**: Xizhe Xue,Guoting Wei,Hao Chen,Haokui Zhang,Feng Lin,Chunhua Shen,Xiao Xiang Zhu
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: REO-VLM：转换VLM以应对地球观察中的回归挑战
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉语言模型（VLM）的快速发展促进了人工智能方面的重大进步，扩大了包括地球观察（EO）在内的各个学科的研究。尽管VLM在EO内增强了图像理解和数据处理，但它们的应用程序主要集中在图像内容描述上。这个有限的重点忽略了它们在地理和科学回归任务中的潜力，这对于各种EO应用至关重要。为了弥合这一差距，本文介绍了一个新颖的基准数据集，称为\ textbf {reo-Instruct}，以统一专门针对EO域的回归和生成任务。该数据集由160万个多模式EO图像和语言对，旨在支持生物质回归和图像内容解释任务。利用此数据集，我们开发了\ textbf {reo-vlm}，这是一个开创性的模型，无缝将回归功能与传统的生成功能集成在一起。通过利用语言驱动的推理来纳入科学领域知识，REO-VLM超越了仅依靠EO图像，从而从EO数据中对复杂的科学属性进行了全面的解释。这种方法建立了新的性能基准，并显着增强了环境监控和资源管理的能力。

### LLaVA-SLT: Visual Language Tuning for Sign Language Translation 
[[arxiv](https://arxiv.org/abs/2412.16524)] [[cool](https://papers.cool/arxiv/2412.16524)] [[pdf](https://arxiv.org/pdf/2412.16524)]
> **Authors**: Han Liang,Chengyu Huang,Yuecheng Xu,Cheng Tang,Weicai Ye,Juze Zhang,Xin Chen,Jingyi Yu,Lan Xu
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: Llava-SLT：手语翻译的视觉语言调整
- **领域**: 计算机视觉和模式识别
- **摘要**: 在手语翻译（SLT）的领域中，依赖昂贵的光泽清单数据集构成了重大障碍。无光泽SLT方法的最新进展已经显示出希望，但就翻译精度而言，它们通常在很大程度上落后于基于光泽的方法。为了缩小这一性能差距，我们介绍了Llava-SLT，这是一种开创性的大型多模型（LMM）框架，旨在通过有效学习的视觉语言嵌入来利用大型语言模型（LLMS）的力量。我们的模型通过三部曲进行了训练。首先，我们提出语言持续预测。我们使用广泛的语料库数据集扩大了LLM并将其调整到手语域，从而有效地增强了其有关手语的文本语言知识。然后，我们采用视觉对比度预处理，以使视觉编码器与大规模预认证的文本编码器对齐。我们提出了层次的视觉编码器，该层次的视觉编码器学习了与LLM令牌嵌入兼容的强大单词级中间表示形式。最后，我们提出视觉语言调整。我们冻结了预处理的型号，并采用了轻巧的MLP连接器。它有效地将验证的视觉语言嵌入到LLM令牌嵌入空间中，从而实现下游SLT任务。我们的全面实验表明，LLAVA-SLT的表现优于最先进的方法。通过使用额外的无注释数据，它甚至可以关闭基于光泽的精度。

### Revisiting MLLMs: An In-Depth Analysis of Image Classification Abilities 
[[arxiv](https://arxiv.org/abs/2412.16418)] [[cool](https://papers.cool/arxiv/2412.16418)] [[pdf](https://arxiv.org/pdf/2412.16418)]
> **Authors**: Huan Liu,Lingyu Xiao,Jiangjiang Liu,Xiaofan Li,Ze Feng,Sen Yang,Jingdong Wang
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 重新访问MLLM：对图像分类能力的深入分析
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着多模式大语言模型（MLLM）的快速发展，已经引入了各种基准来评估其功能。尽管大多数评估都集中在复杂的任务上，例如科学理解和视觉推理，但很少关注评估其基本图像分类能力。在本文中，我们通过对图像分类进行深入分析来彻底重新审视MLLM，以解决这一差距。具体而言，在既定数据集的基础上，我们研究了各种场景，从一般分类任务（例如ImageNet，ObjectNet）到更细粒度的类别，例如鸟类和食物分类。我们的发现表明，最新的MLLM可以在几个数据集上匹配甚至超过剪贴式视觉语言模型，这挑战了先前的假设，即MLLM在Image Classification \ cite \ cite {vlmclassifier}中都不良。为了了解推动这一改进的因素，我们对公共MLLM中使用的网络体系结构，数据选择和培训配方进行了深入的分析。我们的结果将这种成功归因于语言模型的进步和培训数据源的多样性。基于这些观察结果，我们进一步分别分析和归因于概念知识转移和增强目标概念的暴露的潜在原因。我们希望我们的发现将为未来对MLLM的研究及其在图像分类任务中的评估提供宝贵的见解。

### A High-Quality Text-Rich Image Instruction Tuning Dataset via Hybrid Instruction Generation 
[[arxiv](https://arxiv.org/abs/2412.16364)] [[cool](https://papers.cool/arxiv/2412.16364)] [[pdf](https://arxiv.org/pdf/2412.16364)]
> **Authors**: Shijie Zhou,Ruiyi Zhang,Yufan Zhou,Changyou Chen
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: COLING 2025
- **标题**: 高质量的文本丰富图像指令通过混合说明生成调整数据集
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 由于培训数据不足，大型的多模型模型仍然与文本丰富的图像相处。自我指导提供了一种无注释的方式来生成指令数据，但是它的质量很差，因为即使对于最大的模型，多模式的对准仍然是一个障碍。在这项工作中，我们提出了LLAVAR-2，以通过人类注释者和大型语言模型之间的混合教学生成来增强文本丰富图像的多模式对齐。具体而言，它涉及人类注释者的详细图像标题，然后在量身定制的文本提示中使用这些注释，以供GPT-4O策划数据集。它还实现了几种过滤低质量数据的机制，并且所得数据集包括424K高质量的指令。经验结果表明，该数据集上微调的模型比受到自我实施数据训练的模型表现出令人印象深刻的增强。

### Defeasible Visual Entailment: Benchmark, Evaluator, and Reward-Driven Optimization 
[[arxiv](https://arxiv.org/abs/2412.16232)] [[cool](https://papers.cool/arxiv/2412.16232)] [[pdf](https://arxiv.org/pdf/2412.16232)]
> **Authors**: Yue Zhang,Liqiang Jing,Vibhav Gogate
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-23
> **comment**: Accepted by AAAI 2025
- **标题**: 视觉上不稳定：基准，评估器和奖励驱动的优化
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 我们介绍了一项名为“不避免视觉效果”（DVE）的新任务，其目标是允许基于附加更新的图像前提和文本假设之间的累积关系修改。尽管这种概念在自然语言推论中已经建立了良好，但在视觉上仍未探索。在很高的水平上，DVE使模型能够完善其初始解释，从而提高了各种应用程序的准确性和可靠性，例如检测图像中的误导信息，增强视觉问题的答案以及完善自主系统中的决策过程。现有的指标不能充分捕获更新带来的累积关系的变化。为了解决这个问题，我们提出了一种新颖的推理感知评估器，旨在使用成对的对比度学习和分类信息学习，旨在捕获更新引起的需要的变化。此外，我们引入了一种奖励驱动的更新优化方法，以进一步提高由多模式模型生成的更新质量。实验结果证明了我们提出的评估者和优化方法的有效性。

### HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding 
[[arxiv](https://arxiv.org/abs/2412.16158)] [[cool](https://papers.cool/arxiv/2412.16158)] [[pdf](https://arxiv.org/pdf/2412.16158)]
> **Authors**: Chenxin Tao,Shiqian Su,Xizhou Zhu,Chenyu Zhang,Zhe Chen,Jiawen Liu,Wenhai Wang,Lewei Lu,Gao Huang,Yu Qiao,Jifeng Dai
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: Hovle：用整体视觉语言嵌入整体视觉语言模型的力量
- **领域**: 计算机视觉和模式识别
- **摘要**: 大语言模型（LLM）的快速发展促进了视觉模型（VLM）的发展。避免特定于模态编码器的单片VLM为构图提供了一种有希望的替代方案，但面临着劣等性能的挑战。大多数现有的单片VLM都需要调整预训练的LLM来获得视力能力，这可能会降低其语言能力。为了解决这一难题，本文提出了一种新型的高性能单片VLM，名为Hovle。我们注意到，当图像嵌入与文本嵌入对齐时，LLM可以解释图像。当前的单片VLM的挑战实际上在于缺乏视力和语言输入的整体嵌入模块。因此，Hovle引入了一个整体嵌入模块，该模块将视觉和文本输入转换为共享空间，从而允许LLMS以与文本相同的方式处理图像。此外，仔细设计了一种多阶段训练策略，以增强整体嵌入模块的能力。它首先经过训练，可以从训练的视觉编码器和LLM中的文本嵌入中提炼视觉特征，从而使大规模训练具有未配对的随机图像和文本令牌。整个模型进一步对多模式数据进行了下一个预测，以对齐嵌入。最后，纳入了指令调整阶段。我们的实验表明，Hovle可以在各种基准上的领先组合模型上实现性能，从而超过了以前的单片模型。可在https://huggingface.co/opengvlab/hovle上找到。

### PruneVid: Visual Token Pruning for Efficient Video Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.16117)] [[cool](https://papers.cool/arxiv/2412.16117)] [[pdf](https://arxiv.org/pdf/2412.16117)]
> **Authors**: Xiaohu Huang,Hao Zhou,Kai Han
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: Efficient Video Large Language Models
- **标题**: Prunevid：视觉令牌修剪有效的视频大语模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们介绍了Prunevid，这是一种视觉令牌修剪方法，旨在提高多模式视频理解的效率。大型语言模型（LLMS）在视频任务中表现出令人鼓舞的性能，因为它们在理解视觉方式方面的功能扩展。但是，视频数据的实质性冗余性带来了LLMS的重大计算挑战。为了解决这个问题，我们引入了一种无培训方法，该方法1）通过合并时空令牌来最大程度地减少视频冗余，以及2）利用LLMS的推理能力来选择性地修剪与问题代币相关的修剪视觉特征，从而提高模型效率。我们在多个视频基准中验证了我们的方法，这表明Prunevid可以修剪80％的令牌，同时保持竞争性能与不同的模型网络相结合。与现有的修剪方法相比，这强调了其优异的有效性和效率。代码：https：//github.com/visual-ai/prunevid。

### MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection 
[[arxiv](https://arxiv.org/abs/2412.15925)] [[cool](https://papers.cool/arxiv/2412.15925)] [[pdf](https://arxiv.org/pdf/2412.15925)]
> **Authors**: Andrea Moglia,Elia Clement Nastasio,Luca Mainardi,Pietro Cerveri
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 迷你庞克雷亚：胰腺癌分类和检测的多模式大语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 问题：胰腺放射学成像由于尺寸较小，边界模糊以及器官之间形状和位置的变异性而具有挑战性。目标：在这项工作中，我们提出了一种多模式大语模型（MLLM）的Minigpt-Pancreas，作为一种交互式聊天机器人，可通过整合视觉和文本信息来支持胰腺癌诊断的临床医生。方法：Minigpt-V2是一种通用MLLM，以层叠的方式进行了微调，用于胰腺检测，肿瘤分类和肿瘤检测，并与多模式提示结合了问题和来自国家卫生研究院（NIH）的计算机断层扫描（NIH），以及医疗分割Decathlon（MSD）（MSD）数据集合。腹部1K数据集用于检测肝脏，脾脏，肾脏和胰腺。结果：Minigpt-Pancreas分别在NIH和MSD数据集上检测胰腺的联合（IOU）的交集分别为0.595和0.550。对于MSD数据集上的胰腺癌分类任务，准确性，精度和召回率分别为0.876、0.874和0.878。在评估腹部CTCCT-1K数据集上的Minigpt-Pancreas用于多器官检测时，肝脏的IOU为0.8399，肾脏为0.722，脾脏为0.705，胰腺为0.497。对于胰腺肿瘤检测任务，MSD数据集的IOU得分为0.168。结论：迷你甲壳虫代表了一种有前途的解决方案，可以在胰腺肿瘤的胰腺图像中支持临床医生。需要进行未来的研究来改善检测任务的分数，尤其是对于胰腺肿瘤。

### Sparse Point Clouds Assisted Learned Image Compression 
[[arxiv](https://arxiv.org/abs/2412.15752)] [[cool](https://papers.cool/arxiv/2412.15752)] [[pdf](https://arxiv.org/pdf/2412.15752)]
> **Authors**: Yiheng Jiang,Haotian Zhang,Li Li,Dong Liu,Zhu Li
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: Accepted by TCSVT
- **标题**: 稀疏点云协助学习的图像压缩
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 在自主驾驶领域，存在各种传感器数据类型，每种传感器数据类型都代表同一场景的不同方式。因此，利用来自其他传感器的数据来促进图像压缩是可行的。但是，很少有技术探讨了利用模式间相关性来增强图像压缩性能的潜在优势。在本文中，由于最新学习的图像压缩的成功，我们提出了一个新框架，该框架使用稀疏点云来协助在自主驾驶场景中学习的图像压缩。我们首先将3D稀疏点云投射到2D平面上，从而产生稀疏的深度图。利用此深度图，我们继续预测相机图像。随后，我们使用这些预测的图像来提取多尺度的结构特征。然后将这些功能纳入学习的图像压缩管道中，作为其他信息，以提高压缩性能。我们提出的框架与各种主流学术的图像压缩模型兼容，我们使用不同的现有图像压缩方法验证了我们的方法。实验结果表明，将点云辅助纳入压缩管道会始终提高性能。

### Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking 
[[arxiv](https://arxiv.org/abs/2412.15691)] [[cool](https://papers.cool/arxiv/2412.15691)] [[pdf](https://arxiv.org/pdf/2412.15691)]
> **Authors**: Xiantao Hu,Ying Tai,Xu Zhao,Chen Zhao,Zhenyu Zhang,Jun Li,Bineng Zhong,Jian Yang
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 利用视频对象跟踪的多模式时空模式
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式跟踪由于能够有效解决传统RGB跟踪的固有局限性而引起了广泛的关注。但是，现有的多模式跟踪器主要集中于空间特征的融合和增强，或者仅利用视频框架之间的稀疏时间关系。这些方法并不能完全利用多模式视频中的时间相关性，因此很难在复杂场景中捕获目标的动态变化和运动信息。为了减轻这个问题，我们提出了一种名为Sttrack的统一多模式时空跟踪方法。与仅依赖更新参考信息的先前范式相反，我们引入了时间状态发生器（TSG），该范式连续生成包含多模式时间信息的令牌序列。这些时间信息令牌用于指导目标在下一个状态下的定位，建立视频框架之间的远程上下文关系，并捕获目标的时间轨迹。此外，在空间级别，我们引入了MAMBA融合和背景抑制互动（BSI）模块。这些模块建立了一种双阶段机制，用于协调信息相互作用和模态之间的融合。在五个基准数据集上进行了广泛的比较表明，Sttrack在各种多模式跟踪方案中实现了最先进的性能。代码可在以下网址提供：https：//github.com/nju-pcalab/sttrack。

### Multi-Pair Temporal Sentence Grounding via Multi-Thread Knowledge Transfer Network 
[[arxiv](https://arxiv.org/abs/2412.15678)] [[cool](https://papers.cool/arxiv/2412.15678)] [[pdf](https://arxiv.org/pdf/2412.15678)]
> **Authors**: Xiang Fang,Wanlong Fang,Changshuo Wang,Daizong Liu,Keke Tang,Jianfeng Dong,Pan Zhou,Beibei Li
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: Accepted by AAAI 2025
- **标题**: 通过多线程知识转移网络的多对时间句子接地
- **领域**: 计算机视觉和模式识别
- **摘要**: 鉴于一些带有未修剪视频和句子查询的视频拼写对，时间句子接地（TSG）旨在在这些视频中找到与查询相关的段。尽管以前可观的TSG方法取得了显着的成功，但它们分别训练每个视频疑问对，而忽略了不同对之间的关​​系。我们观察到，相似的视频/查询内容不仅有助于TSG模型更好地理解和推广跨模式表示，而且还可以帮助该模型找到一些复杂的视频Query对。以前的方法遵循一个单线程框架，该框架无法共同培训不同的对，通常花费大量时间重新浏览冗余知识，从而限制了他们的真实应用程序。为此，在本文中，我们构成了一个全新的环境：多对TSG，旨在共同培训这些对。特别是，我们提出了一种新颖的视频疑问共同训练方法，即多线程知识传输网络，以有效，有效地定位各种视频Query对。首先，我们在不同的查询中挖掘了空间和时间语义，以相互合作。为了同时学习内模式和模式间表示，我们设计了一个跨模式对比模块，以通过自我监督的策略来探索语义一致性。为了完全对齐不同对之间的视觉和文本表示，我们将原型对齐策略设计为1）匹配对象原型和短语原型的空间对齐，以及2）将活动的原型和句子原型和句子原型匹配，以进行时间对齐。最后，我们开发了一个自适应的否定选择模块，以自适应生成跨模式匹配的阈值。广泛的实验显示了我们提出的方法的有效性和效率。

### AI-generated Image Quality Assessment in Visual Communication 
[[arxiv](https://arxiv.org/abs/2412.15677)] [[cool](https://papers.cool/arxiv/2412.15677)] [[pdf](https://arxiv.org/pdf/2412.15677)]
> **Authors**: Yu Tian,Yixuan Li,Baoliang Chen,Hanwei Zhu,Shiqi Wang,Sam Kwong
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: AAAI-2025; Project page: https://github.com/ytian73/AIGI-VC
- **标题**: AI生成的图像质量评估在视觉交流中
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 评估人工智能生成的图像（AIGI）的质量在其在实际情况下的应用中起着至关重要的作用。但是，传统的图像质量评估（IQA）算法主要集中于低级视觉感知，而现有的IQA在AIGIS上的工作过度强调了生成的内容本身，从而忽略了其在现实世界应用中的有效性。为了弥合这一差距，我们提出了AIGI-VC，这是视觉传播中AI生成图像的质量评估数据库，从信息清晰度和情感互动的角度来看，它在广告领域中研究了AIGI的通信性。该数据集由2500张图像组成，涵盖14个广告主题和8种情感类型。它提供了粗粒的人类偏好注释和细粒度的偏好描述，在偏好预测，解释和推理方面基准了IQA方法的能力。我们对AIGI-VC数据集的现有代表性IQA方法和大型多模型模型进行了实证研究，发现了它们的优势和劣势。

### A New Method to Capturing Compositional Knowledge in Linguistic Space 
[[arxiv](https://arxiv.org/abs/2412.15632)] [[cool](https://papers.cool/arxiv/2412.15632)] [[pdf](https://arxiv.org/pdf/2412.15632)]
> **Authors**: Jiahe Wan
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 一种在语言空间中捕获构图知识的新方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 构图理解允许视觉语言模型解释对象，属性和图像和文本中的关系之间的复杂关系。但是，大多数现有的方法通常依赖于硬性否定示例和微调，这可能会高估改进，并且受到获得硬负面因素的困难而受到限制。在这项工作中，我们介绍了零击的组成理解（ZS-CU），这是一种新的任务，可以增强组成理解而无需硬性负面训练数据。我们提出了Yukino（通过NO的文本反演产生了构图理解知识），该倒置使用文本反演将未标记的图像映射到预训练的剪辑模型中。我们建议引入“否”逻辑正则化，以解决反转中令牌交互的问题。此外，我们建议使用知识蒸馏来减少文本反演的时间复杂性。实验结果表明，Yukino的表现优于现有的多模式SOTA模型，超过8％的糖筛基准，并且在图像检索任务方面也取得了重大改进。

### SaliencyI2PLoc: saliency-guided image-point cloud localization using contrastive learning 
[[arxiv](https://arxiv.org/abs/2412.15577)] [[cool](https://papers.cool/arxiv/2412.15577)] [[pdf](https://arxiv.org/pdf/2412.15577)]
> **Authors**: Yuhao Li,Jianping Li,Zhen Dong,Yuan Wang,Bisheng Yang
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: Under Review
- **标题**: SuroteRIETI2Ploc：使用对比度学习的显着引导的图像点云本地化
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 图像到点云全球本地化对于在被GNSS贬低的环境中的机器人导航至关重要，并且对于多机器人地图融合和城市资产管理变得越来越重要。图像和点云之间的模态差距为跨模式融合带来了重大挑战。当前的跨模式全球本地化解决方案要么需要模态统一，这会导致信息丢失，或者依靠工程训练方案来编码多模式特征，这些功能通常缺乏特征对齐和关系一致性。为了解决这些局限性，我们提出了一个基于对比度学习的新型架构Surotility2Ploc，它将显着性图融合到特征聚集中，并维持多曼佛空间的特征关系一致性。为了减轻数据挖掘的预处理，应用了对比度学习框架，该框架有效地实现了交叉模式映射。上下文显着指导的本地功能聚合模块设计了，该模块完全利用了场景中固定信息的贡献，从而产生了更具代表性的全局功能。此外，为了增强对比度学习过程中的跨模式特征对准，还考虑了不同歧管空间中样品之间的相对关系的一致性。在城市和高速公路情景数据集上进行的实验证明了我们方法的有效性和鲁棒性。具体而言，我们的方法在城市场景评估数据集中获得了78.92％中的1次召回@1中的1个，而召回@20中的20％中的召回率是37.35％和18.07％的召回率，与基线方法相比提高了37.35％和18.07％。这表明我们的体系结构有效地融合了图像和点云，并且代表了跨模式全局本地化迈出的重要一步。项目页面和代码将发布。

### J-EDI QA: Benchmark for deep-sea organism-specific multimodal LLM 
[[arxiv](https://arxiv.org/abs/2412.15574)] [[cool](https://papers.cool/arxiv/2412.15574)] [[pdf](https://arxiv.org/pdf/2412.15574)]
> **Authors**: Takero Yoshida,Yuikazu Ito,Yoshihiro Fujiwara,Shinji Tsuchida,Daisuke Sugiyama,Daisuke Matsuoka
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: J-edi QA：深海有机体特异性多模式LLM的基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 日本海洋 - 地球科学技术机构（JAMSTEC）已提供了Jamstec Earth Deep-Sea Image（J-EDI），这是一个深海视频和图像档案（https://www.godac.jamstec.go.go.jp/jedi/jedi/e/e/index.html）。该档案是对深海图像感兴趣的研究人员和学者的宝贵资源。数据集包括深海现象的图像和视频，主要由海洋生物，以及海底和物理过程的图像和视频。在这项研究中，我们提出了J-Edi QA，这是使用多模式大语言模型（LLM）理解深海生物图像的基准。该基准由100张图像组成，并附有问题和答案，以及Jamstec研究人员为每个图像的四个选项。 QA对以日语提供，基准评估了理解日语深海物种的能力。在本文提出的评估中，OpenAI O1达到了50％的正确响应率。该结果表明，即使截至2024年12月，最先进的模型的功能，深海物种的理解尚未在专家层面上。因此，需要进一步的深海物种特异性LLM。

### Semantics Disentanglement and Composition for Versatile Codec toward both Human-eye Perception and Machine Vision Task 
[[arxiv](https://arxiv.org/abs/2412.18158)] [[cool](https://papers.cool/arxiv/2412.18158)] [[pdf](https://arxiv.org/pdf/2412.18158)]
> **Authors**: Jinming Liu,Yuntao Wei,Junyan Lin,Shengyang Zhao,Heming Sun,Zhibo Chen,Wenjun Zeng,Xin Jin
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 用于人眼感知和机器视觉任务的多功能编解码器的语义分解和组成
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 尽管学到的图像压缩方法在人类的视觉感知或机器视觉任务中取得了令人印象深刻的结果，但它们通常仅针对一个域而专门。该缺点限制了它们在方案中的多功能性和可推广性，还需要重新培训以适应新的应用程序 - 在现实世界中增加了显着的复杂性和成本。在这项研究中，我们介绍了创新的语义分解和构图多功能编解码器（Discover），以同时增强人眼的感知和机器视觉任务。该方法通过多模式大型模型得出了一组标签，然后将基础模型应用于精确的定位，从而使对编码器端的图像组件的全面理解和分离。在解码阶段，通过将这些编码的组件与生成模型的先验一起利用这些编码组件来实现图像的全面重建，从而优化了人类视觉感知和基于机器的分析任务的性能。广泛的实验评估证实了发现的鲁棒性和有效性，在满足人类和机器视觉要求的双重目标方面表现出了卓越的表现。

### UniPLV: Towards Label-Efficient Open-World 3D Scene Understanding by Regional Visual Language Supervision 
[[arxiv](https://arxiv.org/abs/2412.18131)] [[cool](https://papers.cool/arxiv/2412.18131)] [[pdf](https://arxiv.org/pdf/2412.18131)]
> **Authors**: Yuru Wang,Songtao Wang,Zehan Zhang,Xinyan Lu,Changwei Cai,Hao Li,Fu Liu,Peng Jia,Xianpeng Lang
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: UNIPLV：通过区域视觉语言监督迈向标签有效的开放世界3D场景
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出Uniplv，这是一个强大的框架，在单个学习范式中统一点云，图像和文本，以了解开放世界3D场景的理解。 Uniplv采用图像模态作为桥梁，与共享特征空间中的预先对齐的图像和文本共同安装的3D点，而无需精心制作的点云文本对。为了完成多模式的对齐，我们提出了两个关键策略：（i）图像和点云之间的logit和特征蒸馏模块，（ii）给出了一个Vison-Point匹配模块，以明确纠正由对像素投影的点引起的点数。为了进一步提高统一框架的性能，我们采用了四个特定于任务的损失和两阶段的培训策略。广泛的实验表明，我们的方法在语义分段中平均比最先进的方法分别超过15.6％和14.8％。该代码将稍后发布。

### VisionLLM-based Multimodal Fusion Network for Glottic Carcinoma Early Detection 
[[arxiv](https://arxiv.org/abs/2412.18124)] [[cool](https://papers.cool/arxiv/2412.18124)] [[pdf](https://arxiv.org/pdf/2412.18124)]
> **Authors**: Zhaohui Jin,Yi Shuai,Yongcheng Li,Lingcong Cai,Yun Li,Huifen Liu,Xiaomao Fan
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 基于Visionllm的多模式融合网络，用于发光癌早期检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 发光癌的早期检测对于改善患者结局至关重要，因为它可以及时干预，保留声带功能，并显着降低了肿瘤进展和转移的风险。然而，发光癌和声带发育不良之间形态学的相似性导致次优的检测准确性。为了解决这个问题，我们提出了一个基于语言模型的愿景（基于VisionLlm）多模式融合网络，用于发光癌检测，称为MMGC-NET。通过集成图像和文本模式，多模式模型可以捕获互补信息，从而实现更准确，更健壮的预测。在本文中，我们从Sun Yat-Sen University的第一家附属医院收集了一个名为SYSU1H的私人真实造成的癌数据集，其中有5,799个图像文本对。我们利用图像编码器和其他Q形式来提取视觉嵌入和大语言模型meta ai（llama3）以获取文本嵌入。然后，通过喉部特征融合块整合了这些方式，从而使图像和文本特征的全面整合，从而改善了发光癌的识别性能。 SYSU1H数据集的广泛实验表明，MMGC-NET可以实现最先进的性能，这比以前的多峰模型优越。

### Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach 
[[arxiv](https://arxiv.org/abs/2412.18108)] [[cool](https://papers.cool/arxiv/2412.18108)] [[pdf](https://arxiv.org/pdf/2412.18108)]
> **Authors**: Jing Bi,Junjia Guo,Yunlong Tang,Lianggong Bruce Wen,Zhang Liu,Chenliang Xu
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 在语言模型中揭示视觉感知：注意力头分析方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）的最新进展在视觉理解中表现出了显着的进步。这种令人印象深刻的飞跃提出了一个令人信服的问题：最初仅根据语言数据训练的语言模型如何有效地解释和处理视觉内容？本文旨在通过4个模型家族和4个模型量表进行系统的研究来解决这个问题，并揭示了专门针对视觉内容的独特注意力头。我们的分析揭示了这些注意力头的行为，注意力重量的分布与输入中视觉令牌的注意力之间存在很强的相关性。这些发现增强了我们对LLM如何适应多模式任务的理解，证明了它们在文本和视觉理解之间弥合差距的潜力。这项工作为开发能够以各种方式互动的AI系统的开发铺平了道路。

### COMO: Cross-Mamba Interaction and Offset-Guided Fusion for Multimodal Object Detection 
[[arxiv](https://arxiv.org/abs/2412.18076)] [[cool](https://papers.cool/arxiv/2412.18076)] [[pdf](https://arxiv.org/pdf/2412.18076)]
> **Authors**: Chang Liu,Xin Ma,Xiaochen Yang,Yuxiang Zhang,Yanni Dong
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: Como：多模式对象检测
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 单模式对象检测任务在遇到各种情况时通常会经历性能降解。相反，多模式对象检测任务可以通过整合来自各种模式的数据来提供有关对象特征的更全面的信息。当前的多模式对象检测方法通常使用各种融合技术，包括常规神经网络和基于变压器的模型，以实现特征融合策略并实现互补信息。但是，由于多模式图像是由不同的传感器捕获的，因此它们之间经常存在未对准，从而使匹配挑战性。这种错位阻碍了在不同方式上建立相同对象的强相关性的能力。在本文中，我们提出了一种用于多模式对象检测任务的新方法，称为“交叉 - 曼巴相互作用和抵消引导的融合（COMO）框架”。 COMO框架采用交叉曼巴技术来制定特征交互方程，从而实现多模式序列化状态计算。这会导致交互式融合输出，同时降低计算开销并提高效率。此外，Como利用了不对格的影响较小的高级特征，以促进互动和传递方式之间的互补信息，从而解决了由摄像机角度和捕获时间变化引起的位置偏移挑战。此外，COMO在Cross-Mamba模块中结合了一种全局和本地扫描机制，以捕获具有局部相关性的功能，尤其是在遥感图像中。为了保留低级功能，偏移引导的融合机制可确保有效的多尺度功能利用，从而构建了增强检测性能的多尺度融合数据立方体。

### MMFactory: A Universal Solution Search Engine for Vision-Language Tasks 
[[arxiv](https://arxiv.org/abs/2412.18072)] [[cool](https://papers.cool/arxiv/2412.18072)] [[pdf](https://arxiv.org/pdf/2412.18072)]
> **Authors**: Wan-Cyuan Fan,Tanzila Rahman,Leonid Sigal
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: MMFACTORY：一种通用的解决方案搜索引擎，用于视觉语言任务
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 随着基础和视觉模型的进步以及有效的微调技术，已经为各种视觉任务开发了许多通用和特殊用途模型。尽管这些模型具有灵活性和可访问性，但没有一个模型能够处理潜在用户可能会设想的所有任务和/或应用程序。最新的方法，例如带有集成工具的视觉编程和多模式LLM，旨在通过程序合成来解决复杂的视觉任务。但是，这种方法忽略了用户限制（例如性能 /计算需求），生成难以部署的测试时间样本特异性解决方案，有时需要的低级指令可能超出了天真用户的能力。为了解决这些限制，我们引入了MMFactory，这是一个通用框架，其中包括模型和指标路由组件，在各种可用模型上像解决方案搜索引擎一样起作用。基于任务说明和示例输入输出对以及（（可选））资源和/或性能限制，MMFACTORY可以通过实例化和结合其模型存储库中的粘性语言工具来暗示各种程序化解决方案。除了合成这些解决方案外，MMFACTORY还提出了指标和基准性能 /资源特性，使用户可以选择满足其独特设计约束的解决方案。从技术角度来看，我们还介绍了一个基于委员会的解决方案建议者，该建议者利用多代理LLM对话为用户生成可执行，多样，通用和强大的解决方案。实验结果表明，MMFACTORY通过提供针对用户问题规格的最先进的解决方案来优于现有方法。项目页面可从https://davidhalladay.github.io/mmfactory_demo获得。

### BIG-MoE: Bypass Isolated Gating MoE for Generalized Multimodal Face Anti-Spoofing 
[[arxiv](https://arxiv.org/abs/2412.18065)] [[cool](https://papers.cool/arxiv/2412.18065)] [[pdf](https://arxiv.org/pdf/2412.18065)]
> **Authors**: Yingjie Ma,Zitong Yu,Xun Lin,Weicheng Xie,Linlin Shen
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted by ICASSP 2025
- **标题**: Big-Moe：旁路隔离的门控MOE，用于广义的多模式抗旋转
- **领域**: 计算机视觉和模式识别
- **摘要**: 在面部识别安全性的领域，多模式的面部反欺骗（FAS）对于对抗演示攻击至关重要。但是，现有技术遇到了由于模态偏见和失衡以及领域的变化而遇到的挑战。我们的研究介绍了专家（MOE）模型的混合，以有效解决这些问题。我们确定了传统的MOE多模式FAS的三个局限性：（1）粗粒专家无法捕获细微的欺骗指标； （2）门控网络对影响决策的输入噪声的敏感性； （3）Moe对引发令牌的敏感性导致传统学习方法过度拟合。为了减轻这些方法，我们提出了旁路隔离的门控萌（Big-Moe）框架，其中包括：（1）增强对微妙欺骗提示的发现的细粒专家； （2）一种隔离门控机制来抵消输入噪声； （3）一种新型的差异卷积提示，绕过具有关键局部特征的门控网络，从而提高了感知能力。在四个基准数据集上进行的广泛实验表明，多模式FAS任务的概括性能有所改善。该代码在https://github.com/murinj/big-moe上发布。

### An Ensemble Approach to Short-form Video Quality Assessment Using Multimodal LLM 
[[arxiv](https://arxiv.org/abs/2412.18060)] [[cool](https://papers.cool/arxiv/2412.18060)] [[pdf](https://arxiv.org/pdf/2412.18060)]
> **Authors**: Wen Wen,Yilin Wang,Neil Birkbeck,Balu Adsumilli
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted by ICASSP 2025
- **标题**: 使用多模式LLM进行短形式视频质量评估的合奏方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 以各种内容，编辑样式和文物为特征的短形式视频的兴起对基于学习的盲目视频质量评估（BVQA）模型构成了重大挑战。以其出色的概括能力而闻名的多模式大语言模型（MLLMS）提出了有希望的解决方案。本文着重于有效利用经过预定的MLLM进行短形式视频质量评估，对预处理和响应变异性的影响以及将MLLM与BVQA模型相结合的见解。我们首先研究了框架预处理和采样技术如何影响MLLM的性能。然后，我们引入了一种基于轻量学习的集合方法，该方法可以自适应地整合了MLLM和最先进的BVQA模型的预测。我们的结果表明，通过拟议的集合方法，表现出了出色的概括性能。此外，对内容感知的集合权重的分析强调，某些视频特征并未完全由现有的BVQA模型完全代表，从而揭示了潜在的方向，以进一步改善BVQA模型。

### A Multimodal Fusion Framework for Bridge Defect Detection with Cross-Verification 
[[arxiv](https://arxiv.org/abs/2412.17968)] [[cool](https://papers.cool/arxiv/2412.17968)] [[pdf](https://arxiv.org/pdf/2412.17968)]
> **Authors**: Ravi Datta Rachuri,Duoduo Liao,Samhita Sarikonda,Datha Vaishnavi Kondur
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted by IEEE Big Data 2024
- **标题**: 与交叉验证的桥梁缺陷检测的多模式融合框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 本文介绍了一项试点研究，介绍了用于检测和分析桥梁缺陷的多模式融合框架，将非破坏性评估（NDE）技术与高级图像处理集成在一起，以实现精确的结构评估。通过将影响回波（IE）和超声波表面波（USW）方法结合在一起，该初步研究重点是识别混凝土结构内的可缺陷区域，强调了诸如分层和剥离等关键指标。使用α形状的地理空间分析，缺陷点的融合和统一的车道边界，提出的框架巩固了不同的数据源，以增强缺陷定位并促进重叠缺陷区域的识别。与自适应图像处理的交叉验证进一步验证了检测到的缺陷，通过将其坐标与视觉数据对齐，利用基于高级轮廓的映射和边界框技术来精确缺陷识别。实验结果的F1得分为0.83，证明了该方法在改善缺陷定位，降低假阳性和增强检测准确性方面的潜在疗效，这为将来的研究和更大程度的验证提供了基础。这种初步探索将该框架确立为有效的桥梁健康评估的有前途的工具，对主动的结构监测和维护产生了影响。

### ChatGarment: Garment Estimation, Generation and Editing via Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.17811)] [[cool](https://papers.cool/arxiv/2412.17811)] [[pdf](https://arxiv.org/pdf/2412.17811)]
> **Authors**: Siyuan Bian,Chenghao Xu,Yuliang Xiu,Artur Grigorev,Zhen Liu,Cewu Lu,Michael J. Black,Yao Feng
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 聊天：服装估算，发电和编辑大语模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们介绍了Chatgarment，这是一种新型方法，利用大型视觉语言模型（VLM）自动化图像或文本描述中3D服装的估计，生成和编辑。与以前在现实世界中挣扎或缺乏交互式编辑功能的方法不同，Chatgarment可以从野外图像或草图中估算缝纫模式，从文本描述中生成它们，并根据用户说明编辑服装，所有这些都在交互式对话中。然后可以将这些缝纫图案悬挂在3D服装中，这些服装很容易动画和模拟。这是通过列出VLM来直接生成JSON文件来实现的，该文件既包含服装类型和样式的文本说明，又要连续数值属性。然后，该JSON文件用于通过编程参数模型创建缝纫模式。为了支持这一点，我们通过扩展其服装类型的覆盖范围并简化其结构以进行有效的VLM微调来完善现有的编程模型GarmentCode。此外，我们通过自动数据管道构建了一个大型图像对图案的大规模数据集，并通过自动数据构建了文本对距离图案。广泛的评估表明，Chatgarment能够从多模式输入中准确重建，生成和编辑服装，从而强调了其在时尚和游戏应用中彻底改变工作流程的潜力。代码和数据将在https://chatgarment.github.io/上找到。

### Comprehensive Multi-Modal Prototypes are Simple and Effective Classifiers for Vast-Vocabulary Object Detection 
[[arxiv](https://arxiv.org/abs/2412.17800)] [[cool](https://papers.cool/arxiv/2412.17800)] [[pdf](https://arxiv.org/pdf/2412.17800)]
> **Authors**: Yitong Chen,Wenhao Yao,Lingchen Meng,Sihong Wu,Zuxuan Wu,Yu-Gang Jiang
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Code is available at https://github.com/Row11n/Prova/tree/main
- **标题**: 全面的多模式原型是用于广泛的唱机对象检测的简单有效的分类器
- **领域**: 计算机视觉和模式识别
- **摘要**: 使模型能够识别大量的开放世界类别一直是对象检测的长期追求。通过利用视觉模型的概括能力，尽管接受了有限的类别培训，但当前的开放世界检测器仍可以识别出更广泛的词汇范围。但是，当训练过程中类别词汇的规模扩大到现实世界的水平时，先前的分类器与粗制名称保持一致，会大大降低这些检测器的识别性能。在本文中，我们介绍了Prova，这是一种用于广泛唱机对象检测的多模式原型分类器。 PROVA提取物全面的多模式原型作为对齐分类器的初始化，以解决广泛的唱机对象识别失败问题。在V3DET上，这种简单的方法可大大提高一阶段，两阶段和基于DITR的检测器之间的性能，并且在监督和开放式摄影库设置中仅具有额外的投影层。特别是，在V3DET的监督环境中，Prova将更快的R-CNN，FCO和DINO分别提高了3.3、6.2和2.9 AP。对于开放式摄影设置，普罗拉（Prova）以32.8基础AP和11.0 Novel Novel AP实现了新的最新性能，比以前的方法为2.6和4.3增益。

### Reasoning to Attend: Try to Understand How <SEG> Token Works 
[[arxiv](https://arxiv.org/abs/2412.17741)] [[cool](https://papers.cool/arxiv/2412.17741)] [[pdf](https://arxiv.org/pdf/2412.17741)]
> **Authors**: Rui Qian,Xin Yin,Dejing Dou
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: This work has been accepted to CVPR 2025, please refer to https://github.com/rui-qian/READ
- **标题**: 参加的理由：尝试了解<seg>令牌如何工作
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的大型多模型模型（LMMS）授权的视觉接地通常依赖于$ \ texttt {<seg>} $令牌作为文本提示，以共同优化视觉语言模型（例如Llava）和下游任务特定的模型（例如，Sam）。但是，我们观察到很少的研究已经研究了它的工作原理。在这项工作中，我们首先可视化相似性图，这些图是通过计算$ \ texttt {<seg>} $ token与图像令牌嵌入的$ \ texttt {<seg>} $来获得的。有趣的是，我们发现相似性图中激活响应的惊人一致性保持不变，这表明$ \ texttt {<seg>} $ token token在image-text对中的语义相似性有效。具体而言，占词汇量的$ \ texttt {<seg>} $ token在文本词汇中扩展，在单个令牌化的图像贴片中进行了广泛的查询，以匹配对象的语义，从文本到配对的图像，而大语言模型（llms）是精细调整的。在上面的发现下，我们介绍了阅读，这有助于LMMS的弹性$ \ textbf {rea} $ soning功能在何处to the $ \ textbf {d} $在从相似性图中借来的高度激活点的指导下。值得注意的是，阅读具有直观的设计，与点模块（SASP）相似，可以将其无缝应用于$ \ texttt {<seg>} $  - 像插件一样的范式。同样，在推理和refcoco（+/g）数据集上进行了广泛的实验。为了验证在微调之后阅读是否遭受灾难性忘记以前的技能，我们进一步评估了其在增强的FP-Refcoco（+/G）数据集上的生成能力。所有代码和模型均可在https://github.com/rui-qian/read上公开获取。

### EPE-P: Evidence-based Parameter-efficient Prompting for Multimodal Learning with Missing Modalities 
[[arxiv](https://arxiv.org/abs/2412.17677)] [[cool](https://papers.cool/arxiv/2412.17677)] [[pdf](https://arxiv.org/pdf/2412.17677)]
> **Authors**: Zhe Chen,Xun Lin,Yawen Cui,Zitong Yu
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted by ICASSP 2025
- **标题**: EPE-P：基于证据的参数有效的促进多模式学习的提示，缺失了模式
- **领域**: 计算机视觉和模式识别
- **摘要**: 在培训和测试过程中，缺失的方式是现实世界多模式学习方案的普遍挑战。管理缺失模式的现有方法通常需要针对每种模式或丢失案例的单独提示设计，从而导致复杂的设计，并且要学习的参数数量大幅增加。随着模态的数量的增长，由于参数冗余，这些方法变得越来越低效率。为了解决这些问题，我们提出了基于证据的参数有效提示（EPE-P），这是一种新颖的和参数有效的多模式网络的方法。我们的方法引入了一种简化的设计，该设计集成了跨不同模态的提示信息，从而降低了复杂性并减轻冗余参数。此外，我们提出了一个基于证据的损失功能，以更好地处理与缺失模式相关的不确定性，从而改善模型的决策。我们的实验表明，在有效性和效率方面，EPE-P优于现有的基于提示的方法。该代码在https://github.com/boris-jobs/epe-p_mllms-robustness上发布。

### DreamFit: Garment-Centric Human Generation via a Lightweight Anything-Dressing Encoder 
[[arxiv](https://arxiv.org/abs/2412.17644)] [[cool](https://papers.cool/arxiv/2412.17644)] [[pdf](https://arxiv.org/pdf/2412.17644)]
> **Authors**: Ente Lin,Xujie Zhang,Fuwei Zhao,Yuxuan Luo,Xin Dong,Long Zeng,Xiaodan Liang
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted at AAAI 2025
- **标题**: Dreamfit：以服装为中心的人类通过轻巧的任何穿着的编码器
- **领域**: 计算机视觉和模式识别
- **摘要**: 从文本或图像提示中以服装为中心的人类发电的扩散模型引起了人们的注意力的巨大应用潜力。但是，现有方法通常会面临困境：轻巧的方法，例如适配器，容易产生不一致的纹理；尽管基于芬太尼的方法涉及高训练成本，并难以维持经过验证的扩散模型的概括能力，从而限制了它们在各种情况下的性能。为了应对这些挑战，我们提出了DreamFit，它结合了专门为以服装为中心的人类一代量身定制的轻量级编码器。 Dreamfit具有三个关键优势：（1）\ TextBf {轻量级训练}：借助提出的自适应注意力和Lora模块，DreamFit显着将模型复杂性最小化至8340万可训练的参数。 （2）\ textbf {nothing dressing}：我们的模型令人惊讶地概括了多种（非）服装，创意风格和及时的说明，并始终在各种情况下提供高质量的结果。 （3）\ textbf {插件和播放}：Dreamfit经过设计，可与任何用于扩散模型的社区控制插件平滑集成，从而确保易于兼容并最大程度地减少采用障碍。为了进一步提高发电质量，Dreamfit利用了预处理的大型多模式模型（LMM）来通过细粒的服装描述来丰富提示，从而减少了训练和推理之间的及时差距。我们对$ 768 \ times 512 $高分辨率基准和野外图像进行了全面的实验。 Dreamfit超过了所有现有方法，突出了其以服装为中心的人类的最新能力。

### AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2412.17601)] [[cool](https://papers.cool/arxiv/2412.17601)] [[pdf](https://arxiv.org/pdf/2412.17601)]
> **Authors**: Jiaqi Ma,Guo-Sen Xie,Fang Zhao,Zechao Li
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted by TMM 2024
- **标题**: afanet：弱监督的自适应频率感知网络
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 很少有学习旨在通过利用从几个样本中学到的先验知识来识别新颖概念。但是，对于视觉密集的任务，例如少量的语义细分，像素级注释既费时又昂贵。因此，在本文中，我们利用了更具挑战性的图像级注释，并提出了一个自适应频率感知网络（AFANET），以降低了弱监督的少数弹性语义分割（WFSS）。具体而言，我们首先提出了一个跨粒度频率感知模块（CFM），该模块将RGB图像分解为高频和低频分布，并通过重新调整来进一步优化语义结构信息。与大多数现有的WFSS方法不同，使用多模式语言视觉模型中的文本信息，例如剪辑，以离线学习方式剪辑，我们进一步提出了一个剪辑引导的空间适配器模块（CSM），该模块（CSM）执行空间域的适应性转换，从而通过在线学习通过在线学习，从而通过在线学习，从而为富有的交叉信息cod codal for cod confm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm cfm。 Pascal-5 \ TextSuperScript {I}和Coco-20 \ TextSuperScript {I}数据集的广泛实验表明，Afanet已实现了最先进的性能。该代码可在https://github.com/jarch-ma/afanet上找到。

### V$^2$-SfMLearner: Learning Monocular Depth and Ego-motion for Multimodal Wireless Capsule Endoscopy 
[[arxiv](https://arxiv.org/abs/2412.17595)] [[cool](https://papers.cool/arxiv/2412.17595)] [[pdf](https://arxiv.org/pdf/2412.17595)]
> **Authors**: Long Bai,Beilei Cui,Liangyu Wang,Yanheng Li,Shilong Yao,Sishen Yuan,Yanan Wu,Yang Zhang,Max Q. -H. Meng,Zhen Li,Weiping Ding,Hongliang Ren
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: To appear in IEEE Transactions on Automation Science and Engineering (IEEE TASE)
- **标题**: v $^2 $ -SFMLEALNER：学习多模式无线胶囊内窥镜检查的单眼深度和自我感动
- **领域**: 计算机视觉和模式识别,人工智能,机器人技术
- **摘要**: 深度学习可以从胶囊内窥镜视频中预测深度图和胶囊自我 - 动作，从而帮助3D场景重建和病变定位。但是，胃肠道内胶囊内窥镜的碰撞会在训练数据中引起振动。现有的解决方案仅着眼于基于视觉的处理，忽略了其他辅助信号，例如振动，可以降低噪声并提高性能。因此，我们提出了V $^2 $ -SFMLEARNER，这是一种多模式方法，将振动信号整合到基于视觉的深度和胶囊运动估计中，以进行单眼胶囊内窥镜检查。我们构建了一个包含振动和视觉信号的多模式胶囊内窥镜数据集，我们的人工智能解决方案使用视觉振动信号开发了一种无监督的方法，从而有效地通过多模式学习有效地消除了振动扰动。具体来说，我们仔细设计了一个振动网络分支和一个傅立叶融合模块，以检测和减轻振动噪声。融合框架与流行的仅视力算法兼容。对多模式数据集的广泛验证表明，针对仅视觉算法的卓越性能和鲁棒性。不需要大型外部设备，我们的V $^$ -SFMLEARNER可以将其集成到临床胶囊机器人中，从而提供实时且可靠的消化检查工具。这些发现显示了在临床环境中实施实施的希望，从而增强了医生的诊断能力。

### HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data 
[[arxiv](https://arxiv.org/abs/2412.17574)] [[cool](https://papers.cool/arxiv/2412.17574)] [[pdf](https://arxiv.org/pdf/2412.17574)]
> **Authors**: Ting Zhou,Daoyuan Chen,Qirui Jiao,Bolin Ding,Yaliang Li,Ying Shen
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: 22 pages, 23 figures, 7 tables
- **标题**: HumanVbench：通过合成基准数据探索以人为中心的视频理解MLLM的功能
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在多模式大语言模型（MLLM）的领域中，实现以人为本的视频理解仍然是一个巨大的挑战。现有的基准主要强调对象和行动识别，通常会忽略视频内容中人类情感，行为和语音审视的复杂性。我们提出了HumanVbench，这是一种创新的基准测试，精心制作，以弥合视频MLLM的评估中的这些差距。 HumanVbench包括16项精心设计的任务，探讨了两个主要维度：内部情感和外部表现，跨越了静态，动态，基本和复杂，以及单模式和跨模式方面。 HumanVbench借助两个用于视频注释和包括质量检查的QA生成的高级自动化管道，并利用各种最先进的技术（SOTA）技术来简化基准数据综合和质量评估，最大程度地降至人类注释依赖性依赖于人类含有人含有的多态属性的依赖性。对22个SOTA视频MLLM的全面评估揭示了当前表现的显着局限性，尤其是在跨模式和情感感知中，强调了进一步完善的必要性，以实现更类似人类的理解。 HumanVbench是开源的，可以促进视频MLLM中未来的进步和现实应用程序。

### S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit Neural Field 
[[arxiv](https://arxiv.org/abs/2412.17561)] [[cool](https://papers.cool/arxiv/2412.17561)] [[pdf](https://arxiv.org/pdf/2412.17561)]
> **Authors**: Zixi Liang,Guowei Xu,Haifeng Wu,Ye Huang,Wen Li,Lixin Duan
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted to AAAI 2025
- **标题**: S-INF：通过场景隐含神经场迈向现实的室内场景综合
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于学习的方法在3D室内场景综合（ISS）中变得越来越流行，显示出优于基于传统优化的方法。这些基于学习的方法通常使用生成模型对简单但明确的场景表示表示进行建模。但是，由于忽略了场景中多模式关系的详细信息和缺乏指导的明确表示，大多数基于学习的方法都难以生成具有现实的对象安排和样式的室内场景。在本文中，我们介绍了一种新方法，即Scene隐式神经场（S-INF），用于室内场景综合，旨在学习多模式关系的有意义的表示，以增强室内场景综合的现实主义。 S-INF假设场景布局通常与对象详细信息有关。它将多模式关系分解为场景布局关系和详细的对象关系，以后通过隐式神经领域（INF）融合它们。通过学习专业场景布局关系并将它们投射到S-INF中，我们实现了现实的场景布局。此外，S-INF通过可区分的渲染捕获了密集且详细的对象关系，从而确保对象之间的风格一致性。通过基准3D-Front数据集的广泛实验，我们证明我们的方法始终在不同类型的ISS下实现最先进的性能。

### WildPPG: A Real-World PPG Dataset of Long Continuous Recordings 
[[arxiv](https://arxiv.org/abs/2412.17540)] [[cool](https://papers.cool/arxiv/2412.17540)] [[pdf](https://arxiv.org/pdf/2412.17540)]
> **Authors**: Manuel Meier,Berken Utku Demirel,Christian Holz
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted at NeurIPS2024
- **标题**: WILDPPG：长连续录音的现实世界PPG数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 反射光插图学（PPG）已成为可穿戴设备中的默认传感技术，以通过人的心率（HR）监测心脏活动。但是，基于PPG的人力资源估计值可能会受到诸如佩戴者活动，传感器放置和由此产生的运动伪影以及环境特征（例如温度和环境光）等因素的实质性影响。这些因素和其他因素可以显着影响并降低人力资源预测可靠性。在本文中，我们表明，从日常活动中处理\ emph {代表性}数据时，最新的人力资源估计方法可能会在室外环境中的日常活动中进行数据，这可能是因为它们依赖于捕获控制条件的现有数据集。我们介绍了一个新型的多模式数据集和基准结果，用于在16个参与者的户外活动中连续录制的结果，超过13.5个小时，从四个可穿戴传感器捕获，每个传感器都在体内的不同位置佩戴，总计216 \，小时。我们的记录包括加速度计，温度和高度数据，以及用于地面真相HR参考的基于I的同步铅I心电图。参与者在一天的过程中完成了从苏黎世到瑞士一座高山的荣格·弗劳乔赫（Jungfraujoch）。这次旅行包括在各种温度和高度（高达3,571 \，海拔高于海平面）以及使用汽车，火车，缆车和运输升降机上的各种温度和高度（最多3,571 \，M高达3,571 \，M升高3,571 \），以及所有影响参与者的生理动力学。我们还提出了一种新颖的方法，该方法在这种现实情况下比现有基准更牢固地估计HR值。

### An Evaluation Framework for Product Images Background Inpainting based on Human Feedback and Product Consistency 
[[arxiv](https://arxiv.org/abs/2412.17504)] [[cool](https://papers.cool/arxiv/2412.17504)] [[pdf](https://arxiv.org/pdf/2412.17504)]
> **Authors**: Yuqi Liang,Jun Luo,Xiaoxi Guo,Jianqi Bi
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: accepted by AAAI2025
- **标题**: 产品图像的评估框架背景基于人类反馈和产品一致性的介绍
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在产品广告应用程序中，利用产品图像中使用AI技术的背景自动介绍已成为一项重要任务。但是，这些技术仍然遭受不当背景和生成产品图像中产品不一致的问题的困扰，并且现有的评估生成产品图像质量的方法与人类的反馈大多不一致，从而导致对此任务的评估取决于手动注释。为了减轻上述问题，本文提出了人类的反馈和产品一致性（HFPC），可以自动根据两个模块评估生成的产品图像。首先，为了解决不适当的背景，收集了44,000个自动化涂料产品图像的人类反馈，以根据BLIP和比较学习提取的多模式特征来训练奖励模型。其次，要过滤含有不一致产品的过滤生成的产品图像，采用微调分割模型来分割原始产品和生成的产品图像的乘积，然后比较上面两个之间的差异。广泛的实验表明，HFPC可以有效地评估生成的产品图像的质量，并显着降低手动注释的费用。此外，与其他开源视觉质量评估模型相比，HFPC可以实现最新的（精确度为96.4％）。数据集和代码可在以下网址找到：https：//github.com/created-bi/background_inpainting_products_dataset

### Multimodal Preference Data Synthetic Alignment with Reward Model 
[[arxiv](https://arxiv.org/abs/2412.17417)] [[cool](https://papers.cool/arxiv/2412.17417)] [[pdf](https://arxiv.org/pdf/2412.17417)]
> **Authors**: Robert Wijaya,Ngoc-Bao Nguyen,Ngai-Man Cheung
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Project Page: https://pds-dpo.github.io/
- **标题**: 与奖励模型的多模式偏好数据合成对齐
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）具有通过集成视觉和文本数据来回答字幕生成和视觉问题等高级任务。但是，由于其预培训数据和实际用户提示之间的差异，它们有时会产生误导性或幻觉内容。在视觉任务中使用直接偏好优化（DPO）的现有方法通常依赖于诸如GPT-4或剪辑之类的强模型来确定正面和负面响应。在这里，我们提出了一个新的框架，以使用奖励模型来生成合成数据，以代表人类偏爱通过DPO培训进行有效的多模式对齐。在LLAVA-V1.5-7B上评估了最终的DPO数据集范围从2K到9K图像文本对，我们的方法证明了基本模型在多个幻觉模型和视觉 - 语言基准的基础模型的可信度和推理能力方面的显着提高。实验结果表明，整合所选的合成数据，例如来自生成和奖励模型，可以有效地减少对人类通知数据的依赖，同时增强MLLM的对齐能力，从而提供可扩展的解决方案以进行更安全的部署。

### VidCtx: Context-aware Video Question Answering with Image Models 
[[arxiv](https://arxiv.org/abs/2412.17415)] [[cool](https://papers.cool/arxiv/2412.17415)] [[pdf](https://arxiv.org/pdf/2412.17415)]
> **Authors**: Andreas Goulas,Vasileios Mezaris,Ioannis Patras
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Submitted for publication
- **标题**: vidctx：上下文感知的视频问题与图像模型回答
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **摘要**: 为了解决视频提问任务中大型多模式模型的计算和内存限制，最近的几种方法每帧提取文本表示（例如，通过字幕）提取文本表示形式，并将其馈送到大型语言模型（LLM）中，以处理它们以产生最终响应。但是，通过这种方式，LLM无法访问视觉信息，并且经常必须处理附近帧的重复文本描述。为了解决这些缺点，在本文中，我们介绍了Vidctx，这是一个新颖的无培训视频框架，该框架都集成了两种模式，即来自输入帧的视觉信息和对其他框架提供适当上下文的文本描述。更具体地说，在拟议的框架中，提示预先训练的大型多模式（LMM）定期提取视频框架的询问文本描述（字幕）。当提示相同的LMM回答手头给出的问题a）某个帧，b）问题和c）适当框架的上下文/标题时将这些问题用作上下文。为了避免冗余信息，我们选择了遥远框架的描述。最后，使用一种简单而有效的最大汇总机制来汇总框架级别的决策。该方法使该模型能够专注于视频的相关段，并扩展到大量帧。实验表明，VIDCTX在依赖于三个公共视频QA基准的开放模型的方法中实现了竞争性能，即Next-QA，IntentQA和Star。

### Neural-MCRL: Neural Multimodal Contrastive Representation Learning for EEG-based Visual Decoding 
[[arxiv](https://arxiv.org/abs/2412.17337)] [[cool](https://papers.cool/arxiv/2412.17337)] [[pdf](https://arxiv.org/pdf/2412.17337)]
> **Authors**: Yueyang Li,Zijian Kang,Shengyu Gong,Wenhao Dong,Weiming Zeng,Hongjie Yan,Wai Ting Siok,Nizhuan Wang
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 神经-MCRL：基于脑电图的视觉解码的神经多模式对比表示学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于脑电图（EEG）的脑活动的解码神经视觉表示对于推进脑机界面（BMI）至关重要，并且具有神经感觉康复的变革潜力。虽然多模式对比表示学习（MCRL）在神经解码方面表现出了希望，但现有方法通常忽略了模式中的语义一致性和完整性，并且缺乏跨模态的有效语义一致性。这限制了他们捕获视觉神经反应的复杂表示的能力。我们提出了Neural-MCRL，这是一个新颖的框架，通过语义桥梁和交叉注意机制实现了多模式对齐，同时确保了跨模态的方式和一致性的完整性。我们的框架还具有神经编码器，具有频谱 - 周期性适应（NESTA），这是一种EEG编码器，可自适应捕获光谱模式并学习特定于主题的转换。实验结果表明，与最先进的方法相比，视觉解码精度和模型概括的显着改善，从而推进了BMI中基于EEG的神经视觉表示的领域。代码将在以下网址提供：https：//github.com/nzwang/neural-mcrl。

### Revisiting Multimodal Fusion for 3D Anomaly Detection from an Architectural Perspective 
[[arxiv](https://arxiv.org/abs/2412.17297)] [[cool](https://papers.cool/arxiv/2412.17297)] [[pdf](https://arxiv.org/pdf/2412.17297)]
> **Authors**: Kaifang Long,Guoyang Xie,Lianbo Ma,Jiaqi Liu,Zhichao Lu
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 从建筑的角度重新访问3D异常检测的多模式融合
- **领域**: 计算机视觉和模式识别
- **摘要**: 现有的努力促进3D异常检测（3D-AD）的多模式融合，主要集中于设计更有效的多模式融合策略。但是，很少关注分析多模式融合体系结构（拓扑）设计在3D-AD贡献中的作用。在本文中，我们旨在弥合这一差距，并就多模式融合体系设计对3D-AD的影响进行系统研究。这项工作考虑了模块内融合水平的多模式融合体系结构设计，即独立模态特异性模块，涉及具有特定融合操作的早期，中或晚期多模式特征，以及在模块间融合水平上，即融合这些模块的策略。在这两种情况下，我们都首先通过理论和实验探索建筑设计如何影响3D-AD的见解。 Then, we extend SOTA neural architecture search (NAS) paradigm and propose 3D-ADNAS to simultaneously search across multimodal fusion strategies and modality-specific modules for the first time.Extensive experiments show that 3D-ADNAS obtains consistent improvements in 3D-AD across various model capacities in terms of accuracy, frame rate, and memory usage, and it exhibits great potential in dealing with few-shot 3D-AD任务。

### AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues 
[[arxiv](https://arxiv.org/abs/2412.17292)] [[cool](https://papers.cool/arxiv/2412.17292)] [[pdf](https://arxiv.org/pdf/2412.17292)]
> **Authors**: Se Jin Park,Yeonju Kim,Hyeongseop Rha,Bella Godiva,Yong Man Ro
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: AV-Emodialog：与视听用户聊天，利用情感提示
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **摘要**: 在人类交流中，言语和非语言提示在传达情感，意图和意义超越单独的意义方面都起着至关重要的作用。这些非语言信息，例如面部表情，眼神交流，语音和音调，是有效互动的基本要素，通过增加情感和上下文深度来丰富对话。认识到非语言内容在交流中的重要性，我们提出了AV-Emodialog，这是一种对话系统，旨在从用户的视听输入中利用口头和非语言信息，以产生响应迅速和善解人意的互动。 AV-Emodialog系统地利用了视听对话中的情感线索；从语音中提取语音内容和情感色调，分析视觉效果的细颗粒面部表情，并整合这些线索以端到端的方式产生情感意识的反应。通过广泛的实验，我们验证了所提出的AV-Emodialog优于现有的多模式LLM，不仅在情感上适当，而且在上下文上适当的响应。

### ObitoNet: Multimodal High-Resolution Point Cloud Reconstruction 
[[arxiv](https://arxiv.org/abs/2412.18775)] [[cool](https://papers.cool/arxiv/2412.18775)] [[pdf](https://arxiv.org/pdf/2412.18775)]
> **Authors**: Apoorv Thapliyal,Vinay Lanka,Swathi Baskaran
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: obitonet：多模式高分辨率点云重建
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: Obitonet采用交叉注意机制来整合多模式输入，其中视觉变压器（VIT）从图像中提取语义特征，并使用最远的点采样（FPS）和k最近的邻居（KNN）（KNN）来提取几何信息，以进行空间结构捕获。学到的多模式特征被馈入基于变压器的解码器，以进行高分辨率点云重建。这种方法利用了两种模态的互补优势丰富的图像特征和精确的几何细节，即使在稀疏或嘈杂数据等挑战性条件下，也可以确保稳健的点云产生。

### Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 
[[arxiv](https://arxiv.org/abs/2412.18688)] [[cool](https://papers.cool/arxiv/2412.18688)] [[pdf](https://arxiv.org/pdf/2412.18688)]
> **Authors**: Faraz Waseem,Muhammad Shahzad
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: 35 pages, 18 figures, Manuscript submitted to ACM
- **标题**: 视频值得一千张图像：探索长视频的最新趋势
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 图像可以传达一千个单词，但是由数百或数千个图像框架组成的视频讲述了一个更复杂的故事。尽管多模式大语言模型（MLLM）取得了重大进展，但生成扩展视频仍然是一个巨大的挑战。在撰写本文时，当前最新系统Openai的Sora仍然仅限于制作长度长达一分钟的视频。这种局限性源于长期视频的复杂性，这不仅需要生成的AI技术来近似密度功能的基本方面，例如计划，故事开发以及保持空间和时间一致性带来了其他障碍。将生成的AI与分界线和串联方法集成可以提高更长的视频的可扩展性，同时提供更大的控制。在这项调查中，我们研究了当前的长视频生成的景观，涵盖了诸如gan和扩散模型，视频生成策略，大规模培训数据集，评估长视频的质量指标以及未来的研究领域，以解决现有视频生成功能的局限性。我们认为这将是一个全面的基础，提供了广泛的信息，以指导长期视频生成领域的未来进步和研究。

### DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers 
[[arxiv](https://arxiv.org/abs/2412.18607)] [[cool](https://papers.cool/arxiv/2412.18607)] [[pdf](https://arxiv.org/pdf/2412.18607)]
> **Authors**: Yuntao Chen,Yuqi Wang,Zhaoxiang Zhang
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: DrivingGPT：通过多模式自回旋变压器统一驾驶世界建模和计划
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于世界模型的搜索和计划被广泛认为是通往人类体力智力的有前途的途径。但是，当前的驾驶世界模型主要依赖于视频扩散模型，该模型专门研究视觉生成，但缺乏融合其他模式（例如动作）的灵活性。相反，自回旋变压器在建模多模式数据方面表现出了非凡的能力。我们的工作旨在将驱动模型模拟和轨迹计划统一为单个序列建模问题。我们基于交织的图像和动作令牌引入了一种多模式驾驶语言，并开发了通过标准的下一步预测来学习联合世界建模和计划的驾驶。我们的驾驶节目在动作条件的视频生成和端到端计划中都表现出强劲的表现，在大规模的NUPLAN和NAVSIM基准上表现出色。

### DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation 
[[arxiv](https://arxiv.org/abs/2412.18597)] [[cool](https://papers.cool/arxiv/2412.18597)] [[pdf](https://arxiv.org/pdf/2412.18597)]
> **Authors**: Minghong Cai,Xiaodong Cun,Xiaoyu Li,Wenze Liu,Zhaoyang Zhang,Yong Zhang,Ying Shan,Xiangyu Yue
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: 19 pages, 19 figures, Project page: https://onevfall.github.io/project_page/ditctrl ; GitHub repository: https://github.com/TencentARC/DiTCtrl
- **标题**: DITCTRL：探索多模式扩散变压器中的注意力控制，以进行无调的多项目较长的视频生成
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **摘要**: 类似Sora的视频生成模型通过多模式扩散变压器MM-DIT体系结构取得了显着的进步。但是，当前的视频生成模型主要集中在单个promp上，努力通过多个顺序提示生成连贯的场景，以更好地反映现实世界动态方案。尽管一些开创性的作品探索了多项目的视频生成，但它们面临着重大挑战，包括严格的培训数据要求，较弱的及时关注和不自然的过渡。为了解决这些问题，我们提出了DITCTRL，这是第一次使用MM-DIT体系结构下的无训练的多项目生成方法。我们的关键想法是将多项目的视频生成任务作为平滑过渡的时间视频编辑。为了实现这一目标，我们首先分析了MM-DIT的注意机制，发现3D的全部注意力与UNET样扩散模型中的交叉/自我发项障碍的行为相似，从而促进了跨不同提示的掩盖式精确的语义控制，并且对多次启动视频产生的注意力共享。基于我们的仔细设计，DITCTRL生成的视频在没有额外培训的情况下进行了多个顺序提示，可以实现平稳的过渡和一致的对象运动。此外，我们还提出了MPVBench，这是一种专门为多项目视频生成而设计的新基准测试，以评估多项目的性能。广泛的实验表明，我们的方法在没有额外培训的情况下实现了最先进的表现。

### RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction 
[[arxiv](https://arxiv.org/abs/2412.18390)] [[cool](https://papers.cool/arxiv/2412.18390)] [[pdf](https://arxiv.org/pdf/2412.18390)]
> **Authors**: Xiaoping Wu,Jie Hu,Xiaoming Wei
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: 8 pages
- **标题**: RDPM：通过复发令牌预测解决扩散概率模型
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体
- **摘要**: 扩散概率模型（DPM）已成为高保真图像综合的事实方法，在连续VAE上进行了操作扩散过程，这与大语言模型（LLMS）采用的文本生成方法显着差异。在本文中，我们引入了一种新颖的生成框架，即复发性扩散概率模型（RDPM），该模型通过复发的令牌预测机制增强了扩散过程，从而开创了离散扩散领域。通过将高斯噪声逐渐引入图像的潜在表示，并以反复的方式将其编码为矢量定量令牌，RDPM促进了离散值域的唯一扩散过程。此过程迭代地预测了随后的时间步的令牌代码，将初始标准高斯噪声转换为源数据分布，并根据损耗函数与GPT风格的模型对齐。 RDPM表现出卓越的性能，同时受益于仅需要一些推理步骤的速度优势。该模型不仅利用扩散过程来确保高质量的生成，而且还将连续信号转换为一系列高保真离散令牌，从而使用其他离散令牌（例如文本）维持统一的优化策略。我们预计这项工作将有助于开发多模式生成的统一模型，特别是通过将图像，视频和音频等连续信号域与文本相结合。我们将向开源社区发布代码和模型权重。

### Computer Vision-Driven Gesture Recognition: Toward Natural and Intuitive Human-Computer 
[[arxiv](https://arxiv.org/abs/2412.18321)] [[cool](https://papers.cool/arxiv/2412.18321)] [[pdf](https://arxiv.org/pdf/2412.18321)]
> **Authors**: Fenghua Shao,Tong Zhang,Shang Gao,Qi Sun,Liuqingqing Yang
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: 计算机视觉驱动的手势识别：朝向自然和直观的人类计算机
- **领域**: 计算机视觉和模式识别
- **摘要**: 这项研究主要探索基于计算机视觉在人机交互中的自然手势识别的应用，旨在通过手势识别技术提高人类计算机相互作用的流利性和自然性。在虚拟现实的领域，增强现实和聪明的家庭，传统的输入方法逐渐无法满足用户的交互式体验的需求。作为一种直观且方便的互动方法，手势受到了越来越多的关注。本文提出了一种基于三维手骨架模型的手势识别方法。通过模拟手接接头的三维空间分布，构建了简化的手骨架结构。通过连接棕榈和每个手指关节，形成了动态和静态的手势模型，进一步提高了手势识别的准确性和效率。实验结果表明，这种方法可以有效地识别各种手势，并保持高识别精度和在不同环境中的实时响应能力。此外，结合多模式技术（例如眼睛跟踪），手势识别系统的智能水平可以进一步改进，从而带来更丰富，更直观的用户体验。将来，随着计算机视觉，深度学习和多模式互动技术的持续发展，基于手势的自然互动将在更广泛的应用程序场景中发挥重要作用，并促进人类计算机交互的革命进步。

### Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search 
[[arxiv](https://arxiv.org/abs/2412.18319)] [[cool](https://papers.cool/arxiv/2412.18319)] [[pdf](https://arxiv.org/pdf/2412.18319)]
> **Authors**: Huanjin Yao,Jiaxing Huang,Wenhao Wu,Jingyi Zhang,Yibo Wang,Shunyu Liu,Yingjie Wang,Yuxin Song,Haocheng Feng,Li Shen,Dacheng Tao
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: Technical report
- **标题**: Mulberry：通过集体蒙特卡洛树搜索以O1般的推理和反思来授权MLLM
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 在这项工作中，我们旨在开发一个MLLM，通过学习创建所涉及的推理的每个中间步骤，直到最终答案，从而理解和解决问题。为此，我们提出了集体蒙特卡洛树搜索（COMCTS），这是一种针对MLLM的新学习方法，该方法将集体学习的概念介绍到``树搜索''中，以进行有效，有效的推理搜索和学习。 COMCT的核心思想是利用从多个模型到协作猜想的集体知识，搜索和确定有效的推理路径，通过四个迭代操作，包括扩展，仿真和错误定位，反向传播和选择。使用COMCT，我们构建了Mulberry-260K，这是一个多模式数据集，每个问题都有丰富，明确且定义明确的推理节点。使用Mulberry-260k，我们执行集体SFT来训练我们的模型Mulberry，这是一系列具有O1式逐步推理和反射功能的MLLM。广泛的实验证明了我们提出的方法对各种基准的优越性。代码将在https://github.com/hjyao00/mulberry上找到

### Towards Modality Generalization: A Benchmark and Prospective Analysis 
[[arxiv](https://arxiv.org/abs/2412.18277)] [[cool](https://papers.cool/arxiv/2412.18277)] [[pdf](https://arxiv.org/pdf/2412.18277)]
> **Authors**: Xiaohao Liu,Xiaobo Xia,Zhuo Huang,Tat-Seng Chua
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: 迈向模态概括：基准和前瞻性分析
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 多模式学习通过整合来自各种模式的信息，在识别和检索等任务中的卓越表现中取得了巨大的成功，与单峰方法相比。但是，实际情况通常会出现由于资源和隐私限制而在培训期间看不见的新型方式，当前的方法很难解决。本文介绍了模态概括（MG），该文章的重点是使模型概括为看不见的模式。我们定义了两种情况：弱mg，可以通过现有的感知器和强烈的MG映射看到和看不见的方式，而没有这种映射。为了促进进步，我们提出了一个全面的基准测试，该基准具有多模式算法，并调整了专注于概括的现有方法。广泛的实验突出了MG的复杂性，暴露了现有方法的局限性，并确定了未来研究的关键方向。我们的工作为推进强大而适应性的多模式模型提供了基础，使他们能够在现实的场景中处理看不见的模式。

### RaCMC: Residual-Aware Compensation Network with Multi-Granularity Constraints for Fake News Detection 
[[arxiv](https://arxiv.org/abs/2412.18254)] [[cool](https://papers.cool/arxiv/2412.18254)] [[pdf](https://arxiv.org/pdf/2412.18254)]
> **Authors**: Xinquan Yu,Ziqi Sheng,Wei Lu,Xiangyang Luo,Jiantao Zhou
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: 9 pages, 4 figures
- **标题**: RACMC：剩余的薪酬网络具有多个伪造的限制，用于假新闻检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式假新闻检测旨在自动识别真实或假新闻，从而减轻由于这种错误信息而造成的不利影响。尽管流行的方法表明了它们的有效性，但挑战仍然存在于跨模式特征融合和分类的改进。为了解决这个问题，我们提出了一个具有多粒性约束（RACMC）的残留感知薪酬网络，以进行虚假新闻检测，该网络旨在充分互动和融合跨模式特征，同时放大真实新闻和假新闻之间的差异。首先，多尺度残差薪酬模块旨在在不同尺度上进行交互和融合特征，并确保特征相互作用的一致性和排他性，从而获得高质量的特征。其次，实施了多个跨性约束模块，以限制新闻中新闻和图像文本对的分布，从而扩大了新闻和特征级别的真实和假新闻之间的差异。最后，开发了一个主导的特征融合推理模块，以从一致性和不一致的角度全面评估新闻真实性。在包括Weibo17，Politifact和Gossipcop在内的三个公共数据集的实验揭示了该方法的优越性。

### ICM-Assistant: Instruction-tuning Multimodal Large Language Models for Rule-based Explainable Image Content Moderation 
[[arxiv](https://arxiv.org/abs/2412.18216)] [[cool](https://papers.cool/arxiv/2412.18216)] [[pdf](https://arxiv.org/pdf/2412.18216)]
> **Authors**: Mengyang Wu,Yuzhi Zhao,Jialun Cao,Mingjie Xu,Zhongming Jiang,Xuehui Wang,Qinbin Li,Guangneng Hu,Shengchao Qin,Chi-Wing Fu
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: Accepted by the AAAI 2025
- **标题**: ICM辅助：基于规则可解释的图像内容中等的指令调整多模式大语模型
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 有争议的内容在很大程度上淹没了互联网，侵犯了各种文化规范和儿童保护标准。传统的图像内容审核（ICM）模型在为各种标准制定精确的适度决策方面缺乏，而最近的多模式大语言模型（MLLMS）在基于一般规则的ICM采用时，通常会产生与人类主持人不一致的分类和解释结果。为了旨在灵活，可解释和准确的ICM，我们设计了一种新型的基于规则的数据集生成管道，分解简洁的人类定义规则，并利用精心设计的多阶段提示来丰富简短的明确图像注释。我们的ICM教学数据集包括详细的调节说明和Q-A对。基于它，我们在基于规则的ICM框架的框架中创建了ICM辅助模型，使其很容易适用于实际实践。我们的ICM辅助模型表现出出色的性能和灵活性。具体而言，它在各种来源上的表现显着优于现有方法，从而改善了适度分类（平均36.8％）和适度的解释质量（平均为26.6％）（平均为26.6％）。代码/数据可在https://github.com/zhaoyuzhi/icm-assistant上找到。

### TextMatch: Enhancing Image-Text Consistency Through Multimodal Optimization 
[[arxiv](https://arxiv.org/abs/2412.18185)] [[cool](https://papers.cool/arxiv/2412.18185)] [[pdf](https://arxiv.org/pdf/2412.18185)]
> **Authors**: Yucong Luo,Mingyue Cheng,Jie Ouyang,Xiaoyu Tao,Qi Liu
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: Need a lot of refinements
- **标题**: TextMatch：通过多模式优化增强图像文本的一致性
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 文本到图像生成模型在创建文本图像方面表现出色，但要确保输出和提示之间的对齐和一致性努力。本文介绍了TextMatch，这是一个充分利用多模式优化的新颖框架，以解决文本到图像（T2I）生成和编辑中的图像文本差异。 TextMatch采用了由大语言模型（LLM）和视觉提问（VQA）模型提供支持的评分策略，以评估提示和生成图像之间的语义一致性。通过整合多模式的内在学习和思想推理链，我们的方法通过迭代优化动态提示了提示。此过程可确保生成的图像更好地捕获用户意图，从而提高忠诚度和相关性。广泛的实验表明，文本匹配显着提高了多个基准的文本图像一致性，从而建立了一个可靠的框架，以提高文本到图像生成模型的功能。我们的代码可在https://anonymon.4open.science/r/textmatch-f55c/上找到。

### SM3Det: A Unified Model for Multi-Modal Remote Sensing Object Detection 
[[arxiv](https://arxiv.org/abs/2412.20665)] [[cool](https://papers.cool/arxiv/2412.20665)] [[pdf](https://arxiv.org/pdf/2412.20665)]
> **Authors**: Yuxuan Li,Xiang Li,Yunheng Li,Yicheng Zhang,Yimian Dai,Qibin Hou,Ming-Ming Cheng,Jian Yang
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: SM3DET：多模式遥感对象检测的统一模型
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 随着遥感技术的快速发展，高分辨率的多模式图像现在更广泛地访问。传统的对象检测模型经过单个数据集的训练，通常仅限于特定的成像方式和注释格式。但是，这种方法忽略了跨多模式的宝贵共享知识，并限制了模型在更通用的场景中的适用性。本文介绍了一项新任务，称为多模式数据集和用于遥感的多任务对象检测（M2DET），旨在从任何传感器模式中准确检测水平或方向对象。此任务构成了1）管理多模式建模所涉及的权衡以及2）多任务优化的复杂性。为了解决这些问题，我们建立了一个基准数据集，并提出了一个统一的模型SM3DET（多模式数据集的单个模型和多任务对象检测）。 SM3DET利用网格级稀疏的MOE主链来实现联合知识学习，同时为不同方式保留不同的特征表示。此外，它使用动态学习率调整集成了一致性和同步优化策略，从而使其能够有效地处理跨模式和任务的不同水平的学习难度。广泛的实验证明了SM3DET的有效性和概括性，在单个数据集上的表现始终优于专业模型。该代码可在https://github.com/zcablii/sm3det上找到。

### Enhancing Visual Representation for Text-based Person Searching 
[[arxiv](https://arxiv.org/abs/2412.20646)] [[cool](https://papers.cool/arxiv/2412.20646)] [[pdf](https://arxiv.org/pdf/2412.20646)]
> **Authors**: Wei Shen,Ming Fang,Yuxia Wang,Jiafeng Xiao,Diping Li,Huangqun Chen,Ling Xu,Weifeng Zhang
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 增强基于文本的人搜索的视觉表示
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于文本的人搜索旨在根据文本描述从大规模图像数据库中检索匹配的行人。该任务的核心难度是如何从行人图像和文本中提取有效的细节，并在公共潜在空间中实现跨模式对齐。先前的作品采用图像和文本编码在单峰数据上进行了预培训，以分别从图像和文本中提取全局和局部特征，然后明确实现全局本地对齐。但是，这些方法仍然缺乏理解视觉细节的能力，并且检索准确性仍然受身份混乱的限制。为了减轻上述问题，我们重新考虑了视觉特征对于基于文本的人搜索的重要性，并提出了VFE-TPS，这是一种视觉功能增强的基于文本的人搜索模型。它引入了预先训练的多模式骨干剪辑，以学习基本的多模式特征，并构建文本指导的蒙版图像建模任务，以增强模型在不明确注释而学习本地视觉细节的能力。此外，我们设计了身份监督全局视觉功能校准任务，以指导模型学习身份感知的全局视觉特征。我们研究的关键发现是，借助我们提出的辅助任务，可以成功地适应基于文本的人搜索任务，并且该模型的视觉理解能力得到了显着增强。三个基准测试的实验结果表明，我们提出的模型超过了现有方法，而秩-1的准确性可显着提高，显着的余量约为$ 1 \％\％\ sim9 \％$。我们的代码可以在https://github.com/zhangweifeng1218/vfe_tps上找到。

### YOLO-UniOW: Efficient Universal Open-World Object Detection 
[[arxiv](https://arxiv.org/abs/2412.20645)] [[cool](https://papers.cool/arxiv/2412.20645)] [[pdf](https://arxiv.org/pdf/2412.20645)]
> **Authors**: Lihao Liu,Juexiao Feng,Hui Chen,Ao Wang,Lin Song,Jungong Han,Guiguang Ding
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: YOLO-UNIOW：有效的通用开放世界对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 传统的对象检测模型受封闭设置数据集的局限性的限制，仅检测训练过程中遇到的类别。尽管多模型模型通过对齐文本和图像方式具有扩展类别识别，但由于交叉模式融合，它们引入了明显的推理间接费用，并且仍然受到预定义词汇的限制，使它们在开放世界情景中处理未知对象无效。在这项工作中，我们介绍了通用开放世界对象检测（UNI-OWD），这是一种统一开放式摄影和开放世界对象检测任务的新范式。为了应对这种环境的挑战，我们提出了Yolo-Uniow，这是一个新的模型，可以提高效率，多功能性和性能的界限。 Yolo-Uniow结合了自适应决策学习，以替代计算昂贵的跨模式融合，并在剪辑潜在空间中轻巧对准，从而在不损害概括的情况下实现了有效的检测。此外，我们设计了一种通配符学习策略，该策略将无法分布的对象视为“未知”，同时无需增量学习就可以动态词汇扩展。该设计使Yolo-Uniow能够无缝地适应开放世界环境中的新类别。广泛的实验验证了Yolo-Uniow的优势，在LVIS上达到34.6 AP和30.0 APR，推理速度为69.6 fps。该模型还在M-OWODB，S-OWODB和Nuscenes数据集上设置了基准测试，并在开放世界对象检测中展示了其无与伦比的性能。代码和型号可在https://github.com/thu-mig/yolo-uniow上找到。

### HALLUCINOGEN: A Benchmark for Evaluating Object Hallucination in Large Visual-Language Models 
[[arxiv](https://arxiv.org/abs/2412.20622)] [[cool](https://papers.cool/arxiv/2412.20622)] [[pdf](https://arxiv.org/pdf/2412.20622)]
> **Authors**: Ashish Seth,Dinesh Manocha,Chirag Agarwal
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 致幻蛋白质：用于评估大型视觉模型中物体幻觉的基准
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 大型视觉模型（LVLM）在执行复杂的多模式任务方面表现出色。但是，它们仍然受到对象幻觉的困扰：图像中存在的对象的错误识别或错误分类。为此，我们提出了一个新颖的视觉问题回答（VQA）对象幻觉攻击基准，该基准利用各种上下文推理提示在最先进的LVLMS中评估对象幻觉。我们设计了一系列上下文推理幻觉提示，以评估LVLMS准确识别目标图像中对象的能力，同时要求它们执行各种视觉语言任务，例如识别，定位或执行特定对象的视觉推理。此外，我们将基准扩展到高风险的医疗应用，并引入Med-Hallucinogen，对生物医学领域量身定制的幻觉攻击，并评估LVLMS在医疗图像上的幻觉性能，这是精度至关重要的关键领域。最后，我们对多个数据集进行了八个LVLM和两种幻觉缓解策略进行广泛的评估，以表明当前的通用和医疗LVLM仍然容易受到幻觉攻击的影响。

### Do Current Video LLMs Have Strong OCR Abilities? A Preliminary Study 
[[arxiv](https://arxiv.org/abs/2412.20613)] [[cool](https://papers.cool/arxiv/2412.20613)] [[pdf](https://arxiv.org/pdf/2412.20613)]
> **Authors**: Yulin Fei,Yuhui Gao,Xingyuan Xian,Xiaojin Zhang,Tao Wu,Wei Chen
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: Accepted by CoLing 2025 (The 31st International Conference on Computational Linguistics)
- **标题**: 当前的视频LLM是否具有强大的OCR能力？一项初步研究
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着多模式大语言模型的兴起，可以准确地从视频内容中提取和理解文本信息，称为基于视频的光学角色识别（视频OCR），已成为至关重要的功能。本文介绍了一种新颖的基准测试，旨在评估视频中多模式模型的视频OCR性能。该基准包括1,028个视频和2,961个问题 - 答案对，通过6个不同的子任务提出了几个关键挑战：（1）识别文本内容本身及其基本视觉属性，（2）语义和空间对OCR对象的语义和空间理解（3）动态运动检测和时间段落本地化。我们使用半自动化的方法开发了这种基准测试，该方法将图像LLM的OCR能力与手动完善，平衡效率，成本和数据质量相结合。我们的资源旨在帮助推进视频LLM的研究，并强调提高视频LLM的OCR能力的需求。基准将在https://github.com/yuhuigao/fg-bench.git上发布。

### Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2412.20455)] [[cool](https://papers.cool/arxiv/2412.20455)] [[pdf](https://arxiv.org/pdf/2412.20455)]
> **Authors**: Ayush Ghadiya,Purbayan Kar,Vishal Chudasama,Pankaj Wasnik
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: Accepted to CVPR'24 MULA Workshop
- **标题**: 较弱监督视频异常检测的跨模式融合和注意机制
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，弱监督的视频异常检测（WS-VAD）已成为当代研究方向，以识别仅使用视频级标签的视频中暴力和裸体等异常事件。但是，此任务面临着重大挑战，包括解决不平衡的模态信息，并始终如一地区分正常和异常特征。在本文中，我们解决了这些挑战，并提出了一个多模式的WS-VAD框架，以准确检测诸如暴力和裸体等异常情况。在提出的框架内，我们引入了一种称为跨模式融合适配器（CFA）的新融合机制，该机制动态选择并增强了与视觉方式相关的高度相关的视听特征。此外，我们引入了双曲线洛伦兹图（HLGATT），以有效地捕获正常表示和异常表示之间的层次关系，从而提高了特征分离精度。通过广泛的实验，我们证明了所提出的模型在暴力和裸露检测基准数据集上实现了最新的结果。

### Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning 
[[arxiv](https://arxiv.org/abs/2412.20392)] [[cool](https://papers.cool/arxiv/2412.20392)] [[pdf](https://arxiv.org/pdf/2412.20392)]
> **Authors**: Zhifang Zhang,Shuo He,Bingquan Shen,Lei Feng
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 通过排斥视觉提示调谐为多模式的背do型模型辩护
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式对比度学习模型（例如剪辑）可以从大规模图像 - 文本数据集中学习高质量的表示，但是它们表现出很大的脆弱性，可引起后门攻击，从而引发了严重的安全问题。在本文中，我们披露了夹子的漏洞主要源于其对类近级特征的过度编码，这可能会损害模型对输入扰动的视觉特征电阻率，从而使其更容易捕获由后门攻击所插入的触发模式。受这一发现的启发，我们提出了令人反感的视觉及时调整（RVPT），这是一种新型的防御方法，采用了专门设计的深视觉及时调整和更具功能的损失，以消除过多的类近乎近乎偶然的功能，同时优化跨透明镜的损失以保持清洁精度。与现有的多模式后门防御方法不同，通常需要有毒数据或涉及整个型号进行微调，RVPT利用很少的下游干净样品，只能调整少量参数。经验结果表明，RVPT仅相对于夹子而言，参数的0.27％\％的表现显着胜过最先进的基线，从而将攻击成功率从67.53 \％降低至2.76 \％降至2.76 \％，对SOTA攻击，并有效地将其防御能力跨越了多个数据集。

### Tri-Ergon: Fine-grained Video-to-Audio Generation with Multi-modal Conditions and LUFS Control 
[[arxiv](https://arxiv.org/abs/2412.20378)] [[cool](https://papers.cool/arxiv/2412.20378)] [[pdf](https://arxiv.org/pdf/2412.20378)]
> **Authors**: Bingliang Li,Fengyu Yang,Yuxin Mao,Qingwen Ye,Hongkai Chen,Yiran Zhong
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: AAAI 2025 Accepted
- **标题**: 三台：具有多模式条件和LUFS控制的细粒度视频到审计
- **领域**: 计算机视觉和模式识别,多媒体,声音,音频和语音处理
- **摘要**: 视频对审计（V2A）的一代利用仅视觉视频功能来产生与场景相对应的逼真的声音。但是，当前的V2A模型通常缺乏对生成的音频的细粒度控制，尤其是在响度变化和多模式条件的结合方面。为了克服这些局限性，我们引入了Tri-egn，这是一种基于扩散的V2A模型，该模型结合了文本，听觉和像素级的视觉提示，以启用详细且具有语义丰富的音频综合。此外，我们介绍了相对于全尺度（LUFS）嵌入的响度单元，从而可以精确地控制各个音频通道的响度随时间变化，从而使我们的模型能够有效地解决现实世界中Foley Foley Workflows中视频和音频的复杂相关性。 Tri-gregon能够创建44.1 kHz的高保真立体声音频剪辑，其长度的长度高达60秒，这显着优于现有的最新V2A方法，通常在固定持续时间内生成单声道。

### Towards Visual Grounding: A Survey 
[[arxiv](https://arxiv.org/abs/2412.20206)] [[cool](https://papers.cool/arxiv/2412.20206)] [[pdf](https://arxiv.org/pdf/2412.20206)]
> **Authors**: Linhui Xiao,Xiaoshan Yang,Xiangyuan Lan,Yaowei Wang,Changsheng Xu
> **First submission**: 2024-12-28
> **First announcement**: 2024-12-30
> **comment**: TPAMI under review. We keep tracing related works at https://github.com/linhuixiao/Awesome-Visual-Grounding
- **标题**: 迈向视觉接地：调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉接地也称为参考表达理解和短语接地。它涉及基于给定的文本描述在图像中将自然数量的特定区域定位。这项任务的目的是模仿社交对话中普遍的参考关系，使机器为人类般的多模式理解能力提供了装备。因此，它在各个领域都有广泛的应用。然而，自2021年以来，视觉接地取得了重大的进步，新兴的概念，例如扎根的预训练，接地多模式LLM，广义视觉接地和GIGA像素接地，这带来了许多新的挑战。在这项调查中，我们最初研究了视觉接地的发展历史，并提供了基本背景知识的概述。我们系统地跟踪和总结了进步并精心组织视觉接地的各种环境，从而确立了这些设置的精确定义，以标准化未来的研究并确保进行公平的比较。此外，我们深入研究了几个高级主题，并突出了许多视觉接地的应用。最后，我们概述了视觉基础面临的挑战，并为未来的研究提出了宝贵的方向，这可能是后续研究人员的灵感。通过提取共同的技术细节，本调查涵盖了过去十年来每个亚主题中的代表作品。最好的是，本文介绍了接地领域当前可用的最全面的概述。该调查旨在适合初学者和经验丰富的研究人员，是理解关键概念和跟踪最新研究发展的宝贵资源。我们在https://github.com/linhuixiao/awesome-visual-grounding上一直在追踪相关作品。

### Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems 
[[arxiv](https://arxiv.org/abs/2412.20201)] [[cool](https://papers.cool/arxiv/2412.20201)] [[pdf](https://arxiv.org/pdf/2412.20201)]
> **Authors**: Wen-Dong Jiang,Chih-Yung Chang,Hsiang-Chuan Chang,Ji-Yuan Chen,Diptendu Sinha Roy
> **First submission**: 2024-12-28
> **First announcement**: 2024-12-30
> **comment**: IEEE TETC-CS (Under review)
- **标题**: 将解释性和轻质设计注入弱监督的视频异常检测系统
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 弱监督监测异常检测（WSMAD）利用弱监督学习来识别异常，这是智能城市监测的关键任务。但是，由于其复杂性，现有的多模式方法通常无法满足边缘设备的实时和解释性要求。本文介绍了TCVAD（两阶段的跨模式视频异常检测系统），该系统利用知识蒸馏和跨模式对比度学习，以使边缘设备上有效，准确且可解释的异常检测。TCVADS在两个阶段中运行：粗元的快速分类和细化的详细分析。在第一阶段，TCVADS从视频帧中提取功能，并将它们输入到时间序列分析模块中，该模块充当教师模型。然后，通过知识蒸馏转移到简化的卷积网络（学生模型）以进行二进制分类。检测到异常后，采用细粒度的多级分类模型触发第二阶段。该阶段使用剪辑与文本和图像进行跨模式对比度学习，通过特殊设计的三胞胎文本关系增强可解释性并实现精致的分类。实验结果表明，TCVADS在模型性能，检测效率和可解释性方面大大优于现有方法，为智能城市监控应用提供了宝贵的贡献。

### Multi-Modality Driven LoRA for Adverse Condition Depth Estimation 
[[arxiv](https://arxiv.org/abs/2412.20162)] [[cool](https://papers.cool/arxiv/2412.20162)] [[pdf](https://arxiv.org/pdf/2412.20162)]
> **Authors**: Guanglei Yang,Rui Tian,Yongqiang Zhang,Zhun Zhong,Yongqiang Li,Wangmeng Zuo
> **First submission**: 2024-12-28
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 多模式驱动的LORA用于不良条件深度估计
- **领域**: 计算机视觉和模式识别
- **摘要**: 自主驾驶社区越来越集中于解决角病例问题，尤其是与确保在不利条件下驾驶安全有关的问题（例如夜间，雾，雨）。为此，不利条件深度估计（ACDE）的任务引起了人们的重大关注。 ACDE中的先前方法主要依赖于生成模型，这些模型需要其他目标图像将阳光的条件转换为不利天气，或可学习的参数以扩大特征，以适应域间隙，从而增加了模型的复杂性和调整工作。此外，与已经预先对齐的文本和视觉特征的基于夹的方法不同，深度估计模型缺乏多模式特征之间的足够比对，在不利条件下阻碍了连贯的理解。为了解决这些限制，我们提出了多模式驱动的洛拉（MMD-lora），该洛拉（MMD-lora）利用低级适应矩阵从源域到目标域，以有效地进行微调。它由两个核心组成部分组成：迅速驱动的域对齐（PDDA）和视觉文本一致的对比度学习（VTCCL）。在PDDA期间，使用MMD-LORA的图像编码器会生成目标域的视觉表示，并通过对齐损失的监督，即语言和图像之间的源目标差异应相等。同时，VTCCL弥合了从剪辑中的文本特征和扩散模型的视觉特征之间的差距，推动了不同的天气表示（视觉和文本），并将相似的差距融合在一起。通过广泛的实验，所提出的方法在Nuscenes和Oxford Robotcar数据集上实现了最新的性能，从而强调了适应各种不利环境的鲁棒性和效率。

### ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming 
[[arxiv](https://arxiv.org/abs/2412.20105)] [[cool](https://papers.cool/arxiv/2412.20105)] [[pdf](https://arxiv.org/pdf/2412.20105)]
> **Authors**: Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu
> **First submission**: 2024-12-28
> **First announcement**: 2024-12-30
> **comment**: Accepted to AAAI2025
- **标题**: ST $^3 $：通过时空视觉令牌加速多模式的大型语言模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式大语言模型（MLLM）通过集成视觉和文本信息来增强其感知能力。但是，处理大量的视觉令牌会产生大量的计算成本。对MLLM注意机制的现有分析仍然很浅，导致粗颗粒令牌修剪策略无法有效地平衡速度和准确性。在本文中，我们对LLAVA进行了全面研究MLLM注意机制。我们发现在解码过程中，许多视觉令牌和部分注意计算都是多余的。基于此洞察力，我们提出了时空的视觉令牌修剪（$ \ textbf {st}}^{3} $），这是一个旨在加速MLLM推断而无需再再训练的框架。 $\textbf{ST}^{3}$ consists of two primary components: 1) Progressive Visual Token Pruning (\textbf{PVTP}), which eliminates inattentive visual tokens across layers, and 2) Visual Token Annealing (\textbf{VTA}), which dynamically reduces the number of visual tokens in each layer as the generated tokens 生长。与原始LLAVA相比，这些技术一起提供了$ \ Mathbf {2 \ times} $更快的推断，仅$ \ mathbf {30 \％} $ kV缓存内存，同时保持各种数据集的一致性。至关重要的是，$ \ textbf {st}^{3} $可以无缝集成到现有的预训练的MLLMS中，从而提供了插件和播放解决方案，以有效地推断。

### On the Compositional Generalization of Multimodal LLMs for Medical Imaging 
[[arxiv](https://arxiv.org/abs/2412.20070)] [[cool](https://papers.cool/arxiv/2412.20070)] [[pdf](https://arxiv.org/pdf/2412.20070)]
> **Authors**: Zhenyang Cai,Junying Chen,Rongsheng Wang,Weihong Wang,Yonglin Deng,Dingjie Song,Yize Chen,Zixu Zhang,Benyou Wang
> **First submission**: 2024-12-28
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 关于用于医学成像的多模式LLM的组成概括
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 多模式大语言模型（MLLM）在医疗领域具有巨大的潜力，但是它们的功能通常受到某些医学领域的数据不足的限制，强调需要了解MLLM可以使用哪种图像进行概括。当前的研究表明，多任务培训的表现优于单任务，因为不同的任务可以彼此受益，但是他们经常忽略这些任务内的内部关系，从而为选择数据集提供了有限的指导来增强特定任务。为了分析这种现象，我们试图采用组成概括（CG） - 模型通过重新组合学习元素来理解新型组合的能力 - 作为指导框架。由于医疗图像可以通过模态，解剖区域和任务来精确定义，因此自然提供了探索CG的环境。因此，我们组装了106个医疗数据集，以创建Med-Mat进行全面的实验。该实验证实，MLLM可以使用CG了解看不见的医学图像，并将CG确定为在多任务训练中观察到的概括之一。此外，进一步的研究表明，CG有效地支持数据集的数据有限，并在不同的骨架上提供一致的性能，从而突出了其多功能性和广泛的适用性。 Med-Mat可在https://github.com/freedomintelligence/med-mat上公开获取。

### VELoRA: A Low-Rank Adaptation Approach for Efficient RGB-Event based Recognition 
[[arxiv](https://arxiv.org/abs/2412.20064)] [[cool](https://papers.cool/arxiv/2412.20064)] [[pdf](https://arxiv.org/pdf/2412.20064)]
> **Authors**: Lan Chen,Haoxiang Yang,Pengpeng Shao,Haoyu Song,Xiao Wang,Zhicheng Zhao,Yaowei Wang,Yonghong Tian
> **First submission**: 2024-12-28
> **First announcement**: 2024-12-30
> **comment**: In Peer Review
- **标题**: Velora：一种基于有效RGB事件的识别的低级适应方法
- **领域**: 计算机视觉和模式识别,人工智能,神经和进化计算
- **摘要**: 利用RGB和事件摄像机的模式识别可以通过部署使用微调策略的深层神经网络来显着提高性能。受大型模型成功应用的启发，也可以考虑引入这样的大型模型，以进一步提高多模式任务的性能。但是，已经提出了这些模型完全微调这些模型导致效率低下和轻巧的微调方法，例如洛拉和适配器，以在效率和性能之间取得更好的平衡。据我们所知，目前尚无基于预训练的基础模型的RGB事实识别的参数有效的微调（PEFT）。为了解决这个问题，本文提出了一种新型的PEFT策略，以适应基于RGB的基于RGB的基础愿景模型。具体而言，鉴于RGB帧和事件流，我们根据视觉基础模型VIT提取RGB和事件功能，具有特定于模态的LORA调整策略。还考虑了双重模式的框架差异以通过框架差骨干网络捕获运动提示。这些特征是串联的，并将其馈入高级变压器层中，以通过模态共享的洛拉调整进行有效的多模式特征学习。最后，我们将这些特征加入，并将它们喂入分类头，以实现有效的微调。源代码和预训练模型将在\ url {https://github.com/event-ahu/velora}上发布。

### Multimodal joint prediction of traffic spatial-temporal data with graph sparse attention mechanism and bidirectional temporal convolutional network 
[[arxiv](https://arxiv.org/abs/2412.19842)] [[cool](https://papers.cool/arxiv/2412.19842)] [[pdf](https://arxiv.org/pdf/2412.19842)]
> **Authors**: Dongran Zhang,Jiangnan Yan,Kemal Polat,Adi Alhudhaif,Jun Li
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 具有图形稀疏注意机制和双向时间卷积网络的流量时空数据的多模式关节预测
- **领域**: 计算机视觉和模式识别
- **摘要**: 交通流量预测在城市运输系统的管理和运营中起着至关重要的作用。尽管已经对单个运输模式的预测进行了广泛的研究，但对不同运输模式的联合预测的研究相对有限。此外，现有的多模式交通接头模型方法通常缺乏空间特征提取的灵活性。为了解决这些问题，我们提出了一种用双向时间卷积网络（GSABT）的方法，用于多模式交通时空的关节预测。首先，我们使用的多模式图乘以自发项权重来捕获空间局部特征，然后采用Top-U稀疏注意机制获得空间全局特征。其次，我们利用双向时间卷积网络来增强输出和输入数据之间的时间特征相关性，并通过共享唯一模块提取模式间和模式内的时间特征。最后，我们设计了一个多模式的关节预测框架，可以灵活地扩展到空间和时间尺寸。在三个真实数据集上进行的广泛实验表明，所提出的模型始终达到最先进的预测性能。

### Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing 
[[arxiv](https://arxiv.org/abs/2412.19806)] [[cool](https://papers.cool/arxiv/2412.19806)] [[pdf](https://arxiv.org/pdf/2412.19806)]
> **Authors**: Hao Fei,Shengqiong Wu,Hanwang Zhang,Tat-Seng Chua,Shuicheng Yan
> **First submission**: 2024-10-08
> **First announcement**: 2024-12-30
> **comment**: Accepted by NeurIPS 2024
- **标题**: VITRON：统一的像素级视觉LLM用于理解，生成，分割，编辑
- **领域**: 计算机视觉和模式识别,人机交互
- **摘要**: 视力大型语言模型（LLM）的最新发展取得了显着的进步，但仍遇到对多模式通才的挑战，例如粗粒度的实例级别的理解，对图像和视频的统一支持缺乏统一的支持，以及在各种视觉任务中的覆盖范围不足。在本文中，我们介绍了Vivron，这是一种通用像素级视觉LLM，旨在全面理解，生成，分割和编辑静态图像和动态视频。 Votron在LLM主链的基础上建立在其前端模块内的图像，视频和像素级区域视觉效果的编码器，同时采用最先进的视觉专家作为后端，通过该镜头，Vivron支持视觉最终任务，从而从低级别到高级别，从而跨越了视觉上的视觉理解，从而跨越了视觉上。为了确保从LLM传递到后端模块以进行功能调用，我们通过同时整合离散的文本指令和连续信号嵌入来提出一种新型混合方法。此外，我们设计了各种像素级时空视觉语言对准学习，以达到最佳的细粒视觉能力。最后，建议一个跨任务协同模块学习最大化任务不变的细粒视觉特征，从而增强不同视觉任务之间的协同作用。 VITRON展示了超过12个视觉任务，并在22个数据集中进行了评估，并在四个主要视觉任务簇中展示了其广泛的功能。总体而言，这项工作阐明了开发更统一的多模式通才的巨大潜力。项目主页：https：//vitron-llm.github.io/

### MVTamperBench: Evaluating Robustness of Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2412.19794)] [[cool](https://papers.cool/arxiv/2412.19794)] [[pdf](https://arxiv.org/pdf/2412.19794)]
> **Authors**: Amit Agarwal,Srikant Panda,Angeline Charles,Bhargava Kumar,Hitesh Patel,Priyaranjan Pattnayak,Taki Hasan Rafi,Tejaswini Kumar,Dong-Kyu Chae
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: :68T37; 68T05; 68Q32; 68T45; 94A08; 68T40; 68Q85ACM Class:I.2.10; I.2.7; I.5.4; I.4.9; I.4.8; H.5.1
- **标题**: mvtamperbench：评估视觉模型的鲁棒性
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在视频理解方面取得了重大进展，但是它们易受对抗性篡改和操纵的脆弱性。为了解决这一差距，我们引入了MVTAMPERBENCH，这是一种系统地评估MLLM稳健性，以针对五种普遍的篡改技术：旋转，掩蔽，替换，重复和掉落。由3.4k原始视频构建，扩展到超过17k的篡改剪辑，涵盖了19个视频任务。 MVTAMPERBENCH挑战模型，以检测空间和时间连贯性的操纵。我们评估了来自15多个模型家族的45个MLLM，揭示了侵入类型的弹性差异很大，并表明较大的参数计数不一定可以保证鲁棒性。 MVTAMPERBENCH设定了一种新的基准，用于在安全至关重要的应用中开发篡改的MLLM，包括检测ClickBait，防止有害内容分布以及在媒体平台上执行政策。我们发布所有代码和数据，以促进对可信赖的视频理解的开放研究。代码：https：//amitbcp.github.io/mvtamperbench/数据：https：//huggingface.co/datasetsets/srikant86/mvtamperbench

### From Elements to Design: A Layered Approach for Automatic Graphic Design Composition 
[[arxiv](https://arxiv.org/abs/2412.19712)] [[cool](https://papers.cool/arxiv/2412.19712)] [[pdf](https://arxiv.org/pdf/2412.19712)]
> **Authors**: Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: Project Page: $\href{https://elements2design.github.io/}{\text{elements2design}}$
- **标题**: 从元素到设计：一种自动图形设计组成的分层方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项工作中，我们研究了来自多模式图形元素的自动设计组成。尽管最近的研究开发了用于图形设计的各种生成模型，但它们通常面临以下局限性：它们仅关注某些子任务，而不是实现设计组成任务；他们不考虑生成过程中图形设计的层次结构信息。为了解决这些问题，我们将分层设计原理引入大型多模型（LMM），并提出一种称为LADECO的新方法，以完成这项具有挑战性的任务。具体而言，Ladeco首先为给定元素集执行图层计划，并根据其内容将输入元素分为不同的语义层。基于计划结果，它随后预测以层次方式控制设计组成的元素属性，并将以前生成层的渲染图像包含在上下文中。通过这种有见识的设计，Ladeco将艰巨的任务分解为较小的可管理步骤，从而使生成过程更加顺畅，更清晰。实验结果证明了Ladeco在设计组成中的有效性。此外，我们表明LADECO在图形设计中启用了一些有趣的应用程序，例如分辨率调整，元素填充，设计变化等。此外，它甚至在没有任何特定于任务的培训的情况下超过了某些设计子任务中的专用模型。

### A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization 
[[arxiv](https://arxiv.org/abs/2412.19685)] [[cool](https://papers.cool/arxiv/2412.19685)] [[pdf](https://arxiv.org/pdf/2412.19685)]
> **Authors**: Jingchun Lian,Lingyu Liu,Yaxiong Wang,Yujiao Wu,Li Zhu,Zhedong Zheng
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: 10 pages, 4 figures, 4 tabels
- **标题**: 面部图像定位的大规模解释多模式基准
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 图像伪造的定位以识别图像中篡改的像素为中心，已经取得了重大进步。传统方法通常将这一挑战作为图像分割的变体建模，将锻造区域的二进制分割视为最终产品。我们认为，基本的二元伪造面罩不足以解释模型预测。它没有澄清为什么该模型查明某些区域并将所有锻造像素都相同，从而使所有看起来最假的零件都很难。在这项研究中，我们通过为伪造图像产生以区域为重点的解释来减轻上述局限性。为了支持这一点，我们制作了一个多模式的曲折跟踪（MMTT）数据集，其中包括使用DeepFake技术操纵的面部图像，并与手动，可解释的文本注释配对。为了收获高质量的注释，指示注释者精心观察受操纵的图像并阐明伪造区域的典型特征。随后，我们收集了128,303个图像文本对的数据集。利用MMTT数据集，我们开发了ForgeryTalker，这是一种旨在同时进行伪造的定位和解释的建筑。 ForgeryTalker首先训练一个伪造提示网络，以识别解释性文本中的关键线索。随后，该地区求职者被纳入多模式的大语言模型中，以实现本地化和解释的双重目标。在MMTT数据集上进行的广泛实验验证了我们提出的模型的出色性能。将公开使用数据集，代码以及审慎的检查站，以促进进一步的研究并确保我们的结果的可重复性。

### CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2412.19663)] [[cool](https://papers.cool/arxiv/2412.19663)] [[pdf](https://arxiv.org/pdf/2412.19663)]
> **Authors**: Siyu Wang,Cailian Chen,Xinyi Le,Qimin Xu,Lei Xu,Yanzhou Zhang,Jie Yang
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: CAD-GPT：使用空间推理增强的多模式LLMS合成CAD构造序列
- **领域**: 计算机视觉和模式识别,人工智能,图形
- **摘要**: 计算机辅助设计（CAD）可通过实现精确的2D和3D建模，广泛的分析和优化来显着提高设计过程的效率，准确性和创新。创建CAD模型的现有方法依赖于难以获得且存储昂贵的潜在向量或点云。多模式大语言模型（MLLM）的最新进展激发了研究人员使用自然语言说明和图像进行CAD模型构建。但是，这些模型仍然在推断准确的3D空间位置和方向方面努力，导致确定空间3D起点和用于构建几何形状的挤压方向的不准确性。这项工作引入了CAD-GPT，这是一种具有空间推理增强MLLM的CAD合成方法，该方法以单个图像或文本描述为输入。为了获得精确的空间推断，我们的方法引入了3D建模空间机制。此方法使用专门的空间展开机制将3D空间位置和3D草图平面旋转角度映射到1D语言特征空间中，同时将2D草图坐标离散到适当的平面空间中，以精确确定空间起始位置，草图方向和2D草图素描互惠式转换。广泛的实验表明，CAD-GPT在定量和质量上始终优于CAD模型合成中现有的最新方法。

### RecConv: Efficient Recursive Convolutions for Multi-Frequency Representations 
[[arxiv](https://arxiv.org/abs/2412.19628)] [[cool](https://papers.cool/arxiv/2412.19628)] [[pdf](https://arxiv.org/pdf/2412.19628)]
> **Authors**: Mingshu Zhao,Yi Luo,Yong Ouyang
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: Tech report;
- **标题**: RECCONV：多频表示的有效递归卷积
- **领域**: 计算机视觉和模式识别
- **摘要**: 视力变压器（VIT）的最新进展证明了全球建模能力的优势，促使大型内联卷积的广泛整合以扩大有效的接受场（ERF）。但是，参数计数和计算复杂性（FLOPS）相对于内核大小的二次缩放会带来显着的效率和优化挑战。本文介绍了RECCONV，这是一种递归分解策略，该策略可有效地使用小内核卷积构建多频表示。 RECCONV建立了参数增长与分解级别之间的线性关系，这决定了基本内核$ k $和$ \ ell $分解级别的有效内核尺寸$ k \ times 2^\ ell $，而无论erf ERF扩展如何。具体而言，与标准和深度卷积的指数增长（$ 4^\ ell $）相比，RECCONV仅实现了仅$ \ ell+2美元+2美元的参数扩展，最大flops增加了$ 5/3 $乘以。 recnext-m3在可可夫人上以类似的拖鞋上的可可（Coco）上的1.9 $ ap^{box} $胜过m1.1。这项创新为设计各种方式设计高效，紧凑的网络提供了有希望的途径。代码和模型可以在\ url {https://github.com/suuf/recnext}上找到。

### MINIMA: Modality Invariant Image Matching 
[[arxiv](https://arxiv.org/abs/2412.19412)] [[cool](https://papers.cool/arxiv/2412.19412)] [[pdf](https://arxiv.org/pdf/2412.19412)]
> **Authors**: Xingyu Jiang,Jiangwei Ren,Zizhuo Li,Xin Zhou,Dingkang Liang,Xiang Bai
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: The dataset and code are available at https://github.com/LSXI7/MINIMA
- **标题**: minima：模态不变图像匹配
- **领域**: 计算机视觉和模式识别
- **摘要**: 跨视图和交叉模式的图像匹配在多模式感知中起着关键作用。在实践中，由不同的成像系统/样式引起的方式差距为匹配任务带来了巨大的挑战。现有的作品尝试提取特定方式的不变特征，并在有限的数据集上进行训练，显示出较差的概括。在本文中，我们提出了Minima，这是多种跨模式案例的统一图像匹配框架。在不追求精美模块的情况下，我们的最小值旨在从数据扩展的角度提高普遍的性能。为此，我们提出了一个简单而有效的数据引擎，可以自由地生产一个包含多种模式，丰富场景和准确匹配标签的大型数据集。具体而言，我们通过生成模型从廉价但丰富的纯RGB匹配数据中扩展了模式。在此设置下，RGB数据集的匹配标签和丰富的多样性是由生成的多模式数据很好地继承的。从中受益，我们构建了MD-Syn，这是一种新的综合数据集，填补了一般多模式图像匹配的数据差距。使用MD-Syn，我们可以在随机选择的模态对上直接训练所有高级匹配管道以获得交叉模式能力。关于内域和零拍匹配任务的广泛实验，包括$ 19 $的跨模式案例，表明我们的最小值可以显着超过基线，甚至超过特定于模态的方法。该数据集和代码可在https://github.com/lsxi7/minima上找到。

### MLLM-SUL: Multimodal Large Language Model for Semantic Scene Understanding and Localization in Traffic Scenarios 
[[arxiv](https://arxiv.org/abs/2412.19406)] [[cool](https://papers.cool/arxiv/2412.19406)] [[pdf](https://arxiv.org/pdf/2412.19406)]
> **Authors**: Jiaqi Fan,Jianhua Wu,Jincheng Gao,Jianhao Yu,Yafei Wang,Hongqing Chu,Bingzhao Gao
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: MLLM-SUL：用于语义场景的多模式大型语言模型在流量方案中理解和本地化
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大语言模型（MLLM）在许多自动驾驶任务中表现出令人满意的效果。在本文中，MLLM可用于解决联合语义场景的理解和风险本地化任务，而仅依靠前视图像。在拟议的MLLM-SUL框架中，首先设计了双分支视觉编码器，旨在从两种分辨率中提取功能，丰富的视觉信息有助于语言模型准确地描述了不同尺寸的风险对象。然后，对于语言的生成，骆驼模型进行了微调，以预测场景描述，其中包含驾驶场景的类型，风险对象的动作以及驱动意图和自我车辆的建议。最终，基于变压器的网络结合了回归令牌，以定位风险对象。对现有的戏剧 - 罗利人数据集和扩展的戏剧-SRIS数据集进行了广泛的实验表明，我们的方法是有效的，超过了许多基于图像的最新和基于视频的方法。具体而言，我们的方法在现场理解任务中获得了80.1％的BLEU-1分数和298.5％的苹果酒得分，而本地化任务的精度为59.6％。代码和数据集可在https://github.com/fjq-tongji/mllm-sul上找到。

### Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment 
[[arxiv](https://arxiv.org/abs/2412.19326)] [[cool](https://papers.cool/arxiv/2412.19326)] [[pdf](https://arxiv.org/pdf/2412.19326)]
> **Authors**: Ziang Yan,Zhilin Li,Yinan He,Chenting Wang,Kunchang Li,Xinhao Li,Xiangyu Zeng,Zilei Wang,Yali Wang,Yu Qiao,Limin Wang,Yi Wang
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: technical report
- **标题**: 任务偏好优化：通过视觉任务对齐改进多模式大语模型
- **领域**: 计算机视觉和模式识别
- **摘要**: 当前的多模式大型语言模型（MLLM）在视觉效果中对视觉效果进行了精细或精确的理解，尽管它们在视觉应用中具有全面的感知和推理。最近的研究要么将工具使用或将特定的视觉任务统一到自回归框架中，通常是以总体多模式性能为代价。为了解决此问题并以可扩展的方式通过视觉任务增强MLLM，我们提出了任务偏好优化（TPO），这是一种新的方法，它利用了从典型的细粒视觉任务中得出的可区分任务偏好。 TPO引入了可学习的任务令牌，以在多个特定任务的头部和MLLM之间建立连接。通过在训练过程中利用丰富的视觉标签，TPO可显着增强MLLM的多模式功能和特定于任务的性能。通过TPO中的多任务共同培训，我们观察到协同的好处，可以将单个任务绩效提升到通过单任务培训方法可实现的功能。与基线模型相比，我们对VideoChat和Llava对这种方法的实例化表明，多模式性能的总体增长了14.6％。此外，MLLM-TPO展示了各种任务的强大零击功能，与最先进的监督模型相当地执行。该代码将在https://github.com/opengvlab/tpo上发布

### Perceive, Query & Reason: Enhancing Video QA with Question-Guided Temporal Queries 
[[arxiv](https://arxiv.org/abs/2412.19304)] [[cool](https://papers.cool/arxiv/2412.19304)] [[pdf](https://arxiv.org/pdf/2412.19304)]
> **Authors**: Roberto Amoroso,Gengyuan Zhang,Rajat Koner,Lorenzo Baraldi,Rita Cucchiara,Volker Tresp
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: WACV 2025
- **标题**: 感知，查询和原因：通过问题指导的时间查询增强视频质量检查
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频问题回答（视频质量请访问）是一项具有挑战性的视频理解任务，需要模型来理解整个视频，根据给定问题的上下文提示确定最相关的信息，并准确地提供答案的理由。多模式大语言模型（MLLM）的最新进展通过利用其出色的常识性推理功能来改变视频质量质量质量质量质量质量质量质量质量质量标志。这一进步在很大程度上是由视觉数据和MLLM语言空间之间的有效对齐驱动的。但是，对于视频质量检查，额外的时空对齐对在跨帧中提取与问题相关的信息提出了一个巨大的挑战。在这项工作中，我们研究了与MLLM集成的各种时间建模技术，旨在实现问题指导的时间建模，以利用MLLM中的预训练的视觉和文本对齐。我们提出了T-Former，这是一种新型的时间建模方法，它在框架视觉感知和LLM的推理能力之间创建了一个问题引导的时间桥。我们对多个视频基准测试的评估表明，T-Former与现有的时间建模方法竞争，并与视频质量检查中的最新进步保持一致。

### SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model 
[[arxiv](https://arxiv.org/abs/2412.19237)] [[cool](https://papers.cool/arxiv/2412.19237)] [[pdf](https://arxiv.org/pdf/2412.19237)]
> **Authors**: Xuyang Li,Danfeng Hong,Chenyu Li,Jocelyn Chanussot
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: Seamo：多季节和多模式遥感基础模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 遥感（RS）数据包含大量对地球观察至关重要的多维信息。由于其庞大的量，不同的来源和时间属性，RS数据非常适合开发大型视觉基础模型（VFM）。 VFM充当强大的功能提取器，从广泛的RS数据中学习，然后进行微调以在各种地球科学任务中部署。但是，RS结构域中的当前VFM主要是针对RS图像的特定特性进行审议和量身定制的，忽略了利用RS数据的多维特性的潜力。因此，在这项工作中，我们提出了Seamo，这是一个开创性的视觉基础模型，该模型在RS字段中集成了多季节和多模式信息。 SEAMO旨在利用RS数据的多个属性。在蒙版的图像建模框架内，我们采用非对准的裁剪技术来提取空间特性，使用多源输入进行多模式集成，并结合了时间型 - 型融合块，以有效地同化多季节数据。 SEAMO明确对RS数据的多维属性进行建模，从而使模型更全面，健壮和通用。我们将Seamo应用于几个下游地球科学任务，这些任务表现出了出色的性能。进行了广泛的消融研究，以验证该模型的优势。

### Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion 
[[arxiv](https://arxiv.org/abs/2412.19225)] [[cool](https://papers.cool/arxiv/2412.19225)] [[pdf](https://arxiv.org/pdf/2412.19225)]
> **Authors**: Zhiqiang Yan,Zhengxue Wang,Kun Wang,Jun Li,Jian Yang
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: CVPR 2025
- **标题**: 完成为增强：降解感知的选择性图像指导网络，用于深度完成
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 在本文中，我们介绍了选择性图像指导网络（Signet），这是一种新型的降级感知框架，首次将深度完成转化为深度增强。使用卷积神经网络（CNN）超越直接完成，最初通过非CNN致密化工具加密稀疏深度数据，以获得粗糙但密集的深度。这种方法消除了直接卷积在不规则采样的稀疏数据上引起的不匹配和歧义。随后，签名集重新定义完成为增强，建立了粗糙深度和靶向密度深度之间的自制降解桥，以有效地进行RGB-D融合。为了实现这一目标，签名集利用隐式降解来自适应地选择RGB数据的高频组件（例如边缘）来补偿粗糙深度。该降解进一步集成到多模式的条件mamba中，动态生成状态参数以实现有效的全局高频信息交互。我们在NYUV2，DIML，SUN RGBD和TOFDC数据集上进行了广泛的实验，证明了Signet的最新性能（SOTA）性能。

### Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for Enhanced Image-Text Matching 
[[arxiv](https://arxiv.org/abs/2412.19184)] [[cool](https://papers.cool/arxiv/2412.19184)] [[pdf](https://arxiv.org/pdf/2412.19184)]
> **Authors**: Wenjing Chen
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 多头注意力驱动的动态视觉语义嵌入，以增强图像文本匹配
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 随着多模式学习的快速发展，作为连接视觉和语言的桥梁的图像文本匹配任务变得越来越重要。基于现有的研究，本研究提出了一种创新的视觉语义嵌入模型，即多头共识感知的视觉 - 语义嵌入（MH-CVSE）。该模型基于共识感知的视觉语义嵌入模型（CVSE）引入了多头自我发项机制，以并行捕获多个子空间中的信息，从而显着增强了模型理解和表示图像和文本之间复杂关系的能力。此外，我们采用参数化的特征融合策略来灵活地集成不同级别的特征信息，从而进一步提高了模型的表现力。在损失功能设计方面，MH-CVSE模型采用动态的重量调整策略，根据损耗值本身动态调整重量，以便在训练过程中更好地平衡不同损失项的贡献。同时，我们引入了余弦退火率策略，以帮助该模型在培训的后期更稳定。在FlickR30K数据集上进行了广泛的实验验证表明，在双向图像和文本检索任务中，MH-CVSE模型的性能比以前的方法更好，这充分证明了其有效性和优势。

### Mask Approximation Net: A Novel Diffusion Model Approach for Remote Sensing Change Captioning 
[[arxiv](https://arxiv.org/abs/2412.19179)] [[cool](https://papers.cool/arxiv/2412.19179)] [[pdf](https://arxiv.org/pdf/2412.19179)]
> **Authors**: Dongwei Sun,Jing Yao,Changsheng Zhou,Xiangyong Cao,Pedram Ghamisi
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 蒙版近似网：一种用于遥感变更字幕的新型扩散模型方法
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 遥感图像更改描述表示遥感处理领域内的创新多模式任务。这项任务不仅促进了表面条件中的变化的检测，而且还提供了这些变化的全面描述，从而改善了人类的可解释性和互动性。从基础上，现有的基于深度学习的方法主要利用了一个三阶段的框架，该框架连续地使用了特征提取，特征融合，特征融合，并从文本生成之前的特征融合和本地化。但是，这种依赖通常会导致对特定网络体系结构的设计过度关注，并将特征分布限制为手头数据集，进而导致应用程序期间的概括性和鲁棒性有限。要解决这些限制，本文提出了一种新的方法，提出了一种远程感应感应的图像变化检测和描述的新方法，该方法将图像变化检测和描述纳入了分布范围的特征，从而使图像进行了分布的特征，该图像是对模型的分布范围进行了划分的范围。所提出的方法主要包括一个简单的多尺度变更检测模块，其输出特征随后通过设计良好的扩散模型来完善。此外，我们引入了频率引导的复杂滤波器模块，以通过在整个扩散过程中管理高频噪声来提高模型性能。我们验证了我们在几个数据集中提出的方法的有效性，以用于遥感变更检测和描述，并与现有技术相比，展示了其出色的性能。该代码将在可能的出版物后，可在\ href {https://github.com/sundongwei} {maskApproxnet}上找到。

### Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval 
[[arxiv](https://arxiv.org/abs/2412.19178)] [[cool](https://papers.cool/arxiv/2412.19178)] [[pdf](https://arxiv.org/pdf/2412.19178)]
> **Authors**: Yang Du,Yuqi Liu,Qin Jin
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: ACMMM 2024 poster
- **标题**: 及时逆转：跨模式视频检索的新颖的时间强调基准
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,信息检索,机器学习
- **摘要**: 跨模式（例如图像文本，视频文本）检索是信息检索和多模式视觉语言理解领域的重要任务。时间理解使视频文本检索比图像文本检索更具挑战性。但是，我们发现，广泛使用的视频文本基准在全面评估模型的能力方面存在缺点，尤其是在时间理解中，导致大规模图像Text预训练的预训练模型已经可以通过视频Text预训练的模型实现可比的零击性能。在本文中，我们介绍了rtime，这是一种新颖的时间强调视频文本检索数据集。我们首先获得具有重要时间性的动作或事件的视频，然后扭转这些视频以创建更难的负面样本。然后，我们招募注释者来判断候选视频的重要性和可逆性，并为合格视频写标题。我们进一步采用GPT-4以扩展基于人工写的标题的更多标题。我们的RTIME数据集目前由21K视频组成，每个视频中有10个字幕，总计约122小时。根据RTIME，我们提出了三个检索基准任务：RTIME-ORIGIN，RTIME-HARD和RTIME-BINARY。我们进一步增强了在模型培训中使用更艰难的阴性的使用，并在RTIME上基准了各种视频文本模型。广泛的实验分析证明，RTime确实对视频检索构成了新的和更高的挑战。我们发布RTIME数据集\ footNote {\ url {https://github.com/qyr0403/reversed-intime}}，以进一步提高视频 - 文本检索和多模式理解研究。

### Referencing Where to Focus: Improving VisualGrounding with Referential Query 
[[arxiv](https://arxiv.org/abs/2412.19155)] [[cool](https://papers.cool/arxiv/2412.19155)] [[pdf](https://arxiv.org/pdf/2412.19155)]
> **Authors**: Yabing Wang,Zhuotao Tian,Qingpei Guo,Zheng Qin,Sanping Zhou,Ming Yang,Le Wang
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: Accepted by NIPS2024
- **标题**: 引用关注点：通过参考查询改善视觉接地
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉接地旨在将引用对象定位在给定自然语言表达式的图像中。基于DITR的视觉接地方法的最新进展引起了人们的关注，因为它们直接预测目标对象的坐标而不依赖其他努力，例如预先生成的建议候选者或预定的锚点盒。但是，现有研究主要集中于设计更强的多模式解码器，该解码器通常通过随机初始化或使用语言嵌入来产生可学习的查询。这种香草查询生成方法不可避免地增加了模型的学习难度，因为它在解码开始时不涉及任何与目标有关的信息。此外，它们仅在查询学习过程中使用最深的图像功能，从而忽略了其他级别的功能的重要性。为了解决这些问题，我们提出了一种新颖的方法，称为Refformer。它由查询自适应模块组成，该模块可以无缝集成到剪辑中，并生成参考查询，以提供解码器的先前上下文以及特定于任务的解码器。通过将参考查询纳入解码器，我们可以有效地减轻解码器的学习难度，并准确地集中在目标对象上。此外，我们提出的查询自适应模块也可以充当适配器，可以保留剪辑中的丰富知识，而无需调整骨干网络的参数。广泛的实验证明了我们提出的方法的有效性和效率，在五个视觉接地基准上表现出色。

### CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2412.19142)] [[cool](https://papers.cool/arxiv/2412.19142)] [[pdf](https://arxiv.org/pdf/2412.19142)]
> **Authors**: Siyu Jiao,Haoye Dong,Yuyang Yin,Zequn Jie,Yinlong Qian,Yao Zhao,Humphrey Shi,Yunchao Wei
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 夹具：统一视力语言代表与3D高斯裂口
- **领域**: 计算机视觉和模式识别
- **摘要**: 3D多模式学习的最新作品取得了显着的进步。但是，通常3D多模型模型只能处理点云。与新兴的3D表示技术相比，3D高斯脱落（3DG），空间稀疏点云无法描绘3D对象的纹理信息，从而导致劣质的重建功能。此限制限制了基于点云的3D多模式表示学习的潜力。在本文中，我们提出了Clip-GS，这是一种以3DG为基础的新型多式模式的学习框架。我们介绍了GS令牌以生成串行的高斯令牌，然后通过使用点云模型的权重进行构成定位的变压器层处理，从而导致3DGS嵌入。夹子GS利用了3DG和夹具的视觉文本嵌入之间的对比损失，我们引入了图像投票损失，以指导梯度优化的方向性和收敛性。此外，我们开发了一种有效的方法来生成3DG，图像和文本的三联体，从而促进了学习统一的多模式表示形式的夹子。利用良好的多模式表示形式，夹子GS在各种3D任务上表现出多功能性，并且优于基于点云的模型，包括多模式检索，零照片和少量分类。

### Semantic Residual for Multimodal Unified Discrete Representation 
[[arxiv](https://arxiv.org/abs/2412.19128)] [[cool](https://papers.cool/arxiv/2412.19128)] [[pdf](https://arxiv.org/pdf/2412.19128)]
> **Authors**: Hai Huang,Shulei Wang,Yan Xia
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: ICASSP 2025 Accepted
- **标题**: 多模式统一离散表示的语义残差
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 多模式统一表示领域的最新研究主要采用代码书作为表示形式，利用矢量量化（VQ）进行量化，但对其他量化表示形式的探索不足。我们的工作探讨了更精确的量化方法，并引入了一个新的框架，语义残留的跨模式信息分解（SRCID），灵感来自残留矢量量化（RVQ）的数值残留概念。 SRCID采用基于语义残差的信息删除多模式数据，以更好地处理不同方式之间固有的差异。我们的方法增强了统一的多模式表示的能力，并在跨模式概括和跨模式零击检索中表现出了出色的性能。它的平均结果显着超过了现有的最新模型，以及基于这些模式的RVQ和有限标量量化（FSQ）的先前尝试。

### Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for Robustness, Generalizability, and Multi-Domain Impact 
[[arxiv](https://arxiv.org/abs/2412.19124)] [[cool](https://papers.cool/arxiv/2412.19124)] [[pdf](https://arxiv.org/pdf/2412.19124)]
> **Authors**: Valay Bundele,Oğuz Ata Çal,Bora Kargi,Karahan Sarıtaş,Kıvanç Tezören,Zohreh Ghaderi,Hendrik Lensch
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 评估医学成像中的自学学习：鲁棒性，概括性和多域影响的基准
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 自我监督学习（SSL）已成为医学成像中有希望的范式，解决了医疗保健环境中标记有限的数据的慢性挑战。尽管SSL显示出令人印象深刻的结果，但医疗领域的现有研究通常受到范围的限制，专注于特定的数据集或模式，或仅评估模型性能的孤立方面。这种分散的评估方法提出了一个重大挑战，因为在关键医疗环境中部署的模型不仅必须达到高精度，而且还表现出各种数据集和不同条件的稳健性能和可推广性。为了解决这一差距，我们对医学领域内的SSL方法进行了全面的评估，特别关注鲁棒性和概括性。使用MEDMNIST数据集收集作为标准化基准，我们评估了11种不同的医疗数据集中的8种主要SSL方法。我们的研究对模型性能进行了深入的分析，并在探索分布（OOD）样本的检测中，同时探讨了各种初始化策略，模型体系结构和多域预训练的效果。我们通过跨数据库评估以及具有不同标签比例的跨域性能（1％，10％和100％）来进一步评估SSL方法的普遍性，以模拟现实世界中的情况，并有限地监督。我们希望这种全面的基准可以帮助从业者和研究人员在将SSL方法应用于医疗应用程序时做出更明智的决定。

### FACEMUG: A Multimodal Generative and Fusion Framework for Local Facial Editing 
[[arxiv](https://arxiv.org/abs/2412.19009)] [[cool](https://papers.cool/arxiv/2412.19009)] [[pdf](https://arxiv.org/pdf/2412.19009)]
> **Authors**: Wanglong Lu,Jikai Wang,Xiaogang Jin,Xianta Jiang,Hanli Zhao
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: Published at IEEE Transactions on Visualization and Computer Graphics; 21 pages, 26 figures
- **标题**: Facemug：用于本地面部编辑的多模式生成和融合框架
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 现有的面部编辑方法取得了显着的效果，但它们通常在支持多模式有条件的本地面部编辑方面缺乏。重要的证据之一是，它们的输出图像质量在经过几次增量编辑的迭代后急剧降低，因为它们不支持本地编辑。在本文中，我们为全球一致的本地面部编辑（Facemug）提供了一种新型的多式联运和融合框架，该框架可以处理广泛的输入方式，并启用细粒度和语义操纵，同时保持未经修改的未经修改的零件。不同的方式，包括草图，语义图，颜色图，示例图像，文本和属性标签，都可以传达各种条件细节，并且它们的合并协同作用可以为编辑过程提供更明确的指导。因此，我们将所有模式集成到统一的生成潜伏空间中，以实现多模式的局部面部编辑。具体而言，提出了一种新型的多模式融合机制，它是通过利用多模式聚合和样式融合块来融合了潜在和特征空间中的多模式的。我们进一步介绍了一种新型的自我监督潜在扭曲算法，以纠正未对齐的面部特征，从而有效地将编辑图像的姿势转移到给定的潜在代码中。我们通过广泛的实验和最新方法（SOTA）方法来评估我们的面孔。结果证明了Facemug在编辑质量，灵活性和语义控制方面的优势，这使其成为各种本地面部编辑任务的有前途的解决方案。

### MiTREE: Multi-input Transformer Ecoregion Encoder for Species Distribution Modelling 
[[arxiv](https://arxiv.org/abs/2412.18995)] [[cool](https://papers.cool/arxiv/2412.18995)] [[pdf](https://arxiv.org/pdf/2412.18995)]
> **Authors**: Theresa Chen,Yao-Yi Chiang
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: 11 pages, GeoAI Workshop and SIGSPATIAL 2024
- **标题**: Mitree：物种分布建模的多输入变压器生态区编码器
- **领域**: 计算机视觉和模式识别,机器学习,定量方法
- **摘要**: 气候变化对生物多样性构成了极端威胁，因此必须有效地对不同物种的地理范围进行建模。大规模遥感图像和环境数据的可用性促进了在物种分布模型（SDMS）中使用机器学习的，旨在预测任何给定位置的物种的存在。依靠专家观察的传统SDM是劳动密集型的，但是遥感和公民科学数据的进步促进了机器学习方法的SDM开发方法。但是，这些模型通常在利用不同输入之间的空间关系（例如，学习气候数据应如何告知卫星图像中存在的数据）之间困难，而不会对原始输入进行夸大或扭曲原始输入。此外，位置的位置信息和生态特征在预测物种分布模型中起着至关重要的作用，但是这些方面尚未纳入最新方法。在这项工作中，我们介绍了Mitree：具有生态区编码器的基于多输入视觉转换器的模型。 Mitree计算空间跨模式关系，而无需进行更大的采样，并整合了位置和生态环境。我们在SATBIRD夏季和冬季数据集上评估了我们的模型，其目的是预测鸟类遇到的率，我们发现我们的方法在最新的基线上有所改善。

### TopoBDA: Towards Bezier Deformable Attention for Road Topology Understanding 
[[arxiv](https://arxiv.org/abs/2412.18951)] [[cool](https://papers.cool/arxiv/2412.18951)] [[pdf](https://arxiv.org/pdf/2412.18951)]
> **Authors**: Muhammet Esat Kalfaoglu,Halil Ibrahim Ozturk,Ozsel Kilinc,Alptekin Temizel
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: Submitted for consideration in the ACM Transactions on Intelligent Systems and Technology (TIST) Special Issue on Transformers
- **标题**: Topobda：朝着贝齐（Bezier）的畸形关注，以了解道路拓扑理解
- **领域**: 计算机视觉和模式识别
- **摘要**: 了解道路拓扑对于自动驾驶至关重要。本文介绍了topobda（拓扑拓扑，并引起了可变形的注意），这是一种新颖的方法，通过利用bezier可变形的注意来增强道路拓扑的理解（BDA）。 BDA利用Bezier控制点来驱动可变形的注意机制，从而显着改善了伸长和薄的多线结构（例如车道中心线）的检测和表示。 TopoBDA处理多相机360度图像以产生鸟类视图（BEV）特征，这些特征是通过使用BDA的变压器解码器来完善的。此方法提高了计算效率，同时保持了中心线预测的高精度。此外，Topobda还结合了实例掩盖配方和辅助一对多的预测损失策略，以进一步完善中心线检测并改善道路拓扑的理解。对OpenLane-V2数据集的实验评估表明，TopoBDA胜过现有方法，实现最新方法会导致中心线检测和拓扑推理。多模式数据（包括LiDAR和雷达）的集成，特别是用于道路拓扑的理解，进一步提高了模型的性能，强调了其在自主驾驶应用中的重要性。

### UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal Transformer for Image Generation 
[[arxiv](https://arxiv.org/abs/2412.18928)] [[cool](https://papers.cool/arxiv/2412.18928)] [[pdf](https://arxiv.org/pdf/2412.18928)]
> **Authors**: Lunhao Duan,Shanshan Zhao,Wenjun Yan,Yinglun Li,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Mingming Gong,Gui-Song Xia
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: Unic-Audapter：带有多模式变压器的统一图像实施适配器用于图像生成
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 最近，文本到图像生成模型取得了显着的进步，尤其是在促进文本描述中促进高质量图像综合的扩散模型的情况下。但是，这些模型通常会在单独使用文本提示时对像素级布局，对象的外观和全局样式进行精确控制。为了减轻此问题，以前的作品将条件图像作为图像生成的辅助输入引入条件图像，增强控制，但通常需要针对不同类型的参考输入量身定制的专业模型。在本文中，我们探讨了一种新方法，以在单个框架内统一可控生成。具体而言，我们提出了建立在多模式 - 扩散变压器体系结构上的统一图像实施适配器（UNIC-AUPAPTER），以在不需要多个专业模型的情况下在不同条件下进行灵活和可控制的生成。我们的UNIC适配器通过合并有条件的图像和任务指令来有效提取多模式指令信息，并通过旋转位置嵌入增强的跨注意机制将此信息注入图像生成过程。各种任务的实验结果，包括像素级空间控制，主题驱动的图像生成和基于样式图像的图像合成，证明了我们Unic-Adapter在统一可控图像生成中的有效性。

### An Attentive Dual-Encoder Framework Leveraging Multimodal Visual and Semantic Information for Automatic OSAHS Diagnosis 
[[arxiv](https://arxiv.org/abs/2412.18919)] [[cool](https://papers.cool/arxiv/2412.18919)] [[pdf](https://arxiv.org/pdf/2412.18919)]
> **Authors**: Yingchen Wei,Xihe Qiu,Xiaoyu Tan,Jingjing Huang,Wei Chu,Yinghui Xu,Yuan Qi
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: 5 pages, 2 figures, Published as a conference paper at ICASSP 2025
- **标题**: 一个细心的双重编码框架利用自动OSAHS诊断的多模式视觉和语义信息
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 阻塞性睡眠呼吸暂停呼吸症综合征（OSAHS）是由上气道阻塞引起的常见睡眠障碍，导致氧气剥夺和睡眠破坏。使用多渗透学（PSG）的传统诊断昂贵，耗时且不舒服。使用面部图像分析的现有深度学习方法由于面部特征捕获不良和样本量有限而缺乏准确性。为了解决这个问题，我们提出了一个多模式双重编码模型，该模型集成了自动化OSAHS诊断的视觉和语言输入。该模型使用Randomovers采样器平衡数据，将关键面部特征与注意力网格提取，并将生理数据转换为有意义的文本。跨注意结合了图像和文本数据，以更好地提取特征，并有序的回归损失确保稳定学习。我们的方法提高了诊断效率和准确性，在四级严重性分类任务中实现了91.3％的TOP-1准确性，证明了最先进的表现。代码将在接受后发布。

### MotionMap: Representing Multimodality in Human Pose Forecasting 
[[arxiv](https://arxiv.org/abs/2412.18883)] [[cool](https://papers.cool/arxiv/2412.18883)] [[pdf](https://arxiv.org/pdf/2412.18883)]
> **Authors**: Reyhaneh Hosseininejad,Megh Shukla,Saeed Saadatnejad,Mathieu Salzmann,Alexandre Alahi
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: TLDR: We propose a new representation for learning multimodality in human pose forecasting which does not depend on generative models
- **标题**: 运动图：代表人姿势预测中的多模式
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 人姿势的预测本质上是多模式的，因为观察到的姿势序列存在多个未来。但是，由于任务不足，评估多模式是具有挑战性的。因此，我们首先提出了一个替代范式，以使任务充分。接下来，尽管最新方法可以预测多模式，但这需要过度采样大量的预测。这提出了关键问题：（1）我们可以通过有效抽样少量预测来捕获多模式吗？ （2）随后，观察到的姿势序列更有可能预测的期货？我们使用Motionmap解决这些问题，这是一种简单而有效的基于热图的多模态表示。我们扩展热图以表示在所有可能的运动空间上的空间分布，其中不同的局部最大值对应于给定观察结果的不同预测。 MotionMap可以捕获每个观测模式数量的可变数量，并为不同模式提供置信度度量。此外，运动图允许我们在预测的姿势序列上引入不确定性和可控性的概念。最后，MotionMap捕获了稀有模式，这些模式是对安全性至关重要的不平凡的模式。我们通过使用流行的3D人类姿势数据集通过多种定性和定量实验来支持我们的主张：Human3.6M和Amass，突出了我们提出的方法的优势和局限性。项目页面：https：//www.epfl.ch/labs/vita/research/prediction/motionmap/

### Hierarchical Banzhaf Interaction for General Video-Language Representation Learning 
[[arxiv](https://arxiv.org/abs/2412.20964)] [[cool](https://papers.cool/arxiv/2412.20964)] [[pdf](https://arxiv.org/pdf/2412.20964)]
> **Authors**: Peng Jin,Hao Li,Li Yuan,Shuicheng Yan,Jie Chen
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). arXiv admin note: substantial text overlap with arXiv:2303.14369
- **标题**: 用于一般视频语言表示学习的层次结构banzhaf互动
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式表示学习，具有对比性学习，在人工智能领域起着重要作用。作为一个重要的子领域，视频语言表示学习重点是使用预定的视频文本对之间的全球语义互动进行学习表示。但是，为了增强和完善这种粗粒的全球相互作用，对于细粒度的多模式学习是必需的。在这项研究中，我们介绍了一种新方法，该方法将视频文本建模为游戏玩家，使用多元合作游戏理论，以在细粒度的语义互动中处理不确定性，并具有多种粒度，柔性组合和模糊的强度。具体而言，我们设计了层次的banzhaf交互，以模拟视频剪辑和文本单词之间的细粒对应关系。此外，为了减轻Banzhaf相互作用中计算的偏差，我们建议通过融合单模式和跨模式成分来重建表示形式。这种重建的表示可确保与单模式表示相当的细粒度，同时还保留了跨模式表示的自适应编码特征。此外，我们将原始结构扩展到灵活的编码器框架中，从而使模型能够适应各种下游任务。对常用的文本视频检索，视频问题的回答以及视频字幕测试的广泛实验，并具有出色的性能，验证了我们方法的有效性和概括。

### Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2412.20927)] [[cool](https://papers.cool/arxiv/2412.20927)] [[pdf](https://arxiv.org/pdf/2412.20927)]
> **Authors**: Junxiao Xue,Quan Deng,Fei Yu,Yanhao Wang,Jun Wang,Yuehua Li
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: 6 pages, 3 figures, under review
- **标题**: 增强的多模式rag-llm，以进行准确的视觉问题回答
- **领域**: 计算机视觉和模式识别
- **摘要**: 多模式的大型语言模型（MLLM），例如GPT-4O，Gemini，Llava和Flamingo，在整合视觉和文本方式上取得了重大进展，在视觉问题答案（VQA），图像字幕上和内容检索等任务中取得了卓越的进步。它们可以生成图像的连贯和上下文相关的描述。但是，他们在准确识别和计算对象并确定其空间位置时仍然面临挑战，尤其是在复杂的场景中，重叠或小物体。为了解决这些局限性，我们提出了一个基于多模式检索增强生成（RAG）的新型框架，该框架引入了结构化场景图，以增强对象识别，关系识别和图像中的空间理解。我们的框架提高了MLLM处理需要精确视觉描述的任务的能力，尤其是在具有挑战性观点的场景中，例如空中视图或具有密集对象布置的场景。最后，我们对VG-150数据集进行了广泛的实验，该实验的重点是第一人称视觉理解和涉及航空影像的AUG数据集。结果表明，我们的方法始终优于VQA任务中现有的MLLM，在识别，本地化和量化不同空间上下文中的对象方面，它在识别，本地化和量化对象方面脱颖而出，并提供了更准确的视觉描述。

### TiGDistill-BEV: Multi-view BEV 3D Object Detection via Target Inner-Geometry Learning Distillation 
[[arxiv](https://arxiv.org/abs/2412.20911)] [[cool](https://papers.cool/arxiv/2412.20911)] [[pdf](https://arxiv.org/pdf/2412.20911)]
> **Authors**: Shaoqing Xu,Fang Li,Peixiang Huang,Ziying Song,Zhi-Xin Yang
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: 13 pages, 8 figures. arXiv admin note: substantial text overlap with arXiv:2212.13979
- **标题**: tigdistill-bev：通过目标内几何学学习蒸馏的多视图BEV BEV BEV 3D对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 准确的多视图3D对象检测对于诸如自动驾驶等应用至关重要。研究人员一直致力于利用激光雷达的精确空间信息通过深度监督和鸟眼视图（BEV）功能蒸馏来增强基于摄像机的探测器。但是，由于LiDar和相机数据表示之间的固有差异，现有方法通常会面临挑战。在本文中，我们介绍了Tigdistill-Bev，这是一种新颖的方法，通过利用这两个传感器的优势来有效地弥合了这一差距。我们的方法将知识从不同的模式（例如LiDAR）作为教师模型提炼成基于摄像机的学生探测器，并利用目标内部几何学学习方案，通过利用多种方式通过深度和BEV特征来增强基于摄像机的BEV探测器。特别是，我们提出了两个关键模块：一个内在的深度监督模块，以了解对象内的低级相对深度关系，该模块使探测器对对象级的空间结构有更深入的了解，以及一个内部功能的BEV蒸馏模块，以传递前景目标内不同关键点的高级语义。为了进一步缓解域间隙，我们将通道间和播客间蒸馏融合到模型相似性。对Nuscenes基准测试的广泛实验表明，Tigdistill-BEV显着增强了基于摄像机的检测器，只能达到具有62.8％NDS的最先进的探测器，并通过显着的差距超过了先前的方法。这些代码可在以下网址提供：https：//github.com/public-bots/tigdistill-bev.git。

### UniRS: Unifying Multi-temporal Remote Sensing Tasks through Vision Language Models 
[[arxiv](https://arxiv.org/abs/2412.20742)] [[cool](https://papers.cool/arxiv/2412.20742)] [[pdf](https://arxiv.org/pdf/2412.20742)]
> **Authors**: Yujie Li,Wenjia Xu,Guangzuo Li,Zijian Yu,Zhiwei Wei,Jiuniu Wang,Mugen Peng
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: 12 pages, 5 figures
- **标题**: Unirs：通过视觉语言模型统一多个时间远程感应任务
- **领域**: 计算机视觉和模式识别
- **摘要**: 遥感图像和自然图像之间的域间隙最近受到了广泛的关注和视觉语言模型（VLM），在遥感多模式任务中表现出了出色的概括性能。但是，当前的研究仍在探索遥感VLM如何处理不同类型的视觉输入方面仍受到限制。为了弥合这一差距，我们介绍了\ textbf {unirs}，这是第一个视觉模型\ textbf {uni} fyled-temporal \ textbf {r} emote \ textbf {s}在各种视觉输入中随之而来的任务。 Unirs支持单个图像，双重图像对和视频作为输入，从而在统一的框架内实现了全面的遥感时间分析。我们采用统一的视觉表示方法，使模型能够接受各种视觉输入。对于双重图像对任务，我们自定义一个更改提取模块，以进一步增强时空特征的提取。此外，我们设计了针对模型推理过程量身定制的迅速增强机制，利用通用VLM的先验知识为Unir提供线索。为了促进多任务知识共享，该模型在混合数据集上进行了微调。实验结果表明，Unirs在各种任务中实现了最先进的表现，包括视觉问题回答，更改字幕和视频场景分类，突出了其在统一这些多个暂时的遥感任务方面的多功能性和有效性。我们的代码和数据集将很快发布。

### Dialogue Director: Bridging the Gap in Dialogue Visualization for Multimodal Storytelling 
[[arxiv](https://arxiv.org/abs/2412.20725)] [[cool](https://papers.cool/arxiv/2412.20725)] [[pdf](https://arxiv.org/pdf/2412.20725)]
> **Authors**: Min Zhang,Zilin Wang,Liyan Chen,Kunhong Liu,Juncong Lin
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: No comments
- **标题**: 对话总监：在对话可视化中弥合多模式讲故事的差距
- **领域**: 计算机视觉和模式识别
- **摘要**: AI驱动的讲故事的最新进展增强了视频生成和故事的可视化。但是，将以对话为中心的脚本转换为连贯的故事板，由于脚本细节有限，物理上下文理解不足以及整合电影原理的复杂性，这仍然是一个重大挑战。为了应对这些挑战，我们提出了对话可视化，这是一项新的任务，将对话脚本转换为动态的多视图情节板。我们介绍了对话总监，这是一个无训练的多模式框架，其中包括剧本导演，摄影师和情节板制造商。该框架利用大型的多模型模型和基于扩散的架构，采用了诸如思想链推理，检索型生成和多视图综合等技术，以提高脚本理解，物理上下文理解和电影知识整合。实验结果表明，对话总监在脚本解释，物理世界理解和电影原则应用中的表现优于最先进的方法，从而显着提高了基于对话的故事可视化的质量和可控性。

### M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs 
[[arxiv](https://arxiv.org/abs/2412.20718)] [[cool](https://papers.cool/arxiv/2412.20718)] [[pdf](https://arxiv.org/pdf/2412.20718)]
> **Authors**: Bei Yan,Jie Zhang,Zhiyuan Chen,Shiguang Shan,Xilin Chen
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: No comments
- **标题**: m $^3 $ oralbench：LVLMS的多模式道德基准
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 最近，包括大型语言模型（LLM）和大型视觉语言模型（LVLM）在内的大型基础模型已成为法律，金融和医疗保健等关键领域的重要工具。随着这些模型越来越多地整合到我们的日常生活中，有必要进行道德评估，以确保其产量与人类价值观保持一致并保持道德界限。先前的工作主要集中在LLM上，提出道德数据集和基准限于文本模式。但是，鉴于LVLM的快速发展，仍然缺乏多模式的道德评估方法。为了弥合这一差距，我们介绍了M $^3 $ ORALBENCH，这是LVLMS的第一个多模式基准。 M $^3 $ oralbench在道德基础（MFV）中扩展了日常道德场景，并采用文本到图像扩散模型SD3.0，以创建相应的方案图像。它在道德基础理论（MFT）的六个道德基础上进行道德评估，并涵盖了道德判断，道德分类和道德反应中的任务，从而在多模式道德理解和推理中对模型绩效进行了全面评估。对10个受欢迎的开源和封闭源LVLMS进行的广泛实验表明，M $^3 $ oralbench是一个具有挑战性的基准，在当前模型中暴露了显着的道德限制。我们的基准可公开可用。

## 计算机与社会(cs.CY:Computers and Society)

该领域共有 3 篇论文

### Examining Multimodal Gender and Content Bias in ChatGPT-4o 
[[arxiv](https://arxiv.org/abs/2411.19140)] [[cool](https://papers.cool/arxiv/2411.19140)] [[pdf](https://arxiv.org/pdf/2411.19140)]
> **Authors**: Roberto Balestri
> **First submission**: 2024-11-28
> **First announcement**: 2024-12-02
> **comment**: 17 pages, 4 figures, 3 tables. Conference: "14th International Conference on Artificial Intelligence, Soft Computing and Applications (AIAA 2024), London, 23-24 November 2024" It will be published in the proceedings "David C. Wyld et al. (Eds): IoTE, CNDC, DSA, AIAA, NLPTA, DPPR - 2024"
- **标题**: 检查Chatgpt-4O中的多模式性别和内容偏见
- **领域**: 计算机与社会,人工智能,计算语言学,人机交互,其他统计数据
- **摘要**: 这项研究调查了Chatgpt-4O的多模式含量产生，强调了其性内容和裸体与暴力和与药物有关的主题的重大差异。详细的分析表明，Chatgpt-4O始终审查性内容和裸露，同时表现出对暴力和吸毒的宽大处理。此外，与男性特异性含量相比，出现明显的性别偏见，女性特异性内容面临更严格的调节。这种差距可能源于对过去AI争议的媒体审查和公众反对，从而促使科技公司对敏感问题施加了严格的指导，以保护其声誉。我们的发现强调了AI系统迫切需要维护真正的道德标准和问责制，超越政治正确性。这项研究有助于理解AI驱动的语言和多模型模型，呼吁进行更平衡和道德的内容审核。

### Creating a Cooperative AI Policymaking Platform through Open Source Collaboration 
[[arxiv](https://arxiv.org/abs/2412.06936)] [[cool](https://papers.cool/arxiv/2412.06936)] [[pdf](https://arxiv.org/pdf/2412.06936)]
> **Authors**: Aiden Lewington,Alekhya Vittalam,Anshumaan Singh,Anuja Uppuluri,Arjun Ashok,Ashrith Mandayam Athmaram,Austin Milt,Benjamin Smith,Charlie Weinberger,Chatanya Sarin,Christoph Bergmeir,Cliff Chang,Daivik Patel,Daniel Li,David Bell,Defu Cao,Donghwa Shin,Edward Kang,Edwin Zhang,Enhui Li,Felix Chen,Gabe Smithline,Haipeng Chen,Henry Gasztowtt,Hoon Shin, et al. (26 additional authors not shown)
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 通过开源协作创建合作的AI政策制定平台
- **领域**: 计算机与社会,人工智能,机器学习
- **摘要**: 人工智能的进步（AI）提出了重大风险和机会，需要改善治理以减轻社会危害并促进公平的利益。当前的激励结构和监管延迟可能会阻碍负责的AI发展和部署，特别是鉴于大语言模型（LLMS）的变革潜力。 To address these challenges, we propose developing the following three contributions: (1) a large multimodal text and economic-timeseries foundation model that integrates economic and natural language policy data for enhanced forecasting and decision-making, (2) algorithmic mechanisms for eliciting diverse and representative perspectives, enabling the creation of data-driven public policy recommendations, and (3) an AI-driven web platform for supporting transparent, inclusive, and数据驱动的决策。

### Human-Centric NLP or AI-Centric Illusion?: A Critical Investigation 
[[arxiv](https://arxiv.org/abs/2412.10939)] [[cool](https://papers.cool/arxiv/2412.10939)] [[pdf](https://arxiv.org/pdf/2412.10939)]
> **Authors**: Piyapath T Spencer
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: Preprint to be published in Proceedings of PACLIC38
- **标题**: 以人为中心的NLP或以AI为中心的幻觉？：一项批判性调查
- **领域**: 计算机与社会,人工智能,计算语言学,人机交互
- **摘要**: 以人为中心的NLP通常声称优先考虑人类的需求和价值观，但许多实施揭示了基本的以AI为中心的重点。通过分析语言建模，行为测试和多模式一致性方面的案例研究，本研究确定了以人为中心的思想和实际实践的思想之间的显着差距。关键问题包括与以人为中心的设计原则的未对准，减少人为因素仅基准，以及对现实世界影响的不足。讨论探讨了以人为中心的NLP是否体现了真正的人为以人为中心的设计，并强调了对跨学科合作和道德考虑的需求。本文倡导重新定义以人为中心的NLP，敦促更广泛地关注现实世界实用程序和社会含义，以确保语言技术真正地服务和赋予用户。

## 数字图书馆(cs.DL:Digital Libraries)

该领域共有 1 篇论文

### NLLG Quarterly arXiv Report 09/24: What are the most influential current AI Papers? 
[[arxiv](https://arxiv.org/abs/2412.12121)] [[cool](https://papers.cool/arxiv/2412.12121)] [[pdf](https://arxiv.org/pdf/2412.12121)]
> **Authors**: Christoph Leiter,Jonas Belouadi,Yanran Chen,Ran Zhang,Daniil Larionov,Aida Kostikova,Steffen Eger
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: NLLG季度ARXIV报告09/24：目前最有影响力的AI论文是什么？
- **领域**: 数字图书馆,人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: NLLG（自然语言学习与生成）ARXIV报告有助于跨CS.CL，CS.C.CV，CS.AI和CS.LG类别浏览NLP和AI研究的快速发展。这本第四期捕捉了AI历史上的变革时期 - 从2023年1月1日起，在Chatgpt的首次亮相到2024年9月30日之后。我们的分析揭示了该领域的实质新事态发展 - 自上次报告以来，最引人注目的论文中有45％是八个月以前的新参赛作品中的45％，并提供了对新兴趋势和专业群体的见解，例如不同的型号，包括不同的型号，包括不同的型号，包括不同的群组和群组的建筑群体，包括不同的型号。自然语言处理（NLP; CS.CL）仍然是我们前40篇论文列表中的主要主要类别，但其主导地位在下降，支持计算机视觉（CS.CV）和通用机器学习（CS.LG）。该报告还介绍了有关生成AI在学术写作中整合的新发现，并记录了自2022年以来的采用量的增加，同时揭示了一种有趣的模式：与随机样本相比，顶级论文显示AI生成的内容的标记较少。此外，我们跟踪AI相关语言的演变，并确定以前常见指标（例如“ Delve”）的趋势下降。

## 图形(cs.GR:Graphics)

该领域共有 1 篇论文

### Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis 
[[arxiv](https://arxiv.org/abs/2412.08603)] [[cool](https://papers.cool/arxiv/2412.08603)] [[pdf](https://arxiv.org/pdf/2412.08603)]
> **Authors**: Feng Zhou,Ruiyang Liu,Chen Liu,Gaofeng He,Yong-Lu Li,Xiaogang Jin,Huamin Wang
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: Design2 GarmentCode：通过程序合成将设计概念转换为有形服装
- **领域**: 图形,计算机视觉和模式识别
- **摘要**: 缝纫图案，用于切割和剪裁的必需蓝图，是设计概念和可生产服装之间的关键桥梁。但是，现有的单模式缝纫模式生成模型努力用多模式性质进行有效编码复杂的设计概念，并将它们与具有精确的几何结构和复杂的缝纫关系的矢量化缝纫模式相关联。在这项工作中，我们提出了一种基于大型多模型（LMM）的新型缝纫模式生成方法Design2 Gartermentscode，以从多模式设计概念生成参数模式制定程序。 LMM提供了一种直观的界面，用于解释各种设计输入，而图案制定程序可以用作缝纫模式的结构和语义上有意义的表示，并充当连接跨域模式知识的强大桥梁，这些桥梁嵌入了LMMS中，该知识与矢量化的缝纫模式相连。实验结果表明，我们的方法可以灵活地处理各种复杂的设计表达式，例如图像，文本描述，设计师草图或其组合，并使用正确的针迹将它们转换为尺寸优雅的缝纫模式。与以前的方法相比，我们的方法显着提高了训练效率，发电质量和创作灵活性。我们的代码和数据将公开可用。

## 人机交互(cs.HC:Human-Computer Interaction)

该领域共有 7 篇论文

### ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by Multimodal Gemini LLM 
[[arxiv](https://arxiv.org/abs/2412.00627)] [[cool](https://papers.cool/arxiv/2412.00627)] [[pdf](https://arxiv.org/pdf/2412.00627)]
> **Authors**: Rithik Vir,Parsa Madinei
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **摘要**: 烹饪餐可能很困难，导致许多人求助于食谱和在线食谱。但是，依靠这些传统的烹饪方法通常会导致食材，营养危害和不令人满意的饭菜。使用增强现实（AR）可以解决这些问题；但是，当前的AR烹饪应用程序的用户界面差和有限的可访问性。本文提出了将AR和计算机视觉（CV）集成到烹饪过程中的iOS应用程序的原型。我们利用Google的双子座大语言模型（LLM）来识别相机视野中的成分，并通过详细的营养信息生成食谱选择。此外，此应用程序使用Apple的ARKIT来创建与iOS设备兼容的AR用户界面。用户可以通过输入饮食偏好并对每顿饭进行评分来个性化他们的进餐建议。该应用程序的有效性通过三轮用户体验调查进行了评估。该应用程序推动了无障碍烹饪援助技术的领域，旨在减少食物浪费并改善进餐计划的经验。

### Effect of Adaptive Communication Support on LLM-powered Human-Robot Collaboration 
[[arxiv](https://arxiv.org/abs/2412.06808)] [[cool](https://papers.cool/arxiv/2412.06808)] [[pdf](https://arxiv.org/pdf/2412.06808)]
> **Authors**: Shipeng Liu,FNU Shrutika,Boshen Zhang,Zhehui Huang,Gaurav Sukhatme,Feifei Qian
> **First submission**: 2024-11-25
> **First announcement**: 2024-12-10
> **comment**: 13 pages, 7 figures
- **标题**: 自适应沟通支持对LLM驱动的人机合作的影响
- **领域**: 人机交互,人工智能,机器人技术
- **摘要**: 有效的人类机器人协作要求机器人根据人类需求，任务要求和复杂性采用其角色和支持水平。传统的人类机器人组合通常依赖于预定的机器人通信方案，从而限制了复杂任务中的团队工作适应性。利用大语言模型（LLMS）的强大沟通能力，我们提出了具有多模式语言反馈（HRT-ML）的人类机器人组合框架，该框架旨在通过调整基于语言的反馈的频率和内容来增强人类机器人的相互作用。 HRT-ML框架包括两个核心模块：高级，低频战略指导的协调员，以及子任务特定，高频指令的经理，使人与人类队友实现被动和活跃的互动。为了评估语言反馈在协作场景中的影响，我们在增强的过度煮熟的环境中进行了实验，其任务复杂性水平不同（简单，中等，硬）和反馈频率（不活跃，被动，主动，超级活性）。我们的结果表明，随着任务复杂性相对于人类能力的增加，人类队友对机器人的偏好表现出更强的偏好，可以提供频繁的，主动的支持。但是，当任务复杂性超过LLM的能力时，超级活跃机器人代理商的嘈杂和不准确的反馈可能会阻碍团队的绩效，因为这要求人类队友增加努力来解释和响应大量沟通，并且绩效收益有限。我们的结果为机器人代理人提供了一个一般原则，可以动态调整其通信水平和频率，以与人类无缝合作并实现提高的团队绩效。

### IMPROVE: Impact of Mobile Phones on Remote Online Virtual Education 
[[arxiv](https://arxiv.org/abs/2412.14195)] [[cool](https://papers.cool/arxiv/2412.14195)] [[pdf](https://arxiv.org/pdf/2412.14195)]
> **Authors**: Roberto Daza,Alvaro Becerra,Ruth Cobos,Julian Fierrez,Aythami Morales
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-19
> **comment**: Article under review in the journal Scientific Data. GitHub repository of the dataset at: https://github.com/BiDAlab/IMPROVE
- **标题**: 改进：手机对远程在线虚拟教育的影响
- **领域**: 人机交互,计算机视觉和模式识别
- **摘要**: 这项工作介绍了改进的数据集，旨在评估手机使用对在线教育期间学习者的影响。该数据集不仅评估了学习成绩和主观学习者的反馈，而且还捕获了生物识别，行为和生理信号，从而对手机使用对学习的影响进行了全面的分析。从120名学习者分别从120个学习者中收集了多模式数据，这些学习者具有不同的电话交互水平。实施了涉及16个传感器的设置以收集已证明是了解学习者行为和认知的有效指标的数据，包括脑电图波，视频，眼球跟踪器等。数据集包括来自处理视频的元数据，例如面部界限框，面部标志物，面部标志和欧拉孔的角度进行估计。此外，还包括学习者绩效数据和自我报告的表格。手机使用事件被标记为标签，涵盖了主管触发和不受控制的事件。提出了使用头姿势和眼动数据数据的半人工重新标记系统，以提高标记精度。技术验证确认了信号质量，并进行了统计分析，揭示了电话使用过程中的生物特征变化。

### Modular Conversational Agents for Surveys and Interviews 
[[arxiv](https://arxiv.org/abs/2412.17049)] [[cool](https://papers.cool/arxiv/2412.17049)] [[pdf](https://arxiv.org/pdf/2412.17049)]
> **Authors**: Jiangbo Yu,Jinhua Zhao,Luis Miranda-Moreno,Matthew Korp
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 调查和访谈的模块化对话代理
- **领域**: 人机交互,计算语言学,计算机与社会,多媒体
- **摘要**: 调查和访谈被广泛用于收集有关新兴或假设情景的见解。传统的人为主导的方法通常面临与成本，可扩展性和一致性有关的挑战。最近，各种领域已经开始探索由生成人工智能（AI）技术提供动力的对话代理（聊天机器人）的使用。但是，考虑到运输投资和政策的决策通常会带来大量的公共和环境风险，调查和访谈在整合AI代理方面面临着独特的挑战，强调了对严格，资源有效的方法的需求，以增强参与者的参与并确保隐私。本文通过引入模块化方法及其设计AI代理的参数化过程来解决这一差距。我们详细介绍了系统体系结构，整合工程的提示，专业知识库以及可定制的，面向目标的对话逻辑。我们通过三项实证研究证明了模块化方法的适应性，可推广性和功效：（1）旅行偏好调查，突出有条件的逻辑和多模式（语音，文本和图像生成）功能； （2）对新建造的新型基础设施项目的舆论启发，展示问题定制和多语言（英语和法语）功能； （3）有关技术对未来运输系统的影响的专家咨询，突出了针对开放式问题的实时，澄清请求功能，处理不稳定的输入方面的弹性以及有效的成绩单后处理。结果表明，AI代理会提高完成率和响应质量。此外，模块化方法表现出可控性，灵活性和鲁棒性，同时解决了关键的道德，隐私，安全性和令牌消费问题。

### Aria-UI: Visual Grounding for GUI Instructions 
[[arxiv](https://arxiv.org/abs/2412.16256)] [[cool](https://papers.cool/arxiv/2412.16256)] [[pdf](https://arxiv.org/pdf/2412.16256)]
> **Authors**: Yuhao Yang,Yue Wang,Dongxu Li,Ziyang Luo,Bei Chen,Chao Huang,Junnan Li
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: ARIA-UI：GUI指示的视觉接地
- **领域**: 人机交互,人工智能
- **摘要**: 通过直接操纵GUI的数字代理来自动化不同平台的任务，越来越重要。对于这些代理人，由于依赖HTML或Axtree输入，从语言指令到目标元素的基础仍然是一个重大挑战。在本文中，我们介绍了Aria-UI，这是一种专门为GUI接地设计的大型多模式。 Aria-UI采用了一种纯粹的方法，避免了对辅助输入的依赖。为了适应异质计划说明，我们提出了一条可扩展的数据管道，该管道合成了多样化和高质量的指令样本以进行接地。为了处理任务执行中的动态环境，Aria-UI结合了文本图像和文本图像交织的动作历史，从而实现了稳健的上下文感知的理由。 ARIA-UI在离线和在线代理基准中设定了新的最先进的结果，表现优于仅视力和轴心依赖的基线。我们发布了所有培训数据和模型检查点，以在https://ariaui.github.io上促进进一步的研究。

### AI-in-the-loop: The future of biomedical visual analytics applications in the era of AI 
[[arxiv](https://arxiv.org/abs/2412.15876)] [[cool](https://papers.cool/arxiv/2412.15876)] [[pdf](https://arxiv.org/pdf/2412.15876)]
> **Authors**: Katja Bühler,Thomas Höllt,Thomas Schulz,Pere-Pau Vázquez
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: Accepted for publication in IEEE Computer Graphics & Applications
- **标题**: AI-IN-IN-IN-IN-IN-IN-IN-IN-IN-IN-IDETICAL视觉分析应用程序的未来
- **领域**: 人机交互,人工智能,图形
- **摘要**: AI是现代数据分析的主力，并且在许多领域无处不在。如今，大型语言模型和多模式基础模型能够生成代码，图表，可视化等。数据分析中AI的这些大规模发展将如何塑造未来的数据可视化和视觉分析工作流程？ AI重塑未来视觉分析应用的方法和设计的潜力是什么？我们将来作为可视化研究人员的角色将是什么？在越来越强大的人工智能的背景下，哪些机遇，公开挑战和威胁是什么？这种可视化观点在生物医学数据分析的特殊背景下讨论了这些问题，作为一个域的一个例子，在该领域中，基于复杂和敏感的数据采取关键决策，对透明度，效率和可靠性的要求很高。我们将AI中的最新趋势和发展绘制在交互式可视化和视觉分析工作流程的元素上，并突出了AI转化生物医学可视化作为研究领域的潜力。 Given that agency and responsibility have to remain with human experts, we argue that it is helpful to keep the focus on human-centered workflows, and to use visual analytics as a tool for integrating ``AI-in-the-loop''.这与更传统的术语``人类''''相反，后者着重于将人类专业知识纳入基于AI的系统。

### A Multimodal Emotion Recognition System: Integrating Facial Expressions, Body Movement, Speech, and Spoken Language 
[[arxiv](https://arxiv.org/abs/2412.17907)] [[cool](https://papers.cool/arxiv/2412.17907)] [[pdf](https://arxiv.org/pdf/2412.17907)]
> **Authors**: Kris Kraack
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: 10 pages, 6 figures, 3 tables
- **标题**: 多模式情绪识别系统：整合面部表情，身体运动，语音和口语
- **领域**: 人机交互,计算语言学,计算机视觉和模式识别,机器学习,多媒体,声音,音频和语音处理
- **摘要**: 传统的心理评估在很大程度上取决于人类的观察和解释，这些观察和解释容易出现主观性，偏见，疲劳和不一致。为了解决这些局限性，这项工作提出了一种多模式情绪识别系统，该系统提供了一种标准化，客观和数据驱动的工具来支持评估者，例如心理学家，精神科医生和临床医生。该系统集成了对面部表情，言语，口语和身体运动分析的认识，以捕获人类评估中经常被忽略的微妙情感线索。通过结合这些方式，该系统提供了更强大，更全面的情绪状态评估，从而降低了错误和过度诊断的风险。在模拟现实世界中的初步测试证明了该系统提供可靠的情感见解以提高诊断准确性的潜力。这项工作强调了自动多模式分析的希望，是对传统心理评估实践的宝贵补充，并在临床和治疗环境中应用。

## 信息检索(cs.IR:Information Retrieval)

该领域共有 13 篇论文

### Needle: A Generative-AI Powered Monte Carlo Method for Answering Complex Natural Language Queries on Multi-modal Data 
[[arxiv](https://arxiv.org/abs/2412.00639)] [[cool](https://papers.cool/arxiv/2412.00639)] [[pdf](https://arxiv.org/pdf/2412.00639)]
> **Authors**: Mahdi Erfanian,Mohsen Dehghankar,Abolfazl Asudeh
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 针：一种生成型动力的蒙特卡洛方法，用于在多模式数据上回答复杂的自然语言查询
- **领域**: 信息检索,数据库
- **摘要**: 多模式数据（例如图像数据集）通常会错过正确捕获其中编码的丰富信息的详细说明。这使得回答复杂的自然语言查询在这些领域成为主要挑战。特别是，与传统的最近邻居搜索不同，该搜索和查询是在数据立方体中建模为点的点，查询和元组具有不同的本质，使得传统的查询答案解决方案不直接适用于此类设置。现有文献通过对矢量表示，共同培训了自然语言和图像的矢量表示。但是，由于各种原因，该技术在复杂查询方面表现不佳。本文通过引入生成型（Genai）动力的蒙特卡洛方法来解决这一挑战，该方法利用基础模型生成合成样本，以捕获自然语言查询的复杂性并将其转换为多模式数据的相同空间。遵循此方法，我们开发了一个用于图像数据检索的系统，并提出了实用解决方案，以利用Genai和向量表示的未来进步来提高系统的性能。我们在各种基准数据集上进行的全面实验验证了我们的系统的表现明显优于最先进的技术。

### CADMR: Cross-Attention and Disentangled Learning for Multimodal Recommender Systems 
[[arxiv](https://arxiv.org/abs/2412.02295)] [[cool](https://papers.cool/arxiv/2412.02295)] [[pdf](https://arxiv.org/pdf/2412.02295)]
> **Authors**: Yasser Khalafaoui,Martino Lovisetto,Basarab Matei,Nistor Grozavu
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: CADMR：多模式推荐系统的跨注意和解开学习
- **领域**: 信息检索,人工智能,机器学习
- **摘要**: 推荐系统中多模式数据的可用性和多样性的增加提供了新的途径，以提高建议准确性和用户满意度。但是，这些系统必须与高维，稀疏的用户项目评分矩阵抗衡，在该矩阵中，每种用户的首选项目只有少量的首选项目重建矩阵会带来重大挑战。为了解决这个问题，我们提出了CADMR，这是一种新型的基于自动编码器的多模式推荐系统框架。 CADMR利用多头跨注意机制和分解学习，以有效地整合并利用异质的多模式数据重建评级矩阵。我们的方法在保留其相互依存的同时，首先将特定于模式的特征删除，从而学习了共同的潜在代表。然后，使用多头跨注意机制来增强用户 - 项目相互作用表示相对于所学的多模式潜在表示。我们在三个基准数据集上评估了CADMR，这表明对最新方法的性能得到了重大改进。

### Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.04107)] [[cool](https://papers.cool/arxiv/2412.04107)] [[pdf](https://arxiv.org/pdf/2412.04107)]
> **Authors**: Yuhao Wang,Junwei Pan,Xiangyu Zhao,Pengyue Jia,Wanyu Wang,Yuan Wang,Yue Liu,Dapeng Liu,Jie Jiang
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 预训练，对齐和删除：使用大语言模型授权顺序推荐
- **领域**: 信息检索,人工智能
- **摘要**: 顺序推荐（SR）旨在对用户历史互动中的顺序依赖性建模，以更好地捕获其不断发展的兴趣。但是，现有的SR方法主要依赖于协作数据，这导致了诸如寒冷启动问题和次优绩效之类的局限性。同时，尽管大语言模型（LLMS）取得了成功，但它们在工业推荐系统中的应用仍受到高推断潜伏期的阻碍，无法捕获所有分销统计以及灾难性的遗忘。为此，我们提出了一种新颖的预训练，对齐和脱离（PAD）范式，以通过LLMS赋予推荐模型的能力。具体而言，我们首先预先培训SR和LLM模型以获取协作和文本嵌入。接下来，使用多内核最大平均差异与高斯内核提出了特征性建议锚定的对齐损失。最后，以频率感知的方式对三重专家的体系结构进行了细微调整。在三个公共数据集上进行的实验证明了PAD的有效性，显示出与各种SR主链模型（尤其是在冷材中）的显着改进和兼容性。实现代码和数据集将公开可用。

### Multimodal Difference Learning for Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2412.08103)] [[cool](https://papers.cool/arxiv/2412.08103)] [[pdf](https://arxiv.org/pdf/2412.08103)]
> **Authors**: Changhong Li,Zhiqiang Guo
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 顺序推荐的多模式差异学习
- **领域**: 信息检索
- **摘要**: 顺序建议在建模用户的历史行为以预测下一项方面引起了重大关注。随着互联网平台上多模式数据（例如图像，文本）的蓬勃发展，顺序推荐也受益于多模式数据的合并。大多数方法将项目的模态特征作为附带信息，并简单地将它们串联以学习统一的用户兴趣。然而，这些方法在建模多模式差异时遇到了限制。我们认为用户兴趣和项目关系在不同的方式上有所不同。为了解决这个问题，我们提出了一个新型的多模式差异学习框架，用于顺序推荐，MDSREC的简洁性。具体来说，我们首先通过使用行为信号构造模态感知的项目关系图来探讨项目关系的差异，以增强项目表示。然后，为了捕获跨模式的用户兴趣的差异，我们设计了一种兴趣的关注机制，以独立建模不同方式的用户序列表示。最后，我们将用户嵌入从多种方式中融合在一起，以获得准确的项目建议。五个现实世界数据集的实验结果证明了MDSREC优于最先进的基准以及多模式差学习的功效。

### Sentiment and Hashtag-aware Attentive Deep Neural Network for Multimodal Post Popularity Prediction 
[[arxiv](https://arxiv.org/abs/2412.10737)] [[cool](https://papers.cool/arxiv/2412.10737)] [[pdf](https://arxiv.org/pdf/2412.10737)]
> **Authors**: Shubhi Bansal,Mohit Kumar,Chandravardhan Singh Raghaw,Nagendra Kumar
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 情感和标签意识到的专注于多模式的知识预测的深度神经网络
- **领域**: 信息检索,社交和信息网络
- **摘要**: 社交媒体用户阐明了他们对广泛主题的看法，并通过包括多种表达方式的帖子分享他们的经验，从而导致社交媒体平台上这种多模式内容的显着激增。尽管如此，准确地预测这些帖子的受欢迎程度带来了巨大的挑战。流行的方法论主要集中在内容本身上，从而忽略了封装在替代模式中的信息，例如视觉人口统计学，通过主题标签传达的情感，并充分建模了主题标签，文本和随附图像之间复杂的关系。这种疏忽限制了捕获情感联系和受众相关性的能力，从而极大地影响了人们的流行。 To address these limitations, we propose a seNtiment and hAshtag-aware attentive deep neuRal netwoRk for multimodAl posT pOpularity pRediction, herein referred to as NARRATOR that extracts visual demographics from faces appearing in images and discerns sentiment from hashtag usage, providing a more comprehensive understanding of the factors influencing post popularity Moreover, we introduce a hashtag-guided attention mechanism that leverages hashtags as导航提示指导模型专注于文本和视觉方式最相关的特征，从而与目标受众兴趣和更广泛的社交媒体环境保持一致。实验结果表明，叙述者在两个现实世界数据集上的大幅度优于现有方法。此外，消融研究强调了整合视觉人口统计学，主题标签的情感分析以及主题标签指导的注意机制在增强普及后预测的表现方面的功效，从而促进受众群体的相关性，情感上的参与度，情感上的参与度和美观的吸引力。

### Beyond Graph Convolution: Multimodal Recommendation with Topology-aware MLPs 
[[arxiv](https://arxiv.org/abs/2412.11747)] [[cool](https://papers.cool/arxiv/2412.11747)] [[pdf](https://arxiv.org/pdf/2412.11747)]
> **Authors**: Junjie Huang,Jiarui Qin,Yong Yu,Weinan Zhang
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: AAAI 2025. 11 pages, 9 figures
- **标题**: 超越图形卷积：具有拓扑感知MLP的多模式建议
- **领域**: 信息检索
- **摘要**: 鉴于来自不同模式的大量侧面信息，多模式推荐系统变得越来越重要，因为它们利用了除用户项目相互作用以外的更丰富的语义信息。最近的著作强调，利用图形卷积网络（GCN）明确模拟多模式项目 - 项目 - 项目 - 项目 - 项目 - 项目 - 项目 - 项目 - 项目 - 项目关系可以显着提高建议性能。但是，由于GCN固有的过度光滑问题，现有模型仅受益于具有有限代表权的浅GCN。当面对复杂和高维模式（例如多模式数据）时，这种缺点尤其明显，因为它需要大容量模型来适应复杂的相关性。为此，在本文中，我们调查在建模多模式项目关系时绕过GCN。更具体地说，我们建议使用MLP而不是GCN来建模项目之间的关系，提出了一种拓扑感知的多层感知器（TMLP）。 TMLP通过拓扑修剪来增强MLP，以降级项目项目关系和内部 - 模式学习以整合高阶模式相关性。在三个现实世界数据集上进行了广泛的实验，验证了TMLP的优势比九个基础线的优势。我们还发现，通过丢弃GCN中传递的内部消息，这对节点连接敏感，TMLP可以在训练效率和鲁棒性针对现有模型方面取得显着提高。

### STAIR: Manipulating Collaborative and Multimodal Information for E-Commerce Recommendation 
[[arxiv](https://arxiv.org/abs/2412.11729)] [[cool](https://papers.cool/arxiv/2412.11729)] [[pdf](https://arxiv.org/pdf/2412.11729)]
> **Authors**: Cong Xu,Yunhang He,Jun Wang,Wei Zhang
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted at AAAI 2025
- **标题**: 楼梯：操纵协作和多模式信息以供电子商务推荐
- **领域**: 信息检索
- **摘要**: 虽然模式的挖掘是大多数多模式推荐方法的重点，但我们认为，如何完全利用协作和多模式信息在电子商务方案中至关重要，正如这项工作中所阐明的那样，用户行为很少完全由多模态特征确定。为了结合两种不同类型的信息，遇到了一些其他挑战：1）模态擦除：Vanilla图卷积，事实证明在协作过滤方面相当有用，但是擦除了多模式信息； 2）模式忘记：随着推荐损失本质上有助于学习协作信息，多模式信息往往会逐渐被遗忘。为此，我们提出了一种名为STAIR的新颖方法，该方法采用了新颖的逐步图卷积，以在电子商务建议中共存合作和多模式信息。此外，它从原始的多模式作为初始化开始，并且可以通过受约束的嵌入更新来大大减轻遗忘问题。结果，楼梯在三个公共电子商务数据集上实现了最先进的建议性能，其计算和内存成本最低。我们的代码可在https://github.com/yhhe2004/stair上找到。

### Enhancing Healthcare Recommendation Systems with a Multimodal LLMs-based MOE Architecture 
[[arxiv](https://arxiv.org/abs/2412.11557)] [[cool](https://papers.cool/arxiv/2412.11557)] [[pdf](https://arxiv.org/pdf/2412.11557)]
> **Authors**: Jingyu Xu,Yang Wang
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: 10 page, accpted by Conf-SMPL conference
- **标题**: 使用多模式LLMS的MOE体系结构增强医疗保健建议系统
- **领域**: 信息检索,数据库
- **摘要**: 随着多模式数据的可用性的增加，许多领域迫切需要能够有效整合这些不同数据源以解决特定问题的高级体系结构。这项研究提出了一个混合推荐模型，将专家（MOE）框架与大语言模型的混合物结合在一起，以增强医疗保健领域推荐系统的性能。我们构建了一个小型数据集，用于根据患者描述推荐健康食品，并评估了该模型在几个关键指标上的性能，包括精度，召回，NDCG和MAP@5。实验结果表明，在准确性和个性化建议效果方面，混合模型的表现优于基线模型，该模型分别使用MOE或大型语言模型。该论文发现图像数据提供了个性化推荐系统的性能的相对有限的改进，尤其是在解决冷启动问题时。然后，图像重新分类的问题也影响了建议结果，尤其是在处理低质量图像或项目外观变化时，导致了次优性能。这些发现为有力，可扩展和高性能推荐系统的开发提供了宝贵的见解，并推进了在医疗保健等现实世界中的个性化建议技术的应用。

### A Survey on Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2412.12770)] [[cool](https://papers.cool/arxiv/2412.12770)] [[pdf](https://arxiv.org/pdf/2412.12770)]
> **Authors**: Liwei Pan,Weike Pan,Meiyan Wei,Hongzhi Yin,Zhong Ming
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: 关于连续建议的调查
- **领域**: 信息检索
- **摘要**: 与大多数常规建议问题不同，顺序建议通过利用相互作用的项目之间的内部顺序和依赖性来关注学习用户的偏好，这已经受到了研究人员和从业者的极大关注。近年来，我们目睹了这一领域取得的巨大进步和成就，需要进行新的调查。在这项调查中，我们从新的角度研究了SR问题（即，构建项目的属性），并总结了顺序建议中使用的最新技术，例如纯基于ID的SR，带有侧面信息，多模式SR，生成SR，LLM-PAREED SR，Ultra-Long SR，Ultra-Long SR，Ultra-long-long-long-long sr和Data-data-augented sr。此外，我们在顺序推荐中介绍了一些前沿研究主题，例如开放域SR，数据以数据为中心的SR，Come-Edge-Edge-Edge-Edge Cromportation Sr，Contunuel Sr，Sr，SR，for Ode-Oxply和可解释的SR。我们认为，我们的调查可以作为该领域读者的宝贵路线图。

### Spectrum-based Modality Representation Fusion Graph Convolutional Network for Multimodal Recommendation 
[[arxiv](https://arxiv.org/abs/2412.14978)] [[cool](https://papers.cool/arxiv/2412.14978)] [[pdf](https://arxiv.org/pdf/2412.14978)]
> **Authors**: Rongqing Kenneth Ong,Andy W. H. Khong
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: Accepted to ACM Web Search and Data Mining (WSDM) 2025
- **标题**: 基于频谱的模态表示融合图卷积网络用于多模式建议
- **领域**: 信息检索,多媒体
- **摘要**: 将多模式功能作为侧面信息结合起来已成为推荐系统的趋势。为了阐明用户项目的偏好，最近的研究集中于通过串联，元素和注意机制来融合方式。尽管取得了显着的成功，但现有方法并未考虑每种模式中封装的特定于模式的噪声。结果，直接融合方式将导致交叉模式噪声的扩增。此外，每种方式中唯一的噪声变化会导致噪音减轻和融合更具挑战性。在这项工作中，我们提出了一种新的基于频谱的模态表示（SMORE）融合图，旨在捕获单模式和融合偏好，同时抑制模态噪声。具体而言，SMORE将多模式特征投射到频域中，并利用光谱空间进行融合。为了减少每种模式独有的动态污染，我们引入了一个过滤器，以自适应地衰减和抑制态噪声，同时有效地捕获通用模式模式。此外，我们通过设计一个新的多模式图学习模块来捕获相似项目之间的关联语义相关性和通用融合模式来探索潜在结构。最后，我们制定了一个新的模式感知的偏好模块，该模块会注入行为特征，并平衡单型和多模式的特征，以进行精确的偏好建模。这使SMORE能够更准确地推断用户模式特异性和融合偏好。三个现实世界数据集的实验显示了我们提出的模型的功效。这项工作的源代码已在https://github.com/kennethorq/smore上公开提供。

### AlzheimerRAG: Multimodal Retrieval Augmented Generation for PubMed articles 
[[arxiv](https://arxiv.org/abs/2412.16701)] [[cool](https://papers.cool/arxiv/2412.16701)] [[pdf](https://arxiv.org/pdf/2412.16701)]
> **Authors**: Aritra Kumar Lahiri,Qinmin Vivian Hu
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 阿尔茨海默拉格（Alzheimerrag）：PubMed文章的多模式检索增强发电
- **领域**: 信息检索,计算语言学
- **摘要**: 生成AI的最新进展蓬勃发展，发展了高度熟练的大语言模型（LLMS），这些模型（LLMS）整合了各种数据类型以增强决策能力。其中，多模式检索增强的生成（RAG）应用有望结合信息检索和生成模型的优势，增强其在包括生物医学研究在内的各个领域的实用性。本文介绍了Alzheimerrag，这是一种用于生物医学研究用例的多模式RAG管道工具，主要侧重于PubMed文章中的阿尔茨海默氏病。我们的管道结合了多模式融合技术，通过有效索引和访问大量的生物医学文献来整合文本和视觉数据处理。针对基准测试的初步实验结果，例如BioASQ和PubMedQA，已返回了改进的信息检索和合成域特异性信息的结果。我们还在不同阿尔茨海默氏症的临床场景中使用RAG管道展示了一个案例研究。我们推断，阿尔茨海默拉格（Alzheimerrag）可以以与人类非内部和低幻觉的速度产生反应。总体而言，观察到认知任务负荷的减少，这使研究人员能够获得多模式见解，从而提高对阿尔茨海默氏病的理解和治疗。

### Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2412.18176)] [[cool](https://papers.cool/arxiv/2412.18176)] [[pdf](https://arxiv.org/pdf/2412.18176)]
> **Authors**: Yucong Luo,Qitao Qin,Hao Zhang,Mingyue Cheng,Ruiran Yan,Kefan Wang,Jie Ouyang
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: 摩尔：具有协作过滤对准的多模式LLM，以增强顺序建议
- **领域**: 信息检索,人工智能
- **摘要**: 在过去的十年中，顺序推荐（SR）系统已经显着发展，从传统的协作过滤到深度学习方法，再到最近的大语言模型（LLMS）。尽管LLM的采用却取得了重大进步，但这些模型固有地缺乏协作过滤信息，主要依靠文本内容数据忽略了其他模式，因此未能实现最佳的建议性能。为了解决此限制，我们提出了Morlar，这是一种多模式的大语言顺序推荐框架，将多种内容模式与ID信息集成在一起，以有效地捕获协作信号。 Molar使用MLLM从文本和非文本数据中生成统一的项目表示形式，从而促进综合的多模式建模和丰富的项目嵌入。此外，它通过后对准机制结合了协作过滤信号，该机制使基于内容和基于ID的模型的用户表示相符，从而确保了精确的个性化和稳健的性能。通过将多模式内容与协作过滤见解无缝相结合，Morlar捕获了用户兴趣和上下文语义，从而提高了卓越的建议准确性。广泛的实验验证了摩尔的表现明显胜过传统和LLM的基线，从而强调了其在利用多模式数据和协作信号方面的强度来进行连续推荐任务。源代码可从https://anonymon.4open.science/r/molar-8b06/获得。

### Don't Lose Yourself: Boosting Multimodal Recommendation via Reducing Node-neighbor Discrepancy in Graph Convolutional Network 
[[arxiv](https://arxiv.org/abs/2412.18962)] [[cool](https://papers.cool/arxiv/2412.18962)] [[pdf](https://arxiv.org/pdf/2412.18962)]
> **Authors**: Zheyu Chen,Jinfeng Xu,Haibo Hu
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: Accepted by ICASSP 2025
- **标题**: 不要失去自己：通过减少图形卷积网络中的节点 - 邻居差异来提高多模式建议
- **领域**: 信息检索,多媒体
- **摘要**: 多媒体内容的快速扩展导致了多模式推荐系统的出现。它在推荐系统中引起了越来越多的关注，因为它完全利用来自不同模式的数据可以减轻持续的数据稀疏问题。因此，多模式推荐模型可以从视觉和文本方面学习有关节点的个性化信息。为了进一步缓解数据稀疏问题，一些以前的作品为多模式推荐系统引入了图形卷积网络（GCN），以通过捕获它们之间的潜在关系来增强用户和项目的语义表示。但是，采用GCN不可避免地会引入过度光滑的问题，这使节点变得太相似。不幸的是，结合多模式信息会加剧这一挑战，因为太相似的节点将失去通过多模式信息学习的个性化信息。为了解决这个问题，我们提出了一个新颖的模型，该模型通过减少节点邻居差异（redn^nd）来保留特征聚合过程中自我节点的个性化信息。在三个公共数据集上进行的广泛实验表明，Redn^nd在准确性和鲁棒性方面取得了最新的性能，并且对现有基于GCN的多模式框架进行了重大改进。

## 机器学习(cs.LG:Machine Learning)

该领域共有 67 篇论文

### CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives 
[[arxiv](https://arxiv.org/abs/2411.19787)] [[cool](https://papers.cool/arxiv/2411.19787)] [[pdf](https://arxiv.org/pdf/2411.19787)]
> **Authors**: Armin Saghafian,Amirmohammad Izadi,Negin Hashemi Dijujin,Mahdieh Soleymani Baghshah
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: Carel：具有跨模式辅助目标的指导引导的加固学习
- **领域**: 机器学习,人工智能
- **摘要**: 在环境中进行教学是解决语言引导的实现目标增强学习问题的关键步骤。在自动增强学习中，关键问题是增强模型在各种任务和环境中概括的能力。在进球场景中，代理必须在环境环境中理解指令的不同部分，以便成功完成整体任务。在这项工作中，我们提出了Carel（跨模式辅助增强学习）作为一个新框架，可以使用受视频文本检索文献启发的辅助损失功能和一种称为“指导跟踪”的新颖方法来解决此问题，该方法自动地跟踪环境中的进度。我们的实验结果表明，在多模式增强学习问题中，该框架的样本效率和系统的概括都出色。我们的代码库在这里可用。

### JetFormer: An Autoregressive Generative Model of Raw Images and Text 
[[arxiv](https://arxiv.org/abs/2411.19722)] [[cool](https://papers.cool/arxiv/2411.19722)] [[pdf](https://arxiv.org/pdf/2411.19722)]
> **Authors**: Michael Tschannen,André Susano Pinto,Alexander Kolesnikov
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 喷气式形式：原始图像和文本的自回旋生成模型
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 消除建模限制和跨域统一体系结构是训练大型多模式模型的最新进展的关键驱动力。但是，这些模型中的大多数仍然依赖于许多单独训练的组件，例如特定于模式的编码器和解码器。在这项工作中，我们进一步简化了图像和文本的关节生成建模。我们提出了一种自动回归的仅解码器变压器 - 喷气形式 - 该训练可以直接最大化原始数据的可能性，而无需依赖任何单独预处理的组件，并且可以理解和生成文本和图像。具体而言，我们利用标准化流量模型获得了柔软的图像表示，该图像表示由自回归多模式变压器共同训练。归一化流量模型既可以作为感知任务的图像编码器，又是推理过程中图像生成任务的图像解码器。 Jetformer与最近的基于VQ-VAE和VAE基线的基本线实现了文本到图像的质量竞争。这些基线依赖于验证的图像自动编码器，这些自动编码器经过复杂的损失混合物，包括感知性损失。同时，JetFormer展示了强大的图像理解能力。据我们所知，JetFormer是第一个能够产生高保真图像并产生强大的对数可能性界限的模型。

### Explainable Artificial Intelligence for Medical Applications: A Review 
[[arxiv](https://arxiv.org/abs/2412.01829)] [[cool](https://papers.cool/arxiv/2412.01829)] [[pdf](https://arxiv.org/pdf/2412.01829)]
> **Authors**: Qiyang Sun,Alican Akman,Björn W. Schuller
> **First submission**: 2024-11-15
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 可解释的医学应用人工智能：评论
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 由于学者和研究人员的无情努力，人工智能（AI）理论的持续发展已推动了前所未有的高度。在医学领域，AI扮演着关键的角色，利用了健壮的机器学习（ML）算法。医学成像中的AI技术在X射线，计算机断层扫描（CT）扫描和磁共振成像（MRI）诊断中辅助医师，基于声学数据进行模式识别和疾病预测，对患者的疾病类型和发育趋势进行预测，并使用智能的健康管理式耐磨式戴在人类互动技术中，但要命名为几个。尽管这些良好的应用程序在医疗领域的诊断，临床决策和管理方面大大有助于医疗和AI部门之间的协作面临紧迫的挑战：如何证实决策的可靠性？基本问题源于对责任制的需求与医疗场景中结果透明度之间的冲突与AI的黑框模型特征之间的透明度之间的冲突。本文回顾了以解释人工智能（XAI）为基础的最新研究，重点是视觉，音频和多模式观点中的医学实践。我们努力对这些做法进行分类和综合，旨在为未来的研究人员和医疗保健专业人员提供支持和指导。

### Revisiting Generative Policies: A Simpler Reinforcement Learning Algorithmic Perspective 
[[arxiv](https://arxiv.org/abs/2412.01245)] [[cool](https://papers.cool/arxiv/2412.01245)] [[pdf](https://arxiv.org/pdf/2412.01245)]
> **Authors**: Jinouwen Zhang,Rongkun Xue,Yazhe Niu,Yun Chen,Jing Yang,Hongsheng Li,Yu Liu
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 重新审视生成政策：更简单的增强学习算法观点
- **领域**: 机器学习,人工智能
- **摘要**: 生成模型，尤其是扩散模型，在多模式数据的密度估计中取得了巨大的成功，引起了增强学习（RL）社区的重大兴趣，尤其是在连续动作空间中的政策建模中。但是，现有作品在训练方案和RL优化目标中表现出显着差异，并且某些方法仅适用于扩散模型。在这项研究中，我们比较和分析各种生成政策培训和部署技术，确定和验证生成政策算法的有效设计。具体来说，我们重新审视现有的培训目标，并将它们分为两类，每种都链接到更简单的方法。第一种方法是生成模型策略优化（GMPO），它采用本地优势加权回归公式作为训练目标，这比以前的方法要简单得多。第二种方法是生成模型策略梯度（GMPG），提供了本机策略梯度方法的数值稳定实现。我们引入了一个名为Generativerl的标准化实验框架。我们的实验表明，所提出的方法在各种离线RL数据集上实现最先进的性能，为培训和部署生成政策提供了统一且实用的指南。

### EsurvFusion: An evidential multimodal survival fusion model based on Gaussian random fuzzy numbers 
[[arxiv](https://arxiv.org/abs/2412.01215)] [[cool](https://papers.cool/arxiv/2412.01215)] [[pdf](https://arxiv.org/pdf/2412.01215)]
> **Authors**: Ling Huang,Yucheng Xing,Qika Lin,Su Ruan,Mengling Feng
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: Multimodalsurvival analysis, Epistemic random fuzzy sets theory, Uncertainty
- **标题**: EsurvFusion：基于高斯随机数字的证据多模式生存融合模型
- **领域**: 机器学习
- **摘要**: 多模式生存分析旨在结合异质数据源（例如临床，成像，文本，基因组学），以提高生存结果的预测质量。但是，由于数据源之间的高异质性和噪音，该任务在结构，分布和环境方面有所不同。此外，由于不完整的随访数据，通常会审查地面真相（不确定）。在本文中，我们提出了一个新型的证据多模式生存融合模型Esurvfusion，旨在通过基于证据的决策融合层在决策水平上组合多模式数据，该融合层共同解决数据和模型不确定性，同时结合了模态级别的可靠性。具体而言，Esurvususion首先模型具有新引入高斯随机模糊数的单峰数据，从而产生了单型号的生存预测以及相应的质地和认知不确定性。然后，它通过可靠性折现层估算了模态级别的可靠性，以纠正嘈杂数据模式的误导性影响。最后，引入了基于多模式的基于证据的融合层，以结合折扣预测，形成一个统一的，可解释的多模式生存分析模型，从而根据学习的可靠性系数揭示了每种模态的影响。这是研究多模式生存分析具有不确定性和可靠性的第一项工作。对四个多模式生存数据集进行的广泛实验证明了我们模型在处理高异质性数据方面的有效性，并在几个基准上建立了新的最新技术。

### Multi-Scale Representation Learning for Protein Fitness Prediction 
[[arxiv](https://arxiv.org/abs/2412.01108)] [[cool](https://papers.cool/arxiv/2412.01108)] [[pdf](https://arxiv.org/pdf/2412.01108)]
> **Authors**: Zuobai Zhang,Pascal Notin,Yining Huang,Aurélie Lozano,Vijil Chenthamarakshan,Debora Marks,Payel Das,Jian Tang
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 蛋白质适应性预测的多尺度表示学习
- **领域**: 机器学习,生物分子
- **摘要**: 设计新型功能蛋白的设计至关重要的取决于准确建模其适应性景观。鉴于湿LAB实验的功能注释的可用性有限，以前的方法主要依赖于在庞大的，未标记的蛋白质序列或结构数据集中训练的自我监管模型。尽管最初的蛋白质表示学习研究仅集中在序列或结构特征上，但最近的混合体系结构试图合并这些方式以利用其各自的优势。但是，与仅领先序列方法相比，这些序列结构模型仅实现了增量的改进，从而强调了尚未解决的挑战有效地利用了这些模式。此外，某些蛋白质的功能高度取决于其表面拓扑的颗粒状方面，这些方面已被先前模型所忽略。为了解决这些局限性，我们介绍了序列结构表面健身（S3F）模型 - 一种新型的多模式表示学习框架，可在几个尺度上整合蛋白质特征。我们的方法将蛋白质语言模型的序列表示与几何矢量感知到编码蛋白质主链和详细的表面拓扑结合。所提出的方法在包含217个取代的蛋白酶基准上实现了最新的适应性预测，深度突变扫描测定法，并提供了对蛋白质功能决定因素的见解。我们的代码在https://github.com/deepgraphlearning/s3f上。

### Personalized Coupled Tensor Decomposition for Multimodal Data Fusion: Uniqueness and Algorithms 
[[arxiv](https://arxiv.org/abs/2412.01102)] [[cool](https://papers.cool/arxiv/2412.01102)] [[pdf](https://arxiv.org/pdf/2412.01102)]
> **Authors**: Ricardo Augusto Borsoi,Konstantin Usevich,David Brie,Tülay Adali
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 多模式数据融合的个性化耦合张量分解：唯一性和算法
- **领域**: 机器学习,信号处理
- **摘要**: 耦合张量分解（CTD）通过将不同数据集的因子链接起来执行数据融合。尽管已经提出了许多CTD，但是当前的作品并未应对数据融合的重要挑战，其中：1）数据集通常是异质的，构成给定现象的不同“视图”（多模式）； 2）每个数据集都可以包含个性化或数据集特定的信息，构成与其他数据集耦合的不同因素。在这项工作中，我们引入了一个个性化的CTD框架来应对这些挑战。提出了一个灵活的模型，其中每个数据集表示为两个组件的总和，一个通过多线性测量模型与通用张量相关，另一个与每个数据集有关。假定共同和不同的组件都接受多核分解。这概括了几种现有的CTD模型。我们为易于解释的分解的特定和通用唯一性提供条件。这些条件采用了不同单个数据集和测量模型的属性的单模唯一性。提出了两种算法来计算常见和不同的组件：半代数的组件和一种坐标呈散发优化方法。实验结果说明了与艺术方法相比，所提出的框架的优势。

### STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft 
[[arxiv](https://arxiv.org/abs/2412.00949)] [[cool](https://papers.cool/arxiv/2412.00949)] [[pdf](https://arxiv.org/pdf/2412.00949)]
> **Authors**: Nicholas Lenzen,Amogh Raut,Andrew Melnik
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: Accepted at CoRL 2024: Workshop on Lifelong Learning for Home Robots
- **标题**: 史蒂夫·奥德奥（Steve-Audio）：扩大我的Minecraft中具体代理的目标条件方式
- **领域**: 机器学习,人工智能,机器人技术
- **摘要**: 最近，已经引入了Steve-1方法，作为一种训练生成剂以潜在夹子嵌入形式遵循指令的方法。在这项工作中，我们提出了一种方法，可以通过学习从新的输入方式到代理的潜在目标空间来扩展控制模式。我们将方法应用于具有挑战性的Minecraft域，并将目标条件扩展到包括音频方式。由此产生的音频条件的代理能够以与原始文本条件和视觉条件的代理相当地进行性能。具体来说，我们为Minecraft创建了一个音频视频剪辑基础模型和一个音频先前的网络，该网络将音频样本映射到Steve-1策略的潜在目标空间。此外，我们重点介绍了以不同方式进行调节时发生的权衡。我们的Minecraft的培训代码，评估代码和Audio-Video剪辑基础模型是开源的，以帮助促进对多模式通才的顺序决策代理的进一步研究。

### TAROT: Targeted Data Selection via Optimal Transport 
[[arxiv](https://arxiv.org/abs/2412.00420)] [[cool](https://papers.cool/arxiv/2412.00420)] [[pdf](https://arxiv.org/pdf/2412.00420)]
> **Authors**: Lan Feng,Fan Nie,Yuejiang Liu,Alexandre Alahi
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 塔罗牌：通过最佳传输的目标数据选择
- **领域**: 机器学习,计算机视觉和模式识别,机器学习
- **摘要**: 我们提出了塔罗牌，塔罗牌是一个以最佳运输理论为基础的目标数据选择框架。先前的目标数据选择方法主要依赖于基于影响力的贪婪启发式方法来增强特定领域的性能。虽然对有限的单峰数据有效（即遵循单个模式的数据），但这些方法随着目标数据复杂性的增加而努力。具体而言，在多模式分布中，这些启发式方法无法说明多种固有模式，从而导致次优数据选择。这项工作确定了有助于此限制的两个主要因素：（i）在高维影响估计中，主要特征成分的不成比例影响，以及（ii）贪婪选择策略中固有的限制性线性加性假设。为了应对这些挑战，塔罗牌结合了白色特征距离，以减轻主要特征偏见，从而提供了更可靠的数据影响。在此基础上，塔罗牌使用白色特征距离来量化和最小化所选数据和目标域之间的最佳传输距离。值得注意的是，这种最小化还促进了最佳选择比的估计。我们评估跨多个任务的塔罗牌，包括语义细分，运动预测和说明调整。结果始终表明，塔罗牌的表现优于最先进的方法，突出了其在各种深度学习任务中的多功能性。代码可在https://github.com/vita-epfl/tarot上找到。

### Approximate Fiber Product: A Preliminary Algebraic-Geometric Perspective on Multimodal Embedding Alignment 
[[arxiv](https://arxiv.org/abs/2412.00373)] [[cool](https://papers.cool/arxiv/2412.00373)] [[pdf](https://arxiv.org/pdf/2412.00373)]
> **Authors**: Dongfang Zhao
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 近似纤维产品：多模式嵌入对齐的初步代数几何观点
- **领域**: 机器学习,人工智能,代数几何
- **摘要**: 多模式任务（例如图像文本检索和生成）需要将不同模式的数据嵌入到共享表示空间中。在保存共享和特定于模式的信息的同时，将嵌入的嵌入是一个基本挑战。本文提供了将代数几何形状整合到多模式表示学习中的初步尝试，从而提供了进一步探索的基础观点。我们将图像和文本数据建模为多项式，而不是离散戒指，\（\ Mathbb {z} _ {256} [x] \）和\（\ Mathbb {z} _ {| v |} [x] [x] \），以实现ElgeBraic工具的使用来分析诸如Aligneze alignse polorties polorsies。为了适应现实世界的可变性，我们将经典纤维产物扩展到具有公差参数\（ε\）的近似光纤产品，平衡精度和噪声耐受性。我们研究其对\（ε\）的依赖性，揭示了渐近行为，对扰动的鲁棒性以及对嵌入维度的敏感性。此外，我们提出将共享嵌入空间分解到正交子空间中，\（z = z_s \ oplus z_i \ oplus z_t \），其中\（z_s \）捕获共享的语义和\（z_i \），\（z_t \），\（z_t \ \）eNcode模态。这种分解是通过歧管和纤维束对几何解释的，从而提供了对嵌入结构和优化的见解。该框架为分析多模式对齐，揭示鲁棒性，维度分配和代数结构之间的联系建立了原则上的基础。它为使用代数几何学的多模式学习中的嵌入空间进行进一步研究奠定了基础。

### Spatial Clustering of Molecular Localizations with Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2412.00173)] [[cool](https://papers.cool/arxiv/2412.00173)] [[pdf](https://arxiv.org/pdf/2412.00173)]
> **Authors**: Jesús Pineda,Sergi Masó-Orriols,Joan Bertran,Mattias Goksör,Giovanni Volpe,Carlo Manzo
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 通过图神经网络的分子局部定位的空间聚类
- **领域**: 机器学习,生物物理学,数据分析、统计和概率,定量方法
- **摘要**: 单分子定位显微镜生成与荧光团局部化的点云。这些点云的空间簇鉴定和分析对于提取有关分子组织的见解至关重要。但是，在存在定位噪声，高点密度或复杂的生物结构的情况下，该任务变得具有挑战性。在这里，我们介绍了MiRO（通过关系优化的多模式集成），该算法使用复发图神经网络来转换点云，以在应用常规聚类技术时提高聚类效率。我们表明，Miro支持同时处理不同形状和多个尺度的簇，这表明了各种数据集的性能得到了改善。我们的全面评估表明，米罗（Miro）在单分子定位应用中的变革潜力，展示了其彻底改变群集分析并提供准确，可靠的分子体系结构细节的能力。此外，Miro的强大聚类能力对在神经科学等各个领域的应用，用于分析神经连通性模式和环境科学的分析，以研究生态数据的空间分布。

### Visual Error Patterns in Multi-Modal AI: A Statistical Approach 
[[arxiv](https://arxiv.org/abs/2412.00083)] [[cool](https://papers.cool/arxiv/2412.00083)] [[pdf](https://arxiv.org/pdf/2412.00083)]
> **Authors**: Ching-Yi Wang
> **First submission**: 2024-11-26
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 多模式AI中的视觉错误模式：一种统计方法
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,应用领域
- **摘要**: 多模式的大语言模型（MLLM），例如GPT-4O，在整合文本和视觉数据方面表现出色，但是在解释模棱两可或不完整的视觉刺激时会面临系统的挑战。这项研究利用统计建模来分析驱动这些误差的因素，使用以3D，旋转和缺失的面部/侧面特征为特征的几何刺激数据集。我们应用参数方法，非参数方法和集合技术来预测分类误差，非线性梯度增强模型在交叉验证过程中实现了最高性能（AUC = 0.85）。特征重要性分析强调了深度感知的困难和重建不完整的结构，这是导致错误分类的关键因素。这些发现证明了统计方法对发现MLLM中局限性的有效性，并通过整合上下文推理机制提供了可行的见解，以增强模型架构。

### COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection 
[[arxiv](https://arxiv.org/abs/2412.00071)] [[cool](https://papers.cool/arxiv/2412.00071)] [[pdf](https://arxiv.org/pdf/2412.00071)]
> **Authors**: Jinqi Xiao,Shen Sang,Tiancheng Zhi,Jing Liu,Qing Yan,Yuqian Zhang,Linjie Luo,Bo Yuan
> **First submission**: 2024-11-25
> **First announcement**: 2024-12-03
> **comment**: CVPR 2025
- **标题**: COAP：具有相关性梯度投影的记忆效率训练
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 培训视觉中的大规模神经网络和多模式域需要大量的内存资源，这主要是由于优化器状态的存储。虽然洛拉（Lora）是一种流行的参数效率方法，可减少内存使用情况，但由于低级别更新的限制，它通常会遭受次优性能。低级梯度投影方法（例如，galore，Flora）通过通过奇异值分解或随机投影将梯度和矩估计来降低优化器记忆。但是，他们无法解释投票间相关性，导致绩效降低，其投影策略通常会产生高计算成本。在本文中，我们介绍了CoAP（相关感知梯度投影），这是一种记忆有效的方法，可在维持训练性能的同时最大程度地减少计算开销。在各种视觉，语言和多模式任务中进行评估，Coap在训练速度和模型性能方面都优于现有方法。对于Llama-1b，它将优化器内存降低61％，仅额外的时间成本2％，与ADAMW相同的PPL。通过8位量化，CoAp将优化器存储器削减81％，并在Llava-V1.5-7B微调方面达到4倍加速度，同时提供了更高的精度。

### LeMoLE: LLM-Enhanced Mixture of Linear Experts for Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2412.00053)] [[cool](https://papers.cool/arxiv/2412.00053)] [[pdf](https://arxiv.org/pdf/2412.00053)]
> **Authors**: Lingzheng Zhang,Lifeng Shen,Yimin Zheng,Shiyuan Piao,Ziyue Li,Fugee Tsung
> **First submission**: 2024-11-24
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Lemole：线性专家的LLM增强混合时间，预测时间序列
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 最近的研究表明，大型语言模型（LLM）可以有效地用于实际时间序列，因为它们强烈的自然语言理解能力。然而，将时间序列序列序列序列序列LLM的语义空间具有很高的计算成本和推理的复杂性，尤其是对于长期时间序列的生成。本文以最新的线性模型为基础，在使用线性模型的时间序列方面，介绍了线性专家的LLM增强混合物，以进行精确有效的时间序列预测。这种方法涉及开发具有多个回顾性长度和新的多模式融合机制的线性专家的混合物。线性专家的混合使用是由于其简单性而有效的，而多模式融合机制则根据预先训练的大型语言模型从文本模式的学习方式自适应地结合了多个线性专家。在实验中，我们通过现有的时间序列模型来重新考虑将时间序列与LLM保持一致的需求，并进一步讨论其时间序列预测的效率和有效性。我们的实验结果表明，与现有LLM模型相比，提出的Lemole模型提出的预测误差和更高的计算效率。

### WxC-Bench: A Novel Dataset for Weather and Climate Downstream Tasks 
[[arxiv](https://arxiv.org/abs/2412.02780)] [[cool](https://papers.cool/arxiv/2412.02780)] [[pdf](https://arxiv.org/pdf/2412.02780)]
> **Authors**: Rajat Shinde,Christopher E. Phillips,Kumar Ankur,Aman Gupta,Simon Pfreundschuh,Sujit Roy,Sheyenne Kirkland,Vishal Gaur,Amy Lin,Aditi Sheshadri,Udaysankar Nair,Manil Maskey,Rahul Ramachandran
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: No comments
- **标题**: WXC板凳：天气和气候下游任务的新型数据集
- **领域**: 机器学习,人工智能
- **摘要**: 高质量的机器学习（ML） - 就绪数据集在开发新的人工智能（AI）模型（AI）模型或用于科学应用（例如天气和气候分析）的现有模型中起着基础作用。不幸的是，尽管新的深度学习模型的天气和气候发展越来越不断发展，但策划的，预处理的机器学习（ML）已经准备就绪的数据集稀缺。策划这种开发新模型的高质量数据集具有挑战性，尤其是因为输入数据的模式对于解决不同大气尺度（空间和时间）的不同下游任务的不同下游任务差异很大。在这里，我们介绍了WXC板凳（天气和气候台），这是一个多模式数据集，旨在支持在天气和气候研究中为下游用例开发的可推广的AI模型。 WXC板凳被设计为用于为复杂天气和气候系统开发ML模型的数据集的数据集，将选定的下游任务作为机器学习现象解决。 WXC板凳涵盖了几个大气过程，从中$β$（20-200 km）量表到概要尺度（2500公里），例如航空湍流，飓风强度和轨道监测，天气模拟搜索，重力搜索，重力波浪参数化和自然语言报告。我们提供了数据集的全面描述，还提供了基线分析的技术验证。准备ML就绪数据的数据集和代码已在拥抱面上公开可用-https://huggingface.co/datasets/nasa-impact/wxc-bench

### Learning on One Mode: Addressing Multi-Modality in Offline Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2412.03258)] [[cool](https://papers.cool/arxiv/2412.03258)] [[pdf](https://arxiv.org/pdf/2412.03258)]
> **Authors**: Mianchu Wang,Yue Jin,Giovanni Montana
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 在一种模式下学习：解决离线增强学习中的多模式
- **领域**: 机器学习
- **摘要**: 离线增强学习（RL）试图从静态数据集中学习最佳政策，而无需与环境进行交互。一个普遍的挑战是处理多模式作用分布，其中数据中表示多种行为。现有方法通常采用单峰行为策略，在违反此假设时会导致次优性能。我们建议在一种模式（LOM）上提出加权模仿学习，这是一种新颖的方法，侧重于从单一有希望的行为政策模式中学习。通过使用高斯混合模型来识别模式并根据预期的回报选择最佳模式，LOM避免了对冲突动作的平均陷阱。从理论上讲，我们表明LOM可以提高性能，同时保持政策学习中的简单性。从经验上讲，LOM在标准D4RL基准测试上胜过现有方法，并在复杂的多模式场景中证明了其有效性。

### Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges 
[[arxiv](https://arxiv.org/abs/2412.03220)] [[cool](https://papers.cool/arxiv/2412.03220)] [[pdf](https://arxiv.org/pdf/2412.03220)]
> **Authors**: Minghao Shao,Abdul Basit,Ramesh Karri,Muhammad Shafique
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 对不同语言模型体系结构的调查：趋势，基准和挑战
- **领域**: 机器学习
- **摘要**: 大型语言模型（LLMS）代表了一类深度学习模型，旨在理解自然语言并对各种提示或查询产生连贯的响应。这些模型远远超过了常规神经网络的复杂性，通常包含数十个神经网络层，并包含数十亿至万亿个参数。它们通常在庞大的数据集上进行培训，利用基于变压器块的体系结构。当今的LLM是多功能的，能够执行从文本生成和语言翻译到问题答案以及代码生成和分析的一系列任务。这些模型的高级子集（称为多模式大语言模型（MLLM））扩展了LLM功能来处理和解释多个数据模式，包括图像，音频和视频。这种增强功能使MLLM具有视频编辑，图像理解和视觉内容字幕的功能。这项调查概述了LLM的最新进展。我们首先要追踪LLM的发展，然后深入研究MLLM的出现和细微差别。我们分析新兴的最先进的MLLM，探索他们的技术特征，优势和局限性。此外，我们对这些模型进行了比较分析，并讨论了它们的挑战，潜在的局限性以及未来发展的前景。

### BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks 
[[arxiv](https://arxiv.org/abs/2412.04626)] [[cool](https://papers.cool/arxiv/2412.04626)] [[pdf](https://arxiv.org/pdf/2412.04626)]
> **Authors**: Juan Rodriguez,Xiangru Jian,Siba Smarak Panigrahi,Tianyu Zhang,Aarash Feizi,Abhay Puri,Akshay Kalkunte,François Savard,Ahmed Masry,Shravan Nayak,Rabiul Awal,Mahsa Massoud,Amirhossein Abaskohi,Zichao Li,Suyuchen Wang,Pierre-André Noël,Mats Leon Richter,Saverio Vadacchino,Shubbam Agarwal,Sanket Biswas,Sara Shanian,Ying Zhang,Noah Bolger,Kurt MacDonald,Simon Fauvel, et al. (18 additional authors not shown)
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: The project is hosted at https://bigdocs.github.io
- **标题**: BigDocs：一个开放和允许许可的数据集，用于培训文档和代码任务的多模式模型
- **领域**: 机器学习,计算语言学
- **摘要**: 多模式AI有可能显着增强文档的理解任务，例如处理收据，了解工作流程，从文档中提取数据并汇总报告。需要长结构输出的代码生成任务也可以通过多模式来增强。尽管如此，由于访问培训数据和限制性许可，它们在商业应用程序中的使用通常受到限制，这阻碍了公开访问。为了解决这些限制，我们引入了BigDocs-7.5m，这是一个高质量的开放式数据集，其中包括30个任务中的750万个多模式文档。我们使用有效的数据策展过程来确保我们的数据具有高质量和许可证。我们的过程通过过滤规则，可追溯的元数据和仔细的内容分析来强调问责制，责任和透明度。此外，我们介绍了BigDocs-Bench，这是一个基准套件，其中包含10个新颖的任务，我们创建了反映现实世界中用例的数据集，这些案例涉及图形用户界面（GUI）（GUI）和图像中代码生成的推理。我们的实验表明，在文档推理和结构化的输出任务（例如ScreenShot2HTML或Image2Latex生成）中，使用BigDocs板凳培训可在封闭源GPT-4O上提高平均性能高达25.8％。最后，人类评估表明，在GPT-4O上训练BigDocs的模型的产出偏爱。这表明BigDocs可以帮助学者和开源社区利用并改善AI工具来增强多模式能力和文档推理。该项目托管在https://bigdocs.github.io上。

### Leveraging Multimodal Protein Representations to Predict Protein Melting Temperatures 
[[arxiv](https://arxiv.org/abs/2412.04526)] [[cool](https://papers.cool/arxiv/2412.04526)] [[pdf](https://arxiv.org/pdf/2412.04526)]
> **Authors**: Daiheng Zhang,Yan Zeng,Xinyu Hong,Jinbo Xu
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 利用多模式蛋白表示来预测蛋白质熔化温度
- **领域**: 机器学习,计算工程、金融和科学
- **摘要**: 准确预测蛋白质熔化温度变化（Delta TM）对于评估蛋白质稳定性和指导蛋白质工程至关重要。利用多模式蛋白表示，在捕获蛋白质序列，结构和功能之间的复杂关系方面表现出了巨大的希望。在这项研究中，我们使用各种功能提取方法来开发基于强大的蛋白质语言模型，包括ESM-2，ESM-3和Alphafold，以提高预测准确性。通过利用ESM-3模型，我们在S571测试数据集上实现了新的最先进性能，获得了0.50的Pearson相关系数（PCC）。此外，我们进行了公平的评估，以比较Delta TM预测任务中不同蛋白质语言模型的性能。我们的结果表明，整合多模式蛋白表示可以推进蛋白质熔化温度的预测。

### Accelerating Manufacturing Scale-Up from Material Discovery Using Agentic Web Navigation and Retrieval-Augmented AI for Process Engineering Schematics Design 
[[arxiv](https://arxiv.org/abs/2412.05937)] [[cool](https://papers.cool/arxiv/2412.05937)] [[pdf](https://arxiv.org/pdf/2412.05937)]
> **Authors**: Sakhinana Sagar Srinivas,Akash Das,Shivam Gupta,Venkataramana Runkana
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 使用代理Web导航从物料发现加速制造规模，并检索过程工程示意图设计
- **领域**: 机器学习,人工智能,信息检索,多代理系统
- **摘要**: 过程流程图（PFD）以及过程和仪器图（PID）是工业过程设计，控制和安全的关键工具。但是，精确和符合法规的图表的产生仍然是一个重大挑战，尤其是在自动化和数字化时代从物质发现到工业生产的突破。本文介绍了一个自治座框架，通过涉及知识获取和产生的两种方法来应对这些挑战。该框架集成了专门的子代理，用于从公共可用的在线资源中检索和合成多模式数据，并使用图检索仪（Graph rag）范式构建本体知识图。这些功能使图生成和开放域问题答案（ODQA）任务具有很高的上下文准确性。广泛的经验实验表明，通过最少的专家干预提供符合法规的图表的框架能力，强调了其对工业应用的实际实用性。

### Bridging the Gap for Test-Time Multimodal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2412.07121)] [[cool](https://papers.cool/arxiv/2412.07121)] [[pdf](https://arxiv.org/pdf/2412.07121)]
> **Authors**: Zirun Guo,Tao Jin,Wenlong Xu,Wang Lin,Yangyang Wu
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: Accepted to AAAI 2025
- **标题**: 弥合差距进行测试时间多模式分析
- **领域**: 机器学习,计算语言学
- **摘要**: 多模式情感分析（MSA）是一个新兴的研究主题，旨在通过多种方式理解和认识人类情感或情感。但是，在现实世界的动态场景中，目标数据的分布始终在变化，并且与用于训练模型的源数据不同，这会导致性能降级。常见的适应方法通常需要源数据，这可能会构成隐私问题或存储开销。因此，引入了测试时间适应（TTA）方法，以提高推理时模型的性能。现有的TTA方法始终基于概率模型和单峰学习，因此无法将其应用于MSA，通常被视为多模式回归任务。在本文中，我们提出了两种策略：对比度适应和稳定的伪标签生成（CASP），用于测试时间适应多模式分析。两种策略分别通过执行一致性和最小化经验风险来处理MSA的分配变化。广泛的实验表明，CASP在各种分配转移设置和不同的骨架上为模型的性能带来了重大和一致的改进，这表明了其有效性和多功能性。我们的代码可在https://github.com/zrguo/casp上找到。

### In-Application Defense Against Evasive Web Scans through Behavioral Analysis 
[[arxiv](https://arxiv.org/abs/2412.07005)] [[cool](https://papers.cool/arxiv/2412.07005)] [[pdf](https://arxiv.org/pdf/2412.07005)]
> **Authors**: Behzad Ousat,Mahshad Shariatnasab,Esteban Schafir,Farhad Shirani Chaharsooghi,Amin Kharraz
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 通过行为分析的避免回避网络扫描的范围内防御
- **领域**: 机器学习,密码学和安全,信息论
- **摘要**: Web流量已经演变为包括人类用户和自动化代理，从良性的网络爬网一直到对抗性扫描仪，例如能够在网络范围内凭证填充，命令注入和劫持帐户的扫描仪。这些对抗性活动的估计财务成本估计在2023年超过数百亿美元。在这项工作中，我们介绍了Webguard，这是一种低空设置的插入式取证引擎，以启用自动网络扫描仪的强大识别和监视，并帮助减轻相关的安全风险。 WebGuard专注于以下设计标准：（i）集成到Web应用程序中，而无需更改基础软件组件或基础架构，（ii）最小的通信开销，（iii）实时检测的能力，例如，（例如，在数百毫秒内）和（IV）归属能力以确定新的行为模式和新的行为模式。为此，我们为Webguard配备了多模式行为监控机制，例如监视时空数据和浏览器事件。我们还设计了有监督和无监督的学习体系结构，分别针对人类和自动化代理的实时检测和离线归因。提供了信息理论分析和经验评估，以表明多模式数据分析与仅依赖于小鼠运动动态的单模式分析相反，显着提高了检测时间和归因精度。提供了通过WebGuard收集的现实世界数据进行的各种数值评估，可在数百毫秒内实现高精度，并且通信开销低于每秒10 kb。

### MDiFF: Exploiting Multimodal Score-based Diffusion Models for New Fashion Product Performance Forecasting 
[[arxiv](https://arxiv.org/abs/2412.06840)] [[cool](https://papers.cool/arxiv/2412.06840)] [[pdf](https://arxiv.org/pdf/2412.06840)]
> **Authors**: Andrea Avogaro,Luigi Capogrosso,Franco Fummi,Marco Cristani
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-10
> **comment**: Accepted at the FashionAI workshop @ the European Conference on Computer Vision (ECCV) 2024. arXiv admin note: substantial text overlap with arXiv:2412.05566
- **标题**: MDIFF：为新的时尚产品性能预测利用基于多模式的扩散模型
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 快速时装行业由于生产过多和未售出的库存而产生了重大的环境影响。准确地预测未发布产品的销售量可以显着提高效率和资源利用率。但是，由于缺乏历史数据和迅速变化的趋势，预测全新项目的性能是具有挑战性的，而现有的确定性模型在遇到培训数据分布之外的项目时通常会随着域的转移而挣扎。最近提出的扩散模型使用连续的时间扩散过程解决了这个问题。这使我们能够模拟如何采用新项目，从而减少了确定性模型面临的域转移挑战的影响。结果，在本文中，我们提出了MDIFF：一种新型的两步多模型扩散模型，用于新的时尚产品性能预测（NFPPF）。首先，我们使用基于分数的扩散模型来预测随着时间的推移，不同衣服的多个未来销售。然后，我们使用轻巧的多层感知器（MLP）来完善这些多个预测，以获得最终的预测。 MDIFF利用两种体系结构的优势，从而为最先进的时尚行业提供了最准确，最有效的预测系统。可以在https://github.com/intelligolabs/mdiff上找到该代码。

### How to Merge Your Multimodal Models Over Time? 
[[arxiv](https://arxiv.org/abs/2412.06712)] [[cool](https://papers.cool/arxiv/2412.06712)] [[pdf](https://arxiv.org/pdf/2412.06712)]
> **Authors**: Sebastian Dziadzio,Vishaal Udandarao,Karsten Roth,Ameya Prabhu,Zeynep Akata,Samuel Albanie,Matthias Bethge
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: Technical Report. Code at https://github.com/ExplainableML/fomo_in_flux
- **标题**: 如何随着时间的推移将多模型合并？
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 模型合并将多个专家模型结合在一起 - 从基础基础基础模型和域的基础模型进行了填充 - 单个功能更强大的模型。但是，大多数现有的模型合并方法都假定所有专家都可以同时使用。实际上，随着时间的流逝，新任务和领域逐渐出现，需要在可用的专家模型中整合知识的策略：我们称之为时间模型合并的过程。时间维度引入了先前工作中未解决的独特挑战，提出了新的问题，例如：当培训新任务时，专家模型是否应该从合并的过去的专家或原始基础模型开始？我们应该在每个时间步骤中合并所有模型吗？哪些合并技术最适合时间合并？是否应该使用不同的策略来初始化培训并部署模型？为了回答这些问题，我们提出了一个称为时间的统一框架 - 模型专业知识的时间整合 - 定义了跨三个轴合并的时间模型：（1）初始化阶段，（2）部署阶段，以及（3）合并技术。利用时间，我们研究了跨模型大小，计算预算和学习范围的时间模型在FOMO-IN-FLUX基准测试上。我们跨时间的全面实验套件使我们能够发现有关时间模型合并的关键见解，从而更好地了解当前的挑战和最佳实践，以实现有效的时间模型合并。

### A cautionary tale on the cost-effectiveness of collaborative AI in real-world medical applications 
[[arxiv](https://arxiv.org/abs/2412.06494)] [[cool](https://papers.cool/arxiv/2412.06494)] [[pdf](https://arxiv.org/pdf/2412.06494)]
> **Authors**: Francesco Cremonesi,Lucia Innocenti,Sebastien Ourselin,Vicky Goh,Michela Antonelli,Marco Lorenzi
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 关于现实世界中协作AI的成本效益的警示性故事
- **领域**: 机器学习
- **摘要**: 背景。作为一种协作学习范式，联合学习（FL）在敏感的医疗保健应用中获得了协作AI的广泛欢迎。然而，FL的实际实施列出了技术和组织挑战，因为它通常需要复杂的沟通基础设施。在这种情况下，基于共识的学习（CBL）可能代表了一个有希望的协作学习替代方案，这要归功于将本地知识相结合到联合决策系统中，同时可能会减少部署开销。方法。在这项工作中，我们提出了在广泛的协作医学数据分析方案中的一组FL和CBL方法的准确性和成本效益的广泛基准。该基准包括7个不同的医疗数据集，包括3个机器学习任务，8种不同的数据模式以及涉及3至23个客户的多中心设置。发现。我们的结果表明，CBL是FL的成本效益替代品。当在考虑基准的基准中比较整个Medical数据集时，CBL方法提供了与FL的同等准确性。尽管如此，CBL可显着降低训练时间和通信成本（分别为15倍和60倍降低）（p <0.05）。解释。这项研究对在现实世界中的应用中的部署进行了新的观点，而采用具有成本效益的方法对实现AI的可持续性和民主化而减轻了广泛的计算资源的需求对实现AI的可持续性和民主化起来至关重要。

### Can foundation models actively gather information in interactive environments to test hypotheses? 
[[arxiv](https://arxiv.org/abs/2412.06438)] [[cool](https://papers.cool/arxiv/2412.06438)] [[pdf](https://arxiv.org/pdf/2412.06438)]
> **Authors**: Nan Rosemary Ke,Danny P. Sawyer,Hubert Soyer,Martin Engelcke,David P Reichert,Drew A. Hudson,John Reid,Alexander Lerchner,Danilo Jimenez Rezende,Timothy P Lillicrap,Michael Mozer,Jane X Wang
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 基础模型可以在交互式环境中积极收集信息以检验假设吗？
- **领域**: 机器学习,机器学习
- **摘要**: 虽然解决问题是基础模型的标准评估任务，但问题解决问题的关键组成部分（积极，战略性地收集信息以检验假设）并未经过深入研究。为了评估互动环境中基础模型的信息收集能力，我们介绍了一个框架，其中模型必须通过迭代地推理其先前收集的信息并提出其下一个探索性动作来确定影响隐藏奖励功能的因素，以在每个步骤中最大化信息增益。我们在基于文本的环境中实现了此框架，该环境提供了一个紧密控制的设置并启用高通量参数扫描，并在体现的3D环境中，这需要解决与现实世界应用更相关的多模式交互的复杂性。我们进一步研究了自我纠正和推理时间增加等方法是否提高信息收集效率。在需要确定单个奖励功能的相对简单的任务中，我们发现LLM的信息收集能力接近最佳。但是，当模型必须确定奖励特征的连词时，性能是次优的。在性能中的打击部分是由于模型将任务描述转换为策略的模型，部分是由于模型在使用其内在内存内存中的有效性。尽管不完美的视觉对象识别降低了其在3D体现的情况下从收集的信息中得出结论的准确性，但在文本和3D体现环境中的性能都是可比的。对于基于单功能的奖励，我们发现较小的模型的性能更好。对于基于连接的奖励，将自校正纳入模型可以提高性能。

### A Self-guided Multimodal Approach to Enhancing Graph Representation Learning for Alzheimer's Diseases 
[[arxiv](https://arxiv.org/abs/2412.06212)] [[cool](https://papers.cool/arxiv/2412.06212)] [[pdf](https://arxiv.org/pdf/2412.06212)]
> **Authors**: Zhepeng Wang,Runxue Bao,Yawen Wu,Guodong Liu,Lei Yang,Liang Zhan,Feng Zheng,Weiwen Jiang,Yanfu Zhang
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 一种增强阿尔茨海默氏病的图表表示学习的自导体多模式方法
- **领域**: 机器学习,人工智能
- **摘要**: 图神经网络（GNN）是旨在处理不规则结构数据的强大机器学习模型。但是，它们的通用设计通常证明不足以分析阿尔茨海默氏病（AD）中的脑连接组，强调了将域知识纳入最佳性能的需求。将与广告相关的知识注入GNN是一项复杂的任务。现有的方法通常依赖于计算机科学家与域专家之间的协作，这既可以是时间密集型又需要资源。为了解决这些局限性，本文介绍了一种新颖的自引导，具有知识的多模式GNN，该GNN自主将领域知识纳入模型开发过程。我们的方法将领域知识概念化为自然语言，并引入了一种专业的多模式GNN，能够利用这种未经灌输的知识来指导GNN的学习过程，从而可以改善模型性能并增强预测的可解释性。为了评估我们的框架，我们策划了有关AD的最新同行评审论文的全面数据集，并将其与多个现实世界广告数据集集成在一起。实验结果证明了我们方法提取相关领域知识，提供基于图的AD诊断的解释并提高GNN的整体性能的能力。与域专家的手动设计相比，这种方法为AD注入域知识提供了更可扩展，更有效的替代方案，从而提高了AD诊断的预测准确性和可解释性。

### AmCLR: Unified Augmented Learning for Cross-Modal Representations 
[[arxiv](https://arxiv.org/abs/2412.07979)] [[cool](https://papers.cool/arxiv/2412.07979)] [[pdf](https://arxiv.org/pdf/2412.07979)]
> **Authors**: Ajay Jagannath,Aayush Upadhyay,Anant Mehta
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: 16 pages, 2 figures
- **标题**: AMCLR：跨模式表示的统一增强学习
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 对比学习已成为表示学习的关键框架，这是SimClr和Clip等单峰和双峰应用中的进步的基础。为了解决诸如大批量依赖性和双峰之类的基本局限性，诸如SOGCLR诸如全球对比目标的随机优化之类的方法。受SOGCLR的效率和适应性的启发，我们引入了针对双峰视觉模型量身定制的AMCLR和XAMCLR目标功能，以进一步增强对比度学习的鲁棒性。 AMCLR整合了不同的增强量，包括文本释义和图像转换，以加强对比度表示的对齐，将批量尺寸限制在几百个样本中，与需要32,768的批次大小不同以产生合理的结果。 XAMCLR通过在原始模式和增强模态之间进行模式内对齐，从而进一步扩展了此范式，以获得更丰富的特征学习。这些进步产生了一个更具弹性和可概括的对比学习过程，旨在克服缩放和增强多样性的瓶颈。由于我们已经在现有的SOGCLR上建立了框架，因此我们能够以更少的计算资源来展示改进的表示质量，从而为可扩展和强大的多模式学习建立基础。

### Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning 
[[arxiv](https://arxiv.org/abs/2412.07909)] [[cool](https://papers.cool/arxiv/2412.07909)] [[pdf](https://arxiv.org/pdf/2412.07909)]
> **Authors**: Can Yaras,Siyi Chen,Peng Wang,Qing Qu
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: The first two authors contributed equally to this work
- **标题**: 解释和减轻对比多模式学习中的模态差距
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 多模式学习最近已获得了巨大的知名度，证明了各种零击分类任务以及一系列感知和生成应用的令人印象深刻的表现。通过通过对比度学习学习共享表示空间，诸如对比的语言图像预处理（剪辑）之类的模型旨在桥接不同的模式，例如图像和文本。尽管取得了成功，但尚未对多模式学习的工作机制有充分的理解。值得注意的是，这些模型经常表现出一种方式差距，其中不同的方式在共享表示空间内占据了不同的区域。在这项工作中，我们通过表征梯度流学习动力学来深入分析模态差距的出现。具体而言，我们确定了不匹配的数据对的关键作用以及在训练过程中导致和延续模态差距的可学习温度参数。此外，我们的理论见解是通过实践剪辑模型的实验来验证的。这些发现为减轻方式差距提供了原则指导，包括适当的温度调度和方式交换策略。此外，我们证明，闭合模式差距可改善诸如Image-Text检索等任务的性能。

### Comparative Analysis of Deep Learning Approaches for Harmful Brain Activity Detection Using EEG 
[[arxiv](https://arxiv.org/abs/2412.07878)] [[cool](https://papers.cool/arxiv/2412.07878)] [[pdf](https://arxiv.org/pdf/2412.07878)]
> **Authors**: Shivraj Singh Bhatti,Aryan Yadav,Mitali Monga,Neeraj Kumar
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: 6 pages, 5 figures. Presented at IEEE CICT 2024. The paper discusses the application ofmultimodaldata and training strategies in EEG-based brain activity classification
- **标题**: 使用脑电图进行有害大脑活动检测的深度学习方法的比较分析
- **领域**: 机器学习,人工智能,信号处理,神经元和认知
- **摘要**: 有害大脑活动的分类，例如癫痫发作和定期排放，在神经关怀中起着至关重要的作用，从而及时诊断和干预。脑电图（EEG）提供了一种无创的方法来监测大脑活动，但是脑电图信号的手动解释是耗时的，并且严重依赖专家判断。这项研究对深度学习体系结构进行了比较分析，包括卷积神经网络（CNN），视觉变压器（VITS）和EEGNET，应用于使用RAW EEG数据和通过连续波浪转换（CWT）生成的RAW EEG数据和时间频率表示，应用于有害大脑活动的分类。我们评估这些模型的性能使用多模式数据表示，包括高分辨率光谱图和波形数据，并引入多阶段训练策略来提高模型鲁棒性。我们的结果表明，培训策略，数据预处理和增强技术对于模拟成功与建筑选择一样至关重要，多阶段的TinyVit和EfficityNet表现出卓越的性能。这些发现强调了强大的培训制度在实现准确有效的脑电图分类中的重要性，为在临床实践中部署AI模型提供了宝贵的见解。

### Anomaly detection using Diffusion-based methods 
[[arxiv](https://arxiv.org/abs/2412.07539)] [[cool](https://papers.cool/arxiv/2412.07539)] [[pdf](https://arxiv.org/pdf/2412.07539)]
> **Authors**: Aryan Bhosale,Samrat Mukherjee,Biplab Banerjee,Fabio Cuzzolin
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 使用基于扩散的方法的异常检测
- **领域**: 机器学习
- **摘要**: 本文探讨了基于扩散的模型对异常检测的实用性，重点是它们在识别紧凑和高分辨率数据集中偏差方面的功效。基于扩散的架构，包括使用重建目标，评估了其性能，以评估其性能。通过利用这些模型的优势，这项研究基准了其对传统异常检测方法的性能，例如隔离森林，一级SVM和COPOD。结果表明，基于扩散的方法在处理复杂的现实世界检测任务中具有卓越的适应性，可伸缩性和鲁棒性。关键发现突出了重建误差在增强检测准确性方面的作用，并强调了这些模型对高维数据集的可扩展性。未来的方向包括优化编码器架构和探索多模式数据集，以进一步推进基于扩散的异常检测。

### From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields 
[[arxiv](https://arxiv.org/abs/2412.08731)] [[cool](https://papers.cool/arxiv/2412.08731)] [[pdf](https://arxiv.org/pdf/2412.08731)]
> **Authors**: Miltiadis Kofinas,Samuele Papa,Efstratios Gavves
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: Preprint. Source code: https://github.com/mkofinas/neomlp
- **标题**: 从MLP到NEOMLP：利用自我注意力为神经领域
- **领域**: 机器学习,人工智能,机器学习
- **摘要**: 神经场（NEF）最近已成为一种编码各种方式的时空信号的最新方法。尽管NEF在重建单个信号方面取得了成功，但除了缺乏强大且可扩展的条件机制之外，还阻碍了它们在下游任务中用作表示的表示，例如分类或分割。在这项工作中，我们从连接主义原则到设计基于MLP的新体系结构的灵感，我们将其称为NEOMLP。我们从MLP开始，将其视为图形，然后将其从多方图形图转换为配备高维功能的输入，隐藏和输出节点的完整图。我们在所有节点之间进行自我发挥作用。 NEOMLP具有通过隐藏和输出节点进行调节的内置机制，该机制充当一组潜在代码，因此，NEOMLP可以直接用作条件神经场。我们通过拟合高分辨率信号（包括多模式音频数据数据）来证明我们方法的有效性。此外，我们通过使用单个主链体系结构学习特定于实例的潜在代码集，然后将其用于下游任务，从而符合最新的最新方法，从而拟合了神经表示的数据集。源代码在https://github.com/mkofinas/neomlp上进行开源。

### GenPlan: Generative Sequence Models as Adaptive Planners 
[[arxiv](https://arxiv.org/abs/2412.08565)] [[cool](https://papers.cool/arxiv/2412.08565)] [[pdf](https://arxiv.org/pdf/2412.08565)]
> **Authors**: Akash Karthikeyan,Yash Vardhan Pant
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: Accepted in AAAI 2025. Project page: https://aku02.github.io/projects/genplan/
- **标题**: Genplan：作为自适应计划者的生成序列模型
- **领域**: 机器学习,人工智能
- **摘要**: 序列模型通过利用先前收集的示范，在行为计划中表现出了显着的成功。但是，解决多任务任务仍然是一个重大挑战，尤其是当计划者必须适应看不见的约束和任务时，例如发现目标和解锁门。由于以下原因，这种行为计划问题要挑战：a）代理未能适应通过其奖励功能学到的单个任务，b）仅当只有在平面环境中接受培训时，就无法推广到新环境，例如那些有墙壁和锁定的门。因此，最先进的决策方法仅限于在培训示范中得到很好代表的任务，并且可以在短（时间）计划视野中解决。为了解决这个问题，我们提出了Genplan：一种随机和自适应计划者，它利用离散模型进行生成序列建模，从而实现样品有效的探索和开发。该框架依赖于迭代的剥夺程序来产生一系列目标和行动。这种方法捕获了多模式的动作分布，并促进了目标和任务发现，从而推广到分布外任务和环境，即任务不是培训数据的一部分。我们通过多个仿真环境证明了方法的有效性。值得注意的是，在自适应计划任务上，Genplan的表现优于最先进的方法，在自适应计划任务上，代理商适应了多任务任务，同时利用了单芯范围的任务。我们的代码可在https://github.com/cl2-uwaterloo/genplan上找到。

### From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons 
[[arxiv](https://arxiv.org/abs/2412.08442)] [[cool](https://papers.cool/arxiv/2412.08442)] [[pdf](https://arxiv.org/pdf/2412.08442)]
> **Authors**: Andrew Szot,Bogdan Mazoure,Omar Attia,Aleksei Timofeev,Harsh Agrawal,Devon Hjelm,Zhe Gan,Zsolt Kira,Alexander Toshev
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 从多模式LLM到通才体现的代理：方法和课程
- **领域**: 机器学习
- **摘要**: 我们研究了多模式大语言模型（MLLM）的能力，可以解决这些模型通常受过培训的传统语言和视觉任务的不同领域。具体而言，我们的重点在于体现的AI，游戏，UI控制和计划等领域。为此，我们引入了将MLLM适应通才体现的代理（GEA）的过程。 GEA是一个单一的统一模型，能够通过多物种作用令牌将自己跨越这些各种领域的自身地接地。 GEA在大量的体现体验数据集和交互式模拟器中的在线RL上接受了有监督的学习培训。我们探索开发这种模型所需的数据和算法选择。我们的发现揭示了跨域数据和在线RL培训对建筑通才代理商的重要性。与其他通才模型和特定于基准的方法相比，最终的GEA模型实现了强大的概括性能，可以看到各种基准的任务。

### Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision? 
[[arxiv](https://arxiv.org/abs/2412.08174)] [[cool](https://papers.cool/arxiv/2412.08174)] [[pdf](https://arxiv.org/pdf/2412.08174)]
> **Authors**: Zihao Li,Lecheng Zheng,Bowen Jin,Dongqi Fu,Baoyu Jing,Yikun Ban,Jingrui He,Jiawei Han
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: Preprint, 25 pages
- **标题**: 图形神经网络能否在文本监督下学习语言？
- **领域**: 机器学习,人工智能,社交和信息网络
- **摘要**: 尽管在互联网尺度的图像培训对形成对比的语言图像预训练（剪辑）上，在建立视觉模型中取得了巨大的成功，但使用剪贴管构建可转移的图形神经网络（GNN）具有挑战性，因为这三个基本问题：稀有标记的数据和文本监督，下降层次的概念，以及概念上的不同级别的概念，以及domains domains domains domains domains domains domains domains domains domains domains domains gaps domains。在这项工作中，为了解决这些问题，我们利用多模式及时学习有效地将预先训练的GNN适应下游任务和数据，只给出了几个具有语义标记的样本，每个样本都具有极为弱的文本监督。我们的新范式通过同时学习图形提示和文本提示，将图形直接嵌入与大语言模型（LLM）相同的空间中。为此，我们改善了最新的图形提示方法，然后提出了第一种图形语言多模式提示方法，以利用预训练模型中的知识。值得注意的是，由于对微调的监督不足，在我们的范式中，预先培训的GNN和LLM被冷冻，因此可学习的参数比对任何预培训模型进行微调少得多。通过对现实世界数据集的广泛实验，我们在几次，多任务级别和跨域设置中演示了范式的出色性能。此外，我们构建了第一个夹具式零击分类原型，该原型可以概括为GNNs以极度较弱的文本监督而看不见的类。

### Neptune: The Long Orbit to Benchmarking Long Video Understanding 
[[arxiv](https://arxiv.org/abs/2412.09582)] [[cool](https://papers.cool/arxiv/2412.09582)] [[pdf](https://arxiv.org/pdf/2412.09582)]
> **Authors**: Arsha Nagrani,Mingda Zhang,Ramin Mehran,Rachel Hornung,Nitesh Bharadwaj Gundavarapu,Nilpa Jha,Austin Myers,Xingyi Zhou,Boqing Gong,Cordelia Schmid,Mikhail Sirotenko,Yukun Zhu,Tobias Weyand
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: No comments
- **标题**: 海王星：基准长期视频理解的长轨道
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 我们介绍了Neptune，这是一种长期视频理解的基准，需要在长期范围内和不同方式进行推理。许多现有的视频数据集和模型都集中在短片段（10s-30）上。尽管确实存在一些长的视频数据集，但通常可以通过在视频中使用的强大图像模型来解决它们，并且通常以高成本手动注释。为了减轻这两个问题，我们提出了一个可扩展的数据集创建管道，该管道利用大型型号（VLMS和LLMS）自动生成密集的，时间分配的视频字幕，以及艰难的询问答案诱饵集，用于视频段（长达15分钟）。我们的数据集海王星涵盖了广泛的长期视频推理能力，并由一个强调多模式推理的子集组成。由于现有的开放式问题回答的指标是基于规则的，或者可能依赖于专有模型，因此我们提供了一个新的基于开源模型的公制的公制宝石，以在Neptune上对开放式回答进行评分。基准评估表明，大多数当前的开源长视频模型在海王星上的表现较差，尤其是在测试时间顺序，计数和状态变化的问题上。通过Neptune，我们旨在刺激能够理解长视频的更高级模型的开发。该数据集可从https://github.com/google-deepmind/neptune获得

### A Wander Through the Multimodal Landscape: Efficient Transfer Learning via Low-rank Sequence Multimodal Adapter 
[[arxiv](https://arxiv.org/abs/2412.08979)] [[cool](https://papers.cool/arxiv/2412.08979)] [[pdf](https://arxiv.org/pdf/2412.08979)]
> **Authors**: Zirun Guo,Xize Cheng,Yangyang Wu,Tao Jin
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Accepted at AAAI 2025
- **标题**: 通过多模式景观漫步：通过低级序列多模式适配器的有效传输学习
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 有效的转移学习方法（例如基于适配器的方法）在单峰模型和视觉模型中取得了巨大的成功。但是，现有方法在微调多模型模型中面临两个主要挑战。首先，它们是为视觉任务而设计的，无法扩展到有两个以上模式的情况。其次，它们表现出有限的模式和缺乏效率之间相互作用的开采。为了解决这些问题，在本文中，我们提出了低级序列多模式适配器（Wander）。我们首先使用外部产品以有效的方式将来自不同模式的信息融合在一起。为了提高效率，我们使用CP分解将张量分解为等级的组件并实现大量参数降低。此外，我们实施了令牌级的低级分解，以提取模态之间更细粒度的特征和序列关系。通过这些设计，Wander可以以参数效率的方式启用不同模态序列之间的令牌级相互作用。我们在具有不同数量的模式的数据集上进行了广泛的实验，在这些数据集上，始终如一地徘徊在最先进的传输学习方法上。结果充分证明了流浪的有效性，效率和普遍性。

### Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges 
[[arxiv](https://arxiv.org/abs/2412.11427)] [[cool](https://papers.cool/arxiv/2412.11427)] [[pdf](https://arxiv.org/pdf/2412.11427)]
> **Authors**: Chandan K Reddy,Parshin Shojaee
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: AAAI 2025
- **标题**: 通过生成AI：进步，机遇和挑战进行科学发现
- **领域**: 机器学习,人工智能
- **摘要**: 科学发现是一个复杂的认知过程，几个世纪以来一直驱动人类知识和技术进步。尽管人工智能（AI）在自动化科学推理，模拟和实验方面取得了重大进展，但我们仍然缺乏能够执行自动长期科学研究和发现的综合AI系统。本文研究了科学发现的AI的当前状态，强调了大语模型和其他应用于科学任务的其他AI技术的最新进展。然后，我们概述了为开发更全面的AI系统的主要挑战和有希望的研究方向，包括对以科学为中心的AI代理，改进的基准和评估指标，多模式科学表示以及结合推理，定理证明和数据驱动器模型的统一框架。解决这些挑战可能会导致变革性的AI工具，以加速跨学科的进步科学发现。

### ViSymRe: Vision-guided Multimodal Symbolic Regression 
[[arxiv](https://arxiv.org/abs/2412.11139)] [[cool](https://papers.cool/arxiv/2412.11139)] [[pdf](https://arxiv.org/pdf/2412.11139)]
> **Authors**: Da Li,Junping Yin,Jin Xu,Xinxin Li,Juan Zhang
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: Visymre：视觉引导的多模式符号回归
- **领域**: 机器学习,人工智能,符号计算
- **摘要**: 符号回归自动搜索数学方程式以揭示数据集中的基本机制，与黑匣子模型相比，可解释性增强。传统上，符号回归被认为是纯粹是数字驱动的，并且不足以对视觉信息在增强此过程中的潜在贡献的关注。在处理高维和复杂的数据集时，现有的符号回归模型通常效率低下，并且倾向于产生过度复杂的方程式，从而使后续的机制分析变得复杂。在本文中，我们提出了视觉引导的多模式符号回归模型，即Visymre，该模型系统地探讨了视觉信息如何改善符号回归的各种指标。与传统模型相比，我们提出的模型具有以下创新：（1）它整合了三种方式：视觉，符号和数字以增强符号回归，使该模型能够从每种方式的优势中受益； （2）它建立了一个可以从历史经验中学习的元学习框架，以有效地解决新的符号回归问题； （3）它强调了方程式的简单性和结构合理性，而不仅仅是数值拟合。广泛的实验表明，我们提出的模型具有强大的概括能力和抗噪声性。它在拟合效果，简单性和结构准确性方面生成了均优于最先进的仅数字基线的方程式，从而能够促进准确的机制分析和理论模型的发展。

### Deep Learning-Based Noninvasive Screening of Type 2 Diabetes with Chest X-ray Images and Electronic Health Records 
[[arxiv](https://arxiv.org/abs/2412.10955)] [[cool](https://papers.cool/arxiv/2412.10955)] [[pdf](https://arxiv.org/pdf/2412.10955)]
> **Authors**: Sanjana Gundapaneni,Zhuo Zhi,Miguel Rodrigues
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 深度学习基于胸部X射线图像和电子健康记录的2型糖尿病的无创筛查
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 早期检测2型糖尿病（T2DM）的必要性受到其无症状的发作和对次优临床诊断测试的依赖性的挑战，这有助于其广泛的全球流行率。尽管对非侵入性T2DM筛选工具的研究已提前，但由于广泛的功能工程要求，传统的机器学习方法仍然限于单型号输入。相比之下，深度学习模型可以利用多模式数据来对患者的健康状况有更全面的了解。然而，胸部X射线（CXR）成像的潜力是最常见的医疗程序之一，仍然没有被逐渐倍增。这项研究评估了CXR图像与其他非侵入性数据源的整合，包括电子健康记录（EHRS）和心电图信号，用于T2DM检测。利用从模拟物IV数据库精心编译的数据集，我们研究了两个深融合范式：一种基于融合的早期融合多模式变压器和一个模块化关节融合resnet-lstM架构。端到端训练的RESNET-LSTM模型的AUROC为0.86，仅使用9863个训练样本，超过了仅CXR的基线2.3％。这些发现证明了多模式框架内CXR的诊断价值，用于早日识别高危个体。此外，数据集预处理管道也已发布，以支持该领域的进一步研究。

### Higher Order Transformers: Enhancing Stock Movement Prediction On Multimodal Time-Series Data 
[[arxiv](https://arxiv.org/abs/2412.10540)] [[cool](https://papers.cool/arxiv/2412.10540)] [[pdf](https://arxiv.org/pdf/2412.10540)]
> **Authors**: Soroush Omranpour,Guillaume Rabusseau,Reihaneh Rabbany
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: KDD 2024 Workshop on Machine Learning in Finance
- **标题**: 高阶变压器：增强对多模式时间序列数据的库存运动预测
- **领域**: 机器学习,统计金融
- **摘要**: 在本文中，我们通过引入高级变压器来应对预测金融市场中的股票变动的挑战，这是一种旨在处理多元时间序列数据的新型体系结构。我们将自我发挥机制和变压器体系结构扩展到更高阶段，有效地捕获了跨时间和变量的复杂市场动态。为了管理计算复杂性，我们提出了使用张量分解并采用内核注意力的潜在大型注意张量的低级别近似，从而将复杂性降低到相对于数据大小的线性。此外，我们提出了一个编码器模型，该模型利用历史价格和相关推文中的多模式信号整合了技术和基本分析。我们在StockNet数据集上进行的实验证明了我们方法的有效性，强调了其增强金融市场中股票运动预测的潜力。

### Efficient Generative Modeling with Residual Vector Quantization-Based Tokens 
[[arxiv](https://arxiv.org/abs/2412.10208)] [[cool](https://papers.cool/arxiv/2412.10208)] [[pdf](https://arxiv.org/pdf/2412.10208)]
> **Authors**: Jaehyeon Kim,Taehong Moon,Keon Lee,Jaewoong Cho
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 基于残留矢量量化令牌的有效生成建模
- **领域**: 机器学习
- **摘要**: 我们探讨了在矢量定量生成模型中使用残留矢量量化（RVQ）对高保真产生的使用。这种量化技术通过采用更深入的代币来保持较高的数据保真度。但是，在生成模型中增加令牌数会导致推理速度较慢。为此，我们引入了Resgen，这是一种有效的基于RVQ的离散扩散模型，该模型在不损害采样速度的情况下生成高保真样本。我们的关键思想是对矢量嵌入集体令牌而不是单个的直接预测。此外，我们证明，我们提出的令牌掩蔽和多键预测方法可以使用离散的扩散过程和变异推断在原则性的概率框架内制定。我们验证了所提出的方法对不同模态的两个具有挑战性的任务的功效和概括性：在Imagenet 256x256上的条件图像产生}和零拍的文本对语音合成。实验结果表明，Resgen在这两个任务中的表现都优于自回旋的对应物，在不损害采样速度的情况下提供了出色的性能。此外，与类似尺寸的基线模型相比，随着我们扩展RVQ的深度，我们的生成模型表现出增强的产生或更快的采样速度。可以在https://resgen-genai.github.io上找到项目页面

### DLF: Disentangled-Language-Focused Multimodal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2412.12225)] [[cool](https://papers.cool/arxiv/2412.12225)] [[pdf](https://arxiv.org/pdf/2412.12225)]
> **Authors**: Pan Wang,Qiang Zhou,Yawen Wu,Tianlong Chen,Jingtong Hu
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: AAAI 2025 accepted
- **标题**: DLF：以语言为中心的多模式分析
- **领域**: 机器学习,人工智能,计算语言学,多媒体
- **摘要**: 多模式情感分析（MSA）利用语言，视觉和音频等异质方式来增强对人情绪的理解。尽管现有模型通常集中在跨模态或直接融合异质方式上提取共享信息，但由于对所有方式的平等处理以及模态对之间的信息的相互传递，这种方法可以引入冗余和冲突。为了解决这些问题，我们提出了一个以语言为中心的（DLF）多模式表示学习框架，该框架结合了一个功能分离模块，以单独的模态共享和特定于模态的信息。为了进一步降低冗余并增强语言为目标的特征，引入了四种几何措施来完善分离过程。以语言为中心的吸引子（LFA）进一步开发出来，通过通过语言指导的跨注意机制利用互补方式特定信息来增强语言表示。该框架还采用层次预测来提高整体准确性。在两个流行的MSA数据集CMU-MOSI和CMU-MOSEI上进行了广泛的实验，证明了所提出的DLF框架所取得的显着性能增长。全面的消融研究进一步验证了特征分离模块，以语言吸引子和等级预测的有效性。我们的代码可在https://github.com/pwang322/dlf上找到。

### GAMED: Knowledge Adaptive Multi-Experts Decoupling for Multimodal Fake News Detection 
[[arxiv](https://arxiv.org/abs/2412.12164)] [[cool](https://papers.cool/arxiv/2412.12164)] [[pdf](https://arxiv.org/pdf/2412.12164)]
> **Authors**: Lingzhi Shen,Yunfei Long,Xiaohao Cai,Imran Razzak,Guanming Chen,Kang Liu,Shoaib Jameel
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: GAMED：知识自适应多模型的多模式假新闻检测解耦
- **领域**: 机器学习,人工智能
- **摘要**: 多模式假新闻检测通常涉及建模异质数据源，例如视觉和语言。现有的检测方法通常依赖于融合效果和跨模式的一致性来对内容进行建模，从而使理解每种模态如何影响预测准确性的方式变得复杂。此外，这些方法主要基于静态特征建模，因此很难适应不同数据模式之间的动态变化和关系。本文为多模型建模开发了一种具有重要新颖的方法，即GAMED，该方法着重于通过模态解耦来生成独特和歧视性特征，以增强跨模式的协同作用，从而在检测过程中优化了整体性能。 GAMED利用多个并行的专家网络来完善功能和预示的语义知识，以提高专家在信息选择和观点共享方面的能力。随后，根据各自的专家的意见对每种模式的特征分布进行自适应调整。 Gamed还引入了一种新型的分类技术，以动态管理不同方式的贡献，同时提高决策的解释性。 Fakeddit和Yang数据集的实验结果表明，GAMES的性能要比最近开发的最先进的模型更好。可以在https://github.com/slz0925/gamed上访问源代码。

### SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout 
[[arxiv](https://arxiv.org/abs/2412.12129)] [[cool](https://papers.cool/arxiv/2412.12129)] [[pdf](https://arxiv.org/pdf/2412.12129)]
> **Authors**: Chiyu Max Jiang,Yijing Bai,Andre Cornman,Christopher Davis,Xiukun Huang,Hong Jeon,Sakshum Kulshrestha,John Lambert,Shuangyu Li,Xuanyu Zhou,Carlos Fuertes,Chang Yuan,Mingxing Tan,Yin Zhou,Dragomir Anguelov
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-17
> **comment**: Accepted to NeurIPS 2024
- **标题**: SceneDiffuser：高效且可控的驾驶模拟初始化和推出
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 现实且互动场景模拟是自动驾驶汽车（AV）开发的关键先决条件。在这项工作中，我们介绍了SceneDiffuser，这是一个现场级扩散的事先设计，用于交通模拟。它提供了一个统一的框架，该框架解决了模拟的两个关键阶段：场景初始化，其中涉及生成初始流量布局和场景推出，其中包含了代理行为的闭环模拟。尽管已证明扩散模型在学习现实和多模式分布方面有效，但仍然存在一些挑战，包括可控性，在闭环模拟中维持现实主义以及确保推理效率。为了解决这些问题，我们引入了摊销扩散以进行仿真。这种新颖的扩散降级范式摊销了对未来模拟步骤的降低的计算成本，从而大大降低了每次推出步骤的成本（推理步骤减少16倍），同时也减轻了闭环错误。我们通过引入广义硬约束，一种简单而有效的推理时间约束机制以及通过大型语言模型（LLM）的几个提示来进一步增强可控性。我们对模型缩放的研究表明，增加的计算资源可显着改善整体模拟现实主义。我们证明了方法对Waymo Open Sims Agents挑战的有效性，实现了最高的开环性能和扩散模型中最佳的闭环性能。

### CiTrus: Squeezing Extra Performance out of Low-data Bio-signal Transfer Learning 
[[arxiv](https://arxiv.org/abs/2412.11695)] [[cool](https://papers.cool/arxiv/2412.11695)] [[pdf](https://arxiv.org/pdf/2412.11695)]
> **Authors**: Eloy Geenjaar,Lie Lu
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: 柑橘：从低数据的生物信号转移学习中挤出额外的性能
- **领域**: 机器学习
- **摘要**: 生物信号的转移学习最近已成为通过小型生物信号数据集在下游任务上提高预测性能的重要技术。最近的作品表明，在大型数据集（例如脑电图）上进行自我监督任务的神经网络模型预先培训，用线性分类头代替了自我观察的头部，并在不同的下游生物信号数据集（例如，EMG或ECG）上微调模型可以在这些数据集中提高这些数据列表的性能。在本文中，我们提出了一种新的卷积转换型混合模型体系结构，其掩盖自动编码用于低数据，以进行低数据生物信号转移学习，介绍了基于频率的掩盖自动编码任务，采用更全面的评估框架，并评估（多态）（多态）预训练的预定效果。我们还引入了一种极大的性能方法，该方法将具有不同时间长度和抽样率不同的下游数据集与原始的预训练数据集对齐。我们的发现表明，混合模型的仅卷积部分可以在某些低数据下游任务上实现最先进的性能。我们的完整模型通常会进一步提高性能。在基于变压器的模型的情况下，我们发现预训练特别改善了下游数据集的性能，多模式的预训练通常会进一步提高这些收益，而我们的基于频率的预训练在最低和最高数据方案中平均表现最好。

### Multimodal LLM for Intelligent Transportation Systems 
[[arxiv](https://arxiv.org/abs/2412.11683)] [[cool](https://papers.cool/arxiv/2412.11683)] [[pdf](https://arxiv.org/pdf/2412.11683)]
> **Authors**: Dexter Le,Aybars Yunusoglu,Karn Tiwari,Murat Isik,I. Can Dikmen
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted at IEEE Symposium Series on Computational Intelligence (SSCI) 2025
- **标题**: 智能运输系统的多模式LLM
- **领域**: 机器学习
- **摘要**: 在不断发展的运输系统的景观中，集成大型语言模型（LLMS）为跨各种应用程序的智能决策提供了有前途的前沿。本文介绍了一个新颖的三维框架，该框架封装了应用程序，机器学习方法和硬件设备的交集，尤其是强调LLM的作用。我们的框架不使用多个机器学习算法，而是使用一个以数据为中心的LLM架构，可以分析时间序列，图像和视频。我们探讨了LLM如何增强运输中的数据解释和决策。我们将此LLM框架应用于不同的传感器数据集，包括牛津雷达机器人，D-Behavior（D-set），Nuscenes，Motional和Comma2K19等来源的时间序列数据和视觉数据。目的是简化数据处理工作流程，减少部署多个模型的复杂性，并使智能运输系统更加高效和准确。这项研究是使用最先进的硬件进行的，利用AMD RTX 3060 GPU和Intel I9-12900处理器的计算能力进行。实验结果表明，我们的框架在这​​些数据集中达到了91.33 \％的平均准确性，并且在时间序列数据中观察到最高的精度（92.7 \％），从而展示了该模型在处理诸如运动计划和预测性维护等任务的顺序信息方面必不可少的。通过我们的探索，我们证明了LLM在处理运输部门内多模式数据方面的多功能性和功效，最终在现实世界中提供了对其应用的见解。我们的发现与更广泛的会议主题保持一致，强调了LLM在推进运输技术方面的变革潜力。

### EvoLlama: Enhancing LLMs' Understanding of Proteins via Multimodal Structure and Sequence Representations 
[[arxiv](https://arxiv.org/abs/2412.11618)] [[cool](https://papers.cool/arxiv/2412.11618)] [[pdf](https://arxiv.org/pdf/2412.11618)]
> **Authors**: Nuowei Liu,Changzhi Sun,Tao Ji,Junfeng Tian,Jianxin Tang,Yuanbin Wu,Man Lan
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: EVOLLAMA：通过多模式结构和序列表示增强LLM对蛋白质的理解
- **领域**: 机器学习,人工智能
- **摘要**: 当前用于理解蛋白质的大型语言模型（LLMS）主要将氨基酸序列视为文本模式。同时，蛋白质语言模型（PLM），例如ESM-2，已经从天然蛋白质序列的宇宙中学到了大量的顺序进化知识。此外，基于结构的编码器，例如蛋白质MPNN，通过图神经网络学习蛋白质的结构信息。但是，尚未探索蛋白质编码器的掺入是否可以增强对LLM的蛋白质理解。为了弥合这一差距，我们提出了Evollama，这是一种连接基于结构的编码器的多模式框架，基于序列的蛋白质编码器和用于蛋白质理解的LLM。 EVOLLAMA由蛋白质结构编码器，ESM-2蛋白质序列编码器，一个对齐蛋白质和文本表示的多模式投影仪以及Llama-3文本解码器。要训​​练Evollama，我们将其按照面向蛋白质的说明和蛋白质性能预测数据集进行了微调，该数据集通过自然语言指令模板进行了口头表达。我们的实验表明，Evollama的蛋白质理解能力已显着增强，在零光设置中的表现平均超过了其他面向蛋白质的LLM，平均超过了最先进的基线，并超过了最先进的基线，而受监督的细微调整平均为6％。在蛋白质属性预测数据集上，我们的方法取得了有希望的结果，这些结果与最先进的特定任务基准竞争。我们将在未来版本中发布代码。

### Explicit and Implicit Graduated Optimization in Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2412.11501)] [[cool](https://papers.cool/arxiv/2412.11501)] [[pdf](https://arxiv.org/pdf/2412.11501)]
> **Authors**: Naoki Sato,Hideaki Iiduka
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted at AAAI-25
- **标题**: 深度神经网络中的明确和隐式毕业优化
- **领域**: 机器学习
- **摘要**: 渐变优化是一种全局优化技术，用于通过用噪声平滑目标函数并逐渐完善解决方案来最大程度地减少多模式非凸功能。本文通过实验评估了从先前的研究中得出的最佳噪声调度的显式渐变优化算法的性能，并讨论了其局限性。它使用传统的基准功能和经验损失功能来评估现代神经网络体系结构。此外，本文扩展了隐式毕业的优化算法，该算法基于以下事实：SGD的优化过程中随机噪声隐含地平滑目标函数，以动量分析其收敛性，并通过具有重新网络体系的图像分类任务来证明其有效性。

### Modality-Inconsistent Continual Learning of Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.13050)] [[cool](https://papers.cool/arxiv/2412.13050)] [[pdf](https://arxiv.org/pdf/2412.13050)]
> **Authors**: Weiguo Pian,Shijian Deng,Shentong Mo,Yunhui Guo,Yapeng Tian
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: 多模式大语言模型的模态持续学习
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 在本文中，我们介绍了模态持续学习（MICL），这是一种新的多模式大语言模型（MLLMS）的新的持续学习场景，涉及具有不一致的模态（图像，音频或视频）和不同任务类型（字幕或问答或提出问题）的任务。与现有的仅视觉或模态燃料设置不同，MICL结合了模态和任务类型的变化，这两者都驱动了灾难性的遗忘。为了应对这些挑战，我们提出了MOINCL，它采用了伪定位的生成模块来减轻由以前看过的模式中的任务类型变化引起的遗忘。它还结合了基于教学的知识蒸馏，以保留在引入新模型的模型的能力。我们使用总共六个任务进行基准MICL，并进行实验以验证我们提出的MOINCL的有效性。实验结果突出了MOINCL的优势，显示出比代表性和最先进的持续学习基线的显着改善。

### Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning 
[[arxiv](https://arxiv.org/abs/2412.12953)] [[cool](https://papers.cool/arxiv/2412.12953)] [[pdf](https://arxiv.org/pdf/2412.12953)]
> **Authors**: Moritz Reuss,Jyothish Pari,Pulkit Agrawal,Rudolf Lioutikov
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: 有效的扩散变压器政策与专家DeOisers的混合物进行多任务学习
- **领域**: 机器学习,机器人技术
- **摘要**: 扩散策略已被广泛用于模仿学习，提供了几种吸引人的特性，例如产生多模式和不连续的行为。随着模型越来越大以捕获更复杂的功能，其计算需求增加，如最近的缩放定律所示。因此，继续当前的体系结构将呈现一个计算障碍。为了解决这一差距，我们提出了混合降级专家（模式）作为模仿学习的新政策。模式超过了当前的最新变压器扩散策略，同时通过稀疏的专家和噪声条件的路由启用参数有效的扩展，通过专家缓存将活动参数降低了40％，推理成本降低了90％。我们的体系结构将这种有效的缩放与噪声条件的自我发项机制相结合，从而在不同的噪声水平上更有效地降解。模式在四个既定的模仿学习基准（Calvin and Libero）的134个任务上实现了最先进的表现。值得注意的是，通过在不同的机器人数据数据上进行审议模式，我们在Calvin ABC上实现了4.01，在Libero-90上获得了0.95。与默认扩散变压器体系结构相比，它在4个基准中平均超过了基于CNN的基于CNN的和变压器扩散策略的57％，而使用少90％和较少的活动参数。此外，我们在模式的组件上进行了全面的消融，为设计有效且可扩展的变压器体系结构提供了用于扩散策略的见解。代码和演示可在https://mbreuss.github.io/mode_diffusion_policy/上找到。

### Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery 
[[arxiv](https://arxiv.org/abs/2412.13667)] [[cool](https://papers.cool/arxiv/2412.13667)] [[pdf](https://arxiv.org/pdf/2412.13667)]
> **Authors**: ChengAo Shen,Zhengzhang Chen,Dongsheng Luo,Dongkuan Xu,Haifeng Chen,Jingchao Ni
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: No comments
- **标题**: 与工具增强的LLM代理探索多模式集成以进行精确的因果发现
- **领域**: 机器学习,人工智能,方法论
- **摘要**: 因果推论是跨领域决策的必要基础，例如Smart Health，用于药物发现和AIOPS的AI。传统的统计因果发现方法虽然建立良好，但主要依赖于观察数据，并且经常忽略因果关系中固有的语义提示。大型语言模型（LLM）的出现以负担得起的方式利用了语义提示来获取知识驱动的因果发现，但是因果发现的LLM的发展落后于其他领域，尤其是在探索多模式数据方面。为了弥合差距，我们引入了MATMCD，这是一种由工具启动的LLM驱动的多代理系统。 MATMCD有两个关键代理：一个数据增强代理，可检索和处理模态增强数据的数据，以及一个因知识驱动推理的多模式数据集成多模式数据的因果约束代理。内部工作的精致设计确保了代理商的成功合作。我们在七个数据集中进行的实证研究表明，多模式增强因果发现的重要潜力。

### A Universal Model for Human Mobility Prediction 
[[arxiv](https://arxiv.org/abs/2412.15294)] [[cool](https://papers.cool/arxiv/2412.15294)] [[pdf](https://arxiv.org/pdf/2412.15294)]
> **Authors**: Qingyue Long,Yuan Yuan,Yong Li
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 人类流动性预测的通用模型
- **领域**: 机器学习,人工智能
- **摘要**: 预测人类流动性对于城市规划，交通管制和应急响应至关重要。流动行为可以分为个人和集体，这些行为通过各种流动性数据（例如个体轨迹和人群流动）记录。由于移动性数据的不同方式，个体轨迹和人群流具有紧密的耦合关系。人群流源于单个轨迹的自下而上的聚合，而人群流量施加的约束构成了这些单独的轨迹。现有的移动性预测方法仅限于单个任务，这是由于单个轨迹和人群流量之间的模态差异。在这项工作中，我们旨在统一移动性预测，以突破特定于任务模型的局限性。我们提出了一个通用的人类流动性预测模型（名为Unimob），该模型可以应用于个体轨迹和人群流。 UNIMOB利用多视图的迁移率令牌，将轨迹和流数据同时转换为时空令牌，从而通过扩散变压器结构促进统一的顺序建模。为了弥合这两种数据模式的不同特征之间的差距，我们实现了一种新型的双向个体和集体对准机制。这种机制使从不同的迁移率数据中学习常见的时空模式，从而促进了轨迹和流动预测的相互增强。在现实世界数据集上进行的广泛实验验证了我们模型在轨迹和流动预测中的优势。尤其是在嘈杂和稀缺的数据方案中，我们的模型在MAPE和准确性@5的最高性能提高了14％和25％。

### MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data 
[[arxiv](https://arxiv.org/abs/2412.14810)] [[cool](https://papers.cool/arxiv/2412.14810)] [[pdf](https://arxiv.org/pdf/2412.14810)]
> **Authors**: Camillo Maria Caruso,Paolo Soda,Valerio Guarrasi
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-20
> **comment**: No comments
- **标题**: 玛丽亚：一种用于不完整医疗保健数据的多模式变压器模型
- **领域**: 机器学习,人工智能
- **摘要**: 在医疗保健中，多模式数据的整合对于开发全面的诊断和预测模型至关重要。但是，管理丢失的数据仍然是现实世界应用程序中的重大挑战。我们介绍了Maria（多模式的注意力弹性对不完整数据），这是一个基于新颖的变压器的深度学习模型，旨在通过中间融合策略来应对这些挑战。与依赖插补的常规方法不同，玛丽亚（Maria）利用了掩盖的自我发项机制，该机制仅处理可用数据而不产生合成值。这种方法使其能够有效地处理不完整的数据集，增强鲁棒性并最大程度地减少插入方法引入的偏差。我们在8个诊断和预后任务中对玛丽亚进行了10种最先进的机器学习和深度学习模型的评估。结果表明，玛丽亚（Maria）在性能和对不同水平的数据不完整水平方面的表现优于现有方法，从而强调了其在关键医疗保健应用中的潜力。

### A Coalition Game for On-demand Multi-modal 3D Automated Delivery System 
[[arxiv](https://arxiv.org/abs/2412.17252)] [[cool](https://papers.cool/arxiv/2412.17252)] [[pdf](https://arxiv.org/pdf/2412.17252)]
> **Authors**: Farzan Moosavi,Bilal Farooq
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 按需多模式3D自动递送系统的联盟游戏
- **领域**: 机器学习,优化与控制
- **摘要**: 我们引入了一个多模式的自主交付优化框架，作为一个联盟游戏，适用于在两个覆盖网络中运行的无人机和ADR，以解决在城市环境中的最后一英里交付，包括高密度的区域，基于道路的路由以及现实世界中的运营挑战。该问题定义为多个仓库拾取和交付，随着时间窗口的限制，时间窗口（例如车辆电池限制，优先时间窗口和建筑物障碍物）受到限制。随后，使用联盟游戏理论来研究模式之间的合作结构，以捕获车辆之间的战略合作如何提高整体路线效率。为此，一种广义的强化学习模型旨在评估成本分布和分配给不同属性和非空心核心的不同联盟的分配。我们的方法学利用了一种新型时空邻接邻里邻里图表网络和变形金刚使用异质的边缘增强注意模型来利用一种端到端的深度多代理策略梯度方法增强。在密西沙加市的案例研究中进行了几项数值实验，该案例研究的结果表明，尽管在图中包含了一个广泛的网络，但具有两种模式和复杂的训练结构，但模型仍在现实的运营限制和实现高质量的效果上，并且在现有的基于变压器和HEARSISICAL的范围内，并且可以很好地实现高质量的范围，并且可以很好地划分，并且在非质量方面的范围良好地划分了，并且在数据上既可以很好地均匀地进行分配。随机场景下的强劲性能会受到风速和方向的影响。

### Bag of Tricks for Multimodal AutoML with Image, Text, and Tabular Data 
[[arxiv](https://arxiv.org/abs/2412.16243)] [[cool](https://papers.cool/arxiv/2412.16243)] [[pdf](https://arxiv.org/pdf/2412.16243)]
> **Authors**: Zhiqiang Tang,Zihan Zhong,Tong He,Gerald Friedland
> **First submission**: 2024-12-19
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 带有图像，文本和表格数据的多模式汽车的技巧袋
- **领域**: 机器学习
- **摘要**: 本文研究了自动机器学习（AUTOML）的最佳实践。尽管以前的汽车努力主要集中在单峰数据上，但多模式的方面仍未探索。我们的研究深入研究了涉及图像，文本和表格数据的灵活组合的分类和回归问题。我们策划了一个来自不同现实世界应用程序的22个多模式数据集的基准测试，其中包括3种模式的所有4种组合。在这个基准测试中，我们仔细检查了与多模式融合策略，多模式数据增强，将表格数据转换为文本，跨模式对齐以及处理缺失的模态相关的设计选择。通过广泛的实验和分析，我们将有效策略的集合提炼成统一的管道，从而在各种数据集上实现了强劲的性能。

### Statistical Modeling of Univariate Multimodal Data 
[[arxiv](https://arxiv.org/abs/2412.15894)] [[cool](https://papers.cool/arxiv/2412.15894)] [[pdf](https://arxiv.org/pdf/2412.15894)]
> **Authors**: Paraskevi Chasani,Aristidis Likas
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: 30 pages, 9 figures
- **标题**: 单变量多模式数据的统计建模
- **领域**: 机器学习,机器学习
- **摘要**: 单形成构成一个关键属性，指示数据的单个密度模式的分组行为。我们提出了一种方法，将单变量数据通过数据密度的山谷点周围的递归分裂将单变量数据划分为单峰子集。对于Valley点检测，我们在经验累积密度函数（ECDF）图上引入了临界点的属性，该图提供了有关密度阀存在的指示。接下来，我们采用一种单峰数据建模方法，该方法为每个获得的单峰子集的统计模型以均匀混合模型（UMM）的形式提供。因此，以UMM的混合物的形式获得了初始数据集的层次统计模型，该模型称为单峰混合模型（UDMM）。所提出的方法是非参数，无参数，自动估计单峰子集的数量，并提供准确的统计模型，如集群和密度估计任务的实验结果所示。

### Measuring Cross-Modal Interactions in Multimodal Models 
[[arxiv](https://arxiv.org/abs/2412.15828)] [[cool](https://papers.cool/arxiv/2412.15828)] [[pdf](https://arxiv.org/pdf/2412.15828)]
> **Authors**: Laura Wenderoth,Konstantin Hemker,Nikola Simidjievski,Mateja Jamnik
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 测量多模型中的跨模式相互作用
- **领域**: 机器学习
- **摘要**: 在医疗保健中整合AI可以大大提高患者护理和系统效率。但是，AI系统（XAI）缺乏解释性阻碍了他们的临床采用，尤其是在使用越来越复杂的模型体系结构的多模式环境中。大多数现有的XAI方法都集中在单峰模型上，这些模型未能捕获跨模式相互作用对于理解多个数据源的综合影响至关重要。量化跨模式相互作用的现有方法仅限于两种模式，依靠标记的数据并取决于模型性能。这在医疗保健中是有问题的，XAI必须处理多个数据源并提供个性化的解释。本文介绍了Intershap，这是一个跨模式的交互评分，该评分解决了现有方法的局限性。 Intershap使用Shapley相互作用指数精确分离并量化单个模态及其相互作用的贡献，而无需近似。通过将开源实现与Shap软件包集成在一起，我们可以增强可重复性和易用性。我们表明，Intershap准确地测量了跨模式相互作用的存在，可以处理多种模式，并为各个样本提供详细的解释。此外，我们将Intershap应用于多模式医学数据集，并证明其适用于个性化解释。

### Beyond Human Data: Aligning Multimodal Large Language Models by Iterative Self-Evolution 
[[arxiv](https://arxiv.org/abs/2412.15650)] [[cool](https://papers.cool/arxiv/2412.15650)] [[pdf](https://arxiv.org/pdf/2412.15650)]
> **Authors**: Wentao Tan,Qiong Cao,Yibing Zhan,Chao Xue,Changxing Ding
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: AAAI 2025. The code is available at https://github.com/WentaoTan/SENA
- **标题**: 超越人类数据：通过迭代自我进化对齐多模式大语模型
- **领域**: 机器学习
- **摘要**: 人类的偏好一致性可以极大地增强多模式大语模型（MLLM），但是收集高质量的偏好数据是昂贵的。一个有希望的解决方案是自我进化策略，其中模型是对其生成的数据进行迭代训练的。但是，当前的技术仍然依赖于人类或GPT的数据，有时需要其他模型或地面真相答案。为了解决这些问题，我们提出了一个新颖的多模式自我进化框架，该框架使模型能够仅使用未注释的图像自主产生高质量的问题和答案。首先，我们实施了图像驱动的自我询问机制，允许模型根据图像内容创建和评估问题，如果它们是无关紧要的或无法回答的，则将其再生。这为回答生成奠定了坚实的基础。其次，我们介绍了一种自我增强技术，从图像字幕开始以提高答案质量。我们还使用损坏的图像生成被拒绝的答案，形成了不同的优先对以进行优化。最后，我们将图像内容对齐损失函数与直接偏好优化（DPO）损失一起结合起来，以减少幻觉，从而确保模型专注于图像内容。实验表明，我们的框架使用外部信息的方法竞争性能，为MLLM提供了更高效，更可扩展的方法。

### Multimodal Learning with Uncertainty Quantification based on Discounted Belief Fusion 
[[arxiv](https://arxiv.org/abs/2412.18024)] [[cool](https://papers.cool/arxiv/2412.18024)] [[pdf](https://arxiv.org/pdf/2412.18024)]
> **Authors**: Grigor Bezirganyan,Sana Sellami,Laure Berti-Équille,Sébastien Fournier
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 基于折扣信念融合的不确定性量化的多模式学习
- **领域**: 机器学习
- **摘要**: 多模式AI模型越来越多地用于医疗保健，金融和自动驾驶等领域，在这些领域中，从多种来源或模式（例如图像，文本，音频，视频，视频）中汲取信息。但是，有效地管理不确定性 - 由于噪音，证据不足或模式之间的冲突而产生的不确定性 - 对于可靠的决策至关重要。当前的不确定性感知的ML方法利用了证据平均或证据积累低估了高冲突的情况下的不确定性。此外，最先进的证据平均策略在非缔合性方面挣扎，并且未能扩展到多种方式。为了应对这些挑战，我们提出了一种新颖的多模式学习方法，并具有订单不变的证据融合，并引入了一种基于冲突的折现机制，当检测到不可靠的方式时，该方法将不确定的质量重新关注。我们提供理论分析和实验验证，表明与先前的工作不同，根据所提供的不确定性估计值有效地区分了冲突和非冲突样本，并且在基于不确定性的冲突检测中胜过先前的模型。

### A Novel Approach to Balance Convenience and Nutrition in Meals With Long-Term Group Recommendations and Reasoning on Multimodal Recipes and its Implementation in BEACON 
[[arxiv](https://arxiv.org/abs/2412.17910)] [[cool](https://papers.cool/arxiv/2412.17910)] [[pdf](https://arxiv.org/pdf/2412.17910)]
> **Authors**: Vansh Nagpal,Siva Likitha Valluru,Kausik Lakkaraju,Nitin Gupta,Zach Abdulrahman,Andrew Davison,Biplav Srivastava
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: arXiv admin note: substantial text overlap with arXiv:2406.13714
- **标题**: 一种新颖的方法来平衡餐点的便利性和营养，以及长期的小组建议以及对多模式食谱的推理及其在信标中的实施
- **领域**: 机器学习,人工智能
- **摘要**: "A common decision made by people, whether healthy or with health conditions, is choosing meals like breakfast, lunch, and dinner, comprising combinations of foods for appetizer, main course, side dishes, desserts, and beverages. Often, this decision involves tradeoffs between nutritious choices (e.g., salt and sugar levels, nutrition content) and convenience (e.g., cost and accessibility, cuisine type, food source type). We present a以数据驱动的方式考虑了可定制的膳食配置和时间范围的餐点。信标系统。”

### MixMAS: A Framework for Sampling-Based Mixer Architecture Search for Multimodal Fusion and Learning 
[[arxiv](https://arxiv.org/abs/2412.18437)] [[cool](https://papers.cool/arxiv/2412.18437)] [[pdf](https://arxiv.org/pdf/2412.18437)]
> **Authors**: Abdelmadjid Chergui,Grigor Bezirganyan,Sana Sellami,Laure Berti-Équille,Sébastien Fournier
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: ef:2024 IEEE International Conference on Big Data (BigData), Washington, DC, USA, 2024, pp. 3254-3257
- **标题**: Mixmas：基于抽样的搅拌机架构搜索多模式融合和学习的框架
- **领域**: 机器学习
- **摘要**: 为多模式数据融合选择合适的深度学习体系结构是一项具有挑战性的任务，因为它需要有效地集成和处理各种数据类型，每种数据类型都具有不同的结构和特征。在本文中，我们介绍了Mixmas，这是一个针对多模式学习量身定制的基于采样的混合架构搜索的新颖框架。我们的方法自动为给定的多模式学习（MML）任务选择了基于MLP的最佳体系结构。具体而言，Mixmas利用基于抽样的微基准测试策略来探索特定于模态编码器，融合功能和融合网络的各种组合，从而系统地识别最能满足任务性能指标的体系结构。

### Audiopedia: Audio QA with Knowledge 
[[arxiv](https://arxiv.org/abs/2412.20619)] [[cool](https://papers.cool/arxiv/2412.20619)] [[pdf](https://arxiv.org/pdf/2412.20619)]
> **Authors**: Abhirama Subramanyam Penamakuri,Kiran Chhatre,Akshat Jain
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: Accepted to ICASSP 2025
- **标题**: Audiopedia：有知识的音频质量请
- **领域**: 机器学习,多媒体,声音,音频和语音处理
- **摘要**: 在本文中，我们介绍了Audiopedia，这是一项名为Audio问题的新任务。与传统的音频答案（AQA）基准不同，仅凭音频就可以回答的简单查询，Audiopedia针对知识密集型问题。我们定义了三个子任务：（i）单个音频问题回答（S-AQA），其中根据单个音频示例回答问题，（ii）多声明问题答案（M-AQA），需要对多个音频样本进行推理，以及（iii）检索Audio Audio问题答案（R-AQA），该问题涉及回答的问题，这与回答有关。我们在这些子任务上基准了大型音频语言模型（LALMS），并观察到次优性能。为了解决这个问题，我们提出了一个通用框架，该框架可以适应任何LALM，使他们拥有知识推理能力。我们的框架有两个组件：（i）音频实体链接（AEL）和（ii）知识启动的音频大型多模型模型（KA2LM），它们共同提高了知识密集型AQA任务的性能。据我们所知，这是通过Audiopedia等知识密集型任务来解决高级音频理解的第一项工作。

### Multimodal Variational Autoencoder: a Barycentric View 
[[arxiv](https://arxiv.org/abs/2412.20487)] [[cool](https://papers.cool/arxiv/2412.20487)] [[pdf](https://arxiv.org/pdf/2412.20487)]
> **Authors**: Peijie Qiu,Wenhui Zhu,Sayantan Kumar,Xiwen Chen,Xiaotong Sun,Jin Yang,Abolfazl Razi,Yalin Wang,Aristeidis Sotiras
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: AAAI 2025
- **标题**: 多模式变异自动编码器：Barycentric视图
- **领域**: 机器学习,计算机视觉和模式识别,信息论
- **摘要**: 现实现象中自然存在多种信号方式，例如视觉和声音。最近，人们对学习生成模型（尤其是变异自动编码器（VAE））的兴趣越来越多，以进行多模式表示学习，尤其是在缺失模态的情况下。这些模型的主要目标是学习一种模态不变和特定于模态的表示，该表示表征了跨多种方式的信息。以前的多模式VAE的尝试主要通过专家的镜头来实现这一目标，从而将单峰推理分布与专家（POE），专家（MOE）的混合物或两者的组合汇总。在本文中，我们通过Barycenter镜头提供了多模式VAE的替代通用和理论表述。我们首先表明POE和MOE是Barycenters的特定实例，它通过最大程度地减少了对单峰推断分布的不对称加权KL差异而得出。我们的新颖配方将这两个barycenters扩展到了更灵活的选择，通过考虑不同类型的差异。特别是，我们探索了由2-Wasserstein距离定义的Wasserstein Barycenter，它通过捕获与KL Divergence相比捕获模态特异性和模态不变的表示，可以更好地保留单峰分布的几何形状。对三个多模式基准的实证研究证明了该方法的有效性。

### Election of Collaborators via Reinforcement Learning for Federated Brain Tumor Segmentation 
[[arxiv](https://arxiv.org/abs/2412.20253)] [[cool](https://papers.cool/arxiv/2412.20253)] [[pdf](https://arxiv.org/pdf/2412.20253)]
> **Authors**: Muhammad Irfan Khan,Elina Kontio,Suleiman A. Khan,Mojtaba Jafaritadi
> **First submission**: 2024-12-28
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: 通过加强联合脑肿瘤分段的加强学习选举合作者
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 联合学习（FL）可以在保留数据隐私的同时，可以跨分散数据集进行协作模型培训。但是，在动态FL环境中最佳选择参与的合作者仍然具有挑战性。我们提出了RL-HSIMAGG，这是一种新颖的增强学习（RL）和相似性加权聚合（SIMAGG）算法，使用谐波均值来管理离群数据点。本文提议应用多军匪徒算法来改善合作者的选择和模型概括。通过平衡探索探索权衡取舍，这些RL方法可以通过不同的数据集促进资源有效的培训。我们证明了联合脑病变分割的Epsilon-Greedy（EG）和上置信度结合（UCB）算法的有效性。在对内部和外部验证集的仿真实验中，与UCB合作者的RL-HSIMAGG优于所有指标的EG方法，在增强肿瘤（0.7334 vs 0.6797），肿瘤核心（0.7432 vs 0.6821）和整个Tumor（0.825225252 vs vs thumor core（0.7334 vs 0.6797）中，取得了较高的骰子得分（0.7334 vs 0.6797）。因此，对于联合肿瘤分割挑战（FETS 2024），我们认为UCB是多模式MRI的联合胶质母细胞瘤病变细分中的主要客户选择方法。总之，我们的研究表明，基于RL的合作者管理，例如使用UCB，可以在分布式学习环境中，尤其是在脑肿瘤分割等领域，有可能改善模型的鲁棒性和灵活性。

### ProtCLIP: Function-Informed Protein Multi-Modal Learning 
[[arxiv](https://arxiv.org/abs/2412.20014)] [[cool](https://papers.cool/arxiv/2412.20014)] [[pdf](https://arxiv.org/pdf/2412.20014)]
> **Authors**: Hanjing Zhou,Mingze Yin,Wei Wu,Mingyang Li,Kun Fu,Jintai Chen,Jian Wu,Zheng Wang
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: ef:AAAI 2025
- **标题**: ProtClip：功能信息蛋白质多模式学习
- **领域**: 机器学习,人工智能,生物分子
- **摘要**: 对齐蛋白质序列和生物描述的多模式预训练范式已经学习了一般蛋白质表示，并在各种下游应用中实现了有希望的性能。但是，由于对对齐的蛋白质文本配对数据的使用无效，并且缺乏有效的功能知识的预训练范式，因此这些作品仍然无法复制语言监督视觉基础模型的非凡成功。为了解决这些问题，本文策划了一个大规模的蛋白质文本配对的名为Protanno的数据集和属性驱动的采样策略，并引入了一种新型功能信息的蛋白质预训练范式。具体而言，采样策略决定了基于样本置信度和属性覆盖范围的概率，面对大规模嘈杂数据，平衡数据质量和数据数量。此外，以蛋白质特异性功能机制的重要性的启发，提出的范式通过两个细分市场的预训练目标明确模拟了蛋白质静态和动态功能段，并以功能性信息的方式注入细粒度的信息。利用所有这些创新，我们开发了ProtClip，这是一种多模式的基础模型，可全面代表功能感知的蛋白质嵌入。在5种类型的22种不同蛋白质基准上，包括蛋白质功能分类，突变效应预测，跨模式转化，语义相似性推断和蛋白质 - 蛋白质相互作用预测，我们的Protclip始终达到SOTA性能，在五个跨模式转换基准中的平均增长了75％，在59.9％的GO-BPC和39.7％中，平均增长了75％。实验结果验证了ProtClip作为蛋白质多模式基础模型的非凡潜力。

### Context-Aware Deep Learning for Multi Modal Depression Detection 
[[arxiv](https://arxiv.org/abs/2412.19209)] [[cool](https://papers.cool/arxiv/2412.19209)] [[pdf](https://arxiv.org/pdf/2412.19209)]
> **Authors**: Genevieve Lam,Huang Dongyan,Weisi Lin
> **First submission**: 2024-12-26
> **First announcement**: 2024-12-30
> **comment**: Presented as an Oral at International Conference on Acoustics, Speech and Signal Processing 2019, United Kingdom
- **标题**: 多态抑郁检测的上下文感知的深度学习
- **领域**: 机器学习
- **摘要**: 在这项研究中，我们专注于使用多模式机器学习（ML）从临床访谈中检测抑郁症的自动方法。我们的方法与其他成功的ML方法（例如，通过功能工程和端到端的深层神经网络进行抑郁症检测，利用遇险分析访谈语料库的抑郁症检测）区分了情境感知分析。我们提出了一种新颖的方法，该方法结合了：（1）基于文本数据的主题建模的预先训练的变压器与数据增强相结合； （2）用于声学特征建模的深1D卷积神经网络（CNN）。仿真结果证明了训练多模式深度学习模型的建议方法的有效性。我们的深1D CNN和变压器模型分别实现了音频和文本方式的最新性能。将它们结合在多模式框架中，还优于合并设置的最先进。可在https://github.com/genandlam/multi-modal-depression-detection上找到代码

## 多媒体(cs.MM:Multimedia)

该领域共有 6 篇论文

### OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows 
[[arxiv](https://arxiv.org/abs/2412.01169)] [[cool](https://papers.cool/arxiv/2412.01169)] [[pdf](https://arxiv.org/pdf/2412.01169)]
> **Authors**: Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Zichun Liao,Yusuke Kato,Kazuki Kozuka,Aditya Grover
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: 12 pages, 14 figures
- **标题**: Omniflow：多模式整流流的任何一代一代
- **领域**: 多媒体,计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 我们介绍了OmniFlow，这是一种新颖的生成模型，旨在任何一系列一代任务，例如文本到图像，文本到原告和音频图像综合。 OmniFlow推进了文本到图像模型中使用的整流流（RF）框架，以处理多种模式的关节分布。它的表现优于以前在各种任务上的任何对任何模型，例如文本对图像和文本构成的综合。我们的工作提供了三个关键的贡献：首先，我们将RF扩展到多模式设置，并引入一种新颖的指导机制，使用户能够灵活地控制生成的输出中不同方式之间的对齐。其次，我们提出了一种新颖的体系结构，该架构扩展了稳定扩散3的文本对图像MMDIT体系结构，并启用音频和文本生成。扩展的模块可以单独鉴定，并与Vanilla文本对图像MMDIT合并以进行微调。最后，我们对大规模音频和文本生成的整流流变压器的设计选择进行了全面研究，为优化各种方式的性能提供了宝贵的见解。该代码将在https://github.com/jacklishufan/omniflows上找到。

### Multimodal Sentiment Analysis Based on Causal Reasoning 
[[arxiv](https://arxiv.org/abs/2412.07292)] [[cool](https://papers.cool/arxiv/2412.07292)] [[pdf](https://arxiv.org/pdf/2412.07292)]
> **Authors**: Fuhai Chen,Pengpeng Huang,Xuri Ge,Jie Huang,Zishuo Bao
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 基于因果推理的多模式情感分析
- **领域**: 多媒体,计算语言学
- **摘要**: 随着多媒体的快速发展，近年来，从单峰文本情感分析到多模式图像文本情感分析的转变已获得了学术和工业的关注。但是，多模式的情感分析受非峰数据偏见的影响，例如，由于明确的情感语义，文本情感是误导性的，导致最终情感分类的准确性较低。在本文中，我们提出了一种新型的反事实多模式分析框架（CF-MSA），使用因果反事实推断来构建多模式情感因果推断。 CF-MSA通过单峰偏差的直接影响减轻了直接影响，并通过区分方式之间的治疗变量来确保跨模态的异质性。此外，考虑到模式之间的信息互补性和偏见差异，我们提出了一个新的优化目标，以有效地整合不同的方式并减少每种模式中固有的偏见。在两个公共数据集的MVSA单格和MVSA-Multiple上进行的实验结果表明，所提出的CF-MSA具有出色的偏见能力，并实现了新的最先进的性能。我们将发布代码和数据集，以促进未来的研究。

### Enhancing Modality Representation and Alignment for Multimodal Cold-start Active Learning 
[[arxiv](https://arxiv.org/abs/2412.09126)] [[cool](https://papers.cool/arxiv/2412.09126)] [[pdf](https://arxiv.org/pdf/2412.09126)]
> **Authors**: Meng Shen,Yake Wei,Jianxiong Yin,Deepu Rajan,Di Hu,Simon See
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: 11 pages, ACMMM Asia 2024, Oral Presentation
- **标题**: 增强多模式冷启动主动学习的模态表示和对齐
- **领域**: 多媒体,人工智能,机器学习
- **摘要**: 训练多峰模型需要大量标记的数据。积极学习（AL）旨在降低标签成本。大多数AL方法采用温暖的启动方法，这些方法依靠足够的标记数据来训练一个可以评估未标记数据的不确定性和多样性的良好模型。但是，当组装数据集时，标记的数据通常通常很少，从而导致了一个冷启动的问题。此外，大多数AL方法很少解决多模式数据，突出了该领域的研究差距。我们的研究通过开发一种两阶段的多模式冷启动活跃学习（MMCSAL）来解决这些问题。首先，我们观察到模态差距，这是不同模态的表示质心之间的显着距离，当时仅将跨模式配对信息用作自学信号。当我们计算单模式和跨模式距离时，这种模态差距会影响数据选择过程。为了解决这个问题，我们介绍了Uni-Modal原型以弥合模式差距。其次，传统的AL方法通常在多模式的场景中步履蹒跚，在多模式场景中，模态之间的对齐方式被忽略了。因此，我们建议通过正规化增强跨模式对齐，从而提高了AL中选定的多模式数据对的质量。最后，我们的实验证明了MMCSAL在三个多模式数据集中选择多模式数据对的功效。

### Leveraging User-Generated Metadata of Online Videos for Cover Song Identification 
[[arxiv](https://arxiv.org/abs/2412.11818)] [[cool](https://papers.cool/arxiv/2412.11818)] [[pdf](https://arxiv.org/pdf/2412.11818)]
> **Authors**: Simon Hachmeier,Robert Jäschke
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: accepted for presentation at NLP for Music and Audio (NLP4MusA) 2024
- **标题**: 利用在线视频的用户生成的元数据进行封面歌曲标识
- **领域**: 多媒体,信息检索
- **摘要**: YouTube是封面歌曲的丰富来源。由于平台本身是根据视频而不是歌曲来组织的，因此封面的检索并不是一件容易的事。封面歌曲标识的字段解决了此问题，并提供了通常依赖音频内容的方法。但是，包括YouTube上可用的用户生成的视频元数据有望改善识别结果。在本文中，我们提出了一种在线视频平台上的封面歌曲标识的多模式方法。我们使用排名模型将实体分辨率模型与基于音频的方法相结合。我们的发现暗示，利用用户生成的元数据可以稳定YouTube上的封面歌曲识别性能。

### Towards Expressive Video Dubbing with Multiscale Multimodal Context Interaction 
[[arxiv](https://arxiv.org/abs/2412.18748)] [[cool](https://papers.cool/arxiv/2412.18748)] [[pdf](https://arxiv.org/pdf/2412.18748)]
> **Authors**: Yuan Zhao,Rui Liu,Gaoxiang Cong
> **First submission**: 2024-12-24
> **First announcement**: 2024-12-25
> **comment**: Accepted by ICASSP 2025
- **标题**: 通过多尺度多模式上下文互动迈向表现力的视频配音
- **领域**: 多媒体,计算语言学,声音,音频和语音处理
- **摘要**: 自动视频配音（AVD）产生的语音与唇部运动和脚本的面部情感一致。最近的研究重点是建模多模式上下文以增强韵律表现力，但忽略了两个关键问题：1）上下文中的多尺度韵律表达属性会影响当前句子的韵律。 2）上下文中的韵律提示与当前句子相互作用，从而影响了最终的韵律表达。为了应对这些挑战，我们提出了M2CI-Dubber，这是AVD的多尺度多模式上下文相互作用方案。该方案包括两个共享的M2CI编码器，以对多尺度的多模式上下文进行建模，并促进其与当前句子的深入相互作用。通过在上下文中为每种模式提取全局和局部特征，利用基于注意力的机制进行聚集和交互，并采用基于相互作用的图形注意力网络进行融合，提出的方法可以增强当前句子中综合语音的韵律表达。化学数据集上的实验表明，我们的模型在配音表现力中优于基准。代码和演示可在\ textColor [rgb] {0.93,0.0,0.47} {https://github.com/ai-s2-lab/m2ci-dubber}中获得。

### ChartAdapter: Large Vision-Language Model for Chart Summarization 
[[arxiv](https://arxiv.org/abs/2412.20715)] [[cool](https://papers.cool/arxiv/2412.20715)] [[pdf](https://arxiv.org/pdf/2412.20715)]
> **Authors**: Peixin Xu,Yujuan Ding,Wenqi Fan
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: No comments
- **标题**: 图表：图表摘要的大型视觉语言模型
- **领域**: 多媒体,计算语言学
- **摘要**: 图表摘要侧重于从图表中提取关键信息并用自然语言解释，对于通过有效且可访问的数据分析生成和提供见解至关重要。图表理解和摘要的传统方法通常依赖于多阶段管道，这可能会在视觉和文本信息之间产生次优的语义一致性。相比之下，最近开发的基于LLM的方法更依赖于基础图像或语言的能力，同时忽略了图表数据的特征及其相关挑战。为了解决这些限制，我们提出了ChartAdapter，这是一个新颖的轻型变压器模块，旨在弥合图表和文本摘要之间的差距。 ChartAdapter采用可学习的查询向量来从图表数据中提取隐式语义，并结合了跨模式对齐投影仪，以增强视觉到语言生成学习。通过将ChartAdapter与LLM集成，我们可以启用端到端培训和有效的图表汇总。为了进一步增强培训，我们引入了三阶段的分层培训程序，并开发了专门策划图表摘要的大型数据集，包括190,618个样本。标准图表到文本测试集的实验结果表明，我们的方法在生成高质量图表摘要方面显着优于现有方法，包括最先进的模型。消融研究进一步验证了关键组成部分在图表板中的有效性。这项工作突出了量身定制的基于LLM的方法，以提高图表理解，并为该领域的未来研究奠定了坚实的基础。

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

该领域共有 1 篇论文

### Evolution of Thought: Diverse and High-Quality Reasoning via Multi-Objective Optimization 
[[arxiv](https://arxiv.org/abs/2412.07779)] [[cool](https://papers.cool/arxiv/2412.07779)] [[pdf](https://arxiv.org/pdf/2412.07779)]
> **Authors**: Biqing Qi,Zhouyi Qian,Yiang Luo,Junqi Gao,Dong Li,Kaiyan Zhang,Bowen Zhou
> **First submission**: 2024-11-24
> **First announcement**: 2024-12-11
> **comment**: No comments
- **标题**: 思想的演变：通过多目标优化的多样化和高质量推理
- **领域**: 神经和进化计算,人工智能
- **摘要**: 由于多模式大型语言模型（MLLM）越来越多地应用于复杂的推理任务，因此推理路径的多样性和质量成为影响其性能的关键因素。尽管当前的方法旨在通过扩展路径扩展提高推理质量，但它们经常忽略推理路径和有效信息共享的多样性，从而导致本地最佳和效率低下。为了应对这些挑战，我们提出了思想演变（EOT），这是一个多目标框架，旨在通过培养高质量和多样化的推理路径来改善推理。具体而言，我们介绍了非主导的分类遗传算法II进行多目标优化，利用跨界和突变算子来促进推理解决方案中的更大多样性。此外，我们提出了一种凝结 - 聚集机制，以聚类和消除冗余路径，促进了父母节点之间的信息共享的改进，并最终提高了推理过程的效率和质量。对各种视觉语言和语言推理任务的验证实验表明，与其他竞争基线相比，EOT实现了卓越的推理性能和效率。我们的研究提供了有关MLLM的启发式推理框架设计的新颖观点。

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

该领域共有 1 篇论文

### The Streetscape Application Services Stack (SASS): Towards a Distributed Sensing Architecture for Urban Applications 
[[arxiv](https://arxiv.org/abs/2411.19714)] [[cool](https://papers.cool/arxiv/2411.19714)] [[pdf](https://arxiv.org/pdf/2411.19714)]
> **Authors**: Navid Salami Pargoo,Mahshid Ghasemi,Shuren Xia,Mehmet Kerem Turkcan,Taqiya Ehsan,Chengbo Zang,Yuan Sun,Javad Ghaderi,Gil Zussman,Zoran Kostic,Jorge Ortiz
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: Streetscape应用程序服务堆栈（SASS）：迈向城市应用的分布式传感架构
- **领域**: 网络和互联网架构,计算机视觉和模式识别,分布式、并行和集群计算,机器学习
- **摘要**: 随着城市人口的增长，城市变得越来越复杂，推动了相互联系的传感系统的部署，以实现智能城市的愿景。这些系统旨在通过将各种传感器与实时决策相结合的应用来提高安全性，流动性和生活质量。街道景观应用程序涉及行人安全和自适应交通管理依赖于管理分布式，异质传感器数据，跨时间和空间对齐信息的挑战，并启用实时处理。这些任务本质上是复杂的，通常很难扩展。 Streetscape Application Services Stack（SASS）通过三个核心服务解决了这些挑战：多模式数据同步，时空数据融合和分布式边缘计算。通过将这些功能构造为具有清晰语义的清晰，可组合的抽象，SASS可以使开发人员有效地扩展街道景观应用程序，同时最大程度地减少多模式集成的复杂性。我们在两个现实世界的测试床环境中评估了SASS：一个受控的停车场和一个城市交叉路口。这些测试床使我们能够在不同的条件下测试SASS，以证明其实际适用性。多模式数据同步服务将时间误差误差降低了88％，从而在50毫秒内达到了同步精度。时空数据融合服务提高了行人和车辆的检测准确性超过10％，利用了多机构集成。分布式边缘计算服务增加了系统吞吐量的数量级。这些结果共同显示了SASS如何提供支持实时，可扩展的城市应用程序所需的抽象和性能，从而弥合了传感基础架构和可操作的街道景观情报之间的差距。

## 机器人技术(cs.RO:Robotics)

该领域共有 25 篇论文

### PKRD-CoT: A Unified Chain-of-thought Prompting for Multi-Modal Large Language Models in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2412.02025)] [[cool](https://papers.cool/arxiv/2412.02025)] [[pdf](https://arxiv.org/pdf/2412.02025)]
> **Authors**: Xuewen Luo,Fan Ding,Yinsheng Song,Xiaofeng Zhang,Junnyong Loo
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: This paper has been accepted for presentation at ICONIP 2024
- **标题**: PKRD-COT：自动驾驶中多模式大型语言模型的统一链链
- **领域**: 机器人技术,人工智能
- **摘要**: 直接在自主驾驶环境中直接利用强大的多模式大型语言模型（MLLM）的功能，人们越来越兴趣。但是，设计和培训端到端自动驾驶模型的高成本和复杂性使它们对许多企业和研究实体都充满挑战。为了解决这个问题，我们的研究通过提出一个名为pkrd-cot的零射击链（零发出）提示设计，探索了MLLM无缝集成到自主驾驶系统中。 PKRD-COT基于自动驾驶的四个基本能力：感知，知识，推理和决策。这使其特别适合通过逐步模仿人类思维过程来理解和响应动态驾驶环境，从而在实时场景中增强决策。我们的设计使MLLM可以在没有事先经验的情况下解决问题，从而在非结构化的自主驾驶环境中增加了效用。在实验中，我们证明了跨自主驾驶任务的PKRD-COT的GPT-4.0表现出色，突出了其在自主驾驶场景中的有效性。此外，我们的基准分析揭示了PKRD-COT对其他MLLM的可行性，例如Claude，Llava1.6和Qwen-Vl-Plus。总体而言，这项研究为GPT-4.0和其他MLLM在自主驾驶中贡献了一个新颖而统一的及时设计框架，同时还通过全面的比较来严格评估这些广泛认识的MLLM在自主驾驶领域中的功效。

### Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control 
[[arxiv](https://arxiv.org/abs/2412.01034)] [[cool](https://papers.cool/arxiv/2412.01034)] [[pdf](https://arxiv.org/pdf/2412.01034)]
> **Authors**: Seongmin Park,Hyungmin Kim,Wonseok Jeon,Juyoung Yang,Byeongwook Jeon,Yoonseon Oh,Jungwook Choi
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 用于资源有效机器人控制的量化仿制学习学习
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **摘要**: 深度神经网络（DNN）基于视觉语言动作（VLA）模型（例如，通过解释多模式数据）在跨应用程序自动化复杂的决策方面具有变革性。但是，扩展这些模型会大大提高计算成本，这在机器人操纵和需要快速，准确响应的自动驾驶等领域带来了挑战。为了满足在资源有限的硬件上进行部署的需求，我们为基于IL的策略模型提出了一个新的量化框架，该策略模型可以微调参数，以增强培训期间对低位精度错误的鲁棒性，从而在受约束条件下保持效率和可靠性。我们通过代表性的机器人操纵对4位重量量化在真正的边缘GPU上的评估表明，我们的框架可实现高达2.5倍的加速和2.5倍的能量节省，同时保持准确性。对于4位重量和激活量化的自动驾驶模型，该框架可在低端GPU上实现高达3.7倍的加速和3.1倍节省。这些结果突出了在资源受限设备上部署基于IL的政策模型的实际潜力。

### Real-Time Metric-Semantic Mapping for Autonomous Navigation in Outdoor Environments 
[[arxiv](https://arxiv.org/abs/2412.00291)] [[cool](https://papers.cool/arxiv/2412.00291)] [[pdf](https://arxiv.org/pdf/2412.00291)]
> **Authors**: Jianhao Jiao,Ruoyu Geng,Yuanhang Li,Ren Xin,Bowen Yang,Jin Wu,Lujia Wang,Ming Liu,Rui Fan,Dimitrios Kanoulas
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-03
> **comment**: 12 pages, 9 figures, accepted to IEEE Transactions on Automation Science and Engineering
- **标题**: 在室外环境中自动导航的实时度量标准映射
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 编码人类优先知识的公制图表的创建代表了环境的高级抽象。但是，构建这样的地图构成了与多模式传感器数据融合，实时映射性能的融合以及结构和语义信息一致性的挑战。在本文中，我们介绍了一个在线度量语义映射系统，该系统利用LiDar-Visual-Visual惯性传感生成大型室外环境的全球度量标准网格地图。利用GPU加速度，我们的映射过程实现了出色的速度，无论场景尺度如何，框架处理都低于7ms。此外，我们将所得地图无缝地集成到现实世界中的导航系统中，从而在校园环境中实现了基于公制的地形评估和自主点对点导航。通过对包含24个序列的公共可用数据集和自收集的数据集进行的广泛实验，我们证明了我们的映射和导航方法的有效性。代码已公开发布：https：//github.com/gogojjh/cobra

### Realistic Corner Case Generation for Autonomous Vehicles with Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2412.00243)] [[cool](https://papers.cool/arxiv/2412.00243)] [[pdf](https://arxiv.org/pdf/2412.00243)]
> **Authors**: Qiujing Lu,Meng Ma,Ximiao Dai,Xuanhan Wang,Shuo Feng
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 具有多模式大语言模型的自动驾驶汽车的现实角病例生成
- **领域**: 机器人技术,人工智能
- **摘要**: 为了确保自动驾驶汽车（AV）系统的安全性和可靠性，角案件在模拟环境中在罕见且具有挑战性的条件下探索该系统的行为起着至关重要的作用。但是，当前的方法通常在满足各种测试需求和努力概括为新颖的高风险场景的努力方面通常缺乏，这些情况紧密反映了现实世界中的条件。为了应对这一挑战，我们提出了Autoscenario，这是一种多模式的大语言模型（LLM），用于现实角落病例。它将安全 - 关键现实世界的数据从多个来源转换为文本表示形式，从而使关键风险因素的概括在利用LLMS.Furthermore的广泛世界知识和高级推理能力的同时，它整合了来自Urban Mobility（Sumo）的模拟工具（SUMO）和Carla模拟器来简化和执行llms生成的代码。我们的实验表明，AutoScenario可以生成现实且具有挑战性的测试场景，这是针对特定的测试要求或文本描述量身定制的。此外，我们验证了其产生来自涉及风险情况的多模式现实世界数据的多样化和新颖方案的能力，利用LLMS的强大概括能力有效地模拟了广泛的角落病例。

### MLLM-Search: A Zero-Shot Approach to Finding People using Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.00103)] [[cool](https://papers.cool/arxiv/2412.00103)] [[pdf](https://arxiv.org/pdf/2412.00103)]
> **Authors**: Angus Fung,Aaron Hao Tan,Haitong Wang,Beno Benhabib,Goldie Nejat
> **First submission**: 2024-11-27
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: MLLM-Search：使用多模式模型寻找人的零击方法
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 在包括医疗保健环境在内的以人为本环境中的人进行机器人搜索，因为自主机器人需要在没有完整或任何事先了解其日程安排，计划或位置的情况下定位人员。此外，机器人需要能够适应可能影响一个人在环境中计划的实时事件。在本文中，我们介绍了MLLM-Search，这是一种新颖的零击人搜索体系结构，利用多模式大语言模型（MLLM）解决移动机器人的问题，该问题是在事件驱动的情况下以不同的用户时间表来搜索一个人。我们的方法引入了一种新颖的视觉提示方法，该方法通过生成空间接地的路点图，为机器人提供对环境的空间理解，从而通过拓扑图和语义标签代表可导航的航路点。这与区域规划师合并到MLLM中，该区域规划师根据与搜索方案的语义相关选择下一个搜索区域，以及通过我们通过我们独特的空间链链的促进促进方法来考虑语义相关的对象和局部空间上下文来生成搜索路径的航向点计划者。进行了广泛的3D逼真的实验，以验证MLLM-Search的性能，以搜索在不同环境中时间表变化的人。还进行了一项消融研究，以验证MLLM-Search的主要设计选择。此外，与最先进的搜索方法的比较研究表明，MLLM-Search在搜索效率方面优于现有方法。在建筑物的跨室地板上使用移动机器人进行的现实世界实验表明，MLLM-Search能够概括在新的看不见的环境中找到一个人。

### Conveying Emotions to Robots through Touch and Sound 
[[arxiv](https://arxiv.org/abs/2412.03300)] [[cool](https://papers.cool/arxiv/2412.03300)] [[pdf](https://arxiv.org/pdf/2412.03300)]
> **Authors**: Qiaoqiao Ren,Remko Proesmans,Frederick Bossuyt,Jan Vanfleteren,Francis Wyffels,Tony Belpaeme
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: 通过触摸和声音将情绪传达给机器人
- **领域**: 机器人技术,机器学习
- **摘要**: 可以通过细微的触摸手势传达人的情绪。但是，缺乏对如何通过触摸传达给机器人的情绪的理解。这项研究通过整合情感触觉表达的触觉和听觉感觉阅读，探索了基于触摸的情感表达对机器人的一致性。我们开发了一个压电性压力传感器，并分别使用了麦克风来模仿触摸和声通道。在与28名参与者的一项研究中，每个人都使用自发的触摸手势向机器人传达了10个情绪。我们的发现表明，参与者的情绪表达具有统计学意义。但是，某些情绪获得了较低的类内相关值。此外，某些具有相似唤醒或价水平的情绪在传达方式上没有显着差异。随后，我们构建了一个多模式的集成触摸和音频功能来解码10个情绪。支持向量机（SVM）模型表现出最高的精度，在10类中获得了40％，“注意”是最精确传达的情绪，其精度为87.65％。

### IRisPath: Enhancing Costmap for Off-Road Navigation with Robust IR-RGB Fusion for Improved Day and Night Traversability 
[[arxiv](https://arxiv.org/abs/2412.03173)] [[cool](https://papers.cool/arxiv/2412.03173)] [[pdf](https://arxiv.org/pdf/2412.03173)]
> **Authors**: Saksham Sharma,Akshit Raizada,Suresh Sundaram
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-05
> **comment**: No comments
- **标题**: Irispath：通过强大的IR-RGB融合来提高越野导航的成本量
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 农业，建筑，搜救和防御中的申请需要自动越野导航。传统的公路自主方法在动态地形上挣扎，导致越野条件下的车辆控制不佳。最近的深度学习模型已使用感知传感器以及动力学反馈在此类地形上进行导航。但是，这种方法具有范围的不确定性。诸如一天中的时间变化和天气的变化会影响模型的性能。我们提出了一个多模态融合网络“ iRASPATH”，能够使用热和RGB图像来在动态的天气和光条件下提供鲁棒性。为了帮助在该域中进一步起作用，我们还开放了一个带有热和RGB图像的日夜数据集以及伪标记，以供遍历。为了共同注册融合模型，我们还开发了一种新的方法，用于对热，激光镜和RGB摄像机的无目标外部校准，其翻译精度为+/- 1.7厘米，旋转精度为+/- 0.827Degrees。

### Self-supervised cost of transport estimation for multimodal path planning 
[[arxiv](https://arxiv.org/abs/2412.06101)] [[cool](https://papers.cool/arxiv/2412.06101)] [[pdf](https://arxiv.org/pdf/2412.06101)]
> **Authors**: Vincent Gherold,Ioannis Mandralis,Eric Sihite,Adarsh Salagame,Alireza Ramezani,Morteza Gharib
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 多模式路径计划的自制运输成本估算
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **摘要**: 在实际环境中运行的自主机器人通常面临关于如何最好地浏览周围环境的决定。在这项工作中，我们解决了此问题的一个特定实例：机器人如何自主决定遵循有关周围环境的高级目标和信息的能量最佳途径？为了解决这个问题，我们开发了一种自制的学习方法，该方法使机器人仅使用视觉输入来估算周围环境的运输成本。我们将方法应用于多模式移动性形态（M4），该机器人可以在其环境中驱动，飞行，segway和爬网。通过将我们的系统部署在现实世界中，我们表明我们的方法准确地将不同的运输成本分配给了各种类型的环境，例如草与光滑的道路。我们还强调了我们方法的低计算成本，该计算成本部署在NVIDIA JETSON ORIN NANO机器人计算单元上。我们认为，这项工作将允许多模式的机器人平台解锁其全部导航和勘探任务的潜力。

### What's the Move? Hybrid Imitation Learning via Salient Points 
[[arxiv](https://arxiv.org/abs/2412.05426)] [[cool](https://papers.cool/arxiv/2412.05426)] [[pdf](https://arxiv.org/pdf/2412.05426)]
> **Authors**: Priya Sundaresan,Hengyuan Hu,Quan Vuong,Jeannette Bohg,Dorsa Sadigh
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 这是什么举动？混合模仿通过显着点学习
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 虽然模仿学习（IL）为教授机器人的各种行为提供了有希望的框架，但学习复杂的任务仍然具有挑战性。现有的IL政策努力使视觉和空间变化有效地概括，即使是简单任务也是如此。在这项工作中，我们介绍了狮身人面像：基于点点的混合模仿和执行，这是一种灵活的IL政策，利用多模式观察（点云和腕部图像），以及低频，稀疏路线和高频，高频，浓烈的效果效应的效果运动的混合动作空间。给定的3D点云观测值，狮身人面像学会在点云中推断与任务相关的点，或显着点，这些点通过着重于语义上有意义的特征来支持空间概括。这些显着点是预测远程运动的航路点的锚点，例如在自由空间中达到目标姿势。一旦接近一个显着点，狮身人面像会学会切换到给定特写腕部图像的密集末端效果运动，以确保任务的精确阶段。通过利用不同的输入方式和动作表示的强度，用于不同的操纵阶段，狮身人面像以样品有效的，可推广的方式处理复杂的任务。我们的方法在4个现实世界和2个模拟任务中取得了86.7％的成功，在440次现实世界试验中，其下一个最佳最佳IL基线的表现平均超过了41.1％。 Sphinx还概括了新的观点，视觉干扰器，空间布置和执行速度，并以1.7倍的速度高于最具竞争力的基线。我们的网站（http://sphinx-manip.github.io）提供了用于数据收集，培训和评估的开源代码以及补充视频。

### VTD: Visual and Tactile Database for Driver State and Behavior Perception 
[[arxiv](https://arxiv.org/abs/2412.04888)] [[cool](https://papers.cool/arxiv/2412.04888)] [[pdf](https://arxiv.org/pdf/2412.04888)]
> **Authors**: Jie Wang,Mobing Cai,Zhongpan Zhu,Hongjun Ding,Jiwei Yi,Aimin Du
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: VTD：驱动程序状态和行为感知的视觉和触觉数据库
- **领域**: 机器人技术,人工智能
- **摘要**: 在自动驾驶汽车的领域，人车副驾驶系统引起了极大的研究关注。为了解决驾驶员状态和相互作用行为的主观不确定性，这对于人类在循环的共同驾驶系统的安全性至关重要，我们引入了一种新颖的视觉感知方法。利用驾驶模拟平台，已经开发了一个全面的数据集，该数据集包括在疲劳和干扰条件下的多模式数据。实验设置将驱动模拟与信号采集相结合，从15名受试者中产生600分钟的疲劳检测数据，并与17个驱动程序进行了102个接管实验。该数据集跨模态同步，是推进跨模式驱动器行为感知算法的强大资源。

### Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp Generation 
[[arxiv](https://arxiv.org/abs/2412.08468)] [[cool](https://papers.cool/arxiv/2412.08468)] [[pdf](https://arxiv.org/pdf/2412.08468)]
> **Authors**: Haosheng Li,Weixin Mao,Weipeng Deng,Chenyu Meng,Haoqiang Fan,Tiancai Wang,Ping Tan,Hongan Wang,Xiaoming Deng
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: 16 pages, 10 figures
- **标题**: 多graspllm：多模式LLM用于多手语义引导的掌握生成
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 多手语义掌握的生成旨在根据自然语言指示为不同的机器人手生成可行且在语义上适当的抓握姿势。尽管该任务非常有价值，但由于缺乏机器人手和物体之间精细粒度的联系描述的多手掌握数据集，这仍然是一项长期的艰巨任务。在本文中，我们介绍了多段式的，这是第一个具有自动接触注释的大规模多手掌握数据集。基于多重播种，我们提出了多格拉斯普尔姆（Multi-Graspllm），这是一种统一的语言引导的Grasp生成框架，该框架利用大型语言模型（LLM）处理可变长度序列，并在单个统一体系结构中为多种机器人手生成抓手。 Multi-Graspllm首先将编码点云特征和文本特征对齐统一的语义空间。然后，它会通过手感线性映射将随后将每个机器人手转换为每个机器人手的抓脉姿势。实验结果表明，我们的方法在现实世界实验和模拟器中都显着优于现有方法。更多信息可以在我们的项目页面https://multi-graspllm.github.io上找到。

### Grasp Diffusion Network: Learning Grasp Generators from Partial Point Clouds with Diffusion Models in SO(3)xR3 
[[arxiv](https://arxiv.org/abs/2412.08398)] [[cool](https://papers.cool/arxiv/2412.08398)] [[pdf](https://arxiv.org/pdf/2412.08398)]
> **Authors**: Joao Carvalho,An T. Le,Philipp Jahr,Qiao Sun,Julen Urain,Dorothea Koert,Jan Peters
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: GRASP扩散网络：从SO（3）XR3中具有扩散模型的部分点云中学习GRASP发电机
- **领域**: 机器人技术,机器学习
- **摘要**: 在许多机器人操纵任务中，单视摄像头成功地抓住对象至关重要。解决此问题的一种方法是利用仿真来创建大量对象和掌握姿势的数据集，然后学习有条件的生成模型，该模型可以在部署期间快速提示。但是，掌握姿势数据是高度多模式的，因为有几种方法可以掌握对象。因此，在这项工作中，我们学习了一个具有扩散模型的掌握生成模型，以给定对象的部分点云采样候选姿势。我们方法的一个新方面是考虑在旋转的流动空间中扩散，并提出避免碰撞的成本指导，以提高推断期间的掌握成功率。为了加速掌握抽样，我们使用扩散文献中的最新技术来实现更快的推理时间。我们在仿真和现实世界中显示，我们的方法可以从$ 90 \％$成功率的原始深度图像中掌握几个对象，并根据几个基线对其进行基准测试。

### FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model 
[[arxiv](https://arxiv.org/abs/2412.08261)] [[cool](https://papers.cool/arxiv/2412.08261)] [[pdf](https://arxiv.org/pdf/2412.08261)]
> **Authors**: Chongkai Gao,Haozhuo Zhang,Zhixuan Xu,Zhehao Cai,Lin Shao
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: 翻转：以流量为中心的生成计划作为通用操纵世界模型
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 我们旨在为世界模型开发一个基于模型的计划框架，可以通过仅使用语言和视觉输入的通用操纵任务来增加模型和数据预算来扩展。为此，我们提出了以流动为中心的生成计划（FLIP），这是一种基于模型的计划算法，具有三个关键模块：1。多模式流量生成模型作为通用动作动作提案模块； 2。流动条件的视频生成模型作为动力学模块；和3。视觉表示学习模型作为价值模块。鉴于初始图像和语言指令作为目标，翻转可以逐步搜索长马管流程和视频计划，以最大程度地提高折扣回报以完成任务。 Flip能够将跨物体，机器人和任务跨图像流作为一般操作表示的长摩根计划综合，并且密集的流量信息还为长途视频生成提供了丰富的指导。此外，合成的流和视频计划可以指导对机器人执行的低级控制政策的培训。对不同基准的实验表明，翻转可以提高长途视频计划综合的成功率和质量，并具有交互式世界模型属性，为未来的工作开放更广泛的应用程序。

### Semantic Scene Completion Based 3D Traversability Estimation for Off-Road Terrains 
[[arxiv](https://arxiv.org/abs/2412.08195)] [[cool](https://papers.cool/arxiv/2412.08195)] [[pdf](https://arxiv.org/pdf/2412.08195)]
> **Authors**: Zitong Chen,Chao Sun,Shida Nie,Chen Min,Changjiu Ning,Haoyu Li,Bo Wang
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: 12 pages,14 figures
- **标题**: 基于越野地形的基于语义场景完成的3D遍历性估计
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **摘要**: 由于缺乏结构化的道路以及存在复杂的障碍物，例如地形，植被和遮挡，越野环境对自动地面车辆提出了重大挑战。在这些条件下，主要是为结构化环境设计的主要是为结构化环境而设计的传统感知算法，从而导致不准确的遍历性估计。在本文中，提出了一种将激光斑点云与单眼图像相结合的新型多模式方法，该方法提出了从前面的角度产生茂密的遍历占用预测。通过集成多模式数据，可以增强环境特征提取，这对于复杂地形的准确占用估计至关重要。此外，引入了具有3D可遍历注释的数据集Rellis-OCC，并结合了几何特征，例如阶梯高度，斜率和不均匀度。通过对车辆障碍条件的全面分析和车身结构限制的结合，产生了四个遍布性成本标签：致命，中等成本，低成本和免费。实验结果表明，在3D横穿区域识别中，ordormer的表现优于现有方法，尤其是在具有不规则几何形状和部分遮挡的越野环境中。具体而言，与其他型号相比，OrdFormer在场景完成IOU的20 \％改进中实现了。所提出的框架可扩展并适应各种车辆平台，可以调整到占用网格参数以及对高级动态模型的集成以进行遍历性成本估算。

### Diffusion Predictive Control with Constraints 
[[arxiv](https://arxiv.org/abs/2412.09342)] [[cool](https://papers.cool/arxiv/2412.09342)] [[pdf](https://arxiv.org/pdf/2412.09342)]
> **Authors**: Ralf Römer,Alexander von Rohr,Angela P. Schoellig
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Code: https://github.com/ralfroemer99/dpcc. 14 pages, 3 figures, 3 tables
- **标题**: 扩散预测控制和约束
- **领域**: 机器人技术,机器学习,系统与控制
- **摘要**: 由于其捕获高维和多模式分布的能力，扩散模型最近在机器人技术中获得了政策学习的知名度。但是，扩散策略本质上是随机的，通常是经过训练的离线训练，限制了他们处理看不见和动态条件的能力，在训练数据中未表示的新约束条件必须得到满足。为了克服这一限制，我们提出了使用约束（DPCC）的扩散预测控制，这是一种基于扩散的控制算法，具有显式状态和行动约束，可以偏离训练数据中的算法。 DPCC使用约束收紧，并将基于模型的投影纳入训练有素的轨迹扩散模型的转化过程中。这使我们能够为预测控制生成约束满意度，动态可行和目标轨迹的轨迹。我们通过对机器人操纵器的模拟显示，DPCC在满足新型测试时间约束的同时，在维持学习的控制任务上的性能方面优于现有方法。

### Modality-Driven Design for Multi-Step Dexterous Manipulation: Insights from Neuroscience 
[[arxiv](https://arxiv.org/abs/2412.11337)] [[cool](https://papers.cool/arxiv/2412.11337)] [[pdf](https://arxiv.org/pdf/2412.11337)]
> **Authors**: Naoki Wake,Atsushi Kanehira,Daichi Saito,Jun Takamatsu,Kazuhiro Sasabuchi,Hideki Koike,Katsushi Ikeuchi
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: 8 pages, 5 figures, 2 tables. Last updated on December 14th, 2024
- **标题**: 多步灵活操作的模态驱动设计：神经科学的见解
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **摘要**: 多步灵活的操纵是家庭场景中的一项基本技能，但在机器人技术中仍然是一个毫无疑问的领域。本文提出了一种模块化方法，其中操纵过程的每个步骤都会根据有效模态输入的专用策略来解决，而不是依靠单个端到端模型。为了证明这一点，灵巧的机器人手执行了一项操纵任务，涉及拾起和旋转盒子。在来自神经科学的见解的指导下，该任务分解为三个子技能，1）达到2）抓握和提升，以及3）基于人脑中使用的主要感觉方式。从实用的角度使用不同的方法来解决每个子技能：经典控制器，视觉语言行动模型和通过力量反馈分别使用强化学习政策。我们在真正的机器人上测试了管道，以证明我们的方法的可行性。这项研究的关键贡献在于为多步灵活操纵提供一种神经科学启发的，模态驱动的方法。

### Fast and Robust Visuomotor Riemannian Flow Matching Policy 
[[arxiv](https://arxiv.org/abs/2412.10855)] [[cool](https://papers.cool/arxiv/2412.10855)] [[pdf](https://arxiv.org/pdf/2412.10855)]
> **Authors**: Haoran Ding,Noémie Jaquier,Jan Peters,Leonel Rozo
> **First submission**: 2024-12-14
> **First announcement**: 2024-12-16
> **comment**: 14 pages, 10 figures, 9 tables, project website: https://sites.google.com/view/rfmp
- **标题**: 快速，强大的视觉运动riemannian流匹配政策
- **领域**: 机器人技术,机器学习
- **摘要**: 通过有效将视觉数据与高维，多模式的动作分布相结合，基于扩散的视觉运动策略在学习复杂的机器人任务方面表现出色。但是，扩散模型通常由于昂贵的降解过程而遭受缓慢的推断，或者需要最近蒸馏的方法引起的复杂顺序训练。本文介绍了Riemannian流量匹配策略（RFMP），该模型继承了流动匹配（FM）的易于训练和快速推理功能。此外，RFMP固有地纳入了在逼真的机器人应用中常见的几何约束，机器人状态位于riemannian歧管上。为了增强RFMP的鲁棒性，我们提出了稳定的RFMP（SRFMP），该RFMP（SRFMP）利用LaSalle的不变性原理使FM的动力学配备了稳定性，以支持目标Riemannian分布。对八个模拟和现实世界任务的严格评估表明，RFMP成功地学习并综合了具有有效的训练和推理阶段的欧几里得和里曼尼亚空间上的复杂感觉运动策略，超过了扩散策略，同时与一致性策略保持了竞争力。

### Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration 
[[arxiv](https://arxiv.org/abs/2412.10349)] [[cool](https://papers.cool/arxiv/2412.10349)] [[pdf](https://arxiv.org/pdf/2412.10349)]
> **Authors**: Lai Wei,Jiahua Ma,Yibo Hu,Ruimao Zhang
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 通过隐式触觉校准，确保视觉指导的机器人操作中的力安全
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 在动态环境中，机器人在操纵具有特定属性（例如门）的对象时通常会遇到受约束的运动轨迹。因此，施加适当的力对于防止机器人和物体损坏至关重要。但是，当前的视觉引导的机器人国家生成方法在这方面经常会摇摇欲坠，因为它们缺乏触觉感知的整合。为了解决这个问题，本文介绍了一个新颖的状态扩散框架，称为Safediff。它从当前的机器人状态和视觉上下文观察中产生一个前瞻性状态序列，同时结合了实时触觉反馈以完善序列。据我们所知，这是第一项专门针对机器人操作中力安全的研究。它显着增强了国家规划的合理性，安全行动轨迹源于基于此精致计划的反向动态。在实践中，与以前的方法不同的方法来形成视觉和触觉数据以生成未来的机器人状态序列，我们的方法采用触觉数据作为校准信号，以隐含在状态空间内的机器人状态。此外，我们开发了一个称为SAFEDOORMANIP50K的大规模仿真数据集，提供了广泛的多模式数据来训练和评估所提出的方法。广泛的实验表明，我们的视觉效果模型基本上减轻了在模拟和现实世界中开放的门开口和现实世界中有害力的风险。

### Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning 
[[arxiv](https://arxiv.org/abs/2412.11974)] [[cool](https://papers.cool/arxiv/2412.11974)] [[pdf](https://arxiv.org/pdf/2412.11974)]
> **Authors**: Qi Sun,Pengfei Hong,Tej Deep Pala,Vernon Toh,U-Xuan Tan,Deepanway Ghosal,Soujanya Poria
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: https://github.com/declare-lab/Emma-X, https://huggingface.co/declare-lab/Emma-X
- **标题**: Emma-X：具有扎根的思想链和前景空间推理的体现的多模式动作模型
- **领域**: 机器人技术,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 传统的强化学习基于学习的机器人控制方法通常是特定于任务的，并且无法跨越各种环境或看不见的对象和指示。视觉语言模型（VLMS）展示了强大的场景理解和规划能力，但缺乏为特定机器人实施例量身定制的可行政策的能力。为了解决这个问题，视觉语言动作（VLA）模型已经出现，但是它们在长途空间推理和扎根的任务计划中面临挑战。在这项工作中，我们提出了具有扎根的思想链和前景空间推理emma-x的体现多模式作用模型。 Emma-X利用了基于桥接V2的构建的层次实施数据集，其中包含60,000个机器人操纵轨迹自动通道，并具有接地的任务推理和空间指导。此外，我们引入了基于抓斗状态和运动轨迹的轨迹分割策略，这可以帮助减轻幻觉，从而在接地子任务中产生。实验结果表明，EMMA-X在需要空间推理的实际机器人任务中，尤其是在现实世界中的机器人任务中，其表现优于竞争基线。

### C2F-TP: A Coarse-to-Fine Denoising Framework for Uncertainty-Aware Trajectory Prediction 
[[arxiv](https://arxiv.org/abs/2412.13231)] [[cool](https://papers.cool/arxiv/2412.13231)] [[pdf](https://arxiv.org/pdf/2412.13231)]
> **Authors**: Zichen Wang,Hao Miao,Senzhang Wang,Renzhi Wang,Jianxin Wang,Jian Zhang
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: Accepted by AAAI 2025
- **标题**: C2F-TP：不确定性感知轨迹预测的粗到最新的denoising框架
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 准确地预测车辆的轨迹对于确保自动驾驶的安全性和可靠性至关重要。尽管最近已经进行了大量的研究工作，但包括动态驾驶预期在内的各种因素以及多样化的驾驶场景引起的固有轨迹不确定性仍然对准确的轨迹预测构成了重大挑战。为了解决这个问题，我们提出了C2F-TP，这是一个不确定性感知的车辆轨迹预测的粗到最新的降级框架。 C2F-TP具有创新的两阶段的粗略预测过程。具体而言，在时空的相互作用阶段，我们提出了一个时空相互作用模块来捕获车辆间相互作用并学习多模式轨迹分布，从中，采样了一定数量的噪声轨迹。接下来，在轨迹改进阶段，我们设计了一个条件denoising模型，以通过逐步的DeNoising操作来减少采样轨迹的不确定性。在两个真实的数据集NGSIM和HIGD上进行了广泛的实验，这些实验在轨迹预测中广泛采用。结果证明了我们提议的有效性。

### GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering 
[[arxiv](https://arxiv.org/abs/2412.14480)] [[cool](https://papers.cool/arxiv/2412.14480)] [[pdf](https://arxiv.org/pdf/2412.14480)]
> **Authors**: Saumya Saxena,Blake Buchanan,Chris Paxton,Bingqing Chen,Narunas Vaskevicius,Luigi Palmieri,Jonathan Francis,Oliver Kroemer
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Project website: https://saumyasaxena.github.io/grapheqa
- **标题**: grapheqa：使用3D语义场景图用于实时体现问题回答
- **领域**: 机器人技术,计算语言学,计算机视觉和模式识别,机器学习
- **摘要**: 在体现的问题回答（EQA）中，代理必须探索和发展对看不见环境的语义理解，以便充满信心地回答一个位置问题。由于难以获得有用的语义表示，在线更新这些表示以及利用先前的世界知识进行有效的探索和计划，因此这仍然是一个具有挑战性的问题。为了解决这些局限性，我们提出了Grapheqa，这是一种新型方法，它利用实时3D Metric-Smantic场景图（3DSGS）和与任务相关的图像作为接地视觉模型（VLMS）的多模式内存来执行在未看到环境中执行EQA任务。我们采用一种层次计划方法，利用3DSG的层次结构性质进行结构化计划和语义指导探索。通过对HM-EQA数据集进行仿真的实验以及家庭和办公环境中现实世界中的实验，我们证明了我们的方法通过完成具有更高成功率和更少计划步骤的EQA任务来优于关键基准。

### Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models 
[[arxiv](https://arxiv.org/abs/2412.14058)] [[cool](https://papers.cool/arxiv/2412.14058)] [[pdf](https://arxiv.org/pdf/2412.14058)]
> **Authors**: Xinghang Li,Peiyan Li,Minghuan Liu,Dong Wang,Jirong Liu,Bingyi Kang,Xiao Ma,Tao Kong,Hanbo Zhang,Huaping Liu
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Project page: robovlms.github.io. Added limitations and future works. Fix categorization
- **标题**: 迈向通才机器人政策：在建立视觉语言行动模型中很重要
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 基金会视觉语言模型（VLM）在多模式表示，理解和推理方面具有强大的能力。通过将动作组件注入VLM，可以自然形成视觉语言动作模型（VLA），并显示出令人鼓舞的性能。现有工作证明了VLA在多种情况和任务中的有效性和概括。然而，从VLM到VLA的转移并不是一件容易的事，因为现有的VLA在其骨架，动作预测配方，数据分布和培训配方的不同之处。这导致缺少对VLA的设计选择的系统理解。在这项工作中，我们披露了极大地影响VLA性能的关键因素，并专注于回答三个基本设计选择：选择哪种骨干，如何制定VLA体系结构以及何时添加交叉体型数据。获得的结果坚定地说明了我们为什么需要VLA并开发一个新的VLA，Robovlms的新家族，该家族几乎需要手动设计，并在三个模拟任务和现实世界实验中实现新的最新性能。通过我们的广泛实验，包括超过8个VLM骨架，4个策略架构以及600多种不同设计的实验，我们为VLAS的未来设计提供了详细的指南。除了研究外，高度灵活的Robovlms框架支持新的VLM的简单集成和各种设计选择的免费组合，以促进未来的研究。我们开放所有详细信息，包括代码，模型，数据集和工具包，以及详细的培训和评估食谱：robovlms.github.io。

### QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning 
[[arxiv](https://arxiv.org/abs/2412.15576)] [[cool](https://papers.cool/arxiv/2412.15576)] [[pdf](https://arxiv.org/pdf/2412.15576)]
> **Authors**: Xinyang Tong,Pengxiang Ding,Yiguo Fan,Donglin Wang,Wenjie Zhang,Can Cui,Mingyang Sun,Han Zhao,Hongyin Zhang,Yonghao Dang,Siteng Huang,Shangke Lyu
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: Accepted to ICRA 2025; Github page: https://quart-online.github.io
- **标题**: Quart-Online：四足机器人学习的无延迟大型多模式模型
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 本文介绍了与在四倍的视觉语言行动（QUAR-VLA）任务中部署多模式大语模型（MLLM）相关的固有的推理潜伏期挑战。我们的调查表明，传统参数减少技术最终会在动作指导调整阶段损害语言基础模型的性能，从而使其不适合此目的。我们介绍了一种新型的无延迟四倍的MLLM模型，称为夸脱的连接线，旨在提高推理效率，而不会降低语言基础模型的性能。通过合并动作块离散化（ACD），我们压缩原始动作表示空间，将连续的动作值映射到较小的离散代表向量集，同时保留关键信息。随后，我们将MLLM微调以将视觉，语言和压缩动作整合到统一的语义空间中。实验结果表明，Quart在线与现有的MLLM系统同时运行，从而实现了与基础控制器频率同步的实时推断，从而将各种任务的成功率显着提高了65％。我们的项目页面是https://quart-online.github.io。

### Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with A Few Examples 
[[arxiv](https://arxiv.org/abs/2412.17288)] [[cool](https://papers.cool/arxiv/2412.17288)] [[pdf](https://arxiv.org/pdf/2412.17288)]
> **Authors**: Taewoong Kim,Byeonghwi Kim,Jonghyun Choi
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: AAAI 2025 (Project page: https://twoongg.github.io/projects/flare/)
- **标题**: 多模式的基础计划和有效的学习体现代理的重建
- **领域**: 机器人技术,人工智能
- **摘要**: 学习一个机器人助手的感知和推理模块，以计划基于自然语言指令执行复杂任务的步骤通常需要大量的自由语言注释，尤其是对于短期的高级指令。为了降低注释的成本，大语言模型（LLMS）用作少量数据的计划者。但是，在详细说明步骤时，即使是使用LLM的最先进的计划者也主要依赖语言常识，通常会忽略指挥接收中环境的状态，从而导致不适当的计划。为了生成以环境为基础的计划，我们提出了耀斑（带有环境自适应重新启动体现的代理的少量语言），该计划使用语言命令和环境感知来改善任务计划。由于语言说明通常包含歧义或不正确的表达式，因此我们还建议使用代理商的视觉提示来纠正错误。拟议的方案使我们能够使用几个语言对，这要归功于视觉提示和优于最先进的方法。我们的代码可在https://github.com/snumprlab/flare上找到。

### Multi-Scenario Reasoning: Unlocking Cognitive Autonomy in Humanoid Robots for Multimodal Understanding 
[[arxiv](https://arxiv.org/abs/2412.20429)] [[cool](https://papers.cool/arxiv/2412.20429)] [[pdf](https://arxiv.org/pdf/2412.20429)]
> **Authors**: Libo Wang
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: The main text is 5 pages, 2 figures, and 3 tables
- **标题**: 多阶段推理：在人形机器人中解锁认知自主权以进行多模式理解
- **领域**: 机器人技术,人工智能
- **摘要**: 为了提高人形机器人的认知自主权，这项研究提出了一种多阶段推理体系结构，以解决该领域多模式理解的技术缺陷。它利用基于模拟的实验设计，该设计采用了多模式合成（视觉，听觉，触觉），并构建模拟器“ Maha”来执行实验。这些发现证明了这种体系结构在多模式数据中的可行性。它为动态环境中人类机器人的跨模式相互作用策略的探索提供了参考经验。此外，多阶段的推理模拟了人脑在认知水平上对人形机器人的高级推理机制。这一新概念促进了跨阶段的实际任务转移和语义驱动的行动计划。它预示着人类机器人在改变场景中的自我学习和自主行为的未来发展。

## 声音(cs.SD:Sound)

该领域共有 13 篇论文

### Diff4Steer: Steerable Diffusion Prior for Generative Music Retrieval with Semantic Guidance 
[[arxiv](https://arxiv.org/abs/2412.04746)] [[cool](https://papers.cool/arxiv/2412.04746)] [[pdf](https://arxiv.org/pdf/2412.04746)]
> **Authors**: Xuchan Bao,Judith Yue Li,Zhong Yi Wan,Kun Su,Timo Denk,Joonseok Lee,Dima Kuzmin,Fei Sha
> **First submission**: 2024-12-05
> **First announcement**: 2024-12-06
> **comment**: NeurIPS 2024 Creative AI Track
- **标题**: DIFF4STEER：具有语义指导的生成音乐检索的先验性扩散
- **领域**: 声音,信息检索,多媒体,音频和语音处理
- **摘要**: 现代音乐检索系统通常依赖用户偏好的固定表示，从而限制了他们捕获用户多样和不确定的检索需求的能力。为了解决这一限制，我们介绍了Diff4Steer，这是一种新颖的生成检索框架，该框架采用轻质扩散模型来合成从用户查询中的不同种子嵌入，这些嵌入方式代表了音乐探索的潜在方向。与确定性方法将用户查询到嵌入空间的单个点不同，DIFF4Steer在目标模式（音频）上提供了统计先验，以进行检索，从而有效地捕获用户偏好的不确定性和多方面的性质。此外，可以通过图像或文本输入来指导DIFF4Steer，从而使更灵活，可控制的音乐发现与最近的邻居搜索结合在一起。在检索和排名指标方面，我们的框架优于确定性回归方法和基于LLM的生成检索基线，这表明了其在捕获用户偏好方面的有效性，从而导致了更多多样化和相关的建议。可在tinyurl.com/diff4steer上获得听力示例。

### Pilot-guided Multimodal Semantic Communication for Audio-Visual Event Localization 
[[arxiv](https://arxiv.org/abs/2412.06208)] [[cool](https://papers.cool/arxiv/2412.06208)] [[pdf](https://arxiv.org/pdf/2412.06208)]
> **Authors**: Fei Yu,Zhe Xiang,Nan Che,Zhuoran Zhang,Yuandi Li,Junxiao Xue,Zhiguo Wan
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 试验的多式语义传播，用于视听事件本地化
- **领域**: 声音,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 多模式的语义通信集成了各种数据模式，例如文本，图像和音频，可显着提高通信效率和可靠性。它在人工智能，自动驾驶和智能家居等领域具有广泛的应用前景。但是，当前的研究主要依赖于模拟通道，并假定恒定的通道状态（完美的CSI），这对于在现实世界情景中解决动态的物理通道和噪声不足。现有方法通常集中在单个模态任务上，并且无法处理多模式流数据，例如视频和音频及其相应的任务。此外，当前的语义编码和解码模块主要传递单个模态特征，从而忽略了对多模式语义增强和识别任务的需求。为了应对这些挑战，本文提出了一个针对视听事件本地化任务的多模式语义通信的试验指导框架。该框架利用数字飞行员代码和通道模块来指导模拟通道的状态，并在基于Euler的多模式语义编码和解码中，考虑基于动态通道状态的时频特性。这种方法有效地处理了多模式流源数据，特别是对于视听事件本地化任务。广泛的数值实验证明了拟议框架在通道变化中的鲁棒性及其对各种通信方案的支持。实验结果表明，该框架在信噪比（SNR）方面优于现有的基准方法，强调了其在语义通信质量方面的优势。

### WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2412.05558)] [[cool](https://papers.cool/arxiv/2412.05558)] [[pdf](https://arxiv.org/pdf/2412.05558)]
> **Authors**: Feng Li,Jiusong Luo,Wanjun Xia
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-09
> **comment**: Accepted by 31st International Conference on MultiMedia Modeling (MMM2025)
- **标题**: wavfusion：to wav2Vec 2.0多模式语音情感识别
- **领域**: 声音,人工智能,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 由于人类情感的固有复杂性和多样性，言语情感识别（SER）仍然是一项具有挑战性但至关重要的任务。为了解决这个问题，研究人员试图通过多模式学习从其他方式融合信息。但是，现有的多模式融合技术通常会忽略跨模式相互作用的复杂性，从而导致次优特征表示。在本文中，我们提出了WavFusion，这是一个多式模式的语音情感识别框架，该框架解决了有效的多模式融合，模态异质性和歧视性表示学习中的关键研究问题。通过利用封闭式的跨模式注意机制和多模式均质特征差异学习，WavFusion表现出比基准数据集上现有最新方法的性能提高。我们的工作突出了捕获细微的跨模式相互作用和学习判别性表示的重要性，以进行准确的多模式Ser。在两个基准数据集（IEMocap和MELD）上进行的实验结果表明，Wavfusion在情绪识别的最新策略中取得了成功。

### MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time Scenarios with Impaired Visual Cues 
[[arxiv](https://arxiv.org/abs/2412.08247)] [[cool](https://papers.cool/arxiv/2412.08247)] [[pdf](https://arxiv.org/pdf/2412.08247)]
> **Authors**: Junjie Li,Ke Zhang,Shuai Wang,Kong Aik Lee,Haizhou Li
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: No comments
- **标题**: MOMUSE：动量多模式目标扬声器提取用于实时场景的视觉线索的实时场景
- **领域**: 声音,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 视听目标扬声器提取（AV-TSE）旨在使用时间同步的视觉提示将特定目标扬声器的语音与音频混合物隔离。在实际情况下，由于各种损害，视觉提示并不总是可用，这破坏了AV-TSE的稳定性。尽管有这一挑战，即使目标发言人看不见，人类也可以随着时间的推移而保持注意力势头。在本文中，我们介绍了动量多模式目标扬声器提取（MOMUSE），该目标扬声器在内存中保留了扬声器身份动力，从而使模型能够连续跟踪目标扬声器。 Momuse专为实时推理而设计，并在视觉提示和动态更新的扬声器动量的指导下提取了当前的语音窗口。实验结果表明，MOMUSE表现出显着改善，尤其是在严重损害视觉提示的情况下。

### Multimodal Sentiment Analysis based on Video and Audio Inputs 
[[arxiv](https://arxiv.org/abs/2412.09317)] [[cool](https://papers.cool/arxiv/2412.09317)] [[pdf](https://arxiv.org/pdf/2412.09317)]
> **Authors**: Antonio Fernandez,Suzan Awinat
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: Presented as a full paper in the 15th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2024) October 28-30, 2024, Leuven, Belgium
- **标题**: 基于视频和音频输入的多模式情感分析
- **领域**: 声音,人工智能,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 尽管目前从视频和音频上进行了情感分析的研究大量研究，但找到提供最高准确率的最佳模型仍然被认为是该领域研究人员的挑战。本文的主要目的是证明具有视频和音频输入的情感识别模型的可用性。用于训练模型的数据集是音频的Crema-D数据集和视频的Ravdess数据集。使用的微调模型是：音频的Facebook/wav2Vec2-large和视频的Google/Vivit-B-16x2-Kinetics400。在决策框架中使用了前一个模型产生的每个情绪的概率的避免。结果差异之后，如果其中一个模型获得了更高的精度，则创建了另一个测试框架。所使用的方法是加权平均方法，置信度阈值方法，基于置信度方法的动态加权以及基于规则的逻辑方法。这种有限的方法可以令人鼓舞的结果，使未来对这些方法的研究可行。

### YingSound: Video-Guided Sound Effects Generation with Multi-modal Chain-of-Thought Controls 
[[arxiv](https://arxiv.org/abs/2412.09168)] [[cool](https://papers.cool/arxiv/2412.09168)] [[pdf](https://arxiv.org/pdf/2412.09168)]
> **Authors**: Zihao Chen,Haomin Zhang,Xinhan Di,Haoyu Wang,Sizhe Shan,Junjie Zheng,Yunming Liang,Yihan Fan,Xinfa Zhu,Wenjie Tian,Yihua Wang,Chaofan Ding,Lei Xie
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: 16 pages, 4 figures
- **标题**: Yingsound：带有多模式链的视频引导的声音效果生成
- **领域**: 声音,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 为产品级别的视频产生声音效果，其中只有少量标记的数据可用于不同的场景，就需要在几个弹药设置中生产高质量的声音。为了应对实际场景中有限标记的数据的挑战，我们介绍了Yingsound，这是一个为视频引导的声音生成而设计的基础模型，该模型支持几乎没有拍摄设置的高质量音频生成。具体而言，Yingsound由两个主要模块组成。第一个模块使用条件流匹配的变压器来实现在音频和视觉方式之间发电的有效语义对齐。该模块旨在构建可学习的视听聚合器（AVA），该聚合器（AVA）将高分辨率的视觉特征与多个阶段的相应音频功能集成在一起。第二个模块是通过提出的多模式视觉原理链（COT）方法开发的，以在几次弹射设置中产生更精细的声音效果。最后，介绍了一个涵盖各种现实世界情景的行业标准视频对审计（V2A）数据集。我们表明，通过自动评估和人类研究，Yingsound有效地在各种条件输入中产生了高质量的同步声音。项目页面：\ url {https://giantailab.github.io/yingsound/}

### CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models 
[[arxiv](https://arxiv.org/abs/2412.10117)] [[cool](https://papers.cool/arxiv/2412.10117)] [[pdf](https://arxiv.org/pdf/2412.10117)]
> **Authors**: Zhihao Du,Yuxuan Wang,Qian Chen,Xian Shi,Xiang Lv,Tianyu Zhao,Zhifu Gao,Yexin Yang,Changfeng Gao,Hui Wang,Fan Yu,Huadai Liu,Zhengyan Sheng,Yue Gu,Chong Deng,Wen Wang,Shiliang Zhang,Zhijie Yan,Jingren Zhou
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: Tech report, work in progress
- **标题**: Cosyvoice 2：可扩展的流式语音综合与大语言模型
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **摘要**: 在以前的工作中，我们介绍了Cosyvoice，这是一种基于监督离散语音令牌的多语言语音综合模型。通过使用两个流行的生成模型，语言模型（LMS）和流匹配的渐进式语义解码，Cosyvoice在语音中文化学习中表现出很高的韵律自然性，内容一致性和扬声器相似性。最近，在多模式大语模型（LLMS）中取得了重大进展，其中响应潜伏期和语音合成的实时因素在交互式体验中起着至关重要的作用。因此，在本报告中，我们提出了一个改进的流语音合成模型Cosyvoice 2，该模型融合了全面和系统的优化。具体来说，我们引入有限量表量化，以改善语音令牌的代码书利用。对于文本语音LM，我们简化了模型体系结构，以直接使用预训练的LLM作为骨干。此外，我们开发了一个块感知的因果流量匹配模型，以支持各种综合方案，从而在单个模型中既可以在单个模型中进行流和非流综合。通过对大规模多语言数据集进行培训，Cosyvoice 2可以在流媒体模式下实现人类的自然性，最小的响应潜伏期和几乎无损合成质量。我们邀请读者在https://funaudiollm.github.io/cosyvoice2上聆听演示。

### AV-DTEC: Self-Supervised Audio-Visual Fusion for Drone Trajectory Estimation and Classification 
[[arxiv](https://arxiv.org/abs/2412.16928)] [[cool](https://papers.cool/arxiv/2412.16928)] [[pdf](https://arxiv.org/pdf/2412.16928)]
> **Authors**: Zhenyuan Xiao,Yizhuo Yang,Guili Xu,Xianglong Zeng,Shenghai Yuan
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: Submitted to ICRA 2025
- **标题**: AV-DTEC：用于无人机轨迹估计和分类的自我监管的视听融合
- **领域**: 声音,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 紧凑型无人机的使用日益加剧，对公共安全造成了重大威胁，而传统的无人机检测系统通常笨重且昂贵。为了应对这些挑战，我们提出了一种轻巧的自我观察的视听融合系统的AV-DTEC。 AV-DTEC是使用LiDar生成的标签的自制学习训练的，它同时通过平行的选择性状态空间模型来学习音频和视觉特征。凭借博学的功能，专门设计的插入式插件主载体功能增强模块将视觉功能集成到音频功能中，以在跨照明条件下更好地鲁棒性。为了减少对辅助特征和对齐方式的依赖，我们提出了一个教师学生模型，以适应性地调整视觉特征的加权。 AV-DTEC在现实世界多模式数据中表现出非凡的准确性和有效性。代码和训练有素的模型可在github \ url {https://github.com/amazingday1/av-detc}上公开访问。

### SoundLoc3D: Invisible 3D Sound Source Localization and Classification Using a Multimodal RGB-D Acoustic Camera 
[[arxiv](https://arxiv.org/abs/2412.16861)] [[cool](https://papers.cool/arxiv/2412.16861)] [[pdf](https://arxiv.org/pdf/2412.16861)]
> **Authors**: Yuhang He,Sangyun Shin,Anoop Cherian,Niki Trigoni,Andrew Markham
> **First submission**: 2024-12-22
> **First announcement**: 2024-12-23
> **comment**: Accepted by WACV2025
- **标题**: Soundloc3D：使用多模式RGB-D声学摄像机看不见的3D声源定位和分类
- **领域**: 声音,计算机视觉和模式识别,多媒体,音频和语音处理
- **摘要**: 准确地定位3D声音源并估算其语义标签（其中可能不可见，但假定源位于场景中物体的物理表面上）具有许多真实的应用，包括检测气体泄漏和机械故障。在这种情况下，视听弱相关性在得出创新方法方面提出了新的挑战，以回答是否或如何使用跨模式信息来解决任务。为此，我们建议使用由针孔RGB-D摄像头和共面四通道麦克风阵列〜（MIC-Array）组成的声学摄像机钻机。通过使用此钻机来记录来自多视图的音频信号，我们可以使用跨模式提示来估计声源3D位置。具体而言，我们的框架Soundloc3D将任务视为集合预测问题，集合中的每个元素都对应于潜在的声源。考虑到视听弱相关，首先是从单个视图麦克风阵列信号中学到的集合表示，然后通过积极合并从多视频RGB-D图像揭示的物理表面提示进行完善。我们证明了SoundLoc3D在大型模拟数据集上的效率和优势，并进一步显示了其对RGB-D测量不准确性和环境噪声干扰的稳健性。

### Text2midi: Generating Symbolic Music from Captions 
[[arxiv](https://arxiv.org/abs/2412.16526)] [[cool](https://papers.cool/arxiv/2412.16526)] [[pdf](https://arxiv.org/pdf/2412.16526)]
> **Authors**: Keshav Bhandari,Abhinaba Roy,Kyra Wang,Geeta Puri,Simon Colton,Dorien Herremans
> **First submission**: 2024-12-21
> **First announcement**: 2024-12-23
> **comment**: 9 pages, 3 figures, Accepted at the 39th AAAI Conference on Artificial Intelligence (AAAI 2025)
- **标题**: Text2Midi：从字幕中生成符号音乐
- **领域**: 声音,人工智能,计算语言学,音频和语音处理
- **摘要**: 本文介绍了Text2Midi，这是一种端到端模型，可从文本描述中生成MIDI文件。为了利用多模式生成方法的日益普及，Text2Midi利用了文本数据的广泛可用性和大语言模型（LLMS）的成功。我们的端到端系统利用LLM的力量以MIDI文件的形式生成符号音乐。具体而言，我们利用预贴的LLM编码器来处理字幕，然后将自动回归变压器解码器调节以产生准确反映提供的描述的MIDI序列。这种直观和用户友好的方法通过允许用户使用文本提示来生成音乐作品，从而显着简化音乐创建过程。我们进行了全面的经验评估，并结合了自动化和人类研究，这些评估表明我们的模型生成了高质量的MIDI文件，这些文件确实可以由文本字幕控制，其中可能包括音乐理论术语，例如和弦，钥匙和速度。我们在演示页面（https://github.com/amaai-lab/text2midi）上发布代码和音乐样本，以供用户与text2midi进行交互。

### A Classification Benchmark for Artificial Intelligence Detection of Laryngeal Cancer from Patient Speech 
[[arxiv](https://arxiv.org/abs/2412.16267)] [[cool](https://papers.cool/arxiv/2412.16267)] [[pdf](https://arxiv.org/pdf/2412.16267)]
> **Authors**: Mary Paterson,James Moor,Luisa Cutillo
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: 24 pages, 6 figures, 7 tables
- **标题**: 人工智能检测喉癌的分类基准
- **领域**: 声音,机器学习,音频和语音处理,定量方法
- **摘要**: 预计未来几年的喉癌病例将显着增加。当前的诊断途径导致许多患者被错误地提到紧急疑似癌症途径，这对患者和医疗系统造成了不适当的压力。人工智能通过从患者言语中对喉癌进行非侵入性检测提供了一种有希望的解决方案，这可以帮助更有效地推荐并减少非癌症患者的不适当推荐。为了意识到这种潜力，开放科学至关重要。该领域的主要障碍是缺乏开源数据集和可重现的基准，从而迫使研究人员从头开始。我们的工作通过引入一个基准套件来解决这一挑战，该套件包括36个在开源数据集中训练和评估的型号。这些模型可以在公共存储库中访问，为将来的研究提供了基础。他们评估了三种不同的算法和三个音频功能集，提供了全面的基准测试框架。我们提出了标准化的指标和评估方法，以确保在以后的研究中确保一致和可比的结果。呈现的模型包括仅有音频输入和多模式输入，这些输入包含人口统计学和症状数据，从而使其能够在数据集中应用，并具有不同的患者信息。通过提供这些基准，未来的研究人员可以评估其数据集，完善模型并将其用作更高级方法的基础。这项工作旨在为建立可重复的基准测试提供基准，使研究人员能够将新方法与这些标准进行比较，并最终推进AI的开发用于检测喉癌。

### Lla-VAP: LSTM Ensemble of Llama and VAP for Turn-Taking Prediction 
[[arxiv](https://arxiv.org/abs/2412.18061)] [[cool](https://papers.cool/arxiv/2412.18061)] [[pdf](https://arxiv.org/pdf/2412.18061)]
> **Authors**: Hyunbae Jeon,Frederic Guintu,Rayvant Sahni
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: LLA-VAP：LLAMA和VAP的LSTM合奏，用于转弯预测
- **领域**: 声音,计算语言学,人机交互,音频和语音处理
- **摘要**: 转弯预测是预期何时对话中的说话者将其转向另一个发言人开始讲话。该项目通过采用多模式集合方法来扩展现有的转弯预测策略，该方法集成了大型语言模型（LLM）和语音活动投影（VAP）模型。通过将LLM的语言能力与VAP模型的时间精度相结合，我们旨在提高识别脚本和未脚本对话方案中TRP的准确性和效率。我们的方法在交流式语料库（ICC）和教练对话偏好启发（CCPE）数据集上进行了评估，突出了当前模型的优势和局限性，同时提出了一个可能更强大的预测框架。

### MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by Real-time MRI 
[[arxiv](https://arxiv.org/abs/2412.18836)] [[cool](https://papers.cool/arxiv/2412.18836)] [[pdf](https://arxiv.org/pdf/2412.18836)]
> **Authors**: Neil Shah,Ayan Kashyap,Shirish Karande,Vineet Gandhi
> **First submission**: 2024-12-25
> **First announcement**: 2024-12-30
> **comment**: Accepted at IEEE ICASSP 2025
- **标题**: MRI2speech：实时MRI记录的发音运动中的语音综合
- **领域**: 声音,人工智能,音频和语音处理
- **摘要**: 以前的实时MRI（RTMRI）基于语音合成模型在很大程度上取决于嘈杂的地面真相。直接在地面真理中施加损失，将语音内容与MRI噪声纠缠，从而导致可理解性差。我们介绍了一种新颖的方法，该方法适应了来自RTMRI的文本预测的多模式自我监管的AV-Hubert模型，并为扬声器特定的比对结合了一个新的基于流动的持续时间预测指标。然后，语音解码器使用预测的文本和持续时间来综合任何新颖的声音。我们在两个数据集上进行了彻底的实验，并证明了我们的方法的概括能力，可以看不见说话者。我们通过掩盖RTMRI视频的部分来评估框架的性能，以评估不同的铰接器对文本预测的影响。我们的方法在USC-TIMIT MRI语料库上达到了$ 15.18 \％$ Word错误率（WER），这标志着对当前最新技术的巨大改善。语音样本可在https://mri2speech.github.io/mri2speech/上找到

## 软件工程(cs.SE:Software Engineering)

该领域共有 2 篇论文

### Fragmented Layer Grouping in GUI Designs Through Graph Learning Based on Multimodal Information 
[[arxiv](https://arxiv.org/abs/2412.05555)] [[cool](https://papers.cool/arxiv/2412.05555)] [[pdf](https://arxiv.org/pdf/2412.05555)]
> **Authors**: Yunnong Chen,Shuhong Xiao,Jiazhi Li,Tingting Zhou,Yanfang Chang,Yankun Zhen,Lingyun Sun,Liuqing Chen
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-09
> **comment**: 28 pages,6 figures
- **标题**: GUI设计中的碎片层通过基于多模式信息的图形学习
- **领域**: 软件工程,人工智能
- **摘要**: 自动构建不同粒度的GUI群体构成了自动化GUI设计和实施任务的关键智能步骤。具体而言，在工业GUI到代码过程中，碎片层可能会降低生成的代码的可读性和可维护性，这可以通过对设计原型中的语义一致的碎片层进行分组来缓解。这项研究旨在提出一种基于图形学习的方法，以根据设计原型中的多模式信息解决碎片的层分组问题。我们的图形学习模块由自我注意力和图形神经网络模块组成。通过将GUI层的多模式融合表示作为输入，我们通过对GUI层进行分类并同时回归相应GUI组件的边界框来创新分组层。两个现实世界数据集的实验表明，我们的模型可以实现最新的性能。还进行了进一步的用户研究，以验证我们的方法可以帮助智能下游工具生成更可维护和可读的前端代码。

### A Progressive Transformer for Unifying Binary Code Embedding and Knowledge Transfer 
[[arxiv](https://arxiv.org/abs/2412.11177)] [[cool](https://papers.cool/arxiv/2412.11177)] [[pdf](https://arxiv.org/pdf/2412.11177)]
> **Authors**: Hanxiao Lu,Hongyu Cai,Yiming Liang,Antonio Bianchi,Z. Berkay Celik
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 用于统一二进制代码嵌入和知识转移的渐进变压器
- **领域**: 软件工程,机器学习
- **摘要**: 语言模型方法最近已集成到二进制分析任务中，例如功能相似性检测和功能签名恢复。这些模型通常采用两个阶段的培训过程：通过蒙版语言建模（MLM）进行预训练，并在机器代码上进行微调。尽管MLM有助于理解二进制代码结构，但它忽略了基本代码特征，包括控制和数据流，这对模型的概括产生了负面影响。最近的工作利用了基于变压器的方法中的特定于域特异性特征（例如，控制流程图和动态执行轨迹），以改善二进制代码语义理解。但是，这种方法涉及复杂的功能工程，这是一个繁琐且耗时的过程，在处理剥离或混淆的代码时，可以引入预测性不确定性，从而导致性能下降。在本文中，我们介绍了Protst，这是一种基于变压器的新型二进制代码嵌入方法。 Protst采用基于独特的树状结构的层次训练过程，其中知识从根本的基本任务流向了叶子上更专业的任务。这种渐进的教师学生范式使该模型能够基于先前学习的知识，从而产生高质量的嵌入，可以有效利用这些嵌入，以实现多种下游二进制分析任务。在七个二元分析任务中评估了ProTS的有效性，结果表明，与传统的两阶段训练相比，ProTST的平均验证评分（F1，MRR和Recce@1）提高了14.8％，与多模态两阶段框架相比，ProTST得分为14.8％。

## 社交和信息网络(cs.SI:Social and Information Networks)

该领域共有 2 篇论文

### Re-calibrating methodologies in social media research: Challenge the visual, work with Speech 
[[arxiv](https://arxiv.org/abs/2412.13170)] [[cool](https://papers.cool/arxiv/2412.13170)] [[pdf](https://arxiv.org/pdf/2412.13170)]
> **Authors**: Hongrui Jin
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: 11 pages (excluding references), 3 figures
- **标题**: 在社交媒体研究中重新校准方法：挑战视觉，与语音一起工作
- **领域**: 社交和信息网络,信息检索
- **摘要**: 本文从方法论上反映了社交媒体学者在分析中如何有效地与基于语音的数据互动。尽管当代媒体研究已经接受了文本，视觉和关系数据，但听觉维度仍然相对较小。本文以次要口头口感和对纯粹视觉文化的拒绝的概念为基础，认为在规模上考虑语音和语音丰富了我们对多模式数字内容的理解。本文介绍了Tiktok字幕工具包，该工具包提供与现有工作流程易于兼容的可访问语音处理。这样一来，它为大规模查询开辟了新的途径，这些概念将定量见解与定性精确度融合在一起。两个说明性的案例既凸显了语音研究的机会和局限性：虽然Tiktok上的#StoryTime等流派受益于对口语叙事的探索，但非语言或音乐驱动的内容可能不会使用语音数据产生重要的见解。文章鼓励研究人员周到地集成了听觉探索，以补充现有方法，而不是更换它们。我得出的结论是，我们的方法论曲目的扩展可以使对平台的内容的更丰富的解释，以及我们随着数字文化的越来越多的多模式而解开数字文化的能力。

### Modality-Independent Graph Neural Networks with Global Transformers for Multimodal Recommendation 
[[arxiv](https://arxiv.org/abs/2412.13994)] [[cool](https://papers.cool/arxiv/2412.13994)] [[pdf](https://arxiv.org/pdf/2412.13994)]
> **Authors**: Jun Hu,Bryan Hooi,Bingsheng He,Yinwei Wei
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-19
> **comment**: Accepted by AAAI 2025
- **标题**: 与全局变压器的模式无关图形神经网络用于多模式建议
- **领域**: 社交和信息网络,机器学习
- **摘要**: 多模式推荐系统可以从现有的用户项目交互以及与项目相关的多模式数据的语义中学习用户的偏好。许多现有方法通过多模式用户 - 项目图对此进行建模，以将多模式建议作为图形学习任务进行建模。图神经网络（GNN）在该域中表现出了有希望的性能。先前的研究已经利用了GNNS在某些接收领域（通常用啤酒花数，$ k $表示）捕获邻里信息的能力，以丰富用户和项目语义。我们观察到，GNN的最佳接受场会因不同的方式而变化。在本文中，我们提出了具有与模态无依赖性接收场的GNN，这些GNN采用具有独立接收场的独立GNN来增强性能。我们的结果表明，特定数据集的某些模式的最佳$ K $可以低至1或2，这可能会限制GNNS捕获全球信息的能力。为了解决这个问题，我们引入了基于抽样的全球变压器，该变压器利用统一的全局抽样来有效整合GNN的全局信息。我们进行了全面的实验，以证明我们的方法优于现有方法。我们的代码可在https://github.com/crawlscript/mig-gt上公开获取。

## 音频和语音处理(eess.AS:Audio and Speech Processing)

该领域共有 3 篇论文

### Scaling Transformers for Low-Bitrate High-Quality Speech Coding 
[[arxiv](https://arxiv.org/abs/2411.19842)] [[cool](https://papers.cool/arxiv/2411.19842)] [[pdf](https://arxiv.org/pdf/2411.19842)]
> **Authors**: Julian D Parker,Anton Smirnov,Jordi Pons,CJ Carr,Zack Zukowski,Zach Evans,Xubo Liu
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 缩放型变压器用于低焦酸盐高质量语音编码
- **领域**: 音频和语音处理,人工智能,机器学习,声音,信号处理
- **摘要**: 使用神经音频编解码器模型对语音的象征化是单独或在多模式背景下对语音产生或理解的现代AI管道的重要组成部分。传统上，这种象征化模型仅使用具有强诱导性偏见的组件集中在低参数计数架构上。 In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second.受过训练的模型在客观测试和主观测试中都表现出色。

### Investigating Acoustic-Textual Emotional Inconsistency Information for Automatic Depression Detection 
[[arxiv](https://arxiv.org/abs/2412.18614)] [[cool](https://papers.cool/arxiv/2412.18614)] [[pdf](https://arxiv.org/pdf/2412.18614)]
> **Authors**: Rongfeng Su,Changqing Xu,Xinyi Wu,Feng Xu,Xie Chen,Lan Wangt,Nan Yan
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-25
> **comment**: No comments
- **标题**: 调查自动抑郁症检测的声学文本情感不一致信息
- **领域**: 音频和语音处理,人工智能,计算语言学
- **摘要**: 先前的研究表明，单个声学情感标签的情绪特征可以提高抑郁症的诊断精度。此外，根据情绪上下文 - 不敏感理论和我们的试点研究，患有抑郁症的人可能会以意外平静的方式传达负面情绪内容，从而在自然对话中表现出高度的情绪表情不一致。到目前为止，很少有研究认识到并利用了抑郁症检测的情绪表达不一致。在本文中，提出了一种多模式的跨意义方法，以捕获声学文字情绪不一致（ATEI）信息。这是通过分析声学和文本领域的情绪表达的复杂的本地和长期依赖，以及两个领域内情绪内容之间的不匹配。然后提出了一个基于变压器的模型，以将此ATEI信息与检测抑郁症的各种融合策略相结合。此外，采用了一种缩放技术来调整融合过程中的ATEI特征程度，从而增强了该模型在不同程度的不同程度上识别抑郁症患者的能力。据我们所知，这项工作是第一个将情感表达不一致信息纳入抑郁症检测中的作品。咨询对话数据集的实验结果说明了我们方法的有效性。

### Enhancing Multimodal Emotion Recognition through Multi-Granularity Cross-Modal Alignment 
[[arxiv](https://arxiv.org/abs/2412.20821)] [[cool](https://papers.cool/arxiv/2412.20821)] [[pdf](https://arxiv.org/pdf/2412.20821)]
> **Authors**: Xuechen Wang,Shiwan Zhao,Haoqin Sun,Hui Wang,Jiaming Zhou,Yong Qin
> **First submission**: 2024-12-30
> **First announcement**: 2024-12-31
> **comment**: ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
- **标题**: 通过多晶格跨模式对齐增强多模式情绪识别
- **领域**: 音频和语音处理,计算语言学,声音
- **摘要**: 多模式情绪识别（MER），利用语音和文本，已成为人类计算机互动中的关键领域，要求采用有效的多模式整合的复杂方法。在这些方式中对齐特征的挑战是重要的，大多数现有方法都采用了单一的一致性策略。如此狭窄的重点不仅限制了模型性能，而且还无法解决情感表达中固有的复杂性和歧义。作为回应，本文介绍了多个跨模式对齐（MGCMA）框架，其全面方法包括基于基于实例的，基于实例和令牌的对齐模块。该框架使跨模式的情感信息具有多层次的感知。我们对Iemocap的实验表明，我们提出的方法的表现优于当前最新技术。

## 图像和视频处理(eess.IV:Image and Video Processing)

该领域共有 24 篇论文

### Multimodal Whole Slide Foundation Model for Pathology 
[[arxiv](https://arxiv.org/abs/2411.19666)] [[cool](https://papers.cool/arxiv/2411.19666)] [[pdf](https://arxiv.org/pdf/2411.19666)]
> **Authors**: Tong Ding,Sophia J. Wagner,Andrew H. Song,Richard J. Chen,Ming Y. Lu,Andrew Zhang,Anurag J. Vaidya,Guillaume Jaume,Muhammad Shaban,Ahrong Kim,Drew F. K. Williamson,Bowen Chen,Cristina Almagro-Perez,Paul Doucet,Sharifa Sahai,Chengkuan Chen,Daisuke Komura,Akihiro Kawabe,Shumpei Ishikawa,Georg Gerber,Tingying Peng,Long Phi Le,Faisal Mahmood
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: The code is accessible at https://github.com/mahmoodlab/TITAN
- **标题**: 多模式的整体幻灯片基础病理模型
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习,应用领域
- **摘要**: 在基础病理学模型中的最新进展将组织病理学区域（ROI）（ROIS）通过自我监督学习（SSL）编码为多功能且可转移的特征表示形式，已转化为计​​算病理学领域。但是，将这些进步转化为应对患者和幻灯片水平的复杂临床挑战仍然受到疾病特异性队列中的临床数据有限的限制，尤其是对于罕见的临床状况。我们提出了泰坦（Titan），这是一种多模式的整个幻灯基础模型，使用335,645 WSIS通过视觉自我监督学习和视力语言对齐进行了预测，并具有相应的病理学报告和423,122个合成字幕，从多模式的AI生成AI病理学生成。泰坦无需任何固定或需要临床标签，可以提取通用幻灯片表示，并生成病理报告，这些报告将其推广到资源有限的临床情况，例如稀有疾病检索和癌症的预后。我们评估了泰坦的各种临床任务，发现泰坦在机器学习设置（例如线性探测，很少射击和零照片分类，罕见的癌症检索和跨模式检索和病理报告生成）上都优于ROI和幻灯基础模型。

### Enhancing AI microscopy for foodborne bacterial classification via adversarial domain adaptation across optical and biological variability 
[[arxiv](https://arxiv.org/abs/2411.19514)] [[cool](https://papers.cool/arxiv/2411.19514)] [[pdf](https://arxiv.org/pdf/2411.19514)]
> **Authors**: Siddhartha Bhattacharya,Aarham Wasit,Mason Earles,Nitin Nitin,Luyao Ma,Jiyoon Yi
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-02
> **comment**: No comments
- **标题**: 通过跨光学和生物学变异性，增强了通过对抗结构域适应的粮食源性细菌分类的AI显微镜
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 快速检测食品生存细菌对于食品安全和质量至关重要，但是传统的基于培养的方法需要扩展孵化和专门的样品制备。这项研究通过i）提高使用对抗结构域适应和ii）比较单目标和多域适应性的性能来解决i）增强AI支持显微镜对细菌分类的普遍性。分类了三个革兰氏阳性（核芽孢杆菌，枯草芽孢杆菌，李斯特氏菌）和三个革兰氏阴（大肠杆菌，肠肠菌，沙门氏菌，鼠伤寒沙门氏菌）菌株。有效NETV2用作骨干结构，利用小目标的细粒特征提取。很少有学习启用的可伸缩性，域 - 逆转表神经网络（DANNS）解决了在所有目标域中概括的单个域和多人（Mdanns）。对在受控条件下收集的源域数据（相对造影显微镜，60倍放大倍率，3-H细菌孵育）中收集的源域数据进行了培训，并对具有显微镜模态（Brightfield，BF），放大倍率（20倍）和扩展孵化的目标结构域进行了评估，以补偿下分辨率（20 x-5H）。 DANN将目标域分类精度提高了54.45％（20倍），43.44％（20x-5h）和31.67％（BF），而源域降级最小（<4.44％）。 Mdanns在BF域中取得了出色的性能，并在20倍域中获得了可观的增长。 Grad-CAM和T-SNE可视化验证了该模型在不同条件下学习域不变特征的能力。这项研究为细菌分类提供了一个可扩展且适应性的框架，减少了对样本制备的依赖，并在分散和资源有限的环境中启用了应用。

### ASANet: Asymmetric Semantic Aligning Network for RGB and SAR image land cover classification 
[[arxiv](https://arxiv.org/abs/2412.02044)] [[cool](https://papers.cool/arxiv/2412.02044)] [[pdf](https://arxiv.org/pdf/2412.02044)]
> **Authors**: Pan Zhang,Baochai Peng,Chaoran Lu,Quanjin Huang
> **First submission**: 2024-12-02
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: Asanet：RGB和SAR图像土地覆盖分类的不对称语义对齐网络
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 事实证明，合成孔径雷达（SAR）图像是与RGB图像结合使用时多模式土地覆盖分类（LCC）的宝贵提示。关于跨模式融合的大多数现有研究都认为两种模式之间必要一致的特征信息，因此，它们构建网络而没有充分解决每种模式的独特特征。在本文中，我们提出了一种新颖的体系结构，称为非对称语义对齐网络（ASANET），该网络在功能级别引入了不对称性，以解决多模式体系结构经常无法完全利用互补功能的问题。该网络的核心是语义焦点模块（SFM），该模块明确计算了每种模态的差异权重，以说明特定于模态特征的特征。此外，Asanet结合了级联融合模块（CFM），该模块（CFM）更深入地研究到通道和空间表示中，以有效地从两种方式中选择融合方式的特征。通过这两个模块的协作工作，提出的ASANET有效地学习了两种模式之间的相关性，并消除了特征差异引起的噪声。全面的实验表明，Asanet在三个多模式数据集上取得了出色的性能。此外，我们已经建立了一个新的RGB-SAR多模式数据集，在该数据集上，我们的Asanet优于其他主流方法，改进范围为1.21％至17.69％。当输入图像为256x256像素时，ASANET以每秒48.7帧的速度运行。源代码可从https://github.com/whu-pzhang/asanet获得

### Enhancing Brain Age Estimation with a Multimodal 3D CNN Approach Combining Structural MRI and AI-Synthesized Cerebral Blood Volume Data 
[[arxiv](https://arxiv.org/abs/2412.01865)] [[cool](https://papers.cool/arxiv/2412.01865)] [[pdf](https://arxiv.org/pdf/2412.01865)]
> **Authors**: Jordan Jomsky,Zongyu Li,Yiren Zhang,Tal Nuriel,Jia Guo
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: 14 pages, 4 figures
- **标题**: 通过多模式3D CNN方法结合结构MRI和AI合成的大脑血容量数据来增强大脑年龄估计
- **领域**: 图像和视频处理,机器学习
- **摘要**: 日益增长的全球衰老人口需要改进的方法来评估大脑衰老及其相关的神经退行性变化。脑年龄差异估计（脑古）提供了一种神经影像学生物标志物，可通过预测MRI扫描的大脑年龄来理解这些变化。当前方法主要使用T1加权磁共振成像（T1W MRI）数据，仅捕获结构性大脑信息。为了解决这一限制，AI生成的大脑血容量（AICBV）数据（由非对比度MRI扫描合成）通过在标准成像中揭示了无法检测到的微妙的血液组织对比，从而提供了功能见解。我们将AICBV与T1W MRI集成在一起，以预测脑年龄，结合了结构和功能指标。我们使用基于VGG的体系结构来开发了一种深度学习模型，并使用线性回归结合了他们的预测。我们的模型在测试集（$ n = 288 $）上达到了3。95年的平均绝对错误（MAE），$ r^2 $为0.943，表现优于接受过类似数据的现有模型。我们进一步创建了基于梯度的类激活图（GRAD-CAM），以可视化最大程度地影响模型预测的大脑区域，从而为大脑衰老的结构和功能贡献提供了可解释的见解。

### Volumetric Reconstruction of Prostatectomy Specimens from Histology 
[[arxiv](https://arxiv.org/abs/2412.01855)] [[cool](https://papers.cool/arxiv/2412.01855)] [[pdf](https://arxiv.org/pdf/2412.01855)]
> **Authors**: Tom Bisson,Isil Dogan O,Iris Piwonski,Tim-Rasmus Kiehl,Georg Lukas Baumgärtner,Rita Carvalho,Peter Hufnagl,Tobias Penzkofer,Norman Zerbe,Sefer Elezkurtaj
> **First submission**: 2024-11-29
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 从组织学的前列腺切除术的体积重建
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 前列腺癌的手术治疗通常涉及器官去除，即前列腺切除术。病理学报告这些标本传达了与治疗相关的信息。除了这些报告之外，诊断过程还会产生广泛而复杂的信息，这些信息在报告中很难代表，尽管涉及的其他医学专业具有重大兴趣。 3D组织重建将允许更好的空间可视化以及与其他成像方式的组合。该领域的现有方法已被证明是劳动密集型和挑战，可以集成到临床工作流程中。 3D-sliver提供了一个简化的解决方案，该解决方案以开源3DSLICER扩展名实现。我们概述了三个特定的现实情况，以说明其提高诊断工作流程透明度的潜力，并有助于多模式研究。实施3D重建过程涉及四个子模型的3D式式套件：切片协议的数字化，基于该协议的任意3D模型的虚拟切片，使用Cooherent Point Drift Drift Algorithm使用虚拟切片的幻灯片对幻灯片进行注册，以及使用Convex Hulls Splate和Lineareal splatter和Lineareal splatter和Lineareal splatter和lineareal splatter和lineareal splatter和lineare formection drife slices。提出了三种用于采用3D传输剂的用例：病理学工作流程积分的低劳用方法和两个与研究相关的用例，说明了如何对PI-RADS预测进行回顾性评估以及形态学模式的统计学上模拟3D分布。 3D-Sliver允许改进专业之间的跨学科沟通。它的设计是为了简单的应用，可以灵活地集成到各种工作流程和用例中。在这里，我们专注于前列腺癌患者的临床护理，但未来的可能性在其他肿瘤和教育和研究中广泛。

### A Semi-Supervised Approach with Error Reflection for Echocardiography Segmentation 
[[arxiv](https://arxiv.org/abs/2412.00715)] [[cool](https://papers.cool/arxiv/2412.00715)] [[pdf](https://arxiv.org/pdf/2412.00715)]
> **Authors**: Xiaoxiang Han,Yiman Liu,Jiang Shang,Qingli Li,Jiangang Chen,Menghan Hu,Qi Zhang,Yuqi Zhang,Yan Wang
> **First submission**: 2024-12-01
> **First announcement**: 2024-12-03
> **comment**: 6 pages, 4 figure, accepted by 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM 2024)
- **标题**: 半监督方法，具有超声心动图分割的错误反射
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 从超声心动图分割内部结构对于诊断和治疗各种心脏病至关重要。半监督的学习表明了它在减轻注释稀缺性方面的能力。虽然现有的半监督方法在各种医学成像方式的图像分割方面取得了成功，但很少有人试图设计专门针对不良对比，模糊的边缘细节和超声心动图的噪音所带来的挑战。这些特征构成了基于平均教师的半监督分段中高质量伪标签的挑战。受到人类对错误实践的反思的启发，我们为超声心动图半监督分段体系结构设计了错误反射策略。该过程触发该模型以反思未标记的图像分割中的不准确性，从而增强了伪标签生成的鲁棒性。具体而言，该策略分为两个步骤。第一步称为重建反射。该网络的任务是从未标记的图像及其辅助草图的语义面具中重建真实的代理图像，同时最大程度地提高了原始输入和代理之间的结构相似性。第二步称为指导校正。重建误差映射解剖不可靠的分割区域。然后，利用更可能发生在高密度区域附近的可靠数据，以指导可能位于决策边界周围的不可靠数据的优化。此外，我们引入了一种有效的数据增强策略，称为多尺度混合策略，以最大程度地减少标记图像和未标记图像之间的经验分布差距，并感知到心脏解剖结构的各种规模。广泛的实验证明了该方法的竞争力。

### Multi-resolution Guided 3D GANs for Medical Image Translation 
[[arxiv](https://arxiv.org/abs/2412.00575)] [[cool](https://papers.cool/arxiv/2412.00575)] [[pdf](https://arxiv.org/pdf/2412.00575)]
> **Authors**: Juhyung Ha,Jong Sung Park,David Crandall,Eleftherios Garyfallidis,Xuhong Zhang
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: No comments
- **标题**: 多分辨率指导的3D GAN用于医学图像翻译
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 医学图像翻译是从一种成像方式转换为另一种成像方式的过程，以减少同一患者对多次图像采集的需求。这可以通过减少所需的时间，设备和人工来提高治疗效率。在本文中，我们介绍了一个多分辨率引导的生成对抗网络（GAN）基于3D医疗图像翻译的框架。我们的框架使用3D多分辨率注意的UNET（3D-MDAUNET）作为发电机和3D多分辨率UNET作为鉴别器，并以独特的损失函数组合（包括Voxel-Wise Gan损失和2.5D感知损失）进行了优化。我们的方法在各种成像方式，身体区域和年龄段的体积图像质量评估（IQA）中产生了有希望的结果，证明了其稳健性。此外，我们提出了合成到现实的适用性评估，作为评估合成数据在下游应用（例如分割）中的有效性的附加评估。这种全面的评估表明，我们的方法不仅产生高质量的合成医学图像，而且在临床应用中也有可能有用。我们的代码可在github.com/juhha/3d-madunet上找到。

### Initial Study On Improving Segmentation By Combining Preoperative CT And Intraoperative CBCT Using Synthetic Data 
[[arxiv](https://arxiv.org/abs/2412.02294)] [[cool](https://papers.cool/arxiv/2412.02294)] [[pdf](https://arxiv.org/pdf/2412.02294)]
> **Authors**: Maximilian E. Tschuchnig,Philipp Steininger,Michael Gadermayr
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: Accepted at BVM 2025. arXiv admin note: text overlap with arXiv:2406.11650
- **标题**: 使用合成数据结合术前CT和术中CBCT来改善细分的初步研究
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 计算机辅助干预措施使临床医生能够执行精确的，微创的程序，通常依赖于先进的成像方法。尽管经常遭受对准确解释的挑战，但锥形梁计算机断层扫描（CBCT）可用于促进计算机辅助的干预措施。尽管降级的图像质量会影响图像分析，但高质量的可用性，术前扫描为改进提供了潜力。在这里，我们考虑了术前CT和术中CBCT扫描的设置，但是，扫描之间的一致性（注册）无法模拟现实世界的情况。我们提出了一种多模式学习方法，该方法融合了大致比对的CBCT和CT扫描，并研究对分割性能的影响。在此实验中，我们使用包含带有相应素注释的真实CT和合成CBCT体积的合成生成的数据。我们表明，这种融合设置可改善细分性能，$ 18 $在$ 20 $调查的设置中。

### Video Quality Assessment: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2412.04508)] [[cool](https://papers.cool/arxiv/2412.04508)] [[pdf](https://arxiv.org/pdf/2412.04508)]
> **Authors**: Qi Zheng,Yibo Fan,Leilei Huang,Tianyu Zhu,Jiaming Liu,Zhijian Hao,Shuo Xing,Chia-Ju Chen,Xiongkuo Min,Alan C. Bovik,Zhengzhong Tu
> **First submission**: 2024-12-04
> **First announcement**: 2024-12-06
> **comment**: No comments
- **标题**: 视频质量评估：一项全面调查
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 视频质量评估（VQA）是一项重要的处理任务，旨在以高度与人类对感知质量的判断一致的方式预测视频的质量。传统的VQA模型基于自然图像和/或视频统计数据，这些模型既受到现实世界的预测图像的模型和人类视觉系统的双重模型的启发，仅在现实世界中用户生成的内容（UGC）上提供有限的预测性能，这在最近的大型VQA数据库中被大量的VQA数据库所示，其中包含大量多样化视频内容的大量视频内容。幸运的是，深度神经网络和大型多模型模型（LMM）的最新进展已在解决此问题方面取得了重大进展，与先前的手工制作模型相比，结果更好。已经开发了许多基于深度学习的VQA模型，并在此方向上取得了进步，这是由内容多样性的，大规模的人类标记的数据库驱动的，这些数据库提供了地面真相心理学的视频质量数据。在这里，我们对VQA算法开发以及使其成为可能的基准研究和数据库的最新进展进行了全面调查。我们还分析了有关研究设计和VQA算法体系结构的开放研究方向。 github链接：https：//github.com/taco-group/video-equality-ecsessment-a-comprehmiss-survey。

### Comprehensive Evaluation of Multimodal AI Models in Medical Imaging Diagnosis: From Data Augmentation to Preference-Based Comparison 
[[arxiv](https://arxiv.org/abs/2412.05536)] [[cool](https://papers.cool/arxiv/2412.05536)] [[pdf](https://arxiv.org/pdf/2412.05536)]
> **Authors**: Cailian Ruan,Chengyue Huang,Yahe Yang
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 对医学成像诊断中多模式AI模型的全面评估：从数据扩展到基于偏好的比较
- **领域**: 图像和视频处理,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 这项研究介绍了医学成像诊断中多模型模型的评估框架。我们开发了一条管道，该管道结合了数据预处理，模型推断和基于偏好的评估，并通过受控的增强将一组500例临床病例扩展到3,000个。我们的方法将医学图像与临床观察结果结合在一起，以产生评估，使用Claude 3.5十四行诗来独立评估对医师的诊断。结果表明，模型的性能各不相同，在85.27％的病例中，美洲驼（Llama 3.2-90B）的表现优于人类诊断。相比之下，BLIP2和LLAVA等专业视力模型分别在41.36％和46.77％的病例中显示出偏好。该框架突出了大型多模型在某些任务中胜过人类诊断的潜力。

### UniMIC: Towards Universal Multi-modality Perceptual Image Compression 
[[arxiv](https://arxiv.org/abs/2412.04912)] [[cool](https://papers.cool/arxiv/2412.04912)] [[pdf](https://arxiv.org/pdf/2412.04912)]
> **Authors**: Yixin Gao,Xin Li,Xiaohan Pan,Runsen Feng,Zongyu Guo,Yiting Lu,Yulin Ren,Zhibo Chen
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: UNIMIC：迈向通用多模式感知图像压缩
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 我们提出了Unimic，这是一种通用多模式图像压缩框架，打算通过发掘交叉模式生成的先验来统一对多个图像编解码器的速率 - 分数感知（RDP）优化。与大多数需要从头开始设计和优化图像编解码器的现有作品不同，我们的Unimic介绍了Visual编解码器存储库，该存储库包含了数量的代表性图像编解码器，并直接将其用作各种实际应用的基本编解码器。此外，我们提出了多元融合的文本编码，其中设计和编码可变长度的内容提示和压缩提示符，以通过多模式的条件生成来帮助感知重建。特别是，提出了普遍的感知补偿器，以通过重复使用稳定扩散的文本辅助扩散率来提高解码器一侧所有基本编解码器的看法质量。通过上述三种策略的合作，我们的UNIMIC可以对不同压缩编解码器（例如传统和可学习的编解码器）以及不同的压缩成本（例如超低比特率）进行重大改进。该代码将在https://github.com/amygyx/nimic中提供。

### Automatic Prediction of Stroke Treatment Outcomes: Latest Advances and Perspectives 
[[arxiv](https://arxiv.org/abs/2412.04812)] [[cool](https://papers.cool/arxiv/2412.04812)] [[pdf](https://arxiv.org/pdf/2412.04812)]
> **Authors**: Zeynel A. Samak,Philip Clatworthy,Majid Mirmehdi
> **First submission**: 2024-12-06
> **First announcement**: 2024-12-09
> **comment**: The paper is under consideration at Biomedical Engineering Letters (Springer)
- **标题**: 中风治疗结果的自动预测：最新进步和观点
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 中风是导致死亡率和发病率的主要全球健康问题。预测中风干预的结果可以促进临床决策并改善患者护理。参与和开发深度学习技术可以帮助分析大量和多样化的医学数据，包括脑部扫描，医疗报告和其他传感器信息，例如脑电图，ECG，EMG等。尽管医学图像分析领域中存在常见的数据标准化挑战，但中风结果预测中深度学习的未来在于使用多模式信息，包括最终的梗死数据，以更好地预测长期功能结果。本文对中风结果预测中的深度学习的最新进展和应用进行了广泛的综述，包括（i）所使用的数据和模型，（ii）成功的预测任务和衡量标准，（iii）当前的挑战和局限性，以及（iv）未来的方向和潜在收益。这项全面的审查旨在使研究人员，临床医生和政策制定者对这个快速发展和有希望的领域有最新的了解。

### QCResUNet: Joint Subject-level and Voxel-level Segmentation Quality Prediction 
[[arxiv](https://arxiv.org/abs/2412.07156)] [[cool](https://papers.cool/arxiv/2412.07156)] [[pdf](https://arxiv.org/pdf/2412.07156)]
> **Authors**: Peijie Qiu,Satrajit Chakrabarty,Phuc Nguyen,Soumyendu Sekhar Ghosh,Aristeidis Sotiras
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: QCRESUNET：联合主题级和体素级分割质量预测
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 近年来，从磁共振成像（MRI）扫描中，深度学习在自动化的脑肿瘤分割方面取得了长足的进步。但是，这些工具的可靠性受到质量不佳的分割异常值的存在，尤其是在分发样本中，这使其在临床实践中的实施变得困难。因此，需要质量控制（QC）来筛选分割结果的质量。尽管已经开发了用于分割质量筛选的大量自动QC方法，但大多数是为心脏MRI分割而设计的，该方法涉及单一模态和单一组织类型。此外，大多数先前的作品仅提供了分割质量的主题级预测，并且没有确定可能需要细化的错误零件分割。为了解决这些局限性，我们提出了一种新型的多任务深度学习架构，称为QCRESUNET，该结构可为每个可用的组织类别产生主题级分割质量量度以及体素级分割误差图。为了验证所提出方法的有效性，我们进行了实验，以评估其在评估两个不同分割任务的质量方面的性能。首先，我们旨在评估脑肿瘤分割结果的质量。对于此任务，我们对一个内部和两个外部数据集进行了实验。其次，我们旨在评估自动化心脏诊断挑战中心脏磁共振成像（MRI）数据的分割质量。所提出的方法在预测主题级分割质量指标方面达到了高性能，并准确地识别了分割误差的基础。这有可能用于指导人类的反馈以改善临床环境中的分割。

### CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images 
[[arxiv](https://arxiv.org/abs/2412.06314)] [[cool](https://papers.cool/arxiv/2412.06314)] [[pdf](https://arxiv.org/pdf/2412.06314)]
> **Authors**: Yijie Dang,Weijun Ma,Xiaohu Luo
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: CAD-UNET：胶囊网络增强的UNET体系结构，用于从CT图像中准确分割CoVID-19肺感染
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 自2019年COVID-19大流行病爆发以来，医学成像已成为诊断Covid-19肺炎的主要方式。在临床环境中，从计算机断层扫描图像中对肺部感染进行分割，可以快速准确地定量和诊断Covid-19。肺中的共同19感染的分割构成了巨大的挑战，这主要是由于地面玻璃不透明度表现出现了模糊的边界和有限的对比度。此外，浸润，肺组织和肺壁之间的混淆相似性进一步使这一分割任务复杂化。为了应对这些挑战，本文介绍了一种新型的深层网络结构，称为CAD-UNET，用于分割Covid-19肺部感染。在此体系结构中，胶囊网络被合并到现有的UNET框架中。胶囊网络代表一种与传统卷积神经网络不同的新型网络体系结构。他们利用向量进行胶囊之间的信息传递，从而促进了复杂的病变空间信息的提取。此外，我们设计一个胶囊编码器路径，并在UNET编码器和胶囊编码器之间建立一个耦合路径。这种设计最大程度地提高了两个网络结构的互补优势，同时实现了有效的信息融合。 \ noindent最后，在四个公开可用的数据集上进行了广泛的实验，其中包括二进制分割任务和多级分段任务。实验结果证明了所提出的模型的优异分割性能。该代码已在以下网址发布：https：//github.com/amanotooko-jie/cad-unet。

### BSAFusion: A Bidirectional Stepwise Feature Alignment Network for Unaligned Medical Image Fusion 
[[arxiv](https://arxiv.org/abs/2412.08050)] [[cool](https://papers.cool/arxiv/2412.08050)] [[pdf](https://arxiv.org/pdf/2412.08050)]
> **Authors**: Huafeng Li,Dayong Su,Qing Cai,Yafei Zhang
> **First submission**: 2024-12-10
> **First announcement**: 2024-12-11
> **comment**: Accepted by AAAI2025
- **标题**: Bsafusion：一个双向逐步特征对齐网络，用于非对齐的医学图像融合
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 如果可以在统一的处理框架内使用单级方法同时对齐和融合未对齐的多模式医学图像，则它不仅可以实现双重任务的相互促进，而且还有助于降低模型的复杂性。但是，该模型的设计面临着特征融合和对齐方式不兼容要求的挑战。具体而言，特征对齐需要相应的特征之间的一致性，而功能融合需要相互互补的功能。为了应对这一挑战，本文提出了一种称为双向逐步特征对准和融合（BSFA-F）策略的不一致的医学图像融合方法。为了减少模态差异对跨模式特征匹配的负面影响，我们将无模式差异特征表示（MDF-FR）方法纳入BSFA-F中。 MDF-FR利用模式特征表示头（MFRH）来集成输入图像的全局信息。通过将当前图像的MFRH中包含的信息注入其他模态图像中，它有效地减少了模态差异对特征对齐的影响，同时保留了不同图像所携带的互补信息。在特征比对方面，BSFA-F基于两个点之间的矢量位移的路径独立性采用双向逐步比对变形场预测策略。该策略解决了大跨度和单步比对中不准确的变形场预测的问题。最后，多模式特征融合块实现了对齐特征的融合。多个数据集的实验结果证明了我们方法的有效性。源代码可从https://github.com/slrl123/bsafusion获得。

### Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors 
[[arxiv](https://arxiv.org/abs/2412.08240)] [[cool](https://papers.cool/arxiv/2412.08240)] [[pdf](https://arxiv.org/pdf/2412.08240)]
> **Authors**: Ramy A. Zeineldin,Franziska Mathis-Ullrich
> **First submission**: 2024-12-11
> **First announcement**: 2024-12-12
> **comment**: Accepted in the Computer Assisted Radiology and Surgery (CARS 2024) Conference
- **标题**: 统一的HT-CNNS架构：将MRI中各种脑肿瘤从神经胶质瘤分割为小儿肿瘤的转移学习
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 从3D多模式MRI中对脑肿瘤的准确分割对于各种脑肿瘤的诊断和治疗计划至关重要。本文解决了Brats 2023所带来的挑战，并提出了一种适用于更广泛脑肿瘤的统一转移学习方法。我们介绍了HT-CNN，这是混合变压器的合奏和通过转移学习的各种脑肿瘤分割而优化的卷积神经网络。该方法从MRI数据中捕获了空间和上下文细节，并在代表常见肿瘤类型的不同数据集上进行了微调。通过转移学习，HT-CNN从一个任务中利用学习的表示形式来改善另一个任务的概括，从而利用大型数据集中的预训练模型的功能，并在特定的肿瘤类型上进行微调。我们从多个国际分布中进行了多种多样的数据集，从而确保了最常见的脑肿瘤的代表性。我们严格的评估在所有肿瘤类型中采用标准化的定量指标，以确保鲁棒性和概括性。所提出的合奏模型可在Brats验证数据集中实现优于先前获胜方法的卓越分割结果。使用DSC和HD95进行全面的定量评估证明了我们方法的有效性。定性细分预测进一步验证了我们的模型产生的高质量输出。我们的发现强调了医学图像分割中转移学习和合奏方法的潜力，这表明临床决策和患者护理方面有很大的增强。尽管面临与后处理和领域差距有关的挑战，但我们的研究为未来研究脑肿瘤细分的研究树立了新的先例。代码和模型的Docker映像已公开可用，https://hub.docker.com/r/razeineldin/ht-cnns。

### Embeddings are all you need! Achieving High Performance Medical Image Classification through Training-Free Embedding Analysis 
[[arxiv](https://arxiv.org/abs/2412.09445)] [[cool](https://papers.cool/arxiv/2412.09445)] [[pdf](https://arxiv.org/pdf/2412.09445)]
> **Authors**: Raj Hansini Khoiwal,Alan B. McMillan
> **First submission**: 2024-12-12
> **First announcement**: 2024-12-13
> **comment**: 15 pages, 7 figures, 3 tables
- **标题**: 您需要嵌入！通过无训练的嵌入分析来实现高性能医学图像分类
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 开发医学成像的人工智能（AI）和机器学习（ML）模型通常涉及大型数据集上的大量培训和测试，消耗大量的计算时间，能源和资源。有必要采用更有效的方法，这些方法可以实现相当或出色的诊断性能，而无需相关的资源负担。我们研究了使用基于嵌入的方法替换传统培训程序的可行性，该方法利用了医学图像的简洁和语义意义的表示。使用预先训练的基础模型，特定于特定于卷积神经网络（CNN），例如重新网络和多模型模型，例如对比度语言图像预训练（剪辑） - 我们生成的多类分类任务的图像嵌入。然后将简单的线性分类器应用于这些嵌入。该方法跨越了各种医学成像方式，包括视网膜图像，乳房X线摄影，皮肤镜图像和胸部X光片。将性能与使用传统方法训练和测试的基准模型进行了比较。基于嵌入的模型在接收器操作特征曲线（AUC-ROC）得分下超过了基准区域，在各种医学成像模式的多类分类任务中，最多87个百分比。值得注意的是，夹具嵌入模型达到了最高的AUC-ROC得分，表明了出色的分类性能，同时显着降低了计算需求。我们的研究表明，在医学图像分析中，利用预先训练的基础模型的嵌入可以有效替代常规，资源密集型培训和测试程序。这种基于嵌入的方法为图像分割，分类和预测提供了更有效的替代方法，并可能将AI技术集成到临床实践中。

### Macro2Micro: Cross-modal Magnetic Resonance Imaging Synthesis Leveraging Multi-scale Brain Structures 
[[arxiv](https://arxiv.org/abs/2412.11277)] [[cool](https://papers.cool/arxiv/2412.11277)] [[pdf](https://arxiv.org/pdf/2412.11277)]
> **Authors**: Sooyoung Kim,Joonwoo Kwon,Junbeom Kwon,Sangyoon Bae,Yuewei Lin,Shinjae Yoo,Jiook Cha
> **First submission**: 2024-12-15
> **First announcement**: 2024-12-16
> **comment**: The code will be made available upon acceptance
- **标题**: Macro2micro：跨模式磁共振成像合成利用多尺度大脑结构
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 从宏观解剖结构到复杂的显微镜结构（人脑）的复杂系统，需要集成的方法以充分了解其复杂性。然而，由于技术局限性和多模式磁共振成像（MRI）采集的高成本，这些量表之间的非线性关系仍然具有挑战性。在这里，我们介绍了Macro2micro，这是一个深度学习框架，使用生成性对抗网络（GAN）预测宏观结构的大脑微观结构。基于大脑组织的无尺度，自相似的性质，可以从宏观模式中推断出微观信息，从而从宏观模式中推断出Macro2micro明确地将多尺寸的大脑表示形式编码为不同的处理分支。为了进一步增强图像保真度并抑制工件，我们提出了一个简单而有效的辅助歧视者和学习目标。我们的结果表明，Macro2micro忠实地将​​T1加权MRI转换为相应的分数各向异性图像（FA）图像，与以前的方法相比，结构相似性指数量度（SSIM）的提高了6.8％，而保留单个神经生物学特征。

### Predicting Internet Connectivity in Schools: A Feasibility Study Leveraging Multi-modal Data and Location Encoders in Low-Resource Settings 
[[arxiv](https://arxiv.org/abs/2412.12188)] [[cool](https://papers.cool/arxiv/2412.12188)] [[pdf](https://arxiv.org/pdf/2412.12188)]
> **Authors**: Kelsey Doerksen,Casper Fibaek,Rochelle Schneider,Do-Hyung Kim,Isabelle Tingzon
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: 预测学校的互联网连接：一项可行性研究利用多模式数据和位置编码的低资源设置
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,社交和信息网络
- **摘要**: 学校的互联网连通性对于为学生提供现代经济体竞争所需的数字文学技能至关重要。为了使政府有效地实施学校中的数字基础设施开发，需要准确的互联网连接信息。但是，基于调查的传统方法可以超过政府的财务和容量限制。开源地球观测（EO）数据集已解锁我们从空间中观察和理解地球上社会经济状况的能力，并与机器学习（ML）结合使用，可以提供工具来规避基于地面的昂贵调查方法，以支持基础设施的发展。在本文中，我们使用EO和ML介绍了学校互联网连接预测的工作。我们详细介绍了我们的多模式，自由开放的卫星图像和调查信息数据集的创建，利用最新的地理意识到的位置编码器，并介绍了使用新的欧洲航天局PHI-LAB地理意识到的基础模型的第一个结果，以预测博茨瓦纳和Rwanda的Internet Connectivity。我们发现，使用EO和基于地面的辅助数据在两个国家 /地区的辅助数据中都能获得最佳性能，以确保准确性，F1分数和假阳性率，并强调了卢旺达基加利（Kigali）的案例研究，从空间进行了互联网连接预测的挑战。我们的工作展示了一种实用的方法，可以在低资源环境中支持数据驱动的数字基础架构开发，利用免费的信息，并通过联合国儿童基金会与欧洲航天机构Phi-Lab之间的独特合作，为社区提供清洁且标记的数据集，以向社区提供未来的研究。

### Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tumor Analysis in Multimodal MRI 
[[arxiv](https://arxiv.org/abs/2412.11849)] [[cool](https://papers.cool/arxiv/2412.11849)] [[pdf](https://arxiv.org/pdf/2412.11849)]
> **Authors**: Ramy A. Zeineldin,Franziska Mathis-Ullrich
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: Accepted at the MICCAI BraTS Challenge 2023
- **标题**: 集合学习和3D PIX2PIX用于多模式MRI中的全面脑肿瘤分析
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 由于需要在多模式磁共振成像（MRI）中分割和对受神经神经胶质瘤影响的大脑区域的高级解决方案的需求的动机，这项研究提出了一种综合方法，利用了混合变压器模型和卷积神经网络（CNNS）（CNNS）的组合学习的优势，以及3DDDDDDDDD DD PIX2 PIFTAR（GAN）的应用。我们的方法结合了稳健的肿瘤分割能力，利用轴向注意力和变压器编码器来增强空间关系建模，并能够通过3D Pix2pix gan合成生物学上可行的脑组织。这种集成的方法通过提供精确的细分和现实的涂料来解决Brats 2023集群挑战，该挑战是针对各种肿瘤类型和子区域量身定制的。结果表明，通过定量评估（例如骰子相似性系数（DSC），Hausdorff距离（HD95）进行分割的骰子相似性系数（DSC），结构相似性指数指数（SSIM），峰信噪比（PSNR）和平均方误差（MSE）用于中启动。定性评估进一步验证了高质量的临床相关产出。总之，这项研究强调了将高级机器学习技术结合起来进行全面脑肿瘤分析的潜力，并有望在医学成像领域内临床决策和患者护理方面取得重大进步。

### Point Cloud-Assisted Neural Image Compression 
[[arxiv](https://arxiv.org/abs/2412.11771)] [[cool](https://papers.cool/arxiv/2412.11771)] [[pdf](https://arxiv.org/pdf/2412.11771)]
> **Authors**: Ziqun Li,Qi Zhang,Xiaofeng Huang,Zhao Wang,Siwei Ma,Wei Yan
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-17
> **comment**: No comments
- **标题**: 点云辅助神经图像压缩
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 高效的图像压缩是关键要求。在某种情况下，不同的传感器捕获了多种数据模式，其他模式的辅助信息并未被现有仅图像的编解码器完全利用，从而导致次优压缩效率。在本文中，我们在Point Cloud的帮助下提高了图像压缩性能，该云在自动驾驶领域被广泛采用。我们首先将两种模式的数据表示形式统一以促进数据处理。然后，我们提出了点云辅助神经图像编解码器（PCA-NIC），以利用高维点云信息来增强图像纹理和结构的保存。我们进一步引入了一个多模式特征融合变换模块（MMFFT），以捕获更多代表性的图像特征，删除与图像内容无关的通道和模式之间的冗余信息。我们的工作是第一个使用点云改善图像压缩性能并实现最先进的性能的工作。

### Stable Diffusion is a Natural Cross-Modal Decoder for Layered AI-generated Image Compression 
[[arxiv](https://arxiv.org/abs/2412.12982)] [[cool](https://papers.cool/arxiv/2412.12982)] [[pdf](https://arxiv.org/pdf/2412.12982)]
> **Authors**: Ruijie Chen,Qi Mao,Zhengxue Cheng
> **First submission**: 2024-12-17
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: 稳定的扩散是用于分层AI生成图像压缩的天然跨模式解码器
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 人工智能产生的内容（AIGC）的最新进展引起了人们的重大兴趣，伴随着越来越需要传输和压缩大量AI生成的图像（AIGIS）的需求。但是，研究重点是AIGIS的压缩方法存在明显的缺陷。为了解决这个关键的差距，我们引入了一个可扩展的跨模式压缩框架，该框架结合了多种可理想的方式，旨在有效地捕获和继电器为AIGIS捕获和中继基本的视觉信息。特别是，我们的框架将图像编码为由语义层组成的分层bottream，该语义层通过文本提示提供高级语义信息。使用边缘或骨骼图捕获空间细节的结构层；以及通过colormap保留本地纹理的纹理层。将稳定的扩散作为后端，该框架有效地利用了这些多模式先验的图像生成，在对这些先验进行编码时有效地充当解码器。定性和定量结果表明，我们的方法能够熟练地恢复语义和视觉细节，并在极低的比特率（<0.02 bpp）上与基线方法竞争。此外，我们的框架促进了下游编辑应用程序而无需完整的解码，从而为未来的AIGI压缩铺平了新的方向。

### Precision ICU Resource Planning: A Multimodal Model for Brain Surgery Outcomes 
[[arxiv](https://arxiv.org/abs/2412.15818)] [[cool](https://papers.cool/arxiv/2412.15818)] [[pdf](https://arxiv.org/pdf/2412.15818)]
> **Authors**: Maximilian Fischer,Florian M. Hauptmann,Robin Peretzke,Paul Naser,Peter Neher,Jan-Oliver Neumann,Klaus Maier-Hein
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: No comments
- **标题**: 精密ICU资源计划：脑手术结果的多模式模型
- **领域**: 图像和视频处理,计算机视觉和模式识别,神经元和认知
- **摘要**: 尽管脑外科手术技术的进步导致需要重症监护病房（ICU）监测的术后并发症较少，但尽管其成本很高，但将患者常规转移到ICU仍然是临床标准。基于临床数据的预测梯度增强树已试图通过术前识别关键风险因素来优化ICU入院。但是，这些方法忽略了有价值的成像数据，可以提高预测准确性。在这项工作中，我们表明，将临床数据与成像数据相结合的多模式方法优于当前的临床数据，仅使用术前临床数据，从0.29 [f1]到0.30 [F1]，并且用于术前和术后数据，仅使用术前临床数据，从0.37 [F1]到0.41 [F1] [F1] [F1] [F1]。这项研究表明，有效的ICU入院预测受到多模式数据融合的好处，尤其是在严重的阶级失衡的情况下。

### Diff4MMLiTS: Advanced Multimodal Liver Tumor Segmentation via Diffusion-Based Image Synthesis and Alignment 
[[arxiv](https://arxiv.org/abs/2412.20418)] [[cool](https://papers.cool/arxiv/2412.20418)] [[pdf](https://arxiv.org/pdf/2412.20418)]
> **Authors**: Shiyun Chen,Li Lin,Pujin Cheng,ZhiCheng Jin,JianJian Chen,HaiDong Zhu,Kenneth K. Y. Wong,Xiaoying Tang
> **First submission**: 2024-12-29
> **First announcement**: 2024-12-30
> **comment**: No comments
- **标题**: DIFF4MMLIT：通过基于扩散的图像合成和比对，高级多模式肝肿瘤分割
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 由于不同的数据方式提供了多种观点，因此已证明多模式学习可以提高各种临床任务的性能。但是，现有的多模式分割方法依赖于经过良好的多模式数据，这对于现实世界中的临床图像是不现实的，尤其是对于肝肿瘤等模糊和弥漫性区域而言。在本文中，我们引入了DIFF4MMLIT，这是一个四阶段的多模式肝脏肿瘤分割管道：在多模式CTS中靶器官的预注册；注释模态的掩膜扩张，然后在介入中使用以获得无肿瘤的多模式正常CT；使用基于多模态CT特征和随机生成的肿瘤掩模的潜扩散模型和肿瘤的严格比对多模式CT的合成；最后，训练分割模型，从而消除了严格对齐的多模式数据的需求。公共和内部数据集的广泛实验证明了diff4mmlits优于其他最先进的多模式分割方法。

## 信号处理(eess.SP:Signal Processing)

该领域共有 4 篇论文

### VR Based Emotion Recognition Using Deep Multimodal Fusion With Biosignals Across Multiple Anatomical Domains 
[[arxiv](https://arxiv.org/abs/2412.02283)] [[cool](https://papers.cool/arxiv/2412.02283)] [[pdf](https://arxiv.org/pdf/2412.02283)]
> **Authors**: Pubudu L. Indrasiri,Bipasha Kashyap,Chandima Kolambahewage,Bahareh Nakisa,Kiran Ijaz,Pubudu N. Pathirana
> **First submission**: 2024-12-03
> **First announcement**: 2024-12-04
> **comment**: 14 pages, 6 figures
- **标题**: 基于VR的情绪识别，使用深层多模式融合与多个解剖领域的生物信号
- **领域**: 信号处理,人工智能
- **摘要**: 通过整合来自多个领域的多模式生物信号和IMU数据，可以显着增强情绪识别。在本文中，我们引入了一种新型的多尺度基于注意力的LSTM架构，并结合挤压和兴奋（SE）块，通过利用头部（Meta Quest Pro VR耳机），躯干（Equivital Vest）和外围（Empatica Embrace Plus）在通过视觉刺激效果的过程中。每次刺激后，记录了来自23名参与者的信号，以及自我评估的价和唤醒等级。 LSTM层从每种模式中提取特征，而多尺度的注意力捕获了细粒的时间依赖性，而SE会阻止分类前重新校准特征的重要性。我们评估哪个领域的信号在VR体验期间具有最独特的情感信息，确定有助于情感检测的关键生物信号。在用户研究中验证的拟议体系结构表明，在对价值和唤醒水平（高 /低）分类方面表现出了卓越的性能，展示了多域和多模式融合与生物信号（例如Temp，EDA）在IMU数据中（E.G.，ACCELEREMETER）在现实应用中的情绪识别的功效。

### Adaptive Signal Analysis for Automated Subsurface Defect Detection Using Impact Echo in Concrete Slabs 
[[arxiv](https://arxiv.org/abs/2412.17953)] [[cool](https://papers.cool/arxiv/2412.17953)] [[pdf](https://arxiv.org/pdf/2412.17953)]
> **Authors**: Deepthi Pavurala,Duoduo Liao,Chaithra Reddy Pasunuru
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: Accepted by IEEE Big Data 2024
- **标题**: 使用混凝土板中的冲击回波自动化地下缺陷检测的自适应信号分析
- **领域**: 信号处理,人工智能,计算机视觉和模式识别
- **摘要**: 这项试验研究提出了一种新型，自动化和可扩展的方法，用于使用Impact Echo（IE）信号分析检测和评估混凝土板中的地下缺陷区域。该方法集成了高级信号处理，聚类和视觉分析以识别地下异常。独特的自适应阈值方法量身定制了基于频率的缺陷识别每个平板的不同材料特性。该方法生成频率图，二进制掩码和K-均值群集图以自动对缺陷和非缺陷区域进行分类。主要的可视化（包括3D表面图，簇图和轮廓图）用于分析空间频率分布并突出结构异常。该研究利用了在联邦高速公路管理局（FHWA）高级传感技术非破坏性评估实验室中构建的标签数据集。评估涉及地面真相掩蔽，将生成的缺陷图与FHWA提供的信息得出的顶级二进制掩码进行比较。性能指标，特别是F1得分和AUC-ROC，分别达到高达0.95和0.83的值。结果证明了该方法的鲁棒性，始终识别出最小的假阳性和几乎没有缺陷的缺陷领域。自适应频率阈值可确保灵活地解决板之间的变化，从而为检测结构异常提供了可扩展的框架。此外，由于其可推广的阈值机制，该方法可适应其他基于频率的信号，并具有整合多模式传感器融合的潜力。这种自动化和可扩展的管道可最大程度地减少手动干预，确保准确有效的缺陷检测，进一步推进非破坏性评估（NDE）技术。

### Canine EEG Helps Human: Cross-Species and Cross-Modality Epileptic Seizure Detection via Multi-Space Alignment 
[[arxiv](https://arxiv.org/abs/2412.17842)] [[cool](https://papers.cool/arxiv/2412.17842)] [[pdf](https://arxiv.org/pdf/2412.17842)]
> **Authors**: Z. Wang,S. Li,Dongrui Wu
> **First submission**: 2024-12-18
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 犬类脑电图有助于人类：跨物种和跨模性癫痫发作检测通过多空间比对
- **领域**: 信号处理,机器学习
- **摘要**: 癫痫病显着影响全球健康，以及各种动物物种影响全球约6500万人。癫痫的诊断过程通常受到癫痫发作的短暂和不可预测的性质的阻碍。在这里，我们提出了一种基于跨物种和跨模式脑电图（EEG）数据的多空位对准方法，以增强对癫痫发作的检测能力和理解。通过采用深度学习技术，包括域的适应和知识蒸馏，我们的框架使跨物种和跨模式EEG信号保持一致，以增强传统内部内部和模式模型的检测能力。在人类和犬类的多个表面和颅内脑电图数据集上进行的实验表明，检测准确性有了很大的提高，可实现超过90％的AUC分数，用于跨物种和跨模式癫痫发作检测，具有极有限的目标物种/模态的标记数据。据我们所知，这是第一项研究，该研究证明了整合来自不同物种和模式的异质数据以改善基于脑电图的癫痫发作检测性能的有效性。该方法也可以推广到不同的大脑计算机界面范例，并提出结合来自不同物种/模态的数据以增加大型脑电图模型的训练数据量的可能性。

### MANGO: Multimodal Acuity traNsformer for intelliGent ICU Outcomes 
[[arxiv](https://arxiv.org/abs/2412.17832)] [[cool](https://papers.cool/arxiv/2412.17832)] [[pdf](https://arxiv.org/pdf/2412.17832)]
> **Authors**: Jiaqing Zhang,Miguel Contreras,Sabyasachi Bandyopadhyay,Andrea Davidson,Jessica Sena,Yuanfang Ren,Ziyuan Guan,Tezcan Ozrazgat-Baslanti,Tyler J. Loftus,Subhash Nerella,Azra Bihorac,Parisa Rashidi
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-24
> **comment**: No comments
- **标题**: 芒果：智能ICU结果的多模式敏锐变压器
- **领域**: 信号处理,人工智能,机器学习
- **摘要**: 重症监护室（ICU）中患者敏锐度的估计对于确保及时和适当的干预至关重要。人工智能（AI）技术的进步显着提高了敏锐度预测的准确性。但是，先前使用机器学习进行敏锐度预测的研究主要依赖于电子健康记录（EHR）数据，通常忽略ICU停留的其他关键方面，例如患者流动性，环境因素以及表明疼痛或躁动的面部提示。为了解决这一差距，我们提出芒果：智能ICU结果的多模式敏锐变压器，旨在增强患者敏锐度状态，过渡和对生命维持治疗的需求的预测。我们收集了一个多模式数据集ICU-MultiModal，其中包含了四个关键模式，EHR数据，可穿戴传感器数据，患者的面部提示视频以及我们用于训练芒果的环境传感器数据。芒果模型采用由变压器掩盖的自我发项方法提供动力的多模式融合网络，即使在某些模态不存在时，也可以捕获和学习这些不同数据模式的复杂相互作用。我们的结果表明，整合多种方式可显着提高该模型预测敏锐度状态，过渡和维持生命治疗的需求的能力。表现最佳的模型在接收器操作特征曲线（AUROC）下达到了0.76（95％CI：0.72-0.79），以预测敏锐度状态的过渡和维持生命的治疗的需求，而0.82（95％CI：0.69-0.89）用于敏锐的状态预测...

## 优化与控制(math.OC:Optimization and Control)

该领域共有 1 篇论文

### Global Search of Optimal Spacecraft Trajectories using Amortization and Deep Generative Models 
[[arxiv](https://arxiv.org/abs/2412.20023)] [[cool](https://papers.cool/arxiv/2412.20023)] [[pdf](https://arxiv.org/pdf/2412.20023)]
> **Authors**: Ryne Beeson,Anjian Li,Amlan Sinha
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: 47 pages, 23 figures, initial content of this paper appears in Paper 23-352 at the AAS/AIAA Astrodynamics Specialist Conference, Big Sky, MT, August 13-17 2023
- **标题**: 使用摊销和深层生成模型对最佳航天器轨迹的全局搜索
- **领域**: 优化与控制,机器学习,系统与控制
- **摘要**: 初步航天器轨迹优化是一个依赖参数的全局搜索问题，旨在提供一组高质量和多样化的解决方案。在数值解决方案的情况下，它取决于原始的最佳控制问题，控制转录的选择以及基于梯度的数值求解器的行为。在本文中，我们将参数化的全局搜索问题提出，作为对有条件的概率分布进行采样的任务，并在吸引高质量解决方案的当地盆地附近支持​​。使用深层生成模型学习并表示条件分布，以预测当地盆地的变化如何变化。该方法以低推力航天器轨迹优化问题为基准，在圆形限制的三体问题中，显示出对简单的多启动方法和香草机器学习方法的显着加速。该论文还对低临床航天器轨迹优化问题的多模式漏斗结构进行了深入分析。

## 化学物理(physics.chem-ph:Chemical Physics)

该领域共有 1 篇论文

### From Generalist to Specialist: A Survey of Large Language Models for Chemistry 
[[arxiv](https://arxiv.org/abs/2412.19994)] [[cool](https://papers.cool/arxiv/2412.19994)] [[pdf](https://arxiv.org/pdf/2412.19994)]
> **Authors**: Yang Han,Ziping Wan,Lu Chen,Kai Yu,Xin Chen
> **First submission**: 2024-12-27
> **First announcement**: 2024-12-30
> **comment**: COLING2025,We maintain an up-to-date Github repository at: https://github.com/OpenDFM/LLM4Chemistry
- **标题**: 从通才到专家：化学大语模型的调查
- **领域**: 化学物理,人工智能,计算语言学,机器学习
- **摘要**: 大型语言模型（LLMS）显着改变了我们的日常生活，并建立了自然语言处理（NLP）的新范式。但是，在广泛的基于Web的文本上，LLM的主要预读是不足以进行先进的科学发现，尤其是在化学方面。专业化学数据的稀缺性，再加上多模式数据的复杂性，例如2D图，3D结构和频谱，提出了不同的挑战。尽管几项研究已经回顾了化学方面的语言模型（PLM），但有明显的系统调查专门针对化学的LLM。在本文中，我们概述了将域特异性化学知识和多模式信息纳入LLM的方法，我们还将化学LLMS概念化为使用化学工具的代理，并研究了它们加速科学研究的潜力。此外，我们总结了现有的基准测试，以评估LLM的化学能力。最后，我们严格研究当前的挑战，并确定未来研究的有希望的方向。通过这项综合调查，我们旨在帮助研究人员保持化学LLMS开发的最前沿，并激发该领域的创新应用。

## 地球物理学(physics.geo-ph:Geophysics)

该领域共有 1 篇论文

### Geological and Well prior assisted full waveform inversion using conditional diffusion models 
[[arxiv](https://arxiv.org/abs/2412.06959)] [[cool](https://papers.cool/arxiv/2412.06959)] [[pdf](https://arxiv.org/pdf/2412.06959)]
> **Authors**: Fu Wang,Xinquan Huang,Tariq Alkhalifah
> **First submission**: 2024-12-09
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: 地质和良好的辅助全波形反演，使用条件扩散模型
- **领域**: 地球物理学,机器学习
- **摘要**: 全波形反演（FWI）经常由于地震观测不足而面临挑战，从而导致带限制和地质不准确的反转结果。从潜在的速度分布，井信息信息以及我们的地质知识和期望中纳入先前的信息可​​以显着改善FWI融合到现实的模型。尽管与常规FWI相比，通过纳入速度分布相比，扩散调查的FWI表现出改善的性能，但它可以通过融合良好的信息信息和其他地质知识先验来使其受益更多。为了利用这一事实，我们提出了使用条件扩散模型的地质类别和良好信息提交的FWI。这种方法无缝地将多模式信息整合到FWI中，同时获得数据拟合以及通用地质和地球物理学先前匹配，这通常是通过传统的正规化方法实现的。具体而言，我们建议将条件扩散模型与FWI相结合，在此将井log数据和地质类别条件整合到这些条件扩散模型中，使用无分类器指导为多模式的先验匹配，而不是原始速度分布。 OpenFWI数据集和现场海洋数据的数值实验证明了我们方法的有效性与常规FWI和无条件扩散调查的FWI相比。

## 生物分子(q-bio.BM:Biomolecules)

该领域共有 2 篇论文

### Multi-modal Representation Learning Enables Accurate Protein Function Prediction in Low-Data Setting 
[[arxiv](https://arxiv.org/abs/2412.08649)] [[cool](https://papers.cool/arxiv/2412.08649)] [[pdf](https://arxiv.org/pdf/2412.08649)]
> **Authors**: Serbülent Ünsal,Sinem Özdemir,Bünyamin Kasap,M. Erşan Kalaycı,Kemal Turhan,Tunca Doğan,Aybar C. Acar
> **First submission**: 2024-11-22
> **First announcement**: 2024-12-12
> **comment**: mber:kansil-202412-01
- **标题**: 多模式表示学习可以在低数据设置中进行准确的蛋白质功能预测
- **领域**: 生物分子,机器学习
- **摘要**: 在这项研究中，我们提出了Hoper（整体蛋白质表示），这是一种新型的多模式学习框架，旨在在低数据设置中增强蛋白质功能预测（PFP）。预测蛋白质功能的挑战是由标记数据的有限可用性加剧了。在这种情况下，传统的机器学习模型已经在挣扎，尽管深度学习模型大量数据表现出色，但当数据稀缺时，它们也会面临困难。 Hoper通过整合三种不同的模式 - 蛋白质序列，生物医学文本和蛋白质 - 蛋白质相互作用（PPI）网络来解决这个问题，以创建全面的蛋白质表示。该模型利用自动编码器生成整体嵌入，然后使用传输学习将其用于PFP任务。在所有基因本体学类别（即分子功能，生物过程和细胞成分）上，Huper在基准数据集上的现有方法优于现有方法。此外，我们通过鉴定肺腺癌中新的免疫渗透蛋白来证明其实用性，从而提供了对潜在治疗靶标的见解。我们的结果突出了多模式表示学习在克服生物学研究中的数据限制的有效性，有可能实现更准确和可扩展的蛋白质功能预测。 Hoper源代码和数据集可在https://github.com/kansil/hoper上找到

### COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models 
[[arxiv](https://arxiv.org/abs/2412.10347)] [[cool](https://papers.cool/arxiv/2412.10347)] [[pdf](https://arxiv.org/pdf/2412.10347)]
> **Authors**: Yuchen Ren,Wenwei Han,Qianyuan Zhang,Yining Tang,Weiqiang Bai,Yuchen Cai,Lifeng Qiao,Hao Jiang,Dong Yuan,Tao Chen,Siqi Sun,Pan Tan,Wanli Ouyang,Nanqing Dong,Xinzhu Ma,Peng Ye
> **First submission**: 2024-12-13
> **First announcement**: 2024-12-16
> **comment**: No comments
- **标题**: 彗星：全面生物多摩斯评估任务和语言模型的基准
- **领域**: 生物分子,人工智能,机器学习
- **摘要**: 作为中央教条中的关键要素，DNA，RNA和蛋白质通过保证准确的遗传表达和实施在维持生命中起着至关重要的作用。尽管对这些分子的研究对医学，农业和工业等领域产生了深远的影响，但机器学习方法的多样性是从传统的统计方法中进行深度学习模型的传统统计方法，而大型语言模型在选择最适合特定任务的研究人员方面挑战了针对特定任务的最合适模型，尤其是由于缺乏全面的基础基础，尤其是针对交叉词和多机构任务。为了解决这个问题，我们介绍了第一个综合的多摩斯基准彗星（用于生物综合多摩s评估任务和语言模型的基准），旨在评估跨单词，跨词和多媒体任务的模型。首先，我们策划并开发了多种下游任务和数据集的集合，其中涵盖了DNA，RNA和蛋白质中的关键结构和功能方面，包括跨越多个OMICS级别的任务。然后，我们评估了现有的DNA，RNA和蛋白质的基础语言模型，以及新提出的多词方法，为它们在整合和分析来自不同生物学模式的数据方面的性能提供了宝贵的见解。该基准旨在定义多摩斯研究和指导未来方向的关键问题，最终通过综合和不同的OMIC数据分析来促进了解生物过程的进步。

## 定量方法(q-bio.QM:Quantitative Methods)

该领域共有 3 篇论文

### M$^{3}$-20M: A Large-Scale Multi-Modal Molecule Dataset for AI-driven Drug Design and Discovery 
[[arxiv](https://arxiv.org/abs/2412.06847)] [[cool](https://papers.cool/arxiv/2412.06847)] [[pdf](https://arxiv.org/pdf/2412.06847)]
> **Authors**: Siyuan Guo,Lexuan Wang,Chang Jin,Jinxian Wang,Han Peng,Huayang Shi,Wengen Li,Jihong Guan,Shuigeng Zhou
> **First submission**: 2024-12-07
> **First announcement**: 2024-12-10
> **comment**: No comments
- **标题**: M $^{3} $ -2000万：AI驱动药物设计和发现的大型多模式分子数据集
- **领域**: 定量方法,人工智能,机器学习
- **摘要**: 本文介绍了M $^{3} $ -2000万，这是一个大规模的多模式分子数据集，包含超过2000万个分子。旨在支持AI驱动的药物设计和发现，M $^{3} $ -2000m的分子数量是最大的现有数据集的71倍，提供了前所未有的量表，可以高度利益培训或对大型（语言）模型，具有卓越的药物设计和发现性能。该数据集集成了一维微笑，二维分子图，三维分子结构，物理化学特性以及通过Web Crawling收集的文本描述，并通过使用GPT-3.5产生，并提供每个分子的全面视图。为了证明M $^{3} $ -2000万在药物设计和发现中的功能，我们使用包括GLM4，GPT-3.5和GPT-4在内的大语言模型进行了两个关键任务：分子产生和分子属性预测的广泛实验。我们的实验结果表明，M $^{3} $ -2000万可以显着提高这两个任务的模型性能。具体而言，它使模型能够生成更多样化和有效的分子结构，并获得比现有的单模式数据集更高的属性预测准确性，该数据集验证了M $^{3} $ 2000万的价值和潜力，以支持AI-DREAD的药物设计和发现。该数据集可在\ url {https://github.com/bz99bz/m-3}上获得。

### Generative modeling of protein ensembles guided by crystallographic electron densities 
[[arxiv](https://arxiv.org/abs/2412.13223)] [[cool](https://papers.cool/arxiv/2412.13223)] [[pdf](https://arxiv.org/pdf/2412.13223)]
> **Authors**: Sai Advaith Maddipatla,Nadav Bojan Sellam,Sanketh Vedula,Ailie Marx,Alex Bronstein
> **First submission**: 2024-12-16
> **First announcement**: 2024-12-18
> **comment**: No comments
- **标题**: 由晶体学电子密度引导的蛋白质集合的生成模型
- **领域**: 定量方法,人工智能,机器学习
- **摘要**: 蛋白质是动态的，采用了构象的集合。这种构象异质性的性质印在从X射线晶体学实验获得的原始电子密度测量中。将蛋白质结构的合奏拟合到这些测量中是一个具有挑战性的，不适合的反问题。我们提出了一个非i.i.d。集合引导方法使用现有的蛋白质结构生成模型解决此问题，并证明它准确地恢复了在某些单晶体测量中观察到的复杂的多模式替代蛋白质主链构象。

### Mamba-based Deep Learning Approaches for Sleep Staging on a Wireless Multimodal Wearable System without Electroencephalography 
[[arxiv](https://arxiv.org/abs/2412.15947)] [[cool](https://papers.cool/arxiv/2412.15947)] [[pdf](https://arxiv.org/pdf/2412.15947)]
> **Authors**: Andrew H. Zhang,Alex He-Mo,Richard Fei Yin,Chunlin Li,Yuzhi Tang,Dharmendra Gurve,Veronique van der Horst,Aron S. Buchman,Nasim Montazeri Ghahjaverestan,Maged Goubran,Bo Wang,Andrew S. P. Lim
> **First submission**: 2024-12-20
> **First announcement**: 2024-12-23
> **comment**: 24 pages, 13 figures. Authors Andrew H. Zhang, Alex He-Mo, and Richard Fei Yin contributed equally
- **标题**: 基于MAMBA的深度学习方法，用于无线多模式可穿戴系统上的睡眠分期
- **领域**: 定量方法,机器学习
- **摘要**: 研究目标：我们研究了基于MAMBA的深度学习方法，用于睡眠分期，该方法在Anne One（Sibel Health，Evanston，IL）的信号上，一种非侵入性的双模块无线可穿戴系统测量胸部心电图（ECG），加速度测定法和温度和指示光摄影（PPG）和温度。方法：我们从第三级护理睡眠实验室中的360名成年人（PSG）获得了可穿戴的传感器记录。每个PSG记录都是手动评分的，这些注释是培训和评估我们模型的基础真相标签。使用其ECG通道自动对齐PSG和可穿戴传感器数据，并通过视觉检查手动确认。我们在这些录音中培训了基于曼巴省的模型，并使用了卷积发生的神经网络（CRNN）和复发性神经网络（RNN）体系结构。进行了模型变体具有相似体系结构的结合。结果：结合结束后，我们的最佳方法达到了3级（唤醒，非快速眼动[NREM]睡眠，快速眼动[REM]睡眠）平衡精度为84.02％，F1得分为84.23％，Cohen的$κ$κ$ 72.89％，MATTHEWS相关系数（MCC（MCC）的得分为73.003.003.00.00％； 4级（Wake，NREM阶段1/2 [N1/N2]，NREM阶段3 [N3]，REM）平衡精度为75.30％，F1得分为74.10％，Cohen的$κ$κ$ 61.51％，MCC得分为61.95％； 5级（Wake，N1，N2，N3，REM）平衡准确度为65.11％，F1得分为66.15％，Cohen的$κ$ 53.23％，MCC得分为54.38％。结论：深度学习模型可以从没有脑电图（EEG）的可穿戴系统中推断出主要的睡眠阶段，并且可以成功地应用于参加高等教育睡眠诊所的成年人的数据。

## 量子物理学(quant-ph:Quantum Physics)

该领域共有 1 篇论文

### MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption 
[[arxiv](https://arxiv.org/abs/2412.01858)] [[cool](https://papers.cool/arxiv/2412.01858)] [[pdf](https://arxiv.org/pdf/2412.01858)]
> **Authors**: Siddhant Dutta,Nouhaila Innan,Sadok Ben Yahia,Muhammad Shafique,David Esteban Bernal Neira
> **First submission**: 2024-11-30
> **First announcement**: 2024-12-03
> **comment**: 10 pages, 5 figures, 6 Tables. Under Review
- **标题**: MQFL-FHE：具有完全同构加密的多模式量子联合学习框架
- **领域**: 量子物理学,密码学和安全,分布式、并行和集群计算,新兴技术,机器学习
- **摘要**: 联邦学习（FL）中完全同态加密（FHE）的整合导致了数据隐私的重大进展。但是，在聚合阶段，它通常会导致汇总模型的性能下降，从而阻碍了强大的代表性概括的发展。在这项工作中，我们提出了一种新型的多模式量子联合学习框架，该框架利用量子计算来抵消FHE产生的性能下降。在FL中，我们的框架首次将专家（MQMOE）模型的多模式量子混合物与FHE结合在一起，并结合了多模式数据集，用于丰富的表示和特定于任务。我们的MQMOE框架增强了多模式数据集，基因组学和大脑MRI扫描的性能，特别是对于代表性不足的类别。我们的结果还表明，量子增强的方法减轻与FHE相关的性能降低，并提高了各种数据集的分类精度，从而验证了量子干预在增强FL隐私方面的潜力。

## 机器学习(stat.ML:Machine Learning)

该领域共有 2 篇论文

### Can Generative AI Solve Your In-Context Learning Problem? A Martingale Perspective 
[[arxiv](https://arxiv.org/abs/2412.06033)] [[cool](https://papers.cool/arxiv/2412.06033)] [[pdf](https://arxiv.org/pdf/2412.06033)]
> **Authors**: Andrew Jesson,Nicolas Beltran-Velez,David Blei
> **First submission**: 2024-12-08
> **First announcement**: 2024-12-09
> **comment**: No comments
- **标题**: 生成的AI能否解决您的文化学习问题？马丁纳尔的观点
- **领域**: 机器学习,人工智能,计算语言学,机器学习
- **摘要**: 这项工作是关于估计有条件生成模型（CGM）何时可以解决内在学习（ICL）问题的问题。秘密学习（ICL）问题包括CGM，数据集和预测任务。 CGM可能是多模式基础模型。数据集，患者历史的集合，测试结果和记录的诊断；以及将诊断传达给新患者的预测任务。贝叶斯对ICL的解释假设CGM在未知的贝叶斯模型上计算后验预测分布，该模型定义了潜在的解释和可观察数据的联合分布。从这个角度来看，贝叶斯模型批评是评估给定CGM对于ICL问题的适用性的合理方法。但是，这种方法（例如后验预测检查（PPC））通常假定我们可以从贝叶斯模型定义的可能性和后验中进行样本，而贝叶斯模型并未明确给出当代CGMS。为了解决这个问题，我们表明，何时从CGM的预测分布中进行祖先采样等于从假定的贝叶斯模型的后验预测中抽样数据集。然后，我们开发了生成的预测性$ p $ value，该价值使PPC及其堂兄成为当代CGM。然后可以在统计决策过程中使用生成预测$ p $  - 价值，以确定何时适合ICL问题。我们的方法仅需要从CGM生成查询和响应并评估其响应日志概率。我们通过大型语言模型对合成表格，成像和自然语言ICL任务进行经验评估我们的方法。

### More is Less? A Simulation-Based Approach to Dynamic Interactions between Biases in Multimodal Models 
[[arxiv](https://arxiv.org/abs/2412.17505)] [[cool](https://papers.cool/arxiv/2412.17505)] [[pdf](https://arxiv.org/pdf/2412.17505)]
> **Authors**: Mounia Drissi
> **First submission**: 2024-12-23
> **First announcement**: 2024-12-24
> **comment**: 9 pages, 3 figures
- **标题**: 更多是什么？基于仿真的方法，用于多模型中偏见之间的动态相互作用
- **领域**: 机器学习,机器学习
- **摘要**: 多模式的机器学习模型，例如结合文本和图像模式的模型，越来越多地用于关键领域，包括公共安全，安全和医疗保健。但是，这些系统从其单一模式中继承了偏见。这项研究提出了一个用于分析动态多模式偏置相互作用的系统框架。使用MMBIAS数据集，该数据集包含易于偏见的类别，例如宗教，国籍和性取向，本研究采用了基于模拟的启发式方法来计算仅文本，仅图像和多模式嵌入的偏差分数。开发了一个框架来将偏差相互作用分类为扩增（多模式偏置超过单形偏差），缓解措施（多模式偏差低于两者均低于两者）和中性（多模式偏置位于单峰偏差之间），并进行了相互作用模式和动力学的比例分析。研究结果突出显示，当文本和图像偏见是可比的，而缓解措施（11 \％）在文本偏见的主导地位，突出了图像偏置的稳定作用时，就会发生扩增（22 \％）。中性相互作用（67 \％）与较高的文本偏置无关。条件概率突出了文本在缓解和放大案例中的优势和混合贡献，强调了复杂的模态相互作用。为此，该研究鼓励使用此启发式，系统性和可解释的框架来分析多模式偏见相互作用，从而深入了解如何在基于上下文的数据集中进行多模式建模和转移性的实用应用，以对基于上下文的数据集进行动态相互作用，这对于开发公平和公平的AI模型至关重要。

## 其他论文

共有 62 篇其他论文

- [GelSight FlexiRay: Breaking Planar Limits by Harnessing Large Deformations for Flexible,Full-Coverage Multimodal Sensing](https://arxiv.org/abs/2411.18979)
  - **标题**: Gelsight Flexiray：通过利用大变形的弹性，全覆盖的多模式感应来破坏平面限制
  - **Filtered Reason**: none of cs.RO in whitelist
- [A Voice-based Triage for Type 2 Diabetes using a Conversational Virtual Assistant in the Home Environment](https://arxiv.org/abs/2411.19204)
  - **标题**: 在家庭环境中使用会话虚拟助手的2型糖尿病的语音分类
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [From Audio Deepfake Detection to AI-Generated Music Detection -- A Pathway and Overview](https://arxiv.org/abs/2412.00571)
  - **标题**: 从音频深盘检测到AI生成的音乐检测 - 途径和概述
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Alexa, I Wanna See You: Envisioning Smart Home Assistants for the Deaf and Hard-of-Hearing](https://arxiv.org/abs/2412.00514)
  - **标题**: Alexa，我想见您：设想聋哑人和听力障碍的智能家庭助手
  - **Filtered Reason**: none of cs.HC in whitelist
- [Bayesian FFT Modal Identification for Multi-setup Experimental Modal Analysis](https://arxiv.org/abs/2412.00318)
  - **标题**: 贝叶斯FFT模态识别用于多设定实验模态分析
  - **Filtered Reason**: none of cs.CE in whitelist
- [A Performance Investigation of Multimodal Multiobjective Optimization Algorithms in Solving Two Types of Real-World Problems](https://arxiv.org/abs/2412.03013)
  - **标题**: 多模式多目标优化算法的性能研究在解决两种类型的现实问题方面
  - **Filtered Reason**: none of cs.NE in whitelist
- [Fast ground-to-air transition with avian-inspired multifunctional legs](https://arxiv.org/abs/2412.02389)
  - **标题**: 以禽类风格的多功能腿快速对空中过渡
  - **Filtered Reason**: none of eess.SY,cs.RO in whitelist
- [FaaSRCA: Full Lifecycle Root Cause Analysis for Serverless Applications](https://arxiv.org/abs/2412.02239)
  - **标题**: FAASRCA：无服务器应用程序的完整生命周期根本原因分析
  - **Filtered Reason**: none of cs.SE in whitelist
- [Beyond Asymptotics: Practical Insights into Community Detection in Complex Networks](https://arxiv.org/abs/2412.03805)
  - **标题**: 超越渐近学：对复杂网络中社区发现的实用见解
  - **Filtered Reason**: none of cs.SI,stat.AP in whitelist
- [A Benchmark for Math Misconceptions: Bridging Gaps in Middle School Algebra with AI-Supported Instruction](https://arxiv.org/abs/2412.03765)
  - **标题**: 数学误解的基准：与AI支持的指令在中学代数中弥合差距
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Assessing Changes in Thinking about Troubleshooting in Physical Computing: A Clinical Interview Protocol with Failure Artifacts Scenarios](https://arxiv.org/abs/2412.03687)
  - **标题**: 评估物理计算中有关故障排除的思考的变化：一种具有失败伪像的临床访谈协议
  - **Filtered Reason**: none of cs.CY in whitelist
- [Sensing-Aided 6G Drone Communications: Real-World Datasets and Demonstration](https://arxiv.org/abs/2412.04734)
  - **标题**: 感应辅助的6G无人机通信：现实世界数据集和演示
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [Compliant Self Service Access to Secondary Use Clinical Data at Stanford Medicine](https://arxiv.org/abs/2412.04248)
  - **标题**: 在斯坦福医学上合规的自助服务访问次级使用临床数据
  - **Filtered Reason**: none of cs.DB,cs.SE in whitelist
- [CALMM-Drive: Confidence-Aware Autonomous Driving with Large Multimodal Model](https://arxiv.org/abs/2412.04209)
  - **标题**: Calmm-Drive：具有大型多模型的自动驾驶自主驾驶
  - **Filtered Reason**: none of cs.RO in whitelist
- [PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in Pohang Canal](https://arxiv.org/abs/2412.06192)
  - **标题**: Polaris数据集：Pohang运河中的海上对象检测和跟踪数据集
  - **Filtered Reason**: none of cs.RO in whitelist
- [DiTer++: Diverse Terrain and Multi-modal Dataset for Multi-Robot SLAM in Multi-session Environments](https://arxiv.org/abs/2412.05839)
  - **标题**: DITER ++：多机器人大满贯在多课程环境中的各种地形和多模式数据集
  - **Filtered Reason**: none of cs.RO in whitelist
- [pyAMPACT: A Score-Audio Alignment Toolkit for Performance Data Estimation and Multi-modal Processing](https://arxiv.org/abs/2412.05436)
  - **标题**: PYAMPACT：用于性能数据估计和多模式处理的得分原告对齐工具包
  - **Filtered Reason**: none of eess.AS,cs.SD,cs.MM in whitelist
- [Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners](https://arxiv.org/abs/2412.04917)
  - **标题**: 连续的语音令牌使LLMS稳健的多模式学习者
  - **Filtered Reason**: none of eess.SP,eess.AS,cs.SD in whitelist
- [Bridging Culture and Finance: A Multimodal Analysis of Memecoins in the Web3 Ecosystem](https://arxiv.org/abs/2412.04913)
  - **标题**: 桥接文化和金融：Web3生态系统中模因的多模式分析
  - **Filtered Reason**: none of cs.HC in whitelist
- [PERCY: A Multimodal Dataset and Conversational System for Personalized and Emotionally Aware Human-Robot Interaction](https://arxiv.org/abs/2412.04908)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,cs.ET,cs.HC in whitelist
- [A Multi-physics Model of Flow from Coronary Angiography: Insights into Microvascular Function](https://arxiv.org/abs/2412.04798)
  - **标题**: 冠状动脉血管造影的流动多物理模型：对微血管功能的见解
  - **Filtered Reason**: none of cs.CE in whitelist
- [Facade: High-Precision Insider Threat Detection Using Deep Contextual Anomaly Detection](https://arxiv.org/abs/2412.06700)
  - **标题**: 立面：使用深层上下文异常检测的高精度内部人员威胁检测
  - **Filtered Reason**: none of cs.CR in whitelist
- [MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large Language Models](https://arxiv.org/abs/2412.06660)
  - **标题**: Mumu-lalama：通过大语言模型的多模式音乐理解和发电
  - **Filtered Reason**: none of eess.AS,cs.SD,cs.MM in whitelist
- [Reasoning about Strategic Abilities in Stochastic Multi-agent Systems](https://arxiv.org/abs/2412.06509)
  - **标题**: 关于随机多代理系统中战略能力的推理
  - **Filtered Reason**: none of cs.FL,cs.GT,cs.LO in whitelist
- [Multimodal Instruction Disassembly with Covariate Shift Adaptation and Real-time Implementation](https://arxiv.org/abs/2412.07671)
  - **标题**: 多模式指令拆卸，并具有协变量改编和实时实现
  - **Filtered Reason**: none of cs.CR in whitelist
- [CoinCLIP: A Multimodal Framework for Assessing Viability in Web3 Memecoins](https://arxiv.org/abs/2412.07591)
  - **标题**: Conclip：评估Web3 Memecoins生存能力的多模式框架
  - **Filtered Reason**: none of cs.CE in whitelist
- [A Real-time Degeneracy Sensing and Compensation Method for Enhanced LiDAR SLAM](https://arxiv.org/abs/2412.07513)
  - **标题**: 增强激光雷达大满贯的实时变性感应和补偿方法
  - **Filtered Reason**: none of cs.RO in whitelist
- [RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation](https://arxiv.org/abs/2412.07215)
  - **标题**: Robomm：用于机器人操作的多模式大型模型
  - **Filtered Reason**: none of cs.RO,cs.MM in whitelist
- [Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge: Feasibility, Challenges, and Road Ahead](https://arxiv.org/abs/2412.08581)
  - **标题**: 由LLM和方案知识指导的自动肥皂剧测试：可行性，挑战和未来的道路
  - **Filtered Reason**: none of cs.SE in whitelist
- [Zeitgebers-Based User Time Perception Analysis and Data-Driven Modeling via Transformer in VR](https://arxiv.org/abs/2412.08223)
  - **标题**: 基于Zeitgebers的用户时间感知分析和通过VR中的变压器进行数据驱动的建模
  - **Filtered Reason**: none of cs.HC in whitelist
- [Speech-based Multimodel Pipeline for Vietnamese Services Quality Assessment](https://arxiv.org/abs/2412.09829)
  - **标题**: 基于言语的多模型管道越南服务质量评估
  - **Filtered Reason**: none of cs.CY in whitelist
- [The AI Assessment Scale Revisited: A Framework for Educational Assessment](https://arxiv.org/abs/2412.09029)
  - **标题**: AI评估量表重新审视：教育评估的框架
  - **Filtered Reason**: none of cs.CY in whitelist
- [MindScratch: A Visual Programming Support Tool for Classroom Learning Based on Multimodal Generative AI](https://arxiv.org/abs/2412.09001)
  - **标题**: MindsCratch：基于多模式生成AI的课堂学习的视觉编程支持工具
  - **Filtered Reason**: none of cs.HC in whitelist
- [On Scalable Design for User-Centric Multi-Modal Shared E-Mobility Systems using MILP and Modified Dijkstra's Algorithm](https://arxiv.org/abs/2412.10986)
  - **标题**: 关于使用MILP和修改Dijkstra的算法的可扩展设计的可扩展设计
  - **Filtered Reason**: none of cs.CE in whitelist
- [A Clinical Tuning Framework for Continuous Kinematic and Impedance Control of a Powered Knee-Ankle Prosthesis](https://arxiv.org/abs/2412.10154)
  - **标题**: 一个临床调整框架，用于连续运动和阻抗控制的膝盖骨骼假体
  - **Filtered Reason**: none of cs.RO in whitelist
- [Leveraging Multimodal Methods and Spontaneous Speech for Alzheimer's Disease Identification](https://arxiv.org/abs/2412.09928)
  - **标题**: 利用阿尔茨海默氏病鉴定的多模式方法和自发语音
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Multimodal Classification and Out-of-distribution Detection for Multimodal Intent Understanding](https://arxiv.org/abs/2412.12453)
  - **标题**: 多模式分类和分布外检测，以了解多模式意图
  - **Filtered Reason**: none of cs.MM in whitelist
- [SAVGBench: Benchmarking Spatially Aligned Audio-Video Generation](https://arxiv.org/abs/2412.13462)
  - **标题**: Savgbench：基准测试空间对齐的音频视频一代
  - **Filtered Reason**: none of eess.AS,cs.SD,cs.MM in whitelist
- [Detecting Machine-Generated Music with Explainability -- A Challenge and Early Benchmarks](https://arxiv.org/abs/2412.13421)
  - **标题**: 检测具有解释性的机器生成的音乐 - 挑战和早期基准
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Deep Speech Synthesis from Multimodal Articulatory Representations](https://arxiv.org/abs/2412.13387)
  - **标题**: 多模式关节表示的深层语音综合
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Gender Bias and Property Taxes](https://arxiv.org/abs/2412.12610)
  - **标题**: 性别偏见和财产税
  - **Filtered Reason**: none of cs.CY,econ.GN in whitelist
- [EPN: An Ego Vehicle Planning-Informed Network for Target Trajectory Prediction](https://arxiv.org/abs/2412.14442)
  - **标题**: EPN：目标轨迹预测的自我车辆计划信息网络
  - **Filtered Reason**: none of cs.RO in whitelist
- [Mobilizing Waldo: Evaluating Multimodal AI for Public Mobilization](https://arxiv.org/abs/2412.14210)
  - **标题**: 动员Waldo：评估多模式AI进行公共动员
  - **Filtered Reason**: none of cs.HC,cs.CY,cs.SI in whitelist
- [NeckCare: Preventing Tech Neck using Hearable-based Multimodal Sensing](https://arxiv.org/abs/2412.13579)
  - **标题**: 颈部保健：使用可听到的多模式感应防止技术颈
  - **Filtered Reason**: none of eess.AS,cs.HC,cs.SD in whitelist
- [Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data Presentation](https://arxiv.org/abs/2412.13509)
  - **标题**: Vivar：一种用于直观多模式传感器数据显示的生成AR系统
  - **Filtered Reason**: none of cs.HC in whitelist
- [SyncFlow: Toward Temporally Aligned Joint Audio-Video Generation from Text](https://arxiv.org/abs/2412.15220)
  - **标题**: 同步流：朝着文本临时对齐的联合音频视频生成
  - **Filtered Reason**: none of eess.AS,cs.SD,cs.MM in whitelist
- [Fatigue Monitoring Using Wearables and AI: Trends, Challenges, and Future Opportunities](https://arxiv.org/abs/2412.16847)
  - **标题**: 使用可穿戴设备和AI的疲劳监测：趋势，挑战和未来机会
  - **Filtered Reason**: none of cs.ET,cs.HC in whitelist
- [XR for All: Understanding Developer Perspectives on Accessibility Integration in Extended Reality](https://arxiv.org/abs/2412.16321)
  - **标题**: XR全部：了解开发人员对扩展现实中可访问性集成的观点
  - **Filtered Reason**: none of cs.HC in whitelist
- [Tracking the 2024 US Presidential Election Chatter on TikTok: A Public Multimodal Dataset](https://arxiv.org/abs/2412.15583)
  - **标题**: 在Tiktok上跟踪2024年美国总统选举chat不休：公共多模式数据集
  - **Filtered Reason**: none of cs.SI in whitelist
- [Network Models of Expertise in the Complex Task of Operating Particle Accelerators](https://arxiv.org/abs/2412.17988)
  - **标题**: 操作粒子加速器的复杂任务中的专业知识网络模型
  - **Filtered Reason**: none of eess.SY,cs.SI,stat.AP in whitelist
- [Muse: A Multimodal Conversational Recommendation Dataset with Scenario-Grounded User Profiles](https://arxiv.org/abs/2412.18416)
  - **标题**: 缪斯：带有场景的用户配置文件的多模式对话推荐数据集
  - **Filtered Reason**: none of cs.MM in whitelist
- [The Constitutional Filter: Bayesian Estimation of Compliant Agents](https://arxiv.org/abs/2412.18347)
  - **标题**: 宪法过滤器：合规代理的贝叶斯估计
  - **Filtered Reason**: none of cs.RO in whitelist
- [Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration](https://arxiv.org/abs/2412.18292)
  - **标题**: 通过多模式链得分协作增强多机器人语义导航
  - **Filtered Reason**: none of cs.RO in whitelist
- [FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with Autonomous Human-Machine Collaboration](https://arxiv.org/abs/2412.20297)
  - **标题**: FAGEL：Fabric LLMS代理人通过自动人机合作授权体现的智力演变
  - **Filtered Reason**: none of cs.HC in whitelist
- [Motion Planning Diffusion: Learning and Adapting Robot Motion Planning with Diffusion Models](https://arxiv.org/abs/2412.19948)
  - **标题**: 运动计划扩散：通过扩散模型学习和调整机器人运动计划
  - **Filtered Reason**: none of cs.RO in whitelist
- [An Actionable Hierarchical Scene Representation Enhancing Autonomous Inspection Missions in Unknown Environments](https://arxiv.org/abs/2412.19582)
  - **标题**: 可操作的层次场景表示形式增强了未知环境中的自主检查任务
  - **Filtered Reason**: none of cs.RO in whitelist
- [xFLIE: Leveraging Actionable Hierarchical Scene Representations for Autonomous Semantic-Aware Inspection Missions](https://arxiv.org/abs/2412.19571)
  - **标题**: XFLIE：利用可行的层次结构场景表示自主语义意识检查任务
  - **Filtered Reason**: none of cs.RO in whitelist
- [VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware Speech Synthesis](https://arxiv.org/abs/2412.19259)
  - **标题**: 语音：环境吸引语音综合的双条件扩散变压器
  - **Filtered Reason**: none of eess.AS,cs.SD in whitelist
- [Swarm Contract: A Multi-Sovereign Agent Consensus Mechanism](https://arxiv.org/abs/2412.19256)
  - **标题**: 群合同：多种制剂的共识机制
  - **Filtered Reason**: none of cs.CR in whitelist
- [Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations](https://arxiv.org/abs/2412.18940)
  - **标题**: 娱乐：人类与多模式灵感的合作歌曲创作
  - **Filtered Reason**: none of cs.HC in whitelist
- [High-Sensitivity Vision-Based Tactile Sensing Enhanced by Microstructures and Lightweight CNN](https://arxiv.org/abs/2412.20758)
  - **标题**: 微观结构和轻质CNN增强了基于高敏性的基于视觉的触觉感应
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Overview of the development of smart classrooms under information technology: development and innovation of hardware and software](https://arxiv.org/abs/2412.20730)
  - **标题**: 信息技术下的智能教室发展的概述：硬件和软件的开发和创新
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
