# 2023-01 月度论文分类汇总

共有115篇相关领域论文, 另有19篇其他

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 3 篇论文

### Optimization of Image Transmission in a Cooperative Semantic Communication Networks 
[[arxiv](https://arxiv.org/abs/2301.00433)] [[cool](https://papers.cool/arxiv/2301.00433)] [[pdf](https://arxiv.org/pdf/2301.00433)]
> **Authors**: Wenjing Zhang,Yining Wang,Mingzhe Chen,Tao Luo,Dusit Niyato
> **First submission**: 2023-01-01
> **First announcement**: 2023-01-03
> **comment**: 29 pages, 10 figures
- **标题**: 合作语义通信网络中图像传输的优化
- **领域**: 人工智能,计算机视觉和模式识别,信息论
- **摘要**: 在本文中，开发了图像传输的语义通信框架。在调查的框架中，一组服务器合作地将图像传输到利用语义通信技术的一组用户。为了评估研究的语义通信系统的性能，提出了多模式度量标准，以测量提取的语义信息与原始图像之间的相关性。为了满足每个用户的ISS要求，每个服务器必须共同确定要传输要传输的语义信息以及用于语义信息传输的资源块（RB）。我们将此问题提出为优化问题，旨在最大程度地减少每个服务器的传输延迟，同时达到ISS要求。为了解决此问题，提出了基于价值分解的最大最大化多代理增强学习（RL），该学习使服务器能够以分布式的方式协调培训并执行RB分配，以降低训练迭代率，以实现全球最佳性能。与传统的多代理RL相比，拟议的RL改善了服务器的宝贵动作探索以及基于本地观察的全球最佳RB分配政策的可能性。模拟结果表明，与传统的多代理RL相比，所提出的算法可以将传输延迟减少多达16.1％。

### IMKGA-SM: Interpretable Multimodal Knowledge Graph Answer Prediction via Sequence Modeling 
[[arxiv](https://arxiv.org/abs/2301.02445)] [[cool](https://papers.cool/arxiv/2301.02445)] [[pdf](https://arxiv.org/pdf/2301.02445)]
> **Authors**: Yilin Wen,Biao Luo,Yuqian Zhao
> **First submission**: 2023-01-06
> **First announcement**: 2023-01-09
> **comment**: 12pages,10 figures
- **标题**: IMKGA-SM：可解释的多模式知识图图通过序列建模的回答预测
- **领域**: 人工智能,机器学习
- **摘要**: 多模式知识图链接预测旨在提高多模式数据的链接预测任务的准确性和效率。但是，对于复杂的多模式信息和稀疏训练数据，对于大多数方法而言，通常很难同时实现可解释性和高精度。为了解决这一难度，本文开发了一个新模型，即通过序列建模（IMKGA-SM）来解释的多模式知识图形预测。首先，提出了一种多模式的细粒融合方法，采用VGG16和光学特征识别（OCR）技术来有效地从图像和图像中提取文本信息。然后，将知识图链接预测任务建模为一个离线增强学习马尔可夫决策模型，然后将其抽象为统一的序列框架。设计了一种基于交互感知的奖励期望机制和一种特殊的因果掩蔽机制，该机制将查询“转换”到推理路径。然后，提出了一种自回旋的动态梯度调整机制，以减轻多模式优化问题不足。最后，采用了两个数据集进行实验，并使用流行的SOTA基准进行比较。结果表明，在不同尺寸的多模式链接预测数据集上，开发的IMKGA-SM的性能要比SOTA基线要好得多。

### Imitating Human Behaviour with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2301.10677)] [[cool](https://papers.cool/arxiv/2301.10677)] [[pdf](https://arxiv.org/pdf/2301.10677)]
> **Authors**: Tim Pearce,Tabish Rashid,Anssi Kanervisto,Dave Bignell,Mingfei Sun,Raluca Georgescu,Sergio Valcarcel Macua,Shan Zheng Tan,Ida Momennejad,Katja Hofmann,Sam Devlin
> **First submission**: 2023-01-25
> **First announcement**: 2023-01-26
> **comment**: Published in ICLR 2023
- **标题**: 通过扩散模型模仿人类行为
- **领域**: 人工智能,机器学习,机器学习
- **摘要**: 扩散模型已成为文本到图像域中强大的生成模型。本文研究了他们作为模仿顺序环境中人类行为的观察到行动模型的应用。人类的行为是随机的和多模式的，在作用维度之间具有结构化的相关性。同时，行为克隆中的标准建模选择在其表现力上受到限制，并且可能会将偏见引入克隆政策。我们首先指出这些选择的局限性。然后，我们建议扩散模型非常适合模仿人类行为，因为它们在联合动作空间上学习了表达性分布。我们介绍了几项创新，以制造适合顺序环境的扩散模型。设计合适的体系结构，调查指导的作用并制定可靠的抽样策略。在实验上，扩散模型在模拟机器人控制任务和现代3D游戏环境中与人类示范密切相匹配。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 19 篇论文

### SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph 
[[arxiv](https://arxiv.org/abs/2301.01949)] [[cool](https://papers.cool/arxiv/2301.01949)] [[pdf](https://arxiv.org/pdf/2301.01949)]
> **Authors**: Yuxing Long,Binyuan Hui,Fulong Ye,Yanyang Li,Zhuoxin Han,Caixia Yuan,Yongbin Li,Xiaojie Wang
> **First submission**: 2023-01-05
> **First announcement**: 2023-01-06
> **comment**: AAAI 2023
- **标题**: 春季：从增量布局图中预估计的多模式问题的位置对话代理
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习,多媒体
- **摘要**: 现有的多模式对话代理显示出令人印象深刻的能力，可以在简单的情况下找到绝对位置或检索属性，但是当涉及复杂的相对位置和信息对齐时，它们的表现效果不佳，这在响应质量方面构成了瓶颈。在本文中，我们提出了一个位置的对话代理，这些对话代理带有来自增量布局图（春季）的多模式问题，具有推理多求空间关系的能力，并将它们与拥挤的位置场景中的视觉属性联系起来。具体而言，我们设计了两种类型的多模式问答（MQA）任务来预算代理。预处理过程中使用的所有QA对均通过新颖的增量布局图（ILG）产生。 ILG自动注释的QA对难度标签用于促进基于MQA的课程学习。实验结果验证了春季的有效性，表明它在SIMMC 1.0和SIMMC 2.0数据集上都显着胜过最先进的方法。

### Removing Non-Stationary Knowledge From Pre-Trained Language Models for Entity-Level Sentiment Classification in Finance 
[[arxiv](https://arxiv.org/abs/2301.03136)] [[cool](https://papers.cool/arxiv/2301.03136)] [[pdf](https://arxiv.org/pdf/2301.03136)]
> **Authors**: Guijin Son,Hanwool Lee,Nahyeon Kang,Moonjeong Hahm
> **First submission**: 2023-01-08
> **First announcement**: 2023-01-09
> **comment**: Published at The AAAI-2023 Workshop OnMultimodalAI For Financial Forecasting (muffin@AAAI2023)
- **标题**: 从预先训练的语言模型中删除非平稳知识，以用于财务中的实体级别的情感分类
- **领域**: 计算语言学,机器学习,一般财务
- **摘要**: 从新闻文本，股票留言板和商业报告中提取情感信号（用于股票移动预测）一直是财务感兴趣的领域。在过去的文学基础上，最新的作品试图通过引入方面级别的情感分类（ASC）来更好地从具有复杂句法结构的句子中捕捉情感。尽管兴趣越来越大，但由于缺少带注释的金融特定数据，在非英语文献中尚未充分探索细粒度的情感分析。因此，非英语语言有必要利用不同领域，语言和任务的数据集和预训练的语言模型（PLM）来最佳。为了促进韩国语言的特定于金融的ASC研究，我们建立了Korfinasc，这是韩国方面级别的情感分类数据集，用于由12,613个人类宣布的样本组成，并探索中间转移学习的方法。我们的实验表明，过去的研究对培训阶段编码的金融实体的潜在错误知识一无所知，这高估了PLM的预测能力。在我们的工作中，我们使用“非平稳知识”一词来指代先前正确但可能会发生变化的信息，并提出“ TGT掩蔽”，这是一种新颖的掩蔽模式，以限制PLM的猜测。最后，与Korfinasc上的独立模型相比，通过应用TGT遮罩的一系列转移学习，我们提高了分类精度的22.63％。

### Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture 
[[arxiv](https://arxiv.org/abs/2301.03127)] [[cool](https://papers.cool/arxiv/2301.03127)] [[pdf](https://arxiv.org/pdf/2301.03127)]
> **Authors**: Pim Jordi Verschuuren,Jie Gao,Adelize van Eeden,Stylianos Oikonomou,Anil Bandhakavi
> **First submission**: 2023-01-08
> **First announcement**: 2023-01-09
> **comment**: Accepted in AAAI'23: Second Workshop onMultimodalFact-Checking and Hate Speech Detection, February 2023, Washington, DC, USA
- **标题**: 逻辑上的刻度2：基于证据检索技术和变压器编码器体系结构的多模式事实检查系统
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,多媒体
- **摘要**: 在本文中，我们介绍了逻辑上的提交，以消除2个挑战（De-fatify 2023）在多模式事实检查的任务1上。我们描述了我们对这一挑战的提交，包括探讨证据检索和选择技术，预训练的跨模式和单峰模型，以及基于建立的变压器编码器（TE）体系结构的跨模式真实模型，这些模型在很大程度上依赖于自我注意的概念。还对该漏洞的2个数据集进行了探索性分析，该数据集揭示了显着的多模式模式和假设，从而激发了这项工作中提出的架构。进行了一系列初步实验，以研究和基准测试不同的预训练嵌入模型，证据检索设置和阈值。最终系统是标准的两阶段证据的真实检测系统，可产生加权的AVG。任务1上的Val设置和最终盲验测试设置为0.79，在9位参与者中，在排行榜上以较小的表现系统获得了第三名。

### Learning Bidirectional Action-Language Translation with Limited Supervision and Incongruent Input 
[[arxiv](https://arxiv.org/abs/2301.03353)] [[cool](https://papers.cool/arxiv/2301.03353)] [[pdf](https://arxiv.org/pdf/2301.03353)]
> **Authors**: Ozan Özdemir,Matthias Kerzel,Cornelius Weber,Jae Hee Lee,Muhammad Burhan Hafez,Patrick Bruns,Stefan Wermter
> **First submission**: 2023-01-09
> **First announcement**: 2023-01-10
> **comment**: Published in: Applied Artificial Intelligence, 37:1, 2179167
- **标题**: 学习双向动作语言翻译，有限的监督和不一致的输入
- **领域**: 计算语言学,人工智能,神经和进化计算,机器人技术
- **摘要**: 人类婴儿的学习发生在环境探索，与物体的互动以及随便聆听和重复发言的过程中发生，这类似于无监督的学习。只有偶尔，学习婴儿会收到对其所采取的动作的匹配口头描述，这类似于监督学习。这种学习机制可以模仿深度学习。我们使用配对的门控自动编码器（PGAE）模型对这种弱监督的学习范式进行建模，该模型结合了动作和语言自动编码器。在降低了监督训练的比例时观察到性能下降后，我们使用基于变压器的跨模式关注介绍了配对的变换自动编码器（PTAE）模型。 PTAE在语言行动和动作对语言翻译方面的准确性显着更高，尤其是在仅有的监督培训样本时，在现实但困难的情况下。我们还测试了受过训练的模型是否与矛盾的多模式输入相互作用。根据心理学不一致的概念，冲突恶化了模型输出。相互冲突的行动输入比冲突的语言输入更严重，并且更加冲突的特征会导致更大的干扰。 PTAE可以在稀缺的标记数据的主要数据上进行训练，并且在用不一致的输入进行测试时，它的行为可能会出现。

### Universal Multimodal Representation for Language Understanding 
[[arxiv](https://arxiv.org/abs/2301.03344)] [[cool](https://papers.cool/arxiv/2301.03344)] [[pdf](https://arxiv.org/pdf/2301.03344)]
> **Authors**: Zhuosheng Zhang,Kehai Chen,Rui Wang,Masao Utiyama,Eiichiro Sumita,Zuchao Li,Hai Zhao
> **First submission**: 2023-01-09
> **First announcement**: 2023-01-10
> **comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
- **标题**: 通用的多模式表示语言理解
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 表示学习是自然语言处理（NLP）的基础。这项工作提出了新方法，将视觉信息用作通用NLP任务的助理信号。对于每个句子，我们首先从现有的句子图像对中提取的光主题图像表中检索一定数量的图像，或者在现有句子图像对上提取的图像或共享的交叉模式嵌入空间，该空间已预先训练在台面外文本图像对上。然后，文本和图像分别由变压器编码器和卷积神经网络编码。两种表示的序列被注意力层进一步融合，以融合两种方式的相互作用。在这项研究中，检索过程是可控制和灵活的。普遍的视觉表示克服了缺乏大规模双语句子图像对。我们的方法可以轻松地应用于只有手动注释的多模式并行语料库的纯文本任务。我们将提出的方法应用于广泛的自然语言生成和理解任务，包括神经机器翻译，自然语言推断和语义相似性。实验结果表明，我们的方法通常对不同的任务和语言有效。分析表明，视觉信号丰富了内容词的文本表示，提供了有关概念与事件之间关系的细粒度接地信息，并有可能引发歧义。

### MAQA: A Multimodal QA Benchmark for Negation 
[[arxiv](https://arxiv.org/abs/2301.03238)] [[cool](https://papers.cool/arxiv/2301.03238)] [[pdf](https://arxiv.org/pdf/2301.03238)]
> **Authors**: Judith Yue Li,Aren Jansen,Qingqing Huang,Joonseok Lee,Ravi Ganti,Dima Kuzmin
> **First submission**: 2023-01-09
> **First announcement**: 2023-01-10
> **comment**: NeurIPS 2022 SyntheticData4ML Workshop
- **标题**: MAQA：多模式QA基准否定
- **领域**: 计算语言学,人工智能,机器学习,声音,音频和语音处理
- **摘要**: 多模式学习可以从验证的大语言模型（LLMS）的表示能力中受益。但是，最先进的变压器的LLM通常忽略了自然语言的否定，并且没有现有的基准来定量评估多模式变压器是否继承了这种弱点。在这项研究中，我们提出了一个新的多模式问答（QA）基准测试，该基准是根据Audioset中标记的音乐视频（Gemmeke等，2017）改编的，其目的是系统地评估多模式变压器是否可以执行复杂的推理，以识别新概念作为对先前学习的概念的否定概念。我们表明，使用标准的微调方法，多模式变压器仍然无法正确解释否定，而不论模型大小如何。但是，我们的实验表明，通过否定的质量检查示例扩大原始培训任务分布，使该模型可以可靠地否定。为此，我们描述了一个新颖的数据生成程序，该过程促使540B参数棕榈模型自动生成否定的QA示例作为易于访问的视频标签的组成。与基于模板的任务增强方法相比，生成的示例包含更自然的语言模式，而收益非常重要。

### MGeo: Multi-Modal Geographic Pre-Training Method 
[[arxiv](https://arxiv.org/abs/2301.04283)] [[cool](https://papers.cool/arxiv/2301.04283)] [[pdf](https://arxiv.org/pdf/2301.04283)]
> **Authors**: Ruixue Ding,Boli Chen,Pengjun Xie,Fei Huang,Xin Li,Qiang Zhang,Yao Xu
> **First submission**: 2023-01-10
> **First announcement**: 2023-01-11
> **comment**: 10 pages, 5 figures
- **标题**: MGEO：多模式地理预训练方法
- **领域**: 计算语言学
- **摘要**: 作为基于位置的服务（LBS）（例如导航图）的核心任务，查询和兴趣点（POI）匹配将用户与现实世界地理信息联系起来。最近，预训练的模型（PTM）在许多自然语言处理（NLP）任务中取得了进步。基于文本的PTMS没有足够的地理知识来查询POI匹配。为了克服这一局限性，相关的文献尝试采用基于地理相关语料库的领域自适应预训练。但是，查询通常包含多个地理对象，例如附近的道路和感兴趣的区域（ROI）。因此，地理环境（GC），即这些多样化的地理对象及其关系，是检索最相关的POI的关键。单模式PTM几乎无法使用重要的GC，因此性能有限。在这项工作中，我们提出了一种新型的查询poi匹配方法多模式地理语言模型（MGEO），该模型包括一个地理编码器和一个多模式交互模块。 MGEO表示GC是一种新的模式，并且能够完全提取多模式相关性，以进行准确的查询poi匹配。此外，此主题没有公开可用的基准。为了促进进一步的研究，我们建立了一个新的开源大规模基准地理地理文本相似性（GEOTES）。 POI来自开源地理信息系统（GIS）。查询是由注释者手动生成的，以防止隐私问题。与几个强基线相比，对地理位置的广泛实验结果和详细的消融分析表明，即使不提供查询的GC，我们提出的多模式预训练方法也可以显着提高通用PTM的查询POI匹配能力。我们的代码和数据集可在https://github.com/phantomgrapes/mgeo上公开获取。

### Few-shot Learning for Cross-Target Stance Detection by Aggregating Multimodal Embeddings 
[[arxiv](https://arxiv.org/abs/2301.04535)] [[cool](https://papers.cool/arxiv/2301.04535)] [[pdf](https://arxiv.org/pdf/2301.04535)]
> **Authors**: Parisa Jamadi Khiabani,Arkaitz Zubiaga
> **First submission**: 2023-01-11
> **First announcement**: 2023-01-12
> **comment**: To appear in IEEE Transactions on Computational Social Systems
- **标题**: 通过汇总多模式嵌入来进行跨目标姿势检测的几次学习
- **领域**: 计算语言学,社交和信息网络
- **摘要**: 尽管立场检测任务的普及越来越普及，但现有方法主要仅限于使用社交媒体帖子的文本内容进行分类，从而忽略了任务的社会性质。在跨目标分类方案中，立场检测任务变得尤为具有挑战性，即使在几次训练设置中，模型也需要预测对新目标的立场，该模型在训练过程中只有很少的相关样本。为了通过利用任务的社交性质来解决社交媒体中的跨目标立场检测，我们介绍了CT-TN，这是一个新型模型，该模型汇总了从数据的文本和网络特征中得出的多模式嵌入。我们在六个不同组合的源用途目标对组合中，在几次跨目标方案中进行实验。通过将CT-TN与最先进的跨目标姿势检测模型进行比较，我们通过在不同基线模型中实现平均性能改进来证明模型的有效性。具有不同数量的镜头的实验表明，在看到目标目标的300个实例之后，CT-TN可以胜过其他模型。此外，消融实验证明了CT-TN对最终性能的每个组件的积极贡献。我们进一步分析了社交媒体用户之间的网络交互，这揭示了将社交功能用于跨目标姿态检测的潜力。

### Multimodal Inverse Cloze Task for Knowledge-based Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2301.04366)] [[cool](https://papers.cool/arxiv/2301.04366)] [[pdf](https://arxiv.org/pdf/2301.04366)]
> **Authors**: Paul Lerner,Olivier Ferret,Camille Guinaudeau
> **First submission**: 2023-01-11
> **First announcement**: 2023-01-12
> **comment**: Accepted at ECIR 2023
- **标题**: 基于知识的视觉问题回答的多模式逆披oo任务
- **领域**: 计算语言学,信息检索,机器学习,多媒体
- **摘要**: 我们提出了一种新的预训练方法，即多模式的倒置任务，用于基于知识的视觉问题回答有关命名实体（KVQAE）。 KVQAE是一个最近引入的任务，它包括回答有关使用知识库在视觉上下文中的命名实体的问题。因此，模式之间的相互作用对于检索信息至关重要，必须使用复杂的融合模型捕获。由于这些模型需要大量的培训数据，因此我们在文本问答中从现有工作中设计了此预训练任务。它在于将句子视为伪疑问及其上下文作为伪有效的段落，并通过考虑多模式文档中文本附近的图像来扩展。我们的方法适用于不同的神经网络体系结构，并在无预先训练基线的基线上分别带来9％的相对MRR和15％的相对F1增益。

### Multimodal Deep Learning 
[[arxiv](https://arxiv.org/abs/2301.04856)] [[cool](https://papers.cool/arxiv/2301.04856)] [[pdf](https://arxiv.org/pdf/2301.04856)]
> **Authors**: Cem Akkus,Luyang Chu,Vladana Djakovic,Steffen Jauch-Walser,Philipp Koch,Giacomo Loss,Christopher Marquardt,Marco Moldovan,Nadja Sauter,Maximilian Schneider,Rickmer Schulte,Karol Urbanczyk,Jann Goschenhofer,Christian Heumann,Rasmus Hvingelby,Daniel Schalk,Matthias Aßenmacher
> **First submission**: 2023-01-12
> **First announcement**: 2023-01-13
> **comment**: No comments
- **标题**: 多模式深度学习
- **领域**: 计算语言学,机器学习,机器学习
- **摘要**: 这本书是一个研讨会的结果，我们在该研讨会上审查了多模式方法，并试图创建该领域的可靠概述，从深度学习的两个子领域的当前最新方法开始。此外，讨论了一种模型框架，其中一种模态被转化为另一种模式，以及一种模型，其中一种模式被用来增强另一种方式的表示表示。总结第二部分，介绍了同时介绍两种方式的架构。最后，我们还涵盖了其他模式以及通用多模式模型，这些模型能够处理一个统一体系结构中不同模式的不同任务。一本有趣的应用程序（生成艺术）最终限制了这本小册子。

### TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World 
[[arxiv](https://arxiv.org/abs/2301.05880)] [[cool](https://papers.cool/arxiv/2301.05880)] [[pdf](https://arxiv.org/pdf/2301.05880)]
> **Authors**: Hongpeng Lin,Ludan Ruan,Wenke Xia,Peiyu Liu,Jingyuan Wen,Yixin Xu,Di Hu,Ruihua Song,Wayne Xin Zhao,Qin Jin,Zhiwu Lu
> **First submission**: 2023-01-14
> **First announcement**: 2023-01-16
> **comment**: Accepted to ACM Multimedia 2023
- **标题**: tiktalk：一个基于视频的对话数据集，用于现实世界中的多模式chitchat
- **领域**: 计算语言学,人工智能
- **摘要**: 为了促进具有多模式上下文的智能和类似人类的聊天机器人的研究，我们介绍了一个新的基于视频的多模式对话数据集，称为Tiktalk。我们从一个受欢迎的视频共享平台中收集了38K视频，以及下面用户发布的367K对话。用户根据观看视频的多模式经历进行自发对话，这有助于重新创建现实世界中的chitchat环境。与以前的多模式对话数据集相比，Tiktalk中更丰富的上下文类型会导致更多样化的对话，但也增加了从复杂的多模式信息中捕获人类利益以产生个性化响应的困难。此外，在我们的数据集中更常见外部知识。这些事实揭示了多模式对话模型的新挑战。我们定量地展示了Tiktalk的特征，提出了一个基于视频的多模式的chitchat任务，并评估了几个对话基准。实验结果表明，结合大型语言模型（LLM）的模型可以产生更多的响应，而利用知识图引入外部知识的模型则表现出色。此外，没有现有模型可以很好地解决上述所有挑战。即使对于具有视觉扩展的LLM，仍然有一个很大的改进空间。我们的数据集可在\ url {https://ruc-aimind.github.io/projects/tiktalk/}中获得。

### It's Just a Matter of Time: Detecting Depression with Time-Enriched Multimodal Transformers 
[[arxiv](https://arxiv.org/abs/2301.05453)] [[cool](https://papers.cool/arxiv/2301.05453)] [[pdf](https://arxiv.org/pdf/2301.05453)]
> **Authors**: Ana-Maria Bucur,Adrian Cosma,Paolo Rosso,Liviu P. Dinu
> **First submission**: 2023-01-13
> **First announcement**: 2023-01-16
> **comment**: Accepted at ECIR 2023
- **标题**: 这只是时间问题：通过富含时间增强的多模式变压器检测抑郁症
- **领域**: 计算语言学
- **摘要**: 互联网上用户生成的内容的抑郁症检测一直是研究界的一个持久的话题，为心理学家提供了宝贵的筛查工具。无处不在的社交媒体平台的使用是探索帖子和与其他用户互动中心理健康表现的理想途径。社交媒体的当前抑郁症检测方法主要集中在文本处理上，并且只有少数也利用用户发布的图像。在这项工作中，我们提出了一个富含时间增强的多模式变压器体系结构，用于检测社交媒体帖子的抑郁症，使用验证的模型来提取图像和文本嵌入。我们的模型直接在用户级别运行，我们通过使用Time2VEC位置嵌入在帖子之间的相对时间来丰富它。此外，我们提出了另一个模型变体，该变体可以在随机采样和无序的帖子集上进行操作，以使数据集噪声更强大。我们表明，我们的方法使用Emoberta和夹具嵌入式，超过了两个多模式数据集上的其他方法，在流行的多模态Twitter数据集中获得0.931 F1得分的最新结果，而在唯一的多模式RedDit DataSet上获得了0.902 F1。

### In BLOOM: Creativity and Affinity in Artificial Lyrics and Art 
[[arxiv](https://arxiv.org/abs/2301.05402)] [[cool](https://papers.cool/arxiv/2301.05402)] [[pdf](https://arxiv.org/pdf/2301.05402)]
> **Authors**: Evan Crothers,Herna Viktor,Nathalie Japkowicz
> **First submission**: 2023-01-13
> **First announcement**: 2023-01-16
> **comment**: Accepted to AAAI2023 creativeAI workshop
- **标题**: Bloom：人造歌词和艺术中的创造力和亲和力
- **领域**: 计算语言学,机器学习
- **摘要**: 我们将大型多语言模型（Bloom-176b）应用于开放式的中文歌曲歌词，并使用人类审稿人评估产生的歌词，以获得连贯性和创造力。我们发现，当前用于评估大语言模型输出（Mauve）的计算指标在评估创意写作方面有局限性。我们注意到，人类的创造力概念要求歌词既可以理解又独特 - 并且人类评估了某些类型的机器生成的歌词，以比流行艺术家的真实歌词更高。受专辑发行的固有多模式性质的启发，我们利用了一种汉语稳定的扩散模型来生产高质量的抒情式推动专辑艺术，为一个为专辑或单曲寻求灵感的艺术家展示了一种创造性的方法。最后，我们介绍了Mojimlyrics数据集，这是一种流行歌曲的中文数据集，用于未来的研究。

### A Multi-Purpose Audio-Visual Corpus for Multi-Modal Persian Speech Recognition: the Arman-AV Dataset 
[[arxiv](https://arxiv.org/abs/2301.10180)] [[cool](https://papers.cool/arxiv/2301.10180)] [[pdf](https://arxiv.org/pdf/2301.10180)]
> **Authors**: Javad Peymanfard,Samin Heydarian,Ali Lashini,Hossein Zeinali,Mohammad Reza Mohammadi,Nasser Mozayani
> **First submission**: 2023-01-21
> **First announcement**: 2023-01-25
> **comment**: No comments
- **标题**: 多模式波斯语音识别的多功能音频语料库：Arman-AV数据集
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 近年来，在自动唇读中取得了重大进展。但是这些方法需要许多低资源语言不存在的大规模数据集。在本文中，我们为波斯人提供了一个新的多功能音频视频数据集。该数据集由近220个小时的视频组成，其中包括1760个相应的扬声器。除了唇部阅读外，数据集还适用于自动语音识别，视听语音识别和扬声器识别。此外，这是波斯语中第一个大规模的唇读数据集。为每个提到的任务提供了一种基线方法。此外，我们提出了一种技术，以检测波斯语中的观察（视觉等效）。与先前提出的访问相比，通过这种方法获得的观众将唇部阅读任务的准确性提高了7％，这也可以应用于其他语言。

### Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2301.10799)] [[cool](https://papers.cool/arxiv/2301.10799)] [[pdf](https://arxiv.org/pdf/2301.10799)]
> **Authors**: Chenxi Whitehouse,Tillman Weyde,Pranava Madhyastha
> **First submission**: 2023-01-25
> **First announcement**: 2023-01-26
> **comment**: Findings of EACL 2023
- **标题**: 建立一个统一的模型，以在视觉问题答案中产生答案和解释
- **领域**: 计算语言学
- **摘要**: 视觉问题回答（VQA）的领域最近看到研究的激增着重于提供预测答案的解释。但是，当前的系统主要依靠单独的模型来预测答案并产生解释，从而导致较不一致的结果不一致。为了解决这个问题，我们提出了一种多任务学习方法，以实现答案和解释生成（UMAE）的统一模型。我们的方法涉及将人工提示令牌添加到训练数据中，并在各种VQA相关任务上微调多模式编码模型。在我们的实验中，UMAE模型超过了A-OKVQA上先前的最新答案准确性10〜15％，在OK-VQA上显示出竞争性的结果，在A-OKVQA和VCR上取得了新的最先进的解释分数，并在VQA-X上展示了有希望的外部表现。

### LoRaLay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization 
[[arxiv](https://arxiv.org/abs/2301.11312)] [[cool](https://papers.cool/arxiv/2301.11312)] [[pdf](https://arxiv.org/pdf/2301.11312)]
> **Authors**: Laura Nguyen,Thomas Scialom,Benjamin Piwowarski,Jacopo Staiano
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: To be published in EACL 2023
- **标题**: Loralay：用于远距离和布局意识摘要的多语言和多模式数据集
- **领域**: 计算语言学
- **摘要**: 文本摘要是自然语言处理社区的一项流行任务，也是研究的积极研究领域。根据定义，它需要考虑长输入文本，该特征构成了神经模型的计算挑战。此外，现实世界中的文档中有各种复杂，视觉富裕的布局。该信息是非常相关的，无论是突出显示出色的内容还是在文本段落之间编码远程交互。但是，所有公开可用的摘要数据集仅提供纯文本内容。为了促进有关如何利用视觉/布局信息以更好地捕获摘要模型中的远程依赖性的研究，我们提出了Loralay，这是一个数据集的集合，用于远程摘要，并随附的视觉/布局信息。我们使用布局信息扩展了现有和受欢迎的英语数据集（Arxiv和PubMed），并提出了四个新颖的​​数据集 - 始终由学者资源构建 - 涵盖法语，西班牙语，葡萄牙语和韩国语言。此外，我们提出了新的基准，以合并布局感知和远程模型 - 两种正交方法 - 并获得最先进的结果，显示了结合两种研究行的重要性。

### Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim? 
[[arxiv](https://arxiv.org/abs/2301.11219)] [[cool](https://papers.cool/arxiv/2301.11219)] [[pdf](https://arxiv.org/pdf/2301.11219)]
> **Authors**: Shivam Sharma,Atharva Kulkarni,Tharun Suresh,Himanshi Mathur,Preslav Nakov,Md. Shad Akhtar,Tanmoy Chakraborty
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: Accepted at EACL 2023 (Main Track). 9 Pages (main content), Limitations, Ethical Considerations + 4 Pages (Refs.) + Appendix; 8 Figures; 5 Tables; Paper ID: 804
- **标题**: 在有害模因中表征实体：谁是英雄，反派，受害者？
- **领域**: 计算语言学,计算机与社会
- **摘要**: 模因可以以易于批量的方式结合视觉和文本信息，使人们对社交媒体的意见作出影响。由于模因立即转化为病毒，因此推断出其意图并根据需要采取及时措施的潜在有害性变得至关重要。与模因理解相关的一个常见问题在于检测引用和表征每个实体的作用的实体。在这里，我们旨在了解模因是否荣耀，嘲笑或使其所指的每个实体受害。为此，我们解决了有害模因中实体的角色识别的任务，即检测谁是“英雄”，“小人”和模因中的“受害者”（如果有的话）。我们利用HVVMemes-最近发布的《美国政治和covid-19模因的模因数据集》，该数据集是最近发布的@ACL-2022共享任务的一部分。它包含模因，引用的实体及其相关角色：英雄，反派，受害者和其他人。我们进一步设计了矢量（视觉语义角色探测器），这是任务的强大多模式框架，该框架将基于实体的上下文信息集成在多模式表示中，并将其与几种标准的单峰（仅文本或仅图像）或多模式（图像+文本）模型进行比较。我们的实验结果表明，我们提出的模型比最佳基准的提高了4％，比共享任务的最佳竞争独立提交提交的提高1％。除了通过比较分析泄露广泛的实验设置外，我们最终强调了解决模因中语义角色标签的复杂任务所遇到的挑战。

### Learning the Effects of Physical Actions in a Multi-modal Environment 
[[arxiv](https://arxiv.org/abs/2301.11845)] [[cool](https://papers.cool/arxiv/2301.11845)] [[pdf](https://arxiv.org/pdf/2301.11845)]
> **Authors**: Gautier Dagan,Frank Keller,Alex Lascarides
> **First submission**: 2023-01-27
> **First announcement**: 2023-01-30
> **comment**: No comments
- **标题**: 在多模式环境中学习身体动作的影响
- **领域**: 计算语言学
- **摘要**: 大型语言模型（LLMS）处理物理常识信息不足。由于接受了训练有素的环境训练，LLMS通常无法在给定环境中预测动作的结果。但是，在执行动作之前预测其效果对于计划至关重要，在计划中，通常需要采取连贯的动作序列才能实现目标。因此，我们介绍了仅从现实的感觉输入（图像和文本）中预测动作结果的多模式任务。接下来，我们将LLM扩展到对象的潜在表示，以更好地预测环境中的动作结果。我们表明，当使用视觉信息增强时，多模式模型可以捕获物理常识。最后，我们评估了模型在新颖动作和对象上的表现，并发现结合方式有助于模型来概括和学习物理常识性推理。

### Pre-training for Speech Translation: CTC Meets Optimal Transport 
[[arxiv](https://arxiv.org/abs/2301.11716)] [[cool](https://papers.cool/arxiv/2301.11716)] [[pdf](https://arxiv.org/pdf/2301.11716)]
> **Authors**: Phuong-Hang Le,Hongyu Gong,Changhan Wang,Juan Pino,Benjamin Lecouteux,Didier Schwab
> **First submission**: 2023-01-27
> **First announcement**: 2023-01-30
> **comment**: ICML 2023 (oral presentation). This version fixed URLs, updated affiliations & acknowledgements, and improved formatting
- **标题**: 语音翻译的预培训：CTC满足最佳运输
- **领域**: 计算语言学,机器学习,声音,音频和语音处理
- **摘要**: 语音和文本方式之间的差距是语音到文本翻译（ST）的主要挑战。已经提出了不同的方法来减少这一差距，但是其中大多数需要在ST培训中进行建筑变化。在这项工作中，我们建议在培训前阶段减轻此问题，不需要ST模型改变。首先，我们表明连接派时间分类（CTC）损失可以通过设计减少模态差距。我们提供了与更常见的跨透镜损失的定量比较，表明使用CTC的预训练始终达到更好的最终ST精度。然而，CTC只是一种部分解决方案，因此，在我们的第二个贡献中，我们提出了一种组合CTC和最佳运输的新型预训练方法，以进一步降低这一差距。我们的方法预先培训是一个由两个编码器组成的类似暹罗的模型，一个用于声学输入，另一种用于文本输入，以便它们在Wasserstein空间中产生彼此相近的表示。对标准COVOST-2和必C数据集进行的广泛实验表明，在NO-External-DATA设置下，我们应用于香草编码器 - 模型变压器实现了最新性能，并与最近受过外部数据训练的强大多任务学习系统相同。最后，我们的方法也可以应用于这些多任务系统的顶部，从而为这些模型提供进一步的改进。代码和预培训模型可在https://github.com/formiel/fairseq上找到。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 56 篇论文

### Credible Remote Sensing Scene Classification Using Evidential Fusion on Aerial-Ground Dual-view Images 
[[arxiv](https://arxiv.org/abs/2301.00622)] [[cool](https://papers.cool/arxiv/2301.00622)] [[pdf](https://arxiv.org/pdf/2301.00622)]
> **Authors**: Kun Zhao,Qian Gao,Siyuan Hao,Jie Sun,Lijian Zhou
> **First submission**: 2023-01-02
> **First announcement**: 2023-01-03
> **comment**: 16 pages, 16 figures
- **标题**: 可靠的遥感场景分类，使用空中二元视图图像上的证据融合
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 由于它们能够提供比单个视图的数据更全面的信息，因此在遥感任务中更频繁地使用了多视图（多源，多模式，多模式，多模式，多模式等）数据。但是，随着视图数量的增长，数据质量的问题变得更加明显，从而限制了多视图数据的潜在优势。尽管最近基于深的神经网络（DNN）模型可以自适应地学习数据的重量，但是在融合它们时，缺乏明确量化每种视图的数据质量的研究，这使这些模型无法解释，在下游遥感任务中表现不佳和不灵活。为了填补这一差距，在本文中，向空中的深度学习介绍了空中双视遥感场景分类的任务，以模拟每种视图的信誉。具体而言，证据理论用于计算描述每种观点决策风险的不确定性价值。基于这种不确定性，提出了一种新颖的决策级融合策略，以确保风险较低的观点获得更多的权重，从而使分类更加可信。在两个众所周知的，可公开可用的空中双视遥感图像的数据集中，所提出的方法可实现最新的结果，以证明其有效性。本文的代码和数据集可在以下地址提供：https：//github.com/gaopiaoliang/evidential。

### Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding 
[[arxiv](https://arxiv.org/abs/2301.00514)] [[cool](https://papers.cool/arxiv/2301.00514)] [[pdf](https://arxiv.org/pdf/2301.00514)]
> **Authors**: Jiahao Zhu,Daizong Liu,Pan Zhou,Xing Di,Yu Cheng,Song Yang,Wenzheng Xu,Zichuan Xu,Yao Wan,Lichao Sun,Zeyu Xiong
> **First submission**: 2023-01-01
> **First announcement**: 2023-01-03
> **comment**: Accepted by EMNLP Findings, 2022
- **标题**: 重新考虑时间句子接地的视频抽样和推理策略
- **领域**: 计算机视觉和模式识别
- **摘要**: 时间句子接地（TSG）旨在通过句子查询从未修剪视频中确定特定段的时间边界。所有现有作品首先使用稀疏采样策略来提取固定数量的视频帧，然后用查询句子进行推理的多模式相互作用。但是，我们认为这些方法忽略了两个必不可少的问题：1）边界偏置：带注释的目标段通常将两个特定帧表示为相应的开始时间和结束时间戳。视频下采样过程可能会丢失这两个帧，并将相邻的无关框架作为新的边界。 2）推理偏见：这种不正确的新边界框架还导致帧引物相互作用期间的推理偏差，从而降低了模型的概括能力。为了减轻上述局限性，在本文中，我们为TSG提出了一种新颖的暹罗抽样和推理网络（SSRN），该采样网络引入了一种暹罗抽样机制，以生成其他上下文框架，以丰富和完善新的边界。具体而言，制定了一种推理策略来学习这些框架之间的相互关系，并在边界上生成软标签，以实现更准确的框架 - 问题推理。这种机制还能够将缺乏连续的视觉语义补充到采样的稀疏框架中，以进行细粒的活性理解。广泛的实验证明了SSRN对三个具有挑战性的数据集的有效性。

### Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting 
[[arxiv](https://arxiv.org/abs/2301.00493)] [[cool](https://papers.cool/arxiv/2301.00493)] [[pdf](https://arxiv.org/pdf/2301.00493)]
> **Authors**: Benjamin Wilson,William Qi,Tanmay Agarwal,John Lambert,Jagjeet Singh,Siddhesh Khandelwal,Bowen Pan,Ratnesh Kumar,Andrew Hartnett,Jhony Kaesemodel Pontes,Deva Ramanan,Peter Carr,James Hays
> **First submission**: 2023-01-01
> **First announcement**: 2023-01-03
> **comment**: Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks
- **标题**: Argoverse 2：用于自动驾驶感知和预测的下一代数据集
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术
- **摘要**: 我们介绍了Argoverse 2（AV2） - 三个数据集的集合，用于自动驾驶领域中的感知和预测研究。带注释的传感器数据集包含1,000个多模式数据序列，包括七个环摄像机的高分辨率图像，还有两个立体声摄像机，除了激光镜点云外，以及6多型MAP平衡的姿势。序列包含26个对象类别的3D Cuboid注释，所有这些都足够采样以支持3D感知模型的训练和评估。 LIDAR数据集包含20,000个未标记的LiDAR点云和MAP对准姿势的序列。该数据集是有史以来最大的LIDAR传感器数据集合，并支持自我监督的学习和点云预测的新兴任务。最后，运动预测数据集包含25万个场景，用于在每个本地场景中自动驾驶汽车和其他参与者之间的有趣且挑战性的相互作用。模型的任务是在每种情况下对“评分演员”的未来运动的预测，并提供捕获对象位置，标题，速度和类别的轨道历史。在所有三个数据集中，每种情况都包含其自己的高清图，并带有3D车道和人行横道几何形状 - 来自六个不同城市中捕获的数据。我们认为，这些数据集将以现有数据集不提供的方式支持新的和现有的机器学习研究问题。所有数据集均在CC BY-NC-SA 4.0许可下发布。

### Guided Hybrid Quantization for Object detection in Multimodal Remote Sensing Imagery via One-to-one Self-teaching 
[[arxiv](https://arxiv.org/abs/2301.00131)] [[cool](https://papers.cool/arxiv/2301.00131)] [[pdf](https://arxiv.org/pdf/2301.00131)]
> **Authors**: Jiaqing Zhang,Jie Lei,Weiying Xie,Yunsong Li,Xiuping Jia
> **First submission**: 2022-12-31
> **First announcement**: 2023-01-03
> **comment**: This article has been delivered to TRGS and is under review
- **标题**: 通过一对一的自学教学
- **领域**: 计算机视觉和模式识别
- **摘要**: 考虑到计算的复杂性，我们提出了一种具有一对一的自学（ghost}）框架的指导混合量化。更具体地说，我们首先设计了一种称为指导量化自我验证（GQSD）的结构，这是通过量化和蒸馏的协同作用实现轻量级的创新思想。量化模型的训练过程以其完整精确模型为指导，该模型可以节省时间和节省成本，而无需事先准备大型的预训练模型。其次，我们提出了一个混合量化（HQ）模块，以在约束条件下自动获得最佳的位宽度，在这种情况下，中心之间的分布距离和样品之间的分布距离阈值在权重搜索空间中应用。第三，为了改善信息转换，我们提出了一个一对一的自我教学（OST）模块，以使学生网络具有自我判断的能力。开关控制机（SCM）在同一地点的学生网络和教师网络之间建立了桥梁，以帮助老师减少错误的指导并向学生传授重要知识。这种蒸馏方法允许模型从自身学习并获得实质性改进，而无需任何其他监督。在多模式数据集（VEDAI）和单模式数据集（DOTA，NWPU和DIOR）上进行了广泛的实验表明，基于幽灵的对象检测优于现有检测器。与任何基于遥感的，轻质或基于蒸馏的算法相比，微小的参数（<9.7 MB）和比特操作（BOPS）（<2158 g）表明了轻质设计域中的优势。我们的代码和模型将在https://github.com/ecey-zhang/ghost上发布。

### Cross Modal Transformer: Towards Fast and Robust 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2301.01283)] [[cool](https://papers.cool/arxiv/2301.01283)] [[pdf](https://arxiv.org/pdf/2301.01283)]
> **Authors**: Junjie Yan,Yingfei Liu,Jianjian Sun,Fan Jia,Shuailin Li,Tiancai Wang,Xiangyu Zhang
> **First submission**: 2023-01-03
> **First announcement**: 2023-01-04
> **comment**: No comments
- **标题**: 交叉模态变压器：朝向快速，稳健的3D对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们提出了一个稳健的3D检测器，称为Cross Modal变压器（CMT），用于端到端3D多模式检测。如果没有明确的视图转换，CMT将图像和点云代币作为输入，直接输出准确的3D边界框。多模式令牌的空间对齐是通过将3D点编码为多模式特征来执行的。 CMT的核心设计非常简单，而其性能令人印象深刻。在保持快速推理速度的同时，它在Nuscenes测试集上实现了74.1 \％NDS（带有单个模型的最先进）。此外，即使失踪了激光雷达，CMT也具有强大的鲁棒性。代码在https://github.com/junjie18/cmt上发布。

### Common Practices and Taxonomy in Deep Multi-view Fusion for Remote Sensing Applications 
[[arxiv](https://arxiv.org/abs/2301.01200)] [[cool](https://papers.cool/arxiv/2301.01200)] [[pdf](https://arxiv.org/pdf/2301.01200)]
> **Authors**: Francisco Mena,Diego Arenas,Marlon Nuske,Andreas Dengel
> **First submission**: 2022-12-20
> **First announcement**: 2023-01-04
> **comment**: appendix with additional tables. Preprint submitted to journal
- **标题**: 遥感应用中深度视图融合中的常见实践和分类学
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 遥感技术的进步增强了用于地球观察的应用。这些技术提供了具有不同信息级别的多种观察或观点。除了由于传感器校准或恶化而产生不同类型和噪声的噪声外，它们可能包含具有不同分辨率的静态或临时视图。已经应用了各种各样的深度学习模型将这些信息从这些多种视图（称为深度视图或多模式融合学习）中融合在一起。但是，文献中的方法差异很大，因为不同的术语用于指出类似的概念或不同的插图。本文通过关注文献中使用的常见实践和方法来收集对地球观察的多视图融合。我们总结并构建了几个不同出版物的见解，这些出版物集中在统一要点和思想上。在本手稿中，我们提供了一个统一的术语，同时提到文献中使用的各种替代术语。作品所涵盖的主题审查了使用神经网络模型对监督学习的重点。我们希望这篇评论列出了一系列最近的参考文献，可以支持未来的研究，并导致该地区的统一进步。

### Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition 
[[arxiv](https://arxiv.org/abs/2301.00986)] [[cool](https://papers.cool/arxiv/2301.00986)] [[pdf](https://arxiv.org/pdf/2301.00986)]
> **Authors**: Hasan Abed Al Kader Hammoud,Shuming Liu,Mohammed Alkhrashi,Fahad AlBalawi,Bernard Ghanem
> **First submission**: 2023-01-03
> **First announcement**: 2023-01-04
> **comment**: No comments
- **标题**: 看，倾听和攻击：针对视频动作识别的后门攻击
- **领域**: 计算机视觉和模式识别,密码学和安全,机器学习
- **摘要**: 深度神经网络（DNNS）容易受到称为“后门攻击”的攻击，该攻击在后门触发器和目标标签之间建立了关联，攻击者对利用攻击感兴趣。一个后部的DNN在干净的测试图像上表现良好，但在后门触发器的存在下，持续预测任何样品的攻击者定义标签。尽管在图像域中对后门攻击进行了广泛的研究，但很少有作品探索视频域中的此类攻击，并且他们倾向于得出结论，图像后门攻击在视频域中的效果较差。在这项工作中，我们重新访问了传统的后门威胁模型，并将与视频相关的其他方面纳入该模型。我们表明，有毒标签图像后门攻击可以通过两种方式进行静态和动态的方式扩展，从而导致视频域中的高效攻击。此外，我们探索自然视频后门，以突出视频域中这种脆弱性的严重性。而且，我们首次研究了针对视频动作识别模型的多模式（视听）后门攻击，我们表明攻击单一模式足以实现高攻击成功率。

### What You Say Is What You Show: Visual Narration Detection in Instructional Videos 
[[arxiv](https://arxiv.org/abs/2301.02307)] [[cool](https://papers.cool/arxiv/2301.02307)] [[pdf](https://arxiv.org/pdf/2301.02307)]
> **Authors**: Kumar Ashutosh,Rohit Girdhar,Lorenzo Torresani,Kristen Grauman
> **First submission**: 2023-01-05
> **First announcement**: 2023-01-06
> **comment**: Technical Report
- **标题**: 您所说的是您显示的内容：教学视频中的视觉叙述检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 叙述的“操作方法”视频已成为从学习视觉表示到培训机器人策略的广泛学习问题的有前途的数据源。但是，这些数据非常嘈杂，因为叙述并不总是描述视频中所展示的动作。为了解决这个问题，我们介绍了视觉叙述检测的新任务，这需要确定视频中的动作是否在视觉上描绘了叙述。我们提出您所说的是您所显示的内容（Wys^2），该方法利用多模式提示和伪标记学习仅使用弱标记的数据来检测视觉叙述。我们的模型成功地检测了野外视频中的视觉叙述，优于强大的基线，我们证明了它对最先进的摘要和教学视频的时间对齐的影响。

### Seamless Multimodal Biometrics for Continuous Personalised Wellbeing Monitoring 
[[arxiv](https://arxiv.org/abs/2301.03045)] [[cool](https://papers.cool/arxiv/2301.03045)] [[pdf](https://arxiv.org/pdf/2301.03045)]
> **Authors**: João Ribeiro Pinto
> **First submission**: 2023-01-08
> **First announcement**: 2023-01-09
> **comment**: Doctoral thesis presented and approved on the 21st of December 2022 to the University of Porto
- **标题**: 无缝的多模式生物识别技术，用于连续个性化的健康监控
- **领域**: 计算机视觉和模式识别
- **摘要**: 人为聪明的看法越来越多地存在于我们每个人的生活中。车辆也不例外，（...）在不久的将来，模式识别将在车辆中发挥更大的作用，因为自动驾驶汽车将需要自动化的方法来了解周围发生的事情（以及内部）并采取相应的行动。 （...）这项博士工作着重于通过研究生物识别技术和福祉监测的新型计算机视觉和模式识别方法来推进车载感测。主要重点是心电图（ECG）生物识别技术，该特质以其无缝驱动器监测的潜力而闻名。主要的努力致力于在非个人场景中提高识别和身份验证的绩效，以增加噪音和可变性而闻名。在这里，提出了端到端的深度学习ECG生物识别解决方案，并讨论了重要主题，例如跨数据库和长期性能，通过解释性通过波形相关性以及Interlead转换。在这项工作中，还研究了Face Biometrics是无缝无约束场景中心电图的自然补充。解决了生物识别技术中面部识别性和可解释性的公开挑战，以发展朝着更透明，值得信赖和强大的大量闭合的算法发展。在福祉监测的主题中，提出了在人群中的多模式情绪识别和活动/暴力识别的改进解决方案。最后，我们还提出了一种新颖的方法，以在端到端模型内学习模板安全性，驳回其他单独的加密过程，并为顺序数据量身定制的自我监督的学习方法，以确保数据安全性和最佳性能。 （...）

### HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection 
[[arxiv](https://arxiv.org/abs/2301.03036)] [[cool](https://papers.cool/arxiv/2301.03036)] [[pdf](https://arxiv.org/pdf/2301.03036)]
> **Authors**: Bin Tang,Zhengyi Liu,Yacheng Tan,Qian He
> **First submission**: 2023-01-08
> **First announcement**: 2023-01-09
> **comment**: ef:TCSVT2022
- **标题**: Hrtransnet：HRFormer驱动的两个模式显着对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 高分辨率变压器（HRFormer）可以维持高分辨率表示并共享全球接受场。它对输入和输出具有相同分辨率的显着对象检测（SOD）很友好。但是，对于两种模式SOD，需要解决两个关键问题。一个问题是两种模式融合。另一个问题是HRFormer输出的融合。为了解决第一个问题，通过使用全局优化和注意力机制来选择和净化输入级别的模态，将补充方式注入了主要模式。为了解决第二个问题，使用双向短连接融合模块来优化HRFormer的输出功能，从而增强对象在输出级别的详细表示。所提出的名为HRTRANSNET的模型首先引入了用于提取补充模式的辅助流。然后，将特征注入每个多分辨率分支开始时的主要模式。接下来，应用HRFormer来实现转发传播。最后，所有具有不同分辨率的输出功能均通过功能内和功能间交互式变压器汇总。拟议模型的应用会导致驱动两种模式SOD任务的令人印象深刻的改进

### RGB-T Multi-Modal Crowd Counting Based on Transformer 
[[arxiv](https://arxiv.org/abs/2301.03033)] [[cool](https://papers.cool/arxiv/2301.03033)] [[pdf](https://arxiv.org/pdf/2301.03033)]
> **Authors**: Zhengyi Liu,Wei Wu,Yacheng Tan,Guanghui Zhang
> **First submission**: 2023-01-08
> **First announcement**: 2023-01-09
> **comment**: ef:BMVC2022
- **标题**: 基于变压器的RGB-T多模式人群计数
- **领域**: 计算机视觉和模式识别
- **摘要**: 人群计数旨在估计场景中的人数。由于看不见的物体，大多数基于颜色图像的最先进的人群计数方法在较差的照明条件下无法很好地工作。随着红外摄像机的广泛使用，研究了基于颜色和热图像的人群计数。现有方法仅实现多模式融合而无需计数目标限制。为了更好地挖掘多模式信息，我们使用计数引导的多模式融合和模态引导的计数增强，以实现令人印象深刻的性能。提出的计数指导的多模式融合模块利用多尺度令牌变压器在计数信息的指导下与两种模式信息进行交互，并从令牌的角度了解不同的尺度。所提出的模态引导的计数增强模块采用多尺度的变形变压器解码器结构来增强一种模态特征，并通过另一种模态来计数信息。公共RGBT-CC数据集中的实验表明，我们的方法刷新了最新的结果。 https://github.com/liuzywen/rgbtcc

### Multi-scale multi-modal micro-expression recognition algorithm based on transformer 
[[arxiv](https://arxiv.org/abs/2301.02969)] [[cool](https://papers.cool/arxiv/2301.02969)] [[pdf](https://arxiv.org/pdf/2301.02969)]
> **Authors**: Fengping Wang,Jie Li,Chun Qi,Lin Wang,Pan Wang
> **First submission**: 2023-01-07
> **First announcement**: 2023-01-09
> **comment**: No comments
- **标题**: 基于变压器的多尺度多模式微表达识别算法
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 微观表达是一种自发的无意识的面部肌肉运动，可以揭示人们试图隐藏的真正情感。尽管手动方法取得了良好的进步，并且深度学习正在越来越重要。由于微表达的持续时间短，面部区域表达的不同尺度，现有算法不能提取多模式多尺度的面部区域特征，同时考虑到上下文信息以学习基础特征。因此，为了解决上述问题，本文提出了基于变压器网络的多模式多尺度算法，旨在通过微表达的两个模态特征 - 运动特征和纹理特征充分学习微表达的局部多层次特征。为了在不同的尺度上获得面部的局部特征，我们以两种方式学习了不同尺度上的贴片特征，然后融合了多层多头注意权重，以通过加权贴片特征以及结合的跨模式对比度学习来获得有效的功能，以进行模型优化。我们在三个自发数据集上进行了全面的实验，结果表明，单个测量SMIC数据库中提出的算法的准确性高达78.73％，合并数据库的CASMEII上的F1值高达0.9071，高达0.9071。

### Towards early prediction of neurodevelopmental disorders: Computational model for Face Touch and Self-adaptors in Infants 
[[arxiv](https://arxiv.org/abs/2301.02911)] [[cool](https://papers.cool/arxiv/2301.02911)] [[pdf](https://arxiv.org/pdf/2301.02911)]
> **Authors**: Bruno Tafur,Marwa Mahmoud,Staci Weiss
> **First submission**: 2023-01-07
> **First announcement**: 2023-01-09
> **comment**: No comments
- **标题**: 朝着神经发育障碍的早期预测：面部触摸的计算模型和婴儿的自适应者
- **领域**: 计算机视觉和模式识别
- **摘要**: 婴儿的神经发育受到运动技能的严重影响。评估婴儿的运动是理解发育障碍的可能风险的关键。心理学的先前研究表明，测量婴儿的特定运动或手势（例如婴儿的面部触摸）对于分析婴儿如何理解自己和背景至关重要。这项研究提出了第一种自动方法，该方法通过跟踪婴儿的动作和手势来检测视频录制的面部触摸。该研究使用多模式特征融合方法混合空间和时间特征，并利用骨架跟踪信息来生成170多个手，面部和身体的聚合特征。这项研究提出了数据驱动的机器学习模型，以检测和分类婴儿的面部触摸。我们使用交叉数据集测试来评估我们提出的模型。这些模型在检测面部触摸方面的精度达到了87.0％，并且在检测特定的面部触摸位置方面具有71.4％的宏观平均精度，并且比零规则和均匀的随机机会基准进行了显着改善。此外，我们表明，当我们运行模型以提取较大数据集的面部触摸频率时，我们可以预测出生后的前5个月中精细运动技能的发展。

### Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark 
[[arxiv](https://arxiv.org/abs/2301.03432)] [[cool](https://papers.cool/arxiv/2301.03432)] [[pdf](https://arxiv.org/pdf/2301.03432)]
> **Authors**: Fang Xu,Yilei Shi,Patrick Ebel,Wen Yang,Xiao Xiang Zhu
> **First submission**: 2023-01-09
> **First announcement**: 2023-01-10
> **comment**: ef:in IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 1-15, 2024
- **标题**: 高分辨率云去除的多模式和多分辨率数据融合：一种新颖的基线和基准测试
- **领域**: 计算机视觉和模式识别
- **摘要**: 去除云是遥感中的一个重大且具有挑战性的问题，近年来，该领域取得了显着进步。但是，两个主要问题仍然阻碍了云去除的发展：现有数据集的高分辨率图像的不可用，以及对生成结构的语义意义的评估。在本文中，我们介绍了M3R-CR，这是一种基准数据集，用于使用多模式和多分辨率数据融合，用于高分辨率云的去除。使用此数据集，我们通过集成多模式和多分辨率信息来考虑高分辨率光学遥感图像中云去除的问题。在这种情况下，由于固有的成像机制差异和其他因素，我们必须考虑由多分辨率性质引起的对齐误差以及高分辨率图像中更为明显的未对准问题。因此，现有的基于多模式数据融合的方法假设图像对在像素级时准确对齐，因此不适合此问题。为此，我们设计了一个名为Align-CR的新基线，以执行低分辨率SAR图像引导的高分辨率光学图像云的去除。它在重建过程中逐渐扭曲并融合了多模式和多分辨率数据的特征，从而有效地减轻了与未对准有关的问题。在实验中，我们通过使用图像重建指标分析视觉令人愉悦纹理的质量来评估云去除的性能，并进一步分析使用完善的语义细分任务的语义有意义结构的产生。所提出的对齐-CR方法优于两个区域的其他基线方法。

### LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking 
[[arxiv](https://arxiv.org/abs/2301.04748)] [[cool](https://papers.cool/arxiv/2301.04748)] [[pdf](https://arxiv.org/pdf/2301.04748)]
> **Authors**: Zhihua Liu,Bin Yang,Yan Shen,Xuejun Ni,Huiyu Zhou
> **First submission**: 2023-01-11
> **First announcement**: 2023-01-12
> **comment**: No comments
- **标题**: LSDM：弱监督超声标记跟踪的长短差异运动
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着时间的流逝，准确跟踪解剖学地标一直具有疾病评估的浓厚兴趣，例如微创手术和肿瘤放射疗法。超声成像是一种有希望的方式，其低成本和实时获取受益。但是，生成精确的地标曲目非常具有挑战性，因为不同的干扰很容易扭曲尝试，例如地标变形，视觉歧义和部分观察。在本文中，我们提出了一个长短的差异运动网络，该网络是一个多任务框架，在搜索地标的合理变形之前，具有可学习的变形。具体而言，我们在长时间和短的时间域中设计了一种新颖的差异性表示，用于描述运动边缘并减少长期累积跟踪误差。为了进一步减轻局部解剖学歧义，我们提出了一个期望最大化运动比对模块，以迭代优化长变形和短变形，与相同的方向性和空间表示。提出的多任务系统可以以弱监督的方式进行训练，这仅需要进行跟踪的地标注释，而对于长短变形学习，则需要零注释。我们在两个超声地标跟踪数据集上进行了广泛的实验。实验结果表明，与其他最先进的跟踪方法相比，我们提出的方法可以实现更好或竞争性的地标跟踪性能，具有强大的扫描仪类型和不同超声模式的概括能力。

### EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata 
[[arxiv](https://arxiv.org/abs/2301.04647)] [[cool](https://papers.cool/arxiv/2301.04647)] [[pdf](https://arxiv.org/pdf/2301.04647)]
> **Authors**: Chenhao Zheng,Ayush Shrivastava,Andrew Owens
> **First submission**: 2023-01-11
> **First announcement**: 2023-01-12
> **comment**: CVPR 2023 (Highlight). Project link: http://hellomuffin.github.io/exif-as-language
- **标题**: Exif作为语言：学习图像和相机元数据之间的跨模式关联
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 我们学习了一个视觉表示，该表示可以捕获有关录制给定照片的相机的信息。为此，我们在图像补丁和exif元数据之间训练一个自动插入图像文件的Exif元数据之间的多模式嵌入。我们的模型通过简单地将其转换为文本，然后用变压器处理它来代表此元数据。我们学到的功能在下游图像取证和校准任务上的表现明显优于其他自我监督和监督功能。特别是，我们通过聚集图像中所有贴片的视觉嵌入方式，成功地将剪接的图像区域“零拍”定位为“零射击”。

### Generative-Contrastive Learning for Self-Supervised Latent Representations of 3D Shapes from Multi-Modal Euclidean Input 
[[arxiv](https://arxiv.org/abs/2301.04612)] [[cool](https://papers.cool/arxiv/2301.04612)] [[pdf](https://arxiv.org/pdf/2301.04612)]
> **Authors**: Chengzhi Wu,Julius Pfrommer,Mingyuan Zhou,Jürgen Beyerer
> **First submission**: 2023-01-11
> **First announcement**: 2023-01-12
> **comment**: No comments
- **标题**: 从多模式欧几里得输入的3D形状的自我监督的潜在表示的生成对抗性学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们提出了一种结合生成和对比的神经结构，用于学习3D体积形状的潜在表示。该体系结构使用两个编码器分支用于体素网格和来自同一基础形状的多视图图像。主要思想是将结果潜在表示之间的对比损失与额外的重建损失结合在一起。这有助于避免将潜在表示作为一种微不足道的解决方案，以最大程度地减少对比度损失。一种新颖的切换方案用于将两个编码器与共享解码器进行交叉训练。开关方案还可以在随机分支上进行停止梯度操作。进一步的分类实验表明，潜在表示通过我们的自我监管方法学到的潜在表示，隐含地从其他输入数据中整合了更多有用的信息，从而导致更好的重建和分类性能。

### Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing 
[[arxiv](https://arxiv.org/abs/2301.04558)] [[cool](https://papers.cool/arxiv/2301.04558)] [[pdf](https://arxiv.org/pdf/2301.04558)]
> **Authors**: Shruthi Bannur,Stephanie Hyland,Qianchu Liu,Fernando Pérez-García,Maximilian Ilse,Daniel C. Castro,Benedikt Boecking,Harshita Sharma,Kenza Bouzid,Anja Thieme,Anton Schwaighofer,Maria Wetscherek,Matthew P. Lungren,Aditya Nori,Javier Alvarez-Valle,Ozan Oktay
> **First submission**: 2023-01-11
> **First announcement**: 2023-01-12
> **comment**: To appear in CVPR 2023
- **标题**: 学习利用生物医学视觉语言处理的时间结构
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视觉处理中的自我监督学习利用成像和文本方式之间的语义一致性。生物医学VLP的先前工作主要依赖于单图像的比对，并报告对，即使临床注释通常是指先验图像。这不仅引入了模式之间的不良一致性，而且还错过了通过数据中现有的时间内容来利用丰富的自学意义的机会。在这项工作中，我们在培训和微调期间可用时明确说明了先前的图像和报告。我们的方法名为Biovil-T，使用CNN转换器混合多图像编码器与文本模型共同训练。它旨在用途广泛，涉及挑战，例如姿势变化和跨时间丢失的输入图像。最终的模型在单图像和多图像设置中都在下游任务上出色，从而在（i）进展分类，（ii）短语接地以及（iii）报告生成方面实现了最先进的绩效，同时在疾病分类和判决中提供一致的改进。我们发布了一个新型的多模式时间基准数据集MS-CXR-T，以量化视觉表示的质量，以时间语义为方面。我们的实验结果表明，合并先前的图像和报告以大多数使用数据的优势。

### Co-training with High-Confidence Pseudo Labels for Semi-supervised Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2301.04465)] [[cool](https://papers.cool/arxiv/2301.04465)] [[pdf](https://arxiv.org/pdf/2301.04465)]
> **Authors**: Zhiqiang Shen,Peng Cao,Hua Yang,Xiaoli Liu,Jinzhu Yang,Osmar R. Zaiane
> **First submission**: 2023-01-11
> **First announcement**: 2023-01-12
> **comment**: No comments
- **标题**: 与高信心伪标签共同训练，用于半监督医学图像分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 一致性正则化和基于伪标记的半监督方法使用来自多视图输入的伪标签进行共同训练。但是，这种共同训练模型倾向于早日收集到共识，退化为自我训练，并在训练过程中产生低信心的伪标签。为了解决这些问题，我们提出了一个不确定性指导的协作均值老师（UCMT），以使用高信心伪标签进行半监督语义细分。 Concretely, UCMT consists of two main components: 1) collaborative mean-teacher (CMT) for encouraging model disagreement and performing co-training between the sub-networks, and 2) uncertainty-guided region mix (UMIX) for manipulating the input images according to the uncertainty maps of CMT and facilitating CMT to produce high-confidence pseudo labels. UCMT将UMIX的优势与CMT相结合，可以保留模型分歧，并提高伪标记的伪训练分割质量。在包括2D和3D模式在内的四个公共医疗图像数据集上进行的广泛实验证明了UCMT优于最先进的。代码可在以下网址提供：https：//github.com/senyh/ucmt。

### Scene-Aware 3D Multi-Human Motion Capture from a Single Camera 
[[arxiv](https://arxiv.org/abs/2301.05175)] [[cool](https://papers.cool/arxiv/2301.05175)] [[pdf](https://arxiv.org/pdf/2301.05175)]
> **Authors**: Diogo Luvizon,Marc Habermann,Vladislav Golyanik,Adam Kortylewski,Christian Theobalt
> **First submission**: 2023-01-12
> **First announcement**: 2023-01-13
> **comment**: Accepted to Eurographics 2023. See also github: https://github.com/dluvizon/scene-aware-3d-multi-human project page: https://vcai.mpi-inf.mpg.de/projects/scene-aware-3d-multi-human/
- **标题**: 从单个相机中捕获场景感知的3D多人运动捕获
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项工作中，我们考虑了估计场景中多个人的3D位置的问题，以及通过静态摄像头录制的单个RGB视频的身体形状和表达。与昂贵的标记或多视图系统相反，我们的轻量级设置非常适合私人用户，因为它可以易于安装且不需要专家知识，因此可负担得起的3D运动捕获。为了应对这种具有挑战性的环境，我们利用大规模的预训练模型来利用计算机视觉的最新进展，用于各种模式，包括2D身体关节，关节角度，归一化差异图和人体细分面具。因此，我们介绍了第一种基于非线性优化的方法，该方法共同解决了每个人的绝对3D位置，其表达的姿势，其单个形状以及场景的规模。特别是，我们使用2D身体关节和关节角度从归一化的差异预测中估算了场景深度和人的独特量表。考虑到人均场景的深度，我们重建了3D空间中静态场景的点云。最后，鉴于人类和场景点云的人均3D估计值，我们对视频进行了时空连贯的优化，以确保时间，空间和物理上的合理性。我们对既定的多人3D人类姿势基准进行评估我们的方法，在该基准中，我们始终超越先前的方法，并且我们从定性地证明，我们的方法对野外条件具有牢固的态度，包括具有不同大小的人的具有挑战性的场景。

### A Survey on Human Action Recognition 
[[arxiv](https://arxiv.org/abs/2301.06082)] [[cool](https://papers.cool/arxiv/2301.06082)] [[pdf](https://arxiv.org/pdf/2301.06082)]
> **Authors**: Zhou Shuchang
> **First submission**: 2022-12-20
> **First announcement**: 2023-01-16
> **comment**: No comments
- **标题**: 关于人类行动识别的调查
- **领域**: 计算机视觉和模式识别
- **摘要**: 人类行动识别（HAR）是计算机视觉中最重要的任务之一，在过去的十年中迅速发展，并且在健康监测，智能监视，虚拟现实，人体计算机交互等方面有广泛的应用。人类的行为可以由多种方式来代表，例如RGB-D摄像机，音频，惯性传感器等。因此，除了基于单一模态的HAR方法外，由于多模式数据之间的互补特性，越来越多的研究专门用于多模式域。在本文中，我们根据不同的输入方式介绍了HAR方法的调查。同时，考虑到HAR最近的大多数调查都集中在第三个角度上，而这项调查旨在对HAR Neves和研究人员进行更全面的介绍，因此，我们还从近年来的第一角度研究了行动识别方法。最后，我们简要介绍了基准HAR数据集，并显示了这些数据集上不同方法的性能比较。

### Linguistic Query-Guided Mask Generation for Referring Image Segmentation 
[[arxiv](https://arxiv.org/abs/2301.06429)] [[cool](https://papers.cool/arxiv/2301.06429)] [[pdf](https://arxiv.org/pdf/2301.06429)]
> **Authors**: Zhichao Wei,Xiaohao Chen,Mingqiang Chen,Siyu Zhu
> **First submission**: 2023-01-16
> **First announcement**: 2023-01-17
> **comment**: No comments
- **标题**: 语言查询引导的面具生成用于参考图像分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 参考图像分割旨在根据给定的语言表达式分割感兴趣的图像区域，这是典型的多模式任务。现有方法要么采用基于像素分类的掩码生成的基于像素分类或基于可学习的基于查询的框架，因此两者都不足以处理具有修复数量参数原型的各种文本图像对。在这项工作中，我们提出了一个建立在变压器上的端到端框架，以执行语言查询引导的面具生成，称为lgformer。它将语言特征视为查询，以生成用于任意输入图像文本对的专门原型，从而产生更一致的分割结果。此外，我们在编码器和解码器中设计了几个跨模式相互作用模块（\ EG，视觉双向注意模块，VLBA），以实现更好的跨模式比对。

### UATVR: Uncertainty-Adaptive Text-Video Retrieval 
[[arxiv](https://arxiv.org/abs/2301.06309)] [[cool](https://papers.cool/arxiv/2301.06309)] [[pdf](https://arxiv.org/pdf/2301.06309)]
> **Authors**: Bo Fang,Wenhao Wu,Chang Liu,Yu Zhou,Yuxin Song,Weiping Wang,Xiangbo Shu,Xiangyang Ji,Jingdong Wang
> **First submission**: 2023-01-16
> **First announcement**: 2023-01-17
> **comment**: To appear at ICCV2023
- **标题**: UATVR：不确定性自适应文本视频检索
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着Web视频的爆炸性增长和新兴的大规模视觉语言预训练模型，例如剪辑，通过文本指令检索感兴趣的视频引起了人们的关注。一种常见的做法是将文本视频对传递到与特定粒度的某些实体的相同嵌入空间和工艺交叉模式相互作用，以实现语义对应。不幸的是，对适当的跨模式查询粒度的最佳实体组合的固有不确定性进行了研究，这对于具有层次语义语义的模态（例如，视频，文本等）尤其重要。具体而言，我们在编码器中添加了其他可学习的令牌，以适应汇总的多元语义，以灵活的高级推理。在精致的嵌入空间中，我们表示文本视频对作为概率分布，在其中采样原型以进行匹配评估。在四个基准上进行的全面实验证明了我们的UATVR的优越性，该UATVR获得了新的最先进的结果，这是MSR-VTT（50.8％），VATEX（64.5％），MSVD（49.7％）和DIDEMO（45.8％）。该代码可在https://github.com/bofang98/uatvr上找到。

### Representation Learning for Tablet and Paper Domain Adaptation in Favor of Online Handwriting Recognition 
[[arxiv](https://arxiv.org/abs/2301.06293)] [[cool](https://papers.cool/arxiv/2301.06293)] [[pdf](https://arxiv.org/pdf/2301.06293)]
> **Authors**: Felix Ott,David Rügamer,Lucas Heublein,Bernd Bischl,Christopher Mutschler
> **First submission**: 2023-01-16
> **First announcement**: 2023-01-17
> **comment**: Accepted at IAPR Intl. Workshop onMultimodalPattern Recognition of Social Signals in Human Computer Interaction (MPRSS), Montreal, Canada, August 2022
- **标题**: 平板电脑和纸质域改编的表示为在线手写识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 当机器学习模型将其应用于与最初训练的数据相似但不同的域中的数据时，它的性能会降低。域适应（DA）的目标是通过寻找最佳特征转换来学习域不变表示来减轻此域移位问题。这种域移位可以出现在手写识别（HWR）应用中，其中手的运动模式以及笔的运动模式在纸上和平板电脑上写作是不同的。这在具有集成惯性测量单元的笔的在线笔迹（ONHW）的传感器数据中变得可见。本文提出了一种监督的DA方法，以增强平板电脑和纸质数据之间的ONHW识别的学习。我们的方法利用了损失函数，例如最大平均差异和相关对准，以学习域不变特征表示（即平板电脑和纸质特征之间的相似协方差）。我们使用的三重损失损失，该损失需要辅助结构域（即纸样本）的负样本来增加平板数据集的样品量。我们对基于新型的基于序列的ONHW数据集（即单词）进行了评估，并通过使用成对学习通过早期的融合策略对纸质域进行了改进。

### Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models 
[[arxiv](https://arxiv.org/abs/2301.06267)] [[cool](https://papers.cool/arxiv/2301.06267)] [[pdf](https://arxiv.org/pdf/2301.06267)]
> **Authors**: Zhiqiu Lin,Samuel Yu,Zhiyi Kuang,Deepak Pathak,Deva Ramanan
> **First submission**: 2023-01-16
> **First announcement**: 2023-01-17
> **comment**: Published at CVPR 2023. Project site: https://linzhiqiu.github.io/papers/cross_modal/
- **标题**: 多模式有助于单形态：通过多模型的跨模式几次学习
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,声音,音频和语音处理
- **摘要**: 通过最少的指导（称为少数学习）快速学习新任务的能力是智能代理的中心方面。经典的几杆基准测试基准利用了单个模式中的几种样本，但是这样的样本可能不足以表征整个概念类别。相比之下，人类使用跨模式信息有效地学习新概念。在这项工作中，我们证明了一个人确实可以通过$ {\ bf read} $构建一个更好的$ {\ bf visual} $狗分类器，以使他们的吠叫和$ {\ bf list} $ for bark bark。为此，我们利用了一个事实，即最近的多模式基础模型（例如剪贴画）学习了跨模式编码器，这些编码器将不同的模态映射到相同的表示空间。具体来说，我们为$ {\ bf cross-modal} $ $ {\ bf aptaptation} $提出了一个简单的策略：我们将来自不同模式的示例视为其他示例。例如，通过简单地将类名称重新列为额外的培训样本，我们将任何N-shot学习问题都琐碎地转换为（n+1） - 拍摄问题。这使我们能够使用令人尴尬的简单线性分类器产生SOTA结果。我们证明我们的方法可以与现有方法（例如前缀调谐，适配器和分类器结合）结合使用。最后，为了探索超出视觉和语言的其他方式，我们构建了第一个（据我们所知）的视听基准，并使用跨模式训练来提高图像和音频分类的性能。

### PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2301.07301)] [[cool](https://papers.cool/arxiv/2301.07301)] [[pdf](https://arxiv.org/pdf/2301.07301)]
> **Authors**: Rui Wan,Tianyun Zhao,Wei Zhao
> **First submission**: 2023-01-17
> **First announcement**: 2023-01-18
> **comment**: No comments
- **标题**: PTA-DET：3D对象检测的点变压器关联点云和图像
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 在自动驾驶中，在面对车辆周围的复杂环境时，基于多模式数据的3D对象检测已成为必不可少的方法。在多模式检测过程中，LiDAR和相机同时用于捕获和建模。但是，由于激光雷达点和相机图像之间的固有差异，用于对象检测的数据融合会遇到一系列问题。大多数多模式检测方法的性能比仅激光雷达的方法还差。在这项研究中，我们提出了一种名为PTA-DET的方法，以提高多模式检测的性能。在PTA-DET的陪同下，提出了伪点云网络，该网络可以转换图像信息，包括伪点纹理和语义特征。此后，通过基于变压器的点融合过渡（PFT）模块，在基于统一的点表示下，可以将LIDAR点和伪点的特征深入融合。这些模块的结合可以征服跨模式融合特征融合的主要障碍，并意识到提案生成的互补和歧视性表示。 Kitti数据集的广泛实验表明，PTA-DET取得了竞争成果，并支持其有效性。

### Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data 
[[arxiv](https://arxiv.org/abs/2301.07283)] [[cool](https://papers.cool/arxiv/2301.07283)] [[pdf](https://arxiv.org/pdf/2301.07283)]
> **Authors**: Andrej Janda,Brandon Wagstaff,Edwin G. Ng,Jonathan Kelly
> **First submission**: 2023-01-17
> **First announcement**: 2023-01-18
> **comment**: In Proceedings of the Conference on Robots and Vision (CRV'23), Montreal, Canada, Jun. 6-8, 2023. arXiv admin note: substantial text overlap with arXiv:2211.11801
- **标题**: 与图像数据的点云分割网络的自我监督预训练的对比度学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 当标签稀缺且昂贵时，减少监督培训所需的注释数量至关重要。这种减少对于涉及3D数据集的语义细分任务尤其重要，这些任务通常比基于图像的对应物要小得多，并且具有更具挑战性的注释。对未标记数据进行自我监督的预训练是减少所需的手动注释量的一种方法。以前的工作专注于与点云专门进行预训练。虽然有用，但这种方法通常需要两个或多个注册的视图。在目前的工作中，我们首先学习自我监视的图像特征，然后使用这些功能来训练3D模型，从而结合图像和点云模式。通过合并通常包含在许多3D数据集中的图像数据，我们的预训练方法仅需要对场景进行一次扫描，并且可以应用于无法使用本地化信息的情况。我们证明，尽管使用了单扫描，但我们的训练前方法与其他纯云云方法的性能相当。

### Vision Learners Meet Web Image-Text Pairs 
[[arxiv](https://arxiv.org/abs/2301.07088)] [[cool](https://papers.cool/arxiv/2301.07088)] [[pdf](https://arxiv.org/pdf/2301.07088)]
> **Authors**: Bingchen Zhao,Quan Cui,Hao Wu,Osamu Yoshie,Cheng Yang,Oisin Mac Aodha
> **First submission**: 2023-01-17
> **First announcement**: 2023-01-18
> **comment**: Project page: https://bzhao.me/MUG/
- **标题**: 视觉学习者遇到网络图像文本对
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **摘要**: 许多自我监督的学习方法已在经过良好的Imagenet-1k数据集上进行了预训练。在这项工作中，鉴于Web数据的出色可扩展性，我们考虑在嘈杂的Web来源图像文本配对数据上进行自我监督的预训练。首先，我们对在类似的环境中的大规模Web数据上的代表性自我监督预训练方法进行了基准研究。我们比较了一系列方法，包括使用掩盖训练目标的单模式和使用图像文本约束训练的多模式的方法。我们观察到，现有的多模式方法在视觉传输学习任务上的单模式不超过其单模式的对应。我们得出了一种信息理论观点来解释这些基准结果，该结果提供了有关如何设计新型视觉学习者的见解。受此见解的启发，我们提出了一种新的视觉表示预训练方法多模式发生器〜（mug），该方法从可扩展的Web来源的图像文本数据中学习。杯子在各种任务上实现了最新的转移性能，并展示了有希望的缩放属性。接受预培训的模型和代码将在接受后公开。

### A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction 
[[arxiv](https://arxiv.org/abs/2301.06782)] [[cool](https://papers.cool/arxiv/2301.06782)] [[pdf](https://arxiv.org/pdf/2301.06782)]
> **Authors**: Chongshan Lu,Fukun Yin,Xin Chen,Tao Chen,Gang YU,Jiayuan Fan
> **First submission**: 2023-01-17
> **First announcement**: 2023-01-18
> **comment**: No comments
- **标题**: 一个大规模的户外多模式数据集和基准，用于新型视图合成和隐性场景重建
- **领域**: 计算机视觉和模式识别
- **摘要**: 神经辐射场（NERF）在单个对象场景重建和新颖的观点综合中取得了令人印象深刻的结果，在许多单个单一模式和以DTU，BMVS和NERF合成的室内场景数据集（如DTU）中，这已经证明了这些模式和单一对象的室内场景数据集。由于昂贵的数据获取和校准成本，NERF评估。在本文中，我们提出了一个大规模的户外多模式数据集，即OMMO数据集，其中包含具有校准图像，点云和提示注释的复杂地面对象和场景。同时，建立了一些基于室外NERF的任务的新基准，例如新型视图合成，表面重建和多模式NERF。为了创建数据集，我们捕获并收集大量的真实的飞行视频，然后从中选择高质量和高分辨率剪辑。然后，我们设计了一个质量审核模块，以完善图像，删除低质量的框架并通过基于学习的自动评估以及手动评论进行失败的场景。最后，使用许多志愿者为每个场景添加文本说明，并键入键框架，以满足将来潜在的多模式要求。与现有的NERF数据集相比，我们的数据集包含丰富的现实城市和自然场景，这些场景具有各种规模，摄像头轨迹和照明条件。实验表明，我们的数据集可以在不同任务上基准大多数最新的NERF方法。我们将很快发布数据集和模型权重。

### MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval 
[[arxiv](https://arxiv.org/abs/2301.07868)] [[cool](https://papers.cool/arxiv/2301.07868)] [[pdf](https://arxiv.org/pdf/2301.07868)]
> **Authors**: Xiaojie Jin,Bowen Zhang,Weibo Gong,Kai Xu,XueQing Deng,Peng Wang,Zhao Zhang,Xiaohui Shen,Jiashi Feng
> **First submission**: 2023-01-18
> **First announcement**: 2023-01-19
> **comment**: No comments
- **标题**: MV适配器：视频文本检索的多模式视频传输学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 最新的视频检索（VTR）方法通常涉及在特定数据集上完全微调预训练的模型（例如剪辑）。但是，这可能会导致实际应用中的大量存储成本，因为必须存储每个任务的单独模型。为了解决这个问题，我们介绍了我们的开拓性工作，该工作可以使用预训练的模型启用参数有效的VTR，在训练过程中只有少量可调参数。为了实现这一目标，我们提出了一种称为多模式视频适配器（MV-ADAPTER）的新方法，以有效地将预训练的剪辑中的知识从图像文本传输到视频文本。具体而言，MV-Audapter在视频和文本分支中都利用瓶颈结构以及两个新型组件。第一个是一个时间适应模块，该模块已包含在视频分支中，以引入全球和本地时间上下文。我们还训练校准重量以适应整个框架的动态变化。第二个是交叉模态绑定，通过共享交叉模态因子为视频/文本分支生成权重，以更好地对齐方式。得益于上述创新，MV-Audapter可以比标准的全面调整，而用可忽略不计的参数来实现可比或更好的性能。值得注意的是，MV适配器在五个广泛使用的VTR基准（MSR-VTT，MSVD，LSMDC，DIDEMO和Activitynet）上的V2T/T2V任务中始终优于各种竞争方法。

### Temporal Perceiving Video-Language Pre-training 
[[arxiv](https://arxiv.org/abs/2301.07463)] [[cool](https://papers.cool/arxiv/2301.07463)] [[pdf](https://arxiv.org/pdf/2301.07463)]
> **Authors**: Fan Ma,Xiaojie Jin,Heng Wang,Jingjia Huang,Linchao Zhu,Jiashi Feng,Yi Yang
> **First submission**: 2023-01-18
> **First announcement**: 2023-01-19
> **comment**: No comments
- **标题**: 暂时感知视频语言预训练
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 视频语言预训练模型最近显着改善了各种多模式下游任务。先前的主导作品主要采用对比学习，以实现跨模式的全球特征对齐。但是，视频和文本之间的本地关联并未建模，从而限制了训练模型的一般性，尤其是对于需要某些查询文本的时间视频边界的任务。这项工作介绍了一种新颖的文本视频本地化前文本任务，以实现精细的时间和语义对齐，以便训练有素的模型可以准确地感知给定文本描述的视频中的时间界。具体而言，文本视频本地化包含矩检修，它可以预测给定文本描述的视频中的开始和结束边界，以及与文本的子集与视频功能相匹配的文本本地化。为了产生时间边界，手动将几个视频中的框架功能手动合并为与文本序列相互作用的长视频序列。通过本地化任务，我们的方法将细粒框架表示与单词表示形式联系起来，并隐式区分单个模式中不同实例的表示。值得注意的是，全面的实验结果表明，我们的方法可显着提高各种基准的最先进性能，涵盖文本到视频检索，视频问题的回答，视频字幕，时间动作定位和时间瞬间检索。该代码将很快发布。

### HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness 
[[arxiv](https://arxiv.org/abs/2301.07405)] [[cool](https://papers.cool/arxiv/2301.07405)] [[pdf](https://arxiv.org/pdf/2301.07405)]
> **Authors**: Zongwei Wu,Guillaume Allibert,Fabrice Meriaudeau,Chao Ma,Cédric Demonceaux
> **First submission**: 2023-01-18
> **First announcement**: 2023-01-19
> **comment**: No comments
- **标题**: HIDANET：RGB-D显着对象通过层次深度意识检测
- **领域**: 计算机视觉和模式识别
- **摘要**: RGB-D显着性检测旨在融合多模式线索，以准确定位显着区域。现有作品通常采用注意模块进行特征建模，几乎没有明确利用细粒细节与语义提示合并的方法。因此，尽管有辅助深度信息，但现有模型以相似外观但在不同的相机距离处区分对象仍然具有挑战性。在本文中，从新的角度来看，我们提出了一个新型的分层深度意识网络（HIDANET），用于RGB-D显着性检测。我们的动机源于观察到几何先验的多晶格特性与神经网络层次结构息息相关。为了实现多模式和多级融合，我们首先使用基于粒度的注意方案来分别增强RGB和深度特征的歧视力。然后，我们引入了一个统一的交叉双意见模块，用于以粗到精细的方式进行多模式和多级融合。编码的多模式特征逐渐汇总到共享解码器中。此外，我们利用多尺度损失来充分利用分层信息。关于挑战基准数据集的广泛实验表明，我们的Hidanet通过大幅度的最先进方法表现出色。

### Towards Models that Can See and Read 
[[arxiv](https://arxiv.org/abs/2301.07389)] [[cool](https://papers.cool/arxiv/2301.07389)] [[pdf](https://arxiv.org/pdf/2301.07389)]
> **Authors**: Roy Ganz,Oren Nuriel,Aviad Aberdam,Yair Kittenplon,Shai Mazor,Ron Litman
> **First submission**: 2023-01-18
> **First announcement**: 2023-01-19
> **comment**: No comments
- **标题**: 迈向可以看到和阅读的模型
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 视觉问题回答（VQA）和图像字幕（CAP）是最流行的视觉语言任务之一，具有类似的场景文本版本，需要图像中文本中的推理。尽管它们显而易见，但两者是独立对待的，正如我们所显示的那样，可以看出或读取的任务特定方法，但不能两者兼而有之。在这项工作中，我们对这种现象进行了深入的分析，并提出了一种统一的文本 - 文本方法，该方法授予现有的多模式体系结构的场景文本理解能力。具体而言，我们将场景文本信息视为一种附加模式，将其与任何预算的编码器架构通过指定的模块融合在一起。彻底的实验表明，单位NNT会导致成功处理这两种任务类型的第一个单个模型。此外，我们表明，理解能力的场景文本可以将视力语言模型在General VQA和CAP上的性能提高高达2.69％和0.6苹果酒。

### MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation 
[[arxiv](https://arxiv.org/abs/2301.07354)] [[cool](https://papers.cool/arxiv/2301.07354)] [[pdf](https://arxiv.org/pdf/2301.07354)]
> **Authors**: Munan Ning,Donghuan Lu,Yujia Xie,Dongdong Chen,Dong Wei,Yefeng Zheng,Yonghong Tian,Shuicheng Yan,Li Yuan
> **First submission**: 2023-01-18
> **First announcement**: 2023-01-19
> **comment**: Accepted by TPAMI-IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv admin note: substantial text overlap with arXiv:2108.08012
- **标题**: MADAV2：高级基于多锚的活动域自适应细分
- **领域**: 计算机视觉和模式识别
- **摘要**: 无监督的域适应性已被广泛采用，这些任务稀缺。不幸的是，将目标域分布映射到源域无条件可能会扭曲目标域数据的基本结构信息，从而导致较低的性能。为了解决此问题，我们首先建议引入主动样本选择，以协助有关语义分割任务的域适应。通过创新采用多个锚代替单个质心，可以更好地将源域和目标域以多模式分布来表征，在这种方式中，从目标域中选择了更多补充和信息性的样本。只需少量工作量即可手动注释这些活跃的样本，因此可以有效缓解目标域分布的失真，从而实现巨大的性能增长。此外，提出了强大的半监督域适应策略，以减轻长尾分配问题并进一步改善细分性能。广泛的实验是在公共数据集上进行的，结果表明，所提出的方法以大幅度优于最先进的方法，并且实现与完全监督的上行相似的性能，即71.4％miou on GTA5和71.8％miou on ponsythia。每个组件的有效性还通过彻底消融研究来验证。

### Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation 
[[arxiv](https://arxiv.org/abs/2301.09209)] [[cool](https://papers.cool/arxiv/2301.09209)] [[pdf](https://arxiv.org/pdf/2301.09209)]
> **Authors**: Razvan-George Pasca,Alexey Gavryushin,Muhammad Hamza,Yen-Ling Kuo,Kaichun Mo,Luc Van Gool,Otmar Hilliges,Xi Wang
> **First submission**: 2023-01-22
> **First announcement**: 2023-01-23
> **comment**: No comments
- **标题**: 总结过去以预测未来的过去：上下文的自然语言描述增强了多模式互动预期
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 我们在以自我为中心视频中研究对象互动预期。这项任务需要了解过去动作在对象上形成的时空上下文，即创造的动作上下文。我们建议输血，这是一种基于多模式变压器的体系结构。它通过总结动作上下文来利用语言的代表力。输血利用预先训练的图像字幕和视觉语言模型从过去的视频帧中提取动作上下文。此操作上下文与下一个视频框架一起由多模式融合模块处理，以预测下一个对象交互。我们的模型可以更有效地端到端学习。大型预训练的语言模型增加了常识和概括能力。对EGO4D和Epic-Kitchens-100的实验显示了我们多模式融合模型的有效性。他们还强调了在视觉似乎足够的任务中使用基于语言的上下文摘要的好处。在EGO4D测试集的整体地图中，我们的方法以相对术语的相对方式优于最先进的方法。我们通过对Epic-Kitchens-100进行实验来验证输血的有效性。视频和代码可在https://eth-ait.github.io/transfusion-proj/上找到。

### MATT: Multimodal Attention Level Estimation for e-learning Platforms 
[[arxiv](https://arxiv.org/abs/2301.09174)] [[cool](https://papers.cool/arxiv/2301.09174)] [[pdf](https://arxiv.org/pdf/2301.09174)]
> **Authors**: Roberto Daza,Luis F. Gomez,Aythami Morales,Julian Fierrez,Ruben Tolosana,Ruth Cobos,Javier Ortega-Garcia
> **First submission**: 2023-01-22
> **First announcement**: 2023-01-23
> **comment**: Preprint of the paper presented to the Workshop on Artificial Intelligence for Education (AI4EDU) of AAAI 2023
- **标题**: 马特：电子学习平台的多模式注意水平估计
- **领域**: 计算机视觉和模式识别,人机交互,机器学习
- **摘要**: 这项工作为基于多模式的面部分析提供了一种新的多模式系统，用于远程注意力水平估计。我们的多模式方法使用从与认知负载（例如面部手势（例如，眨眼速率，面部动作单位）和用户行动（例如，头姿势，距离距离距离）相关的行为和生理过程获得的不同参数和信号。多模式系统使用基于卷积神经网络（CNN）的以下模块：眼睛眨眼检测，头部姿势估计，面部标志性检测和面部表达特征。首先，我们在估计在线电子学习过程中捕获的学生注意力水平的任务中分别评估了所提出的模块。为此，我们根据每个模块的支持向量机（SVM）训练了二进制分类器（高度注意或低注意力）。其次，我们发现多模式得分水平融合在多大程度上改善了注意力水平的估计。 MEBA数据库用于实验框架，这是一种公共多模式数据库，用于在电子学习环境中获得的注意水平估计，该环境包含来自38位用户的数据，同时执行可变难度的几个电子学习任务（在学生认知负载中创建变化）。

### Champion Solution for the WSDM2023 Toloka VQA Challenge 
[[arxiv](https://arxiv.org/abs/2301.09045)] [[cool](https://papers.cool/arxiv/2301.09045)] [[pdf](https://arxiv.org/pdf/2301.09045)]
> **Authors**: Shengyi Gao,Zhe Chen,Guo Chen,Wenhai Wang,Tong Lu
> **First submission**: 2023-01-21
> **First announcement**: 2023-01-23
> **comment**: Technical report in WSDM Cup 2023
- **标题**: WSDM2023 Toloka VQA挑战的冠军解决方案
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本报告中，我们向WSDM2023 Toloka Visual Essone（VQA）挑战提供了冠军解决方案。与常见的VQA和视觉接地（VG）任务不同，此挑战涉及一个更复杂的方案，即推断和定位由给定疑问问题隐含指定的对象。对于此任务，我们利用无培训的适配器网络VIT-ADAPTER来适应多模式预训练的Uni-Pecteriver，以更好地跨模式定位。我们的方法在排行榜上排名第一，分别在公共和私人测试集上获得77.5和76.347。它表明，VIT-ADAPTER也是将统一感知模型适应下游任务的有效范式。代码和模型将在https://github.com/czczup/vit-adapter/tree/main/main/wsdm2023上发布。

### Novel-View Acoustic Synthesis 
[[arxiv](https://arxiv.org/abs/2301.08730)] [[cool](https://papers.cool/arxiv/2301.08730)] [[pdf](https://arxiv.org/pdf/2301.08730)]
> **Authors**: Changan Chen,Alexander Richard,Roman Shapovalov,Vamsi Krishna Ithapu,Natalia Neverova,Kristen Grauman,Andrea Vedaldi
> **First submission**: 2023-01-20
> **First announcement**: 2023-01-23
> **comment**: Accepted at CVPR 2023. Project page: https://vision.cs.utexas.edu/projects/nvas
- **标题**: 小说视觉合成
- **领域**: 计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 我们介绍了新颖的声学综合（NVAS）任务：鉴于在源观点上观察到的视线和声音，我们可以从看不见的目标视点综合该场景的声音吗？我们提出了一种神经渲染方法：视觉引导的声学合成（VIGAS）网络，该网络通过分析输入音频 - 视觉提示来综合空间中任意点的声音。为了对此任务进行基准测试，我们收集了两个首先的大规模多视频视听数据集，一个是合成的，一个是真实的。我们表明，我们的模型成功地说明了有关空间提示的原因，并在两个数据集上综合了忠实的音频。据我们所知，这项工作代表了解决新颖的声学综合任务的第一个公式，数据集和方法，该任务具有从AR/VR到艺术和设计的令人兴奋的潜在应用。通过这项工作解锁，我们认为，小说视图合成的未来是从视频中学习的多模式学习。

### Zorro: the masked multimodal transformer 
[[arxiv](https://arxiv.org/abs/2301.09595)] [[cool](https://papers.cool/arxiv/2301.09595)] [[pdf](https://arxiv.org/pdf/2301.09595)]
> **Authors**: Adrià Recasens,Jason Lin,Joāo Carreira,Drew Jaegle,Luyu Wang,Jean-baptiste Alayrac,Pauline Luc,Antoine Miech,Lucas Smaira,Ross Hemsley,Andrew Zisserman
> **First submission**: 2023-01-23
> **First announcement**: 2023-01-24
> **comment**: No comments
- **标题**: Zorro：蒙版的多模式变压器
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于注意力的模型吸引了多模式处理，因为可以将多种模式的输入加入并馈送到单个骨干网络 - 因此几乎不需要融合工程。但是，由此产生的表示形式完全纠缠在整个网络中，这可能并不总是可取的：在学习中，对比的是视听自我监督的学习需要独立的音频和视觉功能才能操作，否则学习崩溃了；在推断中，应该在仅具有音频或视频的基准上进行视听模型的评估。在本文中，我们介绍了Zorro，该技术使用掩码来控制每个模态的输入如何在变压器内部路由，从而保持表示模式的某些部分。我们将此技术应用于三个受欢迎的基于变压器的架构（VIT，SWIN和HIP），并表明，通过对比前训练的Zorro实现了多模式任务（Audioset和VggSound）的最相关基准的最先进结果。此外，最终的模型能够对视频和音频基准（例如Kinetics-400或ESC-50）进行单峰推断。

### HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images 
[[arxiv](https://arxiv.org/abs/2301.09460)] [[cool](https://papers.cool/arxiv/2301.09460)] [[pdf](https://arxiv.org/pdf/2301.09460)]
> **Authors**: Kun Li,George Vosselman,Michael Ying Yang
> **First submission**: 2023-01-23
> **First announcement**: 2023-01-24
> **comment**: No comments
- **标题**: HRVQA：一个视觉问题回答高分辨率航空图像的基准测试
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉问题回答（VQA）是计算机视觉中一项重要且挑战性的多模式任务。最近，由于其在灾难监测，城市规划和数字地球产品生成中的现实应用程序的潜在应用，已经做出了一些努力，将VQA任务带入航空图像。但是，不仅在航空图像中概念的外观，规模和方向的巨大变化，而且据称良好的数据集的稀缺性限制了该域中VQA的发展。在本文中，我们介绍了一个新的数据集HRVQA，该数据集提供了53512个1024*1024像素的航空图像和半自动化的1070240 QA Pairs。为了基准对空中图像的VQA模型的理解能力，我们评估了HRVQA上的相关方法。此外，我们提出了一个新型模型GFTRANSFORMER，它具有封闭的注意模块和一个相互融合模块。实验表明，所提出的数据集非常具有挑战性，尤其是与特定属性相关的问题。与以前的最新方法相比，我们的方法取得了出色的性能。数据集和源代码将在https://hrvqa.nl/上发布。

### Object Segmentation with Audio Context 
[[arxiv](https://arxiv.org/abs/2301.10295)] [[cool](https://papers.cool/arxiv/2301.10295)] [[pdf](https://arxiv.org/pdf/2301.10295)]
> **Authors**: Kaihui Zheng,Yuqing Ren,Zixin Shen,Tianxu Qin
> **First submission**: 2023-01-03
> **First announcement**: 2023-01-25
> **comment**: Research project for Introduction to Deep Learning (11785) at Carnegie Mellon University
- **标题**: 用音频上下文进行对象分割
- **领域**: 计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 视觉对象通常具有声学签名，这些签名自然与录音录像中自然同步。对于此项目，我们探讨了用于视频实例分割任务的多模式功能聚合，其中我们将音频功能集成到视频细分模型中以执行视听学习方案。我们的方法基于现有的视频实例细分方法，该方法利用视频帧的富裕上下文信息。由于这是研究视听实例分割的首次尝试，因此收集了一个新颖的数据集，其中包括20种带有同步视频和音频录制的人声类。通过利用组合解码器融合视频和音频功能，我们的模型与基本模型相比显示出略有改进。此外，我们通过进行大量消融来展示不同模块的有效性。

### Proceedings of the 1st International Workshop on Reading Music Systems 
[[arxiv](https://arxiv.org/abs/2301.10062)] [[cool](https://papers.cool/arxiv/2301.10062)] [[pdf](https://arxiv.org/pdf/2301.10062)]
> **Authors**: Jorge Calvo-Zaragoza,Jan Hajič jr.,Alexander Pacha
> **First submission**: 2022-12-01
> **First announcement**: 2023-01-25
> **comment**: Proceedings edited by Jorge Calvo-Zaragoza, Jan Hajič jr. and Alexander Pacha
- **标题**: 关于阅读音乐系统的第一届国际研讨会论文集
- **领域**: 计算机视觉和模式识别,信息检索,机器学习
- **摘要**: 关于阅读音乐系统（蠕虫）的国际研讨会是一个研讨会，试图将开发阅读音乐系统的研究人员（例如在光学音乐识别领域）与其他可以从图书馆员或音乐学家等这样的系统中受益的研究人员和从业人员。讲习班感兴趣的相关主题包括但不限于：音乐阅读系统；光学识别；数据集和绩效评估；音乐分数的图像处理；作者身份证明；音乐分数的创作，编辑，存储和演示系统；多模式系统；音乐创作音乐的新颖输入方法；基于Web的音乐信息检索服务；申请和项目；与书面音乐有关的用例。这些是2018年9月20日在巴黎举行的有关阅读音乐系统的第一届国际研讨会的会议记录。

### Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues 
[[arxiv](https://arxiv.org/abs/2301.09914)] [[cool](https://papers.cool/arxiv/2301.09914)] [[pdf](https://arxiv.org/pdf/2301.09914)]
> **Authors**: Verena Jasmin Hallitschke,Tobias Schlumberger,Philipp Kataliakos,Zdravko Marinov,Moon Kim,Lars Heiliger,Constantin Seibold,Jens Kleesiek,Rainer Stiefelhagen
> **First submission**: 2023-01-24
> **First announcement**: 2023-01-25
> **comment**: Accepted at ISBI 2023; 5 pages, 5 figures
- **标题**: 多模式互动肺部病变分割：基于生理和解剖学提示的注释PET/CT图像的框架
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近，深度学习使医学成像中各种疾病的准确分割。但是，这些表演通常需要大量的手动体素注释。当不是所有必需的信息都可以在单个成像域中获得的情况，而PET/CT数据的情况下，这种乏味的体积数据过程变得更加复杂。我们提出了一个多模式互动分割框架，该框架通过结合PET/CT数据的解剖学和生理提示来减轻这些问题。我们的框架利用地球距离变换来表示用户注释，并在训练过程中实现了一种新颖的基于椭球的用户模拟方案。我们进一步提出了两个注释界面，并进行用户研究以估计其可用性。我们在域内验证数据集和看不见的PET/CT数据集上评估了我们的模型。我们将代码公开可用：https：//github.com/verena-hallitschke/pet-ct-ct-annotate。

### Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning 
[[arxiv](https://arxiv.org/abs/2301.10931)] [[cool](https://papers.cool/arxiv/2301.10931)] [[pdf](https://arxiv.org/pdf/2301.10931)]
> **Authors**: Linfeng Xu,Qingbo Wu,Lili Pan,Fanman Meng,Hongliang Li,Chiyuan He,Hanxin Wang,Shaoxu Cheng,Yu Dai
> **First submission**: 2023-01-25
> **First announcement**: 2023-01-26
> **comment**: IEEE Transactions on Multimedia
- **标题**: 致力于以自我为中心的活动识别：一种多式联运的以持续学习为中心的活动数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着可穿戴摄像机的快速开发，可以使用大量的以自我为中心的视频，用于第一人称视觉感知。使用以自我为中心的视频来预测第一人称活动会面临许多挑战，包括有限的视野，遮挡和不稳定的动作。观察可穿戴设备的传感器数据有助于人类活动的识别，多模式活动识别引起了人们越来越多的关注。但是，相关数据集的缺乏阻碍了以自我为中心活动识别的多模式深度学习的发展。如今，现实世界中的深度学习已导致人们专注于持续学习，这种学习常常遭受灾难性的遗忘。但是，由于数据集的不可用而无法探索，尤其是在多种方式的背景下，灾难性的遗忘问题，尤其是在多种模式的情况下，仍未探索。为了协助这项研究，我们提出了一个多模式的以持续学习为中心的活动数据集，该数据集名为UESTC-MMEA-CL，该数据通过自行开发的眼镜集成了第一人称摄像机和可穿戴传感器。它包含由10种参与者进行的32种日常活动的视频，加速度计和陀螺仪的同步数据。将其类型和规模与其他公开可用的数据集进行了比较。给出了传感器数据的统计分析，以显示不同行为的辅助效应。并在基本网络体系结构上分别使用三种方式，共同使用三种方式：RGB，加速度和陀螺仪。为了探索连续学习任务中的灾难性遗忘，通过不同的多模式组合对四种基线方法进行了广泛的评估。我们希望UESTC-MEA-CL能够促进对可穿戴应用中第一人称活动识别的持续学习的未来研究。

### Detecting Building Changes with Off-Nadir Aerial Images 
[[arxiv](https://arxiv.org/abs/2301.10922)] [[cool](https://papers.cool/arxiv/2301.10922)] [[pdf](https://arxiv.org/pdf/2301.10922)]
> **Authors**: Chao Pang,Jiang Wu,Jian Ding,Can Song,Gui-Song Xia
> **First submission**: 2023-01-25
> **First announcement**: 2023-01-26
> **comment**: ef:SCIENCE CHINA Information Sciences (SCIS) 2023
- **标题**: 检测建筑物的变化使用距离航空图像
- **领域**: 计算机视觉和模式识别
- **摘要**: 北极航空图像的倾斜性质给建筑物变化检测带来了严重的挑战（BCD）问题：附近建筑物的不匹配和建筑物外墙的语义歧义。为了应对这些挑战，我们提出了一个多任务指导的变更检测网络模型，称为MTGCD-NET。提出的模型通过设计三个辅助任务来解决特定的BCD问题，包括：（1）像素的分类任务，以预测建筑物的屋顶和外墙； （2）学习每栋建筑物的屋顶到脚印偏移量的辅助任务，以解释建筑物屋顶实例之间的错位； （3）学习双向航空图像之间相同屋顶匹配流的辅助任务，以解决建筑物屋顶不匹配问题。这些辅助任务提供了必不可少的互补建筑解析和匹配信息。辅助任务的预测最终与多模式蒸馏模块融合到主建筑物变化检测分支。为了训练和测试模型，用于使用非纳迪尔航空图像的BCD问题，我们创建了一个新的基准数据集，名为Bandon。广泛的实验表明，我们的模型比以前的最新竞争对手达到了卓越的性能。

### ITstyler: Image-optimized Text-based Style Transfer 
[[arxiv](https://arxiv.org/abs/2301.10916)] [[cool](https://papers.cool/arxiv/2301.10916)] [[pdf](https://arxiv.org/pdf/2301.10916)]
> **Authors**: Yunpeng Bai,Jiayue Liu,Chao Dong,Chun Yuan
> **First submission**: 2023-01-25
> **First announcement**: 2023-01-26
> **comment**: 8 pages, 10 figures
- **标题**: ITStyler：基于图像优化的基于文本的样式转移
- **领域**: 计算机视觉和模式识别
- **摘要**: 基于文本的样式转移是一个新出现的研究主题，它使用文本信息代替样式图像来指导传输过程，从而大大扩展了样式传输的应用程序方案。但是，以前的方法需要额外的时间进行优化或文本图像配对数据，从而导致有效性有限。在这项工作中，我们获得了一种基于数据的基于文本的样式传输方法，该方法在推理阶段不需要优化。具体来说，我们将文本输入转换为预训练的VGG网络的样式空间，以实现更有效的样式交换。我们还利用Clip的多模式嵌入空间仅使用图像数据集来学习文本到风格的映射。我们的方法可以实时传输任意新样式的文本输入样式，并合成高质量的艺术图像。

### Efficient Flow-Guided Multi-frame De-fencing 
[[arxiv](https://arxiv.org/abs/2301.10759)] [[cool](https://papers.cool/arxiv/2301.10759)] [[pdf](https://arxiv.org/pdf/2301.10759)]
> **Authors**: Stavros Tsogkas,Fengjia Zhang,Allan Jepson,Alex Levinshtein
> **First submission**: 2023-01-25
> **First announcement**: 2023-01-26
> **comment**: 16 pages, 12 figures. Published at the Winter Conference on Application of Computer Vision (WACV) 2023
- **标题**: 有效流动引导的多帧置膜
- **领域**: 计算机视觉和模式识别
- **摘要**: 拍摄照片“野外”通常受到围栏用户和感兴趣现场的围栏障碍的阻碍，而这些障碍物很难避免。围栏是自动从图像中消除此类障碍物的算法过程，从而揭示了场景的无形部分。虽然该问题可以作为围栏分割和图像插入的组合提出，但这通常会导致被阻塞区域的令人难以置信的幻觉。现有的多帧方法依赖于从其暂时邻居那里传播到所选的钥匙扣的信息，但是它们通常效率低下，并且在严重阻碍图像的一致性方面挣扎。在这项工作中，我们从视频完成文献中汲取灵感，并为多帧磁盘开发一个简化的框架，该框架直接从受阻框架中计算出高质量的流量图，并使用它们来准确地对齐框架。我们的主要重点是在现实世界中的效率和实用性：我们算法的输入是一个简短的图像突发（5帧） - 现代智能手机中常见的数据模式 - 输出是单个重建的密钥帧，围栏已移除。我们的方法利用了经过精心生成的合成数据培训的简单而有效的CNN模块，并且在实时运行时，均定量和定性地超过了更复杂的替代方案。

### Improving Cross-modal Alignment for Text-Guided Image Inpainting 
[[arxiv](https://arxiv.org/abs/2301.11362)] [[cool](https://papers.cool/arxiv/2301.11362)] [[pdf](https://arxiv.org/pdf/2301.11362)]
> **Authors**: Yucheng Zhou,Guodong Long
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: EACL 2023
- **标题**: 改善文本指导图像插图的跨模式对齐
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 文本指导的图像介绍（TGII）旨在根据损坏的图像中给定的文本恢复缺失区域。现有方法基于强大的视觉编码器和跨模式融合模型，以集成跨模式特征。但是，这些方法将大部分计算分配给视觉编码，而在建模模式相互作用上进行了光计算。此外，它们具有跨模式融合的深度特征，这忽略了文本和图像之间的细粒度对齐。最近，封装丰富的跨模式对准知识的视觉预训练模型（VLPM）在大多数多模式任务中都提出了发展。在这项工作中，我们通过改善跨模式比对（CMA）提出了一个新型TGII模型。 CMA模型由VLPM作为视觉语言编码器，图像发生器和全局本地鉴别器组成。为了探索用于图像恢复的跨模式比对知识，我们引入了跨模式比对蒸馏和样本内分布蒸馏。此外，我们采用对抗性训练来增强模型，以有效地填补复杂结构中缺失的区域。实验是在两个流行的视觉语言数据集上进行的。结果表明，与其他强大的竞争对手相比，我们的模型可实现最先进的表现。

### Multimodal Event Transformer for Image-guided Story Ending Generation 
[[arxiv](https://arxiv.org/abs/2301.11357)] [[cool](https://papers.cool/arxiv/2301.11357)] [[pdf](https://arxiv.org/pdf/2301.11357)]
> **Authors**: Yucheng Zhou,Guodong Long
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: EACL 2023
- **标题**: 图像引导故事结束一代的多模式事件变压器
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 图像指导的故事结局一代（IGSEG）是根据给定的故事情节和结束图像产生一个故事。现有方法着眼于跨模式特征融合，但忽略了故事情节和结束图像的推理和采矿隐式信息。为了解决这个缺点，我们提出了一个多模式事件变压器，这是一个基于事件的推理框架。具体来说，我们从故事情节和结束图像结束视觉和语义事件图表，并利用基于事件的推理来以单个方式进行推理和矿隐信息。接下来，我们连接视觉和语义事件图，并利用跨模式融合来整合不同模式的特征。此外，我们提出了一个多模式注射器，以自适应通过基本信息到解码器。此外，我们提出了一个不一致的检测，以增强故事情节的理解背景和模型的图形建模的鲁棒性。实验结果表明，我们的方法为图像引导的故事结束一代实现了最新的性能。

### Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring 
[[arxiv](https://arxiv.org/abs/2301.11116)] [[cool](https://papers.cool/arxiv/2301.11116)] [[pdf](https://arxiv.org/pdf/2301.11116)]
> **Authors**: Ruyang Liu,Jingjia Huang,Ge Li,Jiashi Feng,Xinglong Wu,Thomas H. Li
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: No comments
- **标题**: 重新访问基于剪辑的图像到视频知识传输的时间建模
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 图像文本预告片的模型，例如剪辑，已经显示出从大型图像文本数据对中学到的令人印象深刻的一般多模式知识，从而吸引了越来越多的关注，以提高视频域中的视觉表示学习的潜力。在本文中，基于剪辑模型，我们在图像到视频知识传输的上下文中重新审视时间建模，这是将图像文本预读的模型扩展到视频域的关键点。我们发现，当前的时间建模机制是针对高级语义主导任务（例如检索）或低级别视觉模式主导任务（例如识别）而定制的，并且同时处理两种情况。关键困难在于对时间依赖性进行建模，同时利用剪辑模型中的高级和低级知识。为了解决这个问题，我们提出了时空辅助网络（Stan） - 一种简单有效的时间建模机制将剪辑模型扩展到各种视频任务。具体而言，为了实现低级和高级知识的传递，Stan采用了一个分支结构，具有分解的时空模块，使多级剪辑特征能够在空间上进行上下文化。我们在两个代表性的视频任务上评估我们的方法：视频文本检索和视频识别。广泛的实验证明了我们的模型优于各种数据集上最先进的方法，包括MSR-VTT，DIDEMO，LSMDC，MSVD，Kinetics-400和Sothings-some-toshing-v2。代码将在https://github.com/farewellthree/stan上找到

### Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images 
[[arxiv](https://arxiv.org/abs/2301.10951)] [[cool](https://papers.cool/arxiv/2301.10951)] [[pdf](https://arxiv.org/pdf/2301.10951)]
> **Authors**: Nathan Hadjiyski,Ali Vosoughi,Axel Wismueller
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: Accepted to Computer-Aided Diagnosis, SPIE Medical Imaging 2023
- **标题**: 从放射学报告和X射线胸部图像中进行交叉模态全球局部代表性学习
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 深度学习模型可以成功地应用于真实问题；但是，培训大多数模型都需要大量数据。最近的方法使用语言和视觉，但不幸的是，它们依赖于通常不公开可用的数据集。在这里，我们为放射学多模式语言视觉领域的进一步研究铺平了道路。在本文中，我们培训一种代表性学习方法，该方法通过注意机制并基于印第安纳大学放射学报告（IU-RR）数据集使用语言和愿景的本地和全球表示。此外，我们使用学习的表征来诊断五种肺部病理：肺炎胃张力，心脏肿瘤，水肿，胸腔积液和巩固。最后，我们使用监督和零射击分类来广泛分析IU-RR数据集上表示的性能。曲线下的平均面积（AUC）用于评估分类器对五种肺部病理分类的准确性。使用不同的训练数据集（即Chexpert和Chexphoto），在IU-RR测试集上对五种肺部病理进行分类的平均AUC范围为0.85至0.87。这些结果与使用UI-RR的其他研究相比有利。广泛的实验证实了使用语言和视力信息的多模式全局局部表示对肺部病理进行分类的一致结果。

### Affective Faces for Goal-Driven Dyadic Communication 
[[arxiv](https://arxiv.org/abs/2301.10939)] [[cool](https://papers.cool/arxiv/2301.10939)] [[pdf](https://arxiv.org/pdf/2301.10939)]
> **Authors**: Scott Geng,Revant Teotia,Purva Tendulkar,Sachit Menon,Carl Vondrick
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: No comments
- **标题**: 目标驱动二元交流的情感面孔
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **摘要**: 我们介绍了一个视频框架，用于在二元对话期间对口头和非语言交流之间的关联进行建模。鉴于演讲者的输入演讲，我们的方法检索了听众的视频，他的面部表情在背景下在社会上是适当的。我们的方法进一步允许听众以自己的目标，个性或背景为条件。我们的方法通过大型语言模型和视觉语言模型的组成来对话，从而创建了可解释和可控制的内部表示。为了研究多模式的交流，我们提出了一个新的视频数据集，其中包括涵盖各种主题和人口统计学的对话。实验和可视化表明，我们的方法能够输出比基线更适合社会上的听众。但是，仍然存在许多挑战，我们公开发布数据集以促进进一步的进展。有关视频结果，数据和代码，请参见我们的网站：https：//realtalk.cs.cs.columbia.edu。

### Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval 
[[arxiv](https://arxiv.org/abs/2301.12644)] [[cool](https://papers.cool/arxiv/2301.12644)] [[pdf](https://arxiv.org/pdf/2301.12644)]
> **Authors**: Yizhen Chen,Jie Wang,Lijian Lin,Zhongang Qi,Jin Ma,Ying Shan
> **First submission**: 2023-01-29
> **First announcement**: 2023-01-30
> **comment**: Accepted to AAAI 2023 (Oral)
- **标题**: 对齐前标记：集成视频检索的多模式标签
- **领域**: 计算机视觉和模式识别
- **摘要**: 近年来，视频检索的视觉对齐学习引起了很多关注。大多数现有方法要么在不完全探索视频的多模式信息的情况下将图像文本预处理模型的知识传输到视频检索任务，要么简单地以蛮力的方式融合多模式特征而没有明确的指导。在本文中，我们通过标记以明确的方式集成了多模式信息，并将标签用作锚点以获得更好的视频文本对齐。各种预位的专家被用于提取多种模式的信息，包括对象，人员，运动，音频等。为了充分利用这些信息，我们建议该表（在对齐之前进行标记）网络，该表由视觉编码器，标签编码器，一个文本编码器，文本编码和标签编码构建杂音编码和远程编码的远程元素和远程元素组成。此外，为了加强视频和文本之间的相互作用，我们使用[Vision，Tag，Tage，Text]的三联体输入建立了一个联合跨模式编码器，并执行了两个其他监督任务，视频文本匹配（VTM）和蒙版语言建模（MLM）。广泛的实验结果表明，该表模型能够在各种视频文本检索基准测试中实现最先进的性能（SOTA），包括MSR-VTT，MSVD，LSMDC和DIDEMO。

### Multi-video Moment Ranking with Multimodal Clue 
[[arxiv](https://arxiv.org/abs/2301.13606)] [[cool](https://papers.cool/arxiv/2301.13606)] [[pdf](https://arxiv.org/pdf/2301.13606)]
> **Authors**: Danyang Hou,Liang Pang,Yanyan Lan,Huawei Shen,Xueqi Cheng
> **First submission**: 2023-01-29
> **First announcement**: 2023-01-31
> **comment**: 9 pages,6 figures
- **标题**: 多模式线索的多视频力矩排名
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频语料库时刻检索〜（VCMR）是通过自然语言查询从大量未修剪视频中检索相关视频时刻的任务。 VCMR的最新工作基于两阶段方法。在本文中，我们专注于改善两个阶段方法的两个问题：（1）时刻预测偏见：大多数查询的预测时刻来自最高检索的视频，而忽略了目标时刻在底部检索视频中的可能性，这是由于培训和培训和培训和融合过程中共享归一化的不一致而引起的。 （2）潜在的关键内容：视频的不同模式具有不同的关键信息以进行瞬间本地化。为此，我们提出了一个两阶段的模型\ textbf {m} ult \ textbf {i} -video ra \ textbf {n} king，带有m \ textbf {u} l \ textbf {t}在训练和推理期间，分钟使用共享的归一化，以对多个视频的候选力矩进行排名，以求解力矩预测偏差，从而更有效地预测目标力矩。此外，分钟的mutilmdaol线索挖掘〜（MCM）可以发现视频中不同模态的关键内容，以更准确地定位力矩。 Minute在TVR和DIDEMO数据集上的基准优于基准，从而实现了VCMR的新最新。我们的代码将在GitHub上找到。

### Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning 
[[arxiv](https://arxiv.org/abs/2301.13335)] [[cool](https://papers.cool/arxiv/2301.13335)] [[pdf](https://arxiv.org/pdf/2301.13335)]
> **Authors**: Jian Zhu,Hanli Wang,Miaojing Shi
> **First submission**: 2023-01-30
> **First announcement**: 2023-01-31
> **comment**: No comments
- **标题**: 多模式大型语言模型增强了视觉常识性推理的伪3D感知框架
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 视觉常识推理（VCR）任务是选择答案，并根据给定的图像和质地问题提供合理的理由。代表作品首先识别图像中的对象，然后将它们与文本中的关键词相关联。但是，现有方法并不以类似人类的三维（3D）方式考虑对象的确切位置，从而使它们无法准确区分对象并理解视觉关系。最近，多模式的大语言模型（MLLM）已被用作多个多模式任务的强大工具，但对于VCR而言，这需要详细的推理，以对文本引用的特定视觉对象进行详细的推理。鉴于上述，MLLM增强的伪3D感知框架是为VCR设计的。具体而言，我们首先证明对象之间的关系与图像中的对象深度有关，因此将对象深度引入VCR框架中以推断图像中对象的3D位置。然后，提出了一个深度感知的变压器将对象之间的深度差异编码为变压器的注意机制，以将对象与深度引导的视觉场景区分地关联对象。为了将答案与视觉场景的深度联系起来，答案中的每个单词都用伪深度标记，以实现答案单词和对象之间的深度感知关联。另一方面，使用Blip-2作为MLLM来处理图像和文本，并且涉及特定视觉对象的文本中的引用表达式被用语言对象标签修改为可理解的MLLM输入。最后，设计了一种参数优化技术，可以完全考虑基于多层推理置信度的数据批次的质量。 VCR数据集上的实验证明了所提出的框架优于最先进的方法。

### Language-Driven Anchors for Zero-Shot Adversarial Robustness 
[[arxiv](https://arxiv.org/abs/2301.13096)] [[cool](https://papers.cool/arxiv/2301.13096)] [[pdf](https://arxiv.org/pdf/2301.13096)]
> **Authors**: Xiao Li,Wei Zhang,Yining Liu,Zhanhao Hu,Bo Zhang,Xiaolin Hu
> **First submission**: 2023-01-30
> **First announcement**: 2023-01-31
> **comment**: Accepted by CVPR 2024
- **标题**: 语言驱动的锚定为零拍对对抗性鲁棒性
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 已知深层神经网络（DNN）容易受到对抗攻击的影响。先前的研究主要集中于在充分监督的环境中改善对抗性鲁棒性，这使零射击的对抗性鲁棒性具有挑战性的领域是一个悬而未决的问题。在这项工作中，我们通过利用诸如夹子等大型视觉模型的最新进展来研究该领域，以将零射击的对抗性鲁棒性引入DNN。我们提出了LAAT，这是一种基于语言的，基于锚的对抗训练策略。 LAAT将每个类别的文本编码器的特征用作固定锚（归一化功能嵌入）的特征，然后将其用于对抗训练。通过利用文本编码的语义一致性，LAAT旨在增强图像模型在新类别上的对抗性鲁棒性。但是，天真地使用文本编码器会导致不良结果。通过分析，我们确定该问题是文本编码器之间的高余弦相似性。然后，我们设计了一种扩展算法和一个对齐跨凝结损失，以减轻问题。我们的实验结果表明，LAAT显着改善了对最先进方法的零拍对鲁棒性。 LAAT有可能通过大规模多模型来增强对抗性鲁棒性，尤其是在训练过程中无法使用的标记数据时。

## 信息检索(cs.IR:Information Retrieval)

该领域共有 2 篇论文

### MAKE: Vision-Language Pre-training based Product Retrieval in Taobao Search 
[[arxiv](https://arxiv.org/abs/2301.12646)] [[cool](https://papers.cool/arxiv/2301.12646)] [[pdf](https://arxiv.org/pdf/2301.12646)]
> **Authors**: Xiaoyang Zheng,Zilong Wang,Ke Xu,Sen Li,Tao Zhuang,Qingwen Liu,Xiaoyi Zeng
> **First submission**: 2023-01-29
> **First announcement**: 2023-01-30
> **comment**: 5 pages, accepted to The Industry Track of the Web Conference 2023
- **标题**: 制作：淘宝搜索中基于视觉的基于培训的产品检索
- **领域**: 信息检索
- **摘要**: TAOBAO搜索包括两个阶段：检索阶段和排名阶段。给定用户查询，检索阶段为以下排名返回候选产品的子集。最近，预训练和微调的范式表明了其在将视觉线索纳入检索任务中的潜力。在本文中，我们专注于解决淘宝搜索中文本到莫尔多的问题的问题。我们认为用户对标题或图像的关注在产品上有所不同。因此，我们为跨模式融合提出了一个新型的模态适应模块，该模块有助于分配跨产品的文本和图像的适当权重。此外，在电子商务搜索中，用户查询往往是简短的，因此导致用户查询和产品标题之间的语义不平衡。因此，我们设计了一个单独的文本编码器和一个关键字增强机制，以丰富查询表示并改善文本到磁性匹配。为此，我们提出了一种新颖的视觉语言（V+L）预训练方法，以利用（用户查询，产品标题，产品图像）的多模式信息。广泛的实验表明，我们的检索特异性预训练模型（称为Make）优于文本到莫尔多尔检索任务上现有的V+L预训练方法。 Make已在网上部署，并在TAOBAO搜索的检索系统上进行了重大改进。

### Enhancing Dyadic Relations with Homogeneous Graphs for Multimodal Recommendation 
[[arxiv](https://arxiv.org/abs/2301.12097)] [[cool](https://papers.cool/arxiv/2301.12097)] [[pdf](https://arxiv.org/pdf/2301.12097)]
> **Authors**: Hongyu Zhou,Xin Zhou,Lingzi Zhang,Zhiqi Shen
> **First submission**: 2023-01-28
> **First announcement**: 2023-01-30
> **comment**: 17 pages, 3 figures
- **标题**: 使用均匀图增强二元关系以进行多模式推荐
- **领域**: 信息检索,多媒体
- **摘要**: 推荐系统中的用户交互数据是一种二元关系形式，它反映了用户对项目的偏好。学习这两套离散对象，用户和项目的表示对于建议至关重要。最近证明利用多模式特征（例如，图像和文本描述）的多模式建议模型有效地提高了建议准确性。但是，最新的模型通过考虑用户使用或项目项目关系来增强用户与项目之间的二元关系，从而使另一侧的高阶关系（即用户或项目）未经探索。此外，我们通过实验表明，最先进的模型中当前的多模式融合方法可能会降低其建议性能。也就是说，在不包装模型体系结构的情况下，这些模型可以通过Uni-Modal信息获得更好的推荐准确性。最重要的是，我们提出了一个模型，通过构造均匀图来以多模式建议来学习用户和项目的双重表示来增强二元关系。我们将模特称为龙。具体而言，Dragon基于常见相互作用的项目和项目多模式特征的项目 - 项目图构建用户使用图。然后，它在用户项目异质图和均质图（用户用户和项目项目）上使用图形学习来获取用户和项目的双重表示。为了捕获每种模式的信息，Dragon采用一种简单而有效的融合方法，即专注的串联来得出用户和项目的表示。在三个公共数据集和七个基线上进行的广泛实验表明，龙的表现平均可以超过最强的基线22.03％。对龙进行了各种消融研究，以验证其有效性。

## 机器学习(cs.LG:Machine Learning)

该领域共有 18 篇论文

### Multimodal Sequential Generative Models for Semi-Supervised Language Instruction Following 
[[arxiv](https://arxiv.org/abs/2301.00676)] [[cool](https://papers.cool/arxiv/2301.00676)] [[pdf](https://arxiv.org/pdf/2301.00676)]
> **Authors**: Kei Akuzawa,Yusuke Iwasawa,Yutaka Matsuo
> **First submission**: 2022-12-28
> **First announcement**: 2023-01-03
> **comment**: No comments
- **标题**: 半监督语言指令的多模式顺序生成模型以下
- **领域**: 机器学习,人工智能,计算语言学
- **摘要**: 可以遵循语言说明的代理人在诸如导航等各种情况下将很有用。但是，培训基于神经网络的代理需要大量的配对轨迹和语言。本文提议在以下任务中使用多模式生成模型进行半监督学习。模型学习了配对数据的共享表示，并通过通过表示形式重建未配合的数据来启用半监督学习。将模型应用于序列到序列任务的关键挑战，包括以下指令，是学习可变长度的mulitimodal数据的共享表示并结合了注意机制。为了解决问题，本文提出了一种新型的网络体系结构，以吸收多模式数据序列长度的差异。此外，为了进一步提高性能，本文展示了如何将基于生成模型的方法与现有的半监督方法合并为称为扬声器的乘客模型，并提出了一个正则化术语，该术语可以使用未配对的轨迹来改善推理。在Babyai和房间（R2R）环境上进行的实验表明，提出的方法通过利用不配对数据来改善教学的性能，并在R2R中提高了2 \％至4 \％的扬声器追随者模型的性能。

### MessageNet: Message Classification using Natural Language Processing and Meta-data 
[[arxiv](https://arxiv.org/abs/2301.01808)] [[cool](https://papers.cool/arxiv/2301.01808)] [[pdf](https://arxiv.org/pdf/2301.01808)]
> **Authors**: Adar Kahana,Oren Elisha
> **First submission**: 2023-01-04
> **First announcement**: 2023-01-05
> **comment**: No comments
- **标题**: Messagenet：使用自然语言处理和元数​​据的消息分类
- **领域**: 机器学习,计算语言学
- **摘要**: 在本文中，我们提出了一种新的深度学习方法（DL），以进行消息分类。我们的方法基于最先进的自然语言处理（NLP）构建块，结合了一种新颖的技术，用于注入元数据输入，该技术通常在消息中，例如发件人信息，时间戳，随附图像，音频，隶属关系等。正如我们在整篇论文中所展示的那样，通过利用消息中的所有可用渠道超越单纯的文本，可以提高表示的表示和更高的分类精度。为了实现消息表示，每种类型的输入都在适合数据类型的神经网络体系结构中的专用块中处理。这样的实现使所有训练都可以同时培训所有块，并在网络中形成跨频道功能。我们在实验部分中显示，在某些情况下，Message的Meta-Data拥有一个仅从文本中提取的其他信息，并且在使用此信息时，我们实现了更好的性能。此外，我们证明我们的多模式块方法优于将元数据注入文本分类器的其他方法。

### Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching 
[[arxiv](https://arxiv.org/abs/2301.02903)] [[cool](https://papers.cool/arxiv/2301.02903)] [[pdf](https://arxiv.org/pdf/2301.02903)]
> **Authors**: Byoungjip Kim,Sungik Choi,Dasol Hwang,Moontae Lee,Honglak Lee
> **First submission**: 2023-01-07
> **First announcement**: 2023-01-09
> **comment**: 20 pages, 10 figures, NeurIPS 2022
- **标题**: 转移具有跨模式相似性匹配的预训练的多模式表示
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 尽管在零射传输上的性能令人惊讶，但预训练大规模多模型通常会令人难以置信，因为它需要大量的数据和计算资源。在本文中，我们提出了一种方法（BeamClip），该方法可以有效地将大型预训练的多模式模型（夹子VIT）的表示形式转移到一个小目标模型（例如RESNET-18）中。对于无监督的转移，我们介绍了跨模式相似性匹配（CSM），该匹配（CSM）使学生模型可以通过匹配文本提示嵌入的相对相似性分布来学习教师模型的表示。为了更好地编码文本提示，我们设计了基于上下文的提示增强（CPA），可以减轻输入文本提示的词汇歧义。我们的实验表明，预先训练的视力语言模型的无监督表示转移使一个小的RESNET-18可以实现更好的Imagenet-1K Top-1 top-1线性探针准确性（66.2％），而不是视觉自我监督的学习（ssl）方法（例如，SIMCLR：51.8％，升级为63.7％），在封闭的过程中（simclr：simclr：51.8％）。

### Data Distillation: A Survey 
[[arxiv](https://arxiv.org/abs/2301.04272)] [[cool](https://papers.cool/arxiv/2301.04272)] [[pdf](https://arxiv.org/pdf/2301.04272)]
> **Authors**: Noveen Sachdeva,Julian McAuley
> **First submission**: 2023-01-10
> **First announcement**: 2023-01-11
> **comment**: Accepted at TMLR '23. 21 pages, 4 figures
- **标题**: 数据蒸馏：调查
- **领域**: 机器学习,计算机视觉和模式识别,信息检索
- **摘要**: 深度学习的普及导致了大量大量和多种数据集的策划。尽管在单个任务上具有接近人类的性能，但大型数据集中的培训参数模型仍带来多方面的问题，例如（a）高模型训练时间； （b）缓慢的研究迭代； （c）生态可持续性差。作为替代方案，数据蒸馏方法旨在综合TERSE数据摘要，这些方法可以作为原始数据集的有效置换式替换，以用于模型培训，推理，架构搜索等场景。在此调查中，我们提供了一个正式的数据蒸馏框架，并提供了现有方法的详细分类量。此外，我们涵盖了不同数据模式的数据蒸馏方法，即图像，图形和用户项目交互（推荐系统），同时还确定了当前的挑战和未来的研究方向。

### Drug Synergistic Combinations Predictions via Large-Scale Pre-Training and Graph Structure Learning 
[[arxiv](https://arxiv.org/abs/2301.05931)] [[cool](https://papers.cool/arxiv/2301.05931)] [[pdf](https://arxiv.org/pdf/2301.05931)]
> **Authors**: Zhihang Hu,Qinze Yu,Yucheng Guo,Taifeng Wang,Irwin King,Xin Gao,Le Song,Yu Li
> **First submission**: 2023-01-14
> **First announcement**: 2023-01-16
> **comment**: No comments
- **标题**: 药物协同组合通过大规模培训和图形结构学习预测
- **领域**: 机器学习,定量方法
- **摘要**: 药物组合疗法是一种良好的疾病治疗策略，具有更好的有效性和更少的安全性降解策略。但是，由于庞大的组合搜索空间，通过湿LAB实验鉴定新的药物组合是资源密集的。最近，计算方法，特别是深度学习模型已成为发现协同组合的有效方法。尽管以前的方法报告了公平的性能，但它们的模型通常不会利用多模式数据，也无法处理新药或细胞系。在这项研究中，我们从涵盖各种与药物相关方面的各个数据集中收集了数据。然后，我们利用大规模的预训练模型来生成药物，蛋白质和疾病的信息表示和特征。基于此，在顶部构建了一个通讯图，以传播信息以及图形结构学习灵活性。这首先是在生物网络中引入的，使我们能够在图中产生伪关系。我们的框架获得了最新的框架，与其他基于深度学习的方法相比，有关协同预测基准数据集的其他深度方法。我们还能够在针对阿斯利康发布的独立集的测试中推断新药组合数据，其中观察到了10％的改进对以前的方法。此外，与第二好的模型相比，我们对看不见的药物进行了强大的态度，并且超过了近15％的AU ROC。我们认为，我们的框架既有助于未来的湿lab发现新型药物，也有助于建立有前途的精确组合医学指南。

### Causal conditional hidden Markov model for multimodal traffic prediction 
[[arxiv](https://arxiv.org/abs/2301.08249)] [[cool](https://papers.cool/arxiv/2301.08249)] [[pdf](https://arxiv.org/pdf/2301.08249)]
> **Authors**: Yu Zhao,Pan Deng,Junting Liu,Xiaofeng Jia,Mulan Wang
> **First submission**: 2023-01-18
> **First announcement**: 2023-01-19
> **comment**: 8 pages, 5 figures
- **标题**: 多模式流量预测的因果条件隐藏的马尔可夫模型
- **领域**: 机器学习
- **摘要**: 多模式的交通流量可以反映运输系统的健康状况，其预测对于城市交通管理至关重要。最近的作品过分强调了交通流量的时空相关性，而忽略了导致观测产生及其因果关系的物理概念。在不同条件的影响下，时空相关性被认为是不稳定的，并且在观测中可能存在虚假的相关性。在本文中，我们从观察生成原理的角度分析了影响多模层交通流的生成的物理概念，并提出了因果关系有条件的隐藏马尔可夫模型（CCHMM），以预测多模式的交通流。在潜在变量推理阶段，后验网络从条件信息和观察值中解散了感兴趣的概念的因果表示，而因果传播模块将其因果关系挖掘出来。在数据生成阶段，先前的网络将因果潜在变量从先前的分布进行了示例，并将其馈送到发电机中以生成多模式流量流。我们使用相互监督的训练方法为先验和后部，以增强模型的可识别性。现实世界中数据集的实验表明，CCHMM可以有效地解散感兴趣概念并确定因果关系的因果关系，并准确预测多模式的交通流。

### Fully Elman Neural Network: A Novel Deep Recurrent Neural Network Optimized by an Improved Harris Hawks Algorithm for Classification of Pulmonary Arterial Wedge Pressure 
[[arxiv](https://arxiv.org/abs/2301.07710)] [[cool](https://papers.cool/arxiv/2301.07710)] [[pdf](https://arxiv.org/pdf/2301.07710)]
> **Authors**: Masoud Fetanat,Michael Stevens,Pankaj Jain,Christopher Hayward,Erik Meijering,Nigel H. Lovell
> **First submission**: 2023-01-16
> **First announcement**: 2023-01-19
> **comment**: ef:IEEE Transactions on Biomedical Engineering, 2022
- **标题**: 完全Elman神经网络：一种新型的深层复发性神经网络，通过改进的Harris Hawks算法优化用于肺动脉楔压的分类
- **领域**: 机器学习,神经和进化计算,信号处理
- **摘要**: 心力衰竭（HF）是最普遍的威胁生命的心血管疾病之一，在美国，有650万人遭受了650万人的痛苦，在全球范围内遭受了2300万人的痛苦。可以通过将左心室辅助装置（LVAD）植入HF患者作为移植，恢复或目的地治疗的桥梁来实现HF患者的机械循环支持，并且可以通过测量正常和异常的肺动脉楔压力（PAWP）来控制。尽管没有商业的长期植入压力传感器来测量PAWP，但异常和正常PAWP的实时非侵入性估计至关重要。在这项工作中，首先提出并在24个单峰和多模式基准函数上提出并测试了一种称为HHO+的改进的HARRIS HARRIS优化器算法。其次，提出了一种新颖的Elman神经网络（FENN）来提高分类性能。最后，具有多层感知器（CNN-MLP），具有Elman神经网络（CNN-ENN）的CNN，具有Elman Neural Networks（CNN-FENN）的CNN，具有多层感知器（CNN-MLP）的四种新型18层深度学习方法（CNN），具有Elman神经网络（CNN-enn），以及与Elman Nevernets+ Elman Neural Networns+ Algn-fimne+ ALGREN（CNN-nnn）（CNN-enn）（CNN-enn）。为了使用估计的HVAD泵流进行异常和正常PAWP分类，并进行了比较。估计的泵流是通过嵌入商业HVAD控制器中的非侵入性方法得出的。使用5倍交叉验证在不平衡的临床数据集上评估所提出的方法。提出的CNN-FENN-HHO+方法优于提议的CNN-MLP，CNN-ENN和CNN-FENN方法，并改善了5倍交叉验证的分类性能指标。提出的方法可以减少HF患者的危险事件的可能性，例如肺部充血和心室吸力，并通知已确定的医院，临床医生和心脏病专家的异常病例。

### Synthcity: facilitating innovative use cases of synthetic data in different data modalities 
[[arxiv](https://arxiv.org/abs/2301.07573)] [[cool](https://papers.cool/arxiv/2301.07573)] [[pdf](https://arxiv.org/pdf/2301.07573)]
> **Authors**: Zhaozhi Qian,Bogdan-Constantin Cebere,Mihaela van der Schaar
> **First submission**: 2023-01-18
> **First announcement**: 2023-01-19
> **comment**: No comments
- **标题**: 合成：促进不同数据方式中合成数据的创新用例
- **领域**: 机器学习,人工智能
- **摘要**: Synthcity是一个开源软件包，用于在ML公平，隐私和增强中的创新用例中，包括静态数据模式，包括静态数据，常规和不规则时间序列，具有审查，多源数据，多源数据，复合数据等的数据。合成学为从业者提供了一个尖端研究和合成数据工具的单个访问点。它还为社区提供了一个快速实验和原型制作的操场，用于SOTA基准测试的一站式商店以及扩展研究影响的机会。可以在GitHub（https://github.com/vanderschaarlab/synthcity）和PIP（https://pypi.org/proget/project/synthcity/）上访问该库。我们热烈邀请社区通过提供反馈，报告错误和贡献代码来加入发展工作。

### AutoFraudNet: A Multimodal Network to Detect Fraud in the Auto Insurance Industry 
[[arxiv](https://arxiv.org/abs/2301.07526)] [[cool](https://papers.cool/arxiv/2301.07526)] [[pdf](https://arxiv.org/pdf/2301.07526)]
> **Authors**: Azin Asgarian,Rohit Saha,Daniel Jakubovitz,Julia Peyre
> **First submission**: 2023-01-15
> **First announcement**: 2023-01-19
> **comment**: Published at The AAAI-2023 Workshop OnMultimodalAI For Financial Forecasting
- **标题**: Autofraudnet：一个多模式网络，可检测汽车保险行业的欺诈
- **领域**: 机器学习
- **摘要**: 在保险业中，检测欺诈性主张是一项至关重要的任务，具有重大的财务影响。确定欺诈性主张的一种常见策略是在支持证据中寻找不一致之处。但是，对于人类专家来说，这是一项费力的繁重的任务，因为保险索赔通常带有来自不同方式（例如图像，文本和元数据）的大量数据。为了克服这一挑战，研究社区专注于多模式机器学习框架，这些框架可以通过多个数据源有效地推理。尽管多模式学习最近取得了进步，但这些框架仍然遭受（i）由不同模式的不同特征和（ii）由于高模型复杂性而导致的过度拟合趋势引起的联合训练的挑战。在这项工作中，我们通过引入多模式推理框架Autofraudnet（汽车保险欺诈检测网络）来解决这些挑战，以检测欺诈性的自动保险要求。 Autofraudnet利用了级联的慢速融合框架和最先进的融合块塔克（Tucker），以减轻联合培训的挑战。此外，它结合了轻巧的体系结构设计，以及额外的损失，以防止过度拟合。通过在现实世界数据集上进行的广泛实验，我们证明了：（i）与单峰和双峰方法相比，多模式方法的优点，以及（ii）自动方向在融合各种模态以增强性能的有效性（超过3 \％\％\％）。

### Multimodal Side-Tuning for Document Classification 
[[arxiv](https://arxiv.org/abs/2301.07502)] [[cool](https://papers.cool/arxiv/2301.07502)] [[pdf](https://arxiv.org/pdf/2301.07502)]
> **Authors**: Stefano Pio Zingaro,Giuseppe Lisanti,Maurizio Gabbrielli
> **First submission**: 2023-01-16
> **First announcement**: 2023-01-19
> **comment**: 2020 25th International Conference on Pattern Recognition (ICPR)
- **标题**: 用于文档分类的多模式侧调整
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 在本文中，我们建议利用多模式文档分类的侧调框架。侧调是最近引入网络适应的一种方法，以解决与以前的方法相关的一些问题。通过这种技术，实际上可以通过微调来克服模型的刚度和灾难性忘记转移学习。所提出的解决方案使用现成的深度学习体系结构利用侧面调整框架将基本模型与两个侧网的串联结合在一起。我们表明，当考虑不同的数据源时，也可以成功使用侧调。文档分类中的文本和图像。实验结果表明，这种方法进一步推动了文档分类准确性相对于艺术的状态的限制。

### AQuaMaM: An Autoregressive, Quaternion Manifold Model for Rapidly Estimating Complex SO(3) Distributions 
[[arxiv](https://arxiv.org/abs/2301.08838)] [[cool](https://papers.cool/arxiv/2301.08838)] [[pdf](https://arxiv.org/pdf/2301.08838)]
> **Authors**: Michael A. Alcorn
> **First submission**: 2023-01-20
> **First announcement**: 2023-01-23
> **comment**: No comments
- **标题**: Aquaman：快速估计复合物的自回旋，四歧管歧管模型SO（3）分布
- **领域**: 机器学习
- **摘要**: 准确地建模复合物，多模式分布对于最佳决策是必要的，但是对于三维旋转（即SO（3）组）的旋转是必不可少的，这是由于旋转歧管的曲率而具有挑战性的。最近描述的隐式PDF（IPDF）是一种简单，优雅且有效的方法，用于在SO（3）上学习任意分布，直到给定的精度。但是，使用IPDF的推论需要$ n $ forwith通过该网络的最终多层perceptron（其中$ n $在模型可以计算的可能性上放置了上限），这对于那些没有计算资源的人来说，这是非常慢的，而没有计算资源可以使疑问并行化。在本文中，我介绍了Aquamam，这是一个能够在旋转歧管上学习复杂分布的神经网络，并在单个正向通道中计算查询旋转的精确可能性。具体而言，aquamam自动调查对单位四季度的投影组成部分建模为均匀分布的混合物，这些分布将其几何限制的值分开。当在具有模棱两可的观点的“无限”玩具数据集上接受培训时，Aquamam迅速收敛到采样分布，与真实的数据分布非常匹配。相比之下，尽管IPDF接近训练期间的理论最低评估损失，但IPDF的采样分布与真实数据分布显着分歧。当在不同旋转的500,000个模具渲染的构建数据集上接受培训时，Aquamam的测试日志可能比IPDF高14％。此外，与IPDF相比，Aquamam使用的参数少24％，预测吞吐量52 $ \ times $在单个GPU上的速度更快，并且在培训期间的收敛时间类似。

### Mixed Effects Random Forests for Personalised Predictions of Clinical Depression Severity 
[[arxiv](https://arxiv.org/abs/2301.09815)] [[cool](https://papers.cool/arxiv/2301.09815)] [[pdf](https://arxiv.org/pdf/2301.09815)]
> **Authors**: Robert A. Lewis,Asma Ghandeharioun,Szymon Fedor,Paola Pedrelli,Rosalind Picard,David Mischoulon
> **First submission**: 2023-01-23
> **First announcement**: 2023-01-24
> **comment**: 9 pages
- **标题**: 混合效应随机森林，用于个性化临床抑郁严重程度的个性化预测
- **领域**: 机器学习,应用领域
- **摘要**: 这项工作表明，使用多模式生理和数字活动数据从8周的研究中收集的多模式生理和数字活动数据涉及31例重度抑郁症患者，如何实现混合效应随机森林如何对抑郁严重程度进行准确的预测。我们表明，混合效应随机森林在预测汉密尔顿抑郁评分量表评分时，随机森林的表现优于标准森林和个人平均基线（HDRS_17）。与后一个基线相比，每位患者的准确性平均提高了0.199-0.276的平均绝对误差（p <0.05）。这是值得注意的，因为这些简单的基线在心理健康预测任务中经常优于机器学习方法。我们建议，这种改善的性能是由混合效应随机森林对数据集中个体个性化模型参数个性化的能力的。但是，我们发现这些改进仅与训练时模型可用于标记的患者数据的方案有关。调查在推广到新患者时提高准确性的方法是重要的未来工作。

### Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content 
[[arxiv](https://arxiv.org/abs/2301.10871)] [[cool](https://papers.cool/arxiv/2301.10871)] [[pdf](https://arxiv.org/pdf/2301.10871)]
> **Authors**: Liam Hebert,Hong Yi Chen,Robin Cohen,Lukasz Golab
> **First submission**: 2023-01-25
> **First announcement**: 2023-01-26
> **comment**: Accepted at AAAI 2023 AI for Social Good
- **标题**: 图形变压器方法的定性分析来解决仇恨言论：适应动态变化的内容
- **领域**: 机器学习,计算语言学,社交和信息网络
- **摘要**: 我们的工作推进了一种预测社交媒体中仇恨言论的方法，提出了迫切需要考虑在帖子下进行讨论，以成功地检测何时出现仇恨话语。使用Graph Transformer网络，再加上建模注意力和BERT级自然语言处理，我们的方法可以捕获上下文并预测即将到来的反社会行为。在本文中，我们对这种解决方案进行了详细的定性分析，以在社交网络中进行仇恨言论检测，从而洞悉该方法与竞争对手相比，该方法具有最令人印象深刻的结果，并确定了实现理想表现面临挑战的场景。其中包括探索当今渗透社交媒体的各种帖子，包括使用仇恨图像。这表明将我们的模型扩展到更全面的途径。一个关键的见解是，关注对上下文概念的推理我们很好地支持在线帖子的多模式分析。最后，我们对我们解决的问题的反思尤其与动态变化的主题特别相关，这是所有对社会影响的AI解决方案的关键问题。我们还简要评论了我们的工作如何通过策划的内容来提高心理健康状况，以达到帖子的仇恨程度。

### Coincident Learning for Unsupervised Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2301.11368)] [[cool](https://papers.cool/arxiv/2301.11368)] [[pdf](https://arxiv.org/pdf/2301.11368)]
> **Authors**: Ryan Humble,Zhe Zhang,Finn O'Shea,Eric Darve,Daniel Ratner
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: No comments
- **标题**: 无监督异常检测的一致学习
- **领域**: 机器学习
- **摘要**: 异常检测是复杂系统（例如工业设施，制造业，大规模科学实验）的重要任务，在该系统中，子系统中的故障可能会导致较低的产量，有缺陷的产品甚至对组件的损害。尽管复杂的系统通常具有大量数据，但标记为异常通常很少（甚至不存在），并且获取昂贵。因此，无监督的方法是常见的，通常通过输入特征空间（或某些相关的低维表示）中的示例密度或密度搜索异常。本文提出了一种名为COAD的新颖方法，该方法是专门为多模式任务设计的，并根据\ textit {Cocincention}行为识别异常。我们定义一个\ textIt {无监督的}公制，$ \ hat {f}_β$，出于与监督分类的类比$f_β$统计。 COAD使用$ \ hat {f}_β$在\ textit {未标记的数据}上训练异常检测算法，这是基于一种期望，即一种特征切片中的异常行为与另一种特征的异常行为一致。使用合成的离群数据集和基于MNIST的图像数据集说明了该方法，并将其与两个现实世界任务的先验最新任务进行了比较：金属铣削数据集和来自粒子加速器的数据集。

### Rigid Body Flows for Sampling Molecular Crystal Structures 
[[arxiv](https://arxiv.org/abs/2301.11355)] [[cool](https://papers.cool/arxiv/2301.11355)] [[pdf](https://arxiv.org/pdf/2301.11355)]
> **Authors**: Jonas Köhler,Michele Invernizzi,Pim de Haan,Frank Noé
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: International Conference on Machine Learning, 2023
- **标题**: 刚体流动以抽样分子晶体结构
- **领域**: 机器学习,化学物理,计算物理,机器学习
- **摘要**: 标准化流量（NF）是一类强大的生成模型，由于它们能够以高灵活性和表现力对复杂分布进行建模，因此近年来已经获得了知名度。在这项工作中，我们引入了一种新型的归一化流量，该流量是针对三维空间中多个对象（例如晶体中的分子）建模的位置和方向定制的。我们的方法基于两个关键思想：首先，我们在单元四季度对平稳而表现的流动定义，这使我们能够捕获刚体的连续旋转运动；其次，我们使用单位四元组的双层覆盖属性来定义旋转组的适当密度。这样可以确保可以使用基于标准似然的方法或相对于热力学目标密度来训练我们的模型。我们通过训练Boltzmann发电机的两个分子示例来评估该方法，即外部场中四面体系统的多模式密度和TIP4P水模型中的ICE XI相。我们的流可以与在分子自由度上运行的流动结合在一起，并构成了许多相互作用分子分布建模的重要一步。

### Estimating Causal Effects using a Multi-task Deep Ensemble 
[[arxiv](https://arxiv.org/abs/2301.11351)] [[cool](https://papers.cool/arxiv/2301.11351)] [[pdf](https://arxiv.org/pdf/2301.11351)]
> **Authors**: Ziyang Jiang,Zhuoran Hou,Yiling Liu,Yiman Ren,Keyu Li,David Carlson
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: 18 pages, 7 figures, 3 tables, published at the 40th International Conference on Machine Learning (ICML 2023)
- **标题**: 使用多任务深度合奏估算因果效应
- **领域**: 机器学习,机器学习
- **摘要**: 已经提出了许多用于因果效应估计的方法，但很少有人在处理具有复杂结构（例如图像）的数据方面证明了功效。为了填补这一空白，我们提出了因果多任务深度集合（CMDE），这是一个新型框架，从研究人群中学习共享和特定于群体的信息。我们提供了证明CDME与具有核心区域内核先验的多任务高斯过程（GP）的等效性的证据。与多任务GP相比，CMDE有效地处理高维和多模式协变量，并提供因果效应的点不确定性估计。我们在各种类型的数据集和任务上评估了我们的方法，并发现在大多数这些任务上，CMDE优于最先进的方法。

### Global Flood Prediction: a Multimodal Machine Learning Approach 
[[arxiv](https://arxiv.org/abs/2301.12548)] [[cool](https://papers.cool/arxiv/2301.12548)] [[pdf](https://arxiv.org/pdf/2301.12548)]
> **Authors**: Cynthia Zeng,Dimitris Bertsimas
> **First submission**: 2023-01-29
> **First announcement**: 2023-01-30
> **comment**: 6 pages
- **标题**: 全球洪水预测：一种多模式的机器学习方法
- **领域**: 机器学习,计算机与社会
- **摘要**: 洪水是最具破坏性和昂贵的自然灾害之一，气候变化将在全球范围内进一步增加风险。这项工作为多年全球洪水风险预测提供了一种新颖的多模式学习方法，结合了地理信息和历史自然灾害数据集。我们的多模式框架采用最新的处理技术来从每种数据模式中提取嵌入，包括基于文本的地理数据和基于表格的时间序列数据。实验表明，一种组合文本和统计数据的多模式方法优于单模式方法。我们最先进的体系结构采用了在Distilbert模型上使用转移学习提取的嵌入，以75 \％-77 \％Rocauc得分在预测历史上洪水泛滥的地点的下一个1  -  5年洪水事件中。这项工作证明了在自然灾害管理中使用机器学习进行长期计划的潜力。

### Massively Scaling Heteroscedastic Classifiers 
[[arxiv](https://arxiv.org/abs/2301.12860)] [[cool](https://papers.cool/arxiv/2301.12860)] [[pdf](https://arxiv.org/pdf/2301.12860)]
> **Authors**: Mark Collier,Rodolphe Jenatton,Basil Mustafa,Neil Houlsby,Jesse Berent,Effrosyni Kokiopoulou
> **First submission**: 2023-01-30
> **First announcement**: 2023-01-31
> **comment**: Accepted to ICLR 2023
- **标题**: 大规模缩放异质分类器
- **领域**: 机器学习,机器学习
- **摘要**: 在预测逻辑上学习多元高斯分布的异性分类器已被证明在图像分类问题上表现良好。但是，与标准分类器相比，它们引入了额外的参数，以与类数量线性扩展。这使得它们不可避免地适用于大规模的问题。另外，异质分类器引入了必须调整的临界温度超参数。我们提出了HET-XL，这是一种异质分类器，其参数计数与标准分类器相比与类别的类别相比。在我们的大规模设置中，我们表明我们可以通过直接在训练数据上学习来消除温度超参数调整温度超参数的需求。在具有最多4B图像和30k类的大图像分类数据集上，我们的方法需要少14倍的附加参数，不需要调整固定集合上的温度，并且比基线异质分类器分类器始终如一地执行。 HET-XL在多模式对比学习设置中改进Imagenet 0-shot分类，该设置可以看作是35亿个班级分类问题。

## 多媒体(cs.MM:Multimedia)

该领域共有 4 篇论文

### Depression Diagnosis and Analysis via Multimodal Multi-order Factor Fusion 
[[arxiv](https://arxiv.org/abs/2301.00254)] [[cool](https://papers.cool/arxiv/2301.00254)] [[pdf](https://arxiv.org/pdf/2301.00254)]
> **Authors**: Chengbo Yuan,Qianhui Xu,Yong Luo
> **First submission**: 2022-12-31
> **First announcement**: 2023-01-03
> **comment**: No comments
- **标题**: 通过多模式多阶因子融合的抑郁诊断和分析
- **领域**: 多媒体,人工智能,计算机视觉和模式识别
- **摘要**: 抑郁症是全世界死亡的主要原因，抑郁症的诊断是无聊的。多模式学习是自动诊断抑郁症的流行解决方案，现有作品遭受了两个主要缺点：1）不同方式之间的高阶相互作用不能很好地利用； 2）模型的解释性很弱。为了解决这些缺点，我们提出了一种多模式多阶因子融合（MMFF）方法。我们的方法可以通过在共享潜在代理的指导下提取和组装模态因子来很好地利用不同方式之间的高阶相互作用。我们对两个最近和流行的数据集进行了广泛的实验，E-DAIC-WOZ和CMDC，结果表明，与其他现有方法相比，我们的方法的性能明显更好。此外，通过分析因子组装过程，我们的模型可以直观地显示每个因素的贡献。这有助于我们理解融合机制。

### Ring That Bell: A Corpus and Method for Multimodal Metaphor Detection in Videos 
[[arxiv](https://arxiv.org/abs/2301.01134)] [[cool](https://papers.cool/arxiv/2301.01134)] [[pdf](https://arxiv.org/pdf/2301.01134)]
> **Authors**: Khalid Alnajjar,Mika Hämäläinen,Shuo Zhang
> **First submission**: 2022-12-15
> **First announcement**: 2023-01-04
> **comment**: Figlang 2022
- **标题**: 铃响：视频中多模式比喻检测的语料库和方法
- **领域**: 多媒体,计算语言学,计算机视觉和模式识别
- **摘要**: 我们提出了第一个公开可用的多模式隐喻注释语料库。该语料库由专家注释的视频组成，包括音频和字幕。此外，我们提出了一种根据视频的文本内容来检测新数据集中隐喻的方法。该方法可实现隐喻标签的高F1分数（62 \％）。我们还尝试了其他方式和多模式方法。但是，这些方法不超越基于文本的模型。在我们的错误分析中，我们确实确定在某些情况下，视频可以帮助消除隐喻，但是，视觉提示对于我们的模型来说太微妙了，无法捕获。数据可在Zenodo上找到。

### OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset 
[[arxiv](https://arxiv.org/abs/2301.06375)] [[cool](https://papers.cool/arxiv/2301.06375)] [[pdf](https://arxiv.org/pdf/2301.06375)]
> **Authors**: Jeongkyun Park,Jung-Wook Hwang,Kwanghee Choi,Seung-Hyun Lee,Jun Hwan Ahn,Rae-Hong Park,Hyung-Min Park
> **First submission**: 2023-01-16
> **First announcement**: 2023-01-17
> **comment**: No comments
- **标题**: Olkavs：开放的大型韩语音频语音数据集
- **领域**: 多媒体,人工智能,计算语言学,计算机视觉和模式识别,机器学习,声音
- **摘要**: 受到人类以多模式方式理解语音的启发，已经构建了各种视听数据集。但是，大多数现有的数据集都集中在英语上，在数据集准备过程中具有各种预测模型的依赖性，并且只有少量的多视频视频。为了减轻局限性，我们最近开发了开放的大型韩国视听语音（OLKAV）数据集，该数据集是公开可用的音频语音数据集中最大的。该数据集包含1,150个小时的抄录音频，其中来自1,107个韩国扬声器的录音室设置，具有9个不同的观点和各种噪音情况。我们还为两项任务提供了预训练的基线模型，即视听语音识别和唇部阅读。我们根据模型进行了实验，以验证单模式和仅额叶视图训练的多模式和多视图训练的有效性。我们希望Olkavs数据集将促进更广泛领域的多模式研究，例如韩国语音识别，说话者识别，发音水平分类和口腔运动分析。

### M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing System 
[[arxiv](https://arxiv.org/abs/2301.12831)] [[cool](https://papers.cool/arxiv/2301.12831)] [[pdf](https://arxiv.org/pdf/2301.12831)]
> **Authors**: Chenqi Kong,Kexin Zheng,Yibing Liu,Shiqi Wang,Anderson Rocha,Haoliang Li
> **First submission**: 2023-01-30
> **First announcement**: 2023-01-31
> **comment**: No comments
- **标题**: M3FAS：准确稳健的多式模式移动面反动
- **领域**: 多媒体,计算机视觉和模式识别
- **摘要**: 面部演示攻击（FPA），也称为面部欺骗，通过各种恶意申请（例如财务欺诈和隐私泄漏）引起了公众的越来越关注。因此，保护​​面部识别系统免受FPA至关重要。尽管现有的基于学习的面部反欺骗（FAS）模型可以实现出色的检测性能，但它们缺乏普遍的能力，并且在不可预见的环境中遭受了显着的性能下降。在演示攻击检测（PAD）期间，许多方法试图使用辅助模态数据（例如，深度和红外地图）来解决此限制。但是，这些方法可能会受到限制，因为（1）它们需要特定的传感器，例如深度和红外摄像机来捕获数据，这些传感器在商品移动设备上很少可用，并且（2）在缺失或质量较差的情况下，它们在实际情况下无法正常工作。在本文中，我们设计了一个名为M3FAS的准确且强大的多模式移动面抗散热系统，以克服上述问题。这项工作的主要创新在于以下方面：（1）为了实现强大的垫子，我们的系统使用三个常用的传感器结合了视觉和听觉方式：相机，扬声器和麦克风； （2）我们设计了一个具有三个分层特征聚合模块的新型两分支神经网络，以执行跨模式融合； （3）。我们提出了一种多头训练策略，允许该模型从视力，声学和融合头输出预测，从而产生更灵活的垫子。广泛的实验证明了M3FA在各种具有挑战性的实验环境下的准确性，鲁棒性和灵活性。源代码和数据集可在以下网址提供：https：//github.com/chenqikong/m3fas/

## 机器人技术(cs.RO:Robotics)

该领域共有 2 篇论文

### Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments 
[[arxiv](https://arxiv.org/abs/2301.04195)] [[cool](https://papers.cool/arxiv/2301.04195)] [[pdf](https://arxiv.org/pdf/2301.04195)]
> **Authors**: Mayank Mittal,Calvin Yu,Qinxi Yu,Jingzhou Liu,Nikita Rudin,David Hoeller,Jia Lin Yuan,Ritvik Singh,Yunrong Guo,Hammad Mazhar,Ajay Mandlekar,Buck Babich,Gavriel State,Marco Hutter,Animesh Garg
> **First submission**: 2023-01-10
> **First announcement**: 2023-01-11
> **comment**: Project website: https://isaac-orbit.github.io/
- **标题**: 轨道：交互式机器人学习环境的统一模拟框架
- **领域**: 机器人技术,人工智能
- **摘要**: 我们提出了轨道，这是由Nvidia Isaac Sim提供支持的机器人学习的统一和模块化框架。它提供了模块化设计，可轻松有效地创建带有照片现实的场景以及高保真刚性和可变形的身体模拟的机器人环境。借助轨道，我们提供了一套基准的难度的基准任务 - 从单级橱柜开口和布料折叠到多阶段的任务，例如房间重组。为了支持使用不同的观察和动作空间，我们包括具有不同物理传感器和运动发生器的固定臂和移动操纵器。轨道允许培训加强学习政策，并通过利用基于GPU的并行化来从手工制作或专家解决方案中收集大型演示数据集。总而言之，我们提供了一个开源框架，该框架很容易配备16个机器人平台，4个传感器模式，10个运动生成器，20多个基准任务以及4个学习库的包装器。通过此框架，我们旨在支持各个研究领域，包括代表性学习，强化学习，模仿学习以及任务和运动计划。我们希望它有助于在这些社区建立跨学科的合作，并且它的模块化使得将来可以轻松扩展到更多的任务和应用程序。

### Controlling Steering with Energy-Based Models 
[[arxiv](https://arxiv.org/abs/2301.12264)] [[cool](https://papers.cool/arxiv/2301.12264)] [[pdf](https://arxiv.org/pdf/2301.12264)]
> **Authors**: Mikita Balesni,Ardi Tampuu,Tambet Matiisen
> **First submission**: 2023-01-28
> **First announcement**: 2023-01-30
> **comment**: No comments
- **标题**: 用基于能量的模型控制转向
- **领域**: 机器人技术,人工智能,机器学习
- **摘要**: 所谓的与基于能量的模型的隐式行为克隆在机器人操纵任务中显示出令人鼓舞的结果。我们测试了该方法的优势是否可以通过端到端驾驶模型来控制真正的自动驾驶汽车的转向。我们对隐式行为克隆方法与明确的基线方法进行了广泛的比较，所有方法都共享了相同的神经网络骨干架构。对基线显式模型进行了回归（MAE）损失，分类损失（SoftMax和跨熵的离散化）或作为混合密度网络（MDN）的训练。尽管使用基于能量的配方的模型在安全驱动器干预方面与基线方法相当进行，但它们具有更高的白度度量，表明更高的混蛋。为了减轻这一点，我们展示了两种可用于改善转向平稳性的方法。我们确认基于能量的模型的多模式比简单回归略好，但这并没有转化为明显更好的驾驶能力。我们认为，仅转向道路遵循的任务的多模式太少，无法从基于能量的模型中受益。这表明，将隐性行为克隆应用于现实世界任务可能具有挑战性，需要进一步研究以提出基于能量的模型的理论优势。

## 声音(cs.SD:Sound)

该领域共有 2 篇论文

### Multimodal Lyrics-Rhythm Matching 
[[arxiv](https://arxiv.org/abs/2301.02732)] [[cool](https://papers.cool/arxiv/2301.02732)] [[pdf](https://arxiv.org/pdf/2301.02732)]
> **Authors**: Callie C. Liao,Duoduo Liao,Jesse Guessford
> **First submission**: 2023-01-06
> **First announcement**: 2023-01-09
> **comment**: Accepted by 2022 IEEE International Conference on Big Data (IEEE Big Data 2022)
- **标题**: 多模式歌词节律匹配
- **领域**: 声音,机器学习,音频和语音处理
- **摘要**: 尽管最近对音乐人工智能的研究有所增加，但歌词和节奏的关键组成部分之间的突出相关性（例如关键字，压力的音节和强烈的节奏）并不经常研究。这可能是由于挑战，例如音频错位，音节识别中的不准确性以及最重要的是对跨学科知识的需求。为了解决这种缺乏研究，我们提出了一种新颖的多式联运歌词 - 节律匹配方法，在本文中，专门匹配歌词和音乐的关键组成部分，而没有任何语言限制。我们使用音频而不是乐谱音乐，并带有随时可用的元数据，这会带来更多的挑战，但可以提高我们方法的应用灵活性。此外，我们的方法创造性地生成了几种涉及各种多模式的模式，包括音乐强节拍，抒情音节，歌手发音中的听觉变化，尤其是抒情关键字，它们可用于将关键抒情元素与关键节奏元素匹配。这种有利的方法不仅提供了一种独特的方法来研究听觉歌词节律相关性，包括有效的基于节奏的音频一致性算法，而且还提供了与音乐和音乐认知相结合。我们的实验结果表明，平均匹配的概率为0.81，大约30％的歌曲的概率为0.9或更高的关键字降落在较强的节奏上，其中包括12％的歌曲，具有完美的着陆。同样，相似性指标用于评估歌词和节奏之间的相关性。它表明，近50％的歌曲具有0.70的相似性或更高的相似性。总之，我们的方法通过计算揭示有见地的相关性来对歌词节律关系产生重大贡献。

### Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models 
[[arxiv](https://arxiv.org/abs/2301.12661)] [[cool](https://papers.cool/arxiv/2301.12661)] [[pdf](https://arxiv.org/pdf/2301.12661)]
> **Authors**: Rongjie Huang,Jiawei Huang,Dongchao Yang,Yi Ren,Luping Liu,Mingze Li,Zhenhui Ye,Jinglin Liu,Xiang Yin,Zhou Zhao
> **First submission**: 2023-01-29
> **First announcement**: 2023-01-30
> **comment**: Audio samples are available at https://Text-to-Audio.github.io
- **标题**: Make-An-Audio：具有及时增强扩散模型的文本对审计
- **领域**: 声音,机器学习,多媒体,音频和语音处理
- **摘要**: 大规模的多模式生成建模在文本到图像和文本之间创造了里程碑。它在音频上的应用仍然落后于两个主要原因：缺乏具有高质量文本原理对的大型数据集，以及对长连续音频数据进行建模的复杂性。在这项工作中，我们通过迅速增强的扩散模型提出了制作，该模型通过1）通过蒸馏式进行蒸馏的方法来解决这些差距。 2）利用频谱图自动编码器预测自我监督的音频表示，而不是波形。加上强大的对比性语言审计（拍手）表示形式，Make-An-Audio可以实现最新的先进和主观基准评估。此外，我们介绍了其对X-toaudio的可控性和概括，并以“无遗物遗留方式”，这是第一次解锁给定用户定义的模态输入的能力。音频样本可从https://text-to-audio.github.io获得。

## 软件工程(cs.SE:Software Engineering)

该领域共有 1 篇论文

### An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation 
[[arxiv](https://arxiv.org/abs/2301.01224)] [[cool](https://papers.cool/arxiv/2301.01224)] [[pdf](https://arxiv.org/pdf/2301.01224)]
> **Authors**: Kevin Moran,Ali Yachnes,George Purnell,Junayed Mahmud,Michele Tufano,Carlos Bernal-Cárdenas,Denys Poshyvanyk,Zach H'Doubler
> **First submission**: 2023-01-03
> **First announcement**: 2023-01-04
> **comment**: Published in the Proceedings of the 29th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER'22), Honolulu, Hawaii, March 15-18, 2022, pp. 514-525
- **标题**: 对使用图像字幕用于自动软件文档的实证研究
- **领域**: 软件工程,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 用于软件文档的现有自动化技术通常会尝试在两个主要信息来源之间推理：代码和自然语言。但是，这种推理过程通常会因更抽象的自然语言和更结构化的编程语言之间的词汇差距而变得复杂。该差距的一个潜在桥梁是图形用户界面（GUI），因为GUI固有地编码了有关基础程序功能的显着信息，以计入富的基于像素的数据表示。本文对GUIS与软件的功能自然语言描述之间的联系进行了首次全面的实证研究之一。首先，我们收集，分析和开源一个功能性GUI描述的大型数据集，其中包括来自流行的Android应用程序的10,204个屏幕截图的45,998个描述。这些描述是从人类标记中获得的，并经历了多种质量控制机制。为了深入了解GUIS的代表性潜力，我们研究了四个神经图像字幕模型预测自然语言描述的能力，当提供屏幕快照作为输入时。我们使用通用的机器翻译指标定量评估这些模型，并通过大规模的用户研究定性地评估这些模型。最后，我们提供了学习的课程，并讨论了多模型模型所显示的潜力，以增强自动化软件文档的未来技术。

## 图像和视频处理(eess.IV:Image and Video Processing)

该领域共有 3 篇论文

### Morphology-based non-rigid registration of coronary computed tomography and intravascular images through virtual catheter path optimization 
[[arxiv](https://arxiv.org/abs/2301.00060)] [[cool](https://papers.cool/arxiv/2301.00060)] [[pdf](https://arxiv.org/pdf/2301.00060)]
> **Authors**: Karim Kadry,Abhishek Karmakar,Andreas Schuh,Kersten Peterson,Michiel Schaap,David Marlevi,Charles Taylor,Elazer Edelman,Farhad Nezami
> **First submission**: 2022-12-30
> **First announcement**: 2023-01-03
> **comment**: Accepted to IEEE Transactions in Medical Imaging
- **标题**: 通过虚拟导管路径优化的冠状动脉层析成像和血管内图像的基于形态的非刚性登记
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 冠状动脉断层扫描血管造影（CCTA）提供了有关阻塞性冠状动脉疾病的3D信息，但无法完全可视化血管壁内的高分辨率特征。相比之下，血管内成像可以在横截面切片中空间解析动脉粥样硬化，但在捕获每个切片之间的3D关系时受到限制。共同注册CCTA和血管内图像可以实现各种临床研究应用，但耗时且依赖用户。这是由于因成像导管路径中的不规则性而引起的非刚性畸变的血管内图像所致。为了解决这些问题，我们提出了一个基于形态学的框架，用于将血管内图像与CCTA图像的刚性和非刚性匹配。为此，我们发现了最佳的虚拟导管路径，该路径采样了CCTA图像空间中冠状动脉的冠状动脉，以概括在血管内图像中观察到的冠状动脉形态。我们使用分叉地标作为纵向和旋转注册的地面真理，在40名患者组成的多中心队列上验证我们的框架。我们的注册方法极大地优于其他方法进行分叉对齐。通过为多模式血管共同注册提供一个可区分的框架，我们的框架减少了进行大型多模式临床研究所需的手动工作，并能够开发基于机器的基于机器的共同注册方法。

### DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels 
[[arxiv](https://arxiv.org/abs/2301.08798)] [[cool](https://papers.cool/arxiv/2301.08798)] [[pdf](https://arxiv.org/pdf/2301.08798)]
> **Authors**: Yunan Wu,Amil Dravid,Ramsey Michael Wehbe,Aggelos K. Katsaggelos
> **First submission**: 2023-01-20
> **First announcement**: 2023-01-23
> **comment**: No comments
- **标题**: DeepCovid-Fuse：多模式的深度学习模型融合了胸部X-radiographs和临床变量，以预测COVID-19的风险水平
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 建议：介绍DeepCovid-Fuse，这是一个深度学习融合模型，可预测确认的冠状病毒疾病2019（COVID-19）患者的风险水平，并评估预先训练的融合模型在胸部X射线（CXRS）或胸部X线X射线X射线射线射线射线射线X射线的全部或部分组合上的性能。材料和方法：从2020年2月至2020年4月收集了最初的CXR，临床变量和结果（即死亡率，插管，住院时间，ICU入院），并通过反向转录聚合酶链链（RT-PCR）测试结果作为参考标准。风险水平取决于结果。 Fusion模型接受了1657名患者（年龄：58.30 +/- 17.74;女性：807）的培训，并对西北纪念医疗保健系统的428例患者（56.41 +/- 17.03; 190）进行了验证，并接受了439名患者（56.51 +/- 17.78; 205; 205; 205; 205; 205; 205; 205; 205; 205;使用接收器操作特征曲线（AUC）和McNemar测试的DELONG测试在测试集上比较了预训练的融合模型的性能，以进行准确性，精度，召回和F1。结果：在CXR和临床变量上训练的深卵形融合的准确性为0.658，AUC为0.842，其表现明显优于（p <0.05）模型（p <0.05）模型，仅在CXR上训练，精度为0.621，AUC为0.807，并且仅在0.807中，仅在0.44440和0.50和AUC上进行临床变量。仅具有CXRS的预训练融合模型，因为输入将精度提高到0.632，而AUC的精度提高到0.813，并且仅随着临床变量，随着输入的精度将精度提高到0.539，AUC提高到0.733。结论：融合模型在训练过程中学习了跨不同方式的更好的特征表示，即使仅在测试中使用了某些模式，也可以实现良好的结果预测。

### TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction 
[[arxiv](https://arxiv.org/abs/2301.10829)] [[cool](https://papers.cool/arxiv/2301.10829)] [[pdf](https://arxiv.org/pdf/2301.10829)]
> **Authors**: Zeynel A. Samak,Philip Clatworthy,Majid Mirmehdi
> **First submission**: 2023-01-25
> **First announcement**: 2023-01-26
> **comment**: Accepted at IEEE ISBI 2023, 5 pages
- **标题**: Transop：基于变压器的中风治疗结果预测的多模式分类
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 急性缺血性中风是由血液流向脑组织的中断引起的，是全球残疾和死亡率的主要原因。最佳缺血性中风治疗的患者选择是成功结局的关键步骤，因为治疗的作用高度取决于治疗时间。我们提出了一种基于变压器的多模式网络（Transop），用于一种分类方法，该方法采用了临床元数据和成像信息并在医院入院时获得的，以预测基于修改的Rankin量表（MRS）的中风治疗的功能结果。这包括一个融合模块，可有效地结合3D非对比度计算机断层扫描（NCCT）特征和临床信息。在使用MRCLEAN数据集上的单形成和多模式数据的比较实验中，我们达到的最新AUC分数为0.85。

## 信号处理(eess.SP:Signal Processing)

该领域共有 1 篇论文

### Real-Time Digital Twins: Vision and Research Directions for 6G and Beyond 
[[arxiv](https://arxiv.org/abs/2301.11283)] [[cool](https://papers.cool/arxiv/2301.11283)] [[pdf](https://arxiv.org/pdf/2301.11283)]
> **Authors**: Ahmed Alkhateeb,Shuaifeng Jiang,Gouranga Charan
> **First submission**: 2023-01-26
> **First announcement**: 2023-01-27
> **comment**: The 6G digital twin research platform will be available soon on https://deepverse6g.net/
- **标题**: 实时数字双胞胎：6G及以后的视觉和研究方向
- **领域**: 信号处理,信息论,机器学习
- **摘要**: 本文提出了一个愿景，其中\ textit {实时}物理无线环境的数字双胞胎使用分布式基础结构和用户设备的多模式传感数据不断更新，并用于做出通信和感应决策。精确的3D地图，多模式传感，射线追踪计算以及机器/深度学习的进步主要是通过实现了这种愿景。本文详细介绍了这种愿景，解释了构建和利用这些实时数字双胞胎，讨论应用程序和开放问题的不同方法，并提供了一个可用于研究各种数字双胞胎研究方向的研究平台。

## 光学(physics.optics:Optics)

该领域共有 1 篇论文

### Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning 
[[arxiv](https://arxiv.org/abs/2301.06496)] [[cool](https://papers.cool/arxiv/2301.06496)] [[pdf](https://arxiv.org/pdf/2301.06496)]
> **Authors**: Joowon Lim,Jannes Gladrow,Douglas Kelly,Greg O'Shea,Govert Verkes,Ioan Stefanovici,Sebastian Nowozin,Benn Thomsen
> **First submission**: 2023-01-16
> **First announcement**: 2023-01-17
> **comment**: 21 pages, 5 figures
- **标题**: 使用可区分的射线跟踪和机器学习的具有百像图像的多模灯光上的有效数据传输
- **领域**: 光学,人工智能,计算机视觉和模式识别
- **摘要**: 通过多模式纤维传输的图像，由于它们能够在紧凑的系统中有效地限制和运输光线，因此人们越来越感兴趣。在这里，我们演示了基于机器学习的大规模数字图像（页）的解码，从而最大程度地提高了光学存储应用程序的页面容量。使用毫米大小的方形横截面波导，我们为8位空间光调节器成像，将数据显示为符号的矩阵。通常，解码器会引起过度的O（n^2）计算缩放，以在空间上加重的数据中解释n个符号。但是，通过将设置的数字双胞胎与U-NET相结合，我们只能使用高效的卷积操作来检索多达66 KB的功能。我们将基于射线追踪的训练射线追踪与基于本本的双胞胎进行了比较，并表明前者可以通过调整光学瑕疵来克服仿真到实验间隙的能力。我们使用基于VON-MISS分布的可区分相互信息估计器端到端训练管道，通常适用于相位编码通道。

## 生物分子(q-bio.BM:Biomolecules)

该领域共有 1 篇论文

### ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts 
[[arxiv](https://arxiv.org/abs/2301.12040)] [[cool](https://papers.cool/arxiv/2301.12040)] [[pdf](https://arxiv.org/pdf/2301.12040)]
> **Authors**: Minghao Xu,Xinyu Yuan,Santiago Miret,Jian Tang
> **First submission**: 2023-01-27
> **First announcement**: 2023-01-30
> **comment**: Accpeted by ICML 2023 (Oral), code and data released
- **标题**: PROTST：蛋白质序列和生物医学文本的多模式学习
- **领域**: 生物分子,机器学习
- **摘要**: 当前的蛋白质语言模型（PLMS）主要基于其序列学习蛋白质表示，从而很好地捕获了共同进化信息，但是它们无法明确获取蛋白质功能，这是蛋白质表示学习的最终目标。幸运的是，对于许多蛋白质，它们的文本属性描述都可用，其中还描述了它们的各种功能。出于这个事实，我们首先将ProtDeScribe数据集构建以增强蛋白质序列，并使用其功能和其他重要属性的文本描述。基于此数据集，我们提出了ProTST框架，以增强生物医学文本的蛋白质序列预训练和理解。在预训练期间，我们设计了三种类型的任务，即单峰掩码预测，多模式表示对准和多模式掩码预测，以增强具有不同粒度的蛋白质属性信息的PLM，并同时保留PLM的原始表示功能。在下游任务上，ProTS启用了监督学习和零照片预测。我们验证了蛋白质诱导的PLM的优越性，而不是先前的PLM在不同的表示基准上。在零拍设置下，我们显示了ProTS对零摄像蛋白分类的有效性，而ProTST还可以从大规模数据库中获得功能性蛋白质检索，而无需任何功能注释。

## 定量方法(q-bio.QM:Quantitative Methods)

该领域共有 1 篇论文

### Deep Biological Pathway Informed Pathology-Genomic Multimodal Survival Prediction 
[[arxiv](https://arxiv.org/abs/2301.02383)] [[cool](https://papers.cool/arxiv/2301.02383)] [[pdf](https://arxiv.org/pdf/2301.02383)]
> **Authors**: Lin Qiu,Aminollah Khormali,Kai Liu
> **First submission**: 2023-01-06
> **First announcement**: 2023-01-09
> **comment**: No comments
- **标题**: 深层生物途径知情的病理基因组多模式生存预测
- **领域**: 定量方法,机器学习,图像和视频处理,基因组学
- **摘要**: 多模式数据的整合，例如病理图像和基因组数据，对于了解个性化处理的癌症异质性和复杂性以及增强生存预测至关重要。尽管在整合病理学和基因组数据方面取得了进展，但大多数现有方法无法彻底挖掘复杂的模式间关系。此外，从这些模型中鉴定出可解释的特征，这些模型控制临床前发现和临床预测对于癌症诊断，预后和治疗反应研究至关重要。我们提出了PONET-一种新型的生物途径知识的病理基因组深模型，该模型整合了病理图像和基因组数据，不仅可以改善生存预测，还可以鉴定导致患者生存率不同的基因和途径。六个癌症基因组图集（TCGA）数据集的经验结果表明，我们提出的方法可实现出色的预测性能，并揭示了有意义的生物学解释。提出的方法建立了有关如何在多模式生物医学数据上培训生物学知情的深网的洞察力，这些网络将具有一般适用于理解疾病以及预测治疗的反应和抵抗力的一般适用性。

## 统计金融(q-fin.ST:Statistical Finance)

该领域共有 1 篇论文

### Leveraging Vision-Language Models for Granular Market Change Prediction 
[[arxiv](https://arxiv.org/abs/2301.10166)] [[cool](https://papers.cool/arxiv/2301.10166)] [[pdf](https://arxiv.org/pdf/2301.10166)]
> **Authors**: Christopher Wimmer,Navid Rekabsaz
> **First submission**: 2023-01-17
> **First announcement**: 2023-01-25
> **comment**: Accepted atMultimodalAI for Financial Forecasting Workshop (Muffin) at AAAI 2023
- **标题**: 利用视力语言模型用于颗粒市场变化预测
- **领域**: 统计金融,计算语言学,机器学习
- **摘要**: 使用历史数据预测股票市场的未来方向一直是财务预测的基本组成部分。此历史数据包含每个特定时间范围内股票的信息，例如开放，关闭，最低和最高的价格。利用这些数据，通常使用各种时间序列模型（例如长期术语内存网络）预测市场的未来方向。这项工作提出了一种从根本上新的方法进行建模和预测市场运动，即利用图像和基于字节的数字表示，对最近引入的视觉语言模型处理的库存数据。我们对德国股份指数的每小时库存数据进行了大量实验，并使用历史股票数据评估了各种构建。我们通过各种指标对结果进行全面评估，以准确描述各种方法的实际性能。我们的评估结果表明，我们的新方法基于库存数据作为文本（字节）的表示，并且图像的表现明显优于强大的基于深度学习的基准。

## 其他论文

共有 19 篇其他论文

- [Modeling the Rhythm from Lyrics for Melody Generation of Pop Song](https://arxiv.org/abs/2301.01361)
  - **标题**: 从歌词中为流行歌曲的旋律创建节奏
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [An Empirical Investigation into the Reproduction of Bug Reports for Android Apps](https://arxiv.org/abs/2301.01235)
  - **标题**: 对Android应用程序Bug报告复制的实证研究
  - **Filtered Reason**: none of cs.SE in whitelist
- [DOT: A flexible multi-objective optimization framework for transferring features across single-cell and spatial omics](https://arxiv.org/abs/2301.01682)
  - **标题**: 点：灵活的多目标优化框架，用于在单细胞和空间上传输特征
  - **Filtered Reason**: none of q-bio.QM,cs.CE in whitelist
- [TinyVers: A Tiny Versatile System-on-chip with State-Retentive eMRAM for ML Inference at the Extreme Edge](https://arxiv.org/abs/2301.03537)
  - **标题**: Tinyvers：一种在芯片上使用的微型多功能系统，带有状态重新emram用于ML极端的推理
  - **Filtered Reason**: none of cs.AR in whitelist
- [Design, Modeling and Control of a Quadruped Robot SPIDAR: Spherically Vectorable and Distributed Rotors Assisted Air-Ground Amphibious Quadruped Robot](https://arxiv.org/abs/2301.04050)
  - **标题**: 四倍的机器人蜘蛛的设计，建模和控制：球形矢量和分布式转子辅助空中两栖动物的机器人
  - **Filtered Reason**: none of cs.RO in whitelist
- [Regulating For-Hire Autonomous Vehicles for An Equitable Multimodal Transportation Network](https://arxiv.org/abs/2301.05798)
  - **标题**: 调节租用的自动驾驶汽车，以进行公平的多模式运输网络
  - **Filtered Reason**: none of cs.GT,math.OC,econ.GN in whitelist
- [Optimal Mobility Aware Wireless Edge Cloud Support for the Metaverse](https://arxiv.org/abs/2301.06519)
  - **标题**: 最佳移动性意识到无线边缘云支持
  - **Filtered Reason**: none of cs.NI in whitelist
- [Multimodal Robot Programming by Demonstration: A Preliminary Exploration](https://arxiv.org/abs/2301.07189)
  - **标题**: 多模式机器人编程通过演示：初步探索
  - **Filtered Reason**: none of cs.RO,cs.HC in whitelist
- [MultiCalib4DEB: A toolbox exploiting multimodal optimisation in Dynamic Energy Budget parameters calibration](https://arxiv.org/abs/2301.07548)
  - **标题**: MutticalIB4DEB：动态能量预算参数中利用多模式优化的工具箱校准
  - **Filtered Reason**: none of cs.NE in whitelist
- [MR.Brick: Designing A Remote Mixed-reality Educational Game System for Promoting Children's Social & Collaborative Skills](https://arxiv.org/abs/2301.07310)
  - **标题**: 布里克先生：设计一种远程混合现实的教育游戏系统，用于促进儿童的社交和协作技能
  - **Filtered Reason**: none of cs.HC in whitelist
- [Screen Correspondence: Mapping Interchangeable Elements between UIs](https://arxiv.org/abs/2301.08372)
  - **标题**: 屏幕通信：UIS之间的映射可互换元素
  - **Filtered Reason**: none of cs.HC in whitelist
- [Blind as a bat: audible echolocation on small robots](https://arxiv.org/abs/2301.08327)
  - **标题**: 盲人作为蝙蝠：小型机器人上的可听到的回声定位
  - **Filtered Reason**: none of cs.RO in whitelist
- [Self-driving Multimodal Studies at User Facilities](https://arxiv.org/abs/2301.09177)
  - **标题**: 在用户设施的自动驾驶多模式研究
  - **Filtered Reason**: none of cs.HC,cond-mat.mtrl-sci in whitelist
- [A Big-Data Driven Framework to Estimating Vehicle Volume based on Mobile Device Location Data](https://arxiv.org/abs/2301.08660)
  - **标题**: 一个大数据驱动的框架，用于估算基于移动设备位置数据的车辆量
  - **Filtered Reason**: none of eess.SP,cs.CY in whitelist
- [Synesthetic Dice: Sensors, Actuators, And Mappings](https://arxiv.org/abs/2301.11436)
  - **标题**: 综合骰子：传感器，执行器和映射
  - **Filtered Reason**: none of cs.HC in whitelist
- [Emotional Interaction Qualities: Vocabulary, Modalities, Actions, And Mapping](https://arxiv.org/abs/2301.11432)
  - **标题**: 情感互动质量：词汇，方式，动作和映射
  - **Filtered Reason**: none of cs.HC in whitelist
- [A sustainable infrastructure concept for improved accessibility, reusability, and archival of research software](https://arxiv.org/abs/2301.12830)
  - **标题**: 可持续的基础设施概念，可改善可访问性，可重复性和研究软件的档案
  - **Filtered Reason**: none of cs.SE in whitelist
- [Tree-structured Policy Planning with Learned Behavior Models](https://arxiv.org/abs/2301.11902)
  - **标题**: 通过学习的行为模型的树木结构策略规划
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Normalization for multimodal type theory](https://arxiv.org/abs/2301.11842)
  - **标题**: 多模式类型理论的归一化
  - **Filtered Reason**: none of cs.LO in whitelist
