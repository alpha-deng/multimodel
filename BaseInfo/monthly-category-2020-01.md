# 2020-01 月度论文分类汇总

共有60篇相关领域论文, 另有6篇其他

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 2 篇论文

### Conversational Search for Learning Technologies 
[[arxiv](https://arxiv.org/abs/2001.02912)] [[cool](https://papers.cool/arxiv/2001.02912)] [[pdf](https://arxiv.org/pdf/2001.02912)]
> **Authors**: Sharon Oviatt,Laure Soulier
> **First submission**: 2020-01-09
> **First announcement**: 2020-01-10
> **comment**: Dagstuhl Report on Conversational Search (ID 19461) - This document is a report of the breaking group "Conversational Search for Learning Technologies"
- **标题**: 学习技术的对话搜索
- **领域**: 人工智能,人机交互,信息检索
- **摘要**: 会话搜索基于用户系统合作，其目的是解决寻求信息的任务。在本报告中，我们讨论了从用户和系统方面的学习观点的这种合作的含义。我们还专注于通过对话搜索的关键组成部分（即交流方式的多模式性）进行学习的刺激，并在信息检索方面讨论含义。我们以一个研究路线图结尾，描述了有前途的研究方向和观点。

### Modeling and solving the multimodal car- and ride-sharing problem 
[[arxiv](https://arxiv.org/abs/2001.05490)] [[cool](https://papers.cool/arxiv/2001.05490)] [[pdf](https://arxiv.org/pdf/2001.05490)]
> **Authors**: Miriam Enzi,Sophie N. Parragh,David Pisinger,Matthias Prandtstetter
> **First submission**: 2020-01-15
> **First announcement**: 2020-01-16
> **comment**: No comments
- **标题**: 建模和解决多模式的汽车和乘车共享问题
- **领域**: 人工智能
- **摘要**: 我们介绍了多模式的汽车和乘车共享问题（MMCRP），其中使用一台汽车来涵盖一套乘车请求，而未发现的请求则分配给其他运输方式（MOT）。汽车的路线由一次或多个旅行组成。每次旅行都必须具有特定但非预先确定的驱动程序，以仓库开始，然后以（可能不同的）仓库结束。即使两个骑行没有相同的起源和/或目的地，也允许在用户之间共享骑行。用户始终可以根据各个首选项列表使用其他运输方式。该问题可以作为车辆调度问题提出。为了解决该问题，构建了一个辅助图，在该图中，每次旅行在仓库中的启动和结尾，并覆盖可能的乘车共享，以时空图中的形式建模为ARC。我们提出了一种基于列生成的两层分解算法，其中主问题可确保最多只能涵盖每个请求，并且定价问题通过解决时空网络中的一种最短路径问题来生成新的有希望的路线。报告了基于现实实例的计算实验。基准实例基于奥地利维也纳的人口，空间和经济数据。我们通过在合理时间内基于柱生成的方法来解决大型实例，并进一步研究了各种精确和启发式定价方案。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 9 篇论文

### A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts 
[[arxiv](https://arxiv.org/abs/2001.05493)] [[cool](https://papers.cool/arxiv/2001.05493)] [[pdf](https://arxiv.org/pdf/2001.05493)]
> **Authors**: Anant Khandelwal,Niraj Kumar
> **First submission**: 2020-01-15
> **First announcement**: 2020-01-16
> **comment**: 10 pages, 5 Figures, 6 Tables, accepted at CoDS-COMAD 2020
- **标题**: 在英文代码混合和单语文本中的统一侵略性识别系统
- **领域**: 计算语言学,信息检索
- **摘要**: 社交媒体平台的广泛使用增加了侵略的风险，这会导致精神压力，并影响人们的生活，例如心理痛苦，抗击行为和对他人的不尊重。大多数此类对话都包含代码混合语言[28]。此外，用来表达思想或沟通方式的方式也从一个社交媒体平台变为另一个平台（例如，Twitter和Facebook中的沟通方式不同）。这些都增加了问题的复杂性。为了解决这些问题，我们引入了一个统一且强大的多模式深度学习体系结构，该体系结构适用于英语代码混合的数据集和单语英文数据集。设计的系统使用了心理语言特征和非常BA-SIC的语言特征。我们的多模式深度学习体系结构包含深层金字塔CNN，合并的Bilstm和断开的RNN（都带有手套和FastText嵌入）。最后，系统根据模型平均做出决定。我们在英语代码混合TRAC 2018数据集和从Kaggle获得的单语英文数据集进行了评估。实验结果表明，我们提出的系统优于英文代码混合数据集和单语英文数据集的所有先前方法。

### A multimodal deep learning approach for named entity recognition from social media 
[[arxiv](https://arxiv.org/abs/2001.06888)] [[cool](https://papers.cool/arxiv/2001.06888)] [[pdf](https://arxiv.org/pdf/2001.06888)]
> **Authors**: Meysam Asgari-Chenaghlu,M. Reza Feizi-Derakhshi,Leili Farzinvash,M. A. Balafar,Cina Motamed
> **First submission**: 2020-01-19
> **First announcement**: 2020-01-20
> **comment**: No comments
- **标题**: 从社交媒体中命名实体识别的多模式深度学习方法
- **领域**: 计算语言学,机器学习,多媒体,社交和信息网络
- **摘要**: 社交媒体帖子中指定的实体识别（NER）是一项具有挑战性的任务。用户生成的内容构成了社交媒体的性质，是嘈杂的，并且包含语法和语言错误。这种嘈杂的内容使得诸如命名实体识别之类的任务变得更加困难。我们提出了两种利用多模式深度学习和变压器的新型深度学习方法。我们的这两种方法都使用简短的社交媒体帖子中的图像功能来为NER任务提供更好的结果。在第一种方法上，我们使用InceptionV3提取图像功能，并使用Fusion结合文本和图像特征。当用户提供了与实体相关的图像时，这会显示更可靠的名称实体识别。在第二种方法中，我们使用图像功能与文本结合并将其馈入诸如变压器之类的BERT。实验结果，即，与其他最先进的解决方案相比，精确度，回忆和F1评分指标表明我们工作的优越性。

### Modality-Balanced Models for Visual Dialogue 
[[arxiv](https://arxiv.org/abs/2001.06354)] [[cool](https://papers.cool/arxiv/2001.06354)] [[pdf](https://arxiv.org/pdf/2001.06354)]
> **Authors**: Hyounghun Kim,Hao Tan,Mohit Bansal
> **First submission**: 2020-01-17
> **First announcement**: 2020-01-20
> **comment**: AAAI 2020 (11 pages)
- **标题**: 视觉对话的模态平衡模型
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 视觉对话框任务需要一个模型来利用图像和对话上下文信息，以生成对话的下一个响应。但是，通过手动分析，我们发现只能查看图像而无需访问上下文历史记录，而其他人仍然需要对话上下文来预测正确的答案，可以回答大量的对话问题。我们证明，由于这个原因，以前的联合模式（历史和图像）模型过度依赖，并且更容易记住对话历史记录（例如，通过在上下文信息中提取某些关键字或模式来提取某些关键字或模式），而仅图像模型则更具概括性（因为它们无法记住或从历史记录中提取重要的答案），并且在多元范围内进行了多种命令的纠正率（均为折叠率）。因此，该观察结果鼓励我们明确维护两个模型，即仅图像模型和图像历史模型，并结合其互补能力，以获得更平衡的多模式模型。我们通过与共享参数的集合和共识辍学融合提供了两个模型集成的多种方法。从经验上讲，我们的模型在2019年视觉对话挑战挑战赛（NDCG的排名第3和高度平衡）上取得了良好的成果，并且在大多数指标上的视觉对话挑战挑战赛的获胜者都大大优于大多数指标。

### Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue System 
[[arxiv](https://arxiv.org/abs/2001.06206)] [[cool](https://papers.cool/arxiv/2001.06206)] [[pdf](https://arxiv.org/pdf/2001.06206)]
> **Authors**: Yun-Wei Chu,Kuan-Yen Lin,Chao-Chun Hsu,Lun-Wei Ku
> **First submission**: 2020-01-17
> **First announcement**: 2020-01-20
> **comment**: DSTC8 collocated with AAAI2020
- **标题**: 用于场景吸引对话系统的多步联合风格注意网络
- **领域**: 计算语言学
- **摘要**: 了解动态场景和对话环境以与用户交谈一直在对多模式对话系统的挑战。第8个对话框系统技术挑战（DSTC8）提出了一个视觉视觉场景吸引对话框（AVSD）任务，其中包含多种模式，包括音频，视觉和语言，以评估对话系统如何理解对用户的不同方式和响应。在本文中，我们提出了一个基于复发性神经网络（RNN）的多步联合感重网络（JMAN），以理解视频。我们的模型执行了多步的注意机制，并在每个推理过程中共同考虑视觉和文本表示，以更好地整合来自两种不同方式的信息。与AVSD组织者发布的基线相比，我们的模型在Rouge-L得分和苹果酒评分方面的相对相对12.1％和22.4％。

### Recommending Themes for Ad Creative Design via Visual-Linguistic Representations 
[[arxiv](https://arxiv.org/abs/2001.07194)] [[cool](https://papers.cool/arxiv/2001.07194)] [[pdf](https://arxiv.org/pdf/2001.07194)]
> **Authors**: Yichao Zhou,Shaunak Mishra,Manisha Verma,Narayan Bhamidipati,Wei Wang
> **First submission**: 2020-01-20
> **First announcement**: 2020-01-21
> **comment**: 7 pages, 8 figures, 2 tables, accepted by The Web Conference 2020
- **标题**: 通过视觉语言表示为广告创意设计推荐主题
- **领域**: 计算语言学,计算机视觉和模式识别,信息检索,机器学习,多媒体
- **摘要**: 在线广告行业需要多年生植物来刷新广告广告创意，即用于吸引在线用户进入品牌的图像和文本。需要这种刷新来减少在线用户中广告疲劳的可能性，并纳入相关产品类别中其他成功广告系列的见解。鉴于品牌，为新广告提出主题是创意战略家的艰辛和耗时的过程。策略师通常会从用于过去的广告活动的图像和文本以及对品牌的世界知识中汲取灵感。要通过过去的广告系列中的多式模式信息来源自动推断AD主题，我们为广告创意战略家提出了一个主题（键形）推荐系统。主题推荐仪是基于视觉问题回答（VQA）任务的汇总结果，该任务摄入以下内容：（i）广告图像，（ii）与广告中的广告以及Wikipedia页面相关的文本以及ADS中的品牌以及（iii）围绕AD的问题。我们利用基于变压器的跨模式编码来训练VQA任务的视觉语言表示。我们在分类和排名方面研究了VQA任务的两个公式；通过公共数据集的实验，我们表明跨模式表示形式可显着提高分类准确性和排名Precision-Recall指标。与单独的图像和文本表示相比，跨模式表示的性能更好。此外，多模式信息的使用显示了仅使用文本或视觉信息而不是使用文本或视觉信息。

### ManyModalQA: Modality Disambiguation and QA over Diverse Inputs 
[[arxiv](https://arxiv.org/abs/2001.08034)] [[cool](https://papers.cool/arxiv/2001.08034)] [[pdf](https://arxiv.org/pdf/2001.08034)]
> **Authors**: Darryl Hannan,Akshay Jain,Mohit Bansal
> **First submission**: 2020-01-22
> **First announcement**: 2020-01-23
> **comment**: AAAI 2020 (10 pages)
- **标题**: Manymodalqa：多种投入的模式歧义和质量检查
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **摘要**: 我们提出了一个新的多模式问题回答挑战Manymodalqa，其中代理必须通过考虑三种不同的方式来回答问题：文本，图像和表格。我们通过刮擦Wikipedia，然后利用众包收集问答对，收集我们的数据。我们的问题是模棱两可的，因为包含答案的方式不容易基于问题。为了证明这种歧义，我们构建了一个模态选择器（或disamiguator）网络，并且与现有数据集相比，该模型在挑战集上的准确性大大降低，这表明我们的问题更模棱两可。通过分析该模型，我们研究了问题中的哪些单词表示模式。接下来，我们构建了一个简单的基线Manymodalqa模型，该模型基于模态选择器的预测，将启动相应的预训练的最先进的单峰QA模型。我们专注于为社区提供新的多种模式评估集，并且仅提供微调集，并期望在大多数培训中转移现有的数据集和方法，以鼓励低资源概括，而无需为每个新任务提供大型，单层的培训集。我们的基线模型和人类绩效之间存在很大的差距。因此，我们希望这项挑战鼓励端到端的模式歧义和多模式质量质量质量质量质量质量模型以及转移学习的研究。代码和数据可用：https：//github.com/hannandarryl/manymodalqa

### Multi-modal Sentiment Analysis using Super Characters Method on Low-power CNN Accelerator Device 
[[arxiv](https://arxiv.org/abs/2001.10179)] [[cool](https://papers.cool/arxiv/2001.10179)] [[pdf](https://arxiv.org/pdf/2001.10179)]
> **Authors**: Baohua Sun,Lin Yang,Hao Sha,Michael Lin
> **First submission**: 2020-01-28
> **First announcement**: 2020-01-29
> **comment**: 9 pages, 2 figures, 6 tables. Accepted by AAAI 2020 Affective Content Analysis Workshop
- **标题**: 使用超功能CNN加速器设备上的超级角色方法的多模式情感分析
- **领域**: 计算语言学
- **摘要**: 近年来，NLP研究见证了DNN模型的创纪录的准确性提高。但是，功耗是部署NLP系统的实际问题之一。大多数当前最新算法都是在GPU上实现的，该算法不是强力效率，部署成本也很高。另一方面，CNN域特异性加速器（CNN-DSA）已在质量生产中提供低功率和低成本的计算功率。在本文中，我们将在CNN-DSA上实现超级字符方法。此外，我们修改了使用多模式数据的超级字符方法，即Cl-aff共享任务中的文本加上表格数据。

### Multimodal Story Generation on Plural Images 
[[arxiv](https://arxiv.org/abs/2001.10980)] [[cool](https://papers.cool/arxiv/2001.10980)] [[pdf](https://arxiv.org/pdf/2001.10980)]
> **Authors**: Jing Jiang
> **First submission**: 2020-01-15
> **First announcement**: 2020-01-30
> **comment**: This is an undergraduate project report. Completed Dec. 2019 at the Cooper Union
- **标题**: 多模式的故事产生复数图像
- **领域**: 计算语言学,计算机视觉和模式识别,机器学习,机器学习
- **摘要**: 传统上，文本生成模型以一系列文本为输入，并使用预训练的参数迭代生成下一个最可能的单词。在这项工作中，我们建议架构使用图像而不是文本作为文本生成模型的输入，称为StoryGen。在体系结构中，我们设计了一个关系文本数据生成器算法，该算法将不同的功能与多个图像相关联。来自模型的输出样本展示了生成有意义的文本段落的能力，该文本包含来自输​​入图像中提取的特征的能力。这是一份本科项目报告。 2019年12月在库珀联盟完成。

### Introducing the diagrammatic semiotic mode 
[[arxiv](https://arxiv.org/abs/2001.11224)] [[cool](https://papers.cool/arxiv/2001.11224)] [[pdf](https://arxiv.org/pdf/2001.11224)]
> **Authors**: Tuomo Hiippala,John A. Bateman
> **First submission**: 2020-01-30
> **First announcement**: 2020-01-31
> **comment**: 16 pages; accepted at Diagrams 2022
- **标题**: 引入图解符号模式
- **领域**: 计算语言学,人工智能
- **摘要**: 随着许多学科的图表的使用和多样性的增长，对图表研究社区的兴趣越来越令人兴趣，即如何记录和解释这种多样性。在本文中，我们认为，实现图表一般分类的可靠性，覆盖率和实用性提高的一种方法是利用在多模式领域中开发的最近开发的符号原理。为此，我们勾勒出可能暂时称为图形符号模式的内部细节。这提供了自然的描述，即图表表示如何整合自然语言，各种形式的图形，图形元素，例如箭头，线条和其他表达资源，同时仍尊重视觉组织的关键图形贡献。我们使用两个最近的图表来说明了提出的方法，并展示了多模式方法如何支持图表表示的经验分析，尤其是在识别图解组成部分并以跨图类型的推广方式描述其相互关系时，并用于表征不同类型的功能。

## 密码学和安全(cs.CR:Cryptography and Security)

该领域共有 1 篇论文

### Sensor-based Continuous Authentication of Smartphones' Users Using Behavioral Biometrics: A Contemporary Survey 

[[arxiv](https://arxiv.org/abs/2001.08578)] [[cool](https://papers.cool/arxiv/2001.08578)] [[pdf](https://arxiv.org/pdf/2001.08578)]
> **Authors**: Mohammed Abuhamad,Ahmed Abusnaina,DaeHun Nyang,David Mohaisen
> **First submission**: 2020-01-23
> **First announcement**: 2020-01-24
> **comment**: 19 pages
- **标题**: 基于传感器的智能手机用户的连续身份验证使用行为生物识别技术：当代调查
- **领域**: 密码学和安全,人机交互,机器学习
- **摘要**: 移动设备和技术已经变得越来越流行，为台式计算机提供了可比的存储和计算功能，使用户可以存储并与敏感和私人信息进行交互。由于移动设备容易受到未经授权的访问或盗窃的影响，因此对此类个人信息的安全和保护变得越来越重要。用户身份验证是一项至关重要的任务，可以在进入点并连续地通过使用方式授予合法用户的访问权限。通过当今智能手机的嵌入式传感器，通过捕获行为生物识别技术和特质来实现连续和隐式的用户身份验证，使此任务成为可能。在本文中，我们调查了140多种用于连续用户身份验证的基于行为生物识别的方法，包括基于运动的方法（28个研究），基于步态的方法（19个研究），基于键震动力学的方法（20个研究），基于触摸手势的方法（29种研究），基于语音的方法（16个研究）（16个研究）和基于多态的基于多态的方法（34个研究）。该调查概述了使用智能手机嵌入式传感器捕获的行为生物识别技术的当前最新方法，用于连续用户身份验证，包括洞察力和开放挑战的采用，可用性和性能。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 24 篇论文

### Trajectory Forecasts in Unknown Environments Conditioned on Grid-Based Plans 
[[arxiv](https://arxiv.org/abs/2001.00735)] [[cool](https://papers.cool/arxiv/2001.00735)] [[pdf](https://arxiv.org/pdf/2001.00735)]
> **Authors**: Nachiket Deo,Mohan M. Trivedi
> **First submission**: 2020-01-03
> **First announcement**: 2020-01-06
> **comment**: No comments
- **标题**: 以基于网格的计划为条件的未知环境中的轨迹预测
- **领域**: 计算机视觉和模式识别,机器人技术
- **摘要**: 我们解决了在未知环境中预测行人和车辆轨迹的问题，并以其过去的运动和场景结构为条件。轨迹预测是一个具有挑战性的问题，因为场景结构的差异很大和未来轨迹的多模式分布。与以前的方法不同的方法从观察到的上下文到将来的多个轨迹，我们建议根据从基于网格的策略中采样的计划来调节轨迹预测，该计划使用最大的熵逆强化学习（Maxent IRL）来学习。我们重新制定了Maxent IRL，以允许该政策共同推断出合理的代理目标，并在现场定义的粗略2-D网格上实现这些目标。我们提出了一个基于注意力的轨迹生成器，该发生器会生成以从Maxent策略采样的状态序列为条件的连续有价值的未来轨迹。对公共可用的斯坦福无人机和Nuscenes数据集的定量和定性评估表明，我们的模型会产生多种多样的轨迹，代表多模式预测分布，并精确地符合长期预测范围的基础场景结构。

### Multimodal Semantic Transfer from Text to Image. Fine-Grained Image Classification by Distributional Semantics 
[[arxiv](https://arxiv.org/abs/2001.02372)] [[cool](https://papers.cool/arxiv/2001.02372)] [[pdf](https://arxiv.org/pdf/2001.02372)]
> **Authors**: Simon Donig,Maria Christoforaki,Bernhard Bermeitinger,Siegfried Handschuh
> **First submission**: 2020-01-07
> **First announcement**: 2020-01-08
> **comment**: 19 pages, second half in German as published in DHd2020
- **标题**: 多模式的语义转移从文本到图像。通过分布语义的细粒度图像分类
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 在过去的几年中，艺术历史和遗产信息学领域中的神经网络等图像分类过程经历了广泛的分布（Lang and Ommer 2018）。这些方法面临着几个挑战，包括在数字人文科学中处理相对少量的数据以及高维数据。在这里，使用了卷积神经网络（CNN），即像往常一样输出一系列纯文本标签，而是一系列语义上加载的向量。这些向量是由分布语义模型（DSM）产生的，该模型是由内域文本语料库生成的。 -----在Den Letzten Jahren Hat Die Die verwendung von bildkllassifizierungsverfahren wie neuronalen netzwerken auch im bereich im bereich der Imbereich istischen bildischen bildwisschaften under Heritage under Heritage Informatics weite weite weite verbreitung gefunden（lang und ommer 2018）。 Diese Verfahren Stehen Dabei Vor Einer Reihe Von Herausfordorungen，Darunter Dem umgangmit den den vergleichsweise kleinen kleinen datenmengen sowie sowie zugleich zugleich hochdimensionalen da-tenräumenin den den digitalen geisteswissenschaften。 Meist Bilden Diese Methoden DieKlallasifizierung auf Einen vergleichsweise flachen raum ab。 Dieser Flache Zugang verliert imbemühenum ontologische eindeutigkeit eine reihe von seciessenen dimensionen，darunter taxonomische，Mereologische undsogiative bezieHungen zwischengen zwischenden zwischenden zwischenden klassen klassen klassen beziehungsweise dem nichnichtext。 Dabei Wird Ein卷积神经网络（CNN）Genutzt，Dessen Ausgabe IM训练培训，Anders Als AlsHerkömmlich，Nicht Auf Einer Serie Serie flacher Textlabel Beruht，Sondern Auf Einer auf Einer Serie Serie serie serie vonvonvonvonvonvonvonvonvonvonvonvonvektoren。 diese vektoren ructieren aus einem分布语义模型（DSM），韦尔奇aus einem einemdomäne-textkorpusgeneriert wird。

### Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems 
[[arxiv](https://arxiv.org/abs/2001.02223)] [[cool](https://papers.cool/arxiv/2001.02223)] [[pdf](https://arxiv.org/pdf/2001.02223)]
> **Authors**: Isabelle Leang,Ganesh Sistu,Fabian Burger,Andrei Bursuc,Senthil Yogamani
> **First submission**: 2020-01-07
> **First announcement**: 2020-01-08
> **comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation Systems Conference (ITSC) 2020
- **标题**: 自主驾驶系统中多任务网络的动态任务加权方法
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **摘要**: 深度多任务网络对于自动驾驶系统特别感兴趣。他们可能会在预测性能，硬件约束和从多种类型的注释和方式中有效利用信息之间进行良好的权衡。但是，培训这种模型是不平凡的，需要在所有任务上平衡学习，因为它们各自的损失在整个培训中都表现出不同的规模，范围和动态。最近在不同数据集和任务组合上提出了以自适应方式调整损失的多种任务加权方法，因此很难比较它们。在这项工作中，我们在三个汽车数据集（Kitti，CityScapes和Woodscape）上共同审查并系统地评估了九种任务加权策略。然后，我们提出了一种新的方法，结合了进化元学习和基于任务的选择性反向流动，用于计算任务权重，从而进行可靠的网络培训。我们的方法在两任任务应用程序上的大幅度优于最先进的方法。

### Unpaired Multi-modal Segmentation via Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2001.03111)] [[cool](https://papers.cool/arxiv/2001.03111)] [[pdf](https://arxiv.org/pdf/2001.03111)]
> **Authors**: Qi Dou,Quande Liu,Pheng Ann Heng,Ben Glocker
> **First submission**: 2020-01-06
> **First announcement**: 2020-01-10
> **comment**: IEEE TMI, code available
- **标题**: 通过知识蒸馏不成对的多模式分割
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 多模式学习通常使用包含特定于模态层和共享层的网络体系结构进行，并利用不同模态的共同注册图像。我们提出了一种新颖的学习方案，用于未配对的跨模式图像分割，具有高度紧凑的架构可实现出色的分割精度。在我们的方法中，我们通过在CT和MRI上共享所有卷积内核来大力重复使用网络参数，并且仅采用特定于模态的内部归一化层来计算各自的统计数据。为了有效地训练这样一个高度紧凑的模型，我们通过明确限制模式之间我们派生的预测分布的KL差异来引入一个受知识蒸馏启发的新型损失术语。我们已经对两个多类分割问题进行了广泛验证的方法：i）心脏结构分割，ii）腹部器官分割。不同的网络设置，即2D扩张的网络和3D U-NET，用于研究我们方法的一般疗效。两项任务的实验结果表明，我们的新型多模式学习方案始终优于单模式训练和以前的多模式方法。

### Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features 
[[arxiv](https://arxiv.org/abs/2001.04732)] [[cool](https://papers.cool/arxiv/2001.04732)] [[pdf](https://arxiv.org/pdf/2001.04732)]
> **Authors**: Andres Mafla,Sounak Dey,Ali Furkan Biten,Lluis Gomez,Dimosthenis Karatzas
> **First submission**: 2020-01-14
> **First announcement**: 2020-01-15
> **comment**: Winter Conference on Applications of Computer Vision (WACV 2020) Accepted paper
- **标题**: 通过结合视觉和本地汇总的文本功能，细粒度的图像分类和检索
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像中包含的文本具有高级语义，可以利用这些语义来获得更丰富的图像理解。特别是，仅文本的存在提供了强大的指导内容，应采用这些内容来解决各种计算机视觉任务，例如图像检索，细粒度的分类和视觉问题回答。在本文中，我们通过利用文本信息以及视觉提示来理解两种模式之间现有的内在关系，以解决细粒度分类和图像检索的问题。所提出的模型的新颖性包括使用PHOC的描述符，以构建一袋文本单词以及捕获文本形态的Fisher Vector编码。这种方法为这项任务提供了更强的多模式表示形式，正如我们的实验所证明的那样，它在两个不同的任务上实现了最新的结果，即细粒度的分类和图像检索。

### Delving Deeper into the Decoder for Video Captioning 
[[arxiv](https://arxiv.org/abs/2001.05614)] [[cool](https://papers.cool/arxiv/2001.05614)] [[pdf](https://arxiv.org/pdf/2001.05614)]
> **Authors**: Haoran Chen,Jianmin Li,Xiaolin Hu
> **First submission**: 2020-01-15
> **First announcement**: 2020-01-16
> **comment**: 8 pages, 3 figures, European Conference on Artificial Intelligence. ECAI 2020
- **标题**: 深入研究解码器以进行视频字幕
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 视频字幕是一项高级多模式任务，旨在用自然语言句子描述视频剪辑。编码器框架框架是近年来最受欢迎的该任务的最受欢迎的范式。但是，视频字幕模型的解码器中存在一些问题。我们对解码器进行了彻底的调查，并采用了三种技术来提高模型的性能。首先，将变分辍学和层归一化的组合嵌入了复发单元中，以减轻过度拟合的问题。其次，提出了一种新的在线方法来评估验证集上模型的性能，以便选择最佳的测试检查点。最后，提出了一种称为专业学习的新培训策略，该策略利用字幕模型的优势并绕过了其弱点。在Microsoft研究视频描述语料库（MSVD）和文本（MSR-VTT）数据集的实验中证明了这一点，我们的模型已通过BLEU，CIDER，METEOR和ROUGE-L指标获得了最佳结果，并在MSVD上获得了2.5％的高度增长，并在MSVD上获得了3.5％的高度增长。

### EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video 
[[arxiv](https://arxiv.org/abs/2001.05488)] [[cool](https://papers.cool/arxiv/2001.05488)] [[pdf](https://arxiv.org/pdf/2001.05488)]
> **Authors**: Jennifer J. Sun,Ting Liu,Alan S. Cowen,Florian Schroff,Hartwig Adam,Gautam Prasad
> **First submission**: 2020-01-15
> **First announcement**: 2020-01-16
> **comment**: Data subset at https://github.com/google-research-datasets/eev
- **标题**: EEV：一个用于研究视频中唤起表达式的大型数据集
- **领域**: 计算机视觉和模式识别
- **摘要**: 视频可以唤起观众中的一系列情感响应。在观看视频观看视频之前，可以从视频中预测诱发影响的能力可以帮助创建内容和视频推荐。我们介绍了视频（EEV）数据集中的引人注目的表达式，这是一个大规模数据集，用于研究观众对视频的响应。每个视频的注释在6 Hz的注释中，带有15个连续的诱发表达标签，对应于对视频做出反应的观众的面部表达。我们在数据收集框架中使用表达识别模型来实现可扩展性。总共有3670万个观众面部反应注释，以23,574个视频（1,700小时）。我们使用公开可用的视频语料库来获取各种视频内容。我们使用现有的多模式复发模型在EEV数据集上建立基线性能。转移学习实验表明，在EEV预先培训时，LIRISCASCEDE视频数据集的性能有所改善。我们希望EEV数据集的规模和多样性将鼓励在视频理解和情感计算中进一步探索。 EEV的子集在https://github.com/google-research-datasets/eev上发布。

### Contextual Sense Making by Fusing Scene Classification, Detections, and Events in Full Motion Video 
[[arxiv](https://arxiv.org/abs/2001.05979)] [[cool](https://papers.cool/arxiv/2001.05979)] [[pdf](https://arxiv.org/pdf/2001.05979)]
> **Authors**: Marc Bosch,Joseph Nassar,Benjamin Ortiz,Brendan Lammers,David Lindenbaum,John Wahl,Robert Mangum,Margaret Smith
> **First submission**: 2020-01-16
> **First announcement**: 2020-01-17
> **comment**: No comments
- **标题**: 通过融合场景分类，检测和事件的上下文意义，在全部运动视频中
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着成像传感器的扩散，多模式成像的体积远远超过了人类分析人员充分消耗和利用它的能力。全运动视频（FMV）具有包含大量冗余时间数据的额外挑战。我们旨在满足人类分析师在给定空中FMV的消耗和利用数据的需求。我们已经调查并设计了一个能够检测到偏离FMV提要的观察基线模式的事件和感兴趣的活动的系统。我们将问题分为三个任务：（1）上下文意识，（2）对象分类和（3）事件检测。上下文意识的目的是限制视频数据中的视觉搜索和检测问题。自定义图像分类器用一个或多个标签对场景进行分类，以识别操作上下文和环境。此步骤有助于减少下游任务的语义搜索空间，以提高其准确性。第二步是对象编目，其中对象检测器的集合可以找到并标记场景中发现的任何已知对象（人，车辆，车辆，船，飞机，建筑物等）。最后，将上下文信息和检测发送到事件检测引擎以监视某些行为。一系列分析通过跟踪对象计数和对象交互来监视场景。如果未在当前场景中宣布这些对象相互作用，则系统将报告，地理位置和记录事件。感兴趣的事件包括将人们聚集成为会议和/或人群，警告何时在海滩上卸下货物，人们增加了进入建筑物的人数，人们进入和/或从感兴趣的工具中出现的人等。我们已经在各种地理区域的不同分辨率的不同传感器中应用了方法。

### Multi-Layer Content Interaction Through Quaternion Product For Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2001.05840)] [[cool](https://papers.cool/arxiv/2001.05840)] [[pdf](https://arxiv.org/pdf/2001.05840)]
> **Authors**: Lei Shi,Shijie Geng,Kai Shuang,Chiori Hori,Songxiang Liu,Peng Gao,Sen Su
> **First submission**: 2020-01-02
> **First announcement**: 2020-01-17
> **comment**: No comments
- **标题**: 通过四元素产品进行视觉问题回答的多层内容交互
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 近年来，多模式融合技术已大大提高了基于神经网络的视频描述/字幕，视觉询问（VQA）和视觉视觉场景吸引对话框（AVSD）的性能。大多数以前的方法仅探索多层特征融合的最后一层，同时省略了中间层的重要性。为了解决中间层的问题，我们建议有效的四基因块网络（QBN）不仅要学习最后一层的相互作用，而且还同时学习所有中间层的相互作用。在我们提出的QBN中，我们使用整体文本功能来指导视觉功能的更新。同时，汉密尔顿四季度产品可以有效地执行从较高层到较低层的信息流，以视觉和文本方式。评估结果表明，即使使用超级BERT或视觉BERT预训练模型，我们的QBN也提高了VQA 2.0的性能。已经进行了广泛的消融研究，以证明本研究中每个提出的模块的影响。

### Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture Recognition 
[[arxiv](https://arxiv.org/abs/2001.05833)] [[cool](https://papers.cool/arxiv/2001.05833)] [[pdf](https://arxiv.org/pdf/2001.05833)]
> **Authors**: Yi Zhang,Chong Wang,Ye Zheng,Jieyu Zhao,Yuqi Li,Xijiong Xie
> **First submission**: 2019-12-31
> **First announcement**: 2020-01-17
> **comment**: No comments
- **标题**: 用于动态手势识别的短期时间卷积网络
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 手势识别的目的是识别人体的有意义运动，而手势识别是计算机视觉中的重要问题。在本文中，我们提出了一种基于3D密集卷积网络（3D-Densenets）和改进的时间卷积网络（TCN）的多模式识别方法。我们方法的关键思想是找到空间和时间特征的紧凑而有效的表示，该特征有序，分别将手势视频分析的任务分为两个部分：空间分析和时间分析。在空间分析中，我们采用3D-densenets来有效地学习短期时空特征。随后，在时间分析中，我们使用TCN来提取时间特征，并采用改进的挤压和兴奋网络（SENETS）来增强每个TCN层中时间特征的代表性。该方法已在VIVA和NVIDIA手势动态手势数据集上进行了评估。我们的方法在VIVA基准测试中获得了非常有竞争力的性能，分类精度为91.54％，并以86.37％的精度在NVIDIA基准测试中实现了最先进的性能。

### MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning 
[[arxiv](https://arxiv.org/abs/2001.06902)] [[cool](https://papers.cool/arxiv/2001.06902)] [[pdf](https://arxiv.org/pdf/2001.06902)]
> **Authors**: Simon Vandenhende,Stamatios Georgoulis,Luc Van Gool
> **First submission**: 2020-01-19
> **First announcement**: 2020-01-20
> **comment**: Accepted at ECCV2020 (spotlight) - Code: https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch
- **标题**: MTI-NET：多任务学习的多尺度任务交互网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们争论在多任务学习设置中提炼任务信息时，考虑任务交互的重要性。与普遍的信念相反，我们表明，在一定尺度上具有高亲和力的任务不能保证在其他尺度上保留这种行为，反之亦然。我们提出了一种新颖的建筑，即MTI-NET，它以三种方式建立在这一发现的基础上。首先，它通过多尺度的多模式蒸馏单元在每个尺度上明确对任务交互进行建模。其次，它通过特征传播模块传播从下部到更高尺度的蒸馏任务信息。第三，它通过特征聚合单元从所有量表中汇总了精致的任务特征，以产生最终的每个任务预测。在两个多任务密集标签数据集上进行的广泛实验表明，与先前的工作不同，我们的多任务模型可提供多任务学习的全部潜力，即，记忆足迹较小，计算数量减少和更好的性能W.R.T.单任务学习。该代码公开可用：https：//github.com/simonvandenhende/multi-task-learning-pytorch。

### Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences 
[[arxiv](https://arxiv.org/abs/2001.06891)] [[cool](https://papers.cool/arxiv/2001.06891)] [[pdf](https://arxiv.org/pdf/2001.06891)]
> **Authors**: Zhu Zhang,Zhou Zhao,Yang Zhao,Qi Wang,Huasheng Liu,Lianli Gao
> **First submission**: 2020-01-19
> **First announcement**: 2020-01-20
> **comment**: The camera ready version for CVPR 2020
- **标题**: 它存在于哪里：多式句子的时空视频接地
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们考虑了多种句子（STVG）的新任务，时空视频接地。鉴于一个未修剪的视频和描述对象的声明性/疑问句，STVG旨在定位查询对象的时空管。 STVG具有两个具有挑战性的设置：（1）我们需要从未修剪的视频中定位时空对象管，其中该对象可能仅存在于视频的一小部分中； （2）我们处理多格式句子，包括带有明确对象的声明句子和带有未知对象的疑问句子。由于导管预先发电和缺乏对象关系建模，现有方法无法应对STVG任务。因此，我们为此任务提出了一个新颖的时空图推理网络（STGRN）。首先，我们构建了一个时空区域图，以捕获与时间对象动力学的区域关系，该区域涉及每个帧中的隐式和显式空间子图以及跨帧的时间动态子图。然后，我们将文本线索纳入图中，并开发多步横模图推理。接下来，我们引入了一种时空定位器，采用动态选择方法来直接检索没有管前的时空管。此外，我们根据视频关系数据集Vidor贡献了一个大规模的视频接地数据集VIDSTG。广泛的实验证明了我们方法的有效性。

### Accuracy vs. Complexity: A Trade-off in Visual Question Answering Models 
[[arxiv](https://arxiv.org/abs/2001.07059)] [[cool](https://papers.cool/arxiv/2001.07059)] [[pdf](https://arxiv.org/pdf/2001.07059)]
> **Authors**: Moshiur R. Farazi,Salman H. Khan,Nick Barnes
> **First submission**: 2020-01-20
> **First announcement**: 2020-01-21
> **comment**: No comments
- **标题**: 准确性与复杂性：视觉问题回答模型的权衡
- **领域**: 计算机视觉和模式识别,计算复杂度
- **摘要**: 视觉问题回答（VQA）已成为视觉图灵测试，以验证AI剂的推理能力。现有VQA模型的枢轴是通过将图像中的视觉特征和来自给定问题的语义特征组合在一起来学习的关节嵌入。因此，大量文献集中于制定复杂的关节嵌入策略以及视觉注意机制，以有效捕获这两种方式之间的相互作用。但是，在高维（关节嵌入）空间中对视觉和语义特征进行建模在计算上是昂贵的，并且更复杂的模型通常会导致VQA准确性的微不足道改善。在这项工作中，我们系统地研究了模型复杂性与VQA任务的性能之间的权衡。 VQA模型具有多样化的体系结构，包括预处理，特征提取，多模式融合，注意力和最终分类阶段。我们特别关注“多模式融合”在VQA模型中的效果，这通常是VQA管道中最昂贵的步骤。我们彻底的实验评估使我们提出了两个建议，一项提案优化了最小的复杂性，另一个针对最先进的VQA性能进行了优化。

### EMOPAIN Challenge 2020: Multimodal Pain Evaluation from Facial and Bodily Expressions 
[[arxiv](https://arxiv.org/abs/2001.07739)] [[cool](https://papers.cool/arxiv/2001.07739)] [[pdf](https://arxiv.org/pdf/2001.07739)]
> **Authors**: Joy O. Egede,Siyang Song,Temitayo A. Olugbade,Chongyang Wang,Amanda Williams,Hongying Meng,Min Aung,Nicholas D. Lane,Michel Valstar,Nadia Bianchi-Berthouze
> **First submission**: 2020-01-21
> **First announcement**: 2020-01-22
> **comment**: 8 pages
- **标题**: emopain挑战2020：面部和身体表情的多模式疼痛评估
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **摘要**: 2020年Emopain挑战是第一次旨在创建一个统一平台，以比较机器学习和从人类表达行为的自动慢性疼痛评估的多媒体处理方法以及与疼痛相关行为的识别。挑战的目的是促进研究辅助技术的研究，这些技术通过实时监测和反馈来帮助改善慢性疼痛患者的生活质量，以帮助管理自己的状况并保持身体活跃。挑战还旨在鼓励使用相对未充分利用的，尽管身体表达至关重要的表达信号，以自动疼痛和与疼痛有关的情绪识别。本文介绍了挑战，竞争准则，台式标记数据集以及基线系统对三个子任务的架构和性能的描述：面部表情估算疼痛，多模式运动的疼痛认识以及保护性运动行为检测。

### ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data 
[[arxiv](https://arxiv.org/abs/2001.07966)] [[cool](https://papers.cool/arxiv/2001.07966)] [[pdf](https://arxiv.org/pdf/2001.07966)]
> **Authors**: Di Qi,Lin Su,Jia Song,Edward Cui,Taroon Bharti,Arun Sacheti
> **First submission**: 2020-01-22
> **First announcement**: 2020-01-23
> **comment**: No comments
- **标题**: Imagebert：大规模弱监督图像文本数据的跨模式预训练
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们介绍了一个新的视觉语言预培训模型-Imagebert-用于图像文本关节嵌入。我们的模型是一个基于变压器的模型，该模型采用不同的方式作为输入并建模它们之间的关系。该模型同时在四个任务上进行了预训练：蒙版语言建模（MLM），蒙版对象分类（MOC），蒙版区域特征回归（MRFR）和图像文本匹配（ITM）。为了进一步提高培训质量，我们从Web收集了一个大规模的弱监督图像文本（LAIT）数据集。我们首先在此数据集上预先培训模型，然后对概念字幕和SBU字幕进行第二阶段预训练。我们的实验表明，多阶段预训练策略的表现优于单阶段的预训练。我们还可以在图像检索和文本检索任务上微调和评估我们的预训练的Imagebert模型，并在MSCOCO和FLICKR30K数据集上实现新的最新结果。

### Deep Bayesian Network for Visual Question Generation 
[[arxiv](https://arxiv.org/abs/2001.08779)] [[cool](https://papers.cool/arxiv/2001.08779)] [[pdf](https://arxiv.org/pdf/2001.08779)]
> **Authors**: Badri N. Patro,Vinod K. Kurmi,Sandeep Kumar,Vinay P. Namboodiri
> **First submission**: 2020-01-23
> **First announcement**: 2020-01-24
> **comment**: WACV-2020 (Accepted)
- **标题**: 深层贝叶斯网络用于视觉问题的生成
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,多媒体
- **摘要**: 从图像中产生自然问题是一项语义任务，需要使用视觉和语言方式来学习多模式表示。图像可以具有多个视觉和语言提示，例如位置，字幕和标签。在本文中，我们提出了一个原则上的深贝叶斯学习框架，该框架结合了这些线索以产生自然问题。我们观察到，通过增加更多的提示并通过最大程度地减少线索中的不确定性，贝叶斯网络变得更加自信。我们提出了最小化提示（MUMC）混合物的不确定性，从而最大程度地减少了提示专家混合物中存在的不确定性，以产生概率问题。这是一个贝叶斯框架，结果与人类研究验证的自然问题相似。我们观察到，通过增加更多的提示并通过最大程度地减少提示中的不确定性，贝叶斯框架变得更加自信。对我们的模型的消融研究表明，在此任务下，线索的一部分较低，因此优先考虑提示的原则融合。此外，我们观察到，所提出的方法对定量指标的最先进基准（Bleu-N，Meteor，Rouge和Cider）有了显着改善。在这里，我们提供了深贝叶斯VQG \ url {https://delta-lab-iitk.github.io/bvqg/}的项目链接

### Weakly Supervised Few-shot Object Segmentation using Co-Attention with Visual and Semantic Embeddings 
[[arxiv](https://arxiv.org/abs/2001.09540)] [[cool](https://papers.cool/arxiv/2001.09540)] [[pdf](https://arxiv.org/pdf/2001.09540)]
> **Authors**: Mennatullah Siam,Naren Doraiswamy,Boris N. Oreshkin,Hengshuai Yao,Martin Jagersand
> **First submission**: 2020-01-26
> **First announcement**: 2020-01-27
> **comment**: Accepted to IJCAI'20. The first three authors listed contributed equally
- **标题**: 使用视觉和语义嵌入的共同注意力进行弱监督的很少的对象分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 最近在开发少数射门对象分割方法中取得了重大进展。使用像素级，涂鸦和边界框监督，学习表明学习在几次射击分段设置中取得了成功。本文采用另一种方法，即仅需要图像级标签以进行几个弹片对象分割。我们提出了一种新型的多模式相互作用模块，以用于几个射击对象分割，该模块使用视觉和单词嵌入使用共同注意机制。我们使用图像级标签的模型比以前提出的图像级少数点对象分割相比，提高了4.8％。它还优于使用Pascal-5i上使用弱边界框监督的最先进方法。我们的结果表明，使用单词嵌入，很少有射击分割受益，并且我们能够使用带有弱图像级标签的堆叠关节视觉语义处理来执行很少的分割。我们进一步提出了一个新颖的设置，用于几次学习（TOSFL）的新型设置，用于视频。 TOSFL可以用于各种公共视频数据，例如YouTube-VOS，如实例级别和类别级别TOSFL实验中所示。

### TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval 
[[arxiv](https://arxiv.org/abs/2001.09099)] [[cool](https://papers.cool/arxiv/2001.09099)] [[pdf](https://arxiv.org/pdf/2001.09099)]
> **Authors**: Jie Lei,Licheng Yu,Tamara L. Berg,Mohit Bansal
> **First submission**: 2020-01-24
> **First announcement**: 2020-01-27
> **comment**: ECCV 2020 (extended version, with TVC dataset+models; 35 pages)
- **标题**: TVR：用于视频摘要时刻检索的大规模数据集
- **领域**: 计算机视觉和模式识别,计算语言学,信息检索
- **摘要**: 我们介绍了电视节目检索（TVR），这是一个新的多模式检索数据集。 TVR需要系统来了解视频及其相关的字幕（对话）文本，从而使其更现实。该数据集包含从6个电视节目的21.8k视频中收集的109K查询，其中每个查询都与紧密的时间窗口相关联。查询还标有查询类型，这些查询类型指示它们中的每个类型是否与视频或字幕相关或两者都相关，也可以对数据集进行深入分析以及在其顶部构建的方法。应用严格的资格和后解析验证测试，以确保收集的数据的质量。此外，我们提出了几个基线和一个新型的跨模式力矩定位（XML）网络，用于多模式时刻检索任务。提出的XML模型使用新型的卷积起始探测器（Convse）使用晚期融合设计，以很大的边距和更高的效率超过基线，为将来的工作提供了强大的起点。我们还为TVR中的每个注释时刻收集了其他描述，以形成一个带有262K字幕的新的多模式字幕数据集，名为电视节目标题（TVC）。这两个数据集均可公开使用。 TVR：https：//tvr.cs.unc.edu，TVC：https：//tvr.cs.unc.edu/tvc.html。

### DRMIME: Differentiable Mutual Information and Matrix Exponential for Multi-Resolution Image Registration 
[[arxiv](https://arxiv.org/abs/2001.09865)] [[cool](https://papers.cool/arxiv/2001.09865)] [[pdf](https://arxiv.org/pdf/2001.09865)]
> **Authors**: Abhishek Nan,Matthew Tennant,Uriel Rubin,Nilanjan Ray
> **First submission**: 2020-01-27
> **First announcement**: 2020-01-28
> **comment**: Software: https://github.com/abnan/DRMIME
- **标题**: DRMIME：多分辨率图像注册的可区分相互信息和矩阵指数
- **领域**: 计算机视觉和模式识别
- **摘要**: 在这项工作中，我们提出了一种新颖的无监督图像注册算法。它是可区分的端到端，可用于多模式和单模式登记。这是使用共同信息（MI）作为度量的。这里的新颖性是，我们不使用近似MI的传统方法，而是使用称为Mine的神经估计器，并用矩阵指数为转换矩阵计算补充它。与最先进的图像注册工具箱中可用的标准算法相比，这将带来改进的结果。

### Multi-Modal Domain Adaptation for Fine-Grained Action Recognition 
[[arxiv](https://arxiv.org/abs/2001.09691)] [[cool](https://papers.cool/arxiv/2001.09691)] [[pdf](https://arxiv.org/pdf/2001.09691)]
> **Authors**: Jonathan Munro,Dima Damen
> **First submission**: 2020-01-27
> **First announcement**: 2020-01-28
> **comment**: Accepted to CVPR 2020 for an oral presentation
- **标题**: 多模式域的适应性，以识别细粒度的动作
- **领域**: 计算机视觉和模式识别
- **摘要**: 细粒度的动作识别数据集表现出环境偏见，其中多个视频序列是从有限数量的环境中捕获的。在一个环境中训练模型并在另一种环境中部署，导致由于不可避免的域移动而导致性能下降。无监督的域适应性（UDA）方法经常在源和目标域之间使用对抗训练。但是，这些方法尚未探讨每个域内视频的多模式性质。在这项工作中，除了对抗对准外，我们还利用了对UDA的自我监督的对应方式的对应关系。我们使用通常采用的两种用于动作识别的方式：RGB和光流，我们从大规模数据集Epic-Kitchens上测试了三个厨房的方法。我们表明，仅多模式的自我统治会平均将仅源培训的绩效提高2.4％。然后，我们将对抗性训练与多模式的自学训练相结合，这表明我们的方法的表现优于其他UDA方法的3％。

### Explaining with Counter Visual Attributes and Examples 
[[arxiv](https://arxiv.org/abs/2001.09671)] [[cool](https://papers.cool/arxiv/2001.09671)] [[pdf](https://arxiv.org/pdf/2001.09671)]
> **Authors**: Sadaf Gulshad,Arnold Smeulders
> **First submission**: 2020-01-27
> **First announcement**: 2020-01-28
> **comment**: arXiv admin note: substantial text overlap with arXiv:1910.07416, arXiv:1904.08279
- **标题**: 用计数器视觉属性和示例解释
- **领域**: 计算机视觉和模式识别
- **摘要**: 在本文中，我们旨在通过使用多模式信息来解释神经网络的决定。这是违反直觉属性和反视觉示例，当引入扰动样品时出现。与以前有关使用显着图，文本或视觉贴片来解释决策的工作不同，我们建议使用属性和反归因于属性，以及作为视觉解释的一部分示例和反例。当人类解释视觉决定时，他们倾向于通过提供属性和示例来做到这一点。因此，受到本文人类解释方式的启发，我们提供了基于属性的示例解释。此外，人类还倾向于通过添加反归因和反示例来解释未看到的内容来解释其视觉决策。我们在示例中介绍了定向的扰动，以观察哪些属性值在将示例分类为计数类中时会发生变化。这提供了直观的反归因和反例。我们对粗粒和细粒数据集进行的实验表明，属性提供了歧视和人为理解的直觉和违反直觉的解释。

### ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes 
[[arxiv](https://arxiv.org/abs/2001.10692)] [[cool](https://papers.cool/arxiv/2001.10692)] [[pdf](https://arxiv.org/pdf/2001.10692)]
> **Authors**: Charles R. Qi,Xinlei Chen,Or Litany,Leonidas J. Guibas
> **First submission**: 2020-01-29
> **First announcement**: 2020-01-30
> **comment**: No comments
- **标题**: Imvotenet：用图像投票在点云中增强3D对象检测
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于对点云的深度学习进展，3D对象检测已取得了很快的进步。最近的一些作品甚至显示了最先进的性能，并具有仅仅是点云输入（例如投票）。但是，点云数据具有固有的局限性。它们稀疏，缺乏颜色信息，并且经常会遇到传感器噪音。另一方面，图像具有高分辨率和丰富的质感。因此，它们可以补充点云提供的3D几何形状。然而，如何有效地使用图像信息来协助基于点云的检测仍然是一个悬而未决的问题。在这项工作中，我们在投票基础之上建立，并提出了一个3D检测架构，称为IMVotenet，专门用于RGB-D场景。 Imvotenet基于图像中的2D票和3D票的融合。与先前的多模式检测工作相比，我们从2D图像中明确提取几何和语义特征。我们利用相机参数将这些功能提升到3D。为了改善2D-3D功能融合的协同作用，我们还提出了一个较高的训练方案。我们在具有挑战性的Sun RGB-D数据集上验证了模型，将最新结果提高了5.7地图。我们还提供丰富的消融研究，以分析每个设计选择的贡献。

### Modality Compensation Network: Cross-Modal Adaptation for Action Recognition 
[[arxiv](https://arxiv.org/abs/2001.11657)] [[cool](https://papers.cool/arxiv/2001.11657)] [[pdf](https://arxiv.org/pdf/2001.11657)]
> **Authors**: Sijie Song,Jiaying Liu,Yanghao Li,Zongming Guo
> **First submission**: 2020-01-30
> **First announcement**: 2020-01-31
> **comment**: Accepted by IEEE Trans. on Image Processing, 2020. Project page: http://39.96.165.147/Projects/MCN_tip2020_ssj/MCN_tip_2020_ssj.html
- **标题**: 模式补偿网络：跨模式适应行动识别
- **领域**: 计算机视觉和模式识别
- **摘要**: 随着RGB-D摄像机的流行，多模式视频数据已更加可用于人类行动识别。这项任务的一个主要挑战在于如何有效利用其互补信息。在这项工作中，我们提出了一个模式补偿网络（MCN），以探索不同方式的关系，并提高人类行动识别的表示。我们将RGB/光流视频视为源模式，骨骼是辅助模式。我们的目标是借助辅助模式，从源方式中提取更多的判别特征。我们的模型建立在深度卷积神经网络（CNN）和长期短期记忆（LSTM）网络的基础上，通过模态适应性块从源和辅助模式中桥接数据，以实现自适应表示学习，该网络学会在测试时间甚至在训练时间及时补偿骨骼的损失。我们探索多种适应方案，以根据培训中的来源和辅助数据的比对来缩小不同级别的源和辅助模态分布之间的距离。此外，仅在训练阶段需要骨骼。测试时，我们的模型能够通过源数据提高识别性能。实验结果表明，在四个广泛使用的动作识别基准上，MCN胜过最先进的方法。

### Dual Convolutional LSTM Network for Referring Image Segmentation 
[[arxiv](https://arxiv.org/abs/2001.11561)] [[cool](https://papers.cool/arxiv/2001.11561)] [[pdf](https://arxiv.org/pdf/2001.11561)]
> **Authors**: Linwei Ye,Zhi Liu,Yang Wang
> **First submission**: 2020-01-30
> **First announcement**: 2020-01-31
> **comment**: 12 pages, accepted for publication in IEEE Transactions on Multimedia
- **标题**: 双卷积LSTM网络，用于引用图像分割
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们考虑参考图像分割。这是计算机视觉和自然语言理解的交集的问题。给定输入图像和以自然语言句子形式的引用表达式，目标是将语言查询所引用的图像中感兴趣的对象分割。为此，我们提出了双重卷积LSTM（ConvlstM）网络来解决此问题。我们的模型由一个编码器网络和解码器网络组成，该网络在编码器和解码器网络中都使用ConvlSTM来捕获空间和顺序信息。编码器网络在表达式句子中为每个单词的每个单词提取视觉和语言特征，并采用了注意机制，以专注于多模式相互作用中更有信息的单词。解码器网络集成了由编码器网络以多个级别作为输入而生成的功能，并产生最终的精确分割掩码。四个具有挑战性的数据集的实验结果表明，与其他最先进的方法相比，所提出的网络可实现出色的细分性能。

## 图形(cs.GR:Graphics)

该领域共有 1 篇论文

### Unsupervised multi-modal Styled Content Generation 
[[arxiv](https://arxiv.org/abs/2001.03640)] [[cool](https://papers.cool/arxiv/2001.03640)] [[pdf](https://arxiv.org/pdf/2001.03640)]
> **Authors**: Omry Sendik,Dani Lischinski,Daniel Cohen-Or
> **First submission**: 2020-01-10
> **First announcement**: 2020-01-13
> **comment**: No comments
- **标题**: 无监督的多模式样式的内容一代
- **领域**: 图形,机器学习
- **摘要**: 深层生成模型的出现最近使自动生成大量的图形内容，包括2D和3D。在这种情况下，生成的对抗网络（GAN）和样式控制机制（例如自适应实例归一化（ADAIN））已被证明在这种情况下特别有效，最终达到了最先进的样式架构。尽管这样的模型能够学习各种分布，但如果提供了足够大的培训集，但它们并不适合训练数据的分布表现出多模式行为的情况。在这种情况下，将潜在空间重塑或正态分布重塑到数据域中的复杂多模式分布非常具有挑战性，并且发电机可能无法很好地采样目标分布。此外，现有的无监督生成模型无法独立于其他视觉属性控制生成的样品的模式，尽管事实通常在培训数据中脱离了它们。在本文中，我们介绍了一种新颖的建筑，旨在以无监督的方式更好地建模多模式分布。在StyleGan体系结构的基础上，我们的网络以一种完全无监督的方式学习了多种模式，并使用一组学习的权重将它们结合在一起。我们证明，这种方法能够有效地将复杂分布作为多个简单的分布。我们进一步表明，ummgan有效地在模式和样式之间解开了，从而对生成的内容提供了独立的控制程度。

## 人机交互(cs.HC:Human-Computer Interaction)

该领域共有 4 篇论文

### Detecting depression in dyadic conversations with multimodal narratives and visualizations 
[[arxiv](https://arxiv.org/abs/2001.04809)] [[cool](https://papers.cool/arxiv/2001.04809)] [[pdf](https://arxiv.org/pdf/2001.04809)]
> **Authors**: Joshua Y. Kim,Greyson Y. Kim,Kalina Yacef
> **First submission**: 2020-01-13
> **First announcement**: 2020-01-15
> **comment**: 12 pages
- **标题**: 通过多模式叙事和可视化检测二元对话中的抑郁症
- **领域**: 人机交互,人工智能,计算语言学
- **摘要**: 对话包含各种各样的多模式信息，这些信息为我们提供了有关演讲者的情感和情绪的暗示。在本文中，我们开发了一个支持人类分析对话的系统。我们的主要贡献是识别适当的多模式特征，并将此类特征集成到逐字对话成绩单中。我们证明了系统获得广泛的多模式信息的能力，并自动为个体的抑郁状态生成了预测评分。我们的实验表明，这种方法的性能比基线模型更好。此外，多模式叙事方法使从其他学科（例如会话分析和心理学）中融合学习变得容易。最后，这种跨学科和自动化的方法是迈向模拟从业者如何记录治疗过程以及模拟对话分析师如何手工分析对话的一步。

### A Technology-aided Multi-modal Training Approach to Assist Abdominal Palpation Training and its Assessment in Medical Education 
[[arxiv](https://arxiv.org/abs/2001.05745)] [[cool](https://papers.cool/arxiv/2001.05745)] [[pdf](https://arxiv.org/pdf/2001.05745)]
> **Authors**: A. Asadipour,K. Debattista,V. Patel,A. Chalmers
> **First submission**: 2020-01-16
> **First announcement**: 2020-01-17
> **comment**: In Press
- **标题**: 技术辅助的多模式训练方法，以帮助腹部触诊训练及其医学教育评估
- **领域**: 人机交互,计算机视觉和模式识别,图像和视频处理
- **摘要**: 计算机辅助的多模式训练是在各种应用中学习复杂运动技能的有效方法。特别是在进行灵巧的动手检查（临床触诊）方面的学科（例如医疗保健）可能导致症状，严重伤害甚至死亡的误诊。此外，高质量的临床检查可以通过消除不必要的医学成像的需求来帮助排除重要的病理，并减少诊断时间和诊断成本。医疗触诊经常用作世界各地的一种有效的初步诊断方法，但是目前需要多年的培训才能获得能力。本文着重于多模式触诊训练系统，以教授和提高与腹部有关的临床检查技能。我们的目标是通过增加排练的频率，并提供有关如何执行从医疗专家捕获和建模的各种腹部触诊技术的基本增强反馈，从而显着缩短触诊训练持续时间。二十三名医学生分为对照组（n = 8），一个半vis训练的组（n = 8），并邀请了一个完全视觉训练的组（n = 7）执行三项触诊任务（表面，深层和肝脏）。使用基于计算机的方法和基于人类的方法评估了医学生的表演，在生成的分数之间显示了正相关，r = .62，p（单尾）<.05。受视觉训练的组显着胜过对照组，在对照组中，在每次触诊检查期间，向学生提供了对应用力及其手掌位置的抽象可视化（p <.05）。此外，当出现视觉反馈时，在组之间观察到了积极的趋势，j = 132，z = 2.62，r = 0.55。

### Deep Learning for Sensor-based Human Activity Recognition: Overview, Challenges and Opportunities 
[[arxiv](https://arxiv.org/abs/2001.07416)] [[cool](https://papers.cool/arxiv/2001.07416)] [[pdf](https://arxiv.org/pdf/2001.07416)]
> **Authors**: Kaixuan Chen,Dalin Zhang,Lina Yao,Bin Guo,Zhiwen Yu,Yunhao Liu
> **First submission**: 2020-01-21
> **First announcement**: 2020-01-22
> **comment**: No comments
- **标题**: 基于传感器的人类活动识别的深度学习：概述，挑战和机遇
- **领域**: 人机交互,机器学习
- **摘要**: 传感器设备和物联网的巨大扩散使基于传感器的活动识别的应用。但是，存在实质性的挑战，可能会影响实际情况下的识别系统的性能。最近，由于深度学习在许多领域都证明了其有效性，因此已经研究了许多深入的方法来应对活动识别的挑战。在这项研究中，我们介绍了基于传感器的人类活动识别的最新深度学习方法的调查。我们首先介绍感官数据的多模式，并为公共数据集提供信息，这些信息可用于在不同的挑战任务中进行评估。然后，我们提出了一种新的分类法，以通过挑战来构建深度方法。总结并分析了挑战和与挑战有关的深层方法，以概述当前的研究进度。在这项工作结束时，我们讨论了开放问题，并为将来的方向提供了一些见解。

### Gesticulator: A framework for semantically-aware speech-driven gesture generation 
[[arxiv](https://arxiv.org/abs/2001.09326)] [[cool](https://papers.cool/arxiv/2001.09326)] [[pdf](https://arxiv.org/pdf/2001.09326)]
> **Authors**: Taras Kucherenko,Patrik Jonell,Sanne van Waveren,Gustav Eje Henter,Simon Alexanderson,Iolanda Leite,Hedvig Kjellström
> **First submission**: 2020-01-25
> **First announcement**: 2020-01-27
> **comment**: ICMI 2020 Best Paper Award. Code is available. 9 pages, 6 figures
- **标题**: 手球手：语言意识到语音驱动的手势产生的框架
- **领域**: 人机交互,机器学习,音频和语音处理
- **摘要**: 在讲话中，人们自发地打手势，在传达信息中起着关键作用。同样，现实的共同语音手势对于实现与社会代理人的自然和平稳互动至关重要。当前的端到端共同语音手势生成系统使用单个模式来表示语音：音频或文本。因此，这些系统仅限于产生与声学链接的手势或语义链接手势（例如，在说“高”时举起一只手）：他们无法适当学会生成两种手势类型。我们提出了一种旨在共同产生任意节拍和语义手势的模型。我们基于深度学习的模型将语音的声学和语义表示为输入，并以输出为关节角度旋转的序列产生手势。所产生的手势可以应用于虚拟药物和人形机器人。主观和客观评估证实了我们方法的成功。代码和视频可在项目页面https://svito-zar.github.io/genticulator上获得。

## 机器学习(cs.LG:Machine Learning)

该领域共有 9 篇论文

### Dual Adversarial Domain Adaptation 
[[arxiv](https://arxiv.org/abs/2001.00153)] [[cool](https://papers.cool/arxiv/2001.00153)] [[pdf](https://arxiv.org/pdf/2001.00153)]
> **Authors**: Yuntao Du,Zhiwen Tan,Qian Chen,Xiaowen Zhang,Yirong Yao,Chongjun Wang
> **First submission**: 2020-01-01
> **First announcement**: 2020-01-02
> **comment**: No comments
- **标题**: 双重对手域的适应
- **领域**: 机器学习,机器学习
- **摘要**: 无监督的域适应性旨在将知识从标记的源域转移到未标记的目标域。以前的对抗域适应方法主要采用具有二进制或$ k $维输出的歧视器，以独立执行边际或条件对准。最近的实验表明，当歧视器在域中提供域信息和源域中的标签信息时，它可以保留两个域中的复杂的多模式信息和高语义信息。遵循这个想法，我们采用了一个具有$ 2K $维二维输出的歧视器，以同时在单个歧视器中同时执行域级别和类级对齐。但是，单个歧视者无法捕获跨域之间的所有有用信息，并且示例与决策边界之间的关系很少探索。受域名适应的多视图学习和最新进展的启发，除了歧视器和特征提取器之间的对抗过程外，我们还设计了一种新颖的机制来使两个歧视器相互对抗，以便它们可以互相提供多种信息，并避免在源域之外产生目标特征。据我们所知，这是第一次探索领域适应性的双重对立策略。此外，我们还使用半监督的学习正则化来使表示形式更具歧视性。在两个现实世界数据集上的全面实验验证了我们的方法是否优于几种最新的域适应方法。

### Self-Supervised Learning of Generative Spin-Glasses with Normalizing Flows 
[[arxiv](https://arxiv.org/abs/2001.00585)] [[cool](https://papers.cool/arxiv/2001.00585)] [[pdf](https://arxiv.org/pdf/2001.00585)]
> **Authors**: Gavin S. Hartnett,Masoud Mohseni
> **First submission**: 2020-01-02
> **First announcement**: 2020-01-03
> **comment**: 16 pages, 7 figures
- **标题**: 自学对生成旋转玻璃的学习，并具有归一化流量
- **领域**: 机器学习,无序系统和神经网络,量子物理学,机器学习
- **摘要**: 旋转玻璃是通用模型，可以在统计物理和计算机科学的界面上捕获多体系统的复杂行为，包括离散优化，图形模型中的推断以及自动推理。由于其状态空间的组合爆炸，计算这种复杂系统的基本结构和动力学非常困难。在这里，我们开发了深层生成的连续旋转玻璃分布，并具有标准化流程，以模拟通用离散问题中的相关性。我们通过自动从旋转玻璃本身生成数据来使用自我监督的学习范式。我们证明，可以成功地学习旋转玻璃相的关键物理和计算特性，包括亚稳态状态之间的多模式稳态分布和拓扑结构。值得注意的是，我们观察到学习本身对应于训练有素的归一化流动层中的旋转玻璃相变。逆归一化流量学会执行可逆的多尺度粗粒操作，这与典型的不可逆重新规范化组技术截然不同。

### Meta-modal Information Flow: A Method for Capturing Multimodal Modular Disconnectivity in Schizophrenia 
[[arxiv](https://arxiv.org/abs/2001.01707)] [[cool](https://papers.cool/arxiv/2001.01707)] [[pdf](https://arxiv.org/pdf/2001.01707)]
> **Authors**: Haleh Falakshahi,Victor M. Vergara,Jingyu Liu,Daniel H. Mathalon,Judith M. Ford,James Voyvodic,Bryon A. Mueller,Aysenil Belger,Sarah McEwen,Steven G. Potkin,Adrian Preda,Hooman Rokham,Jing Sui,Jessica A. Turner,Sergey Plis,Vince D. Calhoun
> **First submission**: 2020-01-06
> **First announcement**: 2020-01-07
> **comment**: ef:IEEE Transactions on Biomedical Engineering, 2019
- **标题**: 元模式信息流：一种捕获精神分裂症中多模块化学连接性的方法
- **领域**: 机器学习,图像和视频处理,机器学习
- **摘要**: 目的：相同现象的多模式测量提供了互补的信息，并突出了不同的观点，尽管每个观点都有自己的局限性。对单个方式的关注可能会导致不正确的推论，当研究现象是一种疾病时，这一点尤其重要。在本文中，我们介绍了一种利用多模式数据来解决精神分裂症（SZ）中脱节性和功能障碍的假设的方法。方法：我们从使用高斯图形模型（GGM）内外估算和可视化链接开始。然后，我们提出了一种基于模块化的方法，该方法可以应用于GGM，以确定在多模式数据集中与精神疾病相关的链接。通过模拟和真实数据，我们显示了我们的方法揭示了有关与疾病相关的网络中断的重要信息，这些信息以单一方式遗漏了。我们使用功能性MRI（FMRI），扩散MRI（DMRI）和结构MRI（SMRI）来计算低频波动（FALFF），分数各向异性（FA）和灰质（GM）浓度图的分数振幅。使用我们的模块化方法分析了这三种方式。结果：我们的结果显示缺少链接，这些链接仅由跨模式信息捕获，这些信息可能在组件之间的断开性中起重要作用。结论：我们在SZ患者的默认模式网络区域中确定了多模式（Falff，FA和GM）的断​​开性，这在单个模态中无法检测到。意义：所提出的方法为捕获在多个成像方式之间分布的信息提供了重要的新工具。

### Lifted Hybrid Variational Inference 
[[arxiv](https://arxiv.org/abs/2001.02773)] [[cool](https://papers.cool/arxiv/2001.02773)] [[pdf](https://arxiv.org/pdf/2001.02773)]
> **Authors**: Yuqiao Chen,Yibo Yang,Sriraam Natarajan,Nicholas Ruozzi
> **First submission**: 2020-01-08
> **First announcement**: 2020-01-09
> **comment**: AAAI 2020 Workshop on Statistical Relational AI (StarAI 2020)
- **标题**: 提起的混合变量推断
- **领域**: 机器学习,机器学习
- **摘要**: 已经提出了各种提升的推理算法，这些推理算法利用模型对称性来降低计算成本，以在概率关系模型中呈现推理。大多数现有的提起的推理算法仅在离散域或具有限制潜在功能的连续域上运行，例如高斯。我们研究了一种适用于混合域的两种近似抬高的变分方法，并且表现力足以捕获多模式。我们证明，即使在存在大量连续证据的情况下，提出的变分方法既可以扩展，又可以利用近似模型对称性。我们证明，我们的方法与在各种环境中的现有基于消息的方法进行了有利的比较。最后，我们为伯特近似提供了足够的条件，可以在边缘多层室上产生非平凡的估计值。

### Deep Image Translation with an Affinity-Based Change Prior for Unsupervised Multimodal Change Detection 
[[arxiv](https://arxiv.org/abs/2001.04271)] [[cool](https://papers.cool/arxiv/2001.04271)] [[pdf](https://arxiv.org/pdf/2001.04271)]
> **Authors**: Luigi Tommaso Luppino,Michael Kampffmeyer,Filippo Maria Bianchi,Gabriele Moser,Sebastiano Bruno Serpico,Robert Jenssen,Stian Normann Anfinsen
> **First submission**: 2020-01-13
> **First announcement**: 2020-01-14
> **comment**: No comments
- **标题**: 深层图像翻译，具有基于亲和力的更改，用于无监督的多模式更改检测
- **领域**: 机器学习,计算机视觉和模式识别,图像和视频处理,机器学习
- **摘要**: 带有卷积神经网络的图像翻译最近已被用作多模式变化检测的一种方法。现有方法通过利用变更领域的监督信息来训练网络，但是并非总是可用。无监督问题设置的主要挑战是避免变化像素会影响翻译功能的学习。我们提出了两个新的网络体系结构，该架构训练有损失功能，这些功能由先验加权，以减少变更像素对学习目标的影响。先验的变化是以无监督的方式得出的，该方式是由域特异性亲和力矩阵捕获的关系像素信息。具体而言，我们使用与绝对亲和力差异矩阵相关的顶点度，并证明了它们与周期一致性和对抗性训练相结合的效用。将提出的神经网络与最新算法进行比较。在三个真实数据集上进行的实验显示了我们方法论的有效性。

### SGLB: Stochastic Gradient Langevin Boosting 
[[arxiv](https://arxiv.org/abs/2001.07248)] [[cool](https://papers.cool/arxiv/2001.07248)] [[pdf](https://arxiv.org/pdf/2001.07248)]
> **Authors**: Aleksei Ustimenko,Liudmila Prokhorenkova
> **First submission**: 2020-01-20
> **First announcement**: 2020-01-21
> **comment**: No comments
- **标题**: SGLB：随机梯度Langevin提升
- **领域**: 机器学习,机器学习
- **摘要**: 本文介绍了随机梯度Langevin Boosting（SGLB） - 一个强大而有效的机器学习框架，可以处理广泛的损失功能，并具有可证明的概括保证。该方法基于特殊形式的langevin扩散方程，专门为梯度增强而设计。这使我们从理论上保证了全局收敛，即使是多模式损耗函数，而标准梯度增强算法也只能保证本地最佳。我们还从经验上表明，当应用于具有0-1损耗函数的分类任务时，SGLB的表现优于经典梯度的提升，这是多模式的。

### Expected Information Maximization: Using the I-Projection for Mixture Density Estimation 
[[arxiv](https://arxiv.org/abs/2001.08682)] [[cool](https://papers.cool/arxiv/2001.08682)] [[pdf](https://arxiv.org/pdf/2001.08682)]
> **Authors**: Philipp Becker,Oleg Arenz,Gerhard Neumann
> **First submission**: 2020-01-23
> **First announcement**: 2020-01-24
> **comment**: No comments
- **标题**: 预期信息最大化：使用I-Procotion进行混合密度估计
- **领域**: 机器学习,机器学习
- **摘要**: 建模高度多模式数据是机器学习中的一个具有挑战性的问题。大多数算法是基于最大化的可能性，该可能性对应于M（Oment） - 对模型分布的数据分布的预测。 M细投影迫使模型在无法表示的模式下平均。相比之下，I（信息） - 投影忽略了数据中的这些模式，而将重点放在模型所代表的模式上。每当我们处理高度多模式的数据时，这种行为就会吸引人，其中正确对单个模式进行建模比涵盖所有模式更为重要。尽管存在这一优势，但由于缺乏可以根据数据有效优化它的算法，因此很少使用I投票。在这项工作中，我们提出了一种称为预期信息最大化（EIM）的新算法，用于仅基于一般潜在变量模型的样本来计算I-trokostion，我们将重点介绍高斯混合模型和专家的高斯混合物。我们的方法应用了一个与i投影目标的变异上限，该目标将原始目标分解为每个混合物组件以及系数的单个目标，从而有效地优化了。与GAN相似，我们的方法采用歧视器，但使用紧密的上限使用更稳定的优化程序。我们表明，与最近的GAN方法相比，我们的算法在计算I投票方面更有效，我们说明了对两个行人和交通预测数据集建模多模式行为的方法的有效性。

### Multimodal Data Fusion based on the Global Workspace Theory 
[[arxiv](https://arxiv.org/abs/2001.09485)] [[cool](https://papers.cool/arxiv/2001.09485)] [[pdf](https://arxiv.org/pdf/2001.09485)]
> **Authors**: Cong Bao,Zafeirios Fountas,Temitayo Olugbade,Nadia Bianchi-Berthouze
> **First submission**: 2020-01-26
> **First announcement**: 2020-01-27
> **comment**: 12 pages, 5 figures
- **标题**: 基于全球工作空间理论的多模式数据融合
- **领域**: 机器学习,机器学习
- **摘要**: 我们提出了一种新颖的神经网络体系结构，称为全球工作区网络（GWN），该架构解决了多模式数据融合中动态和未指定不确定性的挑战。我们的GWN是跨时间和随时间发展的一种关注模型，并受到认知科学领域良好的全球工作空间理论的启发。 GWN的平均F1得分为疼痛患者和健康参与者的歧视得分为0.92，F1得分的平均F1得分= 0.75，用于对患者进行三个疼痛水平的进一步分类，这都是基于从患有慢性疼痛的人中捕获的多模态emopain数据集，而健康的人在无约束的设置中执行不同类型的运动运动。在这些任务中，GWN显着优于通过串联合并的典型融合方法。我们进一步提供了GWN的行为及其在多模式数据中解决不确定性（隐藏噪声）的能力的广泛分析。

### Urban2Vec: Incorporating Street View Imagery and POIs for Multi-Modal Urban Neighborhood Embedding 
[[arxiv](https://arxiv.org/abs/2001.11101)] [[cool](https://papers.cool/arxiv/2001.11101)] [[pdf](https://arxiv.org/pdf/2001.11101)]
> **Authors**: Zhecheng Wang,Haoyuan Li,Ram Rajagopal
> **First submission**: 2020-01-29
> **First announcement**: 2020-01-30
> **comment**: To appear in Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)
- **标题**: Urban2VEC：将街道视图图像和POI纳入多模式城市社区嵌入
- **领域**: 机器学习,计算机视觉和模式识别,机器学习
- **摘要**: 了解固有的模式和预测城市的时空特征需要全面代表城市社区。现有作品依靠区域内或区域内连接性来生成邻里表示形式，但未能充分利用社区内的信息丰富但异质的数据。在这项工作中，我们提出了Urban2Vec，这是一个无监督的多模式框架，其中包含街道视图图像和利益点（POI）数据以学习邻里的嵌入。具体来说，我们使用卷积神经网络从街道视图图像中提取视觉特征，同时保持地理空间相似性。此外，我们将每个POI建模为包含其类别，评分和审查信息的单词袋。类似于文档嵌入自然语言处理中的模拟，我们建立了邻域（“文档”）与矢量空间中周围pois的单词之间的语义相似性。通过将视觉，文本和地理空间信息共同编码在邻里表示中，Urban2VEC可以比基线模型更好地实现性能，并且可以与下游预测任务中的完全监督的方法相媲美。在美国三个大都市地区进行的广泛实验也证明了模型的解释性，概括能力及其在邻里相似性分析中的价值。

## 多媒体(cs.MM:Multimedia)

该领域共有 1 篇论文

### NAViDAd: A No-Reference Audio-Visual Quality Metric Based on a Deep Autoencoder 
[[arxiv](https://arxiv.org/abs/2001.11406)] [[cool](https://papers.cool/arxiv/2001.11406)] [[pdf](https://arxiv.org/pdf/2001.11406)]
> **Authors**: Helard Martinez,M. C. Farias,A. Hines
> **First submission**: 2020-01-30
> **First announcement**: 2020-01-31
> **comment**: 5 pages
- **标题**: NAVIDAD：基于深度自动编码器的无参考音频质量度量
- **领域**: 多媒体,机器学习,图像和视频处理
- **摘要**: 开发用于音频和视频信号质量预测的模型是一个相当成熟的领域。但是，尽管已经提出了几种多模型模型，但视听质量预测的领域仍然是新兴领域。实际上，尽管通过组合和参数指标获得了合理的性能，但目前尚无基于可靠的像素的视听质量指标。这项工作中介绍的方法基于以下假设：自动编码器以描述性音频和视频功能喂养，可能会产生一组能够描述复杂音频和视频交互的功能。基于这一假设，我们提出了基于深度自动编码器（NAVIDAD）的无参考音频质量度量。模型视觉特征是视频组件的自然场景统计（NSS）和时空测量。同时，通过计算音频组件的频谱图表示获得音频功能。该模型由一个2层框架形成，其中包括深度自动编码层和分类层。这两个层是堆叠和训练以建立深神经网络模型的。使用大量刺激训练和测试该模型，其中包含代表性的音频和视频伪影。当针对UNB-AV和Livenetflix-II数据库进行测试时，该模型表现良好。 ％的结果表明，这种方法会产生与主观质量评分高度相关的质量评分。

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

该领域共有 2 篇论文

### On the Performance of Metaheuristics: A Different Perspective 
[[arxiv](https://arxiv.org/abs/2001.08928)] [[cool](https://papers.cool/arxiv/2001.08928)] [[pdf](https://arxiv.org/pdf/2001.08928)]
> **Authors**: Hamid Reza Boveiri,Raouf Khayami
> **First submission**: 2020-01-24
> **First announcement**: 2020-01-27
> **comment**: Version 0.1: 16 Pages, 5 Figures, and 7 Tables
- **标题**: 关于元启发式的表现：不同的观点
- **领域**: 神经和进化计算,人工智能
- **摘要**: 如今，我们沉浸在数十种新的进化和游泳智能术中，这使得很难选择适当的方法来应用于手头的特定优化问题。另一方面，大多数这些元启发式学不过是基本元启发术的略微修改的变体。例如，分别具有专门操作员或额外的局部搜索的差异进化（DE）或洗牌青蛙跳跃（SFL）只是遗传算法（GA）。因此，想到的是，是否可以根据研究其祖先的规范和特征来研究这种新宣言的行为。在本文中，一项关于一些基本元启发学的全面评估研究，即遗传算法（GA），粒子群优化（PSO），人造蜜蜂菌落（ABC），基于教学学习的优化（TLBO）（TLBO）和杜鹃优化算法（COA），这使我们能够更好地实现效果，以使我们能够进行更深入的效果，因此，我们将为您提供其他效果，因此，我们将效果效果，因此，我们将为您提供其他效果，因此，我们的效果将为我们提供，因此，我们将为您提供效果。变化起源于它们。已经对具有不同特征的20种不同组合优化基准函数进行了大量实验，并且结果向我们揭示了这些基本结论以外的一些基本结论，在这些元数据中，{ABC，PSO，PSO，TLBO，GA，GA，COA，COA} I.E. ABC和COA和COA和COA和COA是最佳和最差的方法。此外，从收敛的角度来看，PSO和ABC分别对单峰和多模式功能具有明显的更好的收敛性，而在许多情况下，GA和COA与局部Optima具有过早的融合，在许多情况下需要替代突变机制来增强多元化和全局搜索。

### Scalable and Customizable Benchmark Problems for Many-Objective Optimization 
[[arxiv](https://arxiv.org/abs/2001.11591)] [[cool](https://papers.cool/arxiv/2001.11591)] [[pdf](https://arxiv.org/pdf/2001.11591)]
> **Authors**: Ivan Reinaldo Meneghini,Marcos Antonio Alves,António Gaspar-Cunha,Frederico Gadelha Guimarães
> **First submission**: 2020-01-26
> **First announcement**: 2020-01-31
> **comment**: 24 pages, 23 figures, to be published in Applied Soft computing
- **标题**: 可扩展且可自定义的基准问题，用于多个目标优化
- **领域**: 神经和进化计算,人工智能
- **摘要**: 解决多目标问题（MAOPS）在多目标优化（MOO）字段中仍然是一个重大挑战。测量算法性能的一种方法是通过使用基准功能（也称为测试功能或测试套件），这是具有定义明确的数学公式，已知解决方案以及各种功能和困难的人为问题。在本文中，我们为MAOPS提出了一个可扩展和可自定义基准问题的参数化发电机。它能够产生问题，这些问题可以重现其他基准中存在的功能，并且还具有一些新功能的问题。我们在这里提出了生成基准测试的概念，其中人们可以通过控制问题应具有的特定特征的参数来生成无限数量的MOO问题：变量和目标数量的可伸缩性，偏见，欺骗性，多模态，可靠性和非bust型和非态度的解决方案，Pareto前和约束的形状。拟议的广义位置距离（GPD）可调基准生成器使用位置距离范式，这是一种用于建筑测试功能的基本方法，用于其他基准，例如Deb，Thiele，Laumanns和Zitzler（DTLZ），步行鱼类组（WFG）等。它在任何数量的变量和目标中都包含可扩展的问题，并且呈现具有不同特征的帕累托前沿。最终的功能易于理解和可视化，易于实现，快速计算，并且他们的帕累托最佳解决方案是已知的。

## 机器人技术(cs.RO:Robotics)

该领域共有 1 篇论文

### Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless Approach 
[[arxiv](https://arxiv.org/abs/2001.06175)] [[cool](https://papers.cool/arxiv/2001.06175)] [[pdf](https://arxiv.org/pdf/2001.06175)]
> **Authors**: Chanoh Park,Peyman Moghadam,Soohwan Kim,Sridha Sridharan,Clinton Fookes
> **First submission**: 2020-01-17
> **First announcement**: 2020-01-20
> **comment**: 8 pages, To appear, IEEE Robotics and Automation Letters 2020
- **标题**: 时空摄像头校准：一种无目标且无构造的方法
- **领域**: 机器人技术,计算机视觉和模式识别
- **摘要**: 由于这些系统提供的鲁棒性，可靠性和准确性的提高，对机器人技术的多模式传感系统的需求正在增长。这些系统还需要在空间和时间上共同注册才能有效。在本文中，我们提出了一种无目标和无结构的时空摄像机校准方法。我们的方法将封闭形式的解决方案与修改的无结构束调节结合在一起，在该捆绑包中，粗到细节的方法没有对时空参数进行初始猜测。同样，由于仅根据三角剖分计算3D特征（结构），因此无需具有校准目标或将2D特征与3D点云匹配，该功能在校准过程和传感器配置中提供了灵活性。我们使用安装在手持，空中和腿部机器人的机器人系统上的多个传感器有效载荷配置来证明所提出方法的准确性和鲁棒性。同样，定性结果以色彩点云可视化的形式给出。

## 声音(cs.SD:Sound)

该领域共有 1 篇论文

### Scattering Features for Multimodal Gait Recognition 
[[arxiv](https://arxiv.org/abs/2001.08830)] [[cool](https://papers.cool/arxiv/2001.08830)] [[pdf](https://arxiv.org/pdf/2001.08830)]
> **Authors**: Srđan Kitić,Gilles Puy,Patrick Pérez,Philippe Gilberton
> **First submission**: 2020-01-23
> **First announcement**: 2020-01-24
> **comment**: Published at IEEE GlobalSIP 2017
- **标题**: 多模式步态识别的散射功能
- **领域**: 声音,机器学习,音频和语音处理,信号处理
- **摘要**: 我们考虑了人们根据步行（步态）模式来识别人们的问题。解决此问题的经典方法基于嵌入地板中的视频录制或压电传感器。在这项工作中，我们分别依赖于从麦克风和地球管传感器获得的声学和振动测量值。这项工作的贡献是双重的。首先，我们提出了一种基于（未经训练的）浅散射网络的特征提取方法，该方法是专门针对步态信号量身定制的。其次，我们证明融合两种方式可以改善实际相关的开放场景方案的识别。

## 社交和信息网络(cs.SI:Social and Information Networks)

该领域共有 1 篇论文

### Mining Disinformation and Fake News: Concepts, Methods, and Recent Advancements 
[[arxiv](https://arxiv.org/abs/2001.00623)] [[cool](https://papers.cool/arxiv/2001.00623)] [[pdf](https://arxiv.org/pdf/2001.00623)]
> **Authors**: Kai Shu,Suhang Wang,Dongwon Lee,Huan Liu
> **First submission**: 2020-01-02
> **First announcement**: 2020-01-03
> **comment**: Submitted as an introductory chapter for the edited book on "Fake News, Disinformation, and Misinformation in Social Media- Emerging Research Challenges and Opportunities", Springer Press
- **标题**: 采矿虚假信息和虚假新闻：概念，方法和最新进步
- **领域**: 社交和信息网络,计算语言学
- **摘要**: 近年来，包括假新闻在内的虚假信息由于其爆炸性增长，尤其是在社交媒体上，已成为一种全球现象。虚假信息和虚假新闻的广泛传播可能会导致有害的社会影响。尽管最近在检测虚假信息和虚假新闻方面取得了进展，但由于其复杂性，多样性，多模式和事实检查或注释的成本，它仍然并非平凡。本章的目的是通过以下方式欣赏挑战和进步的道路：（1）在社交媒体上介绍信息障碍的类型并检查其差异和联系； （2）描述重要和新兴的任务，以打击虚假信息以进行表征，检测和归因； （3）讨论使用有限的标记数据来检测虚假信息的弱监督方法。然后，我们提供了本书中章节的概述，该章节代表了三个相关部分的最新进展：（1）信息障碍传播的用户参与； （2）检测和减轻虚假信息的技术； （3）诸如道德，区块链，点击招标等问题的趋势问题。我们希望这本书成为研究人员，从业人员和学生的便捷切入点，以了解问题和挑战，学习针对其特定需求的最新解决方案，并快速在其领域中确定新的研究问题。

## 图像和视频处理(eess.IV:Image and Video Processing)

该领域共有 3 篇论文

### Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs 
[[arxiv](https://arxiv.org/abs/2001.02040)] [[cool](https://papers.cool/arxiv/2001.02040)] [[pdf](https://arxiv.org/pdf/2001.02040)]
> **Authors**: Andriy Myronenko,Ali Hatamizadeh
> **First submission**: 2020-01-06
> **First announcement**: 2020-01-07
> **comment**: Accepted to 2019 International MICCAI Brainlesion Workshop --MultimodalBrain Tumor Segmentation Challenge (BraTS) 2019. arXiv admin note: substantial text overlap with arXiv:1810.11654
- **标题**: 从3D MRI的脑肿瘤区域的稳健语义分割
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 多模式脑肿瘤分割挑战（BRAT）将研究人员汇集在一起​​，以改善3D MRI脑肿瘤分割的自动化方法。肿瘤分割是诊断和治疗疾病计划所必需的基本视觉任务之一。由于现代GPU的出现，前几年获胜的方法都是基于深度学习的，这些方法可以快速优化深卷积神经网络体系结构。在这项工作中，我们探讨了3D语义细分的最佳实践，包括常规编码器架构以及合并的损失功能，以进一步提高细分精度。我们评估了Brats 2019挑战的方法。

### CHAOS Challenge -- Combined (CT-MR) Healthy Abdominal Organ Segmentation 
[[arxiv](https://arxiv.org/abs/2001.06535)] [[cool](https://papers.cool/arxiv/2001.06535)] [[pdf](https://arxiv.org/pdf/2001.06535)]
> **Authors**: A. Emre Kavur,N. Sinem Gezer,Mustafa Barış,Sinem Aslan,Pierre-Henri Conze,Vladimir Groza,Duc Duy Pham,Soumick Chatterjee,Philipp Ernst,Savaş Özkan,Bora Baydar,Dmitry Lachinov,Shuo Han,Josef Pauli,Fabian Isensee,Matthias Perkonigg,Rachana Sathish,Ronnie Rajan,Debdoot Sheet,Gurbandurdy Dovletov,Oliver Speck,Andreas Nürnberger,Klaus H. Maier-Hein,Gözde Bozdağı Akar,Gözde Ünal, et al. (2 additional authors not shown)
> **First submission**: 2020-01-17
> **First announcement**: 2020-01-20
> **comment**: 23 pages, 11 tables, 9 figures
- **标题**: 混乱挑战 - 合并（CT-MR）健康腹部器官分割
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 多年来，腹部器官的细分一直是一个全面但尚未解决的研究领域。在过去的十年中，深度学习的密集发展（DL）引入了新的最新细分系统。为了扩展这些主题的知识，与IEEE国际生物医学成像研讨会（ISBI）结合组织了混乱（CT -MR）健康的腹部器官分割挑战（ISBI），2019年，在意大利的威尼斯。混乱提供了来自健康受试者的腹部CT和MR数据，用于单腹和多个腹部器官分割。已经设计了五个不同但互补的任务，以从多个角度分析当前方法的功能。与手动注释和互动方法相比，对结果进行了彻底研究。 The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 $\pm$ 0.00 / 0.95 $\pm$ 0.01) but the best MSSD performance remain limited (21.89 $\pm$ 13.94 / 20.85 $\pm$ 10.63 mm).参与模型的表演对于肝脏的跨模式任务显着降低（骰子：0.88 $ \ pm $ 0.15 MSSD：36.33 $ \ pm $ \ pm $ 21.97毫米）和所有器官（骰子：0.85 $ \ $ 0.21 MSSD：0.21 MSSD：33.17 $ \ pm PM $ \ $ 38.93毫米）。尽管在不同的应用程序上进行了相反的示例，但与特定于器官特定的器官相比，旨在分割所有器官的多任务DL模型似乎更差（性能下降约为5 \％）。此外，这种进一步研究的跨模式分割的方向将显着支持现实世界中的临床应用。此外，本文拥有1500多名参与者，另一个重要的贡献是对挑战组织的缺点的分析，例如多种提交和窥视现象的影响。

### Multimodal Deep Unfolding for Guided Image Super-Resolution 
[[arxiv](https://arxiv.org/abs/2001.07575)] [[cool](https://papers.cool/arxiv/2001.07575)] [[pdf](https://arxiv.org/pdf/2001.07575)]
> **Authors**: Iman Marivani,Evaggelia Tsiligianni,Bruno Cornelis,Nikos Deligiannis
> **First submission**: 2020-01-21
> **First announcement**: 2020-01-22
> **comment**: No comments
- **标题**: 多模式的深度展开，用于指导图像超分辨率
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 在低分辨率观察中，高分辨率图像的重建是成像中的一个逆问题。深度学习方法依靠培训数据来学习从低分辨率输入到高分辨率输出的端到端映射。与不包含有关该问题的域知识的现有深层多模型不同，我们提出了一种多模式深度学习设计，该设计结合了稀疏的先验，并允许从另一个图像模式中有效地集成信息到网络体系结构中。我们的解决方案依靠一个新颖的深层展开操作员，执行类似于迭代算法的步骤，用于卷积稀疏编码，并带有侧面信息。因此，提出的神经网络可通过设计解释。深度展开的体系结构用作指导图像超分辨率的多模式框架的核心组成部分。通过采用剩余学习来提高训练效率来研究替代的多模式设计。提出的多模式方法应用于近红外和多光谱图像的超分辨率，以及使用RGB图像作为附带信息的深度进行采样。实验结果表明，我们的模型优于最先进的方法。

## 神经元和认知(q-bio.NC:Neurons and Cognition)

该领域共有 1 篇论文

### Investigating naturalistic hand movements by behavior mining in long-term video and neural recordings 
[[arxiv](https://arxiv.org/abs/2001.08349)] [[cool](https://papers.cool/arxiv/2001.08349)] [[pdf](https://arxiv.org/pdf/2001.08349)]
> **Authors**: Satpreet H. Singh,Steven M. Peterson,Rajesh P. N. Rao,Bingni W. Brunton
> **First submission**: 2020-01-22
> **First announcement**: 2020-01-23
> **comment**: No comments
- **标题**: 通过长期视频和神经记录中的行为挖掘来调查自然主义的手动运动
- **领域**: 神经元和认知,计算机视觉和模式识别,图像和视频处理
- **摘要**: 大脑记录和人工智能方面的最新技术进步正在推动神经科学的新范式超出传统受控实验。自然主义神经科学研究的神经过程并没有专注于提示，重复的试验，而是在无约束的环境中进行的自发行为的神经过程。但是，分析缺乏先验实验设计的这种非结构化数据仍然是一个重大挑战，尤其是当数据是多模式和长期时。在这里，我们描述了一种自动化方法，用于分析同时记录长期自然主义电视学（ECOG）和自然主义行为视频数据的方法。我们采用行为优先的方法来分析长期记录。使用计算机视觉，离散的潜在变量建模以及行为视频数据上的字符串模式匹配的组合，我们发现并注释了自发的人类上LIMB运动事件。我们显示了我们的方法适用于每个受试者在7--9天内收集的12个受试者的数据的结果。我们的管道在行为视频中发现并注释了40,000多个自然主义人类上行运动事件的实例。对同时记录的大脑数据的分析揭示了运动的神经特征，从传统受控实验中证实了先前的发现。我们还针对运动启动检测任务的解码器制作了解码器，以证明管道的功效，作为用于脑部计算机接口应用程序的训练数据的来源。我们的工作解决了研究自然主义人类行为的独特数据分析挑战，并贡献了可能推广到ECOG以外的其他神经记录方式的方法。我们公开发布了我们的策划数据集，提供了一种资源来研究自然主义的神经和行为变异性，以先前不可用的规模。

## 其他论文

共有 6 篇其他论文

- [Re-synchronization using the Hand Preceding Model for Multi-modal Fusion in Automatic Continuous Cued Speech Recognition](https://arxiv.org/abs/2001.00854)
  - **标题**: 在自动连续提示语音识别中使用手动融合的手部模型重新同步进行多模式融合
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Commonly Knowing Whether](https://arxiv.org/abs/2001.03945)
  - **标题**: 通常知道是否
  - **Filtered Reason**: none of cs.LO in whitelist
- [InChorus: Designing Consistent Multimodal Interactions for Data Visualization on Tablet Devices](https://arxiv.org/abs/2001.06423)
  - **标题**: INCENDORUS：设计一致的多模式相互作用以在平板电脑设备上可视化
  - **Filtered Reason**: none of cs.HC in whitelist
- [Nowhere to Hide: Cross-modal Identity Leakage between Biometrics and Devices](https://arxiv.org/abs/2001.08211)
  - **标题**: 无处可隐藏：生物识别技术和设备之间的跨模式身份泄漏
  - **Filtered Reason**: none of cs.CR,cs.CY in whitelist
- [Safe Robot Navigation via Multi-Modal Anomaly Detection](https://arxiv.org/abs/2001.07934)
  - **标题**: 安全机器人通过多模式异常检测导航
  - **Filtered Reason**: none of cs.RO in whitelist
- [Visuohaptic augmented feedback for enhancing motor skills acquisition](https://arxiv.org/abs/2001.11401)
  - **标题**: Visuohaptic增强反馈，以增强运动技能的获取
  - **Filtered Reason**: none of eess.SP,cs.HC in whitelist
