# 2022-01 月度论文分类汇总

共有112篇相关领域论文, 另有17篇其他

## 人工智能(cs.AI:Artificial Intelligence)

该领域共有 6 篇论文

### ExAID: A Multimodal Explanation Framework for Computer-Aided Diagnosis of Skin Lesions 
[[arxiv](https://arxiv.org/abs/2201.01249)] [[cool](https://papers.cool/arxiv/2201.01249)] [[pdf](https://arxiv.org/pdf/2201.01249)]
> **Authors**: Adriano Lucieri,Muhammad Naseer Bajwa,Stephan Alexander Braun,Muhammad Imran Malik,Andreas Dengel,Sheraz Ahmed
> **First submission**: 2022-01-04
> **First announcement**: 2022-01-05
> **comment**: Accepted for publication in Computer Methods and Programs in Biomedicine
- **标题**: Exaid：用于计算机辅助诊断皮肤病变的多模式解释框架
- **领域**: 人工智能,机器学习,图像和视频处理
- **摘要**: 在临床工作流程中成功部署基于AI的计算机辅助诊断（CAD）系统的主要障碍是他们缺乏透明的决策。尽管常用的可解释的AI方法提供了对不透明算法的一些见解，但是除了受过训练有素的专家外，这种解释通常是令人费解的，并且不容易理解。关于皮肤镜图像的皮肤病变恶性肿瘤的决定的解释需要特别清晰，因为基本的医学问题定义本身是模棱两可的。这项工作介绍了Exaid（可解释的AI皮肤病学），这是一个用于生物医学图像分析的新型框架，提供了基于多模式概念的解释，包括易于理解的文本解释，并通过视觉地图补充，证明了预测的合理性。 EXAID依靠概念激活向量将人类概念映射到潜在空间中的任意深度学习模型和概念本地化图中所学的概念，以突出输入空间中的概念。然后，对相关概念的这种识别被用来构建以概念的位置信息补充的细颗粒文本解释，以提供全面，连贯的多模式解释。所有信息均在诊断界面中全面显示，以用于临床例程。教育模式提供数据集级别的解释统计数据和数据和模型探索的工具，以帮助医学研究和教育。通过对EXAID的严格定量和定性评估，我们即使在错误的预测中也显示了对CAD辅助方案的多模式解释的实用性。我们认为，Exaid将为皮肤科医生提供一种有效的筛查工具，它们既可以理解又信任。此外，这将是其他生物医学成像领域中类似应用的基础。

### CLUE: Contextualised Unified Explainable Learning of User Engagement in Video Lectures 
[[arxiv](https://arxiv.org/abs/2201.05651)] [[cool](https://papers.cool/arxiv/2201.05651)] [[pdf](https://arxiv.org/pdf/2201.05651)]
> **Authors**: Sujit Roy,Gnaneswara Rao Gorle,Vishal Gaur,Haider Raza,Shoaib Jameel
> **First submission**: 2022-01-14
> **First announcement**: 2022-01-17
> **comment**: No comments
- **标题**: 线索：视频讲座中用户参与度的上下文化统一学习
- **领域**: 人工智能,机器学习
- **摘要**: 通过使用不同的计算方法来利用视图数量或相关的喜欢，预测视频中的上下文参与是一个长期尝试的问题。最近的十年中，在线学习资源中兴起了，在大流行期间，在线教学视频的指数升高而没有太多的质量控制。如果创建者可以对其内容获得建设性的反馈，则可以提高内容的质量。雇用一组领域专家志愿者向视频提供反馈可能不会扩展。结果，开发计算方法的急剧上升，以预测用户参与得分，该评分表明了某种形式的用户参与度，即用户倾向于与内容互动的级别。当前方法中的缺点是它们以级联的方法分别对各种特征进行建模，这很容易出现错误传播。此外，他们中的大多数都没有提供有关创作者如何改善其内容的关键解释。在本文中，我们提出了一种新的统一模型，即教育领域的线索，该模型从从自由使用的公共在线教学视频中提取的功能中学习，并在视频上提供了可解释的反馈以及用户参与分数。鉴于任务的复杂性，我们的统一框架采用了不同的预培训模型作为分类器的集合。我们的模型利用了各种多模式特征，以建模语言的复杂性，上下文不可知的信息，传递内容的文本情感，动画，扬声器的音调和言语情绪。在转移学习设置下，在统一空间中的总体模型对下游应用程序进行了微调。

### Data Harmonisation for Information Fusion in Digital Healthcare: A State-of-the-Art Systematic Review, Meta-Analysis and Future Research Directions 
[[arxiv](https://arxiv.org/abs/2201.06505)] [[cool](https://papers.cool/arxiv/2201.06505)] [[pdf](https://arxiv.org/pdf/2201.06505)]
> **Authors**: Yang Nan,Javier Del Ser,Simon Walsh,Carola Schönlieb,Michael Roberts,Ian Selby,Kit Howard,John Owen,Jon Neville,Julien Guiot,Benoit Ernst,Ana Pastor,Angel Alberich-Bayarri,Marion I. Menzel,Sean Walsh,Wim Vos,Nina Flerin,Jean-Paul Charbonnier,Eva van Rikxoort,Avishek Chatterjee,Henry Woodruff,Philippe Lambin,Leonor Cerdá-Alberich,Luis Martí-Bonmatí,Francisco Herrera, et al. (1 additional authors not shown)
> **First submission**: 2022-01-17
> **First announcement**: 2022-01-18
> **comment**: 54 pages, 14 figures, accepted by the Information Fusion journal
- **标题**: 数据协调数字医疗保健中的信息融合：最先进的系统评价，荟萃分析和未来的研究方向
- **领域**: 人工智能,计算机视觉和模式识别
- **摘要**: 在大规模数字医疗研究中，消除多中心数据的偏见和差异一直是一个挑战，该研究需要能够整合从不同扫描仪和协议获得的数据中提取的临床特征，以提高稳定性和稳健性。先前的研究描述了融合单个模态多中心数据集的各种计算方法。但是，这些调查很少关注评估指标，并且缺乏计算数据协调研究的清单。在这项系统的综述中，我们总结了数字医疗领域中多模式数据的计算数据协调方法，包括基于不同理论的协调策略和评估指标。此外，提出了一份综合的清单，总结了数据协调研究的常见实践，以指导研究人员更有效地报告其研究结果。最后但并非最不重要的一点是，提出了提供方法论和指标选择的可能方法的流程图，并调查了不同方法的局限性以进行未来的研究。

### Unsupervised Multimodal Word Discovery based on Double Articulation Analysis with Co-occurrence cues 
[[arxiv](https://arxiv.org/abs/2201.06786)] [[cool](https://papers.cool/arxiv/2201.06786)] [[pdf](https://arxiv.org/pdf/2201.06786)]
> **Authors**: Akira Taniguchi,Hiroaki Murakami,Ryo Ozaki,Tadahiro Taniguchi
> **First submission**: 2022-01-18
> **First announcement**: 2022-01-19
> **comment**: Accepted to IEEE TRANSACTIONS ON COGNITIVE DEVELOPMENTAL SYSTEMS
- **标题**: 基于双重关节分析的无监督的多模式词发现，并共同出现线索
- **领域**: 人工智能,计算语言学,机器人技术
- **摘要**: 根据语音分布的统计特性和其他感觉刺激的同时存在，人类婴儿以最少的语言知识获得了口头词典。这项研究提出了一种新颖的无监督学习方法，用于使用语音信息作为分布提示和对象信息作为共同出现提示发现语音单元。所提出的方法可以使用无监督的学习从语音信号中获取单词和音素，并基于多种模态 - 视觉，触觉和听觉来利用对象信息。所提出的方法基于非参数贝叶斯双重关节分析仪（NPB-DAA），从语音特征发现音素和单词，以及对从对象获得的多模式信息进行分类的多模式潜在迪里奇分配（MLDA）。在实验中，提出的方法显示出比基线方法更高的单词发现性能。准确地分割了表达对象特征（即与名词和形容词相对应的单词）的单词。此外，我们研究了学习表现如何受到语言信息重要性差异的影响。相对于固定条件，增加单词模式的重量进一步提高了性能。

### Label Dependent Attention Model for Disease Risk Prediction Using Multimodal Electronic Health Records 
[[arxiv](https://arxiv.org/abs/2201.06779)] [[cool](https://papers.cool/arxiv/2201.06779)] [[pdf](https://arxiv.org/pdf/2201.06779)]
> **Authors**: Shuai Niu,Qing Yin,Yunya Song,Yike Guo,Xian Yang
> **First submission**: 2022-01-18
> **First announcement**: 2022-01-19
> **comment**: No comments
- **标题**: 使用多模式电子健康记录的疾病风险预测的依赖性注意模型
- **领域**: 人工智能
- **摘要**: 疾病风险预测引起了现代医疗保健领域的越来越多的关注，尤其是在人工智能（AI）的最新进展中。包含异构患者信息的电子健康记录（EHR）广泛用于疾病风险预测任务。应用AI模型进行风险预测的一个挑战在于生成可解释的证据以支持预测结果，同时保留预测能力。为了解决这个问题，我们提出了一种共同嵌入单词和标签的方法，其中关注模块根据医学注释与风险预测标签的名称的相关性从医学笔记中学习了单词的权重。这种方法通过采用注意机制并包括模型中预测任务的名称来提高可解释性。但是，其应用仅限于处理文本输入（例如医疗说明）。在本文中，我们提出了一个依赖性注意模型LDAM至1）通过利用临床 - 伯特（预先在大型临床语料库中培训的生物医学语言模型）来提高可解释性，以共同编码生物医学上有意义的特征和标签。 2）将联合嵌入的想法扩展到时间序列数据的处理，并开发一个多模式学习框架，以从医疗注释和时间序列健康状况指标中整合异质信息。为了证明我们的方法，我们将LDAM应用于模拟III数据集以预测不同的疾病风险。我们在定量和定性上评估我们的方法。具体而言，将显示LDAM的预测能力，并将进行案例研究以说明其可解释性。

### A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals 
[[arxiv](https://arxiv.org/abs/2201.09555)] [[cool](https://papers.cool/arxiv/2201.09555)] [[pdf](https://arxiv.org/pdf/2201.09555)]
> **Authors**: Cristian Santini,Genet Asefa Gesese,Silvio Peroni,Aldo Gangemi,Harald Sack,Mehwish Alam
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: No comments
- **标题**: 使用文字的知识图嵌入方法用于作者名称歧义的方法
- **领域**: 人工智能,计算语言学,数字图书馆
- **摘要**: 学术数据正在不断增长，其中包含来自大量场所的文章信息，包括会议，期刊等。已采取许多计划将学术数据作为知识图（KGS）提供。这些努力标准化这些数据并使其可访问的努力也导致了许多挑战，例如探索学术文章，模棱两可的作者等。这项研究更具体地针对了作者名称歧义歧义（和）对学术kgs的问题，并提出了一个新颖的框架，实际上是作者命名歧义（土地），该框架使用了知识图（kens），使用了多个kens（kges）的文献。该框架基于三个组成部分：1）多模式符号，2）阻止过程，最后，3）分层聚集聚类。已经对两个新创建的kg进行了广泛的实验：（i）从1978年开始包含科学计量学期刊的信息（OC-782K），以及（ii）从Aminer（Aminer-534K）提供的著名基准和提供的基准中提取的kg。结果表明，我们所提出的体系结构在F1分数方面优于8-14％的基准，并在诸如Aminer之类的具有挑战性的基准上显示竞争性能。代码和数据集可通过GitHub公开获得：https：//github.com/sntcristian/and-kge和Zenodo：https：//doi.org/10.5281/zenodo.6309855。

## 计算语言学(cs.CL:Computation and Language)

该领域共有 10 篇论文

### DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population 
[[arxiv](https://arxiv.org/abs/2201.03335)] [[cool](https://papers.cool/arxiv/2201.03335)] [[pdf](https://arxiv.org/pdf/2201.03335)]
> **Authors**: Ningyu Zhang,Xin Xu,Liankuan Tao,Haiyang Yu,Hongbin Ye,Shuofei Qiao,Xin Xie,Xiang Chen,Zhoubo Li,Lei Li,Xiaozhuan Liang,Yunzhi Yao,Shumin Deng,Peng Wang,Wen Zhang,Zhenru Zhang,Chuanqi Tan,Qiang Chen,Feiyu Xiong,Fei Huang,Guozhou Zheng,Huajun Chen
> **First submission**: 2022-01-10
> **First announcement**: 2022-01-11
> **comment**: Accepted by EMNLP 2022 System Demonstrations and the project website is http://deepke.zjukg.cn/
- **标题**: DeepKe：基于深度学习的知识提取工具包，用于知识库人群
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **摘要**: 我们提出了一个开源和可扩展的知识提取工具包，以支持知识库人群中复杂的低资源，文档级别和多模式场景。 DeepKe实施了各种信息提取任务，包括指定的实体识别，关系提取和属性提取。借助统一的框架，DeepKe允许开发人员和研究人员根据其要求自定义数据集和模型从非结构化数据中提取信息。具体而言，DeepKe不仅为不同的任务和场景提供了各种功能模块和模型实现，而且还通过一致的框架来组织所有组件，以保持足够的模块化和可扩展性。我们在https://github.com/zjunlp/deepke中与Google COLAB教程和初学者的综合文档发布了源代码。此外，我们在http://deepke.openkg.cn/en/re_doc_show.html中介绍了一个在线系统，用于实时提取各种任务，以及一个演示视频。

### CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition 
[[arxiv](https://arxiv.org/abs/2201.03804)] [[cool](https://papers.cool/arxiv/2201.03804)] [[pdf](https://arxiv.org/pdf/2201.03804)]
> **Authors**: Wenliang Dai,Samuel Cahyawijaya,Tiezheng Yu,Elham J. Barezi,Peng Xu,Cheuk Tung Shadow Yiu,Rita Frieske,Holy Lovenia,Genta Indra Winata,Qifeng Chen,Xiaojuan Ma,Bertram E. Shi,Pascale Fung
> **First submission**: 2022-01-11
> **First announcement**: 2022-01-12
> **comment**: 6 pages
- **标题**: CI-AVSR：一个粤语视听语音数据集用于车内命令识别
- **领域**: 计算语言学,人工智能
- **摘要**: 随着深度学习和智能车辆的兴起，智能助手已成为促进驾驶和提供额外功能的车内组件。车内智能助手应能够处理一般以及与汽车相关的命令并执行相应的操作，从而简化驾驶并提高安全性。但是，低资源语言存在数据稀缺问题，从而阻碍了研究和应用的发展。在本文中，我们介绍了一个新的数据集，广东话音频语音识别（CI-AVSR），以使用视频和音频数据，以用粤语识别车内命令识别。它由30名本地广东话人录制的200个车内命令中的4,984个样本（8.3小时）组成。此外，我们使用常见的车载背景噪声增强数据集以模拟真实环境，从而产生比收集的数据集的10倍。我们提供了数据集的清洁版和增强版本的详细统计信息。此外，我们实施了两个多模式基线，以证明CI-AVSR的有效性。实验结果表明，利用视觉信号可以改善模型的整体性能。尽管我们的最佳模型可以在干净的测试集上实现相当大的质量，但是嘈杂数据上的语音识别质量仍然较低，并且仍然是真正的车载式语音识别系统的一项极具挑战性的任务。数据集和代码将在https://github.com/hltchkust/ci-avsr上发布。

### Group Gated Fusion on Attention-based Bidirectional Alignment for Multimodal Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2201.06309)] [[cool](https://papers.cool/arxiv/2201.06309)] [[pdf](https://arxiv.org/pdf/2201.06309)]
> **Authors**: Pengfei Liu,Kun Li,Helen Meng
> **First submission**: 2022-01-17
> **First announcement**: 2022-01-18
> **comment**: Published in INTERSPEECH-2020
- **标题**: 基于注意的双向对齐方式的组门控融合多模式情绪识别
- **领域**: 计算语言学,声音,音频和语音处理
- **摘要**: 情绪识别是一个具有挑战性且积极研究的研究领域，在情绪感知的人类计算机相互作用系统中起着至关重要的作用。在多模式的环境中，不同方式之间的时间对齐尚未得到很好的研究。本文提出了一种名为封闭式双向对准网络（GBAN）的新模型，该模型由LSTM隐藏状态上的基于注意力的双向对齐网络组成，以明确捕获语音与文本之间的对齐关系，以及一个新颖的集体门控融合（GGF）层（GGF）层以整合不同模态的表示。我们从经验上表明，注意一致的表示的表现优于LSTM的最后一个隐藏状态，而拟议的GBAN模型的表现优于Iemocap数据集中现有的最新多模式方法。

### CM3: A Causal Masked Multimodal Model of the Internet 
[[arxiv](https://arxiv.org/abs/2201.07520)] [[cool](https://papers.cool/arxiv/2201.07520)] [[pdf](https://arxiv.org/pdf/2201.07520)]
> **Authors**: Armen Aghajanyan,Bernie Huang,Candace Ross,Vladimir Karpukhin,Hu Xu,Naman Goyal,Dmytro Okhonko,Mandar Joshi,Gargi Ghosh,Mike Lewis,Luke Zettlemoyer
> **First submission**: 2022-01-19
> **First announcement**: 2022-01-20
> **comment**: No comments
- **标题**: CM3：Internet的因果蒙版多模式模型
- **领域**: 计算语言学
- **摘要**: 我们介绍了CM3，这是一个因果掩盖的生成模型的家族，该模型在大量的结构化多模式文档中训练，这些模型可以包含文本和图像令牌。我们的新因果掩蔽方法从左到右产生令牌，同时还掩盖了在字符串末端生成的少数长令牌跨度，而不是其原始位置。休闲掩蔽对象通过启用完整的生成模型，同时在生成掩盖的跨度时提供双向上下文，从而提供了更常见的因果和掩盖语言模型的混合物。我们在大规模网络和Wikipedia文章上训练因果掩盖的语言图像模型，每个文档都包含所有文本，超文本标记，超链接和图像令牌（来自VQVAE-GAN），它们以它们出现在原始HTML源（掩盖之前）的顺序提供。所得的CM3模型可以在任意掩盖的文档上下文中生成丰富的结构化的多模式输出，从而隐含地学习各种文本，图像和交叉模态任务。可以提示它们以零拍的方式恢复，即DALL-E，类型和HTLM等模型的功能。我们在零摄影摘要，实体链接和实体歧义中设置了新的最新技术，同时在微调环境中保持竞争性能。我们可以无条件地生成图像，以文本为条件（如dall-e），并以单个模型为零射击的所有字幕。

### VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation 
[[arxiv](https://arxiv.org/abs/2201.08054)] [[cool](https://papers.cool/arxiv/2201.08054)] [[pdf](https://arxiv.org/pdf/2201.08054)]
> **Authors**: Yihang Li,Shuichiro Shimizu,Weiqi Gu,Chenhui Chu,Sadao Kurohashi
> **First submission**: 2022-01-20
> **First announcement**: 2022-01-21
> **comment**: Accepted by LREC2022
- **标题**: 签证：视觉场景吸引机器翻译的模棱两可的字幕数据集
- **领域**: 计算语言学
- **摘要**: 现有的多模式翻译（MMT）数据集由图像和视频字幕或一般字幕组成，这些字幕很少包含语言歧义，从而使视觉信息不太有效，无法生成适当的翻译。我们介绍Visa，这是一个由40k日语 - 英语平行句子对组成的新数据集和具有以下关键特征的相应视频剪辑：（1）并行句子是电影和电视剧集的字幕； （2）源字幕是模棱两可的，这意味着它们具有多种可能的翻译，其含义不同； （3）我们根据歧义的原因将数据集分为多义和省略。我们表明，Visa对于最新的MMT系统具有挑战性，我们希望数据集可以促进MMT研究。签证数据集可在以下网址获得：https：//github.com/ku-nlp/visa。

### Supervised Visual Attention for Simultaneous Multimodal Machine Translation 
[[arxiv](https://arxiv.org/abs/2201.09324)] [[cool](https://papers.cool/arxiv/2201.09324)] [[pdf](https://arxiv.org/pdf/2201.09324)]
> **Authors**: Veneta Haralampieva,Ozan Caglayan,Lucia Specia
> **First submission**: 2022-01-23
> **First announcement**: 2022-01-24
> **comment**: Accepted to Journal of Artificial Intelligence Research (JAIR)
- **标题**: 同时进行多模式翻译的视觉关注
- **领域**: 计算语言学
- **摘要**: 最近，多模式机器翻译（MMT）的研究激增，其中其他模式（例如图像）用于提高文本系统的翻译质量。这种多模式系统的特殊用途是同时机器翻译的任务，在该任务中，已证明视觉上下文可以补充源句子提供的部分信息，尤其是在翻译的早期阶段。在本文中，我们提出了第一个基于变压器的同时MMT体系结构，该体系结构以前尚未在现场探索过。此外，我们使用辅助监督信号扩展了该模型，该信号使用标记的短语区域比对来指导其视觉注意机制。我们在三个语言方向上进行全面的实验，并使用自动指标和手动检查进行彻底的定量和定性分析。我们的结果表明，（i）监督视觉注意力一致地提高了MMT模型的翻译质量，并且（ii）通过监督损失对MMT进行微调，比从SCRATCH训练MMT可以提高性能。与最先进的模型相比，我们提出的模型可实现多达2.3 bleu和3.5 Meteor点的改善。

### Multi-channel Attentive Graph Convolutional Network With Sentiment Fusion For Multimodal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2201.10274)] [[cool](https://papers.cool/arxiv/2201.10274)] [[pdf](https://arxiv.org/pdf/2201.10274)]
> **Authors**: Luwei Xiao,Xingjiao Wu,Wen Wu,Jing Yang,Liang He
> **First submission**: 2022-01-25
> **First announcement**: 2022-01-26
> **comment**: No comments
- **标题**: 多渠道专注图卷积网络具有情感融合用于多模式分析
- **领域**: 计算语言学
- **摘要**: 如今，随着社交媒体平台上多模式评论的爆炸性增长，多模式的情感分析最近因与这些社交媒体帖子的高度相关而越来越受欢迎。尽管大多数以前的研究都设计了各种融合框架来学习多种方式的交互式表示，但它们未能将情感知识纳入模式间学习。本文提出了一个多通道的专注图卷积网络（MAGCN），由两个主要组成部分组成：跨模式交互式学习和情感特征融合。为了进行跨模式的交互式学习，我们利用了自我发场机制与密度连接的图形卷积网络相结合以学习模式间动力学。对于情感特征融合，我们利用多头自我注意力将情感知识合并为模式间特征表示。广泛的实验是在三个广泛使用的数据集上进行的。实验结果表明，与几种最新方法相比，所提出的模型在准确性和F1得分上实现了竞争性能。

### Multimodal data matters: language model pre-training over structured and unstructured electronic health records 
[[arxiv](https://arxiv.org/abs/2201.10113)] [[cool](https://papers.cool/arxiv/2201.10113)] [[pdf](https://arxiv.org/pdf/2201.10113)]
> **Authors**: Sicen Liu,Xiaolong Wang,Yongshuai Hou,Ge Li,Hui Wang,Hui Xu,Yang Xiang,Buzhou Tang
> **First submission**: 2022-01-25
> **First announcement**: 2022-01-26
> **comment**: 12 pages, 5 figures accepted for publication in the IEEE Journal of Biomedical and Health Informatics (J-BHI)
- **标题**: 多模式数据事项：语言模型对结构化和非结构化电子健康记录进行预训练
- **领域**: 计算语言学,机器学习
- **摘要**: 作为电子健康记录（EHR）的两种重要文本方式，最近越来越多地应用于医疗保健领域。但是，大多数现有的面向EHR的研究都集中在特定的模态上，或以直接方式整合来自不同模式的数据，通常将结构化和非结构化数据视为有关患者入学的两个独立信息来源，并忽略了它们之间的内在相互作用。实际上，这两种模式是在同一遇到的结构化数据中记录的，其中结构化数据为非结构化数据的文档提供了信息，反之亦然。在本文中，我们提出了一种名为MedM-PLM的医学多模式预训练的语言模型，以了解对结构化和非结构化数据的增强EHR表示，并探索两种模式的相互作用。在MEDM-PLM中，首先采用了两个基于变压器的神经网络组件来从每种模式中学习代表性特征。然后引入跨模块模块以建模其相互作用。我们在模拟III数据集上预先训练MEDM-PLM，并验证了该模型对三个下游临床任务的有效性，即药物建议，30天的再入院预测和ICD编码。与最先进的方法相比，广泛的实验证明了MEDM-PLM的功能。进一步的分析和可视化表明了我们的模型的鲁棒性，这有可能为临床决策提供更全面的解释。

### Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus 
[[arxiv](https://arxiv.org/abs/2201.11313)] [[cool](https://papers.cool/arxiv/2201.11313)] [[pdf](https://arxiv.org/pdf/2201.11313)]
> **Authors**: Chen Wu,Ming Yan
> **First submission**: 2022-01-26
> **First announcement**: 2022-01-27
> **comment**: No comments
- **标题**: 使用codesearchnet语料库学习深层语义模型
- **领域**: 计算语言学,信息检索
- **摘要**: 语义代码搜索是自然语言查询检索相关代码段的任务。与典型的信息检索任务不同，代码搜索需要弥合编程语言和自然语言之间的语义差距，以更好地描述内在的概念和语义。最近，用于代码搜索的深度神经网络一直是一个热门的研究主题。神经代码搜索的典型方法首先表示代码段，并查询文本作为单独的嵌入方式，然后使用向量距离（例如，点产物或余弦）来计算它们之间的语义相似性。存在许多不同的方法，将代码长度或查询令牌的可变长度汇总到可学习的嵌入中，包括bi-编码器，跨编码器和poly-编码器。查询编码器和代码编码器的目的是生成相关的查询对以及相应的所需代码段的嵌入，其中编码器的选择和设计非常重要。在本文中，我们提出了一个新型的深层语义模型，该模型不仅利用多模式源的实用程序，而且还具有提取器，例如自我注意力，聚合的向量，中间表示的组合。我们应用提出的模型来应对有关语义代码搜索的CodesearchNet挑战。我们将多模式学习的跨语言嵌入与大批次和硬采矿相结合，并结合了不同的学习表示形式，以更好地增强表示形式学习。我们的模型在CodesearchNet语料库上进行了培训，并对固定数据进行了评估，最终模型可实现0.384 NDCG，并赢得了该基准测试的第一名。模型和代码可在https://github.com/overwindows/semanticcodesearch.git上找到。

### IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages 
[[arxiv](https://arxiv.org/abs/2201.11732)] [[cool](https://papers.cool/arxiv/2201.11732)] [[pdf](https://arxiv.org/pdf/2201.11732)]
> **Authors**: Emanuele Bugliarello,Fangyu Liu,Jonas Pfeiffer,Siva Reddy,Desmond Elliott,Edoardo Maria Ponti,Ivan Vulić
> **First submission**: 2022-01-27
> **First announcement**: 2022-01-28
> **comment**: ICML 2022
- **标题**: iglue：跨模式，任务和语言转移学习的基准
- **领域**: 计算语言学,计算机视觉和模式识别
- **摘要**: 可靠的评估基准是为了可复制性和全面性而设计的，这推动了机器学习的进步。但是，由于缺乏多语言基准，视觉和语言研究主要集中在英语任务上。为了填补这一空白，我们介绍了图像接地的语言理解评估基准。 Iglue通过汇总已有的数据集并创建新的数据来汇集 - 视觉问题回答，跨模式检索，扎根的推理以及跨20种不同语言的扎根成本。我们的基准测试能够评估多种语言多模型用于转移学习的模型，不仅在零拍设置中，而且还以新定义的少数图学习设置。基于对可用最新模型的评估，我们发现翻译测试转移优于零拍传输，而对于许多任务来说，很难利用射击的学习。此外，下游性能部分用可用的无标记文本数据进行预处理来解释，并且仅通过目标源语言的类型学距离而微弱。我们希望通过向社区释放基准测试来鼓励该领域的未来研究工作。

## 密码学和安全(cs.CR:Cryptography and Security)

该领域共有 2 篇论文

### Hardware Implementation of Multimodal Biometric using Fingerprint and Iris 
[[arxiv](https://arxiv.org/abs/2201.05996)] [[cool](https://papers.cool/arxiv/2201.05996)] [[pdf](https://arxiv.org/pdf/2201.05996)]
> **Authors**: Tariq M Khan
> **First submission**: 2022-01-16
> **First announcement**: 2022-01-17
> **comment**: No comments
- **标题**: 使用指纹和虹膜实现多模式生物识别的硬件
- **领域**: 密码学和安全,计算机视觉和模式识别
- **摘要**: 在本文中，提出了多模式生物识别系统的硬件体系结构，以大大利用固有的并行性。所提出的系统基于多种生物特征融合，该融合使用了两个生物特征性状，指纹和虹膜。通过解决直接影响FAR和FRR的一些问题，首先在软件级别优化每个生物特征。然后提出了两个生物特征性状的硬件体系结构，然后是最终的多模式硬件体系结构。据作者所知，没有其他基于FPGA的设计退出使用这两个特征。

### Hold On and Swipe: A Touch-Movement Based Continuous Authentication Schema based on Machine Learning 
[[arxiv](https://arxiv.org/abs/2201.08564)] [[cool](https://papers.cool/arxiv/2201.08564)] [[pdf](https://arxiv.org/pdf/2201.08564)]
> **Authors**: Rushit Dave,Naeem Seliya,Laura Pryor,Mounika Vanamala,Evelyn Sowells,Jacob mallet
> **First submission**: 2022-01-21
> **First announcement**: 2022-01-24
> **comment**: No comments
- **标题**: 保持和滑动：基于机器学习的基于触摸的连续身份验证模式
- **领域**: 密码学和安全,机器学习
- **摘要**: 近年来，存储在移动设备上的安全信息数量呈指数增长。但是，当前针对生理生物测量和密码等移动设备的安全模式不足以保护此信息。行为生物识别技术已被大量研究，作为解决移动设备安全性缺陷的可能解决方案。这项研究旨在通过评估使用触摸动态和电话运动的多模式基于行为生物识别的用户身份验证方案的性能来为这项创新研究做出贡献。这项研究使用了两个流行的公共可用数据集的融合，手动运动方向和掌握数据集和生物底数据集。这项研究使用三种常见的机器学习算法评估了我们的模型性能，这些算法是随机的森林支持矢量机，而K-Nearest邻居达到的准确率高达82％，每种算法分别针对所有成功指标都执行。

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

该领域共有 45 篇论文

### Memory-Guided Semantic Learning Network for Temporal Sentence Grounding 
[[arxiv](https://arxiv.org/abs/2201.00454)] [[cool](https://papers.cool/arxiv/2201.00454)] [[pdf](https://arxiv.org/pdf/2201.00454)]
> **Authors**: Daizong Liu,Xiaoye Qu,Xing Di,Yu Cheng,Zichuan Xu,Pan Zhou
> **First submission**: 2022-01-02
> **First announcement**: 2022-01-04
> **comment**: Accepted by AAAI2022
- **标题**: 时间句子接地的记忆指导的语义学习网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 时间句子接地（TSG）至关重要，对于视频理解至关重要。尽管现有方法训练了大量数据训练精心设计的深层网络，但我们发现，由于失衡数据分布，它们很容易忘记训练阶段中很少出现的情况，这会影响模型的概括并导致不良性能。为了解决此问题，我们提出了一个名为内存的网络，称为内存引导的语义学习网络（MGSL-NET），该网络学习并记住TSG任务中鲜出现的内容。具体而言，MGSL-NET由三个主要部分组成：一个跨模式的互动模块，存储器增强模块和一个异质注意模块。我们首先通过跨模式图卷积网络对准给定的视频疑问对，然后利用内存模块记录特定于域特定的持久内存中的跨模式共享语义特征。在训练过程中，记忆插槽与常见和罕见情况都动态相关，从而减轻了遗忘问题。在测试中，可以通过检索存储的记忆来增强罕见情况，从而更好地泛化。最后，将异质注意模块用于整合视频和查询域中增强的多模式特征。三个基准测试的实验结果表明我们方法对有效性和效率都具有优势，这不仅在整个数据集上而且在极少数情况下都提高了准确性。

### Attention-based Dual Supervised Decoder for RGBD Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2201.01427)] [[cool](https://papers.cool/arxiv/2201.01427)] [[pdf](https://arxiv.org/pdf/2201.01427)]
> **Authors**: Yang Zhang,Yang Yang,Chenyun Xiong,Guodong Sun,Yanwen Guo
> **First submission**: 2022-01-04
> **First announcement**: 2022-01-05
> **comment**: 12 pages, 6 figures
- **标题**: RGBD语义细分的基于注意的双重监督解码器
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 编码器模型已广泛用于RGBD语义分割，其中大多数是通过两流网络设计的。通常，从RGBD共同推理颜色和几何信息对语义分割有益。但是，大多数现有的方法都无法全面利用编码器和解码器中的多模式信息。在本文中，我们提出了一种基于注意力的双重监督解码器，用于RGBD语义分割。在编码器中，我们设计了一个简单但有效的基于注意力的多模式融合模块，以提取和融合深度多层配对的互补信息。为了了解更多强大的深层表示和丰富的多模式信息，我们引入了双分支解码器，以有效利用不同任务的相关性和互补提示。 NYUDV2和SUN-RGBD数据集的广泛实验表明，我们的方法可以针对最先进的方法实现出色的性能。

### Sound and Visual Representation Learning with Multiple Pretraining Tasks 
[[arxiv](https://arxiv.org/abs/2201.01046)] [[cool](https://papers.cool/arxiv/2201.01046)] [[pdf](https://arxiv.org/pdf/2201.01046)]
> **Authors**: Arun Balajee Vasudevan,Dengxin Dai,Luc Van Gool
> **First submission**: 2022-01-04
> **First announcement**: 2022-01-05
> **comment**: 11 pages, 3 figures
- **标题**: 声音和视觉表示学习，并通过多个训练预处理任务
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 不同的自我监督任务（SSL）从数据中揭示了不同的功能。学习的功能表示形式可以针对每个下游任务表现出不同的性能。从这个角度来看，这项工作旨在结合多个SSL任务（多SSL），这些任务可以很好地概括为所有下游任务。具体而言，对于这项研究，我们分别研究了双耳声音和图像数据。对于双耳声音，我们提出了三个SSL任务，即空间对齐，前景对象的时间同步以及双耳音频和时间间隙预测。我们研究了多SSL的几种方法，并深入了解视频检索，空间声音超级分辨率以及Omniaudio数据集上的语义预测的下游任务性能。我们对双耳声音表示的实验表明，SSL任务的增量学习（IL）多SSL优于单个SSL任务模型，并且在下游任务性能中进行了完全监督的模型。为了检查其他模式的适用性，我们还制定了用于图像表示学习的多SSL模型，并使用最近提出的SSL任务Mocov2和Densecl。在这里，多SSL超过了MOCOV2，DENSECL和DETCO等最新方法，在VOC07分类中，COCO检测的VOC07分类的2.06％，3.27％和1.19％，+2.83，+2.83，+1.56和+1.61 AP。代码将公开可用。

### A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval 
[[arxiv](https://arxiv.org/abs/2201.02772)] [[cool](https://papers.cool/arxiv/2201.02772)] [[pdf](https://arxiv.org/pdf/2201.02772)]
> **Authors**: Zhixiong Zeng,Wenji Mao
> **First submission**: 2022-01-08
> **First announcement**: 2022-01-10
> **comment**: No comments
- **标题**: 对监督跨模式检索的视觉预训练模型的全面经验研究
- **领域**: 计算机视觉和模式识别,计算语言学,多媒体
- **摘要**: 跨模式检索（CMR）是多模式计算和信息检索的重要研究主题，它将一种类型的数据作为查询来检索另一种类型的相关数据。它已被广泛用于许多实际应用中。最近，由剪辑代表的视觉培训模型表明了其在学习视觉和文本表示方面的优越性，并在各种视觉和语言相关的任务上获得了令人印象深刻的表现。尽管剪辑以及以前的预训练模型在无监督的CMR方面表现出了很大的性能提高，但由于缺乏多模级课程相关的共同表示，因此很少探索这些预训练的模型对监督CMR的性能和影响。在本文中，我们将CLIP作为当前的代表性视觉培训预培训模型进行全面的经验研究。我们评估了其性能和对监督CMR的影响，并试图回答几个关键的研究问题。为此，我们首先提出了一种新型的模型clip4CMR（用于跨模式检索的剪辑增强网络），该网络采用预训练的夹子作为骨干网络来执行监督的CMR。然后，通过CLIP4CMR框架，我们在当前CMR方法中重新审视不同学习目标的设计，以提供有关模型设计的新见解。此外，我们调查了应用CMR的最相关方面，包括对模式失衡的鲁棒性和对超参数的敏感性，以为实用应用提供新的观点。通过广泛的实验，我们表明CLIP4CMR在基准数据集上取得了显着改进，可以实现SOTA结果，并且可以用作基本框架，以经验研究监督CMR的关键研究问题，对模型设计和实际考虑以及对模型设计和实际考虑的重要意义。

### MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound 
[[arxiv](https://arxiv.org/abs/2201.02639)] [[cool](https://papers.cool/arxiv/2201.02639)] [[pdf](https://arxiv.org/pdf/2201.02639)]
> **Authors**: Rowan Zellers,Jiasen Lu,Ximing Lu,Youngjae Yu,Yanpeng Zhao,Mohammadreza Salehi,Aditya Kusupati,Jack Hessel,Ali Farhadi,Yejin Choi
> **First submission**: 2022-01-07
> **First announcement**: 2022-01-10
> **comment**: CVPR 2022. Project page at https://rowanzellers.com/merlotreserve
- **标题**: Merlot储备：通过视觉，语言和声音的神经脚本知识
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习,声音,音频和语音处理
- **摘要**: 作为人类，我们在一个多模式的世界中浏览，从我们所有的感官中建立了整体理解。我们介绍了Merlot Reserve，这是一种随着时间的推移而代表视频的模型 - 通过从音频，字幕和视频帧中学习的新培训目标。给定视频，我们用面具令牌替换文本和音频片段；该模型通过选择正确的蒙版片段来学习。我们的目标比替代方案更快地学习，并且表现良好：我们在2000万个YouTube视频上预计。经验结果表明，Merlot储备会学习强大的多模式表示。填补时，它将在视觉常识推理（VCR），TVQA和Kinetics-600上设置最新的。以前的工作分别超过5％，7％和1.5％。消融表明，这些任务受益于音频预处理 - 即使是VCR，也是围绕图像的质量检查任务（无声音）。此外，我们的目标可以实现开箱即用的预测，从而揭示了强烈的多模式常识理解。在一个完全零射击的设置中，我们的模型在四个视频任务上获得了竞争成果，甚至在最近提出的定位推理（Star）基准的情况下，甚至超过了监督的方法。我们分析了为什么音频可以实现更好的视觉语言表示，这为未来的研究提供了很大的机会。我们通过讨论多模式预处理的道德和社会意义来结束。

### Progressive Video Summarization via Multimodal Self-supervised Learning 
[[arxiv](https://arxiv.org/abs/2201.02494)] [[cool](https://papers.cool/arxiv/2201.02494)] [[pdf](https://arxiv.org/pdf/2201.02494)]
> **Authors**: Li Haopeng,Ke Qiuhong,Gong Mingming,Tom Drummond
> **First submission**: 2022-01-07
> **First announcement**: 2022-01-10
> **comment**: No comments
- **标题**: 通过多模式自学学习的渐进视频摘要
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 现代视频摘要方法基于深度神经网络，需要大量的带注释的数据进行培训。但是，现有的视频摘要数据集是小规模的，很容易导致深层模型过度。考虑到大规模数据集的注释是耗时的，我们提出了一个多模式的自学学习框架，以获取视频的语义表示，这使视频摘要任务受益。具体而言，自我监督的学习是通过在粗粒和细粒度的时尚中探索视频和文本之间的语义一致性，并在视频中恢复蒙版框架。多模式框架在由视频文本对组成的新收集的数据集上进行了训练。此外，我们介绍了一种渐进式视频摘要方法，在其中逐步确定了视频中的重要内容以生成更好的摘要。广泛的实验证明了我们方法在等级相关系数和F评分方面的有效性和优势。

### Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image Representations 
[[arxiv](https://arxiv.org/abs/2201.03597)] [[cool](https://papers.cool/arxiv/2201.03597)] [[pdf](https://arxiv.org/pdf/2201.03597)]
> **Authors**: Eva Breznik,Elisabeth Wetzer,Joakim Lindblad,Nataša Sladoje
> **First submission**: 2022-01-10
> **First announcement**: 2022-01-11
> **comment**: No comments
- **标题**: 使用对比度多模式图像表示跨模式子图像检索
- **领域**: 计算机视觉和模式识别
- **摘要**: 在组织表征和癌症诊断中，多模式成像已成为一种强大的技术。得益于计算的进步，可以利用大型数据集发现病理学模式并改善诊断。但是，这需要有效且可扩展的图像检索方法。跨模式图像检索特别具有挑战性，因为不同模式捕获的相似（甚至相同）内容的图像可能具有很少的共同结构。我们提出了一个新的基于应用程序的图像检索（CBIR）系统，用于跨模态的反向（子）图像搜索，该系统结合了深度学习，以生成表示表示（嵌入公共空间中的不同模态）与经典特征提取和词具模型，以实现有效和可靠的回收。我们通过替换研究来说明其优势，探索许多特征提取器和学习的表示形式，以及与最近（跨模式）CBIR方法的比较。对于在Brightfield和第二次谐波生成显微镜图像（公开）数据集上检索（子）图像的任务，结果表明我们的方法优于所有经过测试的替代方案。我们讨论了比较方法的缺点，并观察了CBIR管道中学习表示形式和特征提取器的均衡性和不变特性的重要性。代码可在：\ url {https://github.com/mida-group/crossmodal_imgretretieval}中获得。

### Dynamical Audio-Visual Navigation: Catching Unheard Moving Sound Sources in Unmapped 3D Environments 
[[arxiv](https://arxiv.org/abs/2201.04279)] [[cool](https://papers.cool/arxiv/2201.04279)] [[pdf](https://arxiv.org/pdf/2201.04279)]
> **Authors**: Abdelrahman Younes
> **First submission**: 2022-01-11
> **First announcement**: 2022-01-12
> **comment**: No comments
- **标题**: 动态音频视频导航：在未绘制的3D环境中捕获闻所未闻的动态声源
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术,声音,音频和语音处理
- **摘要**: 关于音频导航的最新工作针对无噪声音频环境中的单个静态声音，并努力概括到闻所未闻的声音。我们介绍了新颖的动态视听导航基准，其中体现的AI代理必须在有分散器和嘈杂的声音的情况下在未模型的环境中捕获移动的声源。我们提出了一种端到端的强化学习方法，该方法依赖于多模式体系结构，该架构可以从双耳音频信号和空间占用图中融合空间音频视频信息，以编码为我们新的复杂任务设置学习强大的导航策略所需的功能。我们证明，我们的方法在两个具有挑战性的3D扫描现实世界数据集副本和Matterport3D上更好地概括了当前的最新技术，并且可以更好地对噪音，并为静态和动态的音频导航基准。我们的小说基准将在http://dav-nav.cs.uni-freiburg.de上提供。

### Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training 
[[arxiv](https://arxiv.org/abs/2201.04026)] [[cool](https://papers.cool/arxiv/2201.04026)] [[pdf](https://arxiv.org/pdf/2201.04026)]
> **Authors**: Yehao Li,Jiahao Fan,Yingwei Pan,Ting Yao,Weiyao Lin,Tao Mei
> **First submission**: 2022-01-11
> **First announcement**: 2022-01-12
> **comment**: ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)
- **标题**: Uni-eden：通过多粒状视觉培训的通用编码器网络网络
- **领域**: 计算机视觉和模式识别,计算语言学,多媒体
- **摘要**: 视觉语言的预训练一直是一个新兴且快速发展的研究主题，它将多模式知识从富裕的资源培训预培训任务转移到有限资源的下游任务。与主要学习单个通用编码器的现有作品不同，我们提出了可识别的通用编码器 - 编码器网络（UNI-EDEN），以促进视觉感知（例如，视觉问题答案）和一代（例如，图像字幕）。 Uni-eden是一个基于两流变压器的结构，由三个模块组成：对象和句子编码器，它们分别了解每种模式的表示形式，而句子解码器则可以通过模式间交互启用多模式推理和句子生成。 Considering that the linguistic representations of each image can span different granularities in this hierarchy including, from simple to comprehensive, individual label, a phrase, and a natural sentence, we pre-train Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object Classification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence Matching (ISM), and Masked Sentence Generation （msg）。这样，Uni-eden赋予了多模式表示提取和语言建模的力量。广泛的实验表明，通过将其微调到四种视觉感知和下游任务的产生，使Uni-Eden具有令人信服的普遍性。

### On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2201.03965)] [[cool](https://papers.cool/arxiv/2201.03965)] [[pdf](https://arxiv.org/pdf/2201.03965)]
> **Authors**: Ankur Sikarwar,Gabriel Kreiman
> **First submission**: 2022-01-11
> **First announcement**: 2022-01-12
> **comment**: No comments
- **标题**: 关于共同注意变压器层在视觉问题回答中的功效
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 近年来，多模式变压器在视觉任务（例如视觉问题答案（VQA））上显示出显着的进展，以相当大的差距表现优于先前的架构。 VQA的这种改进通常归因于视觉和语言流之间的丰富相互作用。在这项工作中，我们研究了共同注意变压器层在回答问题的同时，帮助网络专注于相关区域的功效。我们使用这些共同注意层中问题条件的图像注意分数生成视觉注意图。我们评估了以下关键组件对最新VQA模型的视觉注意的影响：（i）对象区域建议的数量，（ii）问题部分（pos）标签的部分，（iii）问题语义，（iv）共同注意层的数量，以及（v）答案准确性。我们将神经网络的注意图与人类注意力图进行比较，既有定性和定量。我们的发现表明，共同注意变压器模块对于参与图像的相关区域至关重要。重要的是，我们观察到问题的语义含义不是引起视觉关注的内容，而是问题中的特定关键字。我们的工作阐明了共同发音变压器层的功能和解释，突出了当前网络中的差距，并可以指导未来的VQA模型和网络的开发，这些模型和网络同时处理视觉和语言流。

### Semantic Labeling of Human Action For Visually Impaired And Blind People Scene Interaction 
[[arxiv](https://arxiv.org/abs/2201.04706)] [[cool](https://papers.cool/arxiv/2201.04706)] [[pdf](https://arxiv.org/pdf/2201.04706)]
> **Authors**: Leyla Benhamida,Slimane Larabi
> **First submission**: 2022-01-12
> **First announcement**: 2022-01-13
> **comment**: mber:Report 01/2022 of RIIMA Laboratory
- **标题**: 人类动作的语义标签用于视力障碍和盲人场景互动
- **领域**: 计算机视觉和模式识别
- **摘要**: 这项工作的目的是为视觉障碍和盲人的触觉设备开发，以便让他们了解周围人的行动并与他们互动。首先，基于RGB-D序列中人类动作识别的最新方法，我们使用Kinect提供的骨骼信息，并使用分离和统一的多尺度图形卷积（MS-G3D）模型来识别执行的动作。我们在真实场景上测试了该模型，并发现了一些约束和局限性。接下来，我们在MS-G3D的骨架模式与CNN的深度模态之间应用融合，以绕过讨论的局限性。第三，公认的动作是用语义标记的，并将映射到可通过触摸感可感知的输出设备中。

### Robust Contrastive Learning against Noisy Views 
[[arxiv](https://arxiv.org/abs/2201.04309)] [[cool](https://papers.cool/arxiv/2201.04309)] [[pdf](https://arxiv.org/pdf/2201.04309)]
> **Authors**: Ching-Yao Chuang,R Devon Hjelm,Xin Wang,Vibhav Vineet,Neel Joshi,Antonio Torralba,Stefanie Jegelka,Yale Song
> **First submission**: 2022-01-12
> **First announcement**: 2022-01-13
> **comment**: No comments
- **标题**: 强大的对比度学习反对嘈杂的观点
- **领域**: 计算机视觉和模式识别,机器学习
- **摘要**: 对比学习依赖于一个假设，即正对包含相关视图，例如图像的贴片或视频的共发生多模式信号，这些信号共享了有关实例的某些基本信息。但是，如果违反了这个假设怎么办？文献表明，对比学习在存在嘈杂观点的情况下，例如，没有明显的共享信息，会产生次优表示。在这项工作中，我们提出了一种新的对比损失函数，该功能与嘈杂的观点相当强大。我们通过证明与嘈杂的二进制分类的鲁棒对称损失的联系，并建立基于Wasserstein距离度量的相互信息最大化的新对比度，提供了严格的理论理由。拟议的损失完全是情态不平智的，并且可以简单地替换Infonce损失，这使得很容易应用于现有的对比框架。我们表明，我们的方法对图像，视频和图形对比度学习基准测试基准进行了一致的改进，这些基准表现出各种真实的噪声模式。

### A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2201.05299)] [[cool](https://papers.cool/arxiv/2201.05299)] [[pdf](https://arxiv.org/pdf/2201.05299)]
> **Authors**: Feng Gao,Qing Ping,Govind Thattai,Aishwarya Reganti,Ying Nian Wu,Prem Natarajan
> **First submission**: 2022-01-13
> **First announcement**: 2022-01-14
> **comment**: No comments
- **标题**: 一千个单词比一张图片更值得：以自然语言为中心的外部知识视觉问题回答
- **领域**: 计算机视觉和模式识别,计算语言学,信息检索
- **摘要**: 外部知识的视觉问题回答（OK-VQA）要求代理人理解图像，利用整个网络中的相关知识，并消化所有信息以回答问题。以前的大多数工作首先将图像和问题融合在多模式空间中，这是不灵活的，这对于与大量外部知识的进一步融合是不灵活的。在本文中，我们呼吁针对OK-VQA任务进行范式转换，该任务将图像转换为纯文本，以便我们可以在自然语言空间中启用知识段落检索和产生的提问。该范式利用了巨大的巨大知识基础和预训练的语言模型的丰富性。提出了一个转换 - 重新产生的框架（TRIG）框架，可以使用替代图像到文本模型和文本知识库来插入插件。实验结果表明，我们的TRIG框架的表现优于所有最新监督方法的绝对边距至少11.1％。

### Self-semantic contour adaptation for cross modality brain tumor segmentation 
[[arxiv](https://arxiv.org/abs/2201.05022)] [[cool](https://papers.cool/arxiv/2201.05022)] [[pdf](https://arxiv.org/pdf/2201.05022)]
> **Authors**: Xiaofeng Liu,Fangxu Xing,Georges El Fakhri,Jonghye Woo
> **First submission**: 2022-01-13
> **First announcement**: 2022-01-14
> **comment**: Accepted to ISBI 2022
- **标题**: 跨模态脑肿瘤分割的自节义轮廓适应
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 在两个显着不同的领域之间进行无监督的域适应（UDA），学习高级语义一致性是一项至关重要但又具有挑战性的任务。适应。更具体地说，我们提出了一个多任务框架，以学习一个轮廓适应网络以及语义分割适应网络，该网络既采用磁共振成像（MRI）切片及其初始边缘图作为输入。此外，将最小化的自我注入量纳入了进一步提高分割性能。我们评估了BRATS2018数据库的框架，用于与竞争方法相比，脑肿瘤的跨模式分割，显示了我们方法的有效性和优越性。

### Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels 
[[arxiv](https://arxiv.org/abs/2201.05986)] [[cool](https://papers.cool/arxiv/2201.05986)] [[pdf](https://arxiv.org/pdf/2201.05986)]
> **Authors**: Zipeng Ye,Mengfei Xia,Ran Yi,Juyong Zhang,Yu-Kun Lai,Xuwei Huang,Guoxin Zhang,Yong-jin Liu
> **First submission**: 2022-01-16
> **First announcement**: 2022-01-17
> **comment**: in IEEE Transactions on Multimedia
- **标题**: 通过动态卷积内核的音频驱动的说话面部视频生成
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 在本文中，我们提出了卷积神经网络的动态卷积内核（DCK）策略。使用所提出的DCK使用完全卷积的网络，可以实时从多模式源（即无与伦比的音频和视频）生成高质量的说话视频，并且我们训练的模型对不同的身份，头部姿势和输入音频都具有良好的功能。我们提出的DCK是专门为音频驱动的说话面部视频生成而设计的，从而导致了一个简单而有效的端到端系统。我们还提供了理论分析来解释为什么DCK起作用。实验结果表明，我们的方法可以生成具有60 fps的背景的高质量谈话视频。我们的方法和最新方法之间的比较和评估证明了我们方法的优越性。

### Tailor Versatile Multi-modal Learning for Multi-label Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2201.05834)] [[cool](https://papers.cool/arxiv/2201.05834)] [[pdf](https://arxiv.org/pdf/2201.05834)]
> **Authors**: Yi Zhang,Mingyuan Chen,Jundong Shen,Chongjun Wang
> **First submission**: 2022-01-15
> **First announcement**: 2022-01-17
> **comment**: To be published in AAAI 2022
- **标题**: 量身定制的多式模式学习，用于多标签情感识别
- **领域**: 计算机视觉和模式识别,多媒体
- **摘要**: 多模式多标签情感识别（MMER）旨在从异质视觉，音频和文本方式中识别各种人类情绪。先前的方法主要集中于将多种方式投射到一个共同的潜在空间中，并为所有标签学习相同的表示，这忽略了每种模式的多样性，并且从不同的角度捕获了每个标签的更丰富的语义信息。此外，尚未完全利用相关的方式和标签的关系。在本文中，我们提出了多种模式学习的多式模式学习（量身定制），旨在完善多模式表示并增强每个标签的判别能力。具体而言，我们设计了一个对抗性多模式改进模块，以充分探索不同方式之间的共同点并增强每种模式的多样性。为了进一步利用标签模式依赖性，我们设计了一个类似BERT的跨模式编码器，以粒度下降方式逐渐融合私人和常见的模态表示，以及标签引导的解码器，以适应为每个标签的标签代表生成量身定制的表示，并使用标签语义的指导。此外，我们在对齐和非对齐设置的基准MMER数据集CMU-MOSEI上进行实验，这证明了量身定制者优于最先进的。代码可在https://github.com/kniter1/tailor上找到。

### CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks 
[[arxiv](https://arxiv.org/abs/2201.05729)] [[cool](https://papers.cool/arxiv/2201.05729)] [[pdf](https://arxiv.org/pdf/2201.05729)]
> **Authors**: Zhecan Wang,Noel Codella,Yen-Chun Chen,Luowei Zhou,Jianwei Yang,Xiyang Dai,Bin Xiao,Haoxuan You,Shih-Fu Chang,Lu Yuan
> **First submission**: 2022-01-14
> **First announcement**: 2022-01-17
> **comment**: This paper is greatly modified and updated to be re-submitted to another conference. The new paper is under the name "MultimodalAdaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks", https://doi.org/10.48550/arXiv.2204.10496
- **标题**: 剪辑-TD：夹子式蒸馏视觉任务
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,多媒体
- **摘要**: 对比性语言图像预处理（剪辑）将视觉和语言方式连接到统一的嵌入空间，从而产生了视觉语言（VL）任务的巨大潜力。尽管早期的同时工作已经开始研究这一潜力，但重要的问题仍然存在：1）剪辑对未研究的VL任务有什么好处？ 2）剪辑在低射门或域偏移方案中是否提供好处？ 3）剪辑可以改善现有方法而不会影响推理或预处理复杂性吗？在这项工作中，我们试图通过两个主要贡献来回答这些问题。首先，我们介绍了一个评估协议，其中包括视觉常识性推理（VCR），视觉构成（SNLI-VE）和视觉问题答案（VQA），这些数据可用性约束和域移动条件。其次，我们提出了一种名为剪辑靶向蒸馏（夹-TD）的方法，以使用将动态加权的目标应用于每个实例适应性选择的令牌。实验表明，我们提出的夹TD导致VCR的低射击（高达51.9％）和域移位（高达71.3％）的范围（最高71.3％），同时在标准的完全监督条件下（高达2％）（高达2％）的性能在VCR上与其他单个模型相比，在标准的完全监督条件（高达2％）中提高了效果。在SNLI-VE上，夹-TD在低射击条件下（高达6.6％）以及完全监督（最高3％）可产生显着增长。在VQA上，Clip-TD可改善低射击（最高9％），并在全面监督的情况下（高达1.3％）。最后，剪辑-TD的表现优于同时使用夹子进行填充的工程，以及基线天真的蒸馏方法。代码将提供。

### Multimodal registration of FISH and nanoSIMS images using convolutional neural network models 
[[arxiv](https://arxiv.org/abs/2201.05545)] [[cool](https://papers.cool/arxiv/2201.05545)] [[pdf](https://arxiv.org/pdf/2201.05545)]
> **Authors**: Xiaojia He,Christof Meile,Suchendra M. Bhandarkar
> **First submission**: 2022-01-14
> **First announcement**: 2022-01-17
> **comment**: No comments
- **标题**: 使用卷积神经网络模型的鱼类和纳米菌图像的多模式登记
- **领域**: 计算机视觉和模式识别
- **摘要**: 纳米级次级离子质谱法（纳米sims）和荧光原位杂交（FISH）显微镜分别提供了微生物研究中靶向微生物群落的身份和细胞活性的高分辨率，多模式图像表示。尽管对微生物学家的重要性很重要，但鉴于这两个图像中的形态失真和背景噪声，鱼类和纳米图像的多模式登记都在挑战。在这项研究中，我们使用卷积神经网络（CNN）进行多尺度特征提取，用于计算最小转换成本特征匹配的形状上下文和薄板样条（TPS）模型，以用于鱼类和纳米IMIMS图像的多模式注册。使用标准指标对手动注册图像进行定量评估注册精度。尽管所有六个经过测试的CNN模型都表现良好，但根据大多数指标，RESNET18的表现都超过了VGG16，VGG19，GOGLENET和SHUFFLENET和RESNET101。这项研究证明了CNN在具有明显的背景噪声和形态扭曲的多模式图像的注册中的实用性。我们还显示通过二进化保留的聚合形状，是注册多模式微生物学相关图像的可靠特征。

### AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2201.06493)] [[cool](https://papers.cool/arxiv/2201.06493)] [[pdf](https://arxiv.org/pdf/2201.06493)]
> **Authors**: Zehui Chen,Zhenyu Li,Shiquan Zhang,Liangji Fang,Qinghong Jiang,Feng Zhao,Bolei Zhou,Hang Zhao
> **First submission**: 2022-01-17
> **First announcement**: 2022-01-18
> **comment**: Accepted to IJCAI2022
- **标题**: AutoAlign：多模式3D对象检测的Pixel-Instance功能聚合
- **领域**: 计算机视觉和模式识别
- **摘要**: 通过RGB图像或LiDAR点云通过自主驾驶中广泛探索的对象检测。但是，使这两个数据源互补并且彼此有益是一项挑战。在本文中，我们建议\ textit {autoAlign}，这是3D对象检测的自动功能融合策略。我们没有建立与摄像机投影矩阵的确定性对应关系，而是用可学习的对齐图对图像和点云之间的映射关系进行建模。该地图使我们的模型能够以动态和数据驱动的方式自动化非殖民特征的对齐。具体而言，设计了一个跨注意功能对齐模块，以适应每个体素的汇总\ textit {pixel-level}图像特征。为了增强功能对齐过程中的语义一致性，我们还设计了一个自我监视的跨模式交互模块，模型可以通过\ textit {instance-level}特征指导来学习特征聚合。广泛的实验结果表明，我们的方法可以分别在Kitti和Nuscenes数据集上进行2.3 MAP和7.0 MAP改进。值得注意的是，我们的最佳模型在Nuscenes测试排行榜上达到了70.9 ND，在各种最先进的情况下取得了竞争性能。

### TriCoLo: Trimodal Contrastive Loss for Text to Shape Retrieval 
[[arxiv](https://arxiv.org/abs/2201.07366)] [[cool](https://papers.cool/arxiv/2201.07366)] [[pdf](https://arxiv.org/pdf/2201.07366)]
> **Authors**: Yue Ruan,Han-Hung Lee,Yiming Zhang,Ke Zhang,Angel X. Chang
> **First submission**: 2022-01-18
> **First announcement**: 2022-01-19
> **comment**: Accepted by WACV 2024
- **标题**: Tricolo：文本三曲置对比损失以形成检索
- **领域**: 计算机视觉和模式识别
- **摘要**: 与3D形状数据的增长，文本对形状的检索是一个日益相关的问题。关于学习联合嵌入在多模式数据的对比损失方面的最新工作已在诸如检索和分类之类的任务中成功。到目前为止，在3D形状和文本的联合表示学习上的工作着重于通过在表示形式或多任务学习之间建模复杂的注意力来改善嵌入。我们在文本，多视图图像和3D形状体素上提出了三峰学习方案，并表明，通过大批次对比度学习，我们在文本对形状的检索上实现了良好的性能，而没有复杂的注意机制或损失。我们的实验是为构建文本图像形状的三座嵌入的后续工作的基础。

### Context-Aware Scene Prediction Network (CASPNet) 
[[arxiv](https://arxiv.org/abs/2201.06933)] [[cool](https://papers.cool/arxiv/2201.06933)] [[pdf](https://arxiv.org/pdf/2201.06933)]
> **Authors**: Maximilian Schäfer,Kun Zhao,Markus Bühren,Anton Kummert
> **First submission**: 2022-01-18
> **First announcement**: 2022-01-19
> **comment**: 9 pages, 8 figures
- **标题**: 上下文感知场景预测网络（CASPNET）
- **领域**: 计算机视觉和模式识别
- **摘要**: 预测周围道路使用者的未来运动是自动驾驶（AD）和各种高级驾驶员辅助系统（ADA）的至关重要且具有挑战性的任务。规划安全的未来轨迹在很大程度上取决于了解交通现场并预期其动态。这些挑战不仅在于了解复杂的驾驶场景，而且还在于道路使用者和环境之间的众多可能的相互作用，这实际上对于明确的建模而言是不可行的。在这项工作中，我们使用基于新型的卷积神经网络（CNN）和基于重复的神经网络（RNN）体系结构共同学习和预测场景中所有道路使用者的运动来应对上述挑战。此外，通过利用基于网格的输入和输出数据结构，计算成本独立于道路用户的数量，多模式预测成为我们提出的方法的固有属性。对Nuscenes数据集的评估表明，我们的方法达到了预测基准的最先进结果。

### TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning 
[[arxiv](https://arxiv.org/abs/2201.07451)] [[cool](https://papers.cool/arxiv/2201.07451)] [[pdf](https://arxiv.org/pdf/2201.07451)]
> **Authors**: Linhao Qu,Shaolei Liu,Manning Wang,Shiman Li,Siqi Yin,Qin Qiao,Zhijian Song
> **First submission**: 2022-01-19
> **First announcement**: 2022-01-20
> **comment**: No comments
- **标题**: 输出：一个基于统一变压器的图像融合框架使用自我监督的学习
- **领域**: 计算机视觉和模式识别
- **摘要**: 图像融合是一种将来自多个源图像的信息与互补信息整合在一起的技术，以改善单个图像的丰富性。由于特定于任务的训练数据和相应的地面真相，大多数现有的端到端图像融合方法很容易属于过度拟合或乏味的参数优化过程。两阶段的方法通过在大型自然图像数据集上训练编码器 - 编码器网络避免了大量特定于任务的训练数据，并利用提取的功能进行融合，但是自然图像和不同的融合任务之间的域间隙导致性能有限。在这项研究中，我们设计了一种基于编码器的新型图像融合框架，并提出了基于破坏造成的自我监督训练方案，以鼓励网络学习特定于任务的特征。具体而言，我们提出了三个破坏性重建自我监督的辅助任务，用于多模式图像融合，多曝光图像融合和基于像素强度非线性转换，亮度转换和噪声转换的多对聚焦图像融合。为了鼓励不同的融合任务互相提升并提高训练有素的网络的普遍性，我们通过随机选择其中一个在模型培训中破坏自然图像来整合三个自我监管的辅助任务。此外，我们设计了一个新的编码器，将CNN和Transformer结合在一起以进行特征提取，以便训练有素的模型可以利用本地和全局信息。关于多模式图像融合，多曝光图像融合和多聚焦图像融合任务的广泛实验表明，我们所提出的方法在主观和客观评估中都实现了最新的性能。该代码将很快公开可用。

### WebUAV-3M: A Benchmark for Unveiling the Power of Million-Scale Deep UAV Tracking 
[[arxiv](https://arxiv.org/abs/2201.07425)] [[cool](https://papers.cool/arxiv/2201.07425)] [[pdf](https://arxiv.org/pdf/2201.07425)]
> **Authors**: Chunhui Zhang,Guanjie Huang,Li Liu,Shan Huang,Yinan Yang,Xiang Wan,Shiming Ge,Dacheng Tao
> **First submission**: 2022-01-19
> **First announcement**: 2022-01-20
> **comment**: 25 pages
- **标题**: Webuav-3M：揭示百万级深色无人机跟踪力量的基准
- **领域**: 计算机视觉和模式识别
- **摘要**: 无人驾驶飞机（UAV）跟踪对于诸如交货和农业等广泛的应用具有重要意义。该领域的先前基准测试主要集中在小规模的跟踪问题上，同时忽略了数据模式的类型，目标类别和方案的多样性以及所涉及的评估协议的数量，从而极大地隐藏了深度无人机跟踪的巨大功能。在这项工作中，我们提出了迄今为止最大的公共无人机跟踪基准Webuav-3M，以促进深度无人机跟踪器的开发和评估。 Webuav-3M在4,500个视频中包含超过330万帧，并提供223个高度多样化的目标类别。每个视频都通过有效且可扩展的半自动目标注释（SATA）管道密集注释。重要的是，要利用语言和音频的互补优势，我们通过提供自然语言规范和音频描述来丰富Webuav-3M。我们认为，这种补充将大大促进未来的研究，以探索语言功能和音频提示，用于多模式无人机跟踪。此外，构建了scenario约束（UTUSC）评估协议和七个具有挑战性的场景子测验集，以使社区能够开发，适应和评估各种类型的高级跟踪器。我们提供了43个代表性跟踪器的广泛评估和详细分析，并设想了深度无人机跟踪及其他领域的未来研究方向。数据集，工具包和基线结果可在\ url {https://github.com/983632847/webuav-3m}上获得。

### End-to-end Generative Pretraining for Multimodal Video Captioning 
[[arxiv](https://arxiv.org/abs/2201.08264)] [[cool](https://papers.cool/arxiv/2201.08264)] [[pdf](https://arxiv.org/pdf/2201.08264)]
> **Authors**: Paul Hongsuck Seo,Arsha Nagrani,Anurag Arnab,Cordelia Schmid
> **First submission**: 2022-01-20
> **First announcement**: 2022-01-21
> **comment**: ef:Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR) 2022
- **标题**: 多模式视频字幕的端到端生成预处理
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,人机交互
- **摘要**: 最近的视频和语言预处理框架缺乏生成句子的能力。我们提出了多模式的视频生成预处理（MV-GPT），这是一个新的预处理框架，用于从未标记的视频中学习，可有效地用于生成任务，例如多模式视频字幕。与最近的视频预科框架不同，我们的框架既训练多模式的视频编码器，又可以共同训练句子解码器。为了克服未标记视频中缺乏字幕的标题，我们利用未来的话语作为附加的文本来源，并提出了双向发电目标 - 鉴于当前的穆里特莫代语环境以及当前的话语，我们会产生未来的话语，并且鉴于未来的观察。有了这个目标，我们训练一个编码器模型的端到端，以直接从原始像素中产生字幕并直接转录语音。我们的模型实现了在四个标准基准上进行多模式视频字幕的最新性能，以及其他视频理解任务，例如VideoQA，视频检索和动作分类。

### Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing 
[[arxiv](https://arxiv.org/abs/2201.08125)] [[cool](https://papers.cool/arxiv/2201.08125)] [[pdf](https://arxiv.org/pdf/2201.08125)]
> **Authors**: Georgii Mikriukov,Mahdyar Ravanbakhsh,Begüm Demir
> **First submission**: 2022-01-20
> **First announcement**: 2022-01-21
> **comment**: No comments
- **标题**: 遥感大规模跨模式图像检索的深度无监督对比度
- **领域**: 计算机视觉和模式识别
- **摘要**: 由于可用的大规模多模式数据（例如，由不同的传感器，文本句子等获取的卫星图像）档案的开发，可以开发跨模式检索系统，这些系统可以基于任何模式的查询在不同模式中搜索和检索不同模式的语义相关数据，这引起了RS的极大关注。在本文中，我们将注意力集中在跨模式的文本图像检索上，其中一种模式（例如，文本）的查询可以匹配到另一个模式（例如，图像）。大多数现有的跨模式文本图像检索系统都需要大量标记的训练样本，并且由于其内在特征，也不允许快速和记忆效率的检索。这些问题限制了现有的跨模式检索系统在卢比中的大规模应用程序中的适用性。为了解决这个问题，在本文中，我们引入了一种新颖的无监督的跨模式对比度哈希（Duch）方法，用于RS文本图像检索。提出的杜切由两个主要模块组成：1）特征提取模块（提取文本图像模态的深度表示）； 2）哈希模块（该模块学会从提取的表示形式中生成跨模式二进制哈希码）。在哈希模块中，我们引入了一种新型的多目标损耗函数，包括：i）对比目标，可以在内部和模式间相似性中保持相似性； ii）对跨模式表示一致性的两种方式强制执行的对抗目标； iii）生成代表性哈希代码的二进制目标。实验结果表明，在两个多模式（图像和文本）基准的基准档案中，提出的Duch优于最先进的跨模式哈希方法。我们的代码可在https://git.tu-berlin.de/rsim/duch上公开获取。

### Temporal Sentence Grounding in Videos: A Survey and Future Directions 
[[arxiv](https://arxiv.org/abs/2201.08071)] [[cool](https://papers.cool/arxiv/2201.08071)] [[pdf](https://arxiv.org/pdf/2201.08071)]
> **Authors**: Hao Zhang,Aixin Sun,Wei Jing,Joey Tianyi Zhou
> **First submission**: 2022-01-20
> **First announcement**: 2022-01-21
> **comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
- **标题**: 视频中的时间句子接地：调查和未来方向
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体
- **摘要**: 视频（TSGV），\又名自然语言视频本地化（NLVL）或视频瞬间检索（VMR）中的时间句子接地（aka自然语言视频本地化（NLVL），旨在检索暂时的时刻，该时刻在语义上对应于未修剪视频中的语言查询。 TSGV连接计算机视觉和自然语言，引起了两个社区研究人员的极大关注。这项调查试图提供有关TSGV和当前研究状况以及未来研究方向的基本概念的摘要。作为背景，我们以教程样式介绍了TSGV中功能组件的共同结构：从原始视频和语言查询的功能提取，以回答对目标时刻的预测。然后，我们回顾了多模式理解和相互作用的技术，这是TSGV在两种方式之间有效排列的关键重点。我们构建了TSGV技术的分类法，并凭借其优点和劣势详细说明了不同类别的方法。最后，我们与当前的TSGV研究讨论问题，并分享有关有前途的研究方向的见解。

### Mutual Attention-based Hybrid Dimensional Network for Multimodal Imaging Computer-aided Diagnosis 
[[arxiv](https://arxiv.org/abs/2201.09421)] [[cool](https://papers.cool/arxiv/2201.09421)] [[pdf](https://arxiv.org/pdf/2201.09421)]
> **Authors**: Yin Dai,Yifan Gao,Fayu Liu,Jun Fu
> **First submission**: 2022-01-23
> **First announcement**: 2022-01-24
> **comment**: 11 pages, 8 figures
- **标题**: 用于多模式成像计算机辅助诊断的基于相互注意的混合尺寸网络
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 关于多模式3D计算机辅助诊断的最新著作表明，当3D卷积神经网络（CNN）带来更多参数和医学图像时，获得竞争性自动诊断模型仍然不足和具有挑战性。考虑到多模式图像和诊断准确性的兴趣区域的两种一致性，我们提出了一种新型的基于相互注意的混合维度网络，用于多模式3D医疗图像分类（MMNET）。混合尺寸网络将2D CNN与3D卷积模块集成在一起，以生成更深入，更有信息的特征图，并降低3D融合的训练复杂性。此外，可以在2D CNN中使用预训练的ImageNet模型，从而改善了模型的性能。立体注意力集中在3D医学图像中建立该地区丰富的上下文相互依赖性。为了改善多模式医学图像中病理组织的区域相关性，我们进一步设计了网络中的相互关注框架，以在不同图像模态的相似立体镜面区域建立区域一致性，从而提供了一种隐含的方式来指导网络专注于病理组织。 MMNET的表现优于以前的许多解决方案，并且在三个多模式成像数据集（即腮腺肿瘤（PGT）数据集，MRNET数据集和Prostatex数据集及其优势的三个多模式成像数据集（即，在三个多模式成像数据集）上的最新结果竞争。

### Visual Object Tracking on Multi-modal RGB-D Videos: A Review 
[[arxiv](https://arxiv.org/abs/2201.09207)] [[cool](https://papers.cool/arxiv/2201.09207)] [[pdf](https://arxiv.org/pdf/2201.09207)]
> **Authors**: Xue-Feng Zhu,Tianyang Xu,Xiao-Jun Wu
> **First submission**: 2022-01-23
> **First announcement**: 2022-01-24
> **comment**: I prefer not to present this paper due to its subpar quality
- **标题**: 多模式RGB-D视频上的视觉对象跟踪：评论
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉对象跟踪的开发持续了数十年。近年来，作为低成本RGBD传感器的广泛可访问性，在RGB-D视频上进行视觉对象跟踪的任务引起了很多关注。与传统的仅RGB跟踪相比，RGB-D视频可以提供更多信息，从而有助于在某些复杂的情况下进行反对跟踪。这篇评论的目的是总结RGB-D跟踪提交的研究的相对知识。具体来说，我们将概括相关的RGB-D跟踪基准测定数据集以及相应的性能测量。此外，本文总结了现有的RGB-D跟踪方法。此外，我们讨论了RGB-D跟踪领域可能的未来方向。

### FedMed-GAN: Federated Domain Translation on Unsupervised Cross-Modality Brain Image Synthesis 
[[arxiv](https://arxiv.org/abs/2201.08953)] [[cool](https://papers.cool/arxiv/2201.08953)] [[pdf](https://arxiv.org/pdf/2201.08953)]
> **Authors**: Jinbao Wang,Guoyang Xie,Yawen Huang,Jiayi Lyu,Yefeng Zheng,Feng Zheng,Yaochu Jin
> **First submission**: 2022-01-21
> **First announcement**: 2022-01-24
> **comment**: No comments
- **标题**: Fedmed-GAN：无监督的交叉模式大脑图像合成的联合域翻译
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 利用多模式神经影像学数据已被证明可有效研究人类的认知活性和某些病理。但是，由于该系列面临着几个限制，例如高检查成本，长期获取时间和图像损坏，因此中心地获得了整个配对的神经影像学数据并不实用。此外，这些数据被分散到不同的医疗机构中，因此，考虑到隐私问题，不能汇总用于集中培训。显然需要启动联邦学习，并促进来自不同机构的分散数据的整合。在本文中，我们为无监督的大脑图像合成（称为FedMen-gan）提出了一个新的基准，以弥合联邦学习和医学gan之间的差距。 FedMed-GAN可减轻模式崩溃而不牺牲发电机的性能，并广泛应用于具有变化适应属性的不同比例的未配对和配对数据。我们通过联邦平均算法来处理梯度惩罚，然后利用差异隐私梯度下降来正规化训练动力。提供了全面的评估，用于比较FedMen-Gan和其他集中式方法，该方法显示了我们的FedMed-Gan的新最先进的性能。我们的代码已在网站上发布：https：//github.com/m-3lab/fedmed-gan

### Temporal Aggregation for Adaptive RGBT Tracking 
[[arxiv](https://arxiv.org/abs/2201.08949)] [[cool](https://papers.cool/arxiv/2201.08949)] [[pdf](https://arxiv.org/pdf/2201.08949)]
> **Authors**: Zhangyong Tang,Tianyang Xu,Xiao-Jun Wu
> **First submission**: 2022-01-21
> **First announcement**: 2022-01-24
> **comment**: 12 pages, 10 figures
- **标题**: 自适应RGBT跟踪的时间聚集
- **领域**: 计算机视觉和模式识别
- **摘要**: 使用RGB和热红外（TIR）光谱在RGBT跟踪中可用的视觉对象跟踪是一个新颖而充满挑战的研究主题，如今引起了人们越来越多的关注。在本文中，我们提出了一个RGBT跟踪器，该跟踪器将时空线索构成了稳健的外观模型学习，同时构建了用于交叉模式相互作用的自适应融合子网络。与仅包含空间信息的大多数现有的RGBT跟踪器不同，在此方法中进一步考虑了时间信息。 Specifically, different from traditional Siamese trackers, which only obtain one search image during the process of picking up template-search image pairs, an extra search sample adjacent to the original one is selected to predict the temporal transformation, resulting in improved robustness of tracking performance.As for multi-modal tracking, constrained to the limited RGBT datasets, the adaptive fusion sub-network is appended to our method at the decision level to reflect the complementary characteristics以两种方式包含。为了设计热红外辅助RGB跟踪器，在RGB模式的残留连接之前，请考虑从TIR模式中分类头的输出。对三个具有挑战性的数据集的广泛实验结果，即GTOT-RGBT2019，GTOT和RGBT210，验证了我们方法的有效性。代码将以\ textColor {blue} {\ emph {https://github.com/zhangyong-tang/taat}}共享。

### Exploring Fusion Strategies for Accurate RGBT Visual Object Tracking 
[[arxiv](https://arxiv.org/abs/2201.08673)] [[cool](https://papers.cool/arxiv/2201.08673)] [[pdf](https://arxiv.org/pdf/2201.08673)]
> **Authors**: Zhangyong Tang,Tianyang Xu,Hui Li,Xiao-Jun Wu,Xuefeng Zhu,Josef Kittler
> **First submission**: 2022-01-21
> **First announcement**: 2022-01-24
> **comment**: 13 pages, 10 figures
- **标题**: 探索融合策略以进行准确的RGBT视觉对象跟踪
- **领域**: 计算机视觉和模式识别
- **摘要**: 我们解决了视频中多模式对象跟踪的问题，并探索了各种选择，以融合可见光（RGB）和热红外（TIR）模式（包括像素级，功能级，决策级和决策级融合）的互补信息。具体而言，与现有方法不同，图像融合任务的范式在像素级别上被融合了。功能级融合可以通过可选的通道来实现。此外，在决策级别上，提出了一种新颖的融合策略，因为毫无轻松的平均配置表明了优越性。提议的决策级融合策略的有效性归功于许多创新的贡献，包括RGB和TIR贡献的动态权重以及线性模板更新操作。它的一种变体在2020年视觉对象跟踪挑战赛（fot-rgbt2020）上产生了获胜跟踪器。对创新像素和特征级融合策略的并发探索突出了拟议的决策级融合方法的优势。与最先进的方法相比，对三个具有挑战性的数据集（\ textit {i.e。}，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT，GTOT {I.E。}，广泛的实验结果与最先进的方法相比，这证明了该方法的有效性和鲁棒性。代码将在\ textColor {blue} {\ emph {https://github.com/zhangyong-tang/dfat}上共享。

### Question Generation for Evaluating Cross-Dataset Shifts in Multi-modal Grounding 
[[arxiv](https://arxiv.org/abs/2201.09639)] [[cool](https://papers.cool/arxiv/2201.09639)] [[pdf](https://arxiv.org/pdf/2201.09639)]
> **Authors**: Arjun R. Akula
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: No comments
- **标题**: 评估多模式接地中跨数据库变化的问题生成
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉问题回答（VQA）是回答有关输入图像的自然语言问题的多模式任务。通过跨数据库改编方法，可以将知识从具有较大火车样本的源数据集传输到训练集有限的目标数据集。假设在一个数据集火车集中训练的VQA模型无法适应另一个数据集，因此很难确定域失配的根本原因，因为可能存在多种原因，例如图像分布不匹配和问题分布不匹配。在UCLA，我们正在开发一个VQG模块，该模块有助于自动生成OOD偏移，有助于系统地评估VQA模型的跨数据库适应能力。

### SEN12MS-CR-TS: A Remote Sensing Data Set for Multi-modal Multi-temporal Cloud Removal 
[[arxiv](https://arxiv.org/abs/2201.09613)] [[cool](https://papers.cool/arxiv/2201.09613)] [[pdf](https://arxiv.org/pdf/2201.09613)]
> **Authors**: Patrick Ebel,Yajin Xu,Michael Schmitt,Xiaoxiang Zhu
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: ef:IEEE Transactions on Geoscience and Remote Sensing, 2022
- **标题**: SEN12MS-CR-TS：用于多模式多阶段云的遥感数据集
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **摘要**: 大约一半是通过太空传播卫星收集的所有光学观察结果都受雾化或云的影响。因此，云覆盖范围会影响遥感从业者对我们星球进行连续无缝监测的能力。这项工作通过提出一个称为SEN12MS-CR-TS的新型多模式和多时间数据集，解决了光学卫星图像重建和去除云的挑战。我们提出了两个模型，强调了SEN12MS-CR-TS的好处和用例：首先，是一种多模式多阶段3D-Convolution神经网络，该神经网络可预测一系列云状光学图像和雷达图像的序列。其次，一个序列到序列翻译模型，可预测云覆盖时间序列的无云时间序列。两种方法均通过实验评估，其各自的模型对SEN12MS-CR-TS进行了训练和测试。进行的实验突出了我们的数据集对遥感社区的贡献以及多模式和多时间信息对重建噪声信息的好处。我们的数据集可在https://patricktum.github.io/cloud_removal上找到

### Multi-Scale Iterative Refinement Network for RGB-D Salient Object Detection 
[[arxiv](https://arxiv.org/abs/2201.09574)] [[cool](https://papers.cool/arxiv/2201.09574)] [[pdf](https://arxiv.org/pdf/2201.09574)]
> **Authors**: Ze-yu Liu,Jian-wei Liu,Xin Zuo,Ming-fei Hu
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: 40 pages
- **标题**: RGB-D显着对象检测的多尺度迭代改进网络
- **领域**: 计算机视觉和模式识别
- **摘要**: 在显着对象检测中利用了利用RGB-D信息的广泛研究。但是，由于语义差距在不同的特征级别上，显着的视觉提示以各种尺度和RGB图像的分辨率出现。同时，类似的显着模式在跨模式深度图像以及多尺度版本中也提供。在RGB-D显着对象检测任务中，跨模式融合和多尺度改进仍然是一个开放的问题。在本文中，我们首先引入自上而下和自下而上的迭代修复体系结构以利用多尺度功能，然后设计基于注意力的融合模块（ABF）以解决跨模式相关性。我们在七个公共数据集上进行了广泛的实验。实验结果显示了我们设计的方法的有效性

### MGA-VQA: Multi-Granularity Alignment for Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2201.10656)] [[cool](https://papers.cool/arxiv/2201.10656)] [[pdf](https://arxiv.org/pdf/2201.10656)]
> **Authors**: Peixi Xiong,Yilin Shen,Hongxia Jin
> **First submission**: 2022-01-25
> **First announcement**: 2022-01-26
> **comment**: No comments
- **标题**: MGA-VQA：视觉问题回答的多粒度对齐
- **领域**: 计算机视觉和模式识别
- **摘要**: 学会回答视觉问题是一项具有挑战性的任务，因为多模式输入在两个特征空间内。此外，视觉问题回答中的推理要求模型同时了解图像和问题，并在同一空间中对齐它们，而不是简单地记住有关问题解答对的统计信息。因此，必须在不同方式和每种方式之间找到组件连接，以获得更好的关注。以前的作品直接学习了关注权重。但是，改进是有限的，因为这两个模式特征在两个领域中：图像特征是高度多样的，缺乏结构和语法规则作为语言，而自然语言特征的可能性更高，可能会丢失详细信息。为了更好地了解视觉和文本之间的注意力，我们关注如何构建输入分层和嵌入结构信息，以改善不同级别组件之间的对齐。我们提出了用于视觉问题答案任务（MGA-VQA）的多粒性对齐结构，该架构通过多粒度对齐来学习内部和模式间相关性，并通过决策融合模块输出最终结果。与以前的工作相反，我们的模型将对齐方式分为不同的级别，以实现更好的学习，而无需其他数据和注释。 VQA-V2和GQA数据集上的实验表明，我们的模型在没有额外的数据和注释的情况下显着优于两个数据集上的未经预先最先进的方法。此外，它甚至可以在GQA上的预训练方法上取得更好的结果。

### SA-VQA: Structured Alignment of Visual and Semantic Representations for Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2201.10654)] [[cool](https://papers.cool/arxiv/2201.10654)] [[pdf](https://arxiv.org/pdf/2201.10654)]
> **Authors**: Peixi Xiong,Quanzeng You,Pei Yu,Zicheng Liu,Ying Wu
> **First submission**: 2022-01-25
> **First announcement**: 2022-01-26
> **comment**: No comments
- **标题**: SA-VQA：视觉和语义表示的结构化对齐视觉问题回答
- **领域**: 计算机视觉和模式识别
- **摘要**: 视觉问题回答（VQA）引起了行业和学术界的广泛关注。作为一项多模式任务，它具有挑战性，因为它不仅需要视觉和文本理解，还需要使跨模式表示的能力。以前的方法广泛采用实体级别的对齐方式，例如视觉区域及其语义标签之间的相关性，或跨越词和对象特征的相互作用。这些尝试旨在改善跨模式表示，同时忽略其内部关系。取而代之的是，我们建议应用结构化的对齐，这些对齐与视觉和文本内容的图表示，旨在捕获视觉和文本方式之间的深入连接。然而，代表和整合结构化对齐的图并不容易。在这项工作中，我们试图通过首先将不同的模态实体转换为顺序节点和邻接图来解决此问题，然后将它们合并为结构化对齐。正如我们的实验结果所证明的那样，这种结构化的对准可以提高推理性能。此外，我们的模型还可以为每个生成的答案提供更好的解释性。所提出的模型在没有任何预处理的情况下优于GQA数据集上的最新方法，并在VQA-V2数据集上击败了非预先使用的最新方法。

### Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Multi-Person Video 
[[arxiv](https://arxiv.org/abs/2201.10439)] [[cool](https://papers.cool/arxiv/2201.10439)] [[pdf](https://arxiv.org/pdf/2201.10439)]
> **Authors**: Dmitriy Serdyuk,Otavio Braga,Olivier Siohan
> **First submission**: 2022-01-25
> **First announcement**: 2022-01-26
> **comment**: 5 pages, 3 figures, published at Interspeech 2022
- **标题**: 基于变压器的视频前端，用于单一和多人视频的视听语音识别
- **领域**: 计算机视觉和模式识别,机器学习,声音,音频和语音处理
- **摘要**: 视听自动语音识别（AV-ASR）通过引入视频方式作为其他信息来源，扩展了语音识别。在这项工作中，使用说话者嘴的运动中包含的信息用于增强音频功能。传统上，该视频模式是通过3D卷积神经网络（例如VGG的3D版本）处理的。最近，图像变压器网络ARXIV：2010.11929展示了为图像分类任务提取丰富的视觉特征的能力。在这里，我们建议用视频变压器替换3D卷积以提取视觉特征。我们在YouTube视频的大规模语料库上训练基准和提议的模型。在YouTube视频的标记子集以及LRS3-TED公共语料库中评估了我们的方法的性能。我们最好的仅视频模型在YTDEV18上获得了31.4％的WER，在LRS3-TED上获得了17.0％，比我们的卷积基线获得了10％和15％的相对改善。在微调模型（1.6％WER）之后，我们实现了在LRS3-TED上进行视听识别的最先进的状态。此外，在一系列关于多人AV-ASR的实验中，我们在卷积视频前端的平均相对降低了2％。

### TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network 
[[arxiv](https://arxiv.org/abs/2201.10147)] [[cool](https://papers.cool/arxiv/2201.10147)] [[pdf](https://arxiv.org/pdf/2201.10147)]
> **Authors**: Dongyu Rao,Xiao-Jun Wu,Tianyang Xu
> **First submission**: 2022-01-25
> **First announcement**: 2022-01-26
> **comment**: No comments
- **标题**: TGFUSE：基于变压器和生成对抗网络的红外且可见的图像融合方法
- **领域**: 计算机视觉和模式识别
- **摘要**: 端到端图像融合框架已实现了有希望的性能，专用的卷积网络汇总了多模式的本地外观。但是，在现有的CNN融合方法中直接忽略了远程依赖性，阻碍了整个图像级别的感知在复杂场景融合中的平衡。因此，在本文中，我们提出了一种基于轻质变压器模块和对抗性学习的红外且可见的图像融合算法。受到全球互动能力的启发，我们使用变压器技术来学习有效的全球融合关系。特别是，CNN提取的浅特征在提出的变压器融合模块中相互作用，以优化空间范围内和跨通道内的融合关系。此外，对抗性学习是在训练过程中设计的，以通过从输入中施加竞争性一致性来改善输出歧视，这反映了红外和可见图像中的特定特征。实验性能证明了所提出的模块的有效性，并在融合任务中通过变压器和对抗性学习概括了一种新型范式，并具有出色的改进。

### Self-attention fusion for audiovisual emotion recognition with incomplete data 
[[arxiv](https://arxiv.org/abs/2201.11095)] [[cool](https://papers.cool/arxiv/2201.11095)] [[pdf](https://arxiv.org/pdf/2201.11095)]
> **Authors**: Kateryna Chumachenko,Alexandros Iosifidis,Moncef Gabbouj
> **First submission**: 2022-01-26
> **First announcement**: 2022-01-27
> **comment**: No comments
- **标题**: 通过不完整的数据，自我发挥的融合，用于视听情绪识别
- **领域**: 计算机视觉和模式识别,声音,音频和语音处理
- **摘要**: 在本文中，我们考虑了多模式数据分析的问题，并具有视听情感识别的用例。我们提出了一种能够从原始数据中学习的体系结构，并用不同的方式融合机制描述了它的三种变体。尽管以前的大多数作品都考虑了推断期间始终存在两种方式的理想情况，但我们在不受约束的环境中评估了模型的鲁棒性，在这种环境中，一种模式不存在或嘈杂，并提出了一种方法，以一种以一种模态辍学的形式来减轻这些限制的方法。最重要的是，我们发现遵循这种方法不仅在一种模式的缺席/嘈杂表示下会大大提高性能，而且还可以在标准理想环境中提高性能，从而优于竞争方法。

### Discriminative Supervised Subspace Learning for Cross-modal Retrieval 
[[arxiv](https://arxiv.org/abs/2201.11843)] [[cool](https://papers.cool/arxiv/2201.11843)] [[pdf](https://arxiv.org/pdf/2201.11843)]
> **Authors**: Haoming Zhang,Xiao-Jun Wu,Tianyang Xu,Donglin Zhang
> **First submission**: 2022-01-26
> **First announcement**: 2022-01-28
> **comment**: No comments
- **标题**: 跨模式检索的判别监督子空间学习
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **摘要**: 如今，异质数据之间的度量仍然是跨模式检索的开放问题。跨模式检索的核心是如何衡量不同类型数据之间的相似性。已经开发了许多方法来解决问题。作为主流之一，基于子空间学习的方法要注意学习一个共同的子空间，在该子空间中可以直接测量多模式数据之间的相似性。但是，许多现有方法仅着眼于学习潜在子空间。他们忽略了歧视性信息的全部使用，因此语义上的结构信息不能得到很好的保存。因此，令人满意的结果无法按预期实现。我们在本文中提出了跨模式检索（DS2L）的歧视性监督子空间学习，以充分利用判别信息并更好地保留语义上的结构信息。具体来说，我们首先构建一个共享的语义图，以保留每种模式中的语义结构。随后，引入了Hilbert-Schmidt独立标准（HSIC），以保留样品的特征相似性和语义相似性之间的一致性。第三，我们介绍了一个相似性保存项，因此我们的模型可以弥补使用判别性数据不足的缺点，并更好地保留每种模式中的语义结构信息。在三个众所周知的基准数据集上获得的实验结果证明了对经典子空间学习方法的提议方法的有效性和竞争力。

### Pressure Eye: In-bed Contact Pressure Estimation via Contact-less Imaging 
[[arxiv](https://arxiv.org/abs/2201.11828)] [[cool](https://papers.cool/arxiv/2201.11828)] [[pdf](https://arxiv.org/pdf/2201.11828)]
> **Authors**: Shuangjun Liu,Sarah Ostadabbas
> **First submission**: 2022-01-27
> **First announcement**: 2022-01-28
> **comment**: No comments
- **标题**: 压力眼：通过无接触成像进行床内接触压力估算
- **领域**: 计算机视觉和模式识别
- **摘要**: 计算机视觉在解释图像的语义含义方面取得了巨大的成功，但是估计对象的基本（非视觉）物理特性通常仅限于其批量值，而不是重建密集的图。在这项工作中，我们介绍了压力眼（PEYE）方法，以估计人体与她所躺着的表面之间的接触压力，直接从视觉信号直接通过视觉信号分辨率。 PEYE方法最终可以使床结合患者的压力溃疡的预测和早期检测，这目前取决于使用昂贵的压力垫。我们的PEYE网络以双重编码共享解码形式配置，以融合视觉提示和一些相关的物理参数，以重建高分辨率压力图（PMS）。我们还基于天真的贝叶斯假设提出了一种像素的重新采样方法，以进一步提高PM回归性能。还提出了针对感应估计精度评估的正确传感（PC）的百分比，这为在不同误差公差下的性能评估提供了另一种观点。我们通过一系列广泛的实验测试了我们的方法，使用多模式传感技术在躺在床上时收集102名受试者的数据。个人的高分辨率接触压力数据可以从其RGB或长波长红外（LWIR）图像中估算为91.8％和91.2％的估计精度，$ pcs_ {efs0.1} $标准，在相关图像回归/翻译任务中优于目前的最终方法。

### Team Yao at Factify 2022: Utilizing Pre-trained Models and Co-attention Networks for Multi-Modal Fact Verification 
[[arxiv](https://arxiv.org/abs/2201.11664)] [[cool](https://papers.cool/arxiv/2201.11664)] [[pdf](https://arxiv.org/pdf/2201.11664)]
> **Authors**: Wei-Yao Wang,Wen-Chih Peng
> **First submission**: 2022-01-26
> **First announcement**: 2022-01-28
> **comment**: Accepted by AAAI 2022 De-Factify Workshop: First Workshop onMultimodalFact-Checking and Hate Speech Detection
- **标题**: YAO团队在TRAXIFY 2022：使用预训练的模型和共同注意网络进行多模式事实验证
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体
- **摘要**: 近年来，社交媒体使用户能够暴露于无数的错误信息和虚假信息。因此，错误信息引起了研究领域和社会问题的广泛关注。为了解决该问题，我们提出了一个框架，即预先进行的框架，该框架由两个预训练的模型组成，用于从文本和图像中提取特征，以及多个共同注意网络，用于融合相同的模态，但不同的来源和不同的模态。此外，我们通过在预练习中使用不同的预训练模型来采用合奏方法来实现更好的性能。我们进一步说明了消融研究的有效性，并检查了不同的预训练模型以进行比较。我们的团队Yao在由De-Fatify @ AAAI 2022主持的Trovify挑战中获得了第五奖（F1得分：74.585 \％），这表明我们的模型在不使用辅助任务或额外信息的情况下实现了竞争性能。我们工作的源代码可在https://github.com/wywywang/multi-modal-fact-verification-2021上公开获得

### Deep Video Prior for Video Consistency and Propagation 
[[arxiv](https://arxiv.org/abs/2201.11632)] [[cool](https://papers.cool/arxiv/2201.11632)] [[pdf](https://arxiv.org/pdf/2201.11632)]
> **Authors**: Chenyang Lei,Yazhou Xing,Hao Ouyang,Qifeng Chen
> **First submission**: 2022-01-27
> **First announcement**: 2022-01-28
> **comment**: Accepted by TPAMI in Dec 2021; extension of NeurIPS2020 Blind Video Temporal Consistency via Deep Video Prior. arXiv admin note: substantial text overlap with arXiv:2010.11838
- **标题**: 视频一致性和传播的深度视频事先
- **领域**: 计算机视觉和模式识别,人工智能
- **摘要**: 独立地将图像处理算法应用到每个视频框架上通常会导致结果视频中的时间不一致。为了解决这个问题，我们提出了一种新颖的盲目视频时间一致性方法。我们的方法仅在直接的一对原始和处理的视频上而不是大型数据集进行培训。与大多数以前具有与光流有关时间一致性的方法不同，我们表明可以通过在带有深视频先验（DVP）视频的卷积神经网络（DVP）上训练卷积神经网络来实现时间一致性。此外，提出了经过精心设计的迭代重新加权培训策略，以解决具有挑战性的多模式不一致问题。我们证明了方法对视频的7个计算机视觉任务的有效性。广泛的定量和感知实验表明，我们的方法比盲目视频时间一致性的最先进方法获得了更高的性能。我们进一步将DVP扩展到视频传播，并在传播三种不同类型的信息（颜色，艺术风格和对象细分）方面证明了其有效性。还提出了具有伪标签的渐进式传播策略，以增强DVP在视频传播中的性能。我们的源代码可在https://github.com/chenyanglei/deep-video-prior上公开获得。

### A Dataset for Medical Instructional Video Classification and Question Answering 
[[arxiv](https://arxiv.org/abs/2201.12888)] [[cool](https://papers.cool/arxiv/2201.12888)] [[pdf](https://arxiv.org/pdf/2201.12888)]
> **Authors**: Deepak Gupta,Kush Attal,Dina Demner-Fushman
> **First submission**: 2022-01-30
> **First announcement**: 2022-01-31
> **comment**: No comments
- **标题**: 用于医学教学视频分类和问题答案的数据集
- **领域**: 计算机视觉和模式识别,计算语言学
- **摘要**: 本文介绍了一个新的挑战和数据集，以促进研究可以理解医疗视频并为自然语言问题提供视觉答案的系统。我们认为，医疗视频可能会为许多急救和医学教育问题提供最佳答案。为此，我们创建了MEDVIDCL和MEDVIDQA数据集，并介绍了医学视频分类（MVC）和医疗视觉答案本地化（MVAL）的任务，这是两个侧重于跨模式（医学语言和医学视频）理解的任务。拟议的任务和数据集有可能支持可以使公众和医生受益的复杂下游应用程序的开发。我们的数据集由MVC任务的6,117个带注释的视频组成，并从899个视频中回答了3,010个带注释的问题，并回答了MVAL任务的时间戳。这些数据集已通过医学信息学专家进行了验证和纠正。我们还使用创建的MEDVIDCL和MEDVIDQA数据集对每个任务进行了基准测试，并提出了多模式学习方法，这些方法为未来的研究设定了竞争基准。

### MVPTR: Multi-Level Semantic Alignment for Vision-Language Pre-Training via Multi-Stage Learning 
[[arxiv](https://arxiv.org/abs/2201.12596)] [[cool](https://papers.cool/arxiv/2201.12596)] [[pdf](https://arxiv.org/pdf/2201.12596)]
> **Authors**: Zejun Li,Zhihao Fan,Huaixiao Tou,Jingjing Chen,Zhongyu Wei,Xuanjing Huang
> **First submission**: 2022-01-29
> **First announcement**: 2022-01-31
> **comment**: Accepted by ACM MM22
- **标题**: MVPTR：通过多阶段学习进行视觉预训练的多级语义对齐
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **摘要**: 以前的视觉语言预训练模型主要构建具有令牌和对象（像素）的多模式输入，然后在它们之间进行跨模式相互作用。我们认为，只有令牌和对象的输入限制了诸如短语到区域接地之类的高级语义对齐。同时，多层次对齐本质上是一致的，并且能够协同促进表示。因此，在本文中，我们建议学习视觉预训练（MVPTR）的多级语义一致性。在MVPTR中，我们遵循两种方式的嵌套结构，以引入概念为高级语义。为了简化从多模式多级输入的学习，我们的框架分为两个阶段，第一阶段着重于模式内多级表示学习，第二阶段通过粗粒和细粒度的语义一致性任务来实施跨模态的第二阶段。除了常用的图像文本匹配和掩盖语言模型任务外，我们还引入了在第一阶段蒙版概念恢复任务，以增强概念表示学习，并在第二阶段进行另外两个任务，以明确鼓励跨模态的多层次对准。我们的代码可在https://github.com/junction4nako/mvp_pytorch上找到。

## 人机交互(cs.HC:Human-Computer Interaction)

该领域共有 2 篇论文

### Towards a Real-time Measure of the Perception of Anthropomorphism in Human-robot Interaction 
[[arxiv](https://arxiv.org/abs/2201.09595)] [[cool](https://papers.cool/arxiv/2201.09595)] [[pdf](https://arxiv.org/pdf/2201.09595)]
> **Authors**: Maria Tsfasman,Avinash Saravanan,Dekel Viner,Daan Goslinga,Sarah de Wolf,Chirag Raman,Catholijn M. Jonker,Catharine Oertel
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: ef:MuCAI'21: Proceedings of the 2nd ACM Multimedia Workshop onMultimodalConversational AI, 2021
- **标题**: 采取对人类互动中拟人化感知感知的实时度量
- **领域**: 人机交互,人工智能,机器人技术,音频和语音处理,信号处理
- **摘要**: 对话机器人的类似人类需要如何实现长期的人类机器人对话？长期互动的一个基本方面是人类适应对话伴侣的互动和情感的不同程度的能力。韵律，这可以通过（Dis）夹带来实现。尽管语音合成多年来一直是一个限制因素，但在这方面的限制越来越缓解。这些进步现在强调了研究机器人体现对人夹带的影响的重要性。在这项研究中，我们在教育用例场景中进行了一个受试者间的人类机器人互动实验，在该场景中，导师通过人的面孔体现了导师。 43位讲英语的参与者参加了这项研究，我们分别分析了人类或机器人面孔的声学驱逐夹带程度。我们发现，对拟人化的主观和客观感知程度与声学 - 宽容的夹带正相关。

### Objective Prediction of Tomorrow's Affect Using Multi-Modal Physiological Data and Personal Chronicles: A Study of Monitoring College Student Well-being in 2020 
[[arxiv](https://arxiv.org/abs/2201.11230)] [[cool](https://papers.cool/arxiv/2201.11230)] [[pdf](https://arxiv.org/pdf/2201.11230)]
> **Authors**: Salar Jafarlou,Jocelyn Lai,Zahra Mousavi,Sina Labbaf,Ramesh Jain,Nikil Dutt,Jessica Borelli,Amir Rahmani
> **First submission**: 2022-01-26
> **First announcement**: 2022-01-27
> **comment**: No comments
- **标题**: 使用多模式生理数据和个人编年史对明天影响的客观预测：2020年监测大学生福祉的研究
- **领域**: 人机交互,机器学习
- **摘要**: 监测和理解情感状态是健康功能和基于情绪疾病的治疗的重要方面。无处不在的可穿戴技术的最新进展提高了此类工具在检测和准确估算精神状态（例如情绪，压力等）方面的可靠性，从而提供了随着时间的推移对个人的全面和连续监测。以前尝试建模个人精神状态的尝试仅限于主观方法或仅包含几种方式（即电话，手表）。因此，我们研究的目的是通过使用多个商业设备通过全自动和客观的方法来调查更准确预测影响的能力。纵向生理数据和每日评估情绪是从一年多的大学生中收集了一年多的大学生。结果表明，我们的模型能够以与最先进的方法相媲美的准确性来预测第二天的影响。

## 信息检索(cs.IR:Information Retrieval)

该领域共有 1 篇论文

### Multimodal Entity Tagging with Multimodal Knowledge Base 
[[arxiv](https://arxiv.org/abs/2201.00693)] [[cool](https://papers.cool/arxiv/2201.00693)] [[pdf](https://arxiv.org/pdf/2201.00693)]
> **Authors**: Hao Peng,Hang Li,Lei Hou,Juanzi Li,Chao Qiao
> **First submission**: 2021-12-21
> **First announcement**: 2022-01-04
> **comment**: 11 pages, 4 figures
- **标题**: 具有多模式知识库的多模式实体标记
- **领域**: 信息检索,人工智能,计算语言学,计算机视觉和模式识别
- **摘要**: 为了增强对多模式知识库和多模式信息处理的研究，我们提出了一个具有多模式知识库（MKB）的称为多模式实体标签（MET）的新任务。我们还使用现有的MKB开发了一个数据集来解决问题。在MKB中，有实体及其相关的文本和图像。在MET中，给定文本图像对，一个人使用MKB中的信息自动识别文本图像对中的相关实体。我们通过使用信息检索范式来解决任务，并使用NLP和CV中的最新方法实现多个基线。我们进行了广泛的实验，并对实验结果进行了分析。结果表明，任务具有挑战性，但是当前的技术可以实现相对较高的性能。我们将发布数据集，代码和模型，以备将来研究。

## 机器学习(cs.LG:Machine Learning)

该领域共有 17 篇论文

### NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting 
[[arxiv](https://arxiv.org/abs/2201.01770)] [[cool](https://papers.cool/arxiv/2201.01770)] [[pdf](https://arxiv.org/pdf/2201.01770)]
> **Authors**: Linyi Yang,Jiazheng Li,Ruihai Dong,Yue Zhang,Barry Smyth
> **First submission**: 2022-01-05
> **First announcement**: 2022-01-06
> **comment**: Accepted to AAAI-22
- **标题**: NUMHTML：多任务财务预测的面向数字的层次变压器模型
- **领域**: 机器学习,人工智能,计量经济学
- **摘要**: 财务预测一直是机器学习研究的一个重要而活跃的领域，因为它提出的挑战以及潜在的回报，即使预测准确性或预测可能会带来较小的改善。传统上，财务预测在很大程度上依赖于从结构化财务报表中得出的定量指标和指标。收入会议电话数据，包括文本和音频，是使用深度收入和相关方法用于各种预测任务的非结构化数据的重要来源。但是，当前基于深度学习的方法在处理数字数据的方式上受到限制。通常将数字视为普通文本代币，而无需利用其基本数字结构。本文描述了一个面向数字的层次变压器模型，以预测股票收益，并通过利用不同类别的数字（货币，时间，百分比等）及其幅度，使用多模式的收益呼叫数据进行财务风险。我们介绍了使用现实世界公开可用的数据集对NUMHTML进行全面评估的结果。结果表明，NUMHTML的表现明显胜过各种评估指标的当前最新技术，并且它有可能在实际交易环境中提供可观的财务收益。

### Predicting Terrorist Attacks in the United States using Localized News Data 
[[arxiv](https://arxiv.org/abs/2201.04292)] [[cool](https://papers.cool/arxiv/2201.04292)] [[pdf](https://arxiv.org/pdf/2201.04292)]
> **Authors**: Steven J. Krieg,Christian W. Smith,Rusha Chatterjee,Nitesh V. Chawla
> **First submission**: 2022-01-11
> **First announcement**: 2022-01-12
> **comment**: No comments
- **标题**: 使用本地新闻数据预测美国恐怖袭击
- **领域**: 机器学习
- **摘要**: 恐怖主义是全世界的一个主要问题，每年造成数千起死亡和数十亿美元的损失。在更好地理解和缓解这些攻击的结束时，我们提出了一组机器学习模型，这些模型从本地新闻数据中学习，以预测在给定日历日期和给定状态下是否会发生恐怖袭击。最佳模型 - 从新颖的变量长度移动平均值中学习的特征空间的森林 - 在接收器操作特征下的区域在2015年至2018年间受到恐怖主义影响最大的五个州中的四个州的$> 0.667 $。我们的主要发现是将恐怖主义作为一组独立事件，而不是持续的事件，尤其是在统一的过程中，尤其是在水平的过程中，尤其是依据。此外，我们的结果强调了对局部模型的需求，以解释位置之间的差异。从机器学习的角度来看，我们发现随机森林模型在我们的多模式，嘈杂和不平衡的数据集上优于几个深层模型，因此在这种情况下证明了我们新颖的特征表示方法的功效。我们还表明，它的预测对于攻击和观察到的攻击特征之间的时间差距相对牢固。最后，我们分析了限制模型性能的因素，其中包括嘈杂的特征空间和少量可用数据。这些贡献为在美国及其他地区的恐怖主义努力中使用机器学习提供了重要的基础。

### Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2201.03969)] [[cool](https://papers.cool/arxiv/2201.03969)] [[pdf](https://arxiv.org/pdf/2201.03969)]
> **Authors**: Jiahao Zheng,Sen Zhang,Xiaoping Wang,Zhigang Zeng
> **First submission**: 2022-01-09
> **First announcement**: 2022-01-12
> **comment**: No comments
- **标题**: 多模式表示基于共同信息最大化，最小化和身份嵌入多模式情感分析的学习
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **摘要**: 多模式情感分析（MSA）是一个基本的复杂研究问题，因为不同方式之间的异质性差距和人类情感表达的歧义。尽管已经有许多成功的尝试来构建MSA的多模式表示，但仍有两个挑战需要解决：1）需要构建更强大的多模式表示，以弥合异质性差距并应对复杂的多模式相互作用，而2）必须在整个信息流中有效地对上下文动态进行建模。在这项工作中，我们提出了一个基于共同信息最大化，最小化和身份嵌入（MMMIE）的多模式表示模型。我们结合了模态对之间的最大化，以及在输入数据和相应特征之间最小化的相互信息，以挖掘模态不变和与任务相关的信息。此外，提出了身份嵌入，以提示下游网络感知上下文信息。两个公共数据集的实验结果证明了该模型的有效性。

### Winning solutions and post-challenge analyses of the ChaLearn AutoDL challenge 2019 
[[arxiv](https://arxiv.org/abs/2201.03801)] [[cool](https://papers.cool/arxiv/2201.03801)] [[pdf](https://arxiv.org/pdf/2201.03801)]
> **Authors**: Zhengying Liu,Adrien Pavao,Zhen Xu,Sergio Escalera,Fabio Ferreira,Isabelle Guyon,Sirui Hong,Frank Hutter,Rongrong Ji,Julio C. S. Jacques Junior,Ge Li,Marius Lindauer,Zhipeng Luo,Meysam Madadi,Thomas Nierhoff,Kangning Niu,Chunguang Pan,Danny Stoll,Sebastien Treguer,Jin Wang,Peng Wang,Chenglin Wu,Youcheng Xiong,Arbe r Zela,Yang Zhang
> **First submission**: 2022-01-11
> **First announcement**: 2022-01-12
> **comment**: The first three authors contributed equally; This is only a draft version
- **标题**: 赢得Chalearn Autodl挑战赛的解决方案和挑战后分析2019
- **领域**: 机器学习,人工智能
- **摘要**: 本文报告了查勒恩（Chalearn）的自动挑战赛系列的结果和挑战后分析，该系列有助于整理出在各种环境中引入的大量自动解决方案（DL），但缺乏公平的比较。所有输入数据模式（时间序列，图像，视频，文本，表格）均格式化为张量，所有任务都是多标签分类问题。代码提交在隐藏的任务，时间和计算资源有限的情况下执行，推动解决方案迅速获得结果。在这种情况下，尽管流行的神经体系结构搜索（NAS）是不切实际的，但DL方法占主导地位。解决方案依赖于微调的预训练网络，其体系结构匹配数据模式。挑战后测试并未揭示超出施加的时间限制的改进。尽管没有特别原始的组件或新颖的组件，但出现了一个具有“元学习者”，“ data Ingestor”，“模型选择器”，“模型/学习者”和“评估者”的高级模块化组织。这种模块化使消融研究揭示了（离平台）元学习，结合和有效的数据管理的重要性。异质模块组合的实验进一步证实了获胜溶液的（局部）最优性。我们的挑战遗产包括持久的基准（http://autodl.chalearn.org），获奖者的开源代码和免费的“自动自助服务”。

### Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents 
[[arxiv](https://arxiv.org/abs/2201.04990)] [[cool](https://papers.cool/arxiv/2201.04990)] [[pdf](https://arxiv.org/pdf/2201.04990)]
> **Authors**: Junseok Park,Kwanyoung Park,Hyunseok Oh,Ganghun Lee,Minsu Lee,Youngki Lee,Byoung-Tak Zhang
> **First submission**: 2022-01-12
> **First announcement**: 2022-01-13
> **comment**: ICMI2021 Oral Presentation, 9 pages, 9 figures
- **标题**: 幼儿指导学习：关键时期对多模式AI代理的影响
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 关键时期是蹒跚学步的大脑在突变中发展的阶段。为了促进儿童的认知发展，在此阶段至关重要的指导至关重要。但是，目前尚不清楚对AI代理的培训是否也存在这样的关键时期。与人类小孩类似，定时的指导和多模式相互作用也可能会显着提高AI代理的训练效率。为了验证这一假设，我们将关键时期的这一概念调整为在AI代理中学习，并研究AI代理的虚拟环境中的关键时期。我们在强化学习（RL）框架中正式化了关键时期和幼儿指导学习。然后，我们建立了一个具有Veca Toolkit的幼儿环境，以模仿人类幼儿的学习特征。我们研究了三个离散级别的相互作用：弱体指导（稀疏奖励），中度导师指导（辅助奖励）和导师示范（行为克隆）。我们还介绍了由30,000张现实世界图像组成的Eave数据集，以完全反映幼儿的观点。我们从两个角度评估了关键时期对AI代理的影响：如何以及何时在单模式学习中最能指导它们。我们的实验结果表明，在100万和200万培训步骤上具有适度指导指导和关键时期的单模式和多模式的代理都显示出明显的改善。我们通过在EVAVE数据集上的转移学习来验证这些结果，并在同一关键时期和指导上找到绩效的进步。

### Deep Learning on Multimodal Sensor Data at the Wireless Edge for Vehicular Network 
[[arxiv](https://arxiv.org/abs/2201.04712)] [[cool](https://papers.cool/arxiv/2201.04712)] [[pdf](https://arxiv.org/pdf/2201.04712)]
> **Authors**: Batool Salehi,Guillem Reus-Muns,Debashri Roy,Zifeng Wang,Tong Jian,Jennifer Dy,Stratis Ioannidis,Kaushik Chowdhury
> **First submission**: 2022-01-12
> **First announcement**: 2022-01-13
> **comment**: 16 pages
- **标题**: 对车辆网络无线边缘的多模式传感器数据的深入学习
- **领域**: 机器学习,信号处理
- **摘要**: 在车辆场景中，毫米波链路的光束选择是一个具有挑战性的问题，因为在所有候选梁对之间进行了详尽的搜索，因此不能肯定在短时间内完成。我们通过利用从激光雷达，相机图像和GPS等传感器收集的多模式数据来解决这个问题，从而解决了这一问题。我们提出了个人模式和分布式基于融合的深度学习（F-DL）体系结构，这些体系结构可以在本地以及移动边缘计算中心（MEC）进行，并进行了有关相关权衡的研究。我们还制定并解决了一个优化问题，该问题考虑了实用的光束搜索，MEC处理以及传感器到MEC数据传递延迟开销，以确定上述F-DL体系结构的输出维度。对公开可用的合成和本土现实世界中数据集进行的广泛评估的结果表明，比经典的仅射频横梁分别提高了95％和96％的光束选择速度。 F-DL在预测前10个最佳束对时，F-DL还优于最先进的技术。

### Multi-task Joint Strategies of Self-supervised Representation Learning on Biomedical Networks for Drug Discovery 
[[arxiv](https://arxiv.org/abs/2201.04437)] [[cool](https://papers.cool/arxiv/2201.04437)] [[pdf](https://arxiv.org/pdf/2201.04437)]
> **Authors**: Xiaoqi Wang,Yingjie Cheng,Yaning Yang,Yue Yu,Fei Li,Shaoliang Peng
> **First submission**: 2022-01-12
> **First announcement**: 2022-01-13
> **comment**: 44 pages, 11 figures
- **标题**: 在生物医学网络上学习的多项任务联合策略用于药物发现
- **领域**: 机器学习,人工智能,定量方法
- **摘要**: 生物医学网络上的自我监督的表示学习（SSL）为药物发现提供了新的机会。但是，如何有效结合多个SSL模型仍然具有挑战性，并且很少探索。因此，我们提出了对药物发现的生物医学网络的自我监督表示学习的多任务联合策略，名为MSSL2Drug。我们设计了六个基本的SSL任务，灵感来自各种模式特征，包括结构，语义和异质生物医学网络中的属性。重要的是，在两个药物发现场景中，通过基于图的多任务对抗学习框架来评估多个任务的15个组合。结果表明了两个重要的发现。 （1）与其他多任务关节模型相比，多模式任务的组合达到了最佳性能。 （2）当具有相同大小的模态时，局部全球组合模型比随机的两任任务组合产生的性能更高。因此，我们猜想多模式和局部全球组合策略可以视为用于药物发现的多任务SSL的指南。

### A Brief Survey of Machine Learning Methods for Emotion Prediction using Physiological Data 
[[arxiv](https://arxiv.org/abs/2201.06610)] [[cool](https://papers.cool/arxiv/2201.06610)] [[pdf](https://arxiv.org/pdf/2201.06610)]
> **Authors**: Maryam Khalid,Emily Willis
> **First submission**: 2022-01-17
> **First announcement**: 2022-01-18
> **comment**: No comments
- **标题**: 对使用生理数据进行情感预测的机器学习方法的简要调查
- **领域**: 机器学习
- **摘要**: 情感预测是一个关键的新兴研究领域，侧重于识别和预测人类的情绪状态。在其他数据源中，生理数据可以作为情绪的指标，并具有额外的优势，即不能被个人掩盖/篡改，并且可以轻松收集。本文通过自我报告的生态瞬时评估（EMA）得分作为基础真相，调查了多种机器学习方法，该方法将部署智能手机和生理数据实时预测情绪。比较回归，长期记忆（LSTM）网络，卷积神经网络（CNN），强化在线学习（ROL）和深信念网络（DBN），我们展示了用于实现准确情感预测的机器学习方法的可变性。我们比较了最新方法，并强调了实验性能仍然不是很好。可以通过考虑以下问题来改善绩效：提高可伸缩性和可推广性，同步多模式数据，优化EMA采样，将适应性与序列预测相结合，收集无偏见的数据以及利用复杂的功能工程技术。

### TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data 
[[arxiv](https://arxiv.org/abs/2201.07284)] [[cool](https://papers.cool/arxiv/2201.07284)] [[pdf](https://arxiv.org/pdf/2201.07284)]
> **Authors**: Shreshth Tuli,Giuliano Casale,Nicholas R. Jennings
> **First submission**: 2022-01-18
> **First announcement**: 2022-01-19
> **comment**: Accepted in VLDB 2022
- **标题**: Tranad：多元时间序列数据中用于异常检测的深变压器网络
- **领域**: 机器学习
- **摘要**: 多元时间序列数据中有效的异常检测和诊断对于现代工业应用非常重要。但是，构建能够快速准确地确定异常观察的系统是一个具有挑战性的问题。这是由于缺乏异常标签，高数据波动率以及对现代应用中超低推理时间的需求。尽管最近有深度学习方法进行了异常检测的发展，但只有少数可以应对所有这些挑战。在本文中，我们提出了一种基于深度变压器网络检测和诊断模型的Tranad，该模型使用基于注意力的序列编码器来迅速推断数据中更广泛的时间趋势。 Tranad使用基于焦点分数的自我调节来实现强大的多模式特征提取和对抗训练以获得稳定性。此外，模型不足的元学习（MAML）使我们能够使用有限的数据训练模型。对六个公开数据集的广泛实证研究表明，Tranad可以通过数据和时间效率培训在检测和诊断性能方面胜过最先进的基线方法。具体而言，与基准相比，Tranad将F1得分提高了17％，将训练时间减少了99％。

### Adversarial vulnerability of powerful near out-of-distribution detection 
[[arxiv](https://arxiv.org/abs/2201.07012)] [[cool](https://papers.cool/arxiv/2201.07012)] [[pdf](https://arxiv.org/pdf/2201.07012)]
> **Authors**: Stanislav Fort
> **First submission**: 2022-01-18
> **First announcement**: 2022-01-19
> **comment**: 8 pages
- **标题**: 强大的近距离分布检测的对抗性脆弱性
- **领域**: 机器学习
- **摘要**: 最近在神经网络中检测出分布（OOD）的输入方面取得了重大进展，这主要是由于在大型数据集上预计的大型模型以及新兴的多模式使用。即使是当前最强的OOD检测技术，我们也显示出严重的对抗脆弱性。使用对输入像素的小而有针对性的扰动，我们可以将图像分配从分配到外部分配，反之亦然。特别是，我们在OOD CIFAR-100与CIFAR-10任务以及远处的OOD CIFAR-100与SVH的方面表现出严重的对抗性脆弱性。我们研究了几种后加工技术的对抗性鲁棒性，包括最大的软马克斯概率（MSP）的简单基线，Mahalanobis距离以及新提出的\ textit \ textit {相对} Mahalanobis距离。通过比较在各种扰动强度下OOD检测性能的丧失，我们证明了使用OOD检测器的集合的有益效果，以及使用\ textit {相对} Mahalanobis距离在其他后处理方法上使用\ textit {相对}。此外，我们还表明，即使使用夹子和多模式的强零射OOD检测也受到严重缺乏对抗性鲁棒性的影响。我们的代码可从https://github.com/stanislavfort/Adversaries_to_ood_detection获得

### CVAE-H: Conditionalizing Variational Autoencoders via Hypernetworks and Trajectory Forecasting for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2201.09874)] [[cool](https://papers.cool/arxiv/2201.09874)] [[pdf](https://arxiv.org/pdf/2201.09874)]
> **Authors**: Geunseob Oh,Huei Peng
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: No comments
- **标题**: CVAE-H：通过超网络和轨迹预测进行自动驾驶的轨迹预测
- **领域**: 机器学习,人工智能,机器人技术
- **摘要**: 在不同环境中预测道路代理的随机行为的任务是自主驾驶的一个挑战性问题。为了最好地理解场景环境并在不同环境中适应道路代理的未来状态，预测模型应是概率，多模式，上下文驱动和一般的。我们通过超网（CVAE-H）提出条件变异自动编码器；有条件的VAE广泛利用了超级net工作并针对高维问题（例如预测任务）执行生成任务。我们首先在简单的生成实验上评估CVAE-H，以表明CVAE-H是概率，多模式，上下文驱动和一般的。然后，我们证明了提出的模型通过在各种环境中对道路代理的准确预测有效地解决了自动驾驶预测问题。

### MMLatch: Bottom-up Top-down Fusion for Multimodal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2201.09828)] [[cool](https://papers.cool/arxiv/2201.09828)] [[pdf](https://arxiv.org/pdf/2201.09828)]
> **Authors**: Georgios Paraskevopoulos,Efthymios Georgiou,Alexandros Potamianos
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: Accepted, ICASSP 2022
- **标题**: MMLATCH：自下而上的自上而下融合用于多模式情感分析
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 当前的多模式融合的深度学习方法依赖于高层和中级潜在模态表示（晚期/中期融合）或低级感觉输入（早期融合）的自下而上的融合。人类感知的模型突出了自上而下的融合的重要性，其中高级表示会影响感官输入的方式，即认知会影响感知。这些自上而下的互动未在当前的深度学习模型中捕获。在这项工作中，我们提出了一种神经体系结构，该神经体系结构在网络培训期间使用向前传球中的反馈机制捕获自上而下的跨模式相互作用。提出的机制为每种模式提取高级表示，并使用这些表示形式掩盖感觉输入，从而使模型可以执行自上而下的特征掩蔽。我们将提出的模型应用于CMU-MOSEI上的多模式情感识别。我们的方法显示出对良好的多和强大的晚期融合基线的一致改进，从而取得了最新的结果。

### Evaluation of data imputation strategies in complex, deeply-phenotyped data sets: the case of the EU-AIMS Longitudinal European Autism Project 
[[arxiv](https://arxiv.org/abs/2201.09753)] [[cool](https://papers.cool/arxiv/2201.09753)] [[pdf](https://arxiv.org/pdf/2201.09753)]
> **Authors**: A. Llera,M. Brammer,B. Oakley,J. Tillmann,M. Zabihi,T. Mei,T. Charman,C. Ecker,F. Dell Acqua,T. Banaschewski,C. Moessnang,S. Baron-Cohen,R. Holt,S. Durston,D. Murphy,E. Loth,J. K. Buitelaar,D. L. Floris,C. F. Beckmann
> **First submission**: 2022-01-20
> **First announcement**: 2022-01-25
> **comment**: 22 pages, 3 figures, 3 tables
- **标题**: 评估复杂的，深度型数据集中的数据插补策略：欧盟纵横立动物欧洲自闭症项目的情况
- **领域**: 机器学习,应用领域,计算
- **摘要**: 在典型发展的人群以及精神病研究中，已经进行了越来越多的大规模多模式研究计划。由于难以评估大量参与者的多种措施，因此缺少数据是此类数据集中的一个常见问题。当研究人员旨在探索多种措施之间的关系时，丢失数据的后果会累积。在这里，我们旨在评估不同的插图策略，以填补来自大型（总n = 764）的临床数据中的缺失值，并深入表征了N = 453个自闭症个体的临床和认知工具的范围（即管理的临床和认知仪器范围），n = 311个对照个体作为EU-IAMS Longitudinal Europeal Autism Isalism招募的一部分（helectimal uniaginal Autistim projectium）。特别是我们考虑了15个参与者的重叠子集的160项临床指标。我们使用两种简单但常见的单变量策略，平均值和中位数，以及一种循环回归方法，涉及四个独立的多元回归模型，包括线性模型，贝叶斯山脊回归以及几种非线性模型，决策树，额外的树木，额外的树木和K-Neighighbours回归。我们使用传统的平方误差来评估模型，以删除可用的数据，并考虑观察到的分布和估算的分布之间的KL差异。我们表明，与典型的单变量方法相比，所测试的所有多变量方法都提供了实质性的改进。此外，我们的分析表明，在测试的所有15个数据吸收中，额外的树回归方法提供了最佳的全球结果。这允许选择一个唯一的模型为LEAP项目归为缺失的数据，并提供一组固定的估算临床数据，以便将来使用LEAP数据集使用的研究人员使用。

### A Multi-modal Fusion Framework Based on Multi-task Correlation Learning for Cancer Prognosis Prediction 
[[arxiv](https://arxiv.org/abs/2201.10353)] [[cool](https://papers.cool/arxiv/2201.10353)] [[pdf](https://arxiv.org/pdf/2201.10353)]
> **Authors**: Kaiwen Tan,Weixian Huang,Xiaofeng Liu,Jinlong Hu,Shoubin Dong
> **First submission**: 2022-01-22
> **First announcement**: 2022-01-26
> **comment**: No comments
- **标题**: 基于癌症预测的多任务相关性学习的多模式融合框架
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **摘要**: 来自组织病理学图像和基因组数据的分子谱的形态属性是推动癌症诊断，预后和治疗的重要信息。通过整合这些异质但互补的数据，提出了许多多模式方法来研究癌症的复杂机制，其中大多数方法与以前的单模式方法获得了可比或更好的结果。但是，这些多模式方法仅限于单个任务（例如生存分析或等级分类），因此忽略了不同任务之间的相关性。在这项研究中，我们提出了一个基于多任务相关性学习（多福置）的多模式融合框架，用于生存分析和癌症等级分类，该框架结合了多种方式和多种任务的力量。具体而言，预先训练的RESNET-152和稀疏图卷积网络（SGCN）分别用于学习组织病理学图像和mRNA表达数据的表示。然后，这些表示形式由完全连接的神经网络（FCNN）融合，这也是一个多任务共享网络。最后，同时生存分析和癌症等级分类的结果。该框架由替代方案训练。我们使用癌症基因组图集（TCGA）的神经胶质瘤数据集系统地评估了我们的框架。结果表明，比传统的特征提取方法，多f usion学会更好的表示形式。借助多任务交替学习，即使是简单的多模式串联也可以比其他深度学习和传统方法更好地实现性能。多任务学习可以提高多个任务的性能，而不仅仅是其中之一，并且在单模式和多模式数据中都有效。

### Visualizing the Diversity of Representations Learned by Bayesian Neural Networks 
[[arxiv](https://arxiv.org/abs/2201.10859)] [[cool](https://papers.cool/arxiv/2201.10859)] [[pdf](https://arxiv.org/pdf/2201.10859)]
> **Authors**: Dennis Grinwald,Kirill Bykov,Shinichi Nakajima,Marina M. -C. Höhne
> **First submission**: 2022-01-26
> **First announcement**: 2022-01-27
> **comment**: 16 pages, 18 figures
- **标题**: 可视化贝叶斯神经网络学到的表示的多样性
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 可解释的人工智能（XAI）旨在使学习机不透明，并为研究人员和从业人员提供各种工具，以揭示神经网络的决策策略。在这项工作中，我们研究了如何将XAI方法用于探索和可视化贝叶斯神经网络（BNNS）学到的特征表示的多样性。我们的目标是通过制定决策策略来提供对BNN的全球理解。我们的工作提供了有关\ emph {posterior}分布的新见解。我们工作的主要发现如下：1）可以应用全局XAI方法来解释BNN实例决策策略的多样性，2）与多模态后近似值相比，多模态后近似值的多样性近似值的多样性及其多样性的多样性，3）蒙特卡洛辍学表现出更大的特征表示多样性，3）不具体的代表性，3）3）随着网络宽度的增加，多模式后验减小，而内部模式多样性增加。这些发现与最近的深神网络理论一致，提供了有关理论在人类可以理解的概念方面所暗示的其他直觉。

### On the Role of Multi-Objective Optimization to the Transit Network Design Problem 
[[arxiv](https://arxiv.org/abs/2201.11616)] [[cool](https://papers.cool/arxiv/2201.11616)] [[pdf](https://arxiv.org/pdf/2201.11616)]
> **Authors**: Vasco D. Silva,Anna Finamore,Rui Henriques
> **First submission**: 2022-01-27
> **First announcement**: 2022-01-28
> **comment**: No comments
- **标题**: 关于多目标优化到公交网络设计问题的作用
- **领域**: 机器学习,人工智能,优化与控制
- **摘要**: 正在进行的交通变化，包括由Covid-19的大流行触发的交通变化，揭示了使我们的公共交通系统适应不断变化的用户需求的必要性。这项工作表明，单一和多目标立场可以协同合并，以更好地回答公交网络设计问题（TNDP）。单个客观配方是从近似（多目标）帕累托前沿中的网络的额定值动态推断出来的，其中使用回归方法来推断转移需求，时间，距离，覆盖范围，覆盖范围和成本的最佳权重。作为指导案例研究，该解决方案应用于葡萄牙里斯本市的多模式公共交通网络。该系统获取由SmartCard验证在Carris Bus和Metro Subway站点给出的单独旅行数据，并使用它们来估计城市的起源用途需求。然后，使用遗传算法（考虑到单一目标方法和多物镜方法），以重新设计可更好地适合观察到的交通需求的总线网络。拟议的TNDP优化被证明可以改善结果，目标功能的降低高达28.3％。该系统设法大量减少了路线数量，所有相关目标，包括旅行时间和每次旅行的转移，都显着改善。基于自动票价收集数据，该系统可以逐步重新设计总线网络，以动态处理对城市流量的持续变化。

### Sparse Centroid-Encoder: A Nonlinear Model for Feature Selection 
[[arxiv](https://arxiv.org/abs/2201.12910)] [[cool](https://papers.cool/arxiv/2201.12910)] [[pdf](https://arxiv.org/pdf/2201.12910)]
> **Authors**: Tomojit Ghosh,Michael Kirby
> **First submission**: 2022-01-30
> **First announcement**: 2022-01-31
> **comment**: 13 pages,56 figures, 5 tables. Used 12 data sets and 5 state-of-the-art models for comparison
- **标题**: 稀疏质心编码器：用于特征选择的非线性模型
- **领域**: 机器学习,计算机视觉和模式识别
- **摘要**: 自动编码器已被广泛用作降低数据维度的非线性工具。虽然自动编码器不使用标签信息，但质心编码器（CE）\ cite {ghosh20222superpised}在其学习过程中使用类标签。在这项研究中，我们提出了使用Centroid-编码器体系结构进行稀疏优化，以确定一组最小的特征，以区分两个或多个类别。所得的算法，稀疏的质心编码器（SCE），使用稀疏性诱导$ \ ell_1 $ -norm提取歧视性特征，同时将其映射到其类质体的指点。 SCE的一个关键属性是，它可以从多模式数据集（即其类似乎具有多个群集的数据集）中提取信息性功能。该算法应用于多种现实世界数据集，包括单细胞数据，高维生物学数据，图像数据，语音数据和加速度计传感器数据。我们将我们的方法与各种最先进的特征选择技术进行了比较，包括监督的混凝土自动编码器（SCAE），功能选择网络（FSNET），深度特征选择（DFS），随机门（STG）和Lassonet。我们从经验上表明，SCE特征通常比隔离测试集中的其他方法产生更好的分类精度。

## 机器人技术(cs.RO:Robotics)

该领域共有 1 篇论文

### Learning to Act with Affordance-Aware Multimodal Neural SLAM 
[[arxiv](https://arxiv.org/abs/2201.09862)] [[cool](https://papers.cool/arxiv/2201.09862)] [[pdf](https://arxiv.org/pdf/2201.09862)]
> **Authors**: Zhiwei Jia,Kaixiang Lin,Yizhou Zhao,Qiaozi Gao,Govind Thattai,Gaurav Sukhatme
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: Accepted by IROS 2022
- **标题**: 学会与负担能力的多模式神经大满贯
- **领域**: 机器人技术,人工智能
- **摘要**: 近年来，见证了新兴的范式向体现的人工智能转变，在该范式上，代理商必须学会通过与环境互动来解决具有挑战性的任务。解决了具体的多模式任务，包括长马计划，视觉和语言接地和有效的探索，存在一些挑战。我们专注于关键瓶颈，即计划和导航的性能。为了应对这一挑战，我们提出了一种神经大满贯的方法，该方法首次利用几种方式进行探索，预测了负担能力感知的语义图，并同时计划。这大大提高了勘探效率，导致了稳健的长途计划，并实现了有效的视觉和语言基础。通过拟议的负担能力感知的多模式神经大满贯（AMSLAM）方法，我们比先前在Alfred Benchmark上发表的工作获得了40％以上的改善，并以23.48％的成功率在未看到的场景中取得了23.48％的新最先进的概括性能。

## 软件工程(cs.SE:Software Engineering)

该领域共有 2 篇论文

### Better Modeling the Programming World with Code Concept Graphs-augmented Multi-modal Learning 
[[arxiv](https://arxiv.org/abs/2201.03346)] [[cool](https://papers.cool/arxiv/2201.03346)] [[pdf](https://arxiv.org/pdf/2201.03346)]
> **Authors**: Martin Weyssow,Houari Sahraoui,Bang Liu
> **First submission**: 2022-01-10
> **First announcement**: 2022-01-11
> **comment**: 4+1 pages
- **标题**: 更好地通过代码概念图表的多模式学习来更好地建模编程世界
- **领域**: 软件工程,信息检索,编程语言
- **摘要**: 近年来，由于基于最先进的模型体系结构的自然语言处理方法的设计，代码建模的进展一直是巨大的。尽管如此，我们认为当前的最新目的还没有足够关注数据可能带给软件工程学习过程的全部潜力。我们的愿景阐明了利用多模式学习方法来建模编程世界的想法。在本文中，我们调查了我们的愿景的基本思想之一，其基于标识符概念图的目标旨在利用通过特定语言构造操纵的领域概念之间的高级关系。特别是，我们建议通过基于我们的概念图的图形神经网络联合学习来增强现有的代码语言模型。我们进行了初步评估，该评估显示了使用简单的联合学习方法的模型对代码搜索的有效性提高，并提示我们进一步研究我们的研究愿景。

### Lifelong Dynamic Optimization for Self-Adaptive Systems: Fact or Fiction? 
[[arxiv](https://arxiv.org/abs/2201.07096)] [[cool](https://papers.cool/arxiv/2201.07096)] [[pdf](https://arxiv.org/pdf/2201.07096)]
> **Authors**: Tao Chen
> **First submission**: 2022-01-18
> **First announcement**: 2022-01-19
> **comment**: The paper has been accepted as a full technical paper at the 29th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2022)
- **标题**: 自适应系统的终身动态优化：事实还是虚构？
- **领域**: 软件工程,人工智能
- **摘要**: 面对不断变化的环境时，高度可配置的软件系统需要动态搜索有希望的适应计划，以保持最佳性能，例如，较高的吞吐量或较小的延迟 - 自适应系统（SASS）的典型计划问题。但是，鉴于具有多个本地Optima的坚固且复杂的搜索景观，这种SAS计划尤其是在动态环境中具有挑战性的。在本文中，我们提出了Lidos，这是SAS计划的终生动态优化框架。使Lidos唯一的原因是要处理“动态”，我们将SAS计划作为多模式优化问题，旨在保留有用的信息，以更好地处理在动态环境变化下的本地Optima问题。这与现有规划者的不同之处在于，在计划过程中，“动态”并未明确处理。因此，Lidos中的搜索和计划在SAS的寿命中不断运行，仅在离线或搜索空间被环境覆盖时才终止。三个现实世界中的实验结果表明，作为SAS计划搜索的一部分，明确处理动态的概念是有效的，因为Lidos的表现优于其固定量的总体，最高可提高10倍。与最先进的计划者相比，它在总体上可以取得更好的成绩，并在生成有希望的适应计划时以1.4倍至10倍的速度取得了更好的成绩。

## 音频和语音处理(eess.AS:Audio and Speech Processing)

该领域共有 3 篇论文

### Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction 
[[arxiv](https://arxiv.org/abs/2201.02184)] [[cool](https://papers.cool/arxiv/2201.02184)] [[pdf](https://arxiv.org/pdf/2201.02184)]
> **Authors**: Bowen Shi,Wei-Ning Hsu,Kushal Lakhotia,Abdelrahman Mohamed
> **First submission**: 2022-01-05
> **First announcement**: 2022-01-06
> **comment**: ICLR 2022
- **标题**: 通过掩盖的多模式群集预测来学习视听语音表示
- **领域**: 音频和语音处理,计算机视觉和模式识别,声音
- **摘要**: 语音的视频记录包含相关的音频和视觉信息，为语音表示从说话者的唇部运动和产生的声音提供了强烈的信号。我们介绍了视听隐藏单元BERT（AV-HUBERT），这是一个自我监督的表示框架，用于视听语音，该框架掩盖了多流视频输入并预测自动发现并迭代完善的多模式隐藏单元。 Av-Hubert学习了有力的视听语音表示形式，从而使唇部阅读和自动语音识别受益。在最大的公共唇读基准LRS3（433小时）上，AV-Hubert仅使用30小时的标签数据获得32.5％的速度，表现优于以前的最先进方法（33.6％），接受了一千倍的转录视频数据（3.6％）。当使用所有433小时的LRS3标记数据并与自我训练结合使用时，唇部阅读将进一步降低至26.9％。在相同基准上使用我们的视听表示，仅通过音频语音识别，可导致比最新性能的相对降低40％（1.3％vs 2.3％）。我们的代码和型号可在https://github.com/facebookresearch/av_hubert上找到

### Cross-Modal ASR Post-Processing System for Error Correction and Utterance Rejection 
[[arxiv](https://arxiv.org/abs/2201.03313)] [[cool](https://papers.cool/arxiv/2201.03313)] [[pdf](https://arxiv.org/pdf/2201.03313)]
> **Authors**: Jing Du,Shiliang Pu,Qinbo Dong,Chao Jin,Xin Qi,Dian Gu,Ru Wu,Hongwei Zhou
> **First submission**: 2022-01-10
> **First announcement**: 2022-01-11
> **comment**: submit to ICASSP2022, 5 pages, 3 figures
- **标题**: 跨模式ASR ASR后处理系统，用于纠正和拒绝话语
- **领域**: 音频和语音处理,人工智能,声音
- **摘要**: 尽管现代的自动语音识别（ASR）系统可以实现高性能，但它们可能会产生削弱读者经验并损害下游任务的错误。为了提高ASR假设的准确性和可靠性，我们为语音识别器提出了一个跨模式后处理系统，其中1）融合了来自不同模式的声学特征和文本特征，2）关节置信度估计器和多任务学习方式的错误纠正措施和多任务学习方式和3）统一误差纠正和说服抑制模式。与单模式或单任务模型相比，我们提出的系统被证明更加有效。实验结果表明，我们的后处理系统导致单扬声器和多演讲者在我们的工业ASR系统上的角色错误率（CER）的相对相对降低超过10％以上，每个令牌的延迟约为1.7ms，这确保了在流媒体语音识别中可以接受后期处理的额外延迟。

### A Study on the Ambiguity in Human Annotation of German Oral History Interviews for Perceived Emotion Recognition and Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2201.06868)] [[cool](https://papers.cool/arxiv/2201.06868)] [[pdf](https://arxiv.org/pdf/2201.06868)]
> **Authors**: Michael Gref,Nike Matthiesen,Sreenivasa Hikkal Venugopala,Shalaka Satheesh,Aswinkumar Vijayananth,Duc Bach Ha,Sven Behnke,Joachim Köhler
> **First submission**: 2022-01-18
> **First announcement**: 2022-01-19
> **comment**: Submitted to LREC 2022
- **标题**: 关于人类口述历史访谈的人类注释中的歧义的研究，以感知情绪识别和情感分析
- **领域**: 音频和语音处理,计算语言学,声音
- **摘要**: 对于视听访谈档案中的研究，通常不仅感兴趣的话，而且还引起了人们的关注。情感分析和情感识别可以帮助捕获，分类和使这些不同的方面可以搜索。特别是，对于口述历史档案，这种索引技术可能引起人们的极大兴趣。这些技术可以帮助了解情绪在历史记忆中的作用。但是，人类经常对情感和情感模棱两可。此外，口述历史访谈具有多层的复杂水平，有时是矛盾的，有时是非常微妙的情感方面。因此，偶然机器和人类已将其捕获并分配给预定义的类别的问题。本文调查了德国口述历史访谈中人类对情绪和情感的看法的歧义以及对机器学习系统的影响。我们的实验揭示了人类对不同情绪的看法的实质性差异。此外，我们报告了正在进行的机器学习实验不同方式的情况。我们表明，人类的感知歧义和其他挑战，例如阶级失衡和缺乏培训数据，目前限制了这些技术在口述历史档案中的机会。但是，我们的工作揭示了有希望的观察结果和进一步研究的可能性。

## 图像和视频处理(eess.IV:Image and Video Processing)

该领域共有 16 篇论文

### Image Processing Methods for Coronal Hole Segmentation, Matching, and Map Classification 
[[arxiv](https://arxiv.org/abs/2201.01380)] [[cool](https://papers.cool/arxiv/2201.01380)] [[pdf](https://arxiv.org/pdf/2201.01380)]
> **Authors**: V. Jatla,M. S. Pattichis,C. N. Arge
> **First submission**: 2022-01-04
> **First announcement**: 2022-01-05
> **comment**: ef:IEEE Transactions on Image Processing 29 (2019): 1641-1653
- **标题**: 冠状孔分割，匹配和地图分类的图像处理方法
- **领域**: 图像和视频处理,太阳和恒星天体物理学,计算机视觉和模式识别
- **摘要**: 本文提出了多年努力的结果，以开发和验证图像处理方法，以根据太阳图像观察选择最佳的物理模型。该方法包括根据与从图像中提取的冠状孔的一致选择物理模型。最终，目标是使用物理模型来预测地磁风暴。我们将问题分解为三个子问题：（i）基于物理约束的冠状孔分割，（ii）匹配不同地图之间的冠状孔的簇簇，以及（iii）物理地图分类。对于分割冠状孔，我们开发了一种多模式方法，该方法使用从三种不同方法的分割图来初始化一种水平集方法，该方法将初始冠状孔分割发展为磁性边界。然后，我们引入了一种基于线性编程的新方法，用于匹配冠状孔的簇。然后使用随机森林进行最终匹配。使用从多个阅读器，手动聚类，手动图分类和50个地图验证的方法验证的共识图仔细验证了这些方法。提出的多模式分割方法通过提供准确的边界检测来显着优于塞格内特，U-NET，Henney-Harvey和FCN。总体而言，该方法具有95.5％的地图分类精度。

### Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images 
[[arxiv](https://arxiv.org/abs/2201.01266)] [[cool](https://papers.cool/arxiv/2201.01266)] [[pdf](https://arxiv.org/pdf/2201.01266)]
> **Authors**: Ali Hatamizadeh,Vishwesh Nath,Yucheng Tang,Dong Yang,Holger Roth,Daguang Xu
> **First submission**: 2022-01-04
> **First announcement**: 2022-01-05
> **comment**: 13 pages, 3 figures
- **标题**: SWIN UNETR：用于MRI图像中脑肿瘤语义分割的Swin Transformer
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 脑肿瘤的语义分割是一项基本的医学图像分析任务，涉及多种MRI成像方式，可以帮助临床医生诊断患者并依次研究恶性实体的进展。近年来，完全卷积神经网络（FCNN）方法已成为3D医疗图像分割的事实上的标准。流行的“ U形”网络体系结构已在不同的2D和3D语义分段任务以及各种成像方式上实现了最先进的性能基准。但是，由于FCNN中卷积层的内核大小有限，因此它们对长期信息进行建模的性能是亚最佳的，这可能导致肿瘤分割量变化可变的肿瘤分割。另一方面，变压器模型在捕获多个域中的远程信息（包括自然语言处理和计算机视觉）方面具有出色的功能。受视觉变压器及其变体成功的启发，我们提出了一种新颖的分割模型，称为Swin UNET变形金刚（Swin Unetr）。具体而言，将3D脑肿瘤语义分割的任务重新构成为序列预测问题的序列，其中多模式输入数据被投影到1D嵌入式序列中，并用作层次SWIN变压器作为编码器的输入。 Swin Transformer编码器通过使用Skip Connections在每个分辨率下连接到基于FCNN的解码器，通过使用移位的窗口来计算自我注意力，以五个不同的分辨率提取功能。我们参加了Brats 2021分段挑战，我们提出的模型在验证阶段的表现最佳方法中排名。代码：https：//monai.io/research/swin-unetr

### Multiple Sclerosis Lesions Segmentation using Attention-Based CNNs in FLAIR Images 
[[arxiv](https://arxiv.org/abs/2201.01832)] [[cool](https://papers.cool/arxiv/2201.01832)] [[pdf](https://arxiv.org/pdf/2201.01832)]
> **Authors**: Mehdi SadeghiBakhi,Hamidreza Pourreza,Hamidreza Mahyar
> **First submission**: 2022-01-05
> **First announcement**: 2022-01-06
> **comment**: No comments
- **标题**: 使用基于注意的CNN中的多发性硬化病变分割
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 目的：多发性硬化症（MS）是一种自身免疫性和脱髓鞘性疾病，导致中枢神经系统病变。可以使用磁共振成像（MRI）跟踪和诊断该疾病。到目前为止，多种多模式自动生物医学方法用于细分对患者的成本，时间和可用性对患者无益的病变。本文的作者提出了一种仅采用一种模态（FLAIR图像）来准确细分MS病变的方法。方法：基于斑块的卷积神经网络（CNN）的设计，灵感来自3D-RESNET和空间通道注意模块，以分段MS病变。所提出的方法由三个阶段组成：（1）将对比度限制的自适应直方图均衡（CLAHE）应用于原始图像，并与提取的边缘串联以创建4D图像； （2）大小80 * 80 * 80 * 2的贴片是从4D图像中随机选择的； （3）提取的斑块被传递到基于注意力的CNN中，用于细分病变。最后，将提出的方法与以前对同一数据集的研究进行了比较。结果：当前的研究通过ISIB挑战数据进行了测试集评估模型。实验结果表明，所提出的方法在骰子相似性和绝对体积差方面显着超过了现有的方法，而所提出的方法仅使用一种模态（FLAIR）来分割病变。结论：作者引入了一种自动方法，以分割病变，最多基于两种方式作为输入。所提出的结构由卷积，反卷积和SCA-Voxres模块作为注意模块组成。结果表明，所提出的方法优于其他方法。

### A Keypoint Detection and Description Network Based on the Vessel Structure for Multi-Modal Retinal Image Registration 
[[arxiv](https://arxiv.org/abs/2201.02242)] [[cool](https://papers.cool/arxiv/2201.02242)] [[pdf](https://arxiv.org/pdf/2201.02242)]
> **Authors**: Aline Sindel,Bettina Hohberger,Sebastian Fassihi Dehcordi,Christian Mardin,Robert Lämmer,Andreas Maier,Vincent Christlein
> **First submission**: 2022-01-06
> **First announcement**: 2022-01-07
> **comment**: 6 pages, 4 figures, 1 table, accepted to BVM 2022
- **标题**: 基于多模式视网膜图像登记的血管结构的关键点检测和描述网络
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 眼科成像利用不同的成像系统，例如颜色眼底，红外，荧光素血管造影，光学相干断层扫描（OCT）或OCT血管造影。经常分析具有不同方式或采集时间的多个图像以诊断视网膜疾病。通过多模式登记可以自动对齐图像中的容器结构，可以支持眼科医生的工作。我们的方法使用卷积神经网络在多模式视网膜图像中提取血管结构的特征。我们使用分类和跨模式描述符损失函数在小斑块上共同训练关键点检测和描述网络，并将网络应用于测试阶段的完整图像大小。我们的方法证明了与竞争方法相比，我们和公共多模式数据集的最佳注册性能。

### CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwannoma and Cochlea Segmentation 
[[arxiv](https://arxiv.org/abs/2201.02831)] [[cool](https://papers.cool/arxiv/2201.02831)] [[pdf](https://arxiv.org/pdf/2201.02831)]
> **Authors**: Reuben Dorent,Aaron Kujawa,Marina Ivory,Spyridon Bakas,Nicola Rieke,Samuel Joutard,Ben Glocker,Jorge Cardoso,Marc Modat,Kayhan Batmanghelich,Arseniy Belkov,Maria Baldeon Calisto,Jae Won Choi,Benoit M. Dawant,Hexin Dong,Sergio Escalera,Yubo Fan,Lasse Hansen,Mattias P. Heinrich,Smriti Joshi,Victoriya Kashtanova,Hyeon Gyu Kim,Satoshi Kondo,Christian N. Kruse,Susana K. Lai-Yuen, et al. (15 additional authors not shown)
> **First submission**: 2022-01-08
> **First announcement**: 2022-01-10
> **comment**: In Medical Image Analysis
- **标题**: Crossmoda 2021挑战：前庭型雪花瘤和耳蜗分割的跨模式域适应技术的基准
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 域适应（DA）最近对医学成像社区产生了强烈的兴趣。尽管已经提出了各种DA技术进行图像分割，但这些技术中的大多数已在私人数据集或小型公开数据集上进行了验证。此外，这些数据集主要解决了单层问题。为了应对这些限制，与第24届国际医学图像计算和计算机辅助干预措施（MICCAI 2021）一起组织了跨模式适应（Crossmoda）挑战。 Crossmoda是第一个大型且多级的基准，用于无监督的交叉模式DA。挑战的目标是分段前庭造型瘤的随访和治疗计划（VS）的两个关键大脑结构：VS和耳蜗。目前，使用对比增强T1（CET1）MRI进行VS患者的诊断和监测。但是，对使用非对比度序列（例如高分辨率T2（HRT2）MRI）越来越兴趣。因此，我们创建了一个无监督的交叉模式分割基准。该训练集提供了带注释的CET1（n = 105）和未划分的未经许可的HRT2（n = 105）。目的是在测试集中提供的HRT2上自动执行单侧VS和双侧耳蜗（n = 137）。共有16个团队为评估阶段提交了算法。表现最好的球队达到的表现水平惊人（最佳中位数骰子 -  vs：88.4％； Cochleas：85.7％），接近完全监督（中位数骰子-vs -vs：92.5％； Cochleas：Cochleas：87.7％）。所有最佳表现的方法都使用图像到图像翻译方法将源域图像转换为伪目标域图像。然后，使用这些生成的图像和为源图像提供的手动注释对分割网络进行了训练。

### United adversarial learning for liver tumor segmentation and detection of multi-modality non-contrast MRI 
[[arxiv](https://arxiv.org/abs/2201.02629)] [[cool](https://papers.cool/arxiv/2201.02629)] [[pdf](https://arxiv.org/pdf/2201.02629)]
> **Authors**: Jianfeng Zhao,Dengwang Li,Shuo Li
> **First submission**: 2022-01-07
> **First announcement**: 2022-01-10
> **comment**: No comments
- **标题**: 联合对抗性学习用于肝肿瘤分割和多模式的检测非对比度MRI
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **摘要**: 通过使用多模式的非对比度磁共振成像（NCMRI）的同时分割和检测肝肿瘤（血管瘤和肝​​细胞癌（HCC））对于临床诊断至关重要。但是，由于：（1）关于NCMRI的HCC信息是看不见的，或者不足，因此提取肝肿瘤的特征很困难； （2）多模式NCMRI中的各种成像特性导致融合和选择困难； （3）血管瘤和HCC之间关于NCMRI的特定信息引起肝肿瘤的检测困难。在这项研究中，我们提出了一个联合对抗性学习框架（UAL），用于使用多模式NCMRI同时进行肝肿瘤分割和检测。 UAL首先利用多视图的编码器来提取多模式NCMRI信息以进行肝肿瘤分割和检测。在此编码器中，一种新型的边缘差异特征金字塔模块旨在​​促进互补的多模式特征提取。其次，新设计的融合和选择通道用于融合多模式的功能并做出功能选择的决定。然后，提出的与填充协调共享的机制集成了分割和检测的多任务，从而使多任务可以在一个歧视者中执行联合对抗性学习。最后，创新的多相放射素学指导歧视者利用清晰而特定的肿瘤信息，通过对抗性学习策略提高多任务性能。 UAL在相应的多模式NCMRI（即T1FS Pre-Contcontrast MRI，T2FS MRI和DWI）中得到了验证，以及255名临床受试者的三个阶段对比增强的MRI。实验表明，UAL在肝肿瘤的临床诊断中具有巨大的潜力。

### Cross-Modality Deep Feature Learning for Brain Tumor Segmentation 
[[arxiv](https://arxiv.org/abs/2201.02356)] [[cool](https://papers.cool/arxiv/2201.02356)] [[pdf](https://arxiv.org/pdf/2201.02356)]
> **Authors**: Dingwen Zhang,Guohai Huang,Qiang Zhang,Jungong Han,Junwei Han,Yizhou Yu
> **First submission**: 2022-01-07
> **First announcement**: 2022-01-10
> **comment**: published on Pattern Recognition 2021
- **标题**: 跨模式的深度特征学习脑肿瘤分割
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 机器学习和数字医学图像的流行率的最新进展为通过使用深层卷积神经网络解决了充满挑战的脑肿瘤细分（BTS）任务的机会。但是，与非常广泛的RGB图像数据不同，脑肿瘤分割中使用的医学图像数据在数据量表方面相对稀少，但就模态性属性而言包含更丰富的信息。为此，本文提出了一种新型的跨模式深度特征学习框架，以从多模式MRI数据分割脑肿瘤。核心思想是在多模式数据上开采丰富的模式，以弥补数据量表不足。所提出的跨模式深度特征学习框架包括两个学习过程：跨模式特征过渡（CMFT）过程和跨模式特征融合（CMFF）过程，旨在通过分别从不同模态数据融合不同模态数据来通过跨不同模态数据融合知识来学习丰富的特征表示。与基线方法和最新方法相比，在Brats的基准上进行了全面的实验，该实验表明，提出的跨模式深模性深模式学习框架可以有效地改善脑肿瘤分割性能。

### Swin Transformer for Fast MRI 
[[arxiv](https://arxiv.org/abs/2201.03230)] [[cool](https://papers.cool/arxiv/2201.03230)] [[pdf](https://arxiv.org/pdf/2201.03230)]
> **Authors**: Jiahao Huang,Yingying Fang,Yinzhe Wu,Huanjun Wu,Zhifan Gao,Yang Li,Javier Del Ser,Jun Xia,Guang Yang
> **First submission**: 2022-01-10
> **First announcement**: 2022-01-11
> **comment**: 55 pages, 19 figures, submitted to Neurocomputing journal
- **标题**: 快速MRI的Swin Transformer
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **摘要**: 磁共振成像（MRI）是一种重要的非侵入性临床工具，可以产生高分辨率和可重复的图像。但是，高质量的MR图像需要长时间的扫描时间，这会导致患者的疲惫和不适，由于患者的自愿运动和非自愿性生理运动，引起了更多的人工制作。为了加速扫描过程，k空间不足采样和基于深度学习的重建的方法已经普及。这项工作引入了SwinMR，这是一种基于SWIN变压器的新型快速MRI重建方法。整个网络由输入模块（IM），特征提取模块（FEM）和输出模块（OM）组成。 IM和OM是2D卷积层，FEM由残留的Swin变压器块（RSTBS）和2D卷积层组成。 RSTB由一系列SWIN变压器层（STL）组成。 STL的移位窗户多头自我注意力（W-MSA/SW-MSA）是在移动的窗口中进行的，而不是整个图像空间中原始变压器的多头自我注意力（MSA）。通过使用灵敏度图提出了一种新型的多通道损失，该图被证明可以保留更多的纹理和细节。我们在Calgary-campinas公共脑MR数据集中进行了一系列比较研究和消融研究，并在2017年多模式脑肿瘤分割挑战挑战中进行了下游分割实验。结果表明，与其他基准方法相比，我们的SWINMR实现了高质量的重建，并且在噪声中断和不同数据集中显示出极大的鲁棒性，并且具有不同的无底采样口罩。该代码可在https://github.com/ayanglab/swinmr上公开获取。

### COROLLA: An Efficient Multi-Modality Fusion Framework with Supervised Contrastive Learning for Glaucoma Grading 
[[arxiv](https://arxiv.org/abs/2201.03795)] [[cool](https://papers.cool/arxiv/2201.03795)] [[pdf](https://arxiv.org/pdf/2201.03795)]
> **Authors**: Zhiyuan Cai,Li Lin,Huaqing He,Xiaoying Tang
> **First submission**: 2022-01-11
> **First announcement**: 2022-01-12
> **comment**: 5 pages, To be published in ISBI 2022
- **标题**: 花冠：有效的多模式融合框架，具有青光眼评分的监督对比学习
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 青光眼是可能引起失明的眼科疾病之一，对此，早期发现和治疗非常重要。眼底图像和光学相干断层扫描（OCT）图像在诊断青光眼方面都是广泛使用的方式。但是，现有的青光眼分级方法主要利用单一模态，而忽略了眼底和OCT之间的互补信息。在本文中，我们提出了一个有效的多模式监督对比学习框架，名为Corolla，用于青光眼分级。通过层分割以及厚度计算和投影，从原始的OCT卷中提取视网膜厚度图并用作替代方式，从而导致更有效的计算，而记忆使用较少。鉴于医学图像样本之间的结构和分布相似，我们采用了有监督的对比学习，以更好地收敛来增加模型的歧视能力。此外，进行了配对的底面图像和厚度图的特征级融合以提高诊断精度。在伽马数据集上，与最先进的方法相比，我们的花冠框架达到了压倒性的青光眼分级性能。

### Automatic Segmentation of Head and Neck Tumor: How Powerful Transformers Are? 
[[arxiv](https://arxiv.org/abs/2201.06251)] [[cool](https://papers.cool/arxiv/2201.06251)] [[pdf](https://arxiv.org/pdf/2201.06251)]
> **Authors**: Ikboljon Sobirov,Otabek Nazarov,Hussain Alasmawi,Mohammad Yaqub
> **First submission**: 2022-01-17
> **First announcement**: 2022-01-18
> **comment**: 8 pages, 2 figures (3 more figures in Appendix), 2 tables; accepted to MIDL conference
- **标题**: 头部和颈部肿瘤的自动分割：强大的变压器有多强？
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 癌症是全球死亡的主要原因之一，头颈部（H＆N）癌症是最普遍的类型之一。正电子发射断层扫描和计算机断层扫描用于检测，分段和量化肿瘤区域。在临床上，肿瘤分割非常耗时，容易出错。机器学习，尤其是深度学习可以帮助自动化此过程，从而产生与临床医生结果一样准确的结果。在本文中，我们研究了一种基于视觉变压器自动描述H＆N肿瘤的方法，并将其结果与基于卷积神经网络（CNN）模型进行比较。我们使用来自CT和PET扫描的多模式数据来执行分割任务。我们表明，具有基于变压器的模型的解决方案具有与基于CNN的模型相当的结果。通过交叉验证，该模型的平均骰子相似性系数（DSC）为0.736，平均精度为0.766，平均召回率为0.766。就DSC分数而言，这仅比2020年竞赛获胜模型（在内部交叉验证）少0.021。在测试集中，该模型的性能类似，DSC为0.736，精度为0.773，召回0.760，在DSC中仅比2020年竞赛获胜模型低0.023。这项工作表明，通过基于变压器的模型进行癌症分割是一个有前途的研究领域，可以进一步探索。

### Modality Bank: Learn multi-modality images across data centers without sharing medical data 
[[arxiv](https://arxiv.org/abs/2201.08955)] [[cool](https://papers.cool/arxiv/2201.08955)] [[pdf](https://arxiv.org/pdf/2201.08955)]
> **Authors**: Qi Chang,Hui Qu,Zhennan Yan,Yunhe Gao,Lohendran Baskaran,Dimitris Metaxas
> **First submission**: 2022-01-21
> **First announcement**: 2022-01-24
> **comment**: arXiv admin note: substantial text overlap with arXiv:2012.08604
- **标题**: 模式银行：在不共享医疗数据的情况下学习跨数据中心的多模式图像
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 多模式图像已被广泛使用，并为医学图像分析提供了全面的信息。但是，在临床环境中，所有机构中的所有方式都昂贵，而且通常是不可能的。为了利用更全面的多模式信息，我们提出了一个名为ModalityBank的隐私权分散的多模式自适应学习体系结构。我们的方法可以学习一组有效的特定域调制参数，插入了一个公共域 - 不合理的网络。我们通过切换不同的配置组来证明，生成器可以输出特定模式的高质量图像。我们的方法还可以完成所有数据中心的缺失方式，因此可以用于模态完成目的。从合成的多模式样本中训练的下游任务比从一个真实的数据中心学习并实现与所有真实图像相比，可以实现更高的性能。

### Shape-consistent Generative Adversarial Networks for multi-modal Medical segmentation maps 
[[arxiv](https://arxiv.org/abs/2201.09693)] [[cool](https://papers.cool/arxiv/2201.09693)] [[pdf](https://arxiv.org/pdf/2201.09693)]
> **Authors**: Leo Segre,Or Hirschorn,Dvir Ginzburg,Dan Raviv
> **First submission**: 2022-01-24
> **First announcement**: 2022-01-25
> **comment**: No comments
- **标题**: 形状一致的生成对抗网络，用于多模式医学分割图
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 最近，跨域的跨域的图像翻译最近引起了人们的兴趣和大幅改进。在医学成像中，有多种成像方式，具有非常不同的特征。我们的目标是使用CT和MRI整个心脏扫描之间的跨模式适应性进行语义分割。我们为极限数据集提供了使用合成的心脏量的分割网络。我们的解决方案基于一个3D跨模式生成对抗网络，可在模式之间共享信息，并使用未配对的数据集生成合成数据。我们的网络利用语义细分来提高发电机形状的一致性，从而在重新训练分割网络时创建更现实的合成量。我们表明，在使用空间增强以改善生成对抗网络时，可以在小数据集上进行改进的细分。这些增强功能提高了发电机功能，从而提高了分段的性能。在使用建议的架构时，仅使用16 CT和16个MRI心血管体积，比其他分割方法显示了改进的结果。

### Mutual information neural estimation for unsupervised multi-modal registration of brain images 
[[arxiv](https://arxiv.org/abs/2201.10305)] [[cool](https://papers.cool/arxiv/2201.10305)] [[pdf](https://arxiv.org/pdf/2201.10305)]
> **Authors**: Gerard Snaauw,Michele Sasdelli,Gabriel Maicas,Stephan Lau,Johan Verjans,Mark Jenkinson,Gustavo Carneiro
> **First submission**: 2022-01-25
> **First announcement**: 2022-01-26
> **comment**: 4 pages, 4 figures, 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), oral presentation
- **标题**: 无监督的多模式图像的共同信息神经估计
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 图像引导的手术和治疗中的许多应用都需要快速，可靠的非线性，多模式图像登记。最近提出的无监督的基于深度学习的注册方法与迭代方法相比，仅在一小部分时间内表现出了较高的性能。大多数基于学习的方法都集中在单模式图像注册上。多模式注册的扩展取决于使用适当的相似性函数，例如互信息（MI）。我们建议指导基于深度学习的注册方法的培训，并在端到端可训练网络中的图像对之间进行MI估算。我们的结果表明，一个小的2层网络在单模式注册中产生竞争成果，并具有下秒的时间。与迭代和深度学习方法的比较表明，我们基于MI的方法在拓扑和质量上产生了较高的结果，其非呈非形态变换率极低。实时临床应用将受益于更好的解剖结构的视觉匹配和更少的注册故障/离群值。

### DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction 
[[arxiv](https://arxiv.org/abs/2201.10776)] [[cool](https://papers.cool/arxiv/2201.10776)] [[pdf](https://arxiv.org/pdf/2201.10776)]
> **Authors**: Bo Zhou,Neel Dey,Jo Schlemper,Seyed Sadegh Mohseni Salehi,Chi Liu,James S. Duncan,Michal Sofka
> **First submission**: 2022-01-26
> **First announcement**: 2022-01-27
> **comment**: Accepted at WACV 2023
- **标题**: DSFORMER：一种双域自制变压器，用于加速多对比度MRI重建
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **摘要**: 多对比度MRI（MC-MRI）捕获了多种互补成像方式，以帮助放射决策。鉴于需要降低多次收购的时间成本，当前的深度加速MRI重建网络着重于利用多个对比度之间的冗余。但是，现有的作品在很大程度上受到了配对数据和/或过度昂贵的完全采样的MRI序列的监督。此外，重建网络通常依赖于卷积体系结构，这些卷积体系结构受到建模远程相互作用的能力的限制，并且可能导致良好的解剖学细节的次优恢复。对于这些目的，我们提出了一个双域自我监督的变压器（DSFORMER），用于加速MC-MRI重建。 DSFORMER开发了一个深层条件级联变压器（DCCT），该变压器由几个级联的Swin Transformer重建网络（SWINRN）组成，该网络（SWINRN）在两种深度调节策略下训练，以实现MC-MRI信息共享。我们进一步提出了DCCT的双域（图像和K空间）自我监督的学习策略，以减轻获取完全采样的培训数据的成本。 DSFormer会生成高保真重建，从而超过电流完全监督的基线。此外，我们发现，在接受全面监督或我们提出的双域自学训练时，DSFORMER可以实现几乎相同的性能。

### FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss 
[[arxiv](https://arxiv.org/abs/2201.12589)] [[cool](https://papers.cool/arxiv/2201.12589)] [[pdf](https://arxiv.org/pdf/2201.12589)]
> **Authors**: Jinbao Wang,Guoyang Xie,Yawen Huang,Yefeng Zheng,Yaochu Jin,Feng Zheng
> **First submission**: 2022-01-29
> **First announcement**: 2022-01-31
> **comment**: arXiv admin note: text overlap with arXiv:2201.08953
- **标题**: FedMed-ATL：通过仿射变换损失未对准未配对的大脑图像合成
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 完全排列和配对的多模式神经成像数据的存在证明了其在诊断脑疾病中的有效性。但是，收集完整的配对数据和配对数据是不切实际的，因为实际困难可能包括高成本，长期获取，图像腐败和隐私问题。以前，未配对的神经影像数据（称为泥）通常被视为嘈杂的标签。但是，这种基于嘈杂的标签方法在严重发生扭曲时无法完成良好的方法。例如，旋转角度不同。在本文中，我们提出了一种新型联邦自审学习（FedMed），以供大脑图像合成。制定了仿射变换损失（ATL），以利用严重扭曲的图像，而无需违反医院的隐私立法。然后，我们引入了一个新的数据增强程序，以进行自我监督训练，并将其送入三个辅助头，即辅助旋转，辅助翻译和辅助缩放头。提出的方法证明了与其他基于GAN的算法相比，在严重未对准和不配对的数据设置下，我们合成结果的质量的高级性能。提出的方法还减少了对可变形注册的需求，同时鼓励利用未对准和未配对的数据。与其他最先进的方法相比，实验结果验证了我们学习范式的出色表现。

### Few-shot Unsupervised Domain Adaptation for Multi-modal Cardiac Image Segmentation 
[[arxiv](https://arxiv.org/abs/2201.12386)] [[cool](https://papers.cool/arxiv/2201.12386)] [[pdf](https://arxiv.org/pdf/2201.12386)]
> **Authors**: Mingxuan Gu,Sulaiman Vesal,Ronak Kosti,Andreas Maier
> **First submission**: 2022-01-28
> **First announcement**: 2022-01-31
> **comment**: Accepted t0 BVM2022
- **标题**: 多模式心脏图像分割的几个无监督域的适应
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **摘要**: 无监督的域适应（UDA）方法打算通过使用未标记的目标域和标记的源域数据来减少源和目标域之间的差距，但是，在医疗域中，目标域数据可能并不总是很容易获得，并且获取新样本通常是在耗时的。这限制了新领域的UDA方法的开发。在本文中，我们在更具挑战性的情况下探讨了UDA的潜力，而现实的情况只有一个未标记的目标患者样本可用。我们称之为几乎没有监督的域适应性（FUDA）。我们首先从源图像中生成目标风格的图像，并探索具有随机自适应实例标准化（RAIN）的单个目标患者的多种目标样式。然后，通过生成的目标图像以监督的方式对细分网络进行训练。我们的实验表明，与基线相比，FUDA在目标域上的分割性能提高了0.33的骰子分数，并且在更严格的单杆设置中也可以提高骰子得分的0.28。我们的代码可在\ url {https://github.com/mingxuangu/few-shot-uda}上找到。

## 统计理论(math.ST:Statistics Theory)

该领域共有 1 篇论文

### Eikonal depth: an optimal control approach to statistical depths 
[[arxiv](https://arxiv.org/abs/2201.05274)] [[cool](https://papers.cool/arxiv/2201.05274)] [[pdf](https://arxiv.org/pdf/2201.05274)]
> **Authors**: Martin Molina-Fructuoso,Ryan Murray
> **First submission**: 2022-01-13
> **First announcement**: 2022-01-14
> **comment**: No comments
- **标题**: Eikonal深度：统计深度的最佳控制方法
- **领域**: 统计理论,机器学习,偏微分方程分析,机器学习
- **摘要**: 统计深度为较高维度的数据提供了对分位数和中位数的基本概括。本文提出了一种基于控制理论和Eikonal方程的新型全球定义的统计深度，该深度衡量了最小的概率密度，该密度必须在分布支持以外的点以外的指数中通过：例如空间无限。该深度易于解释和计算，表达捕获多模式的行为，并自然地扩展到非欧盟人的数据。我们证明了该深度的各种特性，并提供了计算考虑因素的讨论。特别是，我们证明了这种深度概念在异位限制的对抗模型下是强大的，这是Tukey深度不符合的属性。最后，我们在二维混合模型和MNIST的背景下给出了一些说明性示例。

## 地球物理学(physics.geo-ph:Geophysics)

该领域共有 1 篇论文

### Direct multi-modal inversion of geophysical logs using deep learning 
[[arxiv](https://arxiv.org/abs/2201.01871)] [[cool](https://papers.cool/arxiv/2201.01871)] [[pdf](https://arxiv.org/pdf/2201.01871)]
> **Authors**: Sergey Alyaev,Ahmed H. Elsheikh
> **First submission**: 2021-11-29
> **First announcement**: 2022-01-06
> **comment**: Submitted to Earth and Space Science
- **标题**: 使用深度学习直接对地球物理原木的多模式反转
- **领域**: 地球物理学,机器学习
- **摘要**: 井的地理座谈需要快速解释地球物理日志，这是一个非唯一的反问题。当前的工作提出了一种使用人工深神经网络（DNN）的单一评估来对日志进行多模式概率反转的概念验证方法。混合物密度DNN（MDN）是使用“多个防护性预测”（MTP）损耗函数训练的，该功能避免了传统MDN的典型模式崩溃，并允许在数据之前进行多模式预测。关于伽马射线日志的实时地层反转验证了所提出的方法。多模式预测指标输出了几种可能的逆解决方案/预测，与使用DNN进行确定性回归相比，提供了更准确和更现实的解决方案。对于这些可能的地层曲线，该模型同时预测了它们的概率，这些概率是从训练地质数据中隐含的。从MDN中获得的地层预测及其概率可以在地质不确定性下实现更好的实时决策。

## 神经元和认知(q-bio.NC:Neurons and Cognition)

该领域共有 2 篇论文

### Inferring Brain Dynamics via Multimodal Joint Graph Representation EEG-fMRI 
[[arxiv](https://arxiv.org/abs/2201.08747)] [[cool](https://papers.cool/arxiv/2201.08747)] [[pdf](https://arxiv.org/pdf/2201.08747)]
> **Authors**: Jalal Mirakhorli
> **First submission**: 2022-01-21
> **First announcement**: 2022-01-24
> **comment**: 13 pages, 2 figures
- **标题**: 通过多模式关节图表来推断大脑动力学EEG-FMRI
- **领域**: 神经元和认知,人工智能
- **摘要**: 最近的研究表明，多模型方法可以为分析脑部成分的分析提供新的见解，而这些分析是在单独获取每种模态时不可能的。不同模态的联合表示是一个强大的模型，用于分析同时获得的脑电图和功能磁共振成像（EEG-FMRI）。精确仪器的进步使我们能够通过非侵入性神经成像技术（例如EEG＆fMRI）观察人脑的时空神经动力学。流的非线性融合方法可以在时间和空间的不同维度中提取有效的大脑成分。基于图的分析与大脑结构具有许多相似之处，可以克服大脑映射分析的复杂性。在整个过程中，我们概述了几种不同媒体的相关性，从一个来源转移了基于图和深度学习方法。确定重叠可以为诊断神经可塑性研究的功能变化提供新的观点。

### Multimodal neural networks better explain multivoxel patterns in the hippocampus 
[[arxiv](https://arxiv.org/abs/2201.11517)] [[cool](https://papers.cool/arxiv/2201.11517)] [[pdf](https://arxiv.org/pdf/2201.11517)]
> **Authors**: Bhavin Choksi,Milad Mozafari,Rufin VanRullen,Leila Reddy
> **First submission**: 2021-12-10
> **First announcement**: 2022-01-27
> **comment**: Oral at SVRHM Workshop (NeurIPS 2021)
- **标题**: 多模式神经网络更好地解释海马中的多毒素模式
- **领域**: 神经元和认知,机器学习,神经和进化计算
- **摘要**: 人类海马具有“概念细胞”，即在表现出属于特定概念的刺激时发射的神经元，而不管方式如何。最近，在称为夹子的多模式网络中发现了类似的概念细胞（Radford et。，2021）。在这里，我们询问夹子是否可以比纯粹的视觉（或语言）模型更好地解释人类海马的功能磁共振成像活性。我们将分析扩展到一系列公开可用的单模式模型。我们证明，在评估网络解释海马中多毒素活性的能力时，“多模式”是一个关键组成部分。

## 交易和市场微观结构(q-fin.TR:Trading and Market Microstructure)

该领域共有 1 篇论文

### DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture Fleeting Intraday Trading Opportunities 
[[arxiv](https://arxiv.org/abs/2201.09058)] [[cool](https://papers.cool/arxiv/2201.09058)] [[pdf](https://arxiv.org/pdf/2201.09058)]
> **Authors**: Shuo Sun,Wanqi Xue,Rundong Wang,Xu He,Junlei Zhu,Jian Li,Bo An
> **First submission**: 2021-12-15
> **First announcement**: 2022-01-24
> **comment**: No comments
- **标题**: DeepScalper：一种风险感知的加强学习框架，以捕获短暂的盘中交易机会
- **领域**: 交易和市场微观结构,人工智能,机器学习
- **摘要**: 强化学习（RL）技术在许多具有挑战性的定量交易任务（例如投资组合管理和算法交易）中取得了巨大的成功。尤其是，由于金融市场的盘中行为反映了数十亿个迅速波动的首都，所以盘中交易是最有利可图和有风险的任务之一。但是，绝大多数现有的RL方法都集中在相对较低的频率交易方案（例如日级）上，并且由于两个主要挑战而无法捕获短暂的内盘投资机会：1）如何有效地培训有利可图的RL RL投资代理，以供日内投资决策，涉及高衰减的高级良好的细化的精美的良好的良好的良好的良好的动作空间； 2）如何学习有意义的多模式市场表示，以了解tick级金融市场的盘中行为。由专业人类盘中交易者的有效工作流程的促进，我们提出了DeepScalper，这是一个深入的加强学习框架，用于解决上述挑战。具体而言，DeepScalper包括四个组成部分：1）针对行动分支的决斗Q-Network，以应对日内交易的较大动作空间，以进行有效的RL优化； 2）具有事后奖励的新型奖励功能，以鼓励RL代理商在整个交易日的长期范围内做出交易决策； 3）一种编码器架构，用于学习多模式的临时市场嵌入，其中包括宏观和微观市场信息； 4）在最大化利润和最大程度地降低风险之间保持惊人平衡的风险意识辅助任务。通过对六个金融期货的三年来对现实世界中数据的广泛实验，我们证明，DeepScalper在四个财务标准方面显着优于许多最先进的基线。

## 机器学习(stat.ML:Machine Learning)

该领域共有 2 篇论文

### Coupled Support Tensor Machine Classification for Multimodal Neuroimaging Data 
[[arxiv](https://arxiv.org/abs/2201.07683)] [[cool](https://papers.cool/arxiv/2201.07683)] [[pdf](https://arxiv.org/pdf/2201.07683)]
> **Authors**: Li Peide,Seyyid Emre Sofuoglu,Tapabrata Maiti,Selin Aviyente
> **First submission**: 2022-01-19
> **First announcement**: 2022-01-20
> **comment**: No comments
- **标题**: 耦合支持张量机分类用于多模式神经成像数据
- **领域**: 机器学习,机器学习,方法论
- **摘要**: 多模式数据出现在各种应用中，其中从多个传感器和不同的成像方式中获取了有关相同现象的信息。从多模式数据中学习在机器学习和统计研究中引起了极大的兴趣，因为这提供了捕获方式之间互补信息的可能性。多模式建模有助于解释异质数据源之间的相互依存关系，发现可能无法从单一模式中获得的新见解并改善决策。最近，已经引入了耦合基质量分解以进行多模式数据融合以共同估计潜在因素并确定潜在因素之间的复杂相互依存关系。但是，大多数先前关于耦合矩阵调节因素的工作都集中在无监督的学习上，并且使用共同估计的潜在因素在监督学习方面几乎没有工作。本文考虑了多模式张量数据分类问题。提出了基于从高级耦合矩阵张量分解（ACMTF）共同估算的潜在因素建立的耦合支撑张量机（C-STM）。 C-STM将单个和共享的潜在因子与多个内核相结合，并估计了耦合矩阵张量数据的最大细边分类器。 C-STM的分类风险显示会融合到最佳贝叶斯风险，从而使其在统计上保持一致。通过模拟研究以及同时进行EEG-FMRI分析来验证C-STM。经验证据表明，与传统的单模分类器相比，C-STM可以利用来自多个来源的信息，并提供更好的分类性能。

### Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey 
[[arxiv](https://arxiv.org/abs/2201.09267)] [[cool](https://papers.cool/arxiv/2201.09267)] [[pdf](https://arxiv.org/pdf/2201.09267)]
> **Authors**: Benyamin Ghojogh,Ali Ghodsi,Fakhri Karray,Mark Crowley
> **First submission**: 2022-01-23
> **First announcement**: 2022-01-24
> **comment**: To appear as a part of an upcoming textbook on dimensionality reduction and manifold learning
- **标题**: 光谱，概率和深度度量学习：教程和调查
- **领域**: 机器学习,计算机视觉和模式识别,机器学习
- **摘要**: 这是有关公制学习的教程和调查论文。算法分为光谱，概率和深度度量学习。我们首先是从距离度量，马哈拉氏症距离和广义的Mahalanobis距离开始的开始。在光谱方法中，我们从使用数据分散的方法开始，包括第一光谱度量学习，Fisher判别分析的相关方法，相关组件分析（RCA），判别组件分析（DCA）和Fisher-HSIC方法。然后，涵盖了大量净边缘度量学习，不平衡的度量学习，本地线性指标适应和对抗性度量学习。我们还解释了在特征空间中用于度量学习的几种内核光谱方法。我们还介绍了关于Riemannian歧管的几何度量学习方法。在概率方法中，我们从输入和特征空间中的崩溃类开始，然后解释邻里组件分析方法，贝叶斯公制学习，信息理论方法以及度量学习中的经验风险最小化。在深度学习方法中，我们首先介绍重建自动编码器和指标学习的监督损失功能。然后，解释了暹罗网络及其各种损失功能，三胞胎开采和三重态采样。还综述了基于Fisher判别分析的深层判别分析方法。最后，我们介绍了多模式深度度量学习，神经网络的几何度量学习以及几乎没有射击的度量学习。

## 其他论文

共有 17 篇其他论文

- [Benchmark Functions for CEC 2022 Competition on Seeking Multiple Optima in Dynamic Environments](https://arxiv.org/abs/2201.00523)
  - **标题**: CEC 2022的基准功能在动态环境中寻求多个Optima的竞争
  - **Filtered Reason**: none of cs.NE in whitelist
- [NOMA Computation Over Multi-Access Channels for Multimodal Sensing](https://arxiv.org/abs/2201.00203)
  - **标题**: 多模式传感的多访问通道上的NOMA计算
  - **Filtered Reason**: none of cs.IT,eess.SP in whitelist
- [A unified software/hardware scalable architecture for brain-inspired computing based on self-organizing neural models](https://arxiv.org/abs/2201.02262)
  - **标题**: 统一的软件/硬件可扩展体系结构，用于基于自组织的神经模型的脑启发计算
  - **Filtered Reason**: none of cs.NE in whitelist
- [Multi-modal data fusion of Voice and EMG data for Robotic Control](https://arxiv.org/abs/2201.02237)
  - **标题**: 用于机器人控制的语音和EMG数据的多模式数据融合
  - **Filtered Reason**: none of cs.RO,cs.HC in whitelist
- [Music2Video: Automatic Generation of Music Video with fusion of audio and text](https://arxiv.org/abs/2201.03809)
  - **标题**: Music2Video：与音频和文字融合的自动生成音乐视频
  - **Filtered Reason**: none of cs.MM,eess.AS,cs.SD,cs.GR in whitelist
- [Ability-Based Methods for Personalized Keyboard Generation](https://arxiv.org/abs/2201.04593)
  - **标题**: 基于功能的个性化键盘生成方法
  - **Filtered Reason**: none of cs.HC in whitelist
- [Understanding Political Polarization via Jointly Modeling Users, Connections and Multimodal Contents on Heterogeneous Graphs](https://arxiv.org/abs/2201.05946)
  - **标题**: 通过在异质图上共同建模用户，连接和多模式内容来了解​​政治两极分化
  - **Filtered Reason**: none of cs.SI in whitelist
- [A Simple Evolutionary Algorithm for Multi-modal Multi-objective Optimization](https://arxiv.org/abs/2201.06718)
  - **标题**: 一种用于多模式多目标优化的简单进化算法
  - **Filtered Reason**: none of cs.NE in whitelist
- [CERBERUS: Autonomous Legged and Aerial Robotic Exploration in the Tunnel and Urban Circuits of the DARPA Subterranean Challenge](https://arxiv.org/abs/2201.07067)
  - **标题**: Cerberus：DARPA地下挑战的隧道和城市电路中的自主腿和空中机器人探索
  - **Filtered Reason**: none of cs.RO in whitelist
- [Complex In-Hand Manipulation via Compliance-Enabled Finger Gaiting and Multi-Modal Planning](https://arxiv.org/abs/2201.07928)
  - **标题**: 通过支持合规的手指步态和多模式计划的复杂操纵
  - **Filtered Reason**: none of cs.RO in whitelist
- [Understanding and Detecting Hateful Content using Contrastive Learning](https://arxiv.org/abs/2201.08387)
  - **标题**: 使用对比学习理解和检测可恨的内容
  - **Filtered Reason**: none of cs.CY,cs.SI in whitelist
- [Soft Tracking Using Contacts for Cluttered Objects to Perform Blind Object Retrieval](https://arxiv.org/abs/2201.10434)
  - **标题**: 使用触点进行软跟踪，以进行混乱的对象进行盲物体检索
  - **Filtered Reason**: none of cs.RO in whitelist
- [LP-UIT: A Multimodal Framework for Link Prediction in Social Networks](https://arxiv.org/abs/2201.10108)
  - **标题**: LP-uit：社交网络中链接预测的多模式框架
  - **Filtered Reason**: none of cs.SI in whitelist
- [Artificial Emotional Intelligence in Socially Assistive Robots for Older Adults: A Pilot Study](https://arxiv.org/abs/2201.11167)
  - **标题**: 老年人的社会辅助机器人中的人造情感智力：一项试点研究
  - **Filtered Reason**: none of cs.RO,cs.HC in whitelist
- [Parallel black-box optimization of expensive high-dimensional multimodal functions via magnitude](https://arxiv.org/abs/2201.11677)
  - **标题**: 通过大小的昂贵高维多模式功能的平行黑盒优化
  - **Filtered Reason**: none of math.OC,cs.CE in whitelist
- [Multimodal Maximum Entropy Dynamic Games](https://arxiv.org/abs/2201.12925)
  - **标题**: 多模式最大熵动态游戏
  - **Filtered Reason**: none of math.OC,cs.RO in whitelist
- [Efficient, Out-of-Memory Sparse MTTKRP on Massively Parallel Architectures](https://arxiv.org/abs/2201.12523)
  - **标题**: 在大规模平行体系结构上有效的，无内存的稀疏mttkrp
  - **Filtered Reason**: none of cs.DC,cs.DS,cs.PF in whitelist
